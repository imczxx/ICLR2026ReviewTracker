{"id": "pNvm002gkE", "number": 4560, "cdate": 1757706907212, "mdate": 1763126404897, "content": {"title": "Entropy-Guided Automated Progressive Pruning for Diffusion and Flow Models", "abstract": "Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy tailored for transformer-based diffusion and flow models. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent transfer entropy as a guiding metric. Second, leveraging the entropy ranking, we propose a zero-shot Neural Architecture Search (NAS) framework during training to automatically determine when and how much to prune. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22× inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.", "tldr": "We propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models, achieving up to 2.22× inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.", "keywords": ["Pruning; Diffusion models; Flow models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7fd792006d688434acfc3e120c45727b3da77b71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an automated pruning framework for diffusion / flow-based generative models that aims to reduce computation while preserving image quality. The approach has two key components. First, it estimates the importance of each block using an entropy-based criterion: the authors approximate the model’s intermediate output distribution as Gaussian, compute its entropy, and then define a “transfer entropy” score as the entropy reduction attributable to a given block by masking that block during forward passes. Second, instead of using hand-tuned pruning schedules, the method couples this importance ranking with a zero-shot neural architecture search style selector based on NTK conditioning and ZiCo-like gradient proxies to decide how aggressively to prune at each stage. The complete pipeline progressively removes channels/heads/layers and then finetunes the surviving subnet. Experiments on image generation benchmarks (e.g., CUB, Flowers, ArtBench) with SiT/DiT-style architectures claim up to ~2× speedup with limited FID/IS degradation. Overall the paper positions itself as a step toward automated, importance-aware compression for modern diffusion-like generators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work tackles an important and timely problem: diffusion and flow models are notoriously expensive at inference time, and structured pruning is an appealing alternative to distillation-style acceleration. The paper’s attempt to unify (i) a per-block “information contribution” metric and (ii) an automated pruning schedule/search mechanism is conceptually neat, and goes beyond simple magnitude pruning or uniformly dropping channels. The notion of measuring how much uncertainty a block removes from the model’s own predictions is intuitively aligned with the role of intermediate denoising/score blocks in diffusion. The staged pruning procedure, coupled with an automated search over candidate subnets using zero-shot proxies, is also attractive from an engineering standpoint: in principle it could reduce manual hyperparameter tuning and adapt pruning depth to each model/task pair. Finally, the empirical section covers multiple datasets and reports both quality metrics (FID/IS) and nominal wall-clock improvements, suggesting practical relevance."}, "weaknesses": {"value": "My main concern is that the technical foundations and empirical evidence are not yet strong enough to justify acceptance in the current form. First, the definition of “transfer entropy” used here is nonstandard and seems only loosely related to the classical information-theoretic notion of directed information flow. In the paper, the “transfer entropy” of a block is essentially computed as the difference between (i) the entropy of the model’s intermediate output distribution under normal inference and (ii) the entropy when that block is masked. This is closer to an ablation-style sensitivity score than to true transfer entropy; the manuscript currently blurs that distinction and does not provide a rigorous argument that this difference in (Gaussian-approximated) entropy meaningfully tracks causal importance for generation quality. This conceptual gap weakens the theoretical positioning of the method and makes it hard to assess whether the metric is principled or just an empirical heuristic.\n\nSecond, even if we accept entropy reduction as an importance surrogate, the way entropy is estimated is quite aggressive. The method appears to approximate the high-dimensional intermediate feature distribution as a (factorized or effectively scalar-variance) Gaussian, so that entropy boils down to something like (\\log \\sigma). This collapses spatial/channel correlations and any multimodality of internal activations. In diffusion models, intermediate features can be highly structured, timestep-dependent, and in some cases multimodal across noise seeds. Reducing that to a single scalar variance per block may be too coarse. The paper does not study how sensitive the block ranking is to this Gaussian/variance approximation versus richer statistics (e.g., low-rank covariance, per-channel covariance). Without such an analysis, it is difficult to know whether the pruning policy is stable, or whether it is in fact almost equivalent to a simple variance/magnitude heuristic.\n\nThird, the block masking procedure used to measure entropy-conditioned importance is not obviously equivalent to actually pruning that block in the final network. Masking a block during a forward pass can perturb normalization layers, residual connections, and downstream statistics in a way that is different from structurally removing the block and then finetuning. The paper informally argues that blocks with low “transfer entropy” can be pruned with minimal harm, but I did not see a systematic study of correlation between the proposed score and the eventual FID/IS drop after true pruning and short finetuning. In other words, we need curves like: rank blocks by the proposed score, prune the worst-k%, finetune for X steps, and report final quality vs. the same experiment using random pruning, magnitude-based pruning, or gradient-sensitivity pruning. Without this ablation, it is hard to conclude that the entropy score is actually better than standard pruning heuristics.\n\nFourth, the second stage of the pipeline leans heavily on zero-shot NAS-style proxies (NTK conditioning, ZiCo-like criteria) to automatically choose pruning stages and ratios. These proxies have been explored primarily on classification networks and supervised tasks, and they are known to sometimes have unstable or task-dependent correlation with final accuracy. Here they are assumed to generalize to generative diffusion backbones, but the paper does not report any empirical rank-correlation analysis between these proxy scores and final image quality metrics such as FID/IS. In the absence of such validation, it is not yet convincing that the automated schedule/search truly “knows” which subnetworks are better, as opposed to performing a heuristic search that happened to work on the reported models.\n\nFifth, the experimental comparison set feels too narrow for a pruning/compression paper in 2025. The diffusion acceleration literature is broad, including: structured pruning methods tailored to diffusion U-Nets and Transformers (e.g., LD-Pruner and related layer/channel dropping approaches), progressive distillation and consistency-model-based acceleration (which reduce sampling steps and thus wall-clock time), and various low-rank / factorization / token dropping baselines. The paper references some of these families but does not always reproduce them under a unified protocol, same hardware, same batch size, and same privacy/compute budget. As a result, it is difficult to interpret claims such as “2.2× speedup with comparable FID”: are we comparing to the strongest known pruning baseline, to consistency/distillation-style accelerators, or just to the uncompressed teacher? Especially for a paper that advertises practical acceleration, the absence of a single, apples-to-apples end-to-end latency and throughput table (same GPU/CPU backend, same scheduler, same sampler steps) across all baselines is a noticeable gap.\n\nFinally, there is an important scalability / overhead question that the paper largely sidesteps. To compute the entropy-based importance, the method must (a) run forward passes with each block masked to estimate conditional entropy and (b) gather statistics across noise seeds / timesteps. For modern diffusion transformers with dozens of blocks, this is potentially expensive. That overhead should be counted against the claimed acceleration, particularly if the pruning pipeline is meant to be automated per task/domain. Right now the paper frames the method as “automated,” but does not quantify how costly that automation itself is in wall-clock terms relative to, say, straightforward knowledge distillation. This missing accounting weakens the practicality claim."}, "questions": {"value": "(1) How robust is the proposed “transfer entropy” score as a predictor of final generative quality after actual pruning (not masking) and short finetuning? Concretely, can the authors provide rank-correlation (e.g., Spearman) between block importance scores and the FID/IS degradation observed when pruning those blocks, and contrast this with simpler alternatives such as magnitude-based pruning, random pruning, or gradient-sensitivity pruning?\n\n(2) The entropy estimation assumes (approximately) Gaussian feature statistics so that entropy essentially depends on a scalar variance term. Why is this modeling choice justified for high-dimensional, timestep-dependent diffusion features, which are typically structured and sometimes multimodal across noise seeds? Have the authors tried richer covariance models, or at least channel-wise covariance, and if so does the block ranking (and pruning outcome) change materially?\n\n(3) The automated stage selection / pruning schedule relies on zero-shot NAS proxies (NTK conditioning, ZiCo-like criteria) that were originally validated on supervised classification. What evidence do we have that these proxies correlate with downstream generative metrics in this setting? Can the authors report an experiment where several candidate subnetworks are ranked by these proxies and then actually finetuned/evaluated to show quantitative correlation with FID/IS?\n\n(4) The experimental section reports up to ~2× speedup, but the comparison appears to be mostly against the original unpruned model. For a fair assessment of practical acceleration, can the authors present a unified table with end-to-end latency and throughput (same hardware, batch size, sampler/scheduler backend) alongside strong baselines such as LD-Pruner–style structured pruning, progressive/consistency distillation that reduces sampling steps, and low-rank / token-dropping approaches? Without that head-to-head comparison, it is difficult to judge whether the proposed method is actually competitive as a deployment strategy.\n\nOverall, while the paper is motivated by an important problem and contains interesting ideas, these conceptual and empirical gaps make it difficult for me to be confident in the generality and reliability of the approach at this stage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Y5HYkkZ5m", "forum": "pNvm002gkE", "replyto": "pNvm002gkE", "signatures": ["ICLR.cc/2026/Conference/Submission4560/Reviewer_7ePD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4560/Reviewer_7ePD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790778822, "cdate": 1761790778822, "tmdate": 1762917441478, "mdate": 1762917441478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "iTPLoKXVJl", "forum": "pNvm002gkE", "replyto": "pNvm002gkE", "signatures": ["ICLR.cc/2026/Conference/Submission4560/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4560/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763126403780, "cdate": 1763126403780, "tmdate": 1763126403780, "mdate": 1763126403780, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntPruner, an entropy-guided automatic progressive pruning framework for transformer-based diffusion and flow models. By leveraging transfer entropy to rank block importance and employing zero-shot Neural Architecture Search (NAS), EntPruner dynamically determines pruning schedules to reduce model size while preserving performance. Experiments demonstrate up to 2.22× inference speedup with minimal degradation in generative quality across multiple datasets. The method significantly improves efficiency and generalization, offering a scalable solution for deploying large generative models in resource-constrained settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The method demonstrates strong performance on downstream tasks.\n+ The proposed block pruning approach achieves a high acceleration ratio."}, "weaknesses": {"value": "The paper overall is good. But I still have some questions/suggestions.\n\n1. The proposed method appears to be evaluated only on downstream tasks, despite being positioned as a general approach. It should ideally be applicable not only to downstream tasks but also to upstream datasets such as ImageNet. While I understand that fine-tuning on ImageNet can be costly, I strongly recommend the authors make an effort to include such an experiment. This would significantly broaden the scope and impact of the paper.\n\n2. Figure 3 does not clearly distinguish between the two stages. It should be improved by explicitly labeling them as \"Ranking Stage\" and \"Pruning Stage.\"\n\n3. It would be beneficial to cite and discuss dynamic models such as DyDiT [1], which also includes experiments on transferring to downstream tasks.\n\n4. Experiments under fewer steps (e.g. 50 and 20) should be added.\n\n\n[1] Dynamic Diffusion Transformer, ICLR 2025\n\n\n\n\nMinor:\n1. When \"Transfer Entropy\" first appears in the Introduction, the authors do not cite any references, making it difficult to understand its purpose and functionality."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A7e5nYVUxD", "forum": "pNvm002gkE", "replyto": "pNvm002gkE", "signatures": ["ICLR.cc/2026/Conference/Submission4560/Reviewer_sZeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4560/Reviewer_sZeC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836972806, "cdate": 1761836972806, "tmdate": 1762917441008, "mdate": 1762917441008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors try to propose a method for pruning the transformer-based diffusion and flow model. The primary contribution includes the use of the entropy to measure the information of a masked block; the introduction of the condition number to measure the stability and convergence; and the ratio between the gradients to measure the convergence and generation efficiency. They conduct experiments on ImageNet and achieve a 50% decrease in the computational complexity at the cost of generation performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors give good intuition and justification about the importance of pruning.\n\nThe authors manage to get a 50% pruning rate."}, "weaknesses": {"value": "The paper's writing needs to be improved. There are a few typos and inconsistencies. For instance, in equation 1, gamma is not defined, and also not used later.\n\nIn Figure 2, the authors said they observed a strong positive correlation between the entropy and the resulting loss of the masked block, but it's not clear from the current results, as the increase of entropy does not seem to be correlated naturally with the loss."}, "questions": {"value": "A 1.76 decrease in FID is actually a lot. Have you compared your method to other models with low computational cost (without pruning) but good generative performance?\n\nHow to achieve a balance between the three metrics. Now it looks like a heavy parameter tuning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qOJWsnEyDm", "forum": "pNvm002gkE", "replyto": "pNvm002gkE", "signatures": ["ICLR.cc/2026/Conference/Submission4560/Reviewer_uex6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4560/Reviewer_uex6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178585475, "cdate": 1762178585475, "tmdate": 1762917440006, "mdate": 1762917440006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntPruner, a pruning framework for transformer-based diffusion and flow models. The method consists of two main components: (1) using \"transfer entropy\" to rank block importance by measuring H(X_out) - H(X_out | Mask{block_i}), and (2) employing zero-shot NAS metrics (NTK condition number and ZiCo) to automatically determine pruning schedules during training. Experiments on DiT-XL/2 and SiT-XL/2 demonstrate up to 2.22× inference speedup with competitive generation quality on ImageNet and three downstream datasets (CUB, Flowers, ArtBench)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong empirical results**: Achieves 2.22× speedup at 50% pruning with average FID increase of only 1.76 (Table 1). On ImageNet, 30% pruning yields FID 3.53 vs LD-Pruner's 6.81—a substantial 48.16% improvement (Table 3).\n2. **Comprehensive experimental coverage**: Tests multiple architectures (DiT, SiT), samplers (DDPM, ODE, SDE), and datasets including out-of-distribution evaluation (ArtBench). Qualitative results (Figures 4-5) show clear visual quality improvements over LD-Pruner.\n3. **Novel application**: To my knowledge, this is the first work applying entropy-based importance metrics to diffusion model pruning. The combination with zero-shot NAS for automatic schedule determination is elegant.\n4. **Thorough comparison with efficient fine-tuning**: Table 2 shows favorable comparison against DiffFit, LoRA, and other parameter-efficient methods while providing inference speedup they cannot offer.\n5. **Progressive pruning framework**: The staged approach (k=4) with automatic sub-network selection mitigates catastrophic forgetting, as evidenced by more stable training curves (Figure 8)."}, "weaknesses": {"value": "1. **Unclear theoretical foundation**: The formulation H(X_out) - H(X_out | Mask{block_i}) in Eq. 4 mathematically corresponds to mutual information I(Block_i; Output), not transfer entropy. Transfer entropy as defined by Schreiber (2000) requires:\n    - Temporal ordering: measuring how X's past influences Y's future\n    - Conditioning on Y's past: TE_{X→Y} = I(Y_{n+1}; X_n^{(k)} | Y_n^{(k)})\n    \n    Your formulation lacks both. Could you clarify: (a) Why is this called \"transfer entropy\"? (b) What is the theoretical justification for treating architectural depth as causality? (c) Have you compared against simply using mutual information or conditional entropy?\n    \n2. **No diffusion-specific contributions**: Despite claiming methods tailored for \"diffusion and flow models\" (Abstract), the approach does not leverage their unique temporal structure:\n    - No timestep-dependent importance analysis (varying importance across t)\n    - No noise schedule consideration (α_t, σ_t in Eq. Section 2.1 are never used)\n    - No trajectory deviation analysis despite discussing flow matching ODEs\n    - Section 2.4's \"Flow Matching specific\" NTK formulation (Eq. 6) is actually generic gradient descent dynamics\n    \n    **Question**: Would your method work identically on non-diffusion transformers (e.g., Vision Transformers for classification)? If yes, the \"tailored for transformer-based diffusion and flow models\" claims seem overstated.\n    \n3. **Missing critical ablations** (Table 4 is insufficient):\n    - No comparison: TE vs. mutual information vs. conditional entropy vs. magnitude-based pruning\n    - No timestep analysis: Does block importance vary across diffusion timesteps t?\n    - No hyperparameter sensitivity: How sensitive to k (number of stages), γ (regularization), D (batches for ZiCo)?\n4. **Zero-shot proxy validation missing**: You use NTK condition number and ZiCo to select sub-networks, but never show:\n    - Correlation between H_κ/H_ZiCo and actual FID/IS\n    - How often the selected sub-network is actually optimal\n    - Why equal weighting (Eq. 10) is appropriate vs. learned weights\n5. **Limited baseline comparison**: Only LD-Pruner is compared in main results. Diff-Pruning (Fang et al., 2023) and BK-SDM (Kim et al., 2024) are cited but not compared. Why not include magnitude-based or gradient-based pruning as basic baselines?"}, "questions": {"value": "1. **On Transfer Entropy**: Can you provide mathematical justification for why Eq. 4 constitutes transfer entropy rather than mutual information? Specifically, where is the temporal causality (past→future) in your formulation?\n2. **On Diffusion-Specific Design**: Can you show timestep-dependent importance analysis? For example, do different blocks dominate at different noise levels (large t vs. small t)? This would strongly support your diffusion-specific claims.\n3. **On Comparison with MI**: What happens if you replace TE with simple mutual information I(Block_i; Output) computed via kernel density estimation? Is there a meaningful difference?\n4. **On Ablations**: Can you provide results comparing TE-based importance vs. magnitude-based (||W||) vs. gradient-based (||∂L/∂W||) pruning? This would clarify the value of the information-theoretic approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8ll32juSH", "forum": "pNvm002gkE", "replyto": "pNvm002gkE", "signatures": ["ICLR.cc/2026/Conference/Submission4560/Reviewer_LhhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4560/Reviewer_LhhB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762354689437, "cdate": 1762354689437, "tmdate": 1762917439628, "mdate": 1762917439628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}