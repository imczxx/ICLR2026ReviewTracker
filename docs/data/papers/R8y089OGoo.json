{"id": "R8y089OGoo", "number": 18486, "cdate": 1758288193100, "mdate": 1759897100269, "content": {"title": "Dichotomous Diffusion Policy Optimization", "abstract": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.", "tldr": "", "keywords": ["reinforcement learning", "diffusion model", "autonomous driving", "robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3636b7b9ed874a7897f8cff5271e04d42293cba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DIPOLE (Dichotomous diffusion Policy improvement), a novel reinforcement learning algorithm for training diffusion-based policies. The authors identify a key challenge in prior work: directly optimizing the standard KL-regularized RL objective,\n$$\\max_{\\pi} \\mathbb{E}[G(s,a)] - \\frac{1}{\\beta} D_{KL}(\\pi||\\mu)$$\nis difficult because its closed-form solution, $\\pi^*(a|s) \\propto \\mu(a|s) \\cdot \\exp(\\beta G(s,a))$, leads to an unstable weighted regression loss where the $\\exp(\\cdot)$ term can explode.\nTo overcome this, DIPOLE proposes a new \"greedified\" KL-regularized objective (Eq. 5). The key insight is that the optimal solution to this new objective can be decomposed into a ratio of two \"dichotomous\" policies:\n1. A \"positive\" policy $\\pi^+ \\propto \\mu(a|s) \\cdot \\sigma(\\beta G(s,a))$\n2. A \"negative\" policy $\\pi^- \\propto \\mu(a|s) \\cdot (1 - \\sigma(\\beta G(s,a)))$\n\nCritically, the authors claim these policies can be trained stably using diffusion models with bounded sigmoid weights ($\\sigma$ and $1-\\sigma$), solving the stability-optimality trade-off.\nFurthermore, the paper shows that sampling from the optimal policy $\\pi^*$ can be achieved by linearly combining the scores of the two dichotomous policies, resulting in an inference rule analogous to Classifier-Free Guidance (CFG):\n$$\\tilde{\\epsilon}(a_t, s, t) = (1 + \\omega) \\epsilon_{\\theta_1}^{+}(a_t, s, t) - \\omega \\epsilon_{\\theta_2}^{-}(a_t, s, t)$$\nThe authors demonstrate DIPOLE's effectiveness on ExORL and OGBench benchmarks and scale it successfully to a 1-billion parameter vision-language-action (VLA) model for autonomous driving."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Originality**: The primary strength is the novel formulation. The idea of decomposing the optimization into a reward-maximizing ($\\pi^+$) and a reward-minimizing ($\\pi^-$) policy is creative and provides a new lens for policy optimization.\n2. **Strong Empirical Results & Scalability**: The method clearly works well in practice. The successful application of DIPOLE (using LoRA) to a 1-billion parameter VLA model is a significant achievement and demonstrates the method's practical utility for fine-tuning large models with RL."}, "weaknesses": {"value": "1. **Questionable Necessity (Unsound Premise)**: The paper's motivation is that the standard KL-regularized objective (Eq. 2) is unusable because its solution (Eq. 3) implies an unstable weighted regression (Eq. 4). This implicitly assumes that weighted regression is the only way to optimize this objective. This premise is challenged by recent work like BDPO (Gao et al., 2025, ICML), which tackles the exact same standard objective. BDPO shows that by decomposing the $D_{KL}$ term along the diffusion path ($D_{KL}[p_{0:N}^\\pi || p_{0:N}^\\nu]$), the objective becomes a sum of per-step, analytic KL divergences $\\sum_n D_{KL}[p_{n-1|n}^\\pi || p_{n-1|n}^\\nu]$. Since these per-step transitions are Gaussian, this penalty becomes a simple, stable, analytic MSE between the predicted noise vectors (Eq. 17 in BDPO). This suggests that DIPOLE's \"greedified\" objective (Eq. 5) is an overly complex solution to a problem that has a simpler, more direct solution within the original, standard RL framework.\n2. **Flawed Training Mechanism (Sigmoid Saturation)**: The core of DIPOLE's solution is the replacement of $\\exp(\\beta G)$ with the bounded $\\sigma(\\beta G)$ (Eq. 9). This introduces a new, critical problem: signal saturation. The sigmoid function saturates, meaning its output approaches 1 for all values above a certain threshold (e.g., $\\sigma(10) \\approx \\sigma(20) \\approx 1.0$). This means the training loss for the positive policy $\\epsilon^+$ loses all gradient information that distinguishes \"good\" actions from \"excellent\" actions. The network is not learning a fine-grained reward landscape, but rather a near-binary classification of \"good\" (weight $\\approx 1$) vs. \"bad\" (weight $\\approx 0$).\nThis saturation flaw directly contradicts the goal of the inference step (Eq. 10). The inference mechanism $\\tilde{\\epsilon} = (1+\\omega)\\epsilon^+ - \\omega\\epsilon^-$ relies on the \"greediness factor\" $\\omega$ to amplify the difference between the two policies. However, if $\\epsilon^+$ has already lost the high-reward gradient information due to saturation, $\\omega$ is merely amplifying a \"blurry\" or \"clipped\" signal. It's unclear how this can steer the policy towards truly optimal actions if the network was never trained to distinguish them in the first place.\n3. **Gap Between Theory and Practice (The $k$ parameter)**: This saturation flaw is strongly corroborated by the implementation details. The paper's theoretical derivation (Eq. 9) relies purely on $\\sigma(\\beta G)$. However, Appendix D.2 reveals the practical use of a modified weight, $\\sigma(\\beta G + k)$. This 'shift factor' $k$, which is absent from the main theory, serves as strong evidence that the sigmoid-based weighting is not robust. It implicitly confirms the saturation problem, as the model's performance is highly sensitive to the distribution of $G(s,a)$. The mechanism is therefore not as 'principled' as claimed, requiring an ad-hoc hyperparameter to manually shift the sigmoid's non-saturating region to align with the data.\n\n**References**\n\nChen-Xiao Gao, Chenyang Wu, Mingjun Cao, Chenjun Xiao, Yang Yu, Zongzhang Zhang Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:18630-18657, 2025."}, "questions": {"value": "1. On the Premise (re: BDPO): The paper's motivation rests on the instability of the $\\exp(\\beta G)$ weight (Eq. 4). However, recent work (e.g., BDPO) shows that the standard KL-reg objective (Eq. 2) can be optimized stably via a pathwise KL decomposition, resulting in an analytic MSE penalty. Given this, what is the theoretical advantage of proposing a new \"greedified\" objective (Eq. 5)?\n2. On the Training Mechanism (re: Saturation): The core training relies on $\\sigma(\\beta G)$ (Eq. 9). This weight saturates for high $G(s,a)$ values. How can the network $\\epsilon^+$ learn to distinguish between a \"good\" action ($G=10$) and an \"excellent\" action ($G=20$) if the training signal (the weight) is nearly identical for both?\n3. On the Inference Mechanism (re: $\\omega$): Following Q2, if $\\epsilon^+$ has lost the high-reward gradient information due to saturation, how can the inference factor $\\omega$ (Eq. 10) recover this information? Is it not simply amplifying a \"clipped\" or \"binary\" signal, rather than steering the policy towards the truly optimal (e.g., highest $G(s,a)$) actions? Could the authors comment on what $\\epsilon^+$ is actually learning?\n4. On the shift factor $k$: Regarding the 'shift factor' $k$ introduced in Appendix D.2 (using $\\sigma(\\beta G + k)$): This parameter is absent from the theoretical derivation. Could the authors confirm that this is necessary to counteract the sigmoid saturation and center the function's active region over the data's value distribution? How sensitive is the algorithm's performance to the choice of $k$, and doesn't its necessity undermine the robustness and principled nature of the proposed theoretical framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RSm0tKgt5N", "forum": "R8y089OGoo", "replyto": "R8y089OGoo", "signatures": ["ICLR.cc/2026/Conference/Submission18486/Reviewer_ZjGt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18486/Reviewer_ZjGt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797011425, "cdate": 1761797011425, "tmdate": 1762928177052, "mdate": 1762928177052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DIPOLE (Dichotomous Diffusion Policy Improvement), a stable, scalable, and controllable reinforcement learning framework for training large diffusion policies. Existing RL approaches for diffusion policies either (i) directly optimize value or reward objectives, leading to high gradient variance and training instability, or (ii) approximate Gaussian likelihoods across multiple denoising steps, which are computationally expensive and often inaccurate. By revisiting the KL-regularized RL formulation, DIPOLE proposes a greedified KL-regularized objective that naturally decomposes into two dichotomous sub-policies: the positive policy that favors high-reward actions and the negative policy that models low-reward actions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work derives a new closed-form optimal policy under a modified KL objective with a bounded sigmoid weighting, effectively avoiding unstable exponential terms and preventing gradient explosions. The dual-policy decomposition enables learning from both high- and low-reward samples, mitigating data imbalance and overfitting to rare high-reward trajectories. Empirical results demonstrate that DIPOLE consistently outperforms strong baselines across offline, offline-to-online, and large-scale VLA tasks. Moreover, the training procedure remains simple and modular, as it only modifies the loss weights of standard diffusion objectives, maintaining full compatibility with existing architectures."}, "weaknesses": {"value": "1.\tIn DIPOLE, the weight term $\\sigma(\\beta G(s,a))$ (or $1 - \\sigma(\\beta G(s,a))$) is treated as constant with respect to the diffusion model parameters. This means that the diffusion model learns to denoise under static weighting but does not explicitly learn how to adjust the action distribution to improve $G(s,a)$ directly. Consequently, there is no gradient signal guiding the modification of intermediate noisy actions to increase the expected reward, which may lead to slower convergence or plateaued performance when the current policy’s support does not already include near-optimal actions. As a result, the performance of DIPOLE heavily depends on the quality of the value estimator. If $G(s,a)$ overestimates certain actions, the weighting will amplify these errors, making policy improvement rely entirely on value accuracy rather than direct reward gradients. This issue is particularly pronounced in offline RL, where value estimates can be severely biased in out-of-distribution (OOD) regions. Thus, the diffusion process learns primarily through denoising consistency instead of reward shaping across time. Furthermore, since updates are based on weighted regression rather than policy gradient optimization, there is no stochastic gradient noise or entropy regularization to encourage exploration. In online fine-tuning scenarios (e.g., DIPOLE’s autonomous driving setup), this lack of exploratory signal could slow adaptation to unseen environments.\n2.\tThe paper derives the optimal policy formulation but does not provide rigorous convergence guarantees or theoretical error bounds for the dichotomous approximation.\n\n3.\tAlthough the parameter $\\omega$ controls the degree of greediness, the paper lacks quantitative analysis on how $\\omega$ and $\\beta$ jointly influence training stability and performance.\n\n4.\tTraining two separate diffusion models likely doubles computational and memory costs, yet the paper does not report comparisons on training time or efficiency.\n\n5.\tThe method assumes a reliable reward function or Q-estimator $G(s,a)$, but it remains unclear how performance degrades when the value estimates are noisy or biased."}, "questions": {"value": "1.\tFor DP-VLA, the reward shaping, return computation, and LoRA-based adaptation are briefly described but not rigorously analyzed.\n2.\tThe paper uses both temperature $\\beta$ and greediness $\\omega$, how do they jointly affect stability and optimality?\n3.\tThe linear combination resembles CFG, but is $\\omega$ chosen adaptively per state or fixed globally? How does this choice affect exploration–exploitation balance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khWYEXodlw", "forum": "R8y089OGoo", "replyto": "R8y089OGoo", "signatures": ["ICLR.cc/2026/Conference/Submission18486/Reviewer_noDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18486/Reviewer_noDV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965833309, "cdate": 1761965833309, "tmdate": 1762928176694, "mdate": 1762928176694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DIPOLE, a new framework for training diffusion-based policies in goal-conditioned offline reinforcement learning. Instead of using a single diffusion policy with unstable exponential advantage weighting, the authors propose a dichotomous formulation: two separate diffusion policies are trained — one favoring high-return behaviors ($\\pi^+$) and one suppressing low-return behaviors ($\\pi^-$). The final policy is synthesized at inference by combining their score functions. This approach avoids instability caused by unbounded weighting and enables a controllable trade-off between greediness and safety. Empirically, DIPOLE achieves strong performance across several offline RL tasks (ExORL, OGBench), improves stability in training, and scales to large vision-language-action models in autonomous driving (NAVSIM benchmark), outperforming prior imitation-based diffusion policies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a simple but effective method (DIPOLE) that trains two diffusion policies instead of one, helping stabilize learning in offline RL.\n* The method is well-motivated and theoretically justified, avoiding unstable exponential weighting by using bounded scores.\n* Strong experimental results across many tasks, including large-scale vision-language-action models for autonomous driving.\n* The paper is clearly written, well-organized, and easy to follow."}, "weaknesses": {"value": "* The method is only evaluated in offline or offline-to-online settings. I am not sure why the same idea can't be applied to online RL?\n* The baselines for comparison seem random to me. Not sure what are the reasons to choose those baselines as opposed to some other diffusion-based / non-diffusion-based offline RL baselines. For example, there are plenty of model-based offline RL baselines and I think the authors primarily only choose model-free baselines. Is this intentional? What are the rationals behind choosing these baselines? I have read Sec 4.1 but I am not fully convinced by the explanation."}, "questions": {"value": "Can this algorithm generalize to the online setting? There is some recent work on using KL-regularized RL and mirror descent to define the diffusion policy loss function in the online setting (see below reference), which has the same form as Equation 4. I think the same dichotomous idea may also work there. Could you please explain if this will work or not?\n\n\"Efficient Online Reinforcement Learning for Diffusion Policy\", Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai, ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "064gru5FCj", "forum": "R8y089OGoo", "replyto": "R8y089OGoo", "signatures": ["ICLR.cc/2026/Conference/Submission18486/Reviewer_89ky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18486/Reviewer_89ky"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967362834, "cdate": 1761967362834, "tmdate": 1762928176261, "mdate": 1762928176261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DIPOLE, a novel reinforcement learning (RL) framework for optimizing diffusion-based policies. The key idea is to reformulate the KL-regularized RL objective into a “greedified” version that can be decomposed into two dichotomous diffusion policies — one maximizing reward (positive policy) and one minimizing reward (negative policy). During inference, their score functions are linearly combined, enabling controllable trade-offs between greediness and stability.\nExtensive experiments on ExORL, OGBench, and a large-scale autonomous driving benchmark (NAVSIM) demonstrate performance gains over prior RL and diffusion-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed dichotomous decomposition of the KL-regularized objective is both elegant and conceptually novel. The analogy to classifier-free guidance (CFG) provides a strong intuitive and theoretical bridge between diffusion modeling and RL optimization.\n\n2. The paper presents comprehensive experiments across multiple RL benchmarks and an ambitious large-scale 1B-parameter VLA model for end-to-end driving, showing clear improvements over strong baselines (IQL, FQL, CFGRL, etc.).\n\n3. The paper presents comprehensive experiments across multiple RL benchmarks and an ambitious large-scale 1B-parameter VLA model for end-to-end driving, showing clear improvements over strong baselines (IQL, FQL, CFGRL, etc.)."}, "weaknesses": {"value": "1. The reviewer is a little bit confused about why we need to train a policy that minimizes the rewards. In my opinion, to avoid the large difference between the optimized policy and the behavior policy of offline data, we can directly perform imitation learning on the second diffusion policy rather than minimizing the reward. \n2. How can we get $G(s, a)$ in the proposed method? Should we apply some special technique to learn it, such as CQL [R2]?\n3. The method can be classified as a weighted-based diffusion RL method and lacks the citation of the recent weighted-based diffusion RL method [R1].\n\n[R1] Ding S, Hu K, Zhang Z, et al. Diffusion-based reinforcement learning via q-weighted variational policy optimization[J]. Advances in Neural Information Processing Systems, 2024, 37: 53945-53968.\n\n[R2] Kumar A, Zhou A, Tucker G, et al. Conservative q-learning for offline reinforcement learning[J]. Advances in neural information processing systems, 2020, 33: 1179-1191."}, "questions": {"value": "The proposed method requires training two separate policy net\u0002works $\\epsilon^+, \\epsilon^-$, which effectively doubles the computational and storage costs. However, the paper does not discuss or evaluate this overhead: How long is the training time compared to single-policy baselines? During inference, although the final score is obtained by a linear combination, two models must be executed—what is the resulting latency? In the autonomous driving experiments, the authors mention using LoRA to mitigate this issue, but they do not quantify the number of additional parameters introduced by LoRA or its impact on training efficiency"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YulTbNOXwa", "forum": "R8y089OGoo", "replyto": "R8y089OGoo", "signatures": ["ICLR.cc/2026/Conference/Submission18486/Reviewer_jJbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18486/Reviewer_jJbf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18486/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968883567, "cdate": 1761968883567, "tmdate": 1762928175537, "mdate": 1762928175537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}