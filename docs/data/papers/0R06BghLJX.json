{"id": "0R06BghLJX", "number": 19983, "cdate": 1758301197668, "mdate": 1759897008382, "content": {"title": "MaskPro: Linear-Space Probabilistic Learning for Strict (N:M)-Sparsity on LLMs", "abstract": "The rapid scaling of large language models(LLMs) has made inference efficiency a primary bottleneck in the practical deployment. To address this, semi-structured sparsity offers a promising solution by strategically retaining $N$ elements out of every $M$ weights, thereby enabling hardware-friendly acceleration and reduced memory. However, existing (N:M)-compatible approaches typically fall into two categories: rule-based layerwise greedy search, which suffers from considerable errors, and gradient-driven combinatorial learning, which incurs prohibitive training costs. To tackle these challenges, we propose a novel linear-space probabilistic framework named MaskPro, which aims to learn a prior  categorical distribution for every $M$ consecutive weights and subsequently leverages this distribution to generate the (N:M)-sparsity throughout an $N$-way sampling without replacement. Furthermore, to mitigate the training instability induced by the high variance of policy gradients in the super large combinatorial space, we propose a novel update method by introducing a moving average tracker of loss residuals instead of vanilla loss. Finally, we conduct comprehensive theoretical analysis and extensive experiments to validate the superior performance of MaskPro, as well as its excellent scalability in memory efficiency and exceptional robustness to data samples.", "tldr": "We propose a lightweight learnable method for N:M sparse masking.", "keywords": ["Semi-structured sparsity", "Policy gradient learning", "Probabilistic Relaxation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e5346cbf7e2579fa4ba76fa23462dea48c3ce17.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **MaskPro**, a probabilistic framework to learn strict (N:M)-sparsity masks for LLM weights with **linear memory** in the number of parameters. It models each group of M weights with a categorical distribution and generates N active positions via sampling without replacement. It trains logits using a policy-gradient estimator on loss residuals with a moving average baseline to cut variance. A representation theorem rewrites N:M masks as a probabilistic sum of basis vectors; proofs claim unbiased gradients and reduced variance. Experiments on several 7B models show competitive zero-shot accuracy vs rule-based pruning and proximity to MaskLLM at much lower memory and data cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Memory scalability.** Reduces logits storage from combinatorial $O \\left(\\binom{M}{N}^{d/M}\\right)$ to linear $O(d)$. This is the key lever for practicality.  \n- **Clean probabilistic formulation.** N-way sampling without replacement per group with explicit $p(m\\mid \\pi)$ and REINFORCE-style training.  \n- **Variance control.** Loss-residual update with a moving average tracker yields an unbiased estimator with lower variance than vanilla PG under stated conditions. Empirical curves support stability gains.  \n- **Robustness to tiny data.** Reasonable performance even with very few calibration samples; plots include the 1-sample case.  \n- **Empirical breadth.** Evaluates on multiple 7B models and diverse zero-shot tasks; MaskPro beats rule-based baselines by ~2 points on average and narrows the gap to MaskLLM.  \n- **Training efficiency.** Orders-of-magnitude lower memory and dataset size than MaskLLM; single-GPU feasibility reported."}, "weaknesses": {"value": "- **Novelty vs known tools.** The representation of N:M masks via basis-vector sums and a categorical policy with REINFORCE is conceptually straightforward; the main novelty is the linear-space parameterization plus the residual baseline. The paper should better position against prior PG-based pruning and Gumbel/relaxation methods.  \n- **Sampling cost.** The approach still simulates N-way sampling per group at train time. The paper notes this as a time bottleneck but gives limited profiling or asymptotic-to-wall-clock mapping.  \n- **Baseline fairness and scope.** Results focus on (2:4) with (4:8) pushed to appendix. No strong hardware-level throughput benchmarks for inference with real N:M kernels. Comparisons use C4 for all methods, but details like per-layer masks and calibration choices could bias outcomes; MaskLLM numbers are partly taken from prior work.  \n- **Initialization dependence.** Logit init relies on a good starting mask (Top-N or SparseGPT). The sensitivity to poorer inits and to the magnitude hyperparameter is only partly explored.  \n- **Theory conditions.** The variance-reduction claim depends on a condition $f(m_t \\odot w,\\xi) > \\tfrac12 f(m_0\\odot w,\\xi)$. Practical validity across models and tasks is not deeply tested; switching $m_0$ mid-training is suggested but not ablated.  \n- **Model scale.** Only 7B models are reported. No 13B–70B results, where memory and sampling costs and accuracy regressions are more consequential."}, "questions": {"value": "1. How is $p(m\\mid \\pi)$ computed efficiently for N-way sampling without replacement? Provide explicit formulas and complexity, not just appendix references. Can you replace sampling with a differentiable top-N estimator while keeping linear memory?  \n2. How sensitive is performance to the initial mask $m_0$ and the logit magnitude $C$? Report ablations that start from random $m_0$, weaker heuristics, and different $C$.  \n3. Give wall-clock profiles: percent time in sampling, forward passes, and PG updates. Can cached logits or Gumbel-Top-k variants reduce sampling cost further?  \n4. Show end-to-end throughput and latency on A100/H100 with vendor N:M kernels before and after MaskPro, at fixed accuracy. Accuracy-only tables are insufficient to claim deployment readiness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vElXH4ENgk", "forum": "0R06BghLJX", "replyto": "0R06BghLJX", "signatures": ["ICLR.cc/2026/Conference/Submission19983/Reviewer_HgWG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19983/Reviewer_HgWG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659463237, "cdate": 1761659463237, "tmdate": 1762932886994, "mdate": 1762932886994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MaskPro, a memory-efficient probabilistic framework for learning strict (N:M)-structured sparsity in large language models (LLMs). The method models each group of M weights using a categorical distribution and generates sparse masks via N-way sampling without replacement, reducing the memory complexity from exponential to linear. It also proposes a new policy gradient estimator (PGE) that replaces the vanilla loss metric with loss residuals and employs a smoothing tracker to stabilize optimization. Experiments on several 7B-scale models show moderate improvements in performance and significant reductions in memory usage compared to prior approaches such as MaskLLM."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed update rule based on loss residuals, combined with a moving average tracker, is an interesting and practical way to stabilize the inherently noisy gradients in policy-gradient-based mask learning. The authors demonstrate theoretically that this estimator remains unbiased while reducing variance.\n- MaskPro achieves substantial memory and computational efficiency, requiring roughly 36 GB of memory compared to over 300 GB for MaskLLM, while running on a single GPU. \n- The method performs well even when trained with very small datasets (sometimes as few as one sample), showcasing robustness to data scarcity."}, "weaknesses": {"value": "- The experimental evaluation focuses solely on mid-sized (7B) models and only on (2:4) sparsity configurations, with minimal exploration of other ratios or architectures. There is no validation on larger or smaller models, no finetuning experiments, and no runtime or latency measurements to verify practical benefits. As a result, the empirical evidence supporting MaskPro’s claimed generality and scalability remains limited.\n- The method's success is critically dependent on a complex initialization strategy. Standard random or zero initializations are \"ineffective\", and the training fails if the initial logits magnitude $C$ is not large enough.\n- The comparisons with baselines such as MaskLLM, SparseGPT, and Wanda are not entirely fair, as these methods differ in their reliance on fine-tuning versus frozen weights, training data sizes, and initialization strategies. Moreover, MaskPro benefits from initialization using precomputed SparseGPT masks, which gives it an advantage that is not acknowledged or ablated. The paper would be stronger if these factors were controlled more carefully.\n- Although the authors emphasize MaskPro’s efficiency, the claimed linear-space scaling overlooks constant factors associated with sampling and softmax operations, and the actual runtime improvements are not measured. The claim that the method can train effectively with one sample seems implausible without relying on strong priors, undermining the claim of data robustness. More empirical validation is needed to substantiate these statements.\n- The algorithmic description omits critical implementation details such as how N-way sampling without replacement is realized efficiently, how randomness affects reproducibility, and how logits are updated in parallel. The notation (e.g., the $\\oplus$ operator) is nonstandard and lacks intuition, while figures are schematic and do not convey architectural structure or ablation results. Overall, the presentation is mathematically heavy and could be made clearer."}, "questions": {"value": "- Can the authors provide wall-clock runtime and throughput comparisons with MaskLLM and rule-based baselines?\n- How sensitive is MaskPro to the smoothing parameter $α$ and the initialization constant $C$?\n- What happens when masks are initialized randomly instead of using SparseGPT priors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fmUiA16TSh", "forum": "0R06BghLJX", "replyto": "0R06BghLJX", "signatures": ["ICLR.cc/2026/Conference/Submission19983/Reviewer_rhLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19983/Reviewer_rhLs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861325331, "cdate": 1761861325331, "tmdate": 1762932886383, "mdate": 1762932886383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MaskPro, a linear-space probabilistic framework for learning strict N:M sparsity in LLMs. The work addresses the limitations of existing approaches, rule-based methods that introduce bias and learning-based ones that demand prohibitive computational cost, by proposing a lightweight probabilistic formulation. It models each group of weights with a simple categorical distribution and learns sparsity patterns through a refined policy gradient update that incorporates loss residuals and a moving average tracker, improving both stability and convergence. Empirically, MaskPro achieves strong results across multiple 7B-scale models, reaching good accuracy while reducing memory usage by an order of magnitude and requiring only a handful of training samples. The approach is conceptually clear, theoretically supported, and practically appealing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The reformulation of the (N:M) sparsity problem into a linear-space probabilistic model is both interesting and convincing. It reduces the memory requirement from exponential to linear scale and lowers the data demand compared with prior learning-based methods.\n2. MaskPro demonstrates consistently strong performance across diverse benchmarks and model backbones, achieving comparable or superior accuracy to established baselines while maintaining high efficiency.\n3. The approach is remarkably data-efficient, as evidenced in Figure 3, where stable results are obtained even with minimal training samples, making the method highly practical for large-scale applications."}, "weaknesses": {"value": "1. While the results on 2:4 sparsity are strong, it remains to be seen how well MaskPro generalizes to other configurations, such as 8:16. These settings involve significantly larger combinatorial spaces, roughly 12,870 combinations per group in MaskLLM, and would serve as a valuable test of the proposed probabilistic formulation’s scalability.\n2. It would be interesting to explore whether MaskPro can be extended to jointly optimize both the sparsity pattern and the LLM parameters. Such a joint optimization could potentially yield further improvements in performance and adaptability."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hoCblFzPPQ", "forum": "0R06BghLJX", "replyto": "0R06BghLJX", "signatures": ["ICLR.cc/2026/Conference/Submission19983/Reviewer_4Q5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19983/Reviewer_4Q5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861734646, "cdate": 1761861734646, "tmdate": 1762932885415, "mdate": 1762932885415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new probabilistic framework for learning hardware-friendly semi-structured sparsity in LLMs. Unlike prior methods, MaskPro reduces both memory and computation overhead by modeling sparsity as an N-way sampling without replacement from categorical distributions over M consecutive weights. It leverages a linear-space parameterization that reduces memory complexity to linear. and optimizes via a refined PGE that replaces the raw loss with a loss residual tracked by a moving average, improving stability and variance reduction. Extensive experiments on multiple 7B LLMs show that MaskPro outperforms baselines while approaching the their accuracy at 10x less memory and training cost."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Sound theoretical explanation, linear memory efficiency, and comprehensive validation experiments."}, "weaknesses": {"value": "Experiments focus mainly on 2:4 sparsity and 7B models; scaling to higher sparsity ratios or larger models, e.g., 70B, is not demonstrated. (might due to hardware constraints)"}, "questions": {"value": "How sensitive is MaskPro's performance to the choice of $\\alpha$?\n\nDoes the sampling-without-replacement process still scale linearly in memory and remain computationally tractable for large M? This needs further discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ACb518xVqz", "forum": "0R06BghLJX", "replyto": "0R06BghLJX", "signatures": ["ICLR.cc/2026/Conference/Submission19983/Reviewer_NXWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19983/Reviewer_NXWQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19983/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762961281110, "cdate": 1762961281110, "tmdate": 1762961281110, "mdate": 1762961281110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}