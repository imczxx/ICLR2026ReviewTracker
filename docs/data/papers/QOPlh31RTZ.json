{"id": "QOPlh31RTZ", "number": 15686, "cdate": 1758253910203, "mdate": 1759897289053, "content": {"title": "A Constrained Bi-level Optimization Framework for Constrained Reinforcement Learning from Human Feedback", "abstract": "This paper studies the problem of jointly learning a reward function, a cost function, and a policy from human feedback. We formulate the problem as a constrained bi-level optimization, where the upper level infers the reward and cost functions from feedback, while the lower level optimizes a policy to best align with that feedback. To solve this problem, we propose a double-loop algorithm, Constrained Bi-level Optimization for Reinforcement Learning from Human Feedback (CB-RLHF), which solves the lower-level optimization problem in the inner loop and the upper-level optimization problem in the outer loop. We establish a theoretical guarantee that CB-RLHF converges at a rate of $\\mathcal{O}(\\frac{1}{\\sqrt{K}})$, and we demonstrate its empirical effectiveness across multiple simulation environments.", "tldr": "", "keywords": ["Constrained Reinforcement Learning from Human Feedback"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9de221034a3020d2b3af0e3523766ccb1ea772c.pdf", "supplementary_material": "/attachment/d14ffdec0ebe06b1fae448611c2b24ad4b53d520.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of learning from human feedback in reinforcement learning (RL) scenarios that involve constraints. The authors formulate this problem as a constrained bi-level optimization task. In this formulation, the upper-level problem learns reward and cost functions from human feedback, while the lower-level problem solves for the optimal policy under those given functions.\nTo solve this, the paper introduces an algorithm called CB-RLHF. The algorithm handles the non-convexity of the lower-level problem by using its dual formulation and addresses potential non-differentiability using a Clarke subdifferential approximation. The authors provide a theoretical convergence guarantee and present pseudo experiments on MuJoCo environments to show that the method can address both misalignment and constraint inference limitations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Important Problem Formulation: The paper tackles the important and practical problem of learning from human feedback while simultaneously respecting safety or ethical constraints. The formulation of constrained bi-level optimization problem might be a notable contribution.\n- Theoretical Grounding: The authors make a serious attempt to provide a theoretical foundation for their algorithm. They provide proofs for the convergence of their method, which adds rigor to their claims."}, "weaknesses": {"value": "Insufficient Experimental Validation: The experiments are not sufficient to support the paper's claims about RLHF.\n- Unrealistic Oracle: The experiments use a synthetic oracle based on ground-truth functions, which bypasses the core challenges of noisy, ambiguous, and costly feedback from real humans.\n- Missing Key Baseline: The paper fails to compare against the most obvious baseline: a standard, iterative RLHF approach where the update cycle is simply run much more frequently. It is unclear if the proposed complex optimization is better than just iterating faster.\n- Lack of Ablation Study: The method introduces two key components (a cost function and a bi-level framework) but provides no ablation study to disentangle their effects. We cannot know what is truly responsible for the performance changes.\n- Lack of Analysis: The paper presents results but offers almost no analysis. It claims success even when the method underperforms baselines on some metrics (e.g., Hopper return), with no discussion as to why.\n\nLack of Clarity in Theoretical Justification: The lack of clarity makes the core justification for the method confusing."}, "questions": {"value": "Could the authors clarify how strong duality (Lemma 1) is guaranteed to hold for the problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tJ5wy2UgDY", "forum": "QOPlh31RTZ", "replyto": "QOPlh31RTZ", "signatures": ["ICLR.cc/2026/Conference/Submission15686/Reviewer_mhU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15686/Reviewer_mhU2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761040739081, "cdate": 1761040739081, "tmdate": 1762925939096, "mdate": 1762925939096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a bi-level optimization framework for RLHF. The upper-level optimization learns both a reward and a cost function from preference feedback, while the lower-level optimization solves the dual of a constrained RL problem. To handle the non-smoothness of the objective, the authors employ the Clarke subdifferential framework to approximate the hypergradient and provide convergence and sample complexity analyses. Empirical results demonstrate that the proposed algorithm outperforms three baselines across four environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of a bi-level optimization framework into the constrained RLHF setting is well-motivated and appears novel.\n- Although I did not check the proofs in full detail, the proposed method for addressing the non-convex lower-level optimization problem seems mathematically sound and appropriate."}, "weaknesses": {"value": "- Some notations, particularly those related to the cost function, are unclear (see questions below). I also recommend the authors carefully proofread the Appendix. Several proofs are difficult to follow and contain typos, which hinder readability.\n- The experimental results are not sufficiently comprehensive to support the main claims. Including more environments or additional ablation studies would strengthen the empirical evidence."}, "questions": {"value": "- My understanding is that a high value of $J_c (\\tau)$ indicates a trajectory with a high cumulative cost (i.e., a worse trajectory). Then, In line 197, why does the BT model seem to prefer trajectories with higher costs? Furthermore, in Theorem 2, why is the negative cost of the learned policy upper-bounded?\n- In lines 441–444, when generating the synthetic feedback data, is a greedy model used instead of the BT model? If so, could the authors clarify the rationale?\n- Minor typos: (i) In line 401, $\\epsilon$ should be $\\epsilon_k$. (ii) In line 416, $(\\pi_)$ should be $(\\pi_h)$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qu2mCVinNC", "forum": "QOPlh31RTZ", "replyto": "QOPlh31RTZ", "signatures": ["ICLR.cc/2026/Conference/Submission15686/Reviewer_b4zk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15686/Reviewer_b4zk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750525176, "cdate": 1761750525176, "tmdate": 1762925938719, "mdate": 1762925938719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of jointly learning a reward function, a cost function, and a policy from human feedback. The authors formulate the problem as a constrained bi-level optimization problem, where the upper level infers the reward and cost functions from feedback, while the lower level optimizes a policy to best align with that feedback. To solve this problem, the authors propose a double-loop algorithm, Constrained Bi-level Optimization for Reinforcement Learning from Human Feedback (CB-RLHF), which solves the lower-level optimization problem in the inner loop and the upper-level optimization problem in the outer loop. The authors establish a theoretical guarantee that CB-RLHF converges at a rate of $O(1/\\sqrt{K})$, and demonstrate its effectiveness across multiple simulation environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem, constrained reinforcement learning from human feedback (RLHF), is well-motivated and finds important applications in large language model alignment.\n2. The idea of formulating the constrained RLHF problem as a constrained bi-level optimization problem is interesting.\n3. The authors propose a double-loop algorithm and provide a theoretical guarantee on convergence."}, "weaknesses": {"value": "1. The writing and readability of this paper needs to be improved.\n2. In Eq. (1), what is the motivation of using $H(\\pi)$ as a regularization term, instead of the KL divergence with the reference policy?\n3. It is hard to understand the theoretical results provided in this paper. (i) The abstract of this paper mentioned that they prove that algorithm CB-RLHF converges at a rate of $O(1/\\sqrt{K})$. However, Theorem 1 only states that the gradient is bounded by $O(1/\\sqrt{K})$. How does this result imply the convergence rate to the globally optimal policy?  \n(ii) In Theorem 2, what are the definitions of SubOptR and SubOptC, in particular, what is the definition of human policy $\\pi_h$? Does Theorem 2 provide the performance gap between the optimal policy and the output policy of algorithm CB-RLHF? If it is, why is it not dependent on $K$?  \n4. The constrained LLM alignment problem has been studied in several prior works. The authors should discuss more on the advantages of the proposed constrained bi-level optimization approach compared to the existing primal-dual approaches, e.g., Safe RLHF [Dai et al., 2023].  \n5. This paper only provides experiments on MuJoCo. It would enhance this paper if the authors can provide experiments on LLMs."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RwrZ4zozwC", "forum": "QOPlh31RTZ", "replyto": "QOPlh31RTZ", "signatures": ["ICLR.cc/2026/Conference/Submission15686/Reviewer_Ccqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15686/Reviewer_Ccqt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828204991, "cdate": 1761828204991, "tmdate": 1762925938135, "mdate": 1762925938135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CB-RLHF, a bi-level optimization framework for solving constrained RLHF problems.  \n1. The upper level learns reward and cost functions from human feedback, while the lower level solves a dual-convex reformulation of the constrained RL problem using the Lagrangian dual.  \n2. The method employs Clarke subdifferential and gradient approximation to handle non-differentiable hypergradients and proves a convergence rate of O(1/√K) and an approximation error of O(1/√N).  \n3. The algorithm is compared against PEBBLE, Safe-RLHF, and PARL on four MuJoCo tasks, showing that CB-RLHF improves constraint satisfaction and return performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical formulation appears rigorous and well-grounded, even though I didn't fully verify all the proof details.  \n2. The paper provides a complete methodological pipeline—from problem formulation to convergence theory and experiments—with a coherent narrative.  \n3. The combination of theory and empirical validation makes the proposed framework conceptually convincing."}, "weaknesses": {"value": "1. The method critically relies on strong duality to convert a non-convex constrained problem into a convex one, yet the paper does not discuss or verify the duality gap.  \n   In practical RLHF settings involving approximation and sampling, zero duality gap is unlikely to hold. The authors should analyze or empirically estimate the duality gap’s impact on convergence and correctness, or at least clarify conditions under which strong duality is justified.  \n\n2. The bi-level structure is theoretically elegant but computationally heavy. Frequent estimation of λ*(ϕ, ψ) and inner-loop optimization make the algorithm expensive.  \n   Its feasibility for large-scale RLHF—especially in LLM fine-tuning scenarios—remains untested. The paper would benefit from demonstrating CB-RLHF on a small-scale language model (e.g., 1.5B parameters) to establish practical viability.  \n\n3. The experimental design mixes reward and cost by defining return = reward − cost, which is inconsistent with the problem formulation that treats reward and cost separately.  \n   Because reward and cost are in different units and the dual algorithm is scale-insensitive (replacing λ, c with αλ, c/α does not dramatically change results), this modification obscures interpretability. It would be more principled to report cumulative reward and constraint violation separately.  \n\n4. Experimental fairness is questionable. The total training steps vary across the four environments, and results seem sensitive to stopping criteria.  \n   For instance, in Walker2D, using 1e5 steps (like HalfCheetah and Swimmer) may allow PARL to outperform CB-RLHF; conversely, reducing HalfCheetah to 0.6e5 steps (like Hopper) makes CB-RLHF weaker in both return and constraint violation. This suggests possible cherry-picking of stopping points.  \n\n5. The proposed framework effectively assumes a Bradley–Terry preference model. As such, its applicability is limited to preference-based feedback rather than general RLHF (which can involve scalar rewards, rankings, or textual feedback).  \n   The title should more accurately read “A Constrained Bi-level Framework for Preference-based Human Feedback” rather than “RLHF,” which implies broader generality.  \n\n6. Minor issue: in line 212, z₀ and z₁ should be s₀ and s₁."}, "questions": {"value": "1. How is the number of inner-loop dual updates tₖ determined? Is it adaptive, fixed, or tuned per environment?  \n2. Can the authors provide theoretical guarantees or empirical analysis under small (non-zero) duality gaps? What happens if strong duality fails slightly—does convergence degrade gracefully?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sFoaJTFW5F", "forum": "QOPlh31RTZ", "replyto": "QOPlh31RTZ", "signatures": ["ICLR.cc/2026/Conference/Submission15686/Reviewer_bG2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15686/Reviewer_bG2X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959516666, "cdate": 1761959516666, "tmdate": 1762925937690, "mdate": 1762925937690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}