{"id": "0pGKVsri2B", "number": 15505, "cdate": 1758252093936, "mdate": 1759897302430, "content": {"title": "Enhancing Graph Generation With First-Order Logic Rules", "abstract": "Existing graph generative models produce graphs that are often quite realistic, but sometimes miss domain-specific patterns. Enhancing graph learning with domain knowledge is one of the current frontiers for neural models of graph data. In this paper, we propose a new approach to enhancing deep graph generative models with knowledge that is represented by first-order logic rules. First-order logic provides an expressive formalism for representing interpretable causal knowledge about relational structures. Our conceptual contribution is a new first-order semantic loss function for training a graph generative model on relational data: maximize the model likelihood subject to a rule moment matching constraint, namely that the expected instance count of each rule matches its observed instance count. Our algorithmic contribution is a novel method for computing the expected instance count of a first-order rule for a Variational Graph Autoencoder model, based on matrix multiplication. Empirical evaluation on five benchmark datasets, both homogeneous and heterogeneous, shows that rule moment matching improves the quality of generated graphs substantially (by orders of magnitude on standard graph quality metrics), and improves predictive accuracy on the downstream task of node classification.", "tldr": "Shows how deep graph generation can be enhanced with domain knowledge represented by first-order logic rules with a novel semantic loss function", "keywords": ["Deep graph generation", "First-Order Logic Rules", "Neuro-Symbolic AI", "Deep Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/163ca42fdc40470dba103a6202c93a72814715ce.pdf", "supplementary_material": "/attachment/85a0c45eb4fa568031e8088e0a1de858d0c18328.zip"}, "replies": [{"content": {"summary": {"value": "While existing graph generation methods can achieve realistic results, the generated graphs lack domain-specific patterns. This paper proposes a method that uses first-order logic to enhance VGAE, making the domain-specific patterns in VGAE-generated graphs more closely match those in the original input graphs. The paper introduces a matrix multiplication-based approach to compute the number of domain-specific patterns in generated graphs, enabling their incorporation into the loss function. The authors validate their method on multiple datasets, demonstrating that it effectively improves the quality of generated graphs compared to the original VGAE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes that generated graphs and original input graphs should have a similar number of rules, which is an intuitive and easily understandable idea.\n2. The paper proposes an automated feature extraction method, which is valuable.\n3. To efficiently compute the number of instances of first-order logic rules, the paper develops a novel matrix multiplication-based algorithm. This algorithm can be applied to standard graph generation hybrid models, and for conjunctive formulas that satisfy mild syntactic constraints, the number of instances can be computed through a series of adjacency matrix multiplication operations.\n4. The paper thoroughly discusses the limitations/shortcomings of its own method."}, "weaknesses": {"value": "1. The paper only uses the original VGAE as baseline, while the current mainstream methods for graph generation are diffusion methods and flow matching methods. Although the authors' intention may be to enhance VGAE, I believe it is necessary to compare with the current mainstream diffusion and flow matching methods. In DiGress[1] and CatFlow[2], it is mentioned that on molecular graphs, the validity of results generated by flow matching and diffusion methods is superior to that of ordinary VGAE (if I understand correctly, validity should correspond to the domain-specific patterns in this paper). If the proposed method cannot outperform flow matching and diffusion methods in this aspect, I believe this paper's method lacks practical value, or the authors should demonstrate the advantages of VGAE over flow matching and diffusion methods.\n\n2. In the GRAPH REALISM experiments, the paper directly uses MMD as the metric. Such enormous values like 4.03e18 and 6.84e17 are difficult to understand, making it impossible to know what level the original VGAE's GRAPH REALISM is at, and it's also hard to understand how low the MMD of VGAE+R actually is. I suggest using some completely random method as a baseline, using that method's MMD value as a reference point, and normalizing the results of other methods accordingly to facilitate understanding of the results.\n\n[1] Vignac, Clement, et al. \"Digress: Discrete denoising diffusion for graph generation.\" arXiv preprint arXiv:2209.14734 (2022).\n[2] Eijkelboom, Floor, et al. \"Variational flow matching for graph generation.\" Advances in Neural Information Processing Systems 37 (2024): 11735-11764."}, "questions": {"value": "1. The paper's motivation is that while existing graph generation methods can achieve realistic results, the generated results lack domain-specific patterns and are not semantically valid enough. I don't quite understand what the difference is between realistic and semantically valid?\n2. In the matrix calculation examples in the paper, all matrix elements are binary. In practice, the elements of this matrix should be continuous values. Can you provide an example with continuous values? To be honest, your method is not explained clearly enough, and a more practical example would help better understand your approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ErLNppFjpz", "forum": "0pGKVsri2B", "replyto": "0pGKVsri2B", "signatures": ["ICLR.cc/2026/Conference/Submission15505/Reviewer_UNFG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15505/Reviewer_UNFG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537248488, "cdate": 1761537248488, "tmdate": 1762925793820, "mdate": 1762925793820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors add a first-order (FO) “semantic loss” that moment-matches rule instance counts while training a graph generator; compute rule counts via matrix multiplications on (soft) adjacency/features."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed model establishes a clear and principled connection between SRL/MLN “moment matching” and modern GGM training. In the special case, the FO loss simplifies to propositional semantic loss.\n\n2. Develop a practical counting routine for chain-conjunctive formulas that can be differentiated on probabilistic graphs."}, "weaknesses": {"value": "1. The primary training objective is classic MLN/maximum-entropy moment matching with a neural likelihood term. The novelty lies mainly in an efficient instantiation for centered chain conjunctions using matrix products. It’s uncertain how broadly this extends beyond chains without combinatorial blow-ups or ad-hoc handling. The paper formalizes the chain case (Prop. 3) but lacks comparable rigor for general FO motifs. \n2. Authors (i) learn rules from the training graph (Factorbase), (ii) regularize the generator to match those rule counts, and (iii) evaluate using GNN-MMD with a random reference network. This approach carries the risk of circularity: the model is intentionally nudged to replicate the statistics that the rules capture. A random-weight embedder might inadvertently favor these count-aligned patterns, leading to inflated MMD improvements. The paper only considers random-weight R, neglecting stronger alternatives (e.g., pretrained task-agnostic embedders) that might be less susceptible to a specific set of motifs. It is crucial to provide a justification for why random-GNN MMD is the appropriate arbiter in this context and to include complementary evaluations beyond MMD.\n3. The authors introduce self-loops and symmetrize edges to facilitate message passing. However, it’s crucial to determine whether the counts are matched on the preprocessed graph or the original graph. This distinction can significantly impact motif counts, particularly for triangles and cycles, thereby altering the effective constraint. Therefore, it’s essential to standardize and document the counting domain."}, "questions": {"value": "1. Are rule counts matched on the original graph or on the preprocessed (self-looped, undirected) graph used for message passing? \n2. How robust are results to noisy/misaligned rules? (E.g., random or adversarial rules with high BIC score due to spurious correlations.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vaSd7jB0cf", "forum": "0pGKVsri2B", "replyto": "0pGKVsri2B", "signatures": ["ICLR.cc/2026/Conference/Submission15505/Reviewer_abnm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15505/Reviewer_abnm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635709062, "cdate": 1761635709062, "tmdate": 1762925793437, "mdate": 1762925793437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a semantic loss to be combined with a variational graph auto-encoder (VGAE) in order to inject domain information expressed in the form of first-order logical rules. The key aspects of the loss are that (1) is efficient to compute in practice, and (2) maximizes the training graph's likelihood while minimizing a divergence between observed and expected instance counts. This loss is claimed to improve the quality of the graphs generated by the method, when measured using graph motif counts or by comparing graph embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I believe the paper does a good job at explaining the proposed method and why it should bring improvements while being efficient. Although I think some parts are missing some important details and notation is unclear (see weaknesses), I found the paper to be sufficiently clear and easy to understand.\n\nI have found the experimental setting sufficiently clear. However, my expertise does not enable me to verify whether the large numbers reported in Table 2 for count distance and graph realism make sense (see my discussion in weaknesses)."}, "weaknesses": {"value": "Although I have appreciated the writing, I think the notation is often unclear.\n- In L248 $p_{\\eta}(A\\mid z)$ is said to be a function from $\\mathbb{R}^d\\times\\mathbb{R}^d$ to $[0,1]$. To my understanding, $p_{\\eta}$ defines the likelihood _of a single link_ using the node embeddings. However, this is contrast with the notation where the distribution is conditioned on some latent vector $z$. Is $z$ denoting the embeddings of _all the nodes_? For similar reasons, the notation used for feature and label decoders is not clear.\n- Are the summations in L19 and L20 of Algorithm 1 over all the entries of the matrix? I think adding comments in the algorithm would help.\n- L319. It is not clear what symbols A, X, L with tildes refer to.\n\nI believe the authors could have detailed how their work and the literature about probabilistic databases are related. See e.g. [A] and [B], which are not cited. Although the authors mention the work [C], I was not able to understand how Eq. 1 is directly related to model counting. That is, model counting requires us to sum the weights taken over _all possible worlds_ that are consistent with the logical formula. Eq. 1 only considers conjunctive formulae and the sum is defined over all possible assignments _of only the variables that appear in the logical formula_, which usually consists of only a subset of the relation symbols. Could you please clarify the relationship between the probabilistic instance count (Eq. 1) and the works [A], [B] and [C]?\n\n[A] Nilesh N. Dalvi, Dan Suciu. Efficient Query Evaluation on Probabilistic Databases. 2004.\n[B] Nilesh N. Dalvi, Dan Suciu. The dichotomy of probabilistic inference for unions of conjunctive queries. 2012.\n[C] Ondrej Kuzelka. Counting and sampling models in first-order logic. In International Joint Conference on Artificial Intelligence. 2023.\n\nIn L258-259 the authors mention that \"FO moment matching can be implemented by performing (probabilistic) instance counting in a single graph\". I found this sentence very confusing, as there is not a single graph, but rather a distribution over graphs. The authors say only later in L318-319 and Appendix A.9.2. that the algorithm is run with the probabilistic adjacency matrices, i.e., with entries between 0 and 1.\n\nIn Table 2 the authors show average percentage improvements. Standard deviations are only reported for raw count distance values. I am confident with the significance of the shown results, as the standard deviations are very high (and sometimes close to the means as for the IMBDb dataset). Although the authors mentions some works about the graph realism metric (L364), I could not find previous results having the same order of magnitudes, which arises doubts about how these metrics have been calculated and used.\n\nThe lack of clarity in certain parts about the methodology and about some of the reported numbers is what makes me lean towards a much lower score. I am happy to reconsider these aspects during the rebuttal."}, "questions": {"value": "See my questions and points made in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEaFuk2czI", "forum": "0pGKVsri2B", "replyto": "0pGKVsri2B", "signatures": ["ICLR.cc/2026/Conference/Submission15505/Reviewer_Cj78"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15505/Reviewer_Cj78"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999300213, "cdate": 1761999300213, "tmdate": 1762925793060, "mdate": 1762925793060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}