{"id": "SeQqnw5eNj", "number": 22988, "cdate": 1758337865144, "mdate": 1759896837391, "content": {"title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security", "abstract": "Large Language Models (LLMs) are vulnerable to a range of threats, from adversarial attacks like jailbreaking to the leakage of sensitive information that should have been unlearned. Existing defense mechanisms are often static and require extensive model retraining, making them slow to adapt to evolving threats. To address this, we propose \\ours, a cooperative multi-agent framework that provides adaptive, test-time defense for LLMs. At the core of \\ours is a multi-agent system that can be optimized with a remarkably small number of examples to achieve strong performance on multiple, distinct security challenges. We demonstrate the effectiveness of \\ours on two critical and distinct threats: unlearning and jailbreaking. On the WMDP unlearning benchmark, \\ours achieves near-perfect unlearning (approaching 25\\% random chance) with only 20 training examples. For jailbreaking, our system improves defense by 51\\% over the base model on the StrongReject benchmark, while maintaining a low false refusal rate of only 7.9\\% on PHTest. Furthermore, we show that prompts optimized on one benchmark generalize effectively to others, underscoring the robustness of our approach. Our work highlights the significant advantages of adaptive, agentic reasoning and demonstrates the power of optimization for creating scalable and efficient LLM safety solutions. We provide our code at: https://anonymous.4open.science/r/aegisllm-11B0.", "tldr": "AegisLLM is a test-time multi-agent defense that adaptively protects LLMs from security threats like jailbreaking and sensitive information disclosure without retraining.", "keywords": ["unlearning", "security", "safety", "jailbreaking", "llm"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c3a9bfe54720f7efb373600df992bfda27424ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a multi-agent defense framework called AegisLLM to counter different LLM threats. In particular, the framework consists of four agents (Orchestrator, Deflector, Responder, and Evaluator) and utilizes automated prompt optimization (DSPy) to fine-tune the system prompts of these agents. Comprehensive experiments demonstrate significant defense improvements over baselines in two threat scenarios: \"unlearning\" (removing or preventing knowledge leakage) and \"jailbreaking\" (resisting adversarial inputs), while largely preserving the clean performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ While multi-agentic defense is not new to the literature, the paper introduces a new idea of automated prompt optimization to refine the agents at test time, which could be a practical pathway to enhance the power of multi-agent defenses\n\n+ The paper presents comprehensive experimental results and demonstrates a significant performance gain for the proposed method\n\n+ While greatly enhancing the protective performance, the proposed method can also preserve clean utility and be efficient, which is promising"}, "weaknesses": {"value": "- The Bayesian prompt optimization module is vaguely described in the methodology section\n\n- The use of PHTest for evaluating the clean performance of defenses can be questionable\n\n- Limited experiments are presented on frontier closed-source LLMs, which question the generalizability of the work\n\n- The key insights why the proposed framework can be adaptable to defend distinct threats are not explained well"}, "questions": {"value": "While developing a cooperative multi-agent defense framework that is efficient and adaptable to different categories of security threats does sound intriguing, I believe the paper falls short in the following critical aspects that require detailed clarifications:\n\n1. The paper highlights the novelty of the Bayesian prompt optimization module in the AegisLLM framework, but provides almost no detail about how prompts are parameterized, which acquisition function is used, and how the Bayesian optimization process is carried out. The descriptions of the module in Section 2.2 stay at a very conceptual level. The missing implementation details of such a key module raise questions about the method's reproducibility and also undermine the technical contributions of the work.\n\n2. The use of PHTest for “clean” utility evaluation is questionable, or at least unusual. The dataset was created to generate pseudo-harmful, controversial prompts, rather than serving as a general-purpose benchmark dataset for evaluating the benign utility of LLMs. It is recommended to include more standard benchmarks, such as MMLU and MT-Bench, to more comprehensively validate the argument for clean performance preservation in the proposed framework.\n\n3. The paper predominantly focuses on experimenting with open-source models (Llama 3 8B, DeepSeek-R1, and Qwen2.5-72B). It is unclear whether the proposed agentic framework can be applicable to the frontier closed-source LLMs, such as GPT-4, GPT-4o, or O1. Are there any bottlenecks for extending the proposed framework to closed-source LLMs? \n\n4. One contribution of the work is the adaptability of the proposed framework to distinct threats. However, it is not well-explained why the proposed agentic framework can be adapted to different scenarios. Is it due to the few refinement examples, the automated prompt optimization module, or the design of the agentic framework? These questions remain unanswered, which casts doubt on how generalizable the power of the proposed method can be across the distinct threats we are facing with LLMs nowadays. Can the defense be successfully generalized to other types of threats beyond the two considered categories (unlearning & jailbreaks), or even stronger adaptive attacks?\n\n5. Table 6 compares the computational efficiency of the proposed method. Under the benign scenarios, the time/cost overhead is still quite high (e.g., 3-4 times for Llama-3-8B). Can the efficiency be further optimized within the multi-agent framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0VLjxHsuEt", "forum": "SeQqnw5eNj", "replyto": "SeQqnw5eNj", "signatures": ["ICLR.cc/2026/Conference/Submission22988/Reviewer_Ak6R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22988/Reviewer_Ak6R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828592926, "cdate": 1761828592926, "tmdate": 1762942466182, "mdate": 1762942466182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AegisLLM, a cooperative multi-agent framework designed for adaptive, test-time defense in LLM security. The core idea is to shift from static, training-time interventions to a dynamic, system-level defense that scales security at inference time. AegisLLM employs a sequential, two-stage workflow using four specialized LLM-powered agents: an Orchestrator for pre-generation threat classification , a Responder to generate answers for benign queries , a Deflector to issue safe refusals , and an Evaluator for a final post-generation audit. A key novelty is the use of automated Bayesian prompt optimization , allowing the system to adapt to new threats with a very small number of examples, requiring no model retraining. The authors demonstrate the framework's effectiveness on two distinct threats, unlearning and jailbreaking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its significant and practical contribution to LLM safety, addressing the brittleness of static, training-time defenses. The originality of the work lies in its novel combination of a modular, multi-agent workflow with sample-efficient Bayesian prompt optimization (via DSPy) , reframing defense as an adaptive, inference-time problem. This approach is supported by high-quality experiments demonstrating the framework's generality against two distinct and critical threats: sensitive information disclosure (unlearning) and prompt injection (jailbreaking). The paper's most significant finding is its superior safety-utility trade-off; AegisLLM achieves strong defense while maintaining a low 7.9% false refusal rate on PHTest , making it a far more practical solution. Finally, thorough ablation studies validate the design, confirming that each agentic component is critical to the defense and that the optimization step provides a significant performance boost ."}, "weaknesses": {"value": "1. **Potentially Unfair Baseline Comparison**: The central comparison against baseline defenses in Table 4 and Table 5 appears to be on an uneven playing field. The AegisLLM system is explicitly optimized using a sample of 50 queries from both the StrongREJECT and PHTest datasets. However, it is not stated that the competing baselines (e.g., AutoDefense, Llama Guard, Self-Reminders) were given this same opportunity for sample-efficient optimization. This setup compares a tuned system (AegisLLM) to un-tuned, \"off-the-shelf\" systems. This may overstate AegisLLM's relative performance on these specific benchmarks. A fairer comparison would require either applying the same optimization process to the baselines or focusing the comparison on the unseen dataset (e.g. Wildjailbreak).\n\n2. **Lack of Fine-Grained Attack Analysis**: The jailbreak evaluation, while quantitatively broad, lacks a qualitative analysis of the types of attacks being mitigated. The paper reports aggregate success rates on datasets like StrongREJECT and Wildjailbreak  but does not provide a fine-grained breakdown of its performance against specific, known attack methods (e.g., role-playing, prefix injection, obfuscation, or refusal suppression)[1].\n\n[1] Wei, Alexander, Nika Haghtalab, and Jacob Steinhardt. \"Jailbroken: How does llm safety training fail?.\" Advances in Neural Information Processing Systems 36 (2023): 80079-80110."}, "questions": {"value": "1. Regarding the baseline comparisons in Table 4 and 5: The paper states that AegisLLM was optimized using 50 samples from StrongREJECT and 50 from PHTest. Were the competing baselines (e.g., AutoDefense, Llama Guard, Self-Reminders) also given this same 50-sample optimization opportunity? If not, Could you clarify this methodological choice? To provide a clearer comparison of generalization, the author may use the prompt optimized on StrongREJECT when test on PHTest, and use the prompt optimized on PHTest when test on StrongREJECT.\n\n2. Regarding the fine-grained attack analysis, refusal suppression and prefix injection should be use. The effect of these attack methods on the Orchestrator agent should be analyzed since it may lead to unexpected output from the Orchestrator agent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "imFO1Yjvgk", "forum": "SeQqnw5eNj", "replyto": "SeQqnw5eNj", "signatures": ["ICLR.cc/2026/Conference/Submission22988/Reviewer_GS8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22988/Reviewer_GS8E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988271660, "cdate": 1761988271660, "tmdate": 1762942465940, "mdate": 1762942465940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies defense mechanisms for LLMs. The authors propose AegisLLM, a cooperative multi-agent framework that provides adaptive, test-time defense for LLMs.  At the core of AegisLLM is a multi-agent system that can be optimized with a small number of examples. The authors investigate the effectiveness of AegisLLM on two critical and distinct threats: unlearning and jailbreaking."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves state-of-the-art performance on defense for LLMs.\n2. Various experiments are conducted to show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The method section is hard to follow, without any formalized description provided.\n2. The motivation of the proposed multi-agent framework is not clear. Why are these four agents needed in your framework?\n3. No qualitative cases are provided in the main content, significantly reducing the readability of this paper.\n4. I believe a large amount of the content is written by LLMs, which is awkward-sounding and hard to read."}, "questions": {"value": "Please reply to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U8bFEI5Bf3", "forum": "SeQqnw5eNj", "replyto": "SeQqnw5eNj", "signatures": ["ICLR.cc/2026/Conference/Submission22988/Reviewer_eV5b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22988/Reviewer_eV5b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157628360, "cdate": 1762157628360, "tmdate": 1762942465690, "mdate": 1762942465690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AegisLLM, a multi-agent framework that reframes LLM security as an adaptive, test-time problem rather than a static, training-time one. The system uses a modular, four-component architecture (Orchestrator, Responder, Evaluator, Deflector) to collaboratively manage and mitigate threats. The core novelty lies in using automated Bayesian prompt optimization to rapidly adapt this agentic system to new threats, like jailbreaking and sensitive data unlearning, using only a small number of examples, thus avoiding costly model retraining. The authors validate this approach on major security benchmarks, demonstrating that AegisLLM significantly improves jailbreak defense and achieves near-perfect unlearning while maintaining high utility and low false refusal rates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conducted a lot of experiments for validation.\n\n2. The experimental results of the proposed workflow demonstrate the superiority of the proposed workflow."}, "weaknesses": {"value": "1. The main weakness of this paper is in the novelty and contributions.  This paper adopts a multi-agent framework for defending LLM against jailbreak attacks. The main contribution is the workflow in Figure 1. The contribution to the method seems to be limited.\n\n2. In the method part, the authors adopted a BO-based adaptive defense strategy to optimize the prompt. However, in the paper, it seems that the authors merely employed the BO algorithm without any improvement of the algorithm itself for this specific problem. \n\n3. The details of adaptive defense via prompt optimization are not properly presented. On line 141 of page 3, the authors claim their methods in one sentence: \" we employ Bayesian optimization via DSPy to mitigate ...\". However, BO is a continuous optimization method; how to implement it in a discrete prompt optimization problem? Even though it is the built-in algorithm in DSPy approach, the authors should present the main procedure of BO-based prompt optimization for the sake of completeness.\n\n4. When optimizing the prompt, can it guarantee that the optimized prompts are human-readable? The optimization space for the prompt is the whole token space; it may generate some unreadable sentences.\n\n5. The authors presented a lot of experimental results; however, the introduction of the main contribution on the methods or the workflow in this paper is very limited, which makes the reviewer wonder what the main contribution of this paper is.\n\n6. What is the tunable hyperparametere $\\lambda_{FN}$ anda $\\lambda_{FP}$ on line 140 of page 3? What is the meaning of these two hyperparameters? Which loss function contains these two hyperparameters in this paper?\n\n7. Many agentic optimization works were released in 2025; the related works in this part should be updated."}, "questions": {"value": "See the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OTyo9SwkiD", "forum": "SeQqnw5eNj", "replyto": "SeQqnw5eNj", "signatures": ["ICLR.cc/2026/Conference/Submission22988/Reviewer_LeTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22988/Reviewer_LeTk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762345671569, "cdate": 1762345671569, "tmdate": 1762942463977, "mdate": 1762942463977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AegisLLM, a test-time, multi-agent defense. The system is pitched as a single framework that adapts to distinct threats (jailbreaking and unlearning) with few training examples and no model retraining. Experiments report near-random performance on WMDP unlearning while keeping MMLU/MT-Bench utility, and improved jailbreak robustness on StrongREJECT and WildJailbreak with lower false refusals than some baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The modular design is clear and easy to follow. \n- Framing defense setup as small-sample prompt tuning is pragmatic and lowers the barrier to adoption compared with retraining.\n- The results of the defense performance on unlearning (WMDP, TOFU) and jailbreak (StrongREJECT, PHTest, WildJailbreak) looks promising."}, "weaknesses": {"value": "- “Unified, generalizable” is argued from two text-only risks (jailbreaks and unlearning), with no tools, no multi-modal inputs, which weakens generalizability. \n- PHTest compliance and several safety outcomes are judged entirely by LLM-based evaluators, yet judge calibration and failure cases are not audited\n- Efficiency is discussed via point metrics, but not normalized by achieved compliance and refusal, so it is hard to compare to simpler guards at the same safety-utility operating point; parts of the tables also show AegisLLM slower or heavier than base on benign inputs, making the efficiency improvment less convincing."}, "questions": {"value": "- How trustworthy are the LLM judges on PHTest and other safety metrics in your setup; can you report calibration checks (gold references, adversarial probes)?\n- Can you recompute efficiency under different pricing models (equal in/out costs, streaming, cached prompts) and in longer, tool-heavy dialogues to demonstrate cost/time robustness beyond Table 6? \n- For WildJailbreak, can you add an adaptive attacker that reacts to Orchestrator/Evaluator messages to verify that “no extra optimization” still holds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ccuXU5NlD", "forum": "SeQqnw5eNj", "replyto": "SeQqnw5eNj", "signatures": ["ICLR.cc/2026/Conference/Submission22988/Reviewer_ikzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22988/Reviewer_ikzG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762446726122, "cdate": 1762446726122, "tmdate": 1762942463727, "mdate": 1762942463727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}