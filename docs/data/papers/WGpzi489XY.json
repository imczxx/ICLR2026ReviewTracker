{"id": "WGpzi489XY", "number": 14673, "cdate": 1758241395029, "mdate": 1759897355889, "content": {"title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression", "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight—even at greater computational cost—has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\\Theta(n^2d)$ and $\\Theta(nd^2)$ complexity. We then introduce {FlashLLA}, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models.", "tldr": "", "keywords": ["Attention Mechanism; Sequence Modeling; Test-Time Training; Local Linear Regression; Associative Memory; Hardware-Efficient Attention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35d223ed2fe625287ad62b348ba91044e8803a1b.pdf", "supplementary_material": "/attachment/5194a3988402b2c97bbfbd9f7e3e96f976c61db0.zip"}, "replies": [{"content": {"summary": {"value": "This paper builds upon the test-time regression interpretation of attention mechanisms proposed by Wang et al. (2025) and introduces a new architecture called Local Linear Attention (LLA), which aims to improve upon softmax attention. Specifically, the authors formulate LLA as the solution to local linear regression under the test-time regression framework, claiming that it achieves superior performance compared to softmax attention (corresponding to local constant regression) and MesaNet (corresponding to global linear regression). Furthermore, they propose a practical GPU-optimized algorithm to alleviate the high computational cost of LLA and make it efficient in practice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a theoretically motivated architectural design.\n- It proposes a practical, GPU-aware implementation of the algorithm."}, "weaknesses": {"value": "1. The validity of the proposed architecture heavily relies on theoretical assumptions specific to in-context learning and associative memory. Moreover, the theory depends on test-time regression framework, which provides only a partial understanding of one aspect of the attention mechanism. While theory-driven design is sometimes valuable, the current theoretical justification is not comprehensive enough to fully validate the effectiveness of the proposed architecture and superiority over softmax attention.\n2. The experimental validation, like the theory, is limited to settings where the test-time regression interpretation applies — namely, in-context learning and associative recall. To convincingly justify a new architecture, experiments in a practical scenarios is necessary.\n3. The paper does not include comparisons of computational cost (time and memory) with softmax attention, linear attention, or MesaNet. Beyond demonstrating efficiency improvements via FlashLLA, the paper should also show how competitive the proposed method becomes relative to existing approaches.\n4. The description “it suffers when predicting near the boundary of the data support, particularly with symmetric kernels like RBF” in line 194 is unclear. Since this point seems to be crucial to differentiating the proposed method from softmax attention, a more detailed explanation is needed.\n5. Many references are currently to arXiv preprints, even though several of them likely have published versions in conferences or journals. Please consider citing the published versions instead."}, "questions": {"value": "1. How does the local polynomial regression mentioned in line 197 relate to the proposed method?\n2. In line 236 (and in the title), what exactly does “optimal” refer to? In what sense is it optimal?\n3. Regarding line 470: the paper states that the conjugate gradient method is sufficient with a constant number of iterations. Is there any theoretical justification or empirical evidence supporting this claim quantitatively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ND1WYpPANA", "forum": "WGpzi489XY", "replyto": "WGpzi489XY", "signatures": ["ICLR.cc/2026/Conference/Submission14673/Reviewer_Uxja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14673/Reviewer_Uxja"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497841927, "cdate": 1761497841927, "tmdate": 1762925043860, "mdate": 1762925043860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Local Linear Attention (LLA) as an enhancement to softmax attention. Within the test-time regression framework, the local linear regression objective in LLA can be viewed as an optimal interpolation between linear and softmax attention, yielding improved associative recall capabilities in terms of convergence behavior and boundary bias.\n\nThe authors further optimize LLA’s implementation through targeted improvements to ensure linear memory scalability and propose FlashLLA, a blockwise parallel and hardware-efficient algorithm. Empirical results across four synthetic tasks, e.g., test-time adaptation and in-context learning, demonstrate the method’s efficacy compared to existing attention mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-organized and clearly written, providing readers with a solid understanding of the test-time regression framework and the motivation behind LLA. The presentation of the LLA algorithm is also systematic, including its regression, matrix-parallel, and blockwise form.\n\n2. The authors demonstrate a deep understanding of the problem, offering thorough theoretical analysis and proofs. The resulting local linear regression design is conceptually novel and well-justified.\n\n3. The synthetic experiments indicate that LLA exhibits a distinct superiority over other attention mechanisms in certain abilities, including in-context regression and associative recall."}, "weaknesses": {"value": "1. The experiments are conducted only on synthetic tasks. While I understand that LLA introduces additional complexity in both computation and memory I/O at the kernel level, it remains uncertain how well results from such small-scale, controlled tasks can transfer to real-world LM capabilities.\n\n2. The paper provides limited analysis and interpretation of the experimental results (except for the state-tracking task). For instance, it is confusing why MesaNet outperforms LLA in the first segment of the test-time regression task."}, "questions": {"value": "1. In the regression formulation of LLA, why is the local function instantiated as $W(x-q)+b$? Is this particular form necessary to derive a “well-behaved” solution? I wonder the constraints and considerations behind this choice, and whether alternative linear functions exist that could also work.\n\n2. Please correct me if my understanding is inaccurate: within the proposed regression paradigm, locality is reflected in both the loss function  $f(x)$ and the weighting factor $w_{ij}$. For the former, softmax attention employs a constant value function, whereas LLA uses a parameterized local linear function. Could you clarify the distinct roles of these two forms of locality, and are both components essential to the design of LLA?\n\n3. In the test-time regression experiment (Fig. 2), does MesaNet use decay? It appears that the Attn model fails to effectively utilize in-distribution information—does this imply that parameterization and locality in the loss function are necessary? Moreover, why does MesaNet outperform LLA in the first segment but deteriorate significantly in later segments, and could data-dependent decay influence this observation?\n\n4. In the MQAR experiment (Fig. 4b), why does the in-context recall of Attention perform substantially worse than GDN or Mamba in the short-sequence (<256) setting? Intuitively, for a token-wise matching task of this kind, Attention should be able to learn such dependencies quite easily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mYUVMXz2bX", "forum": "WGpzi489XY", "replyto": "WGpzi489XY", "signatures": ["ICLR.cc/2026/Conference/Submission14673/Reviewer_tiaX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14673/Reviewer_tiaX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766079769, "cdate": 1761766079769, "tmdate": 1762925043433, "mdate": 1762925043433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and insightful theoretical framework for understanding attention mechanisms through the lens of non-parametric statistics. The authors convincingly argue that standard Softmax Attention is analogous to a local constant (Nadaraya-Watson) estimator, which is known to suffer from theoretical limitations such as bias at data boundaries.\n\nAs a principled alternative, the paper proposes Local Linear Attention (LLA), a new attention mechanism derived from local linear regression. This model is theoretically superior, offering faster asymptotic convergence rates and mitigating the boundary bias issues inherent in the local constant approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's greatest strength is the novel connection it establishes between attention and non-parametric regression. Framing Softmax Attention as a local constant estimator is an insightful conceptual leap that provides a new lens for the community to understand and analyze attention.\n-  LLA is not an ad-hoc modification of attention. It is a new mechanism derived directly from a statistically superior estimator (local linear regression). This principled design, which provably addresses issues like boundary bias (as shown in the appendix), is a significant strength.\n- The authors clearly recognized the computational hurdles of LLA and put significant effort into addressing them."}, "weaknesses": {"value": "- The primary experimental evidence comes from a synthetic piecewise-linear regression task. While this task perfectly isolates the theoretical benefits of LLA, it feels somewhat circular: it proves the local linear model is good at solving a local linear problem. It is not immediately obvious if this specific capability (handling sharp, non-stationary linear segments) is a major bottleneck in more common, large-scale tasks like natural language processing. The paper would be much stronger if it could demonstrate this benefit on a more complex, real-world benchmark where this specific type of non-stationarity is known to be an issue.\n- The \"FlashLLA\" name and its benchmarking could be misleading. The name invites a direct comparison to FlashAttention, which is a one-pass I/O-aware algorithm. However, FlashLLA relies on a Conjugate Gradient (CG) solver, which is iterative. As detailed in Algorithm 2, each iteration of the CG solver requires streaming through the Key matrix, resulting in a much higher I/O cost (multiplied by the number of iterations, $T$). The paper's main hardware benchmark (Figure 1) only plots memory usage and omits wall-clock runtime or throughput comparisons against a standard FlashAttention baseline. This makes it impossible to assess the true computational overhead. The authors admit in the limitations that the constant factor is \"higher,\" but this seems to understate the potential I/O bottleneck from the iterative solver."}, "questions": {"value": "Please refer to my weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OIpnoD0ZEq", "forum": "WGpzi489XY", "replyto": "WGpzi489XY", "signatures": ["ICLR.cc/2026/Conference/Submission14673/Reviewer_HAFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14673/Reviewer_HAFq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847667141, "cdate": 1761847667141, "tmdate": 1762925043103, "mdate": 1762925043103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the characteristics and performance of softmax and linear attention under the Test-Time Training (TTT) setting from the perspective of statistical regression. From this viewpoint, the authors propose a novel mechanism named Local Linear Attention (LLA), which they claim demonstrates superior advantages. Furthermore, the paper introduces an I/O-aware operator for LLA, designed to achieve more efficient memory utilization. The effectiveness of the proposed model is validated through toy experiments, which showcase its capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a solid theoretical foundation for its proposed method. The authors conduct a detailed analysis of the properties of linear and softmax attention from a statistical regression perspective. This approach is well-motivated and naturally leads to the formulation of their Local Linear Attention (LLA) framework."}, "weaknesses": {"value": "A major weakness of this work is the limited scope of its experimental validation. The authors didn’t demonstrate the viability of Local Linear Attention (LLA) for pre-training, a critical aspect for assessing new attention mechanisms. Moreover, the paper completely has no evaluations on any practical downstream tasks, confining its results to toy experiments. Consequently, the practical relevance and usefulness of this research are questionable, as it is unclear whether the proposed method offers any real-world benefits."}, "questions": {"value": "1.\tThe paper's second claimed contribution, which asserts to have resolved the quadratic memory complexity, is fundamentally flawed. This is not a novel contribution. The issue of quadratic memory cost in standard self-attention was already solved by seminal works 3 years ago, such as FlashAttention. Furthermore, the authors' comparison is irrelevant for linear attention, which inherently possesses O(Nd) memory complexity due to its operator properties and never had a quadratic memory bottleneck to begin with. Claiming this as a contribution demonstrates a misunderstanding or misrepresentation of the current state-of-the-art in attention mechanisms. In other words, this \"contribution\" merely solves a problem that the proposed LLA method creates for itself.\n2.\tThe paper's second claimed contribution is achieving strong results on artificial \"toy\" experiments. However, the complete absence of validation in a pre-training setting is a critical omission. In the field of attention architecture research, evaluating a new mechanism's performance on large-scale pre-training has been a standard practice for years; failing to do so is highly unusual, even by the standards of a year ago. This limitation severely undermines the claim of \"good results,\" as it is unclear if the performance gains can generalize beyond these contrived scenarios. It gives the impression that the proposed model is only capable of solving problems that are, in a sense, artificially constructed for it.\n3.\tThe computational complexity of LLA raises serious questions about the paper's contribution. The authors state that LLA's complexity surpasses that of softmax attention. This is deeply ironic. For nearly a decade (since circa 2017), the community has dedicated immense effort to making attention mechanisms, particularly linear attention, more efficient. This paper, however, manages to achieve the opposite. In a surprising turn of events, this work presents a \"linear attention\" variant that is less efficient than the very baseline (softmax attention) it sought to outperform. One could sardonically remark that after all these years, this paper finally allows linear attention to generate a larger carbon footprint than softmax attention.\n4.\tThe paper would benefit from a more comprehensive complexity analysis. The proposed operator is unique, and a detailed breakdown of its computational and memory complexity would significantly improve clarity for the reader. I would encourage the authors to include a more thorough analysis to help the audience better understand the operator's behavior and trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sexzl0TtPr", "forum": "WGpzi489XY", "replyto": "WGpzi489XY", "signatures": ["ICLR.cc/2026/Conference/Submission14673/Reviewer_Hn3s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14673/Reviewer_Hn3s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967651371, "cdate": 1761967651371, "tmdate": 1762925042782, "mdate": 1762925042782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}