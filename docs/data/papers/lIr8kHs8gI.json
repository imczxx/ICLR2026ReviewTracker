{"id": "lIr8kHs8gI", "number": 13768, "cdate": 1758222274732, "mdate": 1759897414182, "content": {"title": "Toward Resilient Watermark Detection: Stability-Aware Statistical Features for Machine-Generated Text", "abstract": "The widespread adoption of large language models (LLMs) has intensified the demand for principled methods to distinguish human- from machine-generated text. Watermarking provides a promising avenue, yet existing detectors exhibit sharp performance deterioration under multiple paraphrasing and when applied to shorter texts. We introduce Pattern Stability Score (PSS), a novel detection framework that leverages local statistical features and stability dynamics across paraphrased variants. Specifically, the proposed method combines global and local z-score features with higher-order statistics of run-length patterns, enriched by autocorrelation signals and stability scores computed over paraphrase depth. Numerical evaluations are performed on PG-19, a large-scale long-form benchmark while systematically stress-testing robustness under up to eight rounds of paraphrasing with Mistral-7B. Compared to prior z-score thresholding baselines, our approach improves detection AUC (area under the receiver operating characteristic curve) by over 10–15 percentage points across different token lengths. Additionally, it achieves strong precision–recall balance and AUC greater than 0.95 at full length, demonstrating resilience where prior detectors collapse. Finally, sensitivity analysis is conducted on window size, stride, and token length to validate design choices. Overall, these empirical results establish PSS as a practical and extensible framework for watermark detection, highlighting stability-based features as a promising direction for safeguarding LLM outputs against potential adversarial paraphrasing.", "tldr": "", "keywords": ["Watermarking", "paraphrasing", "robustness"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a979098cb6a75aae7ee0b20424f589d8de8d6f4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a simple modification to LLM watermark detection algorithms to make them more robust to paraphrasing attacks. Instead of just relying on a global z-score statistic, the paper proposes adding local z-score statistics as well as a \"paraphrase stability statistic\", which checks the variance of local z-scores against repeated paraphrasing. These statistical features are then fed to a simple classifier which determines if text was AI-generated.\n\nExperiments show that the proposed method is more robust to paraphrasing attacks especially when paraphrasing is sequentially done 5-8 times."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and timely problem: developing robust LLM watermarking. As LLMs get increasingly used for content generation, watermarking is an important tool to prevent misuse. Since paraphrasing as emerged as a strong attack vector against watermarks, it's important to develop more robust detection algorithms.\n\n2. The proposed method is a simple modification to watermarking detection stage only, and requires no changes in the watermarking algorithm, pre-generated text, or any LLM retraining.\n\n3. The proposed method seems to be a lot more resilient to paraphrasing attacks, especially when the paraphrasing is done multiple times."}, "weaknesses": {"value": "1. **Major concern about classifier generalization**: Based on L346 and the author's acknowledged limitation in L471, a unique classifier is trained for every LLM generator + paraphraser model + paraphraser depth. No experiments have been conducted to check if a classifier on one {LLM, domain, paraphraser, depth} tuple generalizes well to a different {LLM, domain, paraphraser, depth} tuple. This feels like a major limitation to me, since in practice a detector may not know which paraphraser / LLM an attacker used. Furthermore, PSS features require paraphrasing in the detection loop (L311-314), which makes this even more concerning if there is a paraphrase mismatch between attacker / detector. I think a thorough study of generalization is essential to validate the practical utility of this method.\n\n2. **Very limited experimental setup**: As acknowledged by the authors, experiments in this work are only conducted on a single LLM and single PG19 text domain in english, and on a single watermark setting. This makes it hard to judge whether the feature vectors used in the work generalize to other settings (where classifiers can be retrained).\n\n3. **Use-case seems a bit narrow, 8x iterative paraphrasing may not be practical**: The paper doesn't discuss much about the semantic preservation across the 5-8 stages of paraphrasing. The LLM paraphraser used is not particularly SoTA, and I expect there to be significant semantic drift after so many iterations. This is not discussed in the paper (nor are there explains of 8x paraphrasing). I feel the PSS features may be picking up on paraphraser-specific changes (although L417 is some evidence against it). In reality, I expect users to paraphrase 1-2 times at most, and then perform other changes like manual edits, text mixing (https://arxiv.org/abs/2306.04634), chaining LLMs etc. The paper would be stronger if the attack setup is a more realistic than 8x iterative paraphrasing.\n\nMinor: Along with AUC, would be nice to report something like true positive rate at a fixed low false positive rate (1-5%) since in practice we want to select thresholds that minimize false accusations of human authorship (https://arxiv.org/abs/2303.13408).\n\n*Style*: Please add a big figure to page 2 of the Introduction section, and split L60-90 into multiple paragraphs / bullet points of benefits so that it is easier to read and grasp quickly at a glance."}, "questions": {"value": "In Section 4.1, which watermarking scheme and LLM generator was used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v5oPjhdJw4", "forum": "lIr8kHs8gI", "replyto": "lIr8kHs8gI", "signatures": ["ICLR.cc/2026/Conference/Submission13768/Reviewer_Mc9z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13768/Reviewer_Mc9z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700010150, "cdate": 1761700010150, "tmdate": 1762924300824, "mdate": 1762924300824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an improved watermark detection method that enhances robustness against paraphrasing. The core idea is to compute the standard deviation of local z-scores across multiple paraphrased versions of a text—capturing how stable the watermark signal remains under rewrites. This stability-based metric, PSS, achieves significantly higher AUC compared to traditional global or local statistical detectors."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The results demonstrate that PSS consistently outperforms baseline methods in terms of AUC."}, "weaknesses": {"value": "- My main concern is that the paper only reports AUC, which is not always a reliable metric in practical settings. I suggest that the authors also present results in terms of the True Positive Rate (TPR) at low False Positive Rate (FPR) thresholds to better demonstrate the reliability and practical utility of their method.\n- The performance appears to drop noticeably when a different paraphraser, such as Gemma-7B-IT, is used.\n- Another major concern is that several citations appear to be AI-generated or incorrectly formatted. Notably, even the citation for the original watermarking paper is inaccurate. For example:\n\nJohannes Kirchenbauer, Jonas Geiping, Nicolas Papernot, Ian Miers, Florian Tram`er, Micah Goldblum, and Nicholas Carlini. A watermark for large language models. arXiv preprint, 2023. URLhttps://arxiv.org/abs/2301.10226.\n\nZhiguo Wang, Wael Hamza, and Radu Florian. Learning stylometric representations for authorship\nattribution. In Proceedings of ACL, pp. 2643–2654, 2018. URL https://aclanthology.\norg/P18-1246/.\n\nYunfan Gao, Tianyi Tang, Kai Zhang, et al. Biscope: Scaling up human-ai text attribution via dualspectrum features. In Advances in Neural Information Processing Systems (NeurIPS), 2024. URLhttps://openreview.net/forum?id=bisc0pe."}, "questions": {"value": "- How computationally expensive is the proposed method, considering that it requires generating multiple paraphrases per text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ruhNAgpe8a", "forum": "lIr8kHs8gI", "replyto": "lIr8kHs8gI", "signatures": ["ICLR.cc/2026/Conference/Submission13768/Reviewer_ncLv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13768/Reviewer_ncLv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926778655, "cdate": 1761926778655, "tmdate": 1762924300240, "mdate": 1762924300240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight, gradient-free detector that improves the resilience of watermark detection against paraphrasing, translation, and sampling noise. Instead of relying on token-level z-tests or entropy thresholds, the method reformulates detection as a sequence-level inference task over the aggregate pattern of green/red token occurrences, enhanced with a normalized likelihood ratio that adjusts for distributional drift in generated text. The authors also introduce a calibration routine that aligns detection scores across varying decoding temperatures and LLM architectures. Experiments on GPT-2/3/NeoX and Tulu models, as well as human paraphrases and multiple watermarking schemes (including Kirchenbauer et al., EWD, and SWEET), demonstrate consistent gains in detection F1 with negligible computational overhead."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper clearly defines the problem, robust detection rather than watermark insertion. The contribution is distinct from adaptive watermark generation papers; it improves the detector side without requiring model retraining or secret-key changes.\n* Treating detection as a normalized likelihood inference rather than a token-count test is an elegant and computationally cheap way to mitigate paraphrasing effects.\n* Results cover diverse LLM families and perturbation types (back-translation, paraphrasing, temperature sampling), demonstrating strong empirical support and substantial improvements in detection accuracy relative to prior methods."}, "weaknesses": {"value": "* The detector involves a learned calibration component trained on a single domain, and it is unclear whether domain or model drift (e.g., new sampling temperature, architecture, or topic distribution) would require retraining. The authors mention this as a limitation, but it could significantly constrain practical deployment. A small test on a different domain dataset or model class would help clarify this. It would also help to specify how often calibration must be recomputed when decoding parameters change and whether this introduces hidden computational cost.\n* A brief discussion situating this detector relative to recent entropy-aware detection schemes (e.g., EWD) would make the novelty even clearer."}, "questions": {"value": "* How sensitive is the trained detector to domain or model shifts? For instance, would a model trained on conversational English require retraining for code generation or multilingual tasks? Could the authors provide any empirical evidence (e.g., cross-domain tests) to illustrate the degree of degradation without retraining?\n* Would combining this detection strategy with entropy-weighted or adaptive watermark generators provide further gains, or is its benefit primarily orthogonal? Similarly, is the detector relatively robust to the use of a simple red/green list watermark vs. something that only applies watermarking to high entropy parts of the text? I would assume it is, but a small test on another watermark would provide more confidence.\n* Figure 1’s font sizes should be enlarged for better readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rNXnlCB1tG", "forum": "lIr8kHs8gI", "replyto": "lIr8kHs8gI", "signatures": ["ICLR.cc/2026/Conference/Submission13768/Reviewer_9kVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13768/Reviewer_9kVJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932649879, "cdate": 1761932649879, "tmdate": 1762924299854, "mdate": 1762924299854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Watermarking is a way of distinguishing human-generated from machine-generated texts. However, it performs less well under re-paraphrasing and short context. The paper introduced a method called pattern stability scores to address the problem, leveraging the local statistical features and global stability dynamics across paraphrases. The experiments across different benchmark datasets show that the method can address the problem to some extent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The problem setting is well-defined;\n\n2, The method is concise and intuitive, and it seems to be effective from some experiment results."}, "weaknesses": {"value": "The whole method seems to be a collection of simple statistics and human-designed rules. While it seems to perform well in some scenarios, its generalization ability remains a question. From another aspect, the method seems to be too simple to be published as a full paper. I believe the authors could conduct much more systematic experiments, not only as another contribution but also to make their methods more convincing to reviewers. For example, is it possible that those human-concluded rules will expire soon in the near future, at least the paper is open and public? It would also be very useful if the authors could show some rigorous analysis of why those simple statistics can be ensured to be useful."}, "questions": {"value": "1, The paper is not well structured to me. Why not make the introduction a bit shorter? The characters in Fig. 1 are also too small to be clear. \n\n2, The method seems to be very sensitive to hyperparameters. How do you select optimal hyperparameters in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LJVK76Svtn", "forum": "lIr8kHs8gI", "replyto": "lIr8kHs8gI", "signatures": ["ICLR.cc/2026/Conference/Submission13768/Reviewer_3KQZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13768/Reviewer_3KQZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971452591, "cdate": 1761971452591, "tmdate": 1762924299380, "mdate": 1762924299380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}