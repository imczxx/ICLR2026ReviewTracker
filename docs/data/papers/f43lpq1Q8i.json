{"id": "f43lpq1Q8i", "number": 24912, "cdate": 1758361856516, "mdate": 1759896742622, "content": {"title": "When Validity Isn't Enough: Reliability Gaps in Molecular Generation and KRAS Case Study", "abstract": "Molecule generation remains a core challenge in computational chemistry. Practical use of generative models is complicated by strict chemical, structural, and biological constraints: candidate compounds must satisfy physicochemical bounds, avoid reactive or toxic substructures, be synthesizable, and plausibly bind a target. We are the first to perform such comprehensive analysis of modern molecule generators via the Five-Stage Filtering Pipeline, a target-agnostic, practice-oriented benchmark for evaluating de novo generators using the following stages:  (i) physicochemical descriptors; (ii) structural alerts; (iii) synthesis feasibility; (iv) docking and binding affinity estimation; and (v) blind medicinal chemist review. We compare 18 generators across three families (unconditional, ligand-based, and protein-based), and to make it practically relevant, apply the pipeline to KRAS G12D switch-II pocket for conditional design case study. Less than 1% of molecules pass all stages, exposing a gap between high scores on standard generative metrics and practical medicinal chemistry usage. We release our benchmark, and code to enable reproducible evaluation and to focus future model development on practically useful chemical space.", "tldr": "", "keywords": ["Molecule Generation", "Generative Models", "KRAS", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98e09c2d0c1aaf8716b2aa0002ed353623b602ff.pdf", "supplementary_material": "/attachment/836d7f40afd35369c91e74860a15bbb416e13469.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical disconnect in molecular generation research: modern generative models often achieve high scores on standard metrics (e.g., validity, diversity) but fail to produce molecules viable for real-world drug discovery (e.g., synthesizable, target-binding). To bridge this gap, the authors propose the Five-Stage Filtering Pipeline—a target-agnostic, practice-oriented benchmark—and apply it to 18 molecular generators across three families (unconditional, ligand-based, protein-based), with a focus on KRAS G12D (a mutant lacking approved inhibitors) as a case study. Experiments generated 210,000 molecules and found that less than 1% passed all stages"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive model coverage\nThe study includes 18 generators across architectures (VAEs, autoregressive models, diffusion models, genetic algorithms) and families, enabling fair cross-family comparisons. For example, it tests variants like REINVENT4 (V/P/TL) to isolate prior choice effects, and Dragonfly (biased/unbiased) to assess descriptor conditioning.\n2. Physics-informed validation\nThe KRAS G12D case study is biologically meaningful (no approved inhibitors) and uses a well-characterized pocket (PDB 7EW9). Medicinal chemist review includes target-specific checks (e.g., Asp12 interactions for KRAS selectivity), grounding results in real drug design priorities."}, "weaknesses": {"value": "1.  Scalability and System Size Limitations\nThe pipeline and experiments focus on small-to-medium molecular systems but ignore large biomolecular systems (e.g., proteins with millions of atoms, multi-component complexes). This limits relevance for drug discovery scenarios like protein–protein interaction inhibitors or bulk material-based therapeutics. Additionally, the paper does not evaluate how the pipeline scales with increasing molecular size.\n2. Limited target diversity\nThe case study focuses exclusively on KRAS G12D. While this is a high-priority target, the pipeline’s \"target-agnostic\" claim is not fully validated without testing on other targets (e.g., EGFR, CDK2) with different pocket geometries or ligand binding modes. This raises questions about whether the observed pass rate trends (e.g., unconditional model superiority) generalize to other biological targets.\n3. Limited insight into failure mechanisms\nWhile the paper identifies high attrition at synthesis feasibility and medicinal review, it does not deeply analyze why models fail at these stages."}, "questions": {"value": "The authors should seriously address the concerns shown in Weakness point by point to improve the quality of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K0Hf1GpcS8", "forum": "f43lpq1Q8i", "replyto": "f43lpq1Q8i", "signatures": ["ICLR.cc/2026/Conference/Submission24912/Reviewer_acdA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24912/Reviewer_acdA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662077213, "cdate": 1761662077213, "tmdate": 1762943242106, "mdate": 1762943242106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces the Five-Stage Filtering Pipeline, which streamlines evaluating molecular designs. The proposed setup comprises filters on molecular descriptors, structure, synthetic accessibility, docking score, and medicinal chemist feedback. Unconditional, ligand-based, and structure-based generators with different molecule representations are evaluated within the framework for KRAS binding. The main finding of the study is that unconditional generators can more often pass the quality filters than conditional ones, and ligand-based generators tend to outperform structure-based models.\n\nWhile the study contains some interesting outcomes, I am recommending rejecting the current version for the following reasons:\n1. The proposed evaluation framework is a combination of popularly used medicinal chemistry filters, not offering a new aspect to molecular design evaluation.\n2. Only one design task is studied.\n3. The reasons behind the findings are unclear."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "1. A broad set of models is tested, ranging from unconditional to structure-based generators, and from string-based to 3D generators.\n2. A multi-faceted evaluation is performed, and a broad range of molecular quality is considered.\n3. The paper is well-written and easy to follow.\n4. The proposed filters are coming from practical medicinal chemistry considerations, with a strong know-how."}, "weaknesses": {"value": "1. The proposed framework unifies known medicinal chemistry filters in a standard way. Even the popularly adopted library REINVENT implements and adopts some of these filters and contains scoring function implementations. While making these filters a core part of a benchmark is an interesting idea, the work studies only one design task and falls short of being comprehensive as a benchmark. Moreover, the transferability of the model comparisons to other targets remains only speculative.\n2. The work misses the opportunity to provide actionable insights and guidelines for future molecular generation model development. The reasons behind the model performances are not investigated. Otherwise, the work remains purely critical.\n3. The impact of the (pre)training data is never considered during evaluation, which can have a substantial impact on outcomes for unconditional models. It is difficult to understand *why* some models outperform others without considering the training data. For instance, do unconditional string-based models outperform 3D structure-based models because of the model architecture/representation, or because of the larger training data they adopt? I recommend training (some) models of each category with the same training data and repeating the analysis.\n4. The benchmark misses the diversity aspect of evaluation, which is key to molecule design. This should be included in future versions of the benchmark.\n5. While medicinal chemist feedback is important, it is inaccessible to future researchers who want to benchmark their models. Can this part be replaced with a model, e.g., a classifier trained on medicinal chemist feedback data (e.g., www.nature.com/articles/s41467-023-42242-1)? Also, having multiple medicinal chemists in this study could increase the reliability of the current assessments, since chemists can also disagree (www.nature.com/articles/s41467-023-42242-1).\n\nA friendly suggestion (not a weakness): Presenting the tool as a web server/API where people can upload their generated molecules and download the scores would significantly increase the usability and impact of the work. Setting up all the tools used here can be cumbersome for many users."}, "questions": {"value": "1. What is the added value of the work, beyond unifying known medicinal chemistry filters into a single evaluation template?\n2. How generalizable are the findings to other protein targets?\n3. How do authors explain unconditional models - which have no access to binding data - outperforming conditional models, in a complex task such as bioactive molecule design?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IMb8cUYUwi", "forum": "f43lpq1Q8i", "replyto": "f43lpq1Q8i", "signatures": ["ICLR.cc/2026/Conference/Submission24912/Reviewer_c6eY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24912/Reviewer_c6eY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908276282, "cdate": 1761908276282, "tmdate": 1762943241848, "mdate": 1762943241848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a 5 stage pipeline for evaluating molecular generators."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is relatively clear and easy to understand."}, "weaknesses": {"value": "The main weakness of this paper is that the contribution and innovation seem weak. The checks that are performed are not novel in themselves, and not surprising. It’s not clear that the ordering is important. If the point is that few molecules can go on to be drugs, I think that is already accepted in the community."}, "questions": {"value": "One of the steps is to remove salts and solvents. Why are generators generating those, especially the conditional ones? Is it that they are generating lots of junk?\n\nWhy is “keeping the largest fragment” a step? Are these generators generating multiple fragments in a single generation step?\n\nWhat exactly were the unconditional generators doing? By unconditional here, I assume that they were not aware of the target? If so, wouldn’t their molecules basically be random? If so, was the binding affinity just due to randomness?\n\nWhat about off-target binding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "shMSfWGA7H", "forum": "f43lpq1Q8i", "replyto": "f43lpq1Q8i", "signatures": ["ICLR.cc/2026/Conference/Submission24912/Reviewer_zYP8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24912/Reviewer_zYP8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964222548, "cdate": 1761964222548, "tmdate": 1762943241558, "mdate": 1762943241558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an intriguing study on evaluating generative models for molecular design. The proposed benchmark includes five stages: descriptors-based filtering, structural filters, synthesizability, molecular docking, and medicinal chemist evaluation. Each stage incorporates multiple filters and criteria to ensure only the most promising compounds advance to the next stage. Descriptor-based filters are based on various rule sets, such as the Lipinski Rule of Five. Structural filters rely on sets of structural alerts. Synthesizability is evaluated through multiple independent methods, including the prediction of synthesis routes. Docking is conducted using AutoDock Vina. The final stage involves manual inspection of binding poses by experienced senior medicinal chemists. The benchmark is prepopulated with different classes of generative models. The results lead to interesting conclusions, notably the superior performance of unconditional generators in passing all filters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed benchmark provides a comprehensive set of evaluation criteria, offering a multi-perspective view of the generated compounds.\n- Different classes of generative models are benchmarked, and the differences between their performance are discussed.\n- This paper offers valuable insights for practitioners who want to apply generative models in their drug discovery campaigns.\n- All generated molecules are preprocessed in the same way to produce comparable results.\n- The paper is written in a very clear manner, with all filters and property ranges provided in the appendix."}, "weaknesses": {"value": "- In the first paragraph of the introduction, early drug discovery might be somewhat misrepresented. Specifically, the binding pocket is sometimes unknown and not always necessary for discovering a new active molecule. In fact, sometimes the binding pocket is discovered after identifying a binding compound, as a result of the co-crystallization of the ligand with the protein. Furthermore, crystallography and pocket prediction are not the only methods used to identify a pocket; it can also be, for example, a Cryo-EM structure or techniques like HDX-MS.\n- The text says that thresholds for rule sets were extended “to remove clearly outliers.” What criteria were used to determine how much these thresholds should be extended? Was it based on some outlier detection method?\n- The benchmark is limited to only one biological target, so it is unclear if the conclusions would be the same for other targets.\n- The value of this benchmark for the machine learning community is unclear. Undoubtedly, this strict evaluation protocol can have a significant impact on drug discovery projects; however, its applicability to evaluating new generative methods is limited due to the manual nature of the final evaluation stage. Perhaps this paper would be a valuable review of the current state of the field of generative molecular design as a journal paper if it included more models. As a benchmark paper, the last manual stage renders this benchmark practically unusable. Moreover, some criteria may be too stringent as current approved drugs are more and more often beyond the Rule of Five, and synthetic routes predicted by a model can be incorrect or missing available starting materials.\n- Adding more details on the expert evaluation would make this pipeline more reproducible, especially since the paper is still one page below the page limit. The criteria in Section 3.5 seem to be fairly easy to automate. How are these criteria combined with the chemists’ scores? Are they just a summary of what chemists assessed, or are they computed and given to the chemists for evaluation?\n- The code repository linked in the appendix does not work (the content cannot be found). For this benchmark's success, it will be crucial to provide a high-quality code repository that automates the evaluation process. Providing an easy-to-use evaluation code and all necessary data would convince me to increase my score.\n- The emphasis on this benchmark being the first of its kind seems somewhat misleading. These filters, used in various combinations, have also been employed in other benchmarks and practical screening pipelines. Even if this particular combination and exhaustiveness are unique, each step is a well-known and commonly used procedure."}, "questions": {"value": "1. Did you test the variability of chemists’ scores? Do you think this manual evaluation can be reproduced by another chemist? What is the minimum level of expertise required to accurately score the generated compounds?\n2. Do you think that the lack of any approved inhibitors for this target impacts the quality of the models, especially Boltz-2? Do you think that the model may be biased towards other mutants for which approved drugs exist?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n4RAeaRoCp", "forum": "f43lpq1Q8i", "replyto": "f43lpq1Q8i", "signatures": ["ICLR.cc/2026/Conference/Submission24912/Reviewer_oUBS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24912/Reviewer_oUBS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24912/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997197035, "cdate": 1761997197035, "tmdate": 1762943241339, "mdate": 1762943241339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}