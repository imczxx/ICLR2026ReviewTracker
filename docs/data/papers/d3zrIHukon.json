{"id": "d3zrIHukon", "number": 10377, "cdate": 1758168910649, "mdate": 1759897655025, "content": {"title": "SALT: Structure-Aligned Learning for Time-Series Forecasting", "abstract": "Time-series forecasting is vital in domains such as economics, energy, and traffic.  Although many models exploit the decomposable nature of time-series, they are typically trained with a single objective (e.g., MSE), which imposes structural limits on their performance. We empirically demonstrate that this paradigm gives rise to two characteristic challenges: gradient interference, where heterogeneous components conflict, and spectral bias, where dominant low-frequency structures overshadow informative high-frequency ones. To move beyond these limitations, we introduce SALT (\\textbf{S}tructure-\\textbf{A}ligned \\textbf{L}earning for \\textbf{T}ime-Series), which combines Iterative Dominant Extraction (IDE) with Separable Training to optimize components independently. Our theoretical and empirical analyses show that this regime reduces cross-term errors, balances convergence across frequencies, and consistently surpasses the conventional methods across six backbones and nine benchmarks.", "tldr": "", "keywords": ["Time series forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/819a6bfacd616b4ef9e9670732bcd7836f579cfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SALT (Structure-Aligned Learning for Time-Series Forecasting), a training paradigm that addresses gradient interference and spectral bias in time-series models. By combining Iterative Dominant Extraction (IDE) and Separable Training, SALT trains decomposed components independently and then aggregates them."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear identification of two fundamental optimization challenges: gradient interference and spectral bias.\n\n- Method is simple, general, and model-agnostic, applicable to diverse forecasting backbones.\n\n- Provides solid theoretical justification that strengthens the contribution.\n\n- Experimental evaluation is comprehensive: multiple datasets, architectures, and ablation studies demonstrate consistent gains.\n\n- Improves both accuracy and interpretability by aligning optimization with structural decomposition."}, "weaknesses": {"value": "- Could you analyze the performance of your method in the frequency domain as well? The current experiment are all conducted on the model that only includes the time domain, but many recent models transform data into the frequency domain (e.g., via Fourier transforms) for learning. It would be valuable to investigate whether similar issues also arise in the frequency space and how your approach might address them.\n\n- In Table 24, why does your method incur such a large memory increase on the iTransformer model, while the overhead remains small for other backbones? Please clarify the cause of this discrepancy.\n\n- The authors may consider discussing this work[1], which propose a hybrid loss framework combining the global and component losses, dynamically adjusting weights between these overall loss; GCformer[2], which identifies a gap or conflict between features extracted in the time domain and frequency domain, and then train them together to bridge this gap. It would be helpful to include a comparison in the related work section to explain the distinctions between these two methods and the current approach.\n\n[1]Han, R., Feng, D., Du, H., Wang, H. (2025). A Hybrid Loss Framework for Decomposition-Based Time Series Forecasting Methods: Balancing Global and Component Errors. In: Wu, X., et al. Advances in Knowledge Discovery and Data Mining. PAKDD 2025. Lecture Notes in Computer Science(), vol 15873. Springer, Singapore. https://doi.org/10.1007/978-981-96-8183-9_15\n\n[2] Yanjun Zhao, Ziqing` Ma, Tian Zhou, Mengni Ye, Liang Sun, and Yi Qian. 2023. GCformer: An Efficient Solution for Accurate and Scalable Long-Term Multivariate Time Series Forecasting. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23). Association for Computing Machinery, New York, NY, USA, 3464–3473. https://doi.org/10.1145/3583780.3615136"}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cbOANwGxml", "forum": "d3zrIHukon", "replyto": "d3zrIHukon", "signatures": ["ICLR.cc/2026/Conference/Submission10377/Reviewer_1DLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10377/Reviewer_1DLx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760567631393, "cdate": 1760567631393, "tmdate": 1762921697838, "mdate": 1762921697838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SALT aims to addresse gradient interference and spectral bias in time-series forecasting by decomposing data into interpretable components with dedicated predictors. Theoretical results using structure-wise decoupling and NTK spectral analysis are provided to support this approach. Experiments on multiple datasets and models show improved accuracy and training stability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and illustrated with explanatory figures."}, "weaknesses": {"value": "1. The proof of Theorem 1 (Error Reduction via Structure-wise Decoupling) comes across as potentially misleading. In particular, the second point of the “the following two reasonable assumptions” (L674–L680) lacks practical justification. It is unclear how one can reasonably assume that separable training will consistently achieve lower empirical training loss than a single model, which seems unfounded. This argument essentially assumes that any multi-model prediction paradigm will “reasonably” perform better, which does not reflect the realities of neural network training. Such a claim oversimplifies the complex dynamics involved in optimization and generalization.\n\n2. Similarly, the proof of Theorem 2 raises concerns. Prior to the NTK-based analysis, much of the derivation appears to involve manipulation of statistical quantities such as expectations. For instance, if $p_l$ is treated as a stochastic variable, it is not clear how it can be legitimately pulled out of the expectation in Eqs. 28–29. Assumptions should not be introduced merely to simplify derivations or produce desired results.\n\n3. Finally, the benchmark datasets used are relatively old and small, which may allow the models to easily saturate the dynamics, limiting the generalizability of the results."}, "questions": {"value": "Please see the [Weakness] section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TtKT7Z6Gvu", "forum": "d3zrIHukon", "replyto": "d3zrIHukon", "signatures": ["ICLR.cc/2026/Conference/Submission10377/Reviewer_3iUt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10377/Reviewer_3iUt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645400799, "cdate": 1761645400799, "tmdate": 1762921697511, "mdate": 1762921697511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SALT (Structure-Aligned Learning for Time-Series), which decomposes a time series into frequency-aligned components via Iterative Dominant Extraction and trains each component independently (Separable Training). The authors argue that single-loss training suffers from gradient interference and spectral bias, and claim SALT mitigates both through structure-wise decoupling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Identifies and formalizes two genuine optimization issues (gradient interference, spectral bias) often overlooked in forecasting.\n- Consistent performance gains across diverse models suggest broad applicability."}, "weaknesses": {"value": "- The central idea (training decomposed components separately) is conceptually close to existing frequency- or residual-based decompositions (e.g., FEDformer, N-HiTS, or frequency-domain ensembling). The incremental distinction between “Separable Training” and prior modular approaches is not convincingly articulated.\n- Separable training multiplies model cost by the number of components; the paper does not analyze compute scaling or fairness versus single-model baselines.\n- Theoretical results (Theorem 1–2) are mostly restatements of intuitive properties (removing cross-terms) and assume ideal independence between components.."}, "questions": {"value": "- How is this different in essence from training separate frequency-band or residual models already common in decomposition-based forecasting? Does SALT provide a new optimization mechanism beyond architectural modularization?\n- What is the computational overhead of Separable Training (parameter count × N, training time)? Could similar gradient-decoupling effects be achieved with shared-parameter multi-head designs?\n- Would partial parameter sharing (e.g., shared encoder, separate decoders) preserve the claimed theoretical benefits?\n- Why does Figure 5 show minimal improvement on Weather (which is non-stationary), if spectral bias reduction is the main claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IHLawytJWX", "forum": "d3zrIHukon", "replyto": "d3zrIHukon", "signatures": ["ICLR.cc/2026/Conference/Submission10377/Reviewer_tWHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10377/Reviewer_tWHh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981525040, "cdate": 1761981525040, "tmdate": 1762921696997, "mdate": 1762921696997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}