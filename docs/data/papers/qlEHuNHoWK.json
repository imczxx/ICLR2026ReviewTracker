{"id": "qlEHuNHoWK", "number": 14760, "cdate": 1758243182023, "mdate": 1759897350688, "content": {"title": "Is Exploration or Optimization the Problem for Deep Reinforcement Learning?", "abstract": "In the era of deep reinforcement learning, making progress is more complex, as the collected experience must be compressed into a deep model for future exploitation and sampling. Many papers have shown that training a deep learning policy under the changing state and action distribution leads to sub-optimal performance even collapse. This naturally leads to the concern that even if the community creates improved exploration algorithms or reward objectives, will those improvements fall on the \\textit{deaf ears} of optimization difficulties. This work proposes a new \\textit{pracitcal} sub-optimality estimator to determine optimization limitations of deep reinforcement learning algorithms. Through experiments acrossenvironments and RL algorithms, it is shown that the difference between the best data generated is better than the policies' learned performance. This large difference indicates that deep RL methods only exploit half of the good experience they generate.", "tldr": "We propose a method to measure an RL algorithm's ability to learn from its generated data and find that deep reinforcement learning has a larger optimization problem than commonly understood.", "keywords": ["deep learning", "reinforcement learning", "evaluation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73ea51e9b6a61105f68a66d4c009665755a54504.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes an estimator for assessing the sub-optimality and optimization limits of deep reinforcement learning algorithms. The authors show experimental results across different environments and RL methods and reveal that the proposed best experiences generated are 2–3 times better than the learned policy performance, indicating that current deep RL approaches may only exploit only about half of the high-quality experiences they produce."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* I think the potential contributions are strong, and I enjoyed the introduction of the paper, the problem is well motivated. \n* Deciding if the model is failing due to exploration or exploitation is potentially a valuable insight for designing a better algorithm. \n* Finding a way to deduce if a model is not exploiting high value transitions well enough would be very useful in the field of RL. \n* The experiment setting spans a diverse setting of environments, showing potential transferability."}, "weaknesses": {"value": "Unfortunately, I found this paper very difficult to read due to the lack of rigor, lack of claim backing, and missing terms. Often times, it isn't clear on what is trying to be said. For example:\n* Most empirical works use $V(π_θ(s_0))$ for comparing across algorithms to understand which algorithm\nperforms the best on a set of tasks. \n  - Can there be citations provided here?\n* However, if the policy π can not explore optimally, using $π^∗$ is not very informative.\n  - This wasn't clear to me. Informative with respect to exploration? Informative with respect to debugging purposes? Can this be written out explicitly? \n* The challenge is that $π_θ$ can be arbitrarily bad compared to $π^∗$\n  - Bad in what sense? Sub optimality? \n*  If the experience were equal, algorithm B would see the same experience as algorithm A, then B would result in better\nperformance and have a smaller practical sub-optimality.\n   - Can this be proven? This statement does not come across as trivial. If the generated experience of B was as good as A, then it would no longer be B since B is said to generate lesser experience than A. As it reads, the above statement might imply that if A was as good as exploiting as B, then A would be better? Then it is no longer A.\n*  $\\pi_θ$, $\\hat{\\pi}^*$ are broadly defined, but its not clear what $\\hat{\\pi}^\\theta$ means, and it is never defined despite being used in section 4.\n* Can more explanation and set up be shown for EQ 6?\n*  For stochastic environments, the first version the best stocastic policy from the\ncollected experience as top 5% of experience generated by the agent $V^{∗πD_{∞}}(s_0)$, where $D_∞$is all the experience collected by the agent.  \n   - I feel like an appendix should be used to address the terms that are being used, what is top 5%? top 5% most common? top 5% with respect to what? Why is this considered to be the 'best' is it because it comes from the optimal policy on all data? How was that policy obtained? At this point, it is still unclear to me why EQ 6 is a suitable score to determine sub optimality. \n* I feel as though since EQ 6 depends on how V is computed, it should be clearer on how the $D_∞$ is explicitly used to estimate V, assuming V is a functional approximation, the batching of the data and the converge should be talked about to inform the reader about the precautions taken. \n* Figure 1 states expert policy, to better align with the rest of the writing, should this be changed to the 'optimal policy'?\n* After training DQN on Montezuma’s Revenge (Figure 3b)\n  - PPO?\n\nThere seems to be a lot of terms used but no trace to what they explicitly mean. Claims seem to be stated, and quickly moved on from without any explanation or proof. This paper would greatly improve if the authors utilized an appendix, began to prove some of the claims used and were clear on the definitions used. Due to the clouded structure I unfortunately found it difficult to digest the insights made in the experiment section and compare with findings in current RL literature."}, "questions": {"value": "Besides popularity as a choice of agent, have the authors considered actor-critic agents? Being that V is computed, it would be interesting to compare how this is different to learning the critic in Soft Actor Critic, when the agent is actively updating its own value function."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lwPh2M1gFm", "forum": "qlEHuNHoWK", "replyto": "qlEHuNHoWK", "signatures": ["ICLR.cc/2026/Conference/Submission14760/Reviewer_uSzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14760/Reviewer_uSzU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761080037418, "cdate": 1761080037418, "tmdate": 1762925117998, "mdate": 1762925117998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically studies the performance of an RL algorithm by separating its ability to explore from its ability to exploit. Exploration is defined here as the ability to acquire data, and exploitation as the ability to learn the best possible policy (optimization problem) from the acquired data. The study is done by introdcuing a measure of the best policy that the algorithm can learn from the acquired data to measure the \"exploitation\" capability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper covers an important topic in RL.\n2. The proposed definition of exploitation is interesting and allows us to study the convergence of algorithms in a different way.\n3. The experimental results are extensive."}, "weaknesses": {"value": "Here are the main weaknesses I have identified, some of which can be considered issues that should be clarified.\n1. The literature review focuses mainly on problems associated with value-based methods. However, the paper claims to study convergence in general. Typically, for policy gradients, numerous papers have studied and proven convergence towards (global) optima. These often have a hidden assumption of necessary exploration. These results are not discussed.\n2. I feel that the paper overlooks the fact that the exploration method influences the convergence of algorithms in practice. Typically, in the case of policy gradient, entropy regularization makes it possible to eliminate local optima, make optimization robust, etc. This literature is not discussed in the paper.\n3. The literature on exploration methods is rather sparse; it does not cover maximum entropy methods, for example.\n4. In general, the paper is quite difficult to follow. E.g., the definition used for exploration/exploitation is given quite late in the text. The text attempts to provide insight, which is obviously good, but I feel that this sometimes obscures the exactness of certain definitions/discussions.\n5. From what I understand, the paper deals with a fairly general problem of learning policy from a set of data. Here referred to as exploitation. I get the impression that this is a fairly generic and well-studied problem. Convergence of methods, global optima, overfitting/underfitting, etc. Yet the problem is presented as stand-alone here.\n\nSome (non-extensive) elements of the literature may be of interest concerning my previous remarks. I, of course, understand that it is impossible to cite everything and that finding the most related studies is important.\nConcerning convergence:\n1. Bhandari, J. and Russo, D. Global optimality guarantees for policy gradient methods.\n2. Bhatt, S., Koppel, A., and Krishnamurthy, V. Policy gradient using weak derivatives for reinforcement learning. \n3. Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. Optimality and approximation with policy gradient methods in markov decision processes.\n4. Zhang, J., Kim, J., O’Donoghue, B., and Boyd, S. (2021a). Sample efficient reinforcement learning with reinforce.\n5. Montenegroa, A., Cesania, L., Mussia, M., Papinia, M., and Metellia, A. M. Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes.\n\nPapers studing influence of exploration on PG:\n1. Husain, H., Ciosek, K., and Tomioka, R. Regularized policies are reward robust.\n2. Ahmed, Z., Le Roux, N., Norouzi, M., and Schuurmans, D. Understanding the impact of entropy on policy optimization.\n3. Bolland, A., Lambrechts, G., & Ernst, D. Behind the myth of exploration in policy gradients."}, "questions": {"value": "1. Could you clarify if the elements from the literature previously highlighted are related to the current study? \n2. The study is largely justified by the problems caused by non-IID transitions. Your method does nevertheless not study this problem explicitly, from my understanding. Other phenomena, such as the existence of local optima, convergence speed of local search methods, and overfitting, could be the cause of the inefficiency too. Am I right, and would these phenomena be captured by your metric? Also, in PG there is no real problem of IID transitions, as algorithms consider full trajectories that are IID. In PPO, for example, what does the metric tell us about the influence of non-IID samples on the algorithm?\n3. I did not understand equation (4) and section 4.1 beyond intuition. Typically, how does policy (4) choose actions knowing that not all states are in $D^\\infty$? Similarly, I find equation (5) counterintuitive; I interpret it as a redefinition of the value function. Can the authors clarify the equations?\n4. In my opinion, the claims are a little strong, given that the metric has no theoretical basis and is influenced by several factors. Do we really show that the current problem in RL lies in the ability of algorithms to exploit the information acquired?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JDKbyMT0RF", "forum": "qlEHuNHoWK", "replyto": "qlEHuNHoWK", "signatures": ["ICLR.cc/2026/Conference/Submission14760/Reviewer_LnpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14760/Reviewer_LnpL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006231319, "cdate": 1762006231319, "tmdate": 1762925117633, "mdate": 1762925117633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to study an important problem: for practical deep reinforcement learning problems, is exploration or optimization (exploitation) a more severe problem? This paper has discussed ideas to analyze this problem in Section 4, and experimental results are demonstrated in Section 5."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The studied problem, \"is exploration or optimization (exploitation) the problem for (practical) deep RL\", is an important and interesting problem.\n\n- The experimental results in Section 5 are interesting."}, "weaknesses": {"value": "Though this paper is interesting, I do not think the current version is ready for publication, for the following reasons:\n\n- [major] The presentation and discussion in Section 4 are too handwavy. I recommend that the authors make a major revision of it to make it more rigorous. Most importantly, please provide a mathematically rigorous definition of the **experience optimal policy** $\\hat{\\pi}^*$. This is crucial, since the experience optimal policy is a key concept in this paper, and is used to identify whether exploration or optimization is the problem.\n\nSection 4 has also provided methods to compute or estimate the experience optimal policy or its value. With a rigorous definition, this paper should also discuss whether these computes or estimates are accurate, and if not, how large the estimation errors are.\n\n- This paper has many inconsistent and undefined notations, for instance:\n  - the reward function is defined as $R(s_t, a_t)$ in Section 3, but $r(a_t, s_t)$ in Section 4\n  - in Section 4, both $\\pi^\\theta$ and $\\hat{\\pi}^\\theta$ are used for \"achieved exploitation\"\n  - Please provide rigorous definitions for $V^{\\hat{\\pi}^\\ast_{D_\\infty}}$ and $V^{\\hat{\\pi}^\\ast_{D}}$ in Section 4\n\nPlease double-check the notations and fix the typos.\n\n- [Minor] Please make the figures in this paper larger so they are more readable."}, "questions": {"value": "Please address the weaknesses listed above, especially the first one."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3uJOq0K61D", "forum": "qlEHuNHoWK", "replyto": "qlEHuNHoWK", "signatures": ["ICLR.cc/2026/Conference/Submission14760/Reviewer_ajJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14760/Reviewer_ajJV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762499818850, "cdate": 1762499818850, "tmdate": 1762925117287, "mdate": 1762925117287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s core claim is that modern deep RL struggles less with finding rewarding behavior and more with turning those good experiences into strong policies. To tease this apart, the authors propose a simple “practical sub-optimality” estimator: compare a learned policy to the best trajectories it has already produced (the “experience-optimal policy”). Across PPO and DQN on Atari/MinAtar, MuJoCo, Montezuma’s Revenge, and Craftax, they observe large gaps—often 2–3×—between those best trajectories and the final policy. They also find the gap tends to grow when you add exploration bonuses like RND or scale up the model, reinforcing that the bottleneck is exploitation/optimization rather than exploration."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally clear and well-structured. It is well-motivated and takes on an important, under-discussed question: when deep RL stalls, is the bottleneck exploration or exploitation? Bringing this issue to the forefront is valuable for both researchers and practitioners and, in my view, warrants attention regardless of whether one agrees with the specific estimator proposed."}, "weaknesses": {"value": "* Soundness of the estimator: Using the top 5% highest-return trajectories as a proxy for the “experience-optimal policy” is problematic in stochastic environments. High-return episodes may result from lucky transitions or risky, low-expectation action sequences, making them non-reproducible and not necessarily exploitable by a learned policy.\n* Lack of analysis on learnability: The paper does not examine whether these “good trajectories” correspond to behavior that is actually learnable or generalizable. Without connecting high-return episodes to stable, reproducible structure, it is unclear whether the measured gap truly reflects an exploitation failure rather than noise.\n* Arbitrary percentile choice: The decision to use the top 5% of trajectories is not theoretically or empirically justified. The paper does not discuss sensitivity to this choice or explain why this particular percentile should meaningfully approximate an experience-optimal policy.\n* Insufficient number of seeds: All experiments use only 4 seeds, which is too few to support strong conclusions\n\nOverall, the soundness of the proposed estimator remains unclear, and I feel the paper overclaims by asserting that exploitation is the primary bottleneck in deep RL; the experiments and reasoning do not sufficiently support such a broad conclusion"}, "questions": {"value": "The paper repeatedly uses $V(s\\_0)$ in discussion, but since most environments sample $s_0$ from an initial-state distribution, shouldn’t the evaluation be expressed as an expectation $\\mathbb{E}_{s_0}[V(s_0)]$ rather than a single-state value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5fbmyDoLXb", "forum": "qlEHuNHoWK", "replyto": "qlEHuNHoWK", "signatures": ["ICLR.cc/2026/Conference/Submission14760/Reviewer_Ys9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14760/Reviewer_Ys9d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762583105068, "cdate": 1762583105068, "tmdate": 1762925116942, "mdate": 1762925116942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a central and long-standing question in Deep Reinforcement Learning (DeepRL): Is the bottleneck in current deep RL performance primarily due to insufficient exploration or to optimization inefficiencies? To answer this, the authors propose a novel “practical sub-optimality estimator” that measures the gap between: The best experience ever collected by the agent (the experience-optimal policy, denoted \\pi^*_D), and The learned policy’s performance V_{\\pi_\\theta}. If the difference between V_{\\pi^*D} and V{\\pi_\\theta} is large, it implies an exploitation/optimization problem; if the gap is small, it suggests the main issue is exploration. \nUsing this estimator, the paper analyzes common algorithms such as PPO and DQN across a diverse set of environments, including Atari, MinAtar, Craftax, and MuJoCo (HalfCheetah, Walker2d, Humanoid), and under variations such as adding RND exploration bonuses and scaling network size (from small CNNs to ResNet-18). Key findings include 1) The gap between best experience and learned policy performance is often 2–3×, implying that deep RL methods only exploit roughly half of the good experiences they generate. 2) Adding exploration bonuses (e.g., RND) increases the gap, showing that better exploration can worsen optimization inefficiency. 3) Scaling network size also increases sub-optimality, reinforcing the claim that optimization and exploitation, not exploration, are the main limitations of current deep RL methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and well-motivated research question. The paper addresses a fundamental question in RL—exploration vs. exploitation—that has often been debated but rarely quantified. The proposed estimator provides a concrete diagnostic tool to analyze this trade-off empirically.\n- Broad experimental coverage. The authors test across diverse environments and both on-policy and off-policy algorithms (PPO and DQN). Figures 3–5 (pp. 6–8) show consistent trends across MinAtar, Atari, Montezuma’s Revenge, HalfCheetah, etc., supporting the generality of conclusions.\n- Potentially useful diagnostic tool. The proposed sub-optimality metric could serve practitioners as a diagnostic to quickly determine whether performance limits stem from poor exploration or optimization, guiding algorithmic focus."}, "weaknesses": {"value": "- The proposed metric compares best vs. average trajectories but does not causally separate exploration and optimization. For example, an algorithm’s “best experience” may depend heavily on stochastic exploration artifacts rather than a genuine ability to generate diverse high-value data.\n- Limited algorithmic diversity. The experiments focus mainly on PPO and DQN, which, while standard, represent only a subset of deep RL paradigms. Missing are modern algorithms such as SAC, IQL, which emphasize optimization stability and could challenge the generality of conclusions.\n- The conclusion that “deep RL is mainly limited by exploitation” might overstate the findings. In complex domains (e.g., sparse reward tasks like Montezuma’s Revenge), exploration remains a fundamental challenge, even if optimization inefficiency also plays a role.\n- The observed “exploitation gap” could arise from various factors—bootstrapping noise, representation drift, catastrophic forgetting—not purely optimization failure. A deeper analysis separating these sources would strengthen the claim.\n- While the paper is rich in figures (e.g., Fig. 3–6), it lacks aggregated quantitative summaries (e.g., average sub-optimality across tasks in tabular form). This would help communicate results more clearly."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E6NDxtzwUO", "forum": "qlEHuNHoWK", "replyto": "qlEHuNHoWK", "signatures": ["ICLR.cc/2026/Conference/Submission14760/Reviewer_VZUT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14760/Reviewer_VZUT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762843299424, "cdate": 1762843299424, "tmdate": 1762925116625, "mdate": 1762925116625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}