{"id": "44C65zgF2G", "number": 22297, "cdate": 1758329218254, "mdate": 1759896873988, "content": {"title": "HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning", "abstract": "Existing LLM-based automatic test generation methods mainly produce input and expected output pairs to categorize the intended behavior of correct programs. Although straightforward, these methods have limited diversity in generated tests and cannot provide enough debugging information. We propose HarnessLLM, a two-stage training pipeline that enables LLMs to write harness code for testing. Particularly, LLMs generate code that synthesizes inputs and validates the observed outputs, allowing complex test cases and flexible output validation such as invariant checking. To achieve this, we train LLMs with SFT followed by RLVR with a customized reward design. Experiments show that HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity. HarnessLLM further benefits the code generation performance through test-time scaling with our generated test cases as inference-phase validation.", "tldr": "", "keywords": ["Test case generation", "automatic debugging"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12348c34a267a40f7ded3f5deba0ef3d0cf38c2b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose HarnessLLM, a two-stage training pipeline that enables LLMs to write harness code for testing. Particularly, LLMs generate code that synthesizes inputs and validates the observed outputs, allowing complex test cases and flexible output validation such as invariant checking. To achieve this, authors train LLMs with SFT followed by RLVR with a customized reward design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.The writting is good and easy to follow\n2.The paper presents a clear motivation"}, "weaknesses": {"value": "1.The proposed method appears heavily dependent on the base model's inherent capabilities, and given that relatively small models (e.g., Qwen3-4B) were selected for experimentation, it remains unclear whether the approach would yield comparable performance improvements when applied to larger, more capable models.\n2.The experimental evaluation is insufficiently comprehensive: the comparison with larger models is limited, the evaluation datasets contain relatively small sample sizes that raise questions about statistical robustness, the performance gains over the input-output baseline are marginal in some cases, and additional experiments differentiating this work from existing approaches would strengthen the empirical validation.\n3.The related work section lacks thoroughness, as the paper does not adequately position itself within the broader landscape of automated test generation, LLM-based software engineering tools, and property-based testing methodologies.\n4.The novelty of the contribution is limited, as the core idea of generating executable test code rather than test cases has been explored in prior work on property-based testing and programmatic test generation."}, "questions": {"value": "1.What methodology do the authors employ to filter and select suitable training data from the generated test cases? \n2.To what extent can the generated test data surpass the intrinsic performance boundaries of the underlying model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dT6qwrrc77", "forum": "44C65zgF2G", "replyto": "44C65zgF2G", "signatures": ["ICLR.cc/2026/Conference/Submission22297/Reviewer_1wej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22297/Reviewer_1wej"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545380447, "cdate": 1761545380447, "tmdate": 1762942157783, "mdate": 1762942157783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HarnessLLM, a two-stage training approach for effective test generation. The main insight of HarnessLLM is to teach LLMs to write harness code for testing. The generated harness code first synthesizes diverse inputs and then generate validation function to check the correctness of the inputs and outputs. The experiments show that the proposed approach can improve the bug finding capabilities of test generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper targets an important problem (test generation)"}, "weaknesses": {"value": "The paper shapes the test generation problem in a simple and less practical scenario. In particular, the test generation scenario should assume there is a problem statement (which serves as the specification for the correct code). However, such an assumption is not always held, actually there are no problem statements or natural language-described specification for real-world software systems at most cases. Therefore, the application scenario of the proposed approach is narrow and not realistic. And it is almost infeasible to apply the approach to a real-world software repository. \n\nThe motivation of the approach design needs more justification. The paper proposes to train models to generate better test inputs and a better verifier (to check the outputs). The verifier is supposed to distinguish the behavior between the correct ground-truth program and the buggy program. If that is the case, what is the difference between generating a verifier and directly generating the correct program? That is, generating the correct program for the given problem statement can always serve as the verifier. Therefore, the idea of generating the verifier is essentially generating the code program (i.e., a code generation problem), but in a less direct way. \n\n\nThe paper misses state-of-the-art test generation techniques, especially those based on LLMs. In fact, generating test cases have been widely explored in existing literature, including both generating diverse test inputs and generating test oracles. Many of them are not included and discussed in the paper[a,b,c]. \n\n[a] Advancing Code Coverage: Incorporating Program Analysis with Large Language Models. TOSEM\n\n[b] CoverUp: Effective High Coverage Test Generation for Python. FSE’25 \n\n[c] CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models. ICSE’23\n\n\nThe proposed approach uses prompts to generate diverse dynamic test inputs. But it is unclear what is the benefits of the proposed prompt, especially over existing test input generation techniques (e.g., search-based test input generation like Evosuite or random fuzzing techniques like AFL). Currently, there is no justification on the effectiveness and strength of such an approach design. \n\nThe evaluation can be further enhanced with including more diverse programming languages (e.g., Java) and more studied LLMs. \n\nIn addition, the current subjects are too naïve for test generation tasks given the limited complexity of function-level code snippets. It is common practice to study on real-world projects."}, "questions": {"value": "1.\tWhat is the difference between generating a verifier and directly generating the correct code?\n\n2.\tHow does the approach compare to more representative LLM-based test generation techniques?\n\n3.\tHow could the approach be applied to real-world software repositories where there are no natural-language specifications? \n\n4.\tWhy use prompts to generate diverse test inputs? What is the benefits over existing search-based or fuzzing techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMvjcV2VIE", "forum": "44C65zgF2G", "replyto": "44C65zgF2G", "signatures": ["ICLR.cc/2026/Conference/Submission22297/Reviewer_ThXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22297/Reviewer_ThXT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662046207, "cdate": 1761662046207, "tmdate": 1762942157447, "mdate": 1762942157447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose HarnessLLM, a framework that enables large language models to generate complete test harnesses, including both input generators and output validators, rather than simple input–output pairs. Through a two-stage training pipeline that combines supervised fine-tuning and reinforcement learning with verifiable rewards, the framework allows smaller models to effectively learn from larger teacher models and achieve performance comparable to or exceeding them in bug detection tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework advances beyond the conventional comparison between the expected output and the target program’s output. By incorporating invariant checking and brute-force reference implementations, it enables more fine-grained and reliable validation of program behavior.\n\n2. The proposed framework demonstrates that a smaller model such as Qwen3-4B, when fine-tuned on outputs from the teacher model and further optimized with the proposed verifiable-reward-based RL, can even surpass the teacher’s performance. This result highlights the efficiency of the two-stage training pipeline and shows that the method can be effectively applied to smaller models without relying solely on large-scale LLMs, which enhances its practical value and scalability."}, "weaknesses": {"value": "1. The experimental comparison remains narrow. Most results are derived from Qwen3-4B as the student model and Qwen3-32B as the sole teacher model, with only a supplementary experiment on LLaMA3.2-3B presented in the appendix. This limited setup makes it unclear whether the proposed approach generalizes to different model scales, architectures, or teacher–student combinations.\n\n2. The experiment compares the proposed method only against UTGen and simple input–output testing. It would be helpful to include recent bug detection methods such as Fuzz4All [1] or TrickCatcher [2]. The limited baselines make it difficult to validate the claimed improvements and assess the method’s competitiveness.\n\n3. Across Figures 5 and 7, and throughout much of the paper, the analysis remains largely quantitative. While the results present clear numerical trends, there is little causal or qualitative discussion explaining why such patterns occur. The absence of deeper interpretation limits a comprehensive understanding of the proposed method’s improvements and its behavior under different conditions. It would strengthen the paper to include qualitative analyses or case studies that illustrate how the proposed approach achieves its improvements or fails under specific scenarios.\n\n4. The paper presents the validation stage as a central component of the test harness framework. The model-generated check_output() function is used to check invariants or compare results against brute-force or hardcoded outputs. However, there is no evaluation of how accurate or reliable these validation functions are. This limitation is critical because the generated verifiers are used in multiple stages. If these verifiers contain errors, the resulting noise can affect both training and evaluation. Without a quantitative or qualitative analysis of their reliability, the validity of the reported improvements remains uncertain.\n\n[1] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang.  “Fuzz4All: Universal Fuzzing with Large Language Models.”, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE, 2024.\n[2] Kaibo Liu, Zhenpeng Chen, Yiyang Liu, Jie M. Zhang, Mark Harman, Yudong Han, Yun Ma, Yihong Dong, Ge Li, and Gang Huang. “LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs”, In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, ACL 2025."}, "questions": {"value": "1. The authors should clarify the reason for using Qwen3-32B as the sole teacher model for SFT data generation. It remains unclear whether this choice was made due to the model’s reasoning capability, accessibility, or compatibility with the student model. Providing an explanation or additional justification would help readers understand whether the results depend on this specific teacher configuration or can generalize to other large models.\n\n2. It would be helpful if the authors reported the original performance of Qwen3-4B in Table 1 before applying SFT and RL training. Including this baseline would allow readers to clearly assess how much improvement is achieved through the proposed two-stage training pipeline.\n\n3. Since both the teacher model and baselines are based on the Qwen family, the authors should clarify why the UNSEEN setting also uses buggy programs generated by Qwen3-14B. This design choice makes it unclear whether the UNSEEN experiments truly evaluate generalization beyond models within the same family."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4BJAmp0uGO", "forum": "44C65zgF2G", "replyto": "44C65zgF2G", "signatures": ["ICLR.cc/2026/Conference/Submission22297/Reviewer_Y4jL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22297/Reviewer_Y4jL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717555154, "cdate": 1761717555154, "tmdate": 1762942157069, "mdate": 1762942157069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HarnessLLM addresses the limitations of automatic test generation through a two-stage training pipeline that teaches LLMs to write harness code, which is are programs that both synthesise test inputs and validate outputs via flexible checks such as invariants. The model is trained using supervised fine-tuning followed by reinforcement learning with verification rewards and a custom reward design. Experimental results show that HarnessLLM significantly improves bug detection and testing strategy diversity over traditional input–output testing, and further enhances code generation quality through test-time scaling"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper works on test case generation, a very important issue for automatic code generation.\n\nThe proposed approach that uses an automatic validator to validate test outputs is interesting. \n\nThe trained model is demonstrated to work well for the benchmarks selected."}, "weaknesses": {"value": "1. Potential bias in invariant learning and distribution analysis\n\nThe paper assumes that invariants can be effectively extracted or generated from real code, which might be true for competitive programs. But in practice, real-world programs often lack explicitly stated or easily derivable invariants. As a result, the reported distributional analysis of invariants could be biased toward competitive code or simplified code snippets, limiting the generalizability of the findings to realistic software systems.\n\n2. Overfitting and data leakage concerns\n\nThe paper’s overfitting analysis examines whether the model overfits to specific models rather than to tasks. However, overfitting can also occur at the coding task level, especially if the training data or fine-tuning data contain test cases for identical problems to those used in evaluation. This creates a risk of data leakage. \n\n3. Limitations of evaluation benchmarks\n\nThe benchmarks used for evaluation appear to be simplified and not fully representative of real-world programming challenges. They are designed for code generation, not test generation. Consequently, the claimed improvements might not translate to realistic software engineering scenarios. It is recommended that the authors evaluate their approach on benchmarks that are designed for test generation purposely, especially contamination-free benchmarks, such as UnLeakedTestBench (https://arxiv.org/pdf/2508.00408), which explicitly mitigates test case leakage and includes diverse, more challenging real-world coding problems."}, "questions": {"value": "What are the potential risks of data leakage in the evaluation process?\n\nHow does the model perform on test generation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JiXxxPvbz3", "forum": "44C65zgF2G", "replyto": "44C65zgF2G", "signatures": ["ICLR.cc/2026/Conference/Submission22297/Reviewer_W71H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22297/Reviewer_W71H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917071808, "cdate": 1761917071808, "tmdate": 1762942156665, "mdate": 1762942156665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}