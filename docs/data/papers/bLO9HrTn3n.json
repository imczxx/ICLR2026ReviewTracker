{"id": "bLO9HrTn3n", "number": 6373, "cdate": 1757975174863, "mdate": 1759897918972, "content": {"title": "Enhancing LLM Factuality for Structured Data", "abstract": "Large language models (LLMs) are typically optimized to process and output high-quality unstructured text, demonstrating remarkable capabilities in a variety of natural language tasks. Yet in practical settings, many domains, such as safety-critical or enterprise applications, rely on structured data. Improving the factuality of contemporary LLMs in these scenarios remains an open challenge, given their propensity to hallucinate or generate incorrect responses. In this work, we propose a methodology to enhance the factuality of LLMs using structured data. Specifically, we utilize an input knowledge base to generate type-constrained negative samples, and then we seed these samples to a novel verbalization procedure that generates longer context paragraphs. We demonstrate that our generated examples provide a more realistic and effective basis for both factuality evaluation and model improvement.", "tldr": "Enhancing factuality of LLMs using structured data. We propose a methodology to produce robust and realistic data for both evaluation and model improvement, starting from an input knowledge base.", "keywords": ["factuality", "structured data"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67fe9e1c90b78ecbc177ead2fe54ab0f4d948d9b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of improving factual reliability of large language models (LLMs) when dealing with structured data sources such as knowledge graphs. The authors propose a two-stage framework: Typed Constrained Negative Sampling (TCNS) Typed Constrained Negative Sampling (TCNS) generates factually incorrect triples by perturbing subject–object pairs within the same relation type, producing realistic but false samples; and a GRPO-based paragraph generator uses both true and TCNS-generated triples to synthesize training paragraphs that intermix correct and incorrect facts. The resulting data are used to fine-tune LLMs for better factual discrimination. Overall, the paper combines ideas from knowledge graph augmentation and RL-based text generation to strengthen factual grounding in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a meaningful problem—LLMs’ tendency to produce plausible but incorrect statements about structured knowledge.\n\n2. The integration of TCNS with GRPO for controlled paragraph synthesis is novel and logically sound. The design allows the model to experience “hard negatives” in a natural textual form, bridging structured and unstructured knowledge.\n\n3. Across multiple datasets, the model trained with TCNS-GRPO achieves noticeable factuality gains without harming fluency. The ablation studies clearly show that type constraints and GRPO fine-tuning each contribute meaningfully to the improvement."}, "weaknesses": {"value": "1. The paper assumes TCNS generates realistic false triples, but it provides no quantitative or qualitative validation (e.g., semantic plausibility, frequency bias). If the negatives are too easy or too artificial, the fine-tuned model may learn dataset artifacts rather than genuine fact discrimination.\n\n2. While GRPO is used for paragraph generation, the paper lacks details on reward design, optimization stability, and variance control. Since RLHF-style optimization can be unstable, this omission weakens reproducibility.\n\n3. The experiments mainly involve fact-centric QA and sentence verification tasks, but not open-ended generation or reasoning benchmarks. Moreover, comparisons with existing factual enhancement methods—like retrieval-augmented fine-tuning, factual consistency training, or knowledge editing—are missing."}, "questions": {"value": "How does TCNS handle multi-relation entities where a type-constrained substitution might still form a true triple (i.e., false negatives)?\n\nDid you observe any training instability or reward hacking during GRPO paragraph generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7vTkKrH6WQ", "forum": "bLO9HrTn3n", "replyto": "bLO9HrTn3n", "signatures": ["ICLR.cc/2026/Conference/Submission6373/Reviewer_HRWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6373/Reviewer_HRWH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604853924, "cdate": 1761604853924, "tmdate": 1762918662538, "mdate": 1762918662538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the fact-checking capabilities of large language models (LLMs) by designing three scenarios: assessing whether the model can identify the correctness of original triples, constructing two paragraphs of different difficulty levels based on these triples to evaluate fact-checking ability in long texts, and proposing the use of SFT to enhance the model’s capacity to recognize factual knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important problem — how to enhance the fact-checking capability of LLMs.\n\n2. It leverages existing triple-based datasets to construct more challenging long-text samples that contain subtle factual errors. These samples serve two purposes: evaluating the model’s fact-checking ability and providing training data to further improve that ability.\n\n3. The paper validates the fact-checking performance of LLMs under multiple experimental settings. The results demonstrate that the constructed data are indeed more challenging, and that training on them leads to improved model performance."}, "weaknesses": {"value": "1. The title Enhancing LLM Factuality may mislead readers into thinking the paper focuses on improving the factuality of model responses and reducing hallucinations. However, the actual work is about fact-checking, not response factuality enhancement.\n\n2. The novelty of the paper is limited. The main contribution appears to be introducing GRPO to embed incorrect triple-based facts into the context, thereby generating more challenging detection data. These harder examples are then used for SFT training. However, the consistency between this constructed data and the texts encountered in real-world fact-checking scenarios remains questionable.\n\n3. The writing quality could be improved. The table naming is confusing — entries in the same row are not on the same conceptual level. For example, Topline, Zero-Shot Generated Paragraphs, and GRPO Generated Paragraphs do not correspond: the latter two describe methods of text generation, while “Topline” does not describe a text type. It would be clearer to use something like “Triples” instead. Similarly, Simple Prompt does not match the other items in its row — the others appear to be model names used for text generation (though this is not clearly introduced), while Simple Prompt seems to describe the prompt used by the classifier (again, inferred). This inconsistency makes the table difficult to interpret.\n\n4. There is not introduction about metrics which specifies how scores were calculated. Combined with the confusing table structure, the results are hard to understand and took time to parse. The numbers in the table also lack visual emphasis (e.g., bolding key results), leaving readers unsure where to focus.\n\n5. The figures could also be improved. Their current presentation is not very clear, and understanding them requires referring to specific experimental settings. For example, in Figure 2 (top), a concrete example should illustrate how the swapping process works. In the bottom part, the process of transforming triples into paragraphs should be explicitly described.\n\n6. The paper lacks baseline comparisons with existing fact-checking models or benchmarks."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3Td6Jr0DQX", "forum": "bLO9HrTn3n", "replyto": "bLO9HrTn3n", "signatures": ["ICLR.cc/2026/Conference/Submission6373/Reviewer_w2CD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6373/Reviewer_w2CD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795909966, "cdate": 1761795909966, "tmdate": 1762918662050, "mdate": 1762918662050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a data-centric recipe to boost LLM factuality when truth lives in structured sources: generate type-constrained negative samples (plausible-but-fake triples from a knowledge graph), embed them into long, natural paragraphs, then fine-tune models to spot and correct the fakes. \nParagraph generation is trained with a reward that makes the paragraph fool a “student” prompt (no KB fact given) but not an “oracle” prompt (given the source fact), ensuring the fake is present and hard to spot.\nUsing WebNLG and REBEL, the authors show that GRPO-generated paragraphs are harder and more realistic than simple zero-shot generations, and that training on these challenging cases measurably improves fake-fact detection and correct-fact recognition across LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of detecting information that is incosistent with a KB can be an important task and could have several potential real life use-cases. \n- The proposed recipe, seems to show an improvement both on being able to generate more convincing passages with fake info for models to identify. And using these negatively generated samples shows improvement on teaching the LM to detect fake facts.\n- The proposed GRPO self play style algorithm is interesting"}, "weaknesses": {"value": "- I found the results section particularly hard to follow. This might be improved with better describing the hypothesis the authors wish to validate with the experiments and how the results validate them.\n- If the goal is to show that models detect if text is consistent with a KB, perhaps general purpose KBs may not be the best way to test this as LLMs contain a lot of world info and are probably more likely to suffer on tail facts. Perhaps experiments on more specialized KBs could be more informative (eg, biomedical KBs or some technical ontologies) where the owing to its niche the LLMs initial information might be less likely to bias it. \n- Overall the paper is interesting but I believe the experimental section is not very convincing and the paper may benefit from a round of revision. I am happy to reconsider my scores during the rebuttal phase."}, "questions": {"value": "In Tab 1, row 1, does a low topline score indicate that the model does not really know the true facts? If this is the case, wouldn't a good baseline be some continual training with passages with positive facts to see if that improves performance more than negative sampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sobyua7KVz", "forum": "bLO9HrTn3n", "replyto": "bLO9HrTn3n", "signatures": ["ICLR.cc/2026/Conference/Submission6373/Reviewer_1PUt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6373/Reviewer_1PUt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762470970953, "cdate": 1762470970953, "tmdate": 1762918660883, "mdate": 1762918660883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}