{"id": "ql75YH2GhG", "number": 1092, "cdate": 1756838811728, "mdate": 1763106018992, "content": {"title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing", "abstract": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution.  This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.", "tldr": "We use two diffusion models to guide each other, and produce samples that satisfy the condition of each diffusion model", "keywords": ["Diffusion Sampling", "training free", "diffusion guidance"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/726b5c7d19e010f8be1172c4b88669a0eae8d64f.pdf", "supplementary_material": "/attachment/4d72e9dbd9211d91541ca1341ca11410d0db2d76.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces \"Coupled Diffusion Sampling,\" an inference-time method for achieving multi-view consistent image editing using pre-trained 2D editing and multi-view diffusion models. It concurrently samples two diffusion trajectories, one for 2D edits and another for multi-view consistency, using a simple coupling term to mutually guide them. This training-free approach avoids explicit 3D optimization, making it efficient and applicable across various editing tasks like spatial editing, stylization, and relighting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed Coupled Diffusion Sampling offers a computationally efficient and training-free solution for multi-view consistent image editing, elegantly sidestepping the lengthy optimization and instability issues common with explicit 3D representations. Its core strength lies in its ability to leverage existing pre-trained 2D and multi-view diffusion models by introducing a simple, inference-time coupling term, thereby achieving implicit 3D regularization. This approach demonstrates broad applicability across various editing tasks, including spatial editing, stylization, and relighting, and compatibility with diverse diffusion backbones, positioning it as a versatile and general framework for consistent multi-view content generation."}, "weaknesses": {"value": "W1. The proposed method relies on a strict constraint that the two diffusion models must share the same data domain and diffusion schedule. This considerably limits the general applicability of the method. Although the authors emphasize that their approach is “broadly applicable” and works in a “variety of settings”, such requirements contradict these claims.\n\nW2. A core component of the proposed method is the additional term $\\nabla_x U(x, x’) = -\\lambda (x - x’)$. Since this is governed by the hyperparameter $\\lambda$, the performance of the model may highly depend on carefully tuning $\\lambda$. This results in the method being overly reliant on heuristic engineering rather than principled modeling. The paper does not provide sufficient experiments or in-depth analysis on how $\\lambda$ influences performance, except for a brief mention in Figure 10. Furthermore, it appears likely that $\\lambda$ must be adjusted differently across datasets or input domains used in various tasks. If the success of the method depends heavily on such manual tuning, it could indicate that the approach is inherently unstable and lacks robustness.\n\nW3. Missing evaluation protocol.\nThe paper lacks a clear evaluation protocol. It is not specified which datasets are used, how many instances per dataset are evaluated.\n\nW4. Inadequate and misaligned baselines.\nThe paper compares against “Composable Diffusion(Liu et al, 2022)” and “Reduce, Reuse, Recycle (Du et al, 2022)” which are 2D compositional methods without 3D priors and are not trained for 3D content generation or editing. These choices are misaligned with the paper’s tasks. Strong, task-specific baselines are required.\n\n• Stylization / 3D object editing: compare against 3D editing or 3D-aware generation methods such as Vox-E[1], Tailor3D[2], and Treliis[3].\n\n• Spatial editing (3D-aware image edits): compare against Diffusion Handles[4], Magic Fixup[5], and GeoDiffuser[6]. \n\nW5. Limited editing diversity.\nMost results only demonstrate texture- or color-level modifications, which are relatively simple forms of editing. To convincingly validate that the method supports general and versatile editing, it is necessary to include more challenging tasks such as object insertion, removal, or structural manipulation. Without these results, the method’s editing capability appears limited in scope.\n\n\n---\n[1] Sella, Etai, et al. \"Vox-e: Text-guided voxel editing of 3d objects.\"\n\n[2] Qi, Zhangyang, et al. \"Tailor3d: Customized 3d assets editing and generation with dual-side images.\"\n\n[3] Xiang, Jianfeng, et al. \"Structured 3d latents for scalable and versatile 3d generation.\" \n\n[4] Pandey, Karran, et al. \"Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d.\"\n\n[5] Alzayer, Hadi, et al. \"Magic fixup: Streamlining photo editing by watching dynamic videos.\"\n\n[6] Sajnani, Rahul, et al. \"Geodiffuser: Geometry-based image editing with diffusion models.\""}, "questions": {"value": "Q1. Does the hyperparameter $\\lambda $remain fixed across all datasets and input images, or is it adjusted differently depending on the experiment?\n\nQ2. In Equation (4), the objective is stated as a **min** problem for $J^A(x_A, x_B) + J^B(x_A, x_B)$. Given that $J^A$ and $J^B$ are defined as: $p_{\\text{data}}(x) \\exp U(x, x')$ where both $p_{\\text{data}}(x)\\) and \\(\\exp U(x, x')$ (with $U(x, x')$ being a negative Euclidean distance) are terms that represent *desirability* and should ideally be maximized, it seems that a **max** optimization would be more appropriate. Could the authors clarify this choice of **min** versus **max** in their formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w18huFjBqC", "forum": "ql75YH2GhG", "replyto": "ql75YH2GhG", "signatures": ["ICLR.cc/2026/Conference/Submission1092/Reviewer_yapA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1092/Reviewer_yapA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761469960595, "cdate": 1761469960595, "tmdate": 1762915676273, "mdate": 1762915676273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks for all the reviewers for the constructive comments, and we will incorporate the feedback in our revision."}}, "id": "5cF3BCobAy", "forum": "ql75YH2GhG", "replyto": "ql75YH2GhG", "signatures": ["ICLR.cc/2026/Conference/Submission1092/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1092/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763106017969, "cdate": 1763106017969, "tmdate": 1763106017969, "mdate": 1763106017969, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method to edit a single-view but apply a multiview diffusion model to lift it to novel view images. The key idea is to combine the denoising direction of two denoisers (i.e., the single-view editing and the multiview diffusion). Experimental results on several examples show the proposed method enables multiview editing, relighting, and stylization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of combining the denoising directions of two diffusion models is interesting and may be useful in some cases."}, "weaknesses": {"value": "1. The task is not well motivated. I'm not sure why we need to do multiview editing when only a single-view image is given. An alternative baseline would be we first edit the single-view image and then run the multiview diffusion model on the edited single-view image. In this case, we could easily get multiview-consistent edited results.\n2. The quality is not good enough. The videos provided in the supplementary material clearly show inconsistent textures or geometry.\n3. Some recent and strongly relevant baselines like Vox-E (ICCV'23), TIP-Editor (SIGGRAPH'24), and CMD (SIGGRAPH'25) are not discussed and compared. According to my experience, these trained model with explicit 3D representations could have much better performance on this task."}, "questions": {"value": "A major question is why $U$ can be defined as an L2 distance. Here, a noticeable thing is that we apply the direction difference between two denoisers on the input view to all other novel views. I'm not sure whether the direction difference on the first view here is valid for all other novel views if they have different visual structures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IX6IAqQIBS", "forum": "ql75YH2GhG", "replyto": "ql75YH2GhG", "signatures": ["ICLR.cc/2026/Conference/Submission1092/Reviewer_Hyz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1092/Reviewer_Hyz4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471066590, "cdate": 1761471066590, "tmdate": 1762915675868, "mdate": 1762915675868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a novel inference-time method for multi-view consistent image editing using pre-trained 2D image editing models. \nThe main contribution lies in the coupled diffusion sampling which is a simple diffusion sampling technique that jointly samples two trajectories from both a multi-view image distribution and a 2D edited image distribution using a coupling term to enforce consistency.\nThis avoids expensive and time-consuming optimization common in NeRF models. The method proves effective and versatile over a range of tasks: spatial editing, stylization, and relighting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel inference-time method: the method provdies an effective solution to treat the multi-view consistency in image editing through simple changes in DDPM sampling avoiding expensive 3D optimization.\n2. Extended analysis: interesting ablation studies on backbone variations (Figure 8), coupled sampling on different prompts (Figure 9), and guidance strength analysis (Figure 10) which give insights into the method's behavior.\n3. Efficient method: when comparing with optimization-based methods such as NeRF/3DGS, the model is significantly faster and demonstrated to work with sparse views.\n4. Thorough validation: the paper presents results on three editing tasks (spatial editing, stylization, relighting) as well as a  user studies with 25 participants. The method is also shown to be generalisable across different model architectures (MVDream, MV-Adapter, Stable Virtual Camera) and diffusion backbones (SD1.5, SD2.1, SDXL, Flux) suggesting potential for broader applications."}, "weaknesses": {"value": "1. Theoritical basis. The paper is lacking in rigorous theoritical justification for the presented method. Line 192, a clear justification of the relationship between the constant coefficient lambda and the consistency evaluation is missing.\n\n2. Limited analysis of failure cases. Failure cases and insights into when the method would fail is not discussed. \n\n3. Computational overhead. There are several mentioning of the reduced computational overhead by the presented method but no direct comparisons with state-of-the-art methods is given.\n\n4. Framework characteristics. The scalability of the approach is not mentioned. \n\n5. Limitations. The authors state that the \"edited outputs are not perfectly 3D consistent compared to test-time optimization-based method\" (line 648). However, there is no quantification of the consistency here."}, "questions": {"value": "1. What makes this coupling mechanism robust to noise?  Flickering artifcats are shown in Figure 6 for comparison methods but it is not clear why the presented method would be exempt from artifacts.\n\n2. The constant coefficient lambda for the guidance strength seems to be arbitrarily defined. What is the reasoning for the selection of this parameter? \n\n3. For the coupling mechanism, Euclidean distance is selected. Have you explored different distance-based methods ?\n\n4. Have you experimented with more than 21 frames? How scalable is your method?\n\n5. What is the reduction in consistency compared to optimization-based methods? Could you please quantify the consistency loss? The artifacts in Figure 6 for qualitative visualisation do not give a clear view of what is happening."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xpT73nUVYg", "forum": "ql75YH2GhG", "replyto": "ql75YH2GhG", "signatures": ["ICLR.cc/2026/Conference/Submission1092/Reviewer_gCC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1092/Reviewer_gCC9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826654323, "cdate": 1761826654323, "tmdate": 1762915675537, "mdate": 1762915675537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a trainig-free method for multi-view consistent image editing using pre-trained 2D image editing models and pre-trained multi-view diffusion models. The proposed coupled diffusion sampling enables each view to be edited consistently across spatial editing, stylization, and relighting tasks. Experimental results show clear improvements over baselines in both quantitative metrics and human evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed is practical and the proposed method is simple yet effective without additional training.\n2. The performance is promising and surpasses baselines approaches.\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The theoretical analysis is rather shallow and mostly intuitive.\n2. The paper is missing two recent related works on training-free 2D image editing:\n 1. FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models, ICCV 2025\n 2. Highly Consistent and Precise Training-free Visual Editing, SIGGRAPH Asia 2025\n3. Although limitations are discussed in the appendix, it would be helpful to visualize common failure cases. \n4. The relighting performance appears weaker compared to spatial editing and stylization. The background consistency in Figures 1 and 7 seems worse. Clarification on this would strengthen the paper.\n5. In the caption of Figure 8, “multli-view” should be corrected to “multi-view.”"}, "questions": {"value": "1. Could the authors provide visualizations of typical failure cases?\n2. For relighting, what might explain the performance drop compared to the other two tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zO1CQkw3nv", "forum": "ql75YH2GhG", "replyto": "ql75YH2GhG", "signatures": ["ICLR.cc/2026/Conference/Submission1092/Reviewer_4M4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1092/Reviewer_4M4i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828555379, "cdate": 1761828555379, "tmdate": 1762915675028, "mdate": 1762915675028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}