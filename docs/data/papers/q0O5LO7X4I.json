{"id": "q0O5LO7X4I", "number": 982, "cdate": 1756826975606, "mdate": 1759898232863, "content": {"title": "Adaptive Mixture of Disentangled Experts for Dynamic Graphs under Distribution Shifts", "abstract": "Dynamic graph representation learning under distribution shifts has drawn an increasing amount of attention in the research community, given its wide applicability in real-world scenarios. Existing methods typically employ a fixed-architecture design to extract invariant patterns. However, there may exist evolving distribution shifts in dynamic graphs, leading to suboptimal performance of fixed-architecture designs. To address this issue, we propose a novel adaptive-architecture design to handle evolving distribution shifts over time, to the best of our knowledge, for the first time. The proposed adaptive-architecture design introduces an adaptive mixture of architecture experts to capture invariant patterns under evolving distribution shifts, which imposes three challenges: 1) How to detect and characterize evolving distribution shifts to inform architectural decisions; 2) How to dynamically route different expert architectures to handle varying distribution characteristics; 3) How to ensure that the adaptive mixture of experts effectively discovers invariant patterns. To solve these challenges, we propose a novel \\underline{\\textbf{Ada}}ptive \\underline{\\textbf{Mix}}ture of Disentangled Experts (AdaMix) model to adaptively route architecture experts to varying distribution shifts and jointly learn spatio-temporal invariant patterns. Specifically, we propose a spatio-temporal distribution detector to infer evolving distribution shifts by jointly leveraging historical and current information. Building upon this, we develop a prototype-guided mixture of disentangled experts that adaptively routes experts with disentangled factors to different distribution shifts. Finally, we design a distribution-aware intervention mechanism that discovers invariant patterns based on expert selection of nodes. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed (AdaMix) model significantly outperforms state-of-the-art baselines.", "tldr": "We propose a novel adaptive mixture-of-experts framework that dynamically routes disentangled architecture experts to evolving distribution shifts for dynamic graph.", "keywords": ["Dynamic Graph Neural Network; Out of Distribution Generalization; Mixture of Experts"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/caf112903d589075de40e03d850b2f8bef8e7df0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of dynamic graph learning under evolving distribution shifts. The authors propose AdaMix, an adaptive MoE framework that dynamically routes nodes to different GNN experts based on spatio-temporal distribution characteristics. The model includes a memory-augmented distribution detector, prototype-guided disentangled experts, and a distribution-aware intervention mechanism. Experiments on several real-world and synthetic dynamic graph datasets are reported to show improved OOD generalization compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed adaptive mixture formulation is interesting and aligns with the idea of conditional computation in dynamic systems.\n\n2. Experimental results show consistent improvements over several baselines, including recent OOD graph learning models.\n\n3. The inclusion of ablation studies and visualizations helps demonstrate the contribution of individual components."}, "weaknesses": {"value": "1. Novelty is limited. The framework closely follows the structure of prior dynamic OOD works with mostly terminological changes (more like a combination of DIDA, SILD and EAGLE).\n\n2. The “adaptive architecture” claim is somewhat overstated. The architecture is fixed after training, and only expert weights are adaptively combined during inference.\n\n3. The motivation for modeling distribution shifts via expert routing, rather than latent environments or causal factors, is underdeveloped and lacks theoretical grounding.\n\n4. The mathematical formulation remains descriptive rather than rigorous. The objective functions and routing updates are introduced heuristically without clear derivation.\n\n5. The experiments do not provide sufficient diagnostic evaluation (e.g., temporal shift severity, expert specialization visualization) to validate the “adaptive” behavior claimed.\n\n6. Writing style is occasionally redundant and derivative, with repeated phrases and limited conceptual clarity in the theoretical section."}, "questions": {"value": "1. How is the proposed adaptive routing different in principle from a standard MoE gating mechanism conditioned on node embeddings?\n\n2. During inference, does the model truly change its computation path, or merely reweight pre-trained experts?\n\n3. What ensures that each expert captures a distinct distributional pattern rather than overlapping ones?\n\n4. How sensitive is AdaMix to the number of experts? Were there cases where performance degraded with more experts?\n\n5. Could the observed performance gains stem from increased model capacity rather than intrinsic adaptability to distribution shifts?\n\n6. How does the proposed intervention mechanism differ from data augmentation or cross-domain mixing used in prior OOD graph works?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The manuscript exhibits unusually high structural and textual similarity to a previously published NeurIPS 2023 paper (*Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization*), including similar methodological decomposition, variable notation, section layout, and figure organization, with only superficial terminology substitutions (e.g., replacing “environment” with “expert”). Although the earlier work is cited, the overlap appears to exceed standard academic referencing and may constitute **structural or paraphrased plagiarism**.\n\nMoreover, the submission includes the statement:\n\n> “To assist with language clarity and grammatical correctness, a large language model (LLM) was employed for proofreading and text refinement; however, all scientific content, ideas, analyses, and conclusions are solely the work of the authors.”\n\nThis disclaimer is ethically problematic. Given the high degree of overlap with prior literature, such phrasing could be interpreted as an attempt to attribute textual or conceptual similarities to LLM-based rewriting, thereby **deflecting author responsibility** and potentially misleading reviewers. \n\n**Request:**  \nI respectfully ask the Ethics Committee:  \n1. Whether the submission reuses protected text, figures, or structural design from earlier work.  \n2. Whether the above LLM disclaimer constitutes an inappropriate use of AI attribution to obscure authorship accountability."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w7PBYEze31", "forum": "q0O5LO7X4I", "replyto": "q0O5LO7X4I", "signatures": ["ICLR.cc/2026/Conference/Submission982/Reviewer_h4tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission982/Reviewer_h4tg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555547220, "cdate": 1761555547220, "tmdate": 1762915652429, "mdate": 1762915652429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates dynamic graph representation learning under evolving distribution shifts and introduces an adaptive-architecture framework, AdaMix, designed to handle such changes over time. The proposed approach employs a mixture of architecture experts, guided by a spatio-temporal distribution detector, a prototype-based expert routing mechanism, and a distribution-aware intervention module to capture invariant spatio-temporal patterns. The authors identify and address three main challenges: detecting evolving shifts, dynamically routing experts, and ensuring effective invariant learning. Experimental results on synthetic and real-world datasets are reported to demonstrate the method’s performance compared with existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem studied in this paper is important.\n\n2. The experimental datasets are sufficient.\n\n3. The work has a certain degree of theoretical support."}, "weaknesses": {"value": "1. Some techniques are applied too directly without sufficient explanation. For example, why is invariant pattern modeling performed in the spectral domain?\n\n2. The comparison methods in the experiments are not up to date (latest from 2023), lacking evaluations against GraphMoE-type approaches such as GMoE and GraphMETRO.\n\n3. Considering that the proposed method follows an ensemble learning paradigm and incorporates more GNN encoders than the baselines, the performance gains on real-world datasets are relatively small—mostly within a 1% range. This raises the question of whether such limited improvement justifies the increased model complexity.\n\n4. The method performs better on synthetic datasets but only moderately on real-world ones. This discrepancy suggests that the paper’s assumptions might be overly idealized and not well aligned with the characteristics of real-world data distributions."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mRCZkR0PUm", "forum": "q0O5LO7X4I", "replyto": "q0O5LO7X4I", "signatures": ["ICLR.cc/2026/Conference/Submission982/Reviewer_1zib"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission982/Reviewer_1zib"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822563096, "cdate": 1761822563096, "tmdate": 1762915652322, "mdate": 1762915652322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the challenge of dynamic graph representation learning under evolving distribution shifts, a limitation of existing fixed-architecture methods. It proposes an adaptive-architecture framework for this task, comprising three core components: a spatio-temporal distribution detector, prototype-guided disentangled experts, and a distribution-aware intervention mechanism. Extensive experiments show the proposed model outperforms state-of-the-art baselines (e.g., SILD, EAGLE) in link prediction and node classification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using MoE technology to solve the problem of distribution shifts on dynamic graphs is interesting and effective.\n\n2. The distribution-aware mechanism avoids inefficient random interventions by leveraging dominant experts, enhancing invariant pattern extraction.\n\n3. Experiment results on diverse synthetic datasets are good."}, "weaknesses": {"value": "1. Apart from the MoE part, there is existing work on both disentangling (e.g., DIDA, SILD) and intervention (e.g., SILD, EAGLE) on dynamic graphs. Therefore, the overall architecture has a piecemeal feel. It is impossible to discern any fundamental changes in these sections compared to existing work.\n\n2. The MoE section (section 4.1) has relatively low innovation.\n\n3. The performance improvement on real datasets is very limited (less than 1%)."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HMTyk7lmBX", "forum": "q0O5LO7X4I", "replyto": "q0O5LO7X4I", "signatures": ["ICLR.cc/2026/Conference/Submission982/Reviewer_fPRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission982/Reviewer_fPRk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895337105, "cdate": 1761895337105, "tmdate": 1762915652186, "mdate": 1762915652186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses dynamic graph representation learning under evolving distribution shifts, a setting where both graph topology and features change over time and the nature of distribution shifts itself evolves. The authors propose AdaMix, an adaptive mixture-of-experts (MoE) framework that dynamically adjusts model architectures to capture invariant patterns across time. Extensive experiments on real-world (Collab, Yelp, Aminer) and synthetic datasets show that AdaMix consistently outperforms both standard dynamic GNNs (e.g., DySAT, EGCN) and prior OOD methods (DIDA, EAGLE, SILD)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper argues that evolving shifts make fixed architectures sub-optimal, which is well-supported.\n2. The paper proposes a three component framework to dynamically adjust the architecture"}, "weaknesses": {"value": "1. Some baselines show very high variance (e.g., Aminer with SILD has huge std), while AdaMix gains on real data are sometimes modest. For examples, on Aminer15, Aminer16 and Aminer17, Adamix does not have evident improvements. The reported mean ± std of baselines often overlap or even exceed AdaMix in mean. \n2. Adaptive MoE at node-time granularity plus FFT-domain masking and memory updates may increase training/inference cost.\n3. Although an ablation study is reported, the performance drop after removing individual components (e.g., memory module, prototype disentanglement, distribution-aware intervention) is not quantitatively large or clearly demonstrated in the main paper. The results do not convincingly show that each component is essential to the final performance."}, "questions": {"value": "1. What are the training/inference time compared with baselines?\n2. Could you add a “no-FFT” variant (time-domain only) and a “no-memory but deeper router” variant to disentangle spectral vs. temporal-memory contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7lml76010r", "forum": "q0O5LO7X4I", "replyto": "q0O5LO7X4I", "signatures": ["ICLR.cc/2026/Conference/Submission982/Reviewer_CVZa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission982/Reviewer_CVZa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980076960, "cdate": 1761980076960, "tmdate": 1762915652056, "mdate": 1762915652056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}