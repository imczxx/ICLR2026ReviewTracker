{"id": "DuWU9aflmc", "number": 21924, "cdate": 1758323625559, "mdate": 1759896895857, "content": {"title": "SSProNet: Secondary Structure aware Graph Neural Network for Protein Representation Learning", "abstract": "Graph structures are widely leveraged to represent proteins. However, to a large extent, proteins fold into complex three-dimensional conformations that cannot be entirely well-captured by graphs built only from sequence adjacency or distance cutoffs. In this paper, we discover that a more faithful characterization comes from secondary structure elements—such as $\\alpha$-helices and $\\beta$-sheets—that reflect recurring local motifs and stabilizing hydrogen-bond patterns. To this end, we propose a new graph neural network framework that augments node representations with the secondary structure assignment of each residue and introduces a novel edge-construction strategy based on hydrogen bonds weighted by their energetic strength. This formulation captures both local structural context and long-range couplings essential to protein stability. On commonly used benchmarks, our model achieves the leading accuracy compared with state-of-the-art methods while providing improved interpretability through biologically meaningful edges. These results highlight the promise of secondary-structure-aware, energy-weighted graphs as an effective inductive bias for protein representation learning.", "tldr": "", "keywords": ["Protein", "Graph Neural Network"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/949f42e29f771630c899c4ec0a199b13dbdce0bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a method to construct graphs for learning on protein structures. It adds secondary structure information to node features and hydrogen bonding information to edge features."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Incorporating secondary structure and hydrogen bonding information is reasonable.\n- The paper is overall organized and easy to follow."}, "weaknesses": {"value": "- The main contribution of this paper is mostly feature engineering. However, neither secondary structure or hydrogen bonding is novel features or features that have been less investigated. The graph neural network architecture is standard. There is little methodological innovation or new insights in this work.\n- The motivation of using secondary structure and hbond information is too general. It is unclear  it is unclear what improvements these features are expected to bring and how these features would improve. The evaluation benchmarks are general.\n- The performance gains on the three benchmark tasks are marginal."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AAgtyjVqWn", "forum": "DuWU9aflmc", "replyto": "DuWU9aflmc", "signatures": ["ICLR.cc/2026/Conference/Submission21924/Reviewer_G4QC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21924/Reviewer_G4QC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618038979, "cdate": 1761618038979, "tmdate": 1762941984012, "mdate": 1762941984012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SSProNet, a GNN for learning protein representations. The authors argue that standard graph-building methods, which just connect nearby atoms, are not very meaningful. Instead, SSProNet uses a more biologically-informed approach based on protein secondary structure. The model then uses the existing ProNet GNN architecture to process this new, augmented graph. The authors show this method improves performance on tasks like fold classification and ligand binding."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea is chemically sound. Using hydrogen bonds to define the graph is a more physical and meaningful way to represent protein structure than just using an arbitrary distance cutoff.\n\n2. The paper does a good job of isolating its contribution.  It re-uses a strong, existing GNN architecture and only changes the input graph and node features.  This clearly shows the benefit of the new, biology-aware graph."}, "weaknesses": {"value": "1. The method is an incremental improvement. The GNN architecture itself is taken directly from ProNet. The novelty is in the pre-processing, not in the learning model.\n\n2. This method adds a required pre-processing step: running DSSP on every protein to find all the hydrogen bonds and secondary structures before training can even start.\n\n3. The new graph structure significantly increases training time. \n\n4. Performance improvement is not consistant. On the LBA task, the model is actually worse than the baseline on two of the four metrics.\n\n5. H-bond edges makes performance much worse.  This means the H-bonds are just a supplement, not a replacement for standard proximity graphs.\n\n6. The abstract and introduction claim the model uses H-bonds weighted by their energetic strength. However, the method section states that the energies are used for filtering the edges rather than as per-edge weights. This is a contradiction and makes the energy-weighted claim misleading."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nK6cJ80Pbs", "forum": "DuWU9aflmc", "replyto": "DuWU9aflmc", "signatures": ["ICLR.cc/2026/Conference/Submission21924/Reviewer_m7T4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21924/Reviewer_m7T4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641074704, "cdate": 1761641074704, "tmdate": 1762941983744, "mdate": 1762941983744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a graph neural network for protein learning that addresses the secondary structures. Key to the method is the introduction of the hydrogen-bond edges that address the biophysical structures of proteins, in addition to the proximity edges that focus merely on the geometric structures. It further introduces some network architectural designs based on this. Experiments show the effectiveness of the new network architecture on various protein learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The work is well-motivated by analyzing the nature of the protein learning problems. The proposed network addresses the biophysical structures in these problems, in addition to the geometric features.\n- The paper proposes a valid framework following this intuition with performance improvements."}, "weaknesses": {"value": "- I am not from a biology background, but I wonder, for the three tasks in Section 4.3-4.5, how closely are they related to the hydrogen-bond information? To my feeling, the network achieves better performances not only because of a better architectural design, but it also relies on the additional information of hydrogen-bond edges, which essentially comes from additional annotations (e.g., from DSSP). In this sense, the network actually takes more inputs than the baseline methods, and this is required at both training and inference time?\n\n- If the previous point is true (that the network actually relies on more inputs/annotations), the performance improvements compared to the baselines look a bit marginal? -- correct me if the gap is actually quite large, as I'm not familiar with these tasks."}, "questions": {"value": "- See weaknesses. I'm mostly interested in: (i) if additional inputs are introduced, and (ii) if so, how relevant are these inputs to the tasks.\n- The two types of edges $E_{\\text{rad}}$ and $E_{\\text{HB}}$ are treated completely equally?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FYbbwIWdSk", "forum": "DuWU9aflmc", "replyto": "DuWU9aflmc", "signatures": ["ICLR.cc/2026/Conference/Submission21924/Reviewer_snsM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21924/Reviewer_snsM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871867325, "cdate": 1761871867325, "tmdate": 1762941983414, "mdate": 1762941983414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SSProNet, a graph neural network for protein representation learning that incorporates secondary structure as additional node features and hydrogen bond awareness as additional weighted edges. The authors claim that their model achieves state of the art results on protein fold classification, enzyme reaction classification, and ligand binding affinity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I like the motivation of incorporating domain knowledge to increase model interpretability. Considering hydrogen bonds is a nice way of including longer-range interactions in an informed manner.\n- In general, the paper is quite clearly written and easy to follow."}, "weaknesses": {"value": "- My main concern is with the novelty of this work, since most of the performance gain seems to come from enriching node features with secondary structure information. While this is a nice practical result, I’m not sure that it constitutes enough of a machine learning contribution, since there are several other models that improve model performance by injecting structural information [1-3], and are much more general in their results.\n    - Can the authors please clarify how their model meaningfully builds upon these works? \n- I’m not entirely convinced by the experimental results in this work either. In particular:\n    - The authors are missing baseline results from models that are better designed for long-range reasoning, which their H-bond edge construction seeks to tackle. An analysis comparing SSProNet to leading graph transformer or GNN with virtual node models would provide a more fair comparison here and strengthen their claim.\n    - The results for LBA are not very strong, given that ProNet-Backbone+SCHull outperforms SSProNet on half of the evaluation metrics, and is over 30% faster. \n    - Are there any additional datasets where the authors can show stronger results?\n\nOverall, I think this work would be more fitting as a workshop paper or for a more applied venue. Therefore, I recommend rejection. \n\nReferences:\n1. Bouritas et. al. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. In ICLR, 2021.\n2. Barcelo et. al. Graph Neural Networks with Local Graph Parameters. In NeurIPS, 2021.\n3. Jin et al. Homomorphism Counts for Graph Neural Networks. In ICML, 2024."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G1AuNoWze6", "forum": "DuWU9aflmc", "replyto": "DuWU9aflmc", "signatures": ["ICLR.cc/2026/Conference/Submission21924/Reviewer_1yVb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21924/Reviewer_1yVb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931043709, "cdate": 1761931043709, "tmdate": 1762941983192, "mdate": 1762941983192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}