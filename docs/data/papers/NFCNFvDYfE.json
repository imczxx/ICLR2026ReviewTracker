{"id": "NFCNFvDYfE", "number": 20774, "cdate": 1758309928933, "mdate": 1759896959266, "content": {"title": "In-Context Learning for Pure Exploration", "abstract": "We study the problem _active sequential hypothesis testing_, also known as _pure exploration_: given a new task, the learner _adaptively collects data_ from the environment to  efficiently determine an underlying correct hypothesis. A classical instance of this problem is the task of identifying the best arm in a multi-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions index hypotheses. Another important case is generalized  search, a problem of determining the correct label through a sequence of strategically\nselected queries that indirectly reveal information about the label.\nIn this work, we introduce _In-Context Pure Exploration_ (ICPE), which meta-trains Transformers to map _observation histories_ to _query actions_ and a _predicted hypothesis_, yielding a model that transfers in-context. At inference time,  ICPE actively gathers evidence on new tasks and infers the true hypothesis without parameter updates.\nAcross deterministic, stochastic, and structured benchmarks, including BAI  and generalized search,  ICPE is competitive with  adaptive baselines while requiring no explicit modeling of information structure. Our results support Transformers as practical architectures for _general sequential testing_.", "tldr": "", "keywords": ["active sequential hypothesis testing", "pure exploration", "reinforcement learning", "in-context learning", "best arm identification"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/827ad3399d7db5700564c79a08eec429cf03e85d.pdf", "supplementary_material": "/attachment/817f1c4412c9c42e99af03ab3a16d35491b168d2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces In-Context Pure Exploration (ICPE), a meta-learning approach for active sequential hypothesis testing that connects the pure exploration theory in bandits to sequential modeling using transformers. The key idea is to adaptively collect data from an environment M to identify a ground-truth hypothesis H* with minimal samples in either fixed-confidence or fixed-budget (N samples) settings. They have extensive theoretical analysis, starting with Proposition 3.1 that establishes that the optimal inference rule is $I^{\\star}(z) = arg \\max_H P(H^{\\star}=H|D_t=z)$, making posterior computation central. For policy optimization, the paper derives reward functions that emerge from the dual formulation - in fixed-budget, rewards equal the maximum posterior value at termination, while fixed-confidence uses a Lagrangian formulation with rewards $r_t, \\lambda = -1 + \\lambda\\cdot1_{stop}·\\max_H P(H^{\\star}=H|D_t)$. Note that this is heavily influenced by the Best-Arm Identification (BAI) setting in MAB. This is a special case where hypotheses coincide with actions (H=A) and $H^{\\star}=\\arg \\max_a E[\\text{reward}|a]$. They also discuss how ICPE generalizes beyond standard BAI by handling history-dependent observation kernels $P_t(·|D_t,a)$ rather than memoryless $P(·|H^{\\star},a)$, and learning the inference rule rather than assuming known likelihood functions. The paper proves that optimal policies satisfy Bellman equations with Q-functions incorporating posterior distributions, establishing the MDP formulation's validity. ICPE uses two Transformer networks: $I_{\\phi}$ approximating posteriors via cross-entropy loss, and $Q_{\\theta}$ learning the exploration policy via DQN with information-theoretic rewards. This architecture handles the credit assignment problem in sequential exploration. The approach unifies several exploration settings - it recovers classical algorithms like binary search, exploits structured dependencies in \"magic action\" bandits where certain arms reveal information about others, and generalizes to non-bandit settings like image classification, where actions (pixel regions) differ from hypotheses (digit classes). Experiments demonstrate that ICPE matches theoretical lower bounds in structured bandits (Theorem B.20) while maintaining competitive performance in standard settings, validating both theoretical predictions and practical effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a rigorous theoretical justification showing how the optimal inference rule and policy come from existing bandit theory. The problem formulation, along with the reward function, is principled rather than heuristic. The connection between posterior maximization and Q-learning is well established.\n2. The evaluation spans diverse domains from classical bandits to structured environments with hidden information, demonstrating ICPE's versatility. The magic action experiments particularly showcase the method's ability to discover and exploit non-obvious information structures.\n3. ICPE learns both components from data, handling history-dependent observations and unknown likelihood functions, making it more applicable to (some) real-world scenarios."}, "weaknesses": {"value": "1. My main concern is that it is not clear to me how a single approach works for both best confidence and best budget setting and is simultaneously optimal. Is this a best-of-both-worlds approach? Or is there a tradeoff? This requires more discussion.\n2. The writing needs more improvement. In the discussion of Algorithm 1, the stop action was suddenly introduced in lines 258-262. It is not clear to me why it is necessary. Also, the stop action is not directly motivated from theory, and seems to be heuristic. Each theorem requires a dedicated discussion section to bring out the novelty in the proofs. Again, the novelty of the theoretical proofs is not clear to me. I have not checked the appendix.\n3. While the paper proves optimality under certain conditions, it doesn't provide finite-sample complexity bounds for ICPE or formal guarantees on the meta-learning convergence, leaving uncertainty about when the approach will succeed. This deviates from the traditional BAI analysis under the bandit framework"}, "questions": {"value": "1. Can a single model trained on the trajectory-level data handle both settings at test time? How does performance degrade when using a fixed-budget trained model on fixed-confidence tasks (or vice versa)?\n2. Given the theoretical nature of the paper, the above question should also be answered from a theoretical standpoint. For example, see https://papers.nips.cc/paper_files/paper/2012/file/8b0d268963dd0cfb808aac48a549829f-Paper.pdf.\n3. Why can't the stopping decision emerge from the value function without an explicit action ($a_{stop}$)? How does forcing stop as an action affect the exploration strategy compared to value-based stopping?\n4. How many training tasks are needed for $\\epsilon$-optimal performance? The paper provides no sample complexity for the meta-learning phase.\n5. More rigorous real-world experiments should be done.\n\nIf my questions and concerns are addressed, I am willing to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not applicable."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppRkre4NdK", "forum": "NFCNFvDYfE", "replyto": "NFCNFvDYfE", "signatures": ["ICLR.cc/2026/Conference/Submission20774/Reviewer_g3VE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20774/Reviewer_g3VE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559767401, "cdate": 1761559767401, "tmdate": 1762999997743, "mdate": 1762999997743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies Active Sequential Hypothesis Testing (ASHT) in Bayesian formulation, where each environment is drawn from a prior $M \\sim \\mathcal{P}$. This problem is also known as pure exploration where actions index hypotheses. There are two settings (i) with a fixed confidence $\\delta$ (i.e., stop as soon as the predicted hypothesis is correct w.p. at least  1- $\\delta$) and (ii) a fixed budget (use $N$ samples to predict the correct hypothesis).\nIn both settings, the paper introduces In-Context Pure Explorer (ICPE), a Transformer-based\narchitecture meta-trained on a family of tasks to jointly learn a sampling  policy  $\\pi$ that collects data $\\mathcal{D}$ and an inference rule $I(\\mathcal{D})$ to identify an environment-specific ground-truth hypothesis $H^*$. \nThe training phase is implemented using the formulation of Q-learning.  The inference phase consists of the $a  \\mathrm{argmax} Q_\\theta (D_t,a)$ and the final hypothesis $\\hat{H}_N= \\mathrm{argmax}_H I_{\\phi}(H \\mid D_N)$."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- When the generative mechanism $P_t$  is not history-dependent, this coincides with Best Arm Identification of multi-armed bandits. The paper  proposes to go beyond  classical ASHT by allowing environment-specific, history-dependent observation kernels $x_{t+1} \\sim $P_t (\\cdot \\mid D_t, a_t)$. The model is very realistic and may be useful in practical cases in real-world settings.\n\n\n\n- Policy optimality for both settings using RL is shown.\nIn the fixed budget setting, the reward is assigned to $r_N(D_N)=\\max_H P(H^*=H \\mid D_N)$ at the last time step and $0$ otherwise. By defining $V_T(D_T)=r_T(D_T)$, we can define $Q$-function and  the policy $\\pi^t(D_t)= \\mathrm{argmax}_a Q_t(D_t,a)$. Optimizing with\nrespect to this reward function yields an optimal solution (Theorem 3.2). Similar results can be shown in fixed confidence settings (Theorem 3.3).\n\n\n- The literature review is thorough and provides an excellent summary connecting the work to other online learning problems. The connections with BAI and RL, in particular, are well-discussed.\nThe experiments are also thorough, including several widely used baselines from BAI."}, "weaknesses": {"value": "- Description of the stopping action is not fully specified in the paper. I am assuming a_stop = \\mathrm{argmax}_a Q_\\theta (s_t,a). \n- In the fixed-budget setting experiment, the probability of correctness is evaluated by varying the number of actions. In this test, ICPE’s performance does not consistently surpass the other baselines. What is the performance sensitivity to the budget size N?"}, "questions": {"value": "- Is the stopping condition in fixed confidence setting related to existing principles in the BAI literature or completely independent? \n-  (Regarding the fixed-budget setting) Could the authors elaborate on ICPE's performance sensitivity to the budget N, given that it did not outperform all baselines when varying the number of actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NdVENprXLe", "forum": "NFCNFvDYfE", "replyto": "NFCNFvDYfE", "signatures": ["ICLR.cc/2026/Conference/Submission20774/Reviewer_HFZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20774/Reviewer_HFZw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675224252, "cdate": 1761675224252, "tmdate": 1762999997732, "mdate": 1762999997732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the pure exploration problem in both fixed-confidence and fixed-budget settings. It introduces an in-context pure exploration approach. The approach meta-trains Transformers to map observation histories to query actions and a predicted hypothesis. Experiments on various multi-armed bandit problems, general search problems illustrate the advantage of the approach in both the averaged stopping time and correctness of the selected hypothesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using a transformer-based approach to solve the pure exploration problem is novel and interesting.\n2. The approach is theoretically sound. For both the fixed confidence and fixed budget settings, the paper defines an MDP structure with corresponding reward functions. The paper shows that achieving the pure exploration objective is equivalent to finding an optimal policy for the corresponding MDP problem. \n3. Compared with existing approaches, the proposed in-context pure exploration does not pose structural assumptions in the underlying problem structure and can perform well without prior knowledge of the underlying problem. \n4. Extensive experiments in different BAI problem settings show that the proposed approach achieves better stopping time and a guaranteed correctness level."}, "weaknesses": {"value": "1. My main concern lies in the setup of the training set, as the experimental section does not disclose these details. In general, a core challenge in online learning is that data arrive sequentially. If the method requires an extensive training set, along with prior knowledge of $H^*$ on that training set, and assumes that for each chosen $a_t$ during the training process, the corresponding $x_{t+1}$ can be observed, this may significantly limit its applicability in real-world scenarios. I hope the authors can provide more discussion on this aspect.\n2. In experiments, the authors use different metrics in different settings, e.g., Figure 2 presents $P(\\tau>t)$ and Figure 4 presents cumulative regret. I think it would be better to compare algorithms in consistent metrics. \n3. For deterministic bandits under a fixed-budget setting, it seems natural—even for traditional algorithms—to enumerate all possible actions and then select the one that performs best. What is the author trying to illustrate through this example?"}, "questions": {"value": "Please see the last part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DAx5LyeQgj", "forum": "NFCNFvDYfE", "replyto": "NFCNFvDYfE", "signatures": ["ICLR.cc/2026/Conference/Submission20774/Reviewer_Ncbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20774/Reviewer_Ncbv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893445100, "cdate": 1761893445100, "tmdate": 1762999998099, "mdate": 1762999998099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The pure exploration problem in sequential decision making is the task of identifying a correct hypothesis based on sequential testing. Applications included best arm identification in multi-armed bandits. This paper presents a method for using in context learning to solve pure exploration problems. Within a multi-task setting, this paper proposes to learn two transformer networks: one for action selection conditioned on history and one for posterior inference conditioned on history. The approach is evaluated on both synthetic multi-armed bandits and real-world inspired domains where it is shown that the approach has a higher probability of returning the correct hypothesis or can reach a set confidence threshold faster. Overall, this paper provides an interesting exploration of learning to learn for the pure exploration problem setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is written very well in my opinion. I am not deeply familiar with the pure exploration problem but I still feel like the paper was accessible and did a good job of conveying necessary background and its own technical contribution.\n- The experiments have a good breadth of domains covered, including both toy and real-world inspired. Across these domains, the proposed method improves the key performance criteria compared to baselines.\n- I find the approach of meta-learning both an inference function and sampling function to be quite interesting. I'm not sure if it entirely novel in the field of meta- or in-context RL but, if it is, it is a nice contribution."}, "weaknesses": {"value": "- The empirical study only considers 3-5 seeds per baseline ran. This seems much too little for understanding the true spread of results. Confidence intervals are computed with hierachical bootstrapping but no explanation for why this method was chosen is given.\n- The importance of the theoretical results in this paper is unclear."}, "questions": {"value": "Let's discuss the weaknesses raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZHwqUNEFky", "forum": "NFCNFvDYfE", "replyto": "NFCNFvDYfE", "signatures": ["ICLR.cc/2026/Conference/Submission20774/Reviewer_fitX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20774/Reviewer_fitX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967218882, "cdate": 1761967218882, "tmdate": 1762934368862, "mdate": 1762934368862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces In-Context Pure Exploration, a Transformer-based meta-RL framework that learns how to collect information adaptively and identify hypotheses without any parameter updates at test time. ICPE shows that in-context learning can implement adaptive experiment design—learning both data-acquisition and inference rules from experience."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The theoretical formulation is principled and elegant. The authors derive an information-theoretic reward function directly from the posterior optimality conditions, avoiding ad hoc design choices.\n(2) By showing that Transformers can meta-learn to explore—learning both when and how to query—ICPE opens a new research direction connecting in-context learning with active learning and sequential testing."}, "weaknesses": {"value": "(1) It remains unclear how ICPE scales to unseen environment distributions or how its meta-training distribution affects performance.\n(2) The experimental suite, though diverse, primarily focuses on low-dimensional or discrete problems. These settings validate proof-of-concept behavior but do not test ICPE’s limits in large or continuous hypothesis spaces.\n(3) Many experiments rely on synthetic or well-defined priors over tasks, where the environment distribution and hypotheses are known. In more realistic scenarios, such priors are unavailable or nonstationary.\n(4) While ablations such as I-DPT and I-IDS isolate the inference component, the paper lacks a deeper analysis of what the Transformer learns in-context.\n(5) Meta-training Transformers on families of ASHT tasks could be expensive."}, "questions": {"value": "(1) How sensitive is ICPE to the distribution of training environments?\n(2) How would ICPE extend to continuous or combinatorial hypothesis spaces?\n(3) The method involves several interacting components (Q-network, inference network, and λ updates). How sensitive is performance to the stability of these updates or to hyperparameters like λ’s learning rate, buffer size, and synchronization intervals (Tθ, Tϕ)?\n(4) In the fixed-confidence setting, λ and the stop-action encode correctness constraints. How robust is this mechanism in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UuJL0PqYea", "forum": "NFCNFvDYfE", "replyto": "NFCNFvDYfE", "signatures": ["ICLR.cc/2026/Conference/Submission20774/Reviewer_J4j4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20774/Reviewer_J4j4"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328357998, "cdate": 1762328357998, "tmdate": 1762934315455, "mdate": 1762934315455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}