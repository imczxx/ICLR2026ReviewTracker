{"id": "nDSTDq0uDK", "number": 16256, "cdate": 1758262407426, "mdate": 1759897251784, "content": {"title": "Watermarks for Language Model via Probabilistic Automata", "abstract": "A recent watermarking scheme for language models achieves distortion-free embedding and robustness to edit-distance attacks. However, it suffers from limited generation diversity and high detection overhead.\nIn parallel, recent research has focused on undetectability—a property ensuring that watermarks remain difficult for adversaries to detect and spoof.\nIn this work, we introduce a new class of watermarking schemes constructed through *probabilistic automata*.\nWe present two instantiations: (i) a practical scheme with exponential generation diversity and computational efficiency, and (ii) a theoretical construction with formal undetectability guarantees under cryptographic assumptions. Extensive experiments on LLaMA-3B and Mistral-7B validate the superior performance of our scheme in terms of robustness and efficiency.", "tldr": "", "keywords": ["language modes", "watermarks"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5aa4193d79b603941a0ae6b32651c584f44e51e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper uses probabilistic automata for LLM watermarking."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "They claim to have higher diversity than the watermark that simply biases all text towards a fixed set of polynomially many strings.\nThe experiments might be promising, I did not really look at them."}, "weaknesses": {"value": "I cannot figure out what their scheme is actually doing. For the parts I was able to understand, there are serious flaws (see \"Questions\").\n\nI do not understand how they can claim to achieve undetectability, when their detector appears to compute edit distance between the given text and a polynomial-sized list of strings from the key. These cannot both be true: If the generations have some correlation with a polynomial sized list of strings, then they are not pseudorandom.\nMoreover, LPN or sparse parities are not mentioned anywhere in the description of their scheme. This seems like a big problem given that their entire claim of undetectability rests on LPN.\n\nIf there is actually something useful about the probabilistic automata perspective, I do not know what it is. The only paragraph I found that explains how the probabilistic automaton is used for watermarking is this:\n\n\"The automaton begins at an initial state q0 and terminates at a final state qf , progressing through |V|\nlayers that each encode a binary vector µi\n. The first layer starts with: q0 → σ1,1, and each layer\nproceeds through intermediate bitwise states: σi,j → σi,j+1, for 1 ≤ i ≤ |V|, 1 ≤ j < b, where σi,j\nencodes the j-th bit of µi\n. At σi,b, the automaton branches into two parallel Boolean paths: σi,b →\nιi,b+1, σi,b → ˆιi,b+1, which continue as: ιi,j → ιi,j+1, ιi,j → ˆιi,j+1, ˆιi,j → ιi,j+1, ˆιi,j → ˆιi,j+1,\nwhere ιi,j = 0 and ˆιi,j = 1 represent bit encodings of µi\n. Between layers, transitions connect the\nterminal states of layer i to the initial states of layer i+1: ιi,c → σi+1,1, ˆιi,c → σi+1,1, for b ≤ j <\nc, and the automaton concludes after the final layer with ι|V|,c, ˆι|V|,c → qf .\""}, "questions": {"value": "1. It appears that your undetectability proof is incorrect. You define undetectability in the natural way, following Christ et al., but then in your proof you say \"Suppose there exists an algorithm A that KL-PAC-learns the class of such distributions...\" and show a \"decision-to-search\" reduction, which is backwards! It is not difficult to give a correct search-to-decision reduction for sparse LPN, but it requires more care. The argument is slightly non-trivial, you have to re-randomize / use random self-reducibility.\n\nIt is possible that you are attempting to use Proposition 5 to bridge this issue. Proposition 5 is conspicuously missing a proof or a citation. That's for a very good reason: It's false! For instance, consider the case where F is a set of functions indexed by pseudorandom function (PRF) keys k which do the following: If the first bit of the input is 0, output 0; otherwise output PRF_k(x) where x is the remainder of the input.\nClearly this class is not efficiently PAC learnable, because on half of all inputs you only have a negligible advantage in guessing the output. But it's also clearly not a weak PRF, because it's trivially distinguishable from random.\n\nTaking a step back, your construction is essentially that of Christ and Gunn, and your formulation in terms of weak PRFs is exactly the generalization given by Golowich and Moitra. In both of those papers, correct proofs of pseudorandomness are given from well-known assumptions. So you're using the exact method of those papers for the exact same purpose (constructing binary pseudorandom codes).\nThis raises the question: _Is there any reason why you state your results in terms of this particular scheme, rather than stating it more generally using an arbitrary pseudorandom code?_ That would make your method simpler to understand as well as more modular---then if new pseudorandom codes are found, they could be plugged in to the rest of your scheme to make it stronger.\n\n2. There appears to be a fundamental misunderstanding about the meaning of \"negligible.\" The paper correctly states that \"A function is called negligible if it becomes asymptotically smaller than the inverse of any polynomial,\" but then immediately it says that \"negligible functions can be expressed as negl(\\lambda) = O(1/poly(\\lambda)).\" There is a subtle but crucial distinction here: Negligible means decaying faster than _every polynomial_, whereas 1/poly(\\lambda) (the O is unnecessary) means decaying faster than _some polynomial_. For instance, 1/n^2 is 1/poly(n) but not negl(n). This misunderstanding shows up again in equation (49) of page 21, where the paper says: \"exp(−\\Omega(log \\lambda)) = negl(\\lambda)\", which is not true at all. In fact exp(−\\Omega(log \\lambda)) = 1/poly(\\lambda)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QRyjL69Q1W", "forum": "nDSTDq0uDK", "replyto": "nDSTDq0uDK", "signatures": ["ICLR.cc/2026/Conference/Submission16256/Reviewer_tcvQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16256/Reviewer_tcvQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929163510, "cdate": 1761929163510, "tmdate": 1762926409850, "mdate": 1762926409850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a PA-based watermarking scheme for large language models (LLMs) that aims to achieve provable undetectability under computational hardness assumptions. Building on pseudorandomness and the sparse Learning Parity with Noise (LPN) problem, the authors claim their approach offers stronger theoretical guarantees than existing watermarking methods such as KGW, STA-M, and WEPA. The paper provides theoretical arguments about the connection between undetectability and LPN hardness, and presents experimental evaluations on detection accuracy and p-values."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Watermarking is a timely topic. Investigating the application of PA in watermarking is an interesting direction."}, "weaknesses": {"value": "1. I am not convinced that a PA-based watermarking scheme can be computationally efficient, in theory or in practice. In theory, the PA scheme is still based on a randomized process, like other watermarking schemes. In practice, the detection efficiency (in Table 1) is much slower than the WEPA.\n2. It is unclear how the theoretical guarantees translate into real-world scenarios. E.g., the hardness of sparse LPN is in the asymptotic of the input problem’s length, for watermarking, how large is this length? If it is 10-20, then even though the problem is hard (for large-size inputs), it may not be difficult to solve in practice (for small inputs), so the undetectability result may not be that useful.\n3. The improvement in p-value is small when text length is large (Fig2-3), considering that p-value is already small, in 1e-2 to 1e-4 (Figure 4), further improving it by a small amount will not make a practical difference. In addition, when the text length is small, having a hardness-based theoretical guarantee on undetectability is not convincing. \n4. There is no evaluation of the utility of the generated text."}, "questions": {"value": "See my weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfRp6a4T53", "forum": "nDSTDq0uDK", "replyto": "nDSTDq0uDK", "signatures": ["ICLR.cc/2026/Conference/Submission16256/Reviewer_V6ip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16256/Reviewer_V6ip"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940809829, "cdate": 1761940809829, "tmdate": 1762926409299, "mdate": 1762926409299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes WEPA, a family of text watermarking schemes built from probabilistic automata (PA). It argues this PA view subsumes cyclic key–sequence, “distortion-free” watermarks and enables two instantiations: (i) a practical, model-agnostic scheme that claims much higher generation diversity and near-linear detection, and (ii) a theoretical, undetectable construction via PNFA under sparse-LPN hardness. Experiments on LLaMA-3B and Mistral-7B (news-like C4 prompts) report strong edit-distance robustness and materially faster detection than the cyclic baseline, with comparable or better p-value curves; limits include no paraphrase robustness, reliance on a private key, and loose statistical bounds. The major issue is that this paper did not benchmark its method against many other SOTA watermarking methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The PA formalization is neat and unifying: it shows cyclic distortion-free watermarks (Kuditipudi et al.) are a special case of a PDFA and cleanly generalizes to PNFA, clarifying detectability vs. undetectability through learnability of automata. This perspective helps situate green-red lists and other decoder-based designs in one lens.\n2. The practical WEPA design preserves the LM’s next-token distribution (exponential-min sampling), while increasing generation diversity from Θ(λ) to Ω(λ^d n) and cutting detector complexity from Θ(λ n k²) to Θ(λ n) via a dynamic-programming alignment against the PA’s support language. That directly addresses “deterministic outputs” and high detection cost in distortion-free baselines."}, "weaknesses": {"value": "1. Evaluation breadth is narrow for a watermark meant to be robust “in practice.” There is no evaluation against paraphrase/semantic attacks, detector-aware adversaries, or order-agnostic sampling—gaps that matter given known fragility of text watermarks to paraphrase. Please consider discussing related papers (e.g., https://arxiv.org/abs/2410.13808).\n2. Baselines omit key contemporaries the audience will expect: there’s no comparison to low entropy watermarking (https://arxiv.org/abs/2405.14604v3), nor to DiPMark (https://arxiv.org/abs/2310.07710) or SynthID (https://www.nature.com/articles/s41586-024-08025-4) for cross-modality discussion of detection error modes (even if image/video, they set operational expectations on robustness and thresholding). The “unbiased watermark” and green-red set are included, but the more recent methods are not included.\n3. The deterministic output of unbiased watermark was observed and discussed, please cite previous studies (e.g. https://arxiv.org/abs/2406.02603)."}, "questions": {"value": "No more questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not needed for this paper."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fl4p1r6lnR", "forum": "nDSTDq0uDK", "replyto": "nDSTDq0uDK", "signatures": ["ICLR.cc/2026/Conference/Submission16256/Reviewer_zziE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16256/Reviewer_zziE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964223026, "cdate": 1761964223026, "tmdate": 1762926408617, "mdate": 1762926408617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}