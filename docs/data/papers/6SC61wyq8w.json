{"id": "6SC61wyq8w", "number": 11679, "cdate": 1758203035633, "mdate": 1759897561609, "content": {"title": "Controlling Video Generation with Vision Language Models", "abstract": "Controlling video generation models typically requires finetuning on video datasets with explicit control labels. However, collecting such datasets is costly, and the control modality in the data inherently restricts the controllability of the trained models. In contrast, vision language models (VLMs) can readily generalize to new tasks with pretrained knowledge and in-context learning. Motivated by this capability, we introduce Ask-A-Video, a test-time training paradigm that formulates controllable video generation as visual question answering (VQA): a video generator produces video frames, a frozen VLM answers control-related questions, and the VQA loss is directly backpropagated to the video generator. By leveraging the generalization of VLMs, Ask-A-Video enables efficient and flexible control for any off-the-shelf video generator without the need for any video data. Empirically, our method improves controllability for both text-to-video and image-to-video models across different families and scales. Compared to adding constraints via prompt extension, Ask-A-Video yields stronger prompt following and more physically plausible dynamics. It also enables fine-grained spatial and motion control through visual prompting. In addition, since our method distills controllability into the model weights, it allows reusing the learned control for new prompts without additional cost.", "tldr": "", "keywords": ["Controllable Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9fcc3947149c5fd407f529cd19531eeef41d453.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript introduces Ask-A-Video, a novel test-time training paradigm that enhances the controllability of off-the-shelf video generation models without requiring any video training data. The core idea is to frame controllable video generation as a Visual Question Answering (VQA) task. A frozen Vision-Language Model (VLM) acts as a differentiable judge: it analyzes frames generated by the video model, answers control-related questions (e.g., \"Is the car red?\"), and the resulting VQA loss is directly backpropagated through the VAE decoder to fine-tune the video generator. This approach enables fine-grained spatial, motion, and physical plausibility control through simple text or visual prompts."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation of controllable video generation as a differentiable VQA task is highly innovative. It provides a unified and flexible control interface that bypasses the need for expensive, modality-specific video datasets and dedicated control modules.\n2. The method's ability to work with any off-the-shelf video generator without video data is a significant practical advantage. It lowers the barrier to achieving fine-grained control, making it accessible for users who lack large-scale, annotated video datasets for fine-tuning."}, "weaknesses": {"value": "1. The quality of control is entirely dependent on the VLM's perceptual and reasoning capabilities. This creates a ceiling for the method's performance and means its weaknesses are directly inherited from the frozen VLM, which the method itself cannot improve.\n2. The method inherently requires multiple forward/backward passes through both the video generator and a large VLM (7B parameters in the experiments) during test-time training. This introduces significant computational overhead compared to a single inference pass, which may limit its use in real-time or resource-constrained applications. A more detailed analysis of the runtime and memory cost compared to standard inference would be beneficial.\n3. The quantitative results are presented as a single table with percentage improvements, but the metric used (e.g., success rate judged by humans? the VLM itself?) is not explicitly defined. Furthermore, comparisons are primarily made against \"prompt extension,\" which is a relatively weak baseline. A comparison against state-of-the-art methods that use dedicated control modules (even if they require video data) would better situate the performance of Ask-A-Video.\n4. The paper would be strengthened by ablations that quantify the contribution of key components. For example: How critical is the high-order UniPC solver? What is the impact of the number of frames sampled for VLM input? How does the performance scale with the rank of the LoRA adapters? An ablation on the choice of VLM (e.g., a smaller vs. larger model) would also shed light on the dependency mentioned in Weakness #1.\n5. The model's reliance on a single VLM query for supervision may be insufficient for tasks requiring the understanding of temporal dependencies, such as logical sequences (e.g., following a recipe), causal relationships (e.g., a light turning on after a switch is flipped), or basic physical prerequisites (e.g., a door opening before someone walks through it). The current architecture lacks an explicit mechanism to decompose and verify these multi-step constraints."}, "questions": {"value": "1. The VLM is used to judge individual or a small subset of frames. How does the method ensure that the imposed control (e.g., an object's position or a physical law) is enforced consistently across the entire temporal sequence, and not just on the sampled frames?\n2. The failure case shows the model \"hacking\" the VLM by changing the color of feet instead of the position. Could a multi-question or chain-of-thought prompting strategy for the VLM (e.g., \"Is there a dot on the ground? Is the person's foot on the dot?\") help mitigate such adversarial failures?\n3. The method requires access to the VLM's weights for gradient backpropagation. Have you explored any proxy methods or approximations that could make this approach work with proprietary, black-box VLMs (e.g., GPT-4V)?\n4. Could you provide more details on the human evaluation process used to generate the quantitative results in Table 1? For instance, how many evaluators were involved, and what were the specific instructions given to them?\n5. The paper focuses on short video clips (25-81 frames). How would the memory-efficient strategy scale to generating significantly longer videos, and what new challenges might arise in maintaining coherence and control over a longer horizon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CTNheUNevc", "forum": "6SC61wyq8w", "replyto": "6SC61wyq8w", "signatures": ["ICLR.cc/2026/Conference/Submission11679/Reviewer_SA7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11679/Reviewer_SA7h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826012848, "cdate": 1761826012848, "tmdate": 1762922729774, "mdate": 1762922729774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a strategy that enables prompt adherence and physically plausible controllable video generation by using a VLM as a judge. Specifically, instead of existing reward model-based RL approaches, it directly backpropagates VQA loss through DiT + VAE decoder + VLM to update the LoRA of the DiT, where several strategies are proposed to reduce excessive memory usage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The approach of updating DiT LoRA by directly integrating VLM loss is intuitive.\n- The qualitative results presented in the paper are impressive.\n- The strategies for saving memory during the process of updating DiT through VAE and VLM with VLM loss are well-designed and efficient."}, "weaknesses": {"value": "- Comparison with existing (e.g., RL-based) works that integrate VLM judges is absent (for example, works discussed in the 'VLM-as-a-Judge' section of Related Work).\n- Quantitative evaluation is insufficient. Only a user study is included, and other quantitative metrics such as VBench are not reported.\n- Video results are not presented, which makes the qualitative results (frames) in the paper less convincing."}, "questions": {"value": "It would be helpful if VQA loss graph visualization can be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vPVRa7bwDg", "forum": "6SC61wyq8w", "replyto": "6SC61wyq8w", "signatures": ["ICLR.cc/2026/Conference/Submission11679/Reviewer_BbDs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11679/Reviewer_BbDs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929478351, "cdate": 1761929478351, "tmdate": 1762922729053, "mdate": 1762922729053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting and timely topic, using vision-language models (VLMs) as powerful evaluators for diverse controllable video generation tasks without relying on labeled training data. The proposed method is a test-time fine-tuning approach that formulates video generation as an optimization process by backpropagating the VQA loss from the VLM to the video generator. It requires no additional control modules, supports any input modality, and can be applied to any video generation model without training on video data. Experiments are conducted on both text-to-video (T2V) and image-to-video (I2V) settings across multiple generators, demonstrating the generalizability of the proposed framework. The results show consistent improvements in prompt adherence and visual prompting quality. Furthermore, the learned weights can be distilled into the generator’s parameters for efficient deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using VLMs as judges for controllable video generation is an interesting and underexplored direction. The idea of employing VLMs as differentiable supervision signals to guide video generators is novel and promising.\n2. The proposed framework is flexible — it can be applied to control any modality that the VLM supports. It does not rely on explicit video or control labels and is compatible with any video generator.\n3. The improvements in prompt adherence and physical plausibility shown in Figures 3 and 4 are impressive. The visual prompting results in Figure 5 are also intriguing.\n4. The failure cases are informative, illustrating two key weaknesses: (1) the strong visual priors of the generator make it difficult to produce meaningful variations, and (2) the generator may sometimes “trick” the VLM into predicting incorrect but seemingly plausible answers."}, "weaknesses": {"value": "1. As discussed in the related works section, Luo et al. (2025) explored a similar idea of leveraging VLMs for image generation. The paper should better clarify the differences in challenges and methodologies, particularly explaining why directly applying Luo et al. (2025) to the video domain would be problematic.\n2. Since the proposed approach is based on a fine-tuning framework, a discussion on computational cost and training time when using different video backbones (e.g., Wan, FramePack, and HunyuanVideo) is needed. This would help readers understand the trade-offs in efficiency and scalability.\n3. There are no supplementary videos provided, making it difficult to assess the visual quality of the model."}, "questions": {"value": "1. The paper mentions that four frames are uniformly sampled from each generated video and paired with a VQA prompt. With only four frames, how does the VLM ensure temporal consistency or capture motion-related aspects of the video?\n2. In Figure 5, the visual prompting results show a generated hand with a reddish color, which may reveal a limitation of the visual prompting approach. Is there any explanation or potential mitigation for this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jaG0SWOlvD", "forum": "6SC61wyq8w", "replyto": "6SC61wyq8w", "signatures": ["ICLR.cc/2026/Conference/Submission11679/Reviewer_5Bmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11679/Reviewer_5Bmf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983701542, "cdate": 1761983701542, "tmdate": 1762922728569, "mdate": 1762922728569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Ask-A-Video, where a pre-trained VLM is used to improve video generation models by formulating the problem into a VQA problem. The framework freezes the VLM and inputs the prompt and the generated video into the VLM to verify whether the video is generated accordingly to the prompt, and uses this answer to backpropagate the VQA loss to directly optimize the LoRA layers within the video model. The method shows drastic improvements in terms of prompt adherence, and shows that the optimized LoRA weights can be transferred across similar scenes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and the method is very convincing and straightforward. It is also interesting that LoRA layers in video diffusion models can be directly optimized with the VQA loss, and reflects the prompt in the generated video afterwards.\n\n2. The LoRA weights are also transferrable across similar scenes, which mitigates its limitations of requiring optimization for each video. \n\n3. The generated videos with the method shows clear improvements visually, along with the quantitative results. Furthermore, the authors also include discussions for failure cases, where the method fails when prompted to make drastic changes that contradicts its generative prior, which sounds reasonable. \n\n4. The authors also share details about the implementations and efforts for mitigating the computation. While this may seem minor, this adds quite a bit to the paper as practical problems, such as handling outputs from noisy latents or handling memory constraints are commonly encountered when handling video diffusion models and VLMs."}, "weaknesses": {"value": "1. The results does not seem to show the quantitative results in terms of the visual quality for the generated videos. Considering that the method involves training signal from an external VLM, it would be crucial to show that the VQA loss is able to improve prompt adherence without much damaging the visual quality.\n\n2. The method is missing specifications for the time required to optimize the LoRA weights rather than stating \"sometimes minutes\", where  a more concrete number would help to understand how much the method costs. Similarly, some details such as training steps required for the LoRA weights seems missing."}, "questions": {"value": "1. Would it be possible to group videos with similar actions or objects and train object-wise or action-wise LoRA layers with the method? Given the results from transferring LoRA weights, it seems possible to learn a set of LoRA weights, which can be maybe ensembled in a MoE-fashion to improve video models in general in the future."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8Ywkcul1bB", "forum": "6SC61wyq8w", "replyto": "6SC61wyq8w", "signatures": ["ICLR.cc/2026/Conference/Submission11679/Reviewer_ZZRB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11679/Reviewer_ZZRB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986184694, "cdate": 1761986184694, "tmdate": 1762922728218, "mdate": 1762922728218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}