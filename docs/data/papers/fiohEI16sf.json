{"id": "fiohEI16sf", "number": 10275, "cdate": 1758165717564, "mdate": 1759897661683, "content": {"title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style", "abstract": "The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because strong empirical results can justify stronger claims, it is often unclear whether bold language reflects genuine evidence or merely rhetorical style. We introduce a counterfactual, LLM-based framework to disentangle rhetorical style from substantive content: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. Visionary framing significantly predicts downstream attention, including citations and media coverage, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, aligning with the growing use of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments for improving how ML research is evaluated.", "tldr": "", "keywords": ["AI for Metascience", "Preference Models", "LLM-as-Judge", "Computational Social Science", "LLM Personas", "Rhetorical Style Measurement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab0dc97e4af1466cdf56e0be44009473a43d72c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a counterfactual, LLM-based framework for measuring rhetorical style independently of content. It uses a calibrated panel of LLM personas to generate counterfactual texts, aggregates pairwise comparisons with a Bradley–Terry model, and infers rhetorical strength for new writings. Validated against human judgments, the method is robust to persona choice and produces fine-grained scores. Applied to ICLR submissions, it shows that rhetorical style predicts citations and media attention, revealing a notable increase in rhetorical strength since 2023."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces a counterfactual, LLM-based framework to measure rhetorical style independently of content.\n\n2. It develops a calibrated panel of LLM personas to generate counterfactual writings, aggregates pairwise comparisons via a Bradley–Terry model, and provides a method to infer rhetorical strength for new texts. \n\n3. The approach is validated against human annotations, is robust to persona choice, and produces fine-grained scores. Applied to a large dataset of ICLR submissions, the framework reveals that rhetorical style predicts citations and media attention, and that rhetorical strength has increased notably since 2023."}, "weaknesses": {"value": "1. It remains unclear whether the Bradley-Terry score is sufficiently representative or effective for mimicking rhetorical style, especially given its low correlation with reviewer scores. Further analysis is needed (see my questions below).\n\n2. The experimental section feels somewhat weak. There is no clear evidence of the reliability of the Bradley-Terry score, nor a detailed discussion of how it might generalize to broader research contexts.\n\n3. The method depends on LLM-generated counterfactual abstracts and a calibrated panel of LLM judges. This setup introduces potential biases stemming from the choice of LLM personas and their training data, which may influence the assessment of rhetorical strength.\n\n4. It is unclear whether the observed trends generalize beyond ICLR submissions, as all findings are drawn from a single conference domain."}, "questions": {"value": "1. The selection of personas will largely affect the model performance. Therefore, it's important to elaborate how these personas are selected? Whether the LLMs strictly follow the personas.\n\n2. According to Line 308, if rhetorical style were measured across full papers rather than just abstracts, would the correlation with peer-review scores increase, and could it then meaningfully predict reviewer evaluations?\n\n3. If abstracts (Y) are used as a proxy for rhetorical style while the full paper content (X) represents the substantive content, could the limited scope of abstracts lead to an incomplete or biased estimation of Z? In other words, does using only abstracts risk conflating substantive content with rhetorical framing, since abstracts may omit key details present in X?\n\n4. In Line 101, the authors compare the setup with several methods such as GAN, RLHF, DPO. It remains unclear to me how the setup is connected to this method. It would be helpful if the authors could elaborate more on it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l8LUAK8l9Q", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_sF21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_sF21"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673257949, "cdate": 1761673257949, "tmdate": 1762921627894, "mdate": 1762921627894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper assumes that each paper abstract has a particular style of presenting the problem, methods and results, which they call the “rhetorical style” of the paper. The authors build a system that can predict the rhetorical score for a paper given its abstract. The method involves 30 different rewrite prompts that use LLMs to rewrite an abstract in different styles. These styles are assumed to be “counterfactuals”. The method is used for a series of ICLR paper submissions and the results show that, once controlling for review scores, the papers with a higher rhetorical score tend to be cited and discussed more."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I found the correlation between the rhetorical score and the “popularity” of a paper an interesting result, which may show how humans are biased by the presentation style."}, "weaknesses": {"value": "I don’t see any serious weaknesses. Wondering what the practical application and implications are. Shall we all adopt a writing style that results in a high rhetorical score? :) Also not clear at all why the method is called “counterfactual”."}, "questions": {"value": "It was not clear to me why the method is called “counterfactuals”. Not clear why the rewriting prompts are assumed to produce counterfactual abstracts. Could you please clarify this term choice? It seems to be inappropriate for this context. \n\nWhat are the practical implications and/or applications of this technique?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BtqS8TANlt", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_Kcz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_Kcz4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677629218, "cdate": 1761677629218, "tmdate": 1762921627311, "mdate": 1762921627311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a counterfactual framework powered by LLM that automatically measures the rhetorical style of research papers independent of its substantive content. The framework models rhetorical style as a one-dimensional variable which is independent to the substantive content of the paper, which jointly derive the surface text form of the paper. To measure this rhetorical style variable, the paper proposes to 1) extract objective and descriptive content of the paper; 2) steer the LLM with a wide spectrum of persona to generate diverse abstracts with counterfactual writings; 3) use LLM-as-judge to provide Bradly-Terry based pair-wise comparison for estimating the rhetorical strength score for each persona to form a spectrum of rhetorical strength; 4) situate the actual (query) abstract in this spectrum to obtain the rhetorical strength estimation for the real abstract. With this framework, the authors estimates the rhetorical strength of paper abstracts of ICLR submissions from 2017 to 2025 and conduct statistical analysis of the correlation between the strength and review scores/downstream attention. The results showcases that 1) the rhetorical estimation by the proposed framework is more effective than baseline methods; 2) while the estimated rhetorical strength showcase minimal correlation with review scores, stronger rhetorical style does lead to larger downstream attention,"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed framework address the issue of entanglement of substantive content with rhetorical style, overcomes the challenges of biased measurement of rhetorical style in prior work.\n2. With the multi-persona counterfactual generation & Bradley-Terry scoring approach, the proposed framework yield high quality and less biased estimation of rhetorical scores than prior approaches.\n3. The analysis of the predictive power of rhetorical scores on peer-review scores and downstream impact/attentions provide insightful findings of how different rhetorical styles affect the recognition of the work by the community."}, "weaknesses": {"value": "1. The single-dimension formulation of the rhetorical strength measurement might have made an over-simplified assumption. For instance, the strength of rhetorical style might be multi-faceted: a paper might argue significant generalizability of their contributions and simultaneously put less emphasis of the novelty/impact. I am thus concerned if the single-dimension rhetorical strength could capture such variability."}, "questions": {"value": "Please see the weaknessess above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E7qSrB3L3D", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_3KCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_3KCn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986082934, "cdate": 1761986082934, "tmdate": 1762921626976, "mdate": 1762921626976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a counterfactual LLM-based framework for measuring rhetorical style in machine learning papers, independent of substantive content. The core innovation is using diverse LLM personas to generate counterfactual abstracts from identical technical content, then aggregating pairwise comparisons via Bradley-Terry modeling to create a calibrated rhetorical strength scale. Applied to 8,485 ICLR submissions (2017-2025), the framework reveals that visionary framing significantly predicts citations and media attention even after controlling for peer-review scores, and documents a sharp post-2023 increase in rhetorical strength coinciding with LLM-based writing tool adoption.\n\nNovelty. The counterfactual design for disentangling style from substance seems genuinely original. While Bradley-Terry models are standard in RLHF, and counterfactual text generation is used for fairness testing, their combination for scientific rhetoric measurement at scale is, as far as one can tell, novel. The framework produces fine-grained, content-controlled scores unlike prior lexicon-based or direct classification approaches.\n\nSignificance. The work addresses a documented crisis in ML research communication with immediate practical relevance. The temporal findings converge with independent evidence of LLM adoption in scientific writing, though the framework's value lies in measuring rhetorical style beyond mere vocabulary shifts. The methodological contribution generalizes to grant evaluation, news analysis, and peer review systems.\n\nBao, H., Sun, M., & Teplitskiy, M. (2025). Where there's a will there's a way: ChatGPT is used more for science in countries where it is prohibited. Quantitative Science Studies.\n\nHyland, K., & Jiang, K. F. (2021). Hyping the REF: Promotional elements in impact submissions. Higher Education.\n\nKobak, D., González-Márquez, R., Horváth, E., & Lause, J. (2024). Delving into ChatGPT usage in academic writing through excess vocabulary. arXiv:2406.07016.\n\nLiang, W., et al. (2024). Mapping the increasing use of LLMs in scientific papers. arXiv:2404.01268.\n\nPeng, H., Qiu, H. S., Fosse, H. B., & Uzzi, B. (2024). Promotional language and the adoption of innovative ideas in science. Proceedings of the National Academy of Sciences, 121(25), e2320066121.\n\nZheng, L., et al. (2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Original counterfactual paradigm: Possibly the first work to control for substantive content when measuring rhetorical style in real scientific documents, moving beyond surface-level lexical analysis of prior work (Peng et al., 2024; Hyland & Jiang, 2021)\n\n- Scale and rigor: 250k+ counterfactuals with robust validation ,including 88.4% human agreement, persona-subset stability, and fine-grained continuous scores versus coarse direct ratings\n\n- Methodological innovation: Novel use of calibrated LLM persona panel with Bradley-Terry aggregation as measurement instrument, distinct from RLHF preference learning applications\n\n- Actionable findings: Rhetorical style predicts attention with effect sizes comparable to peer-review quality (24 citations per unit increase), providing direct evidence for conference policy discussions on hype\n\n- Convergent temporal evidence: Sharp post-2023 increase aligns with independent documentation of LLM adoption surge in scientific papers (Liang et al., 2024; Kobak et al., 2024), but measures style beyond vocabulary shifts\n\n- Generalizable framework: Methodology applicable to grant proposals, news analysis, and other domains requiring style-content disentanglement"}, "weaknesses": {"value": "- Confounding interpretation unclear: Post-2023 increase could reflect (a) authors choosing stronger rhetoric OR (b) LLMs nudging authors toward LLM-preferred styles. Framework cannot disentangle these mechanisms since LLMs themselves exhibit rhetorical preferences—this is the paper's most significant limitation\n\n- Ad-hoc persona construction: 30 hand-crafted personas lack systematic derivation. No validation that personas span human rhetorical space rather than just LLM preference distributions. Could benefit from comparison to simpler counterfactual baselines or human-written counterfactuals\n\n- Abstract-only measurement scope: Pragmatically justified but limits claims about full-paper rhetoric and how reviewers actually evaluate submissions, potentially explaining the null correlation with review scores\n\n- LLM judge validation incremental: 88.4% human agreement is strong but incremental given MT-Bench established GPT-4 achieves 80-85% alignment matching inter-human agreement (Zheng et al., 2023)"}, "questions": {"value": "1) LLM adoption vs. rhetorical intent. Can you directly compare your rhetorical scores against LLM-detection scores using existing frameworks (e.g., Liang et al., 2024)? Can you share the correlation and explain how your method captures the author's rhetorical choices beyond LLM usage patterns? This is essential to further establishing the framework's unique contribution.\n\n2) Temporal mechanism disentanglement. The post-2023 increase could reflect competitive pressure, LLM tool adoption nudging style, or both. Can you differentiate these mechanisms? For example, do papers with high LLM-adoption signatures show different rhetorical patterns than low-adoption papers in 2024-2025? This confounding threatens the paper's primary temporal finding.\n\n3) Reviewer correlation interpretation. The near-zero correlation with peer review deserves dedicated investigation. Have you examined: a) whether rhetorical style predicts review score variance across reviewers, b) full-paper rhetoric vs. abstract-only rhetoric, or c) non-linear relationships? Given your policy implications focus on peer review, this hypothesis finding should be explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWXB9T4Scg", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_WLet"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_WLet"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199934700, "cdate": 1762199934700, "tmdate": 1762960472615, "mdate": 1762960472615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}