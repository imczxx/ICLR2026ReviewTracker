{"id": "fiohEI16sf", "number": 10275, "cdate": 1758165717564, "mdate": 1763756324006, "content": {"title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style", "abstract": "The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because strong empirical results can justify stronger claims, it is often unclear whether bold language reflects genuine evidence or merely rhetorical style. We introduce a counterfactual, LLM-based framework to disentangle rhetorical style from substantive content: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. Visionary framing significantly predicts downstream attention, including citations and media coverage, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, aligning with the growing use of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments for improving how ML research is evaluated.", "tldr": "", "keywords": ["AI for Metascience", "Preference Models", "LLM-as-Judge", "Computational Social Science", "LLM Personas", "Rhetorical Style Measurement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46f773af7f877743771b59edeb45336bbc218e61.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a counterfactual, LLM-based framework for measuring rhetorical style independently of content. It uses a calibrated panel of LLM personas to generate counterfactual texts, aggregates pairwise comparisons with a Bradley–Terry model, and infers rhetorical strength for new writings. Validated against human judgments, the method is robust to persona choice and produces fine-grained scores. Applied to ICLR submissions, it shows that rhetorical style predicts citations and media attention, revealing a notable increase in rhetorical strength since 2023."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work introduces a counterfactual, LLM-based framework to measure rhetorical style independently of content.\n\n2. It develops a calibrated panel of LLM personas to generate counterfactual writings, aggregates pairwise comparisons via a Bradley–Terry model, and provides a method to infer rhetorical strength for new texts. \n\n3. The approach is validated against human annotations, is robust to persona choice, and produces fine-grained scores. Applied to a large dataset of ICLR submissions, the framework reveals that rhetorical style predicts citations and media attention, and that rhetorical strength has increased notably since 2023."}, "weaknesses": {"value": "1. It remains unclear whether the Bradley-Terry score is sufficiently representative or effective for mimicking rhetorical style, especially given its low correlation with reviewer scores. Further analysis is needed (see my questions below).\n\n2. The experimental section feels somewhat weak. There is no clear evidence of the reliability of the Bradley-Terry score, nor a detailed discussion of how it might generalize to broader research contexts.\n\n3. The method depends on LLM-generated counterfactual abstracts and a calibrated panel of LLM judges. This setup introduces potential biases stemming from the choice of LLM personas and their training data, which may influence the assessment of rhetorical strength.\n\n4. It is unclear whether the observed trends generalize beyond ICLR submissions, as all findings are drawn from a single conference domain."}, "questions": {"value": "1. The selection of personas will largely affect the model performance. Therefore, it's important to elaborate how these personas are selected? Whether the LLMs strictly follow the personas.\n\n2. According to Line 308, if rhetorical style were measured across full papers rather than just abstracts, would the correlation with peer-review scores increase, and could it then meaningfully predict reviewer evaluations?\n\n3. If abstracts (Y) are used as a proxy for rhetorical style while the full paper content (X) represents the substantive content, could the limited scope of abstracts lead to an incomplete or biased estimation of Z? In other words, does using only abstracts risk conflating substantive content with rhetorical framing, since abstracts may omit key details present in X?\n\n4. In Line 101, the authors compare the setup with several methods such as GAN, RLHF, DPO. It remains unclear to me how the setup is connected to this method. It would be helpful if the authors could elaborate more on it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l8LUAK8l9Q", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_sF21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_sF21"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673257949, "cdate": 1761673257949, "tmdate": 1762921627894, "mdate": 1762921627894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper assumes that each paper abstract has a particular style of presenting the problem, methods and results, which they call the “rhetorical style” of the paper. The authors build a system that can predict the rhetorical score for a paper given its abstract. The method involves 30 different rewrite prompts that use LLMs to rewrite an abstract in different styles. These styles are assumed to be “counterfactuals”. The method is used for a series of ICLR paper submissions and the results show that, once controlling for review scores, the papers with a higher rhetorical score tend to be cited and discussed more."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I found the correlation between the rhetorical score and the “popularity” of a paper an interesting result, which may show how humans are biased by the presentation style."}, "weaknesses": {"value": "I don’t see any serious weaknesses. Wondering what the practical application and implications are. Shall we all adopt a writing style that results in a high rhetorical score? :) Also not clear at all why the method is called “counterfactual”."}, "questions": {"value": "It was not clear to me why the method is called “counterfactuals”. Not clear why the rewriting prompts are assumed to produce counterfactual abstracts. Could you please clarify this term choice? It seems to be inappropriate for this context. \n\nWhat are the practical implications and/or applications of this technique?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BtqS8TANlt", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_Kcz4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_Kcz4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677629218, "cdate": 1761677629218, "tmdate": 1762921627311, "mdate": 1762921627311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a counterfactual framework powered by LLM that automatically measures the rhetorical style of research papers independent of its substantive content. The framework models rhetorical style as a one-dimensional variable which is independent to the substantive content of the paper, which jointly derive the surface text form of the paper. To measure this rhetorical style variable, the paper proposes to 1) extract objective and descriptive content of the paper; 2) steer the LLM with a wide spectrum of persona to generate diverse abstracts with counterfactual writings; 3) use LLM-as-judge to provide Bradly-Terry based pair-wise comparison for estimating the rhetorical strength score for each persona to form a spectrum of rhetorical strength; 4) situate the actual (query) abstract in this spectrum to obtain the rhetorical strength estimation for the real abstract. With this framework, the authors estimates the rhetorical strength of paper abstracts of ICLR submissions from 2017 to 2025 and conduct statistical analysis of the correlation between the strength and review scores/downstream attention. The results showcases that 1) the rhetorical estimation by the proposed framework is more effective than baseline methods; 2) while the estimated rhetorical strength showcase minimal correlation with review scores, stronger rhetorical style does lead to larger downstream attention,"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed framework address the issue of entanglement of substantive content with rhetorical style, overcomes the challenges of biased measurement of rhetorical style in prior work.\n2. With the multi-persona counterfactual generation & Bradley-Terry scoring approach, the proposed framework yield high quality and less biased estimation of rhetorical scores than prior approaches.\n3. The analysis of the predictive power of rhetorical scores on peer-review scores and downstream impact/attentions provide insightful findings of how different rhetorical styles affect the recognition of the work by the community."}, "weaknesses": {"value": "1. The single-dimension formulation of the rhetorical strength measurement might have made an over-simplified assumption. For instance, the strength of rhetorical style might be multi-faceted: a paper might argue significant generalizability of their contributions and simultaneously put less emphasis of the novelty/impact. I am thus concerned if the single-dimension rhetorical strength could capture such variability."}, "questions": {"value": "Please see the weaknessess above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E7qSrB3L3D", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_3KCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_3KCn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986082934, "cdate": 1761986082934, "tmdate": 1762921626976, "mdate": 1762921626976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "# General response\n\nWe appreciate the reviewers' constructive feedback and insightful comments, which greatly help us improve our work. We are especially encouraged that the reviewers recognized the novelty and methodological strength of our approach, highlighting the “scale and rigor” of our counterfactual framework and its “robust validation.” We are also very greateful that the reviews commended our method for “overcoming the challenges of biased measurement” inherent in prior work, as well as our findings regarding the impact of rhetorical style on scientific popularity.\n\nTo directly address the question of whether our rhetorical scores simply reflect LLM usage patterns rather than capturing genuine author intent, we conducted additional experiments using the LLM detection framework of Liang et al. (2024). Our analyses reveal a strong correlation between LLM usage and rhetorical score.\n\n## New Experiment: rhetorical score vs LLM usage\n\nLiang et al. (2024)'s method analyzes statistical distributions of linguistic features across many documents to infer the proportion of LLM-generated text in a corpus (denoted as α). Their Maximum Likelihood Estimation (MLE) approach estimates LLM usage at the population level, not per-abstract. Therefore, to compare our per-abstract rhetorical scores against LLM usage estimates using Liang et al. (2024)'s method, we grouped abstracts by rhetorical score and estimated LLM usage for each group. \n\n### Validation of Liang et al. (2024) estimator\n\nWe begin by validating the Liang et al. (2024) estimator on two dimensions. First, abstracts submitted in 2017-2023 ICLR cycle were drafted during a period when modern LLM-based writing tools were not yet widely adopted by researchers. As such, these abstracts serve as a natural ‘pre-LLM’ baseline, and their estimated LLM usage should be close to zero. Consistent with this expectation, we find that the estimated LLM usage during 2017-2023 is 0.5%, indeed very close to 0%. \n\nSecond, in our experiment all persona abstracts are known to be LLM-generated, so their estimated LLM usage should be substantially higher than zero. The average estimated α across persona groups is 57.3%. While this value is well below 100%, it is nevertheless far greater than zero, indicating that the estimator reliably distinguishes LLM-generated text from human-written text at the population level, even if it does not fully capture the true proportion in this synthetic setting.\n\n### Findings: LLM Usage and rhetorical style are strongly correlated\n\n\nWe now examine the relationship between rhetorical scores and estimated LLM usage. Having validated that LLM usage is effectively zero in the 2017–2023 submissions, we restrict our analysis to the 2024–2025 cohort (n = 2,000). We divide these papers into 10 equally sized groups (each containing approximately 200 papers), based on their rhetorical scores, with each group representing 10% of the distribution. The correlation between group-level mean rhetorical score and group-level estimated LLM usage is strongly positive, with a Pearson correlation coefficient of 0.904.\n\n\nTable 1. Estimated LLM Usage by Rhetorical Groups during 2024–2025\n| **Group** | **Rhetoric Range**     | **Mean Rhetoric** | **Estimated LLM Usage (α)** |\n|-----------|-------------------------|--------------------|------------------------------|\n| 0 (lowest) | [-4.74, -2.95] | -3.534 | 9.0%  |\n| 1          | [-2.95, -2.25] | -2.453 | 11.1% |\n| 2          | [-2.25, -1.69] | -1.895 | 10.0% |\n| 3          | [-1.69, -1.19] | -1.353 | 16.7% |\n| 4          | [-1.19, -0.75] | -0.904 | 15.4% |\n| 5          | [-0.75, -0.13] | -0.459 | 15.7% |\n| 6          | [-0.13, 0.27]  | 0.019  | 18.5% |\n| 7          | [0.27, 0.86]   | 0.518  | 17.9% |\n| 8          | [0.86, 1.96]   | 1.323  | 17.6% |\n| 9 (highest) | [1.96, 4.53]  | 3.138  | 20.9% |\n\nTable 1 presents the estimated LLM usage in the 2024–2025 paper batch across the ten rhetorical groups. It reveals clear evidence of differential LLM adoption by rhetorical intensity. The pattern is strongly monotonic: **groups with higher mean rhetorical scores exhibit substantially higher estimated LLM usage.** For example, papers in the highest-rhetoric group (Group 9; mean score = 3.14) show an overall estimated LLM usage of α = 20.9%, which is approximately 2.3× higher than the lowest-rhetoric group (Group 0; mean score = –3.53; α = 9%). Intermediate groups follow the same upward gradient, with each increase in rhetorical intensity corresponding to progressively higher estimated LLM usage."}}, "id": "1IH3fngR62", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763676837136, "cdate": 1763676837136, "tmdate": 1763676837136, "mdate": 1763676837136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a counterfactual LLM-based framework for measuring rhetorical style in machine learning papers, independent of substantive content. The core innovation is using diverse LLM personas to generate counterfactual abstracts from identical technical content, then aggregating pairwise comparisons via Bradley-Terry modeling to create a calibrated rhetorical strength scale. Applied to 8,485 ICLR submissions (2017-2025), the framework reveals that visionary framing significantly predicts citations and media attention even after controlling for peer-review scores, and documents a sharp post-2023 increase in rhetorical strength coinciding with LLM-based writing tool adoption.\n\nNovelty. The counterfactual design for disentangling style from substance seems genuinely original. While Bradley-Terry models are standard in RLHF, and counterfactual text generation is used for fairness testing, their combination for scientific rhetoric measurement at scale is, as far as one can tell, novel. The framework produces fine-grained, content-controlled scores unlike prior lexicon-based or direct classification approaches.\n\nSignificance. The work addresses a documented crisis in ML research communication with immediate practical relevance. The temporal findings converge with independent evidence of LLM adoption in scientific writing, though the framework's value lies in measuring rhetorical style beyond mere vocabulary shifts. The methodological contribution generalizes to grant evaluation, news analysis, and peer review systems.\n\nBao, H., Sun, M., & Teplitskiy, M. (2025). Where there's a will there's a way: ChatGPT is used more for science in countries where it is prohibited. Quantitative Science Studies.\n\nHyland, K., & Jiang, K. F. (2021). Hyping the REF: Promotional elements in impact submissions. Higher Education.\n\nKobak, D., González-Márquez, R., Horváth, E., & Lause, J. (2024). Delving into ChatGPT usage in academic writing through excess vocabulary. arXiv:2406.07016.\n\nLiang, W., et al. (2024). Mapping the increasing use of LLMs in scientific papers. arXiv:2404.01268.\n\nPeng, H., Qiu, H. S., Fosse, H. B., & Uzzi, B. (2024). Promotional language and the adoption of innovative ideas in science. Proceedings of the National Academy of Sciences, 121(25), e2320066121.\n\nZheng, L., et al. (2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. arXiv:2306.05685."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Original counterfactual paradigm: Possibly the first work to control for substantive content when measuring rhetorical style in real scientific documents, moving beyond surface-level lexical analysis of prior work (Peng et al., 2024; Hyland & Jiang, 2021)\n\n- Scale and rigor: 250k+ counterfactuals with robust validation ,including 88.4% human agreement, persona-subset stability, and fine-grained continuous scores versus coarse direct ratings\n\n- Methodological innovation: Novel use of calibrated LLM persona panel with Bradley-Terry aggregation as measurement instrument, distinct from RLHF preference learning applications\n\n- Actionable findings: Rhetorical style predicts attention with effect sizes comparable to peer-review quality (24 citations per unit increase), providing direct evidence for conference policy discussions on hype\n\n- Convergent temporal evidence: Sharp post-2023 increase aligns with independent documentation of LLM adoption surge in scientific papers (Liang et al., 2024; Kobak et al., 2024), but measures style beyond vocabulary shifts\n\n- Generalizable framework: Methodology applicable to grant proposals, news analysis, and other domains requiring style-content disentanglement"}, "weaknesses": {"value": "- Confounding interpretation unclear: Post-2023 increase could reflect (a) authors choosing stronger rhetoric OR (b) LLMs nudging authors toward LLM-preferred styles. Framework cannot disentangle these mechanisms since LLMs themselves exhibit rhetorical preferences—this is the paper's most significant limitation\n\n- Ad-hoc persona construction: 30 hand-crafted personas lack systematic derivation. No validation that personas span human rhetorical space rather than just LLM preference distributions. Could benefit from comparison to simpler counterfactual baselines or human-written counterfactuals\n\n- Abstract-only measurement scope: Pragmatically justified but limits claims about full-paper rhetoric and how reviewers actually evaluate submissions, potentially explaining the null correlation with review scores\n\n- LLM judge validation incremental: 88.4% human agreement is strong but incremental given MT-Bench established GPT-4 achieves 80-85% alignment matching inter-human agreement (Zheng et al., 2023)"}, "questions": {"value": "1) LLM adoption vs. rhetorical intent. Can you directly compare your rhetorical scores against LLM-detection scores using existing frameworks (e.g., Liang et al., 2024)? Can you share the correlation and explain how your method captures the author's rhetorical choices beyond LLM usage patterns? This is essential to further establishing the framework's unique contribution.\n\n2) Temporal mechanism disentanglement. The post-2023 increase could reflect competitive pressure, LLM tool adoption nudging style, or both. Can you differentiate these mechanisms? For example, do papers with high LLM-adoption signatures show different rhetorical patterns than low-adoption papers in 2024-2025? This confounding threatens the paper's primary temporal finding.\n\n3) Reviewer correlation interpretation. The near-zero correlation with peer review deserves dedicated investigation. Have you examined: a) whether rhetorical style predicts review score variance across reviewers, b) full-paper rhetoric vs. abstract-only rhetoric, or c) non-linear relationships? Given your policy implications focus on peer review, this hypothesis finding should be explained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWXB9T4Scg", "forum": "fiohEI16sf", "replyto": "fiohEI16sf", "signatures": ["ICLR.cc/2026/Conference/Submission10275/Reviewer_WLet"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10275/Reviewer_WLet"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199934700, "cdate": 1762199934700, "tmdate": 1762960472615, "mdate": 1762960472615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}