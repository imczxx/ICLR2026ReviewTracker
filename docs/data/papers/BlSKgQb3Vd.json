{"id": "BlSKgQb3Vd", "number": 1119, "cdate": 1756843897779, "mdate": 1763643141955, "content": {"title": "ReLi3D: Relightable Multi-view 3D Reconstruction with Disentangled Illumination", "abstract": "Reconstructing 3D assets from images has long required separate pipelines for geometry reconstruction, material estimation, and illumination recovery, each with distinct limitations and computational overhead. We present MIDR-3D, the first unified end-to-end pipeline that simultaneously reconstructs complete 3D geometry, spatially-varying physically-based materials, and environment illumination\nfrom sparse multi-view images in under one second. Our key insight is that multi-view constraints can dramatically improve material and illumination disentanglement, a problem that remains fundamentally ill-posed for single image methods. Key to our approach is the fusion of the multi-view input via a transformer cross-conditioning architecture, followed by a novel unified two path prediction strategy. The first path predicts the object’s structure and appearance, while the second path predicts the environment illumination from image background or object reflections. This combined with a differentiable Monte Carlo multiple importance sampling renderer, creates an optimal illumination disentanglement training pipeline. Further with our mixed-domain training protocol, combining synthetic PBR datasets with real-world RGB captures, we establish generalizable results across geometry, material accuracy, and illumination quality. By unifying previously separate reconstruction tasks into a single feed-forward pass, we enable near-instantaneous generation of complete, relightable 3D assets.", "tldr": "Reconstructs geometry, PBR materials, and HDR environment lighting from a few views in sub-second time via cross-view fusion and a two-path illumination disentanglement design.", "keywords": ["Material", "Reconstruction", "Large Reconstruction Model", "Multi-View", "Illumination"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e4fbbef708619a78d4db2d70f84b0cdc790397a.pdf", "supplementary_material": "/attachment/f449f9b7fc4474d023d124e9b304f37f8062618a.zip"}, "replies": [{"content": {"summary": {"value": "ReLi3D is a feed-forward framework that takes a set of variably posed images as input and outputs a textured 3D mesh with PBR materials and an environment map. The model adopts a two-path architecture that separately learns geometry–material and environment–lighting representations, which are then fused through cross-attention layers. A differentiable renderer jointly renders outputs from both paths, enforcing material–illumination disentanglement and enabling image-space self-supervision. Extensive experiments demonstrate the superior performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Fast relightable 3D generation. The method can produce relightable 3D assets in under a second, which significantly enhances the practicality and usability of 3D generation systems.\n\n2. Self-supervised learning on real data. The two-path architecture, combined with a differentiable renderer, allows the model to be trained on real-world images via self-supervision—an important advancement over prior works (e.g., Hunyuan, Trellis) that rely mainly on synthetic datasets.\n\n3. Strong quantitative results. The approach achieves large improvements over baseline methods across multiple benchmarks."}, "weaknesses": {"value": "1. No comprehensive visual results are provided for real-world data beyond the single example shown in the teaser. Also, including quantitative material evaluations on Stanford ORB would help substantiate the method’s real-world performance.\n\n2. In the real-world example (the backpack in the teaser), noticeable artifacts appear in roughness and metallic predictions. Similarly, in Figure 2 (lamp case), the predicted high-frequency details appear blurry.\n\n3. The presented examples mostly involve objects with simple geometry and uniform materials. It would strengthen the paper to include results on more complex, multi-material objects with intricate geometry.\n\nI am happy to increase my score if the authors address these concerns."}, "questions": {"value": "How does the choice of the hero view $h$ affect the final reconstruction quality? What is the selection strategy for $h$ ?\n\nIn Table 2, why doesn’t the performance consistently improve as the number of input views increases, given that additional views should generally reduce the task difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yKj3j2QfWV", "forum": "BlSKgQb3Vd", "replyto": "BlSKgQb3Vd", "signatures": ["ICLR.cc/2026/Conference/Submission1119/Reviewer_7ZmD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1119/Reviewer_7ZmD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442533571, "cdate": 1761442533571, "tmdate": 1762915684500, "mdate": 1762915684500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We thank the reviewers for their constructive feedback and positive recognition of ReLi3D's contributions. Reviewers acknowledge that \"to my knowledge, the suggested approach is the first which jointly reconstructs mesh, PBR, and environment (HDR), all in a feedforward manner and at impressive speeds\" (FiCS), constituting \"a significant contribution\" (FiCS). ReLi3D is recognized as \"the first unified end-to-end system that reconstructs complete 3D geometry, spatially-varying materials, and environment illumination from sparse multi-view images in under one second\" (BeZw), with \"fast relightable 3D generation\" (7ZmD) that \"significantly enhances the practicality and usability of 3D generation systems\" (7ZmD).\n\nThe architectural design is appreciated: \"the two-path feed-forward framework jointly reconstructs geometry, materials, and illumination under multi-view constraints, showing a clear and coherent design\" (7UYe), and \"the idea of fusing arbitrary number of views with one 'hero' view and other views with latent mixing (what the authors call 'cross-view feature fusion') is novel and insightful\" (FiCS). The training approach using \"Monte Carlo integration with MIS for training supervision\" (7UYe) leads to \"more consistent and realistic reconstructions\" (7UYe), while \"self-supervised learning on real data\" (7ZmD) represents \"an important advancement over prior works\" (7ZmD).\n\nResults are described as \"convincing\" (FiCS), with \"strong quantitative results\" (7ZmD) showing \"large improvements over baseline methods across multiple benchmarks\" (7ZmD), and the \"writing is clear and easy to follow\" (BeZw).\n\n---\n\n## Important: Please Read the Rebuttal PDF\n\n**We strongly urge all reviewers to read the rebuttal PDF document rather than relying solely on the individual comment responses below.**\n\nWe have prepared a detailed rebuttal PDF that addresses all reviewer concerns with new experimental results, ablations, updated figures and tables, and detailed explanations. The rebuttal PDF can be found in the supplemental.zip file uploaded with this submission.\n\nThe individual comment responses below contain content copied from the rebuttal PDF for convenience, but the PDF format provides much better visual presentation of figures and tables, proper formatting, and complete context for all responses.\n\nWe have already transferred the rebuttal PDF contents into the main paper, including expanded real-world evaluation, comprehensive ablation studies, quantitative illumination evaluation, failure case analysis, improved figures, and detailed training protocol documentation in Appendix B.2.\n\nWe address all reviewers concerns and questions in detail in the rebuttal PDF, and we encourage reviewers to review that document for the complete account of our responses."}}, "id": "FnoVEzD6YI", "forum": "BlSKgQb3Vd", "replyto": "BlSKgQb3Vd", "signatures": ["ICLR.cc/2026/Conference/Submission1119/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1119/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1119/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643331188, "cdate": 1763643331188, "tmdate": 1763643331188, "mdate": 1763643331188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to reconstruct 3D assets with PBR materials + HDR envmaps from posed 2D images.\n\nMain components of the method include:\n- Efficient fusion of input views with cross-view feature fusion\n- spatially-varying material prediction - a common approach to predict materials with MLP, given triplane features (inspired by LGM)\n- separate branches for svBRDF (mesh + PBR) and HDR prediction\n- Training on mix of gt PBR materials and envmaps (where available) and in \"self-supervised\" manner with MC renderer and 2D supervision"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "To my knowledge, the suggested approach is the first which jointly reconstructs mesh, PBR, **and** environment (HDR), all in a feedforward manner and at impressive speeds. To me, this constitutes a significant contribution.\nAdditionally, the paper contains novel ideas (more on that below) and is clear and well-written.\n\n1. The idea of fusing arbitrary number of views with one \"hero\" view and other views with latent mixing (what the authors call \"cross-view feature fusion\") is novel and insightful.\n2. Authors' choice of decoding svBRDF with an MLP -> Flexicubes from triplane features is very common in industry, which I think is a well-suited and efficient selection for 3D representation.\n3. Both illumination reconstruction (Fig 4), PBR & relighting metrics (Table 1.) look convincing."}, "weaknesses": {"value": "1. The most important ablation is missing on whether to use multiple paths (one for geometry & appearance, another for illumination path) or compute them all in a single path. \n2. While the method was trained and inferred either on real-world or synthetic data, it would be valuable to see how it generalizes to generated (e.g. with diffusion / flow models) images. This might improve practicality of this approach.\n3. While 3D+Image metrics (Table 2) look convincing at first, qualitative results in Fig. 6 are hard to inspect."}, "questions": {"value": "1. \"Although rare, failure cases occur where the decomposition fails to disentangle lighting and materials, resulting in baked-in lighting affecting the material maps.\" - please provide visuals with failure cases\n2. See weakness 1 - please provide ablation on using multiple paths vs single path.\n3. Please clarify how the hero view is selected.\n4. L292 \"Critically, out training employs... background masking\" - please provide more details on this approach. Theoretically, this should work similarly to taking masked / not masked render with some probability, not sure why custom occlusion logic is needed here.\n5. See weakness 2 - although it's not critical please discuss or show at least a few 3D samples generated from images from diffusion / flow models.\n6. See weakness 3 - please consider improving the visuals in Table 2. to better evaluate the quality of this method.\n7. On choice of HDR prior (RENI++) - do you think the same framework could plug in alternative HDR priors (spherical harmonics / gaussians), did you experiment with any?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4n2DU8dslS", "forum": "BlSKgQb3Vd", "replyto": "BlSKgQb3Vd", "signatures": ["ICLR.cc/2026/Conference/Submission1119/Reviewer_FiCS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1119/Reviewer_FiCS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878314519, "cdate": 1761878314519, "tmdate": 1762915684372, "mdate": 1762915684372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReLi3D is the first unified end-to-end system that reconstructs complete 3D geometry, spatially-varying materials, and environment illumination from sparse multi-view images in under one second. By leveraging a transformer-based multi-view fusion and dual-path prediction architecture with differentiable rendering, it achieves fast, generalizable, and high-quality relightable 3D asset reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The writing is clear and easy to follow.\n2.The proposed pipeline is new and makes a meaningful contribution to the field."}, "weaknesses": {"value": "please see weakness for detail"}, "questions": {"value": "1. Regarding mesh reconstruction alignment:\n\nBased on my understanding, the meshes reconstructed by Huanyuan3D are represented in their own coordinate systems. Consequently, it may not be straightforward to obtain a direct correspondence with the input view — that is, the pose in which the input RGB view can be rendered. Under such a situation, it is unclear how the authors performed the image-based metric comparison in Table 2. The authors should clarify how they ensured pose alignment or view consistency before evaluating these metrics.\n\n2. Regarding the training protocol (Appendix B.2):\n\nAccording to Section B.2, the training process is divided into several stages. However, the current version of the paper does not clearly explain the details of each stage — for instance, which losses are used in each stage, which network components are trained or frozen. These details are crucial for reproducibility but are currently missing from both the main paper and the supplementary material. The authors should elaborate on these aspects and consider including a concise description of each training stage (objectives, losses, and training parts) in the main paper to facilitate re-implementation by readers.\n\n3. Regarding ablation studies:\n\nThe current version lacks sufficient ablation experiments. At present, only a single experiment in the supplementary material ablates the MC-Render component. A more comprehensive ablation study is needed to strengthen the paper. Specifically, the authors are encouraged to include:\n\nPerformance comparison after each training stage;\n\nAblation of the main architectural modules;\n\nAnalysis of how performance changes if geometry&materials and environment illumination are predicted using two independent networks rather than a unified one.\n\nIf these necessary ablation results can be provided in the rebuttal, I would be inclined to reconsider and potentially raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OvAWs7schd", "forum": "BlSKgQb3Vd", "replyto": "BlSKgQb3Vd", "signatures": ["ICLR.cc/2026/Conference/Submission1119/Reviewer_BeZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1119/Reviewer_BeZw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895436774, "cdate": 1761895436774, "tmdate": 1762915684266, "mdate": 1762915684266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReLi3D presents a feed-forward framework that reconstructs 3D geometry, spatially varying PBR materials, and HDR illumination from sparse multi-view images. It employs a two-path transformer architecture for joint material–lighting disentanglement, guided by a differentiable Monte Carlo renderer. Trained on mixed synthetic and real data, the model achieves fast (<1 s) and high-quality relightable 3D reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The two-path feed-forward framework jointly reconstructs geometry, materials, and illumination under multi-view constraints, showing a clear and coherent design.\n\n2. Uses Monte Carlo integration with MIS for training supervision, leading to more consistent and realistic reconstructions.\n\n3. The model runs efficiently and shows some degree of cross-domain generalization with mixed synthetic–real training."}, "weaknesses": {"value": "1. Limited evaluation diversity. The test data mostly covers diffuse or moderately lit objects. The paper lacks challenging cases such as metallic, transparent materials, or strong HDR illumination, where disentanglement performance would be most critical.\n\n2. Lack of illumination disentanglement evaluation. The paper does not provide quantitative evaluation of the predicted lighting quality (e.g., comparison against SPAR3D or DiffusionLight) or at least sufficient qualitative examples demonstrating the accuracy of recovered illumination.\n\n3. Potential dataset-specific bias. Since the real-world training data (UCO3D) includes only RGB supervision, the model might have learned dataset-specific entangled appearance cues rather than true material–lighting disentanglement on real data. More real-scene examples and an ablation showing the individual contribution of synthetic vs. real data would strengthen the claim of cross-domain generalization.\n\n4. Missing ablation for cross-view fusion. Cross-view fusion is one of the key claimed contributions, but there is no explicit ablation showing how it improves view consistency compared to models without this fusion module."}, "questions": {"value": "1. How is the hero view (line 231) chosen in multi-view fusion? Is it random or learned, and how sensitive is the model to this choice?\n\n2. Have the authors tested failure cases such as baked-in lighting or specular highlight misinterpretation? Could you show such examples or briefly discuss possible solutions and limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xufdgJy7as", "forum": "BlSKgQb3Vd", "replyto": "BlSKgQb3Vd", "signatures": ["ICLR.cc/2026/Conference/Submission1119/Reviewer_7UYe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1119/Reviewer_7UYe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916278529, "cdate": 1761916278529, "tmdate": 1762915684023, "mdate": 1762915684023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}