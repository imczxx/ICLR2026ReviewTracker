{"id": "cwxJJDS4vo", "number": 7950, "cdate": 1758045497169, "mdate": 1759897820231, "content": {"title": "Difference-aware Visiolinguistic Regularization for Image Change Captioning", "abstract": "Image Change Captioning (ICC) has emerged as an important task in multi-modal generative AI, aiming to generate natural language descriptions that  reflect the differences between two similar images. Unlike traditional image captioning, ICC requires strong cross-image difference reasoning and language generation capabilities to handle diverse and complex scenarios. Recent advances have introduced MLLM-based methods for ICC, achieving impressive results. However, these approaches rely solely on caption-level supervision to implicitly infer and describe changes, which often results in the omission of fine-grained differences and suboptimal caption quality. To address this, we propose a Difference-Aware Visiolinguistic Regularization (DAVIR) paradigm that jointly regularizes the fine-tuning of MLLM from both visual and linguistic perspectives, enabling better adaptation to  ICC. Specifically, we first introduce a fine-grained attention control  module to regularize the final-layer self-attention maps of the MLLM’s encoder, guiding it to focus on subtle changes during feature extraction. Second, we propose an entity prompt construction  scheme to guide the MLLM’s decoder and enhance caption generation quality. Extensive experiments on three benchmark datasets across different scenarios demonstrate that our method achieves state-of-the-art performance. The code will be released publicly.", "tldr": "We propose DAVIR which improves MLLM fine-tuning by guiding the encoder to focus on subtle visual changes and enhancing decoder caption generation via entity prompts, achieving state-of-the-art results across multiple benchmarks.", "keywords": ["Change Captioning", "Image Captioning", "Vision-Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/58391f4f49399a2b343cef97411c93edec16d8f9.pdf", "supplementary_material": "/attachment/74f9f5cd57793232428b249b80ef3edbb5abeb5d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DAVIR, a method for image change captioning using a vision-language model. DAVIR modifies InstructBLIP by adding two components: Fine-Grained Attention Control (FAC) and Entity Prompt Construction (EPC). FAC uses change masks from a segmentation model to regularize the encoder’s attention maps via KL divergence. EPC extracts noun phrases from training captions, matches them with visual regions using CLIP, and uses them to guide the decoder. \nDAVIR is evaluated on three datasets and shows improved performance over existing approaches. However, all experiments are conducted in supervised settings using InstructBLIP, and generalization to other models, tasks, or zero-shot scenarios remains untested."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Originality:\nThis paper does not introduce a new task or model architecture but combines known components—attention map supervision and entity-based prompting—in a structured way. Consequently, the novelty lies in the integration of these components into a unified training framework, rather than in the individual techniques themselves.\n\n## Quality\nThe curren paper is supported by extensive experiments across three datasets, with consistent metric improvements and ablation studies isolating the contributions of each module. \n\n## Clarity \nThis is maintained throughout the paper. The method is described with sufficient detail, and the figures and tables are well-organized. The rationale behind design choices is explained, and the experimental setup is reproducible based on the provided information.\n\n## Significance\nThis confined to the image change captioning domain. The method improves performance on existing benchmarks and addresses known limitations of prior approaches, such as hallucination and poor localization."}, "weaknesses": {"value": "## Reliance on Existing Components and Limited Novelty \nThe contribution relies heavily on existing components. The use of attention map supervision via KL divergence and entity extraction through CLIP are both established techniques. \nIts novelty lies in their combination, but similar ideas have been explored in prior works such as FINER-MLLM (Zhang et al., 2024), which also uses retrieved textual elements to guide decoding. \nThe current paper fails to sufficiently differentiate its approach from these methods in terms of conceptual innovation or architectural design.\n\n## Limited Evaluation Scope and Practicality\nThe experiments are limited to three datasets within the same task domain. All evaluations are conducted in fully supervised settings using ground-truth captions for entity extraction. This leaves critical questions about the method’s robustness in zero-shot or low-resource scenarios. \nThe reliance on ground-truth captions for EPC also severely restricts applicability to real-world settings where such annotations may not be available.\nThis scope is narrow, and the applicability to other tasks or models is not explored, which significantly limits broader impact.\nThe current evaluation is limited to supervised settings and a single model architecture, leaving questions about generalization and robustness unanswered.\n\n## Tight Architectural Coupling\nThis method is tightly coupled to the InstructBLIP architecture. FAC depends on the structure of ViT attention maps, and EPC assumes the presence of a Q-Former and a decoder that can accept structured prompts. There is no evidence that the approach generalizes to other MLLMs or vision-language frameworks, which significantly limits its broader relevance.\n\n## Lack of Computational Analysis\nThis paper does not address computational efficiency. FAC involves segmentation with SAM and KL-based supervision, while EPC requires CLIP-based retrieval. These steps may introduce latency or resource overhead, but the paper does not report inference time, memory usage, or scalability metrics.\n\n## Unanalyzed Robustness of Entity Prompting \nThe entity prompting mechanism assumes that CLIP retrieval yields semantically relevant phrases. \nThe quality and stability of these entities are not analyzed. There is no evaluation of how mismatched or noisy entities affect caption generation, nor is there a fallback strategy when retrieval fails."}, "questions": {"value": "**Q1 How transferable is the proposed method to other multi-modal models beyond InstructBLIP?** This paper does not discuss whether FAC and EPC can be applied to architectures without Q-Former or with different encoder-decoder interfaces. Clarifying this would help assess the generality of the approach.\n\n**Q2 What happens when ground-truth captions are unavailable or noisy?** EPC depends on training captions to extract noun phrases. It is unclear that authors considered unsupervised or retrieval-based alternatives, and how the model performs in low-resource or zero-shot settings.\n\n**Q3: How sensitive is the method to segmentation quality?** FAC relies on SAM-generated masks, but this paper does not evaluate the impact of inaccurate or noisy masks. An experiment showing performance degradation under mask perturbation would help assess robustness.\n\n**Q4: What is the computational cost of the full pipeline?** The method involves segmentation, attention supervision, and CLIP-based entity retrieval. The paper does not report inference time, memory usage, or scalability. Including these metrics would help evaluate practical feasibility.\n\n**Q5: How reliable is the entity prompting mechanism?** The paper assumes that CLIP retrieval yields semantically relevant entities, but does not analyze failure cases or provide statistics on retrieval accuracy. A breakdown of entity relevance and its effect on caption quality would clarify the stability of EPC.\n\n**Q6: Why does performance degrade when using multiple entities in prompts?** The ablation shows that one entity performs best, but the underlying reason is not explained. A qualitative analysis of examples with conflicting or noisy entities could help clarify this behavior.\n\n**Q7:How does the method compare conceptually and empirically to FINER-MLLM?** Both approaches use textual elements to guide decoding. A more detailed comparison of design choices, assumptions, and performance trade-offs would help distinguish the contribution.\n\n**Q8:Is the method applicable to tasks beyond image change captioning?** This paper focuses exclusively on ICC, but the components may be relevant to other tasks involving visual difference reasoning. Clarifying whether the authors considered such extensions would help assess broader significance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vZhu6c2MaY", "forum": "cwxJJDS4vo", "replyto": "cwxJJDS4vo", "signatures": ["ICLR.cc/2026/Conference/Submission7950/Reviewer_ad1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7950/Reviewer_ad1p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376112913, "cdate": 1761376112913, "tmdate": 1762919967636, "mdate": 1762919967636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the Image Change Captioning (ICC) task by proposing DAVIR, a visiolinguistic regularization framework designed to enhance Multi-modal LLMs. DAVIR introduces two key modules: (1) Fine-Grained Attention Control (FAC) to align encoder attention with change masks, and (2) Entity Prompt Construction (EPC) to guide the decoder with relevant textual entities. Experimental results on three benchmarks demonstrate consistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe problem this paper focuses on is reasonable and worth exploring.\n2.\tExperiments on three diverse datasets demonstrate consistent improvements."}, "weaknesses": {"value": "1.\tChange Mask Extraction. The method generates change regions via pixel-wise subtraction and subsequent clustering. This approach, however, is plagued by pseudo-changes (e.g., viewpoint or illumination variations).\n2.\tEntity Prompt Construction may inadvertently worsen hallucinations. While designed to mitigate them, if CLIP misidentifies entities, it can induce or reinforce hallucinations in the LLM. Furthermore, CLIP itself can exhibit more hallucinations than powerful MLLMs like InstructBLIP, making it unclear whether this component is actually beneficial."}, "questions": {"value": "1.\tEffectiveness of Change Mask Extraction on CLEVR-DC"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lif8tuttuP", "forum": "cwxJJDS4vo", "replyto": "cwxJJDS4vo", "signatures": ["ICLR.cc/2026/Conference/Submission7950/Reviewer_BEpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7950/Reviewer_BEpt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907228394, "cdate": 1761907228394, "tmdate": 1762919967315, "mdate": 1762919967315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on the task of image change captioning. Existing methods rely solely on caption-level supervision, a limitation that may lead to the oversight of fine-grained changes. To address this, the authors propose a difference-aware vision-language regularization framework, which regularizes the MLLM using visual change masks and textual change entities. Specifically, their proposed fine-grained attention control module extracts change masks for each image; these masks are then used to regularize the self-attention maps of the MLLM’s encoder. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method incorporates visual change masks to regularize the fine-tuning of the MLLM’s encoder and capture subtle changes.\n2. The entity-prompt construction scheme is designed to regularize the MLLM’s decoder, thereby enabling more accurate image change caption generation.\n3. Experimental results demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The derivation of change masks relies primarily on pixel-level discrepancies, but there are cases where the views of the two images are not closely aligned. In such scenarios, the derived change masks may be inaccurate, thereby impacting subsequent feature extraction and change caption generation.\n2. In the entity-guided prompting strategy, the extracted entities largely rely on the SAM model, which may perform poorly in scenarios where SAM fails to detect objects successfully."}, "questions": {"value": "As listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VOhrF4i8M8", "forum": "cwxJJDS4vo", "replyto": "cwxJJDS4vo", "signatures": ["ICLR.cc/2026/Conference/Submission7950/Reviewer_CUgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7950/Reviewer_CUgz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001676484, "cdate": 1762001676484, "tmdate": 1762919966939, "mdate": 1762919966939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework built upon InstructBLIP to enhance the description of image changes. The main contributions are two techniques: Fine-grained Attention Control (FAC) and Entity Prompt Construction (EPC). FAC directs the model’s attention to the regions where changes occur, while EPC improves the quality of change-aware caption generation. The proposed method demonstrates strong performance across three benchmark datasets, and the authors conduct comprehensive ablation studies to analyze the contribution of each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper is clearly stated.\n2. The authors evaluate their method on multiple datasets, ensuring a comprehensive comparison.\n3. Across all datasets, the proposed approach consistently outperforms or matches existing baselines."}, "weaknesses": {"value": "1. In the ablation study, the performance trend is not consistent. For example, in Table 4, the configuration using both FAC and EPC does not achieve the best results. This suggests that the two proposed components might have conflicting effects.\n2. The paper lacks analysis or explanation for the performance variations observed in Table 4, which makes it difficult to understand the interaction between FAC and EPC.\n3. The selected baselines are somewhat outdated. Recent models such as Qwen-VL and InternVL have demonstrated strong performance on image change captioning (ICC) tasks. Including results from these frontier open-source models would make the comparison more comprehensive.\n4. The ablation study focuses mainly on the Spot-the-Diff dataset, which primarily contains object-level changes. This dataset alone is not representative enough to fully evaluate the generalizability of the proposed approach.\n5. A failure case analysis is missing. Providing examples of where the method fails could offer valuable insights into its limitations and potential areas for improvement."}, "questions": {"value": "The paper only provides examples involving object-level changes. It remains unclear how the proposed method would handle non-object changes, such as color or style transformations (e.g., making the entire image hue red). It would be helpful if the authors could include such examples or discuss how their approach generalizes to these types of changes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "17LCvOSy6Q", "forum": "cwxJJDS4vo", "replyto": "cwxJJDS4vo", "signatures": ["ICLR.cc/2026/Conference/Submission7950/Reviewer_Hdfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7950/Reviewer_Hdfj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7950/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158583334, "cdate": 1762158583334, "tmdate": 1762919966513, "mdate": 1762919966513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}