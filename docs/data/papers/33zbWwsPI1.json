{"id": "33zbWwsPI1", "number": 24263, "cdate": 1758354710032, "mdate": 1763363379884, "content": {"title": "BrainMIND: Interpret Fine-grained Spatial Mapping of Brain Activity to Multi-semantic Concepts", "abstract": "Understanding how population-coding in the human visual cortex shape high-level semantic representations remains a significant challenge. Prior work has either focused on region-level text decoding or relied on simple linear models to probe single-semantic decoding at the voxel level. Consequently, systematic exploration of semantic diversity remains limited at both the region level and the fine-grained voxel level. To address this gap, we introduce BrainMIND, a data-driven framework for analyzing multi-concept semantic selectivity in the visual cortex. We use a conditional variational autoencoder (CVAE) whose latent space is constrained by brain data and spatial locations of voxels. The CVAE decodes the structured latent space into CLIP-aligned semantic embeddings, which then condition a fine-tuned large language model to generate interpretable captions. We validate BrainMIND on widely recognized cortical regions, demonstrating interpretable region-level and voxel-level semantic selectivity. We reveal that individual voxels exhibit mixed selectivity across multiple semantic dimensions, and filling a key gap in voxel-wise neural decoding. Our results demonstrate that BrainMIND provides an interpretable bridge from brain regions to their constituent voxels, enabling controlled, fine-grained exploration of semantic organization in the higher visual cortex.", "tldr": "We provide an interpret fine-grained spatial mapping of brain activity to multi-semantic concepts, and explore semantic organization in the higher visual cortex.", "keywords": ["computational neuroscience", "visual cortex", "fmri", "mixselectivity", "brain map", "brain decoding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c7782d9e128400b10d7ddfb4a07588209d210ee2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes BrainMIND, a conditional variational autoencoder (CVAE) with a dynamic mixture-of-Gaussians prior, conditioned on both fMRI voxel positions and brain responses, to decode multi-semantic representations from the Natural Scenes Dataset (NSD). The decoded latent features are mapped into CLIP space and then converted to natural language via a fine-tuned large language model (LLM). The authors claim this approach reveals mixed selectivity at both the region and voxel levels and provides interpretable multi-semantic mappings across the visual cortex."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-The work addresses a meaningful and underexplored problem — interpretable, voxel-level, multi-semantic decoding rather than simple reconstruction or region-level prediction.\n\n-Authors connect findings to established cortical selectivity patterns (FFA, PPA, VWFA) and report consistency with known priors, suggesting neuroscientific validity.\n\n-The paper promises open code and weights, and the ethics section is solid and transparent."}, "weaknesses": {"value": "generally the text is poorly written, with some mistakes and lot of AI generated content. This is not a problem per-se but please double check the whole text and especially the citations. Many of them are kind of wrong with maybe the right author but incorrect titles or the other way around.\n\nFurthermore, the paper oscillates between semantic reconstruction and interpretability. It’s unclear whether BrainMIND is intended as a decoding model (predicting content from brain data) or as a representational analysis tool. If it is intended as a decoding model lot of comparison is missing with existent literature. There is only one comparison with BrainSCUBA, with limited improvements.\n\nThe paper completely lack evaluation, do we need the router? What is the impact?\n\nAlso I'm not fully convinced by the math presented in the paper. For example, the gating is not-differentiable, but there are other potential flaws."}, "questions": {"value": "Honestly I think the paper should be improved in several directions:\n\n- Improved text for clarity and readability\n- Improved explanation of what is the research question here, and why (and how) the method proposed is solving the question\n- Clear math\n- Fair comparison with prior work (properly cited)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T2aIpbkKf5", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Reviewer_oNbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24263/Reviewer_oNbE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539969112, "cdate": 1760539969112, "tmdate": 1762943022223, "mdate": 1762943022223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "yQCjjpm8Ii", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763363379133, "cdate": 1763363379133, "tmdate": 1763363379133, "mdate": 1763363379133, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BrainMIND, a voxel-level fMRI decoding framework for interpretable brain-to-text reconstruction. Voxel-wise neural decoding is an important problem for understanding the relationship between human brain representations and latent embeddings of intelligent models. However, the manuscript contains several serious issues that undermine confidence in the work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Exploring fine-grained mapping between brain activity patterns and AI model representations is a meaningful direction in bridging neuroscience and AI."}, "weaknesses": {"value": "* The bibliography contains multiple citation errors. For example, several references list non-existent or placeholder author names such as [1–2], and some cited works cannot be traced to public sources or published papers (e.g., [2]). \n\n  [1] Aoxiao Luo, John D Smith, and Jane Doe. Brain diffusion for visual exploration: Cortical discovery using large scale generative models. arXiv preprint arXiv:2306.03089, 2023.\n\n  [2] Yujia Wang, John D Smith, and Jane Doe. Incorporating clip into brain decoding: Zero-shot learning for fmri analysis. NeuroImage, 250:118956, 2022.\n\n* Another major concern to me is the lack of comparison to existing work on the topic of fMRI-to-caption decoding. While the paper compares against BrainSCUBA, the brain-to-text decoding field already contains numerous advanced methods such as [1-3].\n\n  [1] Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction. NeurIPS 2024.\n\n  [2] Exploring the Visual Feature Space for Multimodal Neural Decoding. ICCV 2025.\n\n  [3] Bridging the Gap between Brain and Machine in Interpreting Visual Semantics: Towards Self-adaptive Brain-to-Text Decoding, ICCV2025.\n\n* The current methods lack crucial details for understanding."}, "questions": {"value": "Implementation details are missing, such as the description and configuration of comparison methods, the choice of LLM, raising questions about reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kH2SFVHkAq", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Reviewer_L1We"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24263/Reviewer_L1We"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761285861177, "cdate": 1761285861177, "tmdate": 1762943022014, "mdate": 1762943022014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BrainMIND, a position‑conditioned CVAE with a dynamic mixture prior that decodes semantics embeddings and uses LLMs to produce natural‑language semantics at both ROI and voxel levels. I found that  the submission raises serious Ethical & Reproducibility Concerns. There are multiple placeholder‑style or internally inconsistent references. These issues impede a fair scientific assessment at this time."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n.a."}, "weaknesses": {"value": "n.a."}, "questions": {"value": "n.a."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The References contain repeated placeholder‑style author names, across multiple entries—e.g., Ferrante et al., 2023 (p. 10), Luo et al., 2023 (p. 11), Wang et al., 2022 (p. 12). There is also an underspecified citation “Clip‑decoding … In NeurIPS 2023” with minimal metadata (p. 10), which I cannot find the original one. There are also instances where completely unrelated papers are miscited. For example, “Seeing through things: Exploring the design space of privacy‑aware data‑enabled objects.” (p. 10) appears misaligned with the brain‑decoding topic and is likely mis‑cited within this context.\nSuch patterns are red flags for reference accuracy and raise concerns about automated/AI‑assisted generation. Moreover, these issues undermine the credibility of the manuscript and its experiments, and the inclusion of non‑existent citations makes peer review difficult."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "reGXyKAjoy", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Reviewer_p5of"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24263/Reviewer_p5of"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822342062, "cdate": 1761822342062, "tmdate": 1762943021833, "mdate": 1762943021833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a way to generate words from voxel-wise selectivity. The task is not new, and this paper contains numerous issues with AI generated citations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Investigating the selectivity in higher visual cortex is an interesting problem."}, "weaknesses": {"value": "I have serious serious concerns regarding this paper.\n\nThere are numerous citations which are totally made up and reference non-existent papers:\n* Yujia Wang, John D Smith, and Jane Doe. Incorporating clip into brain decoding: Zero-shot learning for fmri analysis. NeuroImage, 250:118956, 2022.\n* Aoxiao Luo, John D Smith, and Jane Doe. Brain diffusion for visual exploration: Cortical discovery using large scale generative models. arXiv preprint arXiv:2306.03089, 2023.\n* Enrico Ferrante, John D Smith, and Jane Doe. Brain captioning: Decoding human brain activity into images and text. arXiv preprint arXiv:2305.11560, 2023.\n* Justin Giles, Andrew Luo, and Leyla Isik. Clip-decoding: A generalist brain decoder for reconstructing arbitrary image-caption pairs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\n\nSecond, by the commonly accepted definitions of encoding and decoding by Thomas Naselaris, this work does not perform decoding, yet says `Our framework achieves multi-semantic and position-aware decoding at both the coarse ROI scale and the fine-grained voxel level`. This work is **investigating selectivity** not decoding. This sentence shows that the authors are seemingly unaware of the question they are investigating. \n\nThird, the figures are very strange. There are no cortical flat-maps or inflated maps, instead the authors seemingly plot out all figures using voxel positions. Which may change depending on the reference frame the MRI was transformed into. Note that this may vary from subject to subject. \n\nFourth, the authors at no point in the paper describe how the words for each voxel are generated."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "Numerous AI generated citations."}}, "id": "gNjFjQinKn", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Reviewer_txyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24263/Reviewer_txyh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981789637, "cdate": 1761981789637, "tmdate": 1762943021616, "mdate": 1762943021616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Apology for incorrect references and clarification for authenticity of main research content"}, "comment": {"value": "Dear PC, AC, and Reviewers,\n\nWe sincerely apologize for the incorrect literature references, as disclosed in the check list, we only intend to use LLMs to assist and polish writing, while we improperly use these tools in searching and formatting a few references. We take full responsibility for not carefully validating the sources of their references.  We have updated the PDF accordingly, with incorrect references replaced, and also incorporated additional feedbacks as reviewers suggested. Meanwhile, we want to confirm that the main body of the research work, including the idea, methodology, experiments, and reported results, is entirely based on authentic and reproducible work, without any fabricated data. The full source code can be provided for further verification and reproducibility. We will withdraw our paper and offer our sincere apologies for this oversight. We will take greater care and will strictly follow any submission guidelines in our future work."}}, "id": "1PyTrZNS7B", "forum": "33zbWwsPI1", "replyto": "33zbWwsPI1", "signatures": ["ICLR.cc/2026/Conference/Submission24263/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24263/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24263/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763005384939, "cdate": 1763005384939, "tmdate": 1763005384939, "mdate": 1763005384939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}