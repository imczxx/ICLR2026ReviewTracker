{"id": "6Wkh0mwzwH", "number": 13554, "cdate": 1758219167887, "mdate": 1763603064258, "content": {"title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback", "abstract": "The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback---only a fraction of samples get rewards labeled---a fundamental question arises: *which* samples should be labeled to maximize policy performance?\nWe formalize this problem of *reward selection* for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.", "tldr": "", "keywords": ["Reward Selection", "Reinforcement Learning", "Learning from Limited Feedback"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e145aee7824ca155ff3fd699212474e7c038e24.pdf", "supplementary_material": "/attachment/ae8a14ad195cd3ccf306f273a2e44d3536b22dae.zip"}, "replies": [{"content": {"summary": {"value": "The authors tackle the problem of reward selection in reinforcement learning from limited feedback within the context of Markov Decision Processes with discrete state and action spaces. They focus on an offline setting, where a static dataset of tuples (state, action, next state) is available, but no reward labels. Under the assumption that obtaining labels (e.g., from human feedback) involves a significant cost, the question arises which samples should receive a reward label to obtain the best possible policy. The authors translate this to the problem of selecting the subset of states of a given cardinality that yields the best policy. After formalizing this task, they suggest both heuristics and learning-based solution approaches, which are either iterative or choose all relevant states at the same time. The approaches are compared on several prototypical and four MinAtar domains. Finally, the authors present some insights on patterns that evolve in all successful state selection strategies."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The problem presented is interesting, and the formalization of the optimal state selection independent of questions of state reachability and exploration seems unexplored, while relevant.\n- The heuristics and training-based approaches present interesting starting points for solving the presented problem. Evaluation of the approaches is overall well done and statistically significant. \n- The observed patterns in state selection provide valuable insights for devising task-specific heuristics or new selection algorithms."}, "weaknesses": {"value": "- The proposed problem formalization and solution approaches are mainly devised for discrete state spaces. Many of the presented motivational use cases involve continuous state spaces. Either the motivation or the methodology should be improved. Also, problem complexity will increase further for continuous action spaces. It would be valuable to discuss this at some point in the paper. \n- Presentation could be improved by making figures 4 and 5 larger. Figure 4 is not really discussed in the text and it does not become clear why it cannot simply be integrated with Table 1. In Figure 3, there seems to be come mistake, as a wall clock time of 1e9 days or higher seems rather unlikely."}, "questions": {"value": "- In Eq. (4), do you mean $\\hat{d}^{\\pi_d}_\\text{prev}$?\n- In Fig. (4), why are the on-policy variants of the 'visitation' and 'guided' heuristic not presented?\n- Are any of the proposed solution approaches applicable to continuous state spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6J2uZ49b5D", "forum": "6Wkh0mwzwH", "replyto": "6Wkh0mwzwH", "signatures": ["ICLR.cc/2026/Conference/Submission13554/Reviewer_pvN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13554/Reviewer_pvN8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511015085, "cdate": 1761511015085, "tmdate": 1762924153976, "mdate": 1762924153976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes a new problem setting called Reward Selection for Reinforcement Learning under Limited Feedback (RLLF). The core idea is that rewards are expensive to obtain in many real-world applications such as RLHF for large language models or molecule discovery. Therefore, instead of assuming all states have labeled rewards, the authors consider a constrained setting where only a limited number of states can be reward-labeled.\nThe key research question is: Which subset of states should be labeled with rewards to maximize the performance of the resulting policy?\nThe paper proposes both heuristic (training-free) and training-based reward selection strategies. Experiments include simple synthetic scenarios and larger benchmarks (MinAtar)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The concept of \"reward selection\" is good, and fills a gap between active RL (which couples exploration and reward querying) and active reward modeling (which focuses on reward estimation rather than policy performance).\n\n2. The authors argue the importance of limited-feedback RL in settings like RLHF, where feedback cost dominates computational cost. Furthermore, the experiments are well-organized and multi-scale. The inclusion of both synthetic toy domains and more realistic environments (MinAtar) helps generalize findings."}, "weaknesses": {"value": "1. The work is entirely empirical. There is no formal analysis of why certain reward selections yield near-optimal policies, or whether reward selection admits approximation guarantees (e.g., submodularity, monotonicity). Without such theory, claims of near-optimality remain heuristic.\n\n2. The training-phase setup assumes access to an \"oracle evaluator\", that can compute policy returns under the true reward function. This is a strong and somewhat impractical assumption, especially in RLHF or drug discovery, where the true reward is unknown and expensive.\n\n3. The offline assumption avoids the exploration problem, but the datasets still seem to come from a \"behavior policy.\" The paper does not clarify how sensitive reward selection is to dataset coverage or distribution shift."}, "questions": {"value": "1. Can the authors formalize conditions (e.g., approximate submodularity) under which the sequential-greedy method is guaranteed to achieve a bounded approximation to the optimal reward selection?  The notion of \"which rewards matter\" remains qualitative. What precisely defines a useful state, e.g., maximal marginal improvement in return, uncertainty reduction, or information gain? Without a formal criterion, the selection strategies risk degenerating into heuristic subset search. A theoretical grounding (e.g., influence-function or information-theoretic analysis) would make the contribution more solid.  \n\n2. How does this differ from Active Preference-Based Reward Learning?\n\n3. The paper treats unlabeled samples as having zero (or minimal) rewards, following the UDS baseline. However, if high-value states are systematically unlabeled, this assumption introduces a structural bias in the Bellman backups and may lead to overly conservative policies.\n\n4. Could reward selection be cast as an information gain problem over state-action pairs?\n\n5. How realistic is the assumption of an evaluator providing exact policy returns?\n\n6. In RLHF, one cannot compute expected returns directly, only obtain relative preference judgments. How would the framework adapt if evaluator feedback is pairwise, noisy, or delayed?\n\n7. For large-scale LLM alignment, the state space is on the order of 10e+10 - 10e+12. How would the proposed strategies (especially sequential-greedy or ES) scale?\n\n8. Could approximate embeddings or surrogate models be used to guide reward selection?\n\n9. Since the authors use the UDS algorithm that imputes missing rewards as zeros, does this create bias in Q-function estimation?\n\n10. The results show strong domain dependence of heuristics. Could the authors quantify what structural properties (e.g., reward sparsity, connectivity, stochasticity) determine which heuristic is optimal?\n\n11. The reward selection is formulated as a static one-shot decision over an offline dataset. In practice, however, labeling decisions are often adaptive, which means the set of informative states depends on the evolving policy. How would the proposed framework extend to an iterative or online selection process? Would previously \"unhelpful\" states become valuable after the policy improves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aZqBQWhz1q", "forum": "6Wkh0mwzwH", "replyto": "6Wkh0mwzwH", "signatures": ["ICLR.cc/2026/Conference/Submission13554/Reviewer_XCoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13554/Reviewer_XCoh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542591820, "cdate": 1761542591820, "tmdate": 1762924153120, "mdate": 1762924153120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defines a problem in offline reinforcement learning: given a fixed dataset, how can one select a subset to label with rewards so that the resulting policy achieves the best performance? The authors explore several data selection strategies and evaluate their feasibility, performance, and computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a common practical issue: selecting a useful subset from a large dataset for training.\n2. The problem formulation is clearly defined with mathematical expressions.\n3. The paper is well organized and easy to follow."}, "weaknesses": {"value": "1. The problem setting is straightforward, and the formalization is rather direct. Although the paper discusses brute-force, sequential-greedy, and evolutionary strategies, these approaches are standard and lack conceptual novelty. The work reads more like a comparative survey than a contribution of a new algorithmic idea.\n2. The motivation centers on high labeling-cost scenarios such as LLMs or drug discovery, yet all experiments are conducted on standard RL environments where labeling is cheap. This weakens the paper’s claim that it addresses real high-cost labeling settings."}, "questions": {"value": "1. How exactly does the sequential-greedy strategy select the (i)-th new state? Since each step requires maximizing $\\Delta(s | S_{[b]})$, does this imply comparing all candidates in $D - S_{i-1}$?\n2. For the three training-phase strategies, is the evaluation cost prohibitively high? Comparing two subsets $S_{[B]1}$  and $S_{[B]2}$ requires training a full offline RL policy, which could be extremely expensive in domains like LLMs. The cost of policy training might dominate the cost of data selection itself.\n3. Why was the UDS algorithm (Yu et al., 2022) chosen as the offline RL backbone? It is not SOTA. Using a stronger recent ORL algorithm might change conclusions, especially under low feedback ratios. The observed performance gaps may stem from the baseline’s limitations rather than the selected data. In other words, maybe with different selected labeled data, a stronger ORL algorithm can achieve similar performance?\n4. How does the proposed framework handle sparse-reward settings? When many states have originally zero rewards, selecting such states is ambiguous, i.e., are they effectively “selected” or not? Moreover, many modern ORL algorithms already handle sparse rewards well, so it is unclear whether reward selection still offers additional benefits in these cases.\n5. The paper title and term “reward selection” (and “Which reward matters?”) may be misleading. The problem focuses on **data selection**, not on choosing among reward functions. A more precise term might improve clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fo9IyFiVJx", "forum": "6Wkh0mwzwH", "replyto": "6Wkh0mwzwH", "signatures": ["ICLR.cc/2026/Conference/Submission13554/Reviewer_DKrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13554/Reviewer_DKrd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914137915, "cdate": 1761914137915, "tmdate": 1762924152723, "mdate": 1762924152723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes the problem of Reward Selection for Reinforcement Learning from Limited Feedback (RLLF). The paper asks: given a fixed budget B for reward labels, which subset of states from an offline dataset should be labeled to maximize the performance of the final policy?\n\nThe authors use an offline RL setup. A reward selection strategy $\\mathcal{Q}^{(B)}$ selects a state subset to be labeled. A policy is then learned from the resulting partially-labeled dataset. The paper investigates two categories of strategies (1) heuristic strategies (no training): and (2) strategies with a training phase\n\nExperiments show that heuristic performance is domain-dependent. The authors conclude by identifying structural patterns of optimal state sets, such as prioritizing \"anchor\" states on optimal paths and critical penalty states."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper formalizes the offline RLLF problem\n- The problem is well-motivated by high-impact, real-world applications where feedback is the primary bottleneck\n- The paper has a very thorough experimental validation"}, "weaknesses": {"value": "- It is not entirely clear how the on-policy methods compute the state-visitation distributions. Could the author clarify if they assume the full transition dynamics to be known in order to compute the visitations?\n- The \"training phase\" seems to be quite computationally expensive, given that for every additional labelled feedback, it requires retraining a policy. Could the authors elaborate on this point and the justification for this cost?\n- While it is true that the cost analysis for EM does not depend on S and B, I found it slightly confusing to present only k and m as parameters chosen by the user. Optimal k and m are probably still functions of S and B and the evolutionary strategy used to update $\\theta$. Is there any way to relate k and m with S and B? This would give a more direct comparison."}, "questions": {"value": "See Weaknesses\n\nAdditionally, could the authors please clarify how the state-visitation distribution $d^{\\pi_{[b-1]}}$ for the updated policy is estimated from a fixed offline dataset? Does this approach assume access to a model of the dynamics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cETwjRzbw3", "forum": "6Wkh0mwzwH", "replyto": "6Wkh0mwzwH", "signatures": ["ICLR.cc/2026/Conference/Submission13554/Reviewer_QqrC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13554/Reviewer_QqrC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976175253, "cdate": 1761976175253, "tmdate": 1762924152034, "mdate": 1762924152034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}