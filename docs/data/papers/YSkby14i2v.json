{"id": "YSkby14i2v", "number": 5120, "cdate": 1757851299475, "mdate": 1759897993429, "content": {"title": "MR.PEA: The Meta-Reasoning Prompt Engineering Agent", "abstract": "Prompt optimization is critical for maximizing the performance of large language models (LLMs). However, it often relies on costly labeled data. Self-supervised methods reduce data dependency, but they suffer from optimization ambiguity or high computational costs.\nTo address these limitations, we propose the Meta-Reasoning Prompt Engineering Agent (MR.PEA), a self-supervised prompt optimization framework that operates with minimal input. MR.PEA leverages meta-reasoning to iteratively build task-specific knowledge, including problem-solving strategies and evaluation criteria, while adaptively retrieving external information to enhance its understanding. This knowledge guides the generation of diverse validation examples, targeted prompt refinement, and comprehensive quality assessments. \nExperiments on GSM8K and Big-Bench Hard show that MR.PEA outperforms existing baselines, achieving an average performance gain of 7.4% with an optimization cost as low as $0.01 per task.", "tldr": "", "keywords": ["prompt engineering", "prompt optimization", "large language model", "self-supervised"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ccbb7aff8d913aec3bb4f23dea74018cca2d9a3.pdf", "supplementary_material": "/attachment/bf487f4649ec799658e8306d0e9a0bb3fe83a372.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a prompt optimization framework (MR.PEA) featuring a meta-reasoning approach that builds task-specific knowledge for prompt optimization. The method has 5 main modules which covers various aspects and challenges in prompt optimization.\nThe method achieves strong empirical results on GSM8k and BIG-bench hard tasks.\nFurther the method uses fewer tokens and thus saves the optimization cost compared to other methods. \nThe paper also conducts ablation study to justify each components in the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Realistic problem setting. Prompt optimization with limited data under budget constraint is a realistic problem with a lot of use cases.\n* Comprehensive experiments. The authors mentioned reimplementing the baseline methods to ensure fair comparison, which is a lot of efforts and should be encouraged. \n* Comprehensive ablation. The authors conducted detailed ablation on each module of the proposed method."}, "weaknesses": {"value": "* Unclear motivation for using meta-reasoning: The introduction begins by introducing limitations of prior methods, but it's unclear why would meta-reasoning address these limitations (e.g., reliance on labeled data, high computation costs). I still see gaps between these limitations and the proposed method.\n* Unclear distinction between \"meta-reasoning\" and \"reasoning\". The authors defined \"meta-reasoning\" as \"the ability to reflect on and improve one's reasoning process\". Is this mainly referring to phase 1 in your approach? What makes it \"meta\"? Which part of the reasoning is improved after applying your method?"}, "questions": {"value": "* Are all 5 phases in the method newly proposed in this paper, or were some of them used before in prior works? I recommend providing a table to highlight the distinctions from prior works.\n* How are supervised prompt optimization methods used in Table 1? Do you apply them directly in the self-supervised setting with only one training example?\n* Since MR.PEA introduces a new meta-reasoning centric module compared to prior work, I'm unsure why MR.PEA is cost-effective compared to the baseline methods. Could you please further clarify? Why are fewer tokens needed comparing to the baselines?\n* Do you control the prompt lengths in your experiments? In the example in figure 2, it seems that the prompt generated by MR.PEA is significantly longer than the other two, which could give this method some advantages.\n* I'm a bit confused at the terminology \"meta-reasoning knowledge\"/\"knowledge\" (e.g., line 198/419). If it's referring to strategies like \"break down complex problems into smaller, manageable steps\", this seems to be part of the final prompt of GSM8k (\"Break it into\nclear, sequential steps...\"). How is curating knowledge (the problem solving strategy part) different from writing the final prompt?\n\n* I feel the following work is highly related. Could the authors explain the relevance and differences with this work? https://proceedings.neurips.cc/paper_files/paper/2024/hash/e41efb03e20ca3c231940a3c6917ef6f-Abstract-Conference.html"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nxwWZ4XeoK", "forum": "YSkby14i2v", "replyto": "YSkby14i2v", "signatures": ["ICLR.cc/2026/Conference/Submission5120/Reviewer_XDMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5120/Reviewer_XDMT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761436433081, "cdate": 1761436433081, "tmdate": 1762917892972, "mdate": 1762917892972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MR.PEA, a self-supervised framework for prompt optimization that uses meta-reasoning to build task-specific strategies and evaluation criteria from minimal input. It iteratively refines prompts through knowledge curation, example generation, evaluation, and ranking. Results on GSM8K and Big-Bench Hard show clear gains over existing methods at very low cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Conceptually interesting meta-reasoning approach with minimal supervision and input requirements.\n- Well-structured and modular iterative framework.\n- Strong empirical results especially on Math tasks.\n- Clear ablation results showing each component’s contribution.\n- Low cost and scalability demonstrated."}, "weaknesses": {"value": "- Internal decision logic remains heuristic and not quantitatively validated.\n- Evaluation limited to GPT-4.1-nano with temperature of 0.2, leaving generalization and robustness untested.\n- Comparisons (Table 1) may be partially unfair, as MR.PEA can access external web information while most baselines cannot."}, "questions": {"value": "- Have you evaluated MR.PEA’s generalization across different model families or sizes beyond GPT-4.1-nano?\n- Have you analyzed how sensitive MR.PEA’s prompt generation is to variations in internal heuristic choices and decoding temperature?\n- To what extent can the reported performance gains of MR.PEA over baselines be attributed to its access to external web information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pRvxkDYFPG", "forum": "YSkby14i2v", "replyto": "YSkby14i2v", "signatures": ["ICLR.cc/2026/Conference/Submission5120/Reviewer_UQ6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5120/Reviewer_UQ6n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920829563, "cdate": 1761920829563, "tmdate": 1762917892270, "mdate": 1762917892270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the MR.PEA, a novel self-supervised prompt optimization framework. The primary goal of MR.PEA is to overcome the limitations of current LLM prompt optimization methods, namely their reliance on costly labeled data and the optimization ambiguity or high computational cost associated with traditional self-supervised approaches. The core mechanism of MR.PEA is meta-reasoning, which allows the agent to iteratively build and refine task-specific knowledge using only minimal input. This approach aims to achieve efficient and adaptive prompt optimization in a self-supervised manner."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of a \"meta-reasoning\" mechanism for prompt engineering presents an innovative paradigm distinct from conventional search-based or LLM-generation/filtering optimization methods. This mechanism promises to make the optimization process more interpretable and directed.\n\n2. The framework's emphasis on \"self-supervision\" and \"minimal input\" offers significant practical value by substantially reducing the cost and complexity of deploying LLM applications, especially in domains where large amounts of labeled data are scarce.\n\n3. MR.PEA iteratively distills general strategies and principles. This means the output is not just an optimal prompt, but a reusable \"prompt engineering knowledge base\" applicable to similar tasks.\n\n4. Prompt engineering is a critical bottleneck for the practical deployment of LLM-based systems. Proposing an automated, low-cost optimization framework is highly relevant and timely in the current research landscape."}, "weaknesses": {"value": "1. While data cost is reduced, the meta-reasoning mechanism itself typically involves multiple, complex LLM calls and knowledge structuring steps. The paper needs to clearly quantify and compare the overhead of MR.PEA in terms of total token consumption and latency against relevant baselines.\n\n2. Given the inherent uncertainty in LLM generation, the robustness of MR.PEA's iterative optimization process is vital. The authors should provide evidence of performance stability and generalization capability across diverse task domains (e.g., classification, summarization, code generation) and when using different LLM backbones."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tD9aecjK5O", "forum": "YSkby14i2v", "replyto": "YSkby14i2v", "signatures": ["ICLR.cc/2026/Conference/Submission5120/Reviewer_bqab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5120/Reviewer_bqab"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967590854, "cdate": 1761967590854, "tmdate": 1762917891747, "mdate": 1762917891747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a simple self-supervised prompt optimization approach MR.PEA, that uses minimal inputs to curate some strategies and evaluation criteria, generates different validation examples, refines prompts using the knowledge, and conducts criteria based pairwise comparisons while maintaining a ranked prompt pool with explore/exploit scoring. Additionally, it also leverages web search for extracting related knowledge to improve the prompt. Overall, the methodology is incremental in novelty for prompt optimization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Mr. PEA seems to be a well engineered system, using a well orchestrated and meta prompted pipeline it can improve performance. The five phase loop is clearly specified with pseudocode and control parameters.\n2. Mr. PEA can improve the prompt with only only example as the seed. \n3. The paper writing is really easy to follow."}, "weaknesses": {"value": "1. The proposed methodology is quite incremental to the ongoing works in this domain. The whole system is a simple loop of orchestrated LLM calls with very well engineered prompts. That significantly lowers the novelty of the proposed approach.\n 2. My primary concern lies in allowing the LLM to search web for related information. As it searches it should ofcourse get significantly more information regarding the task. It is not really surprising that it is one of the primary aspects of improved performance. Also, for optimization, it can search upto 20 times in the web, that is a significant amount of additional information which definitely should improve performance.\n 3. I could not find the details about it, but web searching for a task like GSM8K or BBH tasks significantly increases the risk of seeing more data as well as gathering more information. A simple web search with one of the examples, already bring up all the examples in the web from that dataset (BBH). I think with web searches, prompt optimization needs to be done on more sophisticated tasks to showcase the performance benefits. \n 4. Baselines suggestions: comparison against APO, PE2, TextGrad would be interesting to see."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ChJxMTbzuK", "forum": "YSkby14i2v", "replyto": "YSkby14i2v", "signatures": ["ICLR.cc/2026/Conference/Submission5120/Reviewer_ak1k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5120/Reviewer_ak1k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145729366, "cdate": 1762145729366, "tmdate": 1762917891303, "mdate": 1762917891303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}