{"id": "WnM3sluiVn", "number": 3178, "cdate": 1757351297419, "mdate": 1763708536487, "content": {"title": "Enhancing Hallucination Detection through Noise Injection", "abstract": "Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations. Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs. Recent research has linked hallucinations to model uncertainty, \nsuggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from multiple samples drawn from a model. While drawing from the distribution over tokens defined by the model is a natural way to obtain samples, in this work, we argue that it is sub-optimal for the purpose of detecting hallucinations. We show that detection can be improved significantly by taking \ninto account model uncertainty in the Bayesian sense. To this end, we propose a very simple, training-free approach based on perturbing an appropriate subset of model parameters, or equivalently hidden unit activations, during sampling. We demonstrate that our approach significantly improves inference-time hallucination detection over standard sampling across diverse datasets, model architectures, and uncertainty metrics.", "tldr": "", "keywords": ["Bayesian Inference", "Uncertainty Quantification", "Hallucination Detection"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b6046ad04595a4d9727eb07c453d6e468556a24.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an approach for uniform noise injection to LLMs which is motivated from a Bayesian point of view.\nThe authors use the approach for hallucination detection and show improvements over baselines on reasoning and QA tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method improves over baseline hallucination detection methods on a range of benchmarks and clearly seems to separate hallucinated vs. non-hallucinated outputs better."}, "weaknesses": {"value": "- I believe the paper is not precise enough about many of the theoretical concepts. First of all, the authors argue a lot for distinguishing between epistemic (connected to lack of knowledge) and aleatoric uncertainty (connected to inherent uncertainty in the problem).\nHowever, they claim that \"More formally, sampling from the model using next-token prediction can be considered as a way to capture uncertainty in the data distribution,\"(L80-82), i.e. aleatoric uncertainty. This is arguably (as discussed in e.g. Depeweg 2018, Hüllermeier & Waegemann 2021) only the case if there is no epistemic uncertainty, because then the model has sufficiently learned the true distribution and the randomness in the outputs is indeed the randomness of the problem. But this is generally not the case for e.g. LLMs.\nI am also surprised that the authors do not use any explicit decomposition of both uncertainties, which is discussed in these works and also in Malinin & Gales 2021, which is cited. Often Bayesian treatment is precisely motivated from this angle. The authors also motivated a Bayesian view but do not exploit it.\n\n- Connected to this, the formulation of Eq. 2 is imprecise. It shows a token-level model averaging but it is not clear why this should be a better choice than sequence-level averaging (these are in general not the same). This is heavily discussed in Malinin & Gales 2021 but not picked up here. Furthermore, Alg. 1 also seems to rather approximate a sequence-level average as all full sequences are sampled using the same noise sample for each token within them if I understand the pseudo code correctly. As a consequence, it is also not clear to me how the baseline from their paper is calculated here: do you use an ensemble on a token-level or sequence-level or no ensemble at all?\n\n- I believe claims that the claim \"since LLMs are very expensive to train, Bayesian treatment of LLMs for hallucination detection has remained elusive\" (L148-149) is not correct, as e.g. Shen et al. 2024 show Bayesian training of smaller LLMs using variational learning which could easily be used for hallucination detection.\n\n- Regarding the experiments, the work does not compare to any other Bayesian methods like Laplace or variational learning, which can feasibly be done for LLMs also using LoRA. Therefore, it remains unclear how the uniform noise added here compares to related works and especially to learned noise which might turn out to perform better.\n\n- For me it is unclear if the proposed answer entropy will work well outside of QA tasks, because it might be unlikely to sample the same (long and open-ended) answer twice. It would be good to add such an experiment if possible."}, "questions": {"value": "- How exactly are the sequences y calculated in your algorithm? Is it correct that you are fixing the noise for a given input and then calculate the outputs by sampling from a model with the same noise applied for the entire sequence?\n\n- Can you please provide details on how you apply the baseline from Malinin & Gales 2021?\n\n- The citation Manakul et al. is broken.\n\n- How would you calculate answer entropy for more open-ended tasks? It seems to me like there might very often be cases where all counts are 1 leading to maximal entropy just because it is much less likely to sample the same output many times when the output sequences are long and open-ended.\n\n## References \n\nDepeweg, Stefan, et al. \"Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning.\" International conference on machine learning. PMLR, 2018.\n\nHüllermeier, Eyke, and Willem Waegeman. \"Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.\" Machine learning 110.3 (2021): 457-506.\n\nMalinin, Andrey, and Mark Gales. \"Uncertainty estimation in autoregressive structured prediction.\" ICLR, 2021.\n\nShen, Yuesong, et al. \"Variational learning is effective for large deep networks.\" ICML, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K6xygNXnnI", "forum": "WnM3sluiVn", "replyto": "WnM3sluiVn", "signatures": ["ICLR.cc/2026/Conference/Submission3178/Reviewer_fG6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3178/Reviewer_fG6Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760617174064, "cdate": 1760617174064, "tmdate": 1762916585660, "mdate": 1762916585660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free, Bayesian-inspired sampling scheme for hallucination detection that perturbs a subset of model internals during generation to capture epistemic uncertainty, and combines it with standard temperature sampling that captures aleatoric uncertainty. Concretely, the method injects uniform noise into MLP activations (top layers by default), which is shown to be approximately equivalent to bias perturbation, and then samples answers K times; uncertainty is computed via Answer Entropy (and other metrics), yielding improved AUROC for hallucination detection across GSM8K, CSQA, and TriviaQA and multiple model families (Gemma, Phi-3, Llama-2/3, Mistral). Extensive ablations vary K, temperature, noise magnitude, and perturbed layers, and show complementarity with input perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, training-free mechanism with negligible latency overhead; easy to bolt onto many LLMs and sampling-based detectors. \n2. Principled motivation: separates aleatoric from epistemic uncertainty; implements the latter via internal perturbations approximating a posterior over models. \n3. Consistent gains across datasets/models/metrics. \n4. Thorough ablations. \n5. Complementarity with input perturbations and with standard sampling - jointly using both sources yields the strongest results."}, "weaknesses": {"value": "1. Noise distribution centering: The paper repeatedly injects U(0, α) (positive-only) noise and even defines​ q(omega) such that it is not centered at zero around the checkpoint parameter; this can introduce a mean shift rather than a purely variance-based epistemic perturbation. A symmetric zero-mean alternative is only partially explored and merits direct head-to-head comparison. \n2. Metric dependence: A central result uses Answer Entropy (counting distinct final answers). This is natural for GSM8K/CSQA, but for long-form/free-form generation the mapping from perturbation to an “answer string” can be brittle; although the authors do show improvements for other metrics on TriviaQA, more analysis on long-form generation would strengthen claims. \n3. Hyperparameter tuning burden: α is selected per model/dataset (and even per layer range). While inexpensive, this introduces an extra validation loop and raises questions about sensitivity/transfer across domains."}, "questions": {"value": "1. Why not adopt zero-mean perturbations by default? Do your AUROC and accuracy trends hold when eliminating mean shifts? \n2. How sensitive are results to reusing the same noise vector across layers vs. independent per-layer noise? Does structured, shared noise bias the uncertainty estimate? \n3. Can you report calibration (e.g., coverage vs. error curves) to complement AUROC and show that epistemic perturbations improve calibrated abstention? \n4. For long-form or multi-span answers (beyond a single numeric/string field), which aggregation metric worked best in your trials, and how robust is answer extraction? \n5. What is the worst-case degradation in task accuracy across α, K, and perturbed layers? A Pareto curve (AUROC vs. accuracy) would clarify trade-offs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6H0QVEiGUm", "forum": "WnM3sluiVn", "replyto": "WnM3sluiVn", "signatures": ["ICLR.cc/2026/Conference/Submission3178/Reviewer_XrUM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3178/Reviewer_XrUM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832907450, "cdate": 1761832907450, "tmdate": 1762916585428, "mdate": 1762916585428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "There are many existing methods in literature for detecting hallucinations in an LLM that involve generating multiple samples from the LLM. This paper proposes adding noise to (some of) the parameters of the LLM during the sampling process. The authors argue that this noise injection is justified from a bayesian perspective and accounts for model uncertainty. The paper provides experimental results on the effect of adding noise to the intermediate layers of an LLM. The experiments section includes adequate comparisons to relevant baselines on established hallucination benchmarks, with noise injection showing favorable results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I like that the method is simple and justified from a theoretical perspective.\n- The discussion on the complementary effect of aleatoric and epistemic uncertainty was insightful. \n- The experiments section includes a variety of ablation studies which vary several relevant factors such as noise, temperature, layers in which noise is injected etc., Since, noise injection is easy to combine with other sample based approaches, I appreciate that the authors already included an ablation study that does this and it appears that injecting noise boosts the performance of those other methods."}, "weaknesses": {"value": "Some choices and details were unclear to me and it would be nice to see more justification/discussion pertinent to these:\n- Why did you pick the uniform distribution for the noise? \n- How did you measure accuracy in Figure 4b? I am asking because there is no obvious way to judge the correctness of an LLM response compared to a ground truth. I believe it is important to be explicit about this detail (If you did include this in the manuscript already and I missed it, I apologize).\n\nI am happy to raise my score if the above points and the questions I list below are sufficiently addressed.\n\nMinor:\n- Table 1 formatting: answer strings are not in bold as stated in the caption."}, "questions": {"value": "- Is there a need to limit the layers for which noise is added? Maybe it does not matter that you add noise to all the model parameters as long as the magnitude (in some $L_p$ sense, is under some threshold). In light of the fact that the AUROC numbers in table 6 are all close to each other, I am more inclined to believe that this is the case.\n- I understand you compare with Gaussian noise in Appendix C. But the AUROC performance is nearly identical with uniform noise. So does the noise distribution not matter? Why or why not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V4UVMP9gWW", "forum": "WnM3sluiVn", "replyto": "WnM3sluiVn", "signatures": ["ICLR.cc/2026/Conference/Submission3178/Reviewer_Wqmu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3178/Reviewer_Wqmu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950044192, "cdate": 1761950044192, "tmdate": 1762916584954, "mdate": 1762916584954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes introducing noise in LLMs during inference (by random perturbations of its parameters) as a way to capture epistemic uncertainty. Combined with temperature-based sampling, i.e., aleatoric uncertainty, the paper shows how the overall entropy is a better measure of uncertainty and thus does a better job in hallucination detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper focuses on a targeted research question, and does a good job of providing an in-depth discussion.\n2. The improvement in hallucination detection is obvious. But for me, beyond these improvements, the paper provides a more accurate way to measure the overall uncertainty of an LLM, and thus, the technique here can be highly valuable.\n3. The paper is well written, and I enjoyed reading it. The depth of the experiments is good, and the paper provides answers to many different ablation questions.\n\nI should note that I'm not too familiar with the rapidly growing literature in hallucination detection, and separately, in LLM uncertainty measurement. So my assessment of the novelty of this work would be limited."}, "weaknesses": {"value": "There are many other questions that could have been asked, but in my opinion, the paper did a good job focusing on one problem statement and providing appropriate depth.\n\nMy only complaint, and I hate to give such cliched feedback, but I really wanted to see experiments on a bigger variety of datasets. Different datasets seem to have different trends in the 3 datasets used in the paper, and an analysis over a larger set of datasets would have been very interesting. For instance, how well does this technique work for factual hallucinations vs faithfulness hallucinations? How well is it able to handle knowledge-intensive hallucination datasets vs commonsense reasoning datasets? Etc."}, "questions": {"value": "1. Based on the trends in Figure 2, as well as the fact that improvements by adding epistemic uncertainty calculations are not extreme (although they are clearly substantial), it seems that epistemic uncertainty plays a smaller role in hallucination detection than aleatoric uncertainty. Can you comment on why you think that might be? Is the expectation that there would be less epistemic uncertainty? Or is the problem instead that the approximation is not good enough, and future work should focus on improving how we can measure epistemic uncertainty?\n2. Input perturbations should be a form of epistemic uncertainty, not aleatoric uncertainty as mentioned in the paper. I would like to hear the authors' reasoning if they disagree. Assuming it is a form of epistemic uncertainty (which I believe it is), why do input perturbations have a complementary effect to an existing measure of epistemic uncertainty? This probably connects to my first question, but a broader discussion on what is lost when trying to 'approximate' epistemic uncertainty, since we cannot perform a proper Bayesian treatment of LLMs, and how it can be supplemented with other additions, would help the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SFLLGpr6h3", "forum": "WnM3sluiVn", "replyto": "WnM3sluiVn", "signatures": ["ICLR.cc/2026/Conference/Submission3178/Reviewer_VoHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3178/Reviewer_VoHR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762082211489, "cdate": 1762082211489, "tmdate": 1762916584706, "mdate": 1762916584706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}