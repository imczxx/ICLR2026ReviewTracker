{"id": "FZYtfAlndh", "number": 18601, "cdate": 1758289395970, "mdate": 1759897092929, "content": {"title": "You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs", "abstract": "Large language models (LLMs) are increasingly deployed in specialized domains such as finance, medicine, and agriculture, where they face significant distribution shifts from their training data. Domain-specific fine-tuning can mitigate this challenge but relies on high-quality labeled data that is expensive and slow to collect in expertise-limited settings. We study label-free test-time adaptation for language models and present SyTTA, an inference-time framework that adapts models on-the-fly without additional supervision. SyTTA couples two complementary uncertainty signals that arise under distribution shift: input-side perplexity, indicating mismatch with domain-specific terminology and patterns, and output-side predictive entropy, indicating diffuse and unstable token probabilities during generation. Unlike prior test-time approaches for LLMs that optimize a single signal, SyTTA integrates both within a unified self-supervised objective that automatically balances their influence, stabilizing generation while improving domain awareness. Across diverse model architectures and domain-specific benchmarks, SyTTA delivers consistent gains. Notably, on agricultural question answering, SyTTA improves ROUGE-Lsum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query. These results show that effective test-time adaptation for language models is achievable without labeled examples, supporting deployment in label-scarce domains. The code will be made available upon acceptance.", "tldr": "We introduce Synergistic Test-time Adaptation (SYTTA), a label-free framework for adapting autoregressive language models at inference.", "keywords": ["Large language models", "Entropy minimization", "Test-time adaptation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a203fdf29403555bbcdbc89f9917c699f3ad3d30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work is in line with test-time adaptation of LLMs. It proposes a novel label-free test-time adaptation framework (SYTTA) for LLMs. It couples two complementary uncertainty signals from different positions: input-side perplexity and output-side predictive entropy. Existing works typically take only one signal into account. A novel strategy, dynamic importance weighting, is applied to incorporate these two signals effectively. The experimental results demonstrate that the proposed method significantly outperforms strong baselines, verifying its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The writing and structure are excellent.\n- The proposed framework is novel and interesting, especially the dynamic weighting part.\n- Two different signals are well exploited via the dynamic weighting.\n- Potential collapse and distribution drift are also considered; to avoid such issues, KL is applied.\n- The experiments are comprehensive; a concise case study would further clarify the improvement over baselines.\n- Strong and consistent gains over strong baselines across the experiments."}, "weaknesses": {"value": "- From my point of view, no major weakness.\n- The discussion of prefix token length could be deepened. More detailed analysis of the generated prefix tokens and other perspectives (e.g., attention sink) would be helpful and interesting.\n- It would be beneficial to also compare the computational overhead and latency."}, "questions": {"value": "**Questions and Suggestions**\n\n- I am curious what the exact generated prefix tokens are. I would appreciate a discussion regarding the generated prefix tokens to analyse their contributions from a semantic perspective. \n- Does attention sink also affect the prefix token length setting?\n- It would be better to have a concise case study section to compare different models straightforwardly.\n\n\nLastly, I would be happy to increase the rating if my questions are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZOD3CissC7", "forum": "FZYtfAlndh", "replyto": "FZYtfAlndh", "signatures": ["ICLR.cc/2026/Conference/Submission18601/Reviewer_9aHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18601/Reviewer_9aHm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462789795, "cdate": 1761462789795, "tmdate": 1762928318543, "mdate": 1762928318543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SYTTA, a test-time adaptation framework that jointly (i) reduces input-side mismatch via perplexity-driven Input Distribution Adaptation and (ii) sharpens output confidence by minimizing predictive entropy with a reverse-KL constraint to the base model; a dynamic importance weighting balances the two losses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear and well-structured writing.\n2.\tThe paper targets test-time learning/adaptation for LLMs, a rapidly emerging direction with clear utility for real-world deployments that face distribution shift and scarce labels."}, "weaknesses": {"value": "1.\t**Limited novelty over TLM.** The paper directly reuses input perplexity minimization from prior TLM and mainly adds an output entropy term. Conceptually, this reads as an incremental extension that couples two known self-supervised signals rather than a fundamentally new objective.\n2.\t**Overstated “Contributions.”** Of the three listed contributions, #2 (“consistent performance gains”) and #3 (“extensive empirical analysis/ablations”) summarize results and diagnostics rather than introduce additional methodological innovations. This weakens the crispness of the claimed contributions.\n3.\t**Unsubstantiated claim around L191–193.** The statement that “Input Distribution Adaptation reduces input perplexity but does not ensure coherent or confident generation; models may still exhibit high predictive entropy or drift” is plausible but lacks a formal theoretical justification or a targeted empirical test explicitly verifying this claim. \n4.\t**Benchmark coverage is narrow.** Experiments follow AdaptEval (DomainBench and InstructBench) as in TLM, but do not include ReasoningBench, leaving uncertainty about the method’s behavior on reasoning-heavy evaluations.\n5.\t**Reproduction gap relative to TLM.** Given that official TLM checkpoints are publicly available, please clarify any cases where reproduced results fall below TLM.\n6.\t**Computational cost.** Please report latency and memory cost to assess feasibility in realistic serving settings.\n7.\t**Effectiveness on quantized LLMs.** Are the gains preserved for common INT8/INT4 deployments?\n8.\t**Bibliography quality issues.** While minor in isolation, the number of bibliographic errors raises concerns about the manuscript’s overall rigor. For example: (i) the author list and venue for Test-time Learning for Large Language Models are incorrect; (ii) the author list for Efficient Test-time Model Adaptation without Forgetting is incorrect."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nn5wwADRxo", "forum": "FZYtfAlndh", "replyto": "FZYtfAlndh", "signatures": ["ICLR.cc/2026/Conference/Submission18601/Reviewer_FMEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18601/Reviewer_FMEh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719441288, "cdate": 1761719441288, "tmdate": 1762928318116, "mdate": 1762928318116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a technique for test-time LLM adaptation (SyTTA) that relies on minimizing the input prompt perplexity on the one hand, and learning a 4-16 token output prefix on the other hand. The two objectives are weighted dynamically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Consistent gains across multiple model families and datasets are demonstrated\n- Clear writing style"}, "weaknesses": {"value": "Since the two driving forces here seem to be input distribution adaptation and output confidence shaping, the two obvious ablations are doing only one of them. While the first one (input distribution adaptation) is provided in form of the TLM baseline, an ablation with isolated output confidence shaping is missing.\n\nHaving ROUGE-Lsum as the single metric in the main paper is limiting. Table 4 in the appendix does contain BERTScore results, but the gains are less conclusive.\n\nI don't have a background in test time adaptation, so this may be my ignorance. But I have a hard time buying the motivation behind the output confidence shaping. According to the authors, it \"introduce an output-oriented objective that regularizes the next-token distribution\" (L194). But IIUC the algorithm is actually the opposite of traditional regularization as it makes the model accumulate more probability mass on its own prediction.\n\nFigs. 3-5 are too tiny, especially Fig. 5. I feel that this has become more common recently, but imo it should be a reason for desk-rejection if there is absolutely no chance of reading it on a print-out.\n\nMinor comments:\nThe Hu et al. (2025) citation is for ICLR, but it should be ICML."}, "questions": {"value": "- The approach seems quite expensive in terms of test-time compute - can you say more about the compoutational complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LHehJTSi25", "forum": "FZYtfAlndh", "replyto": "FZYtfAlndh", "signatures": ["ICLR.cc/2026/Conference/Submission18601/Reviewer_iemY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18601/Reviewer_iemY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943489155, "cdate": 1761943489155, "tmdate": 1762928317692, "mdate": 1762928317692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SYTTA (Synergistic Test‑Time Adaptation), a label‑free test‑time adaptation framework for LLMs that couples \n\n(i) input‑side perplexity minimization (IDA) with \n(ii) output‑side confidence shaping via entropy minimization plus a reverse‑KL “trust region” (OCS). \n\nA lightweight Dynamic Importance Weighting scheme balances the two signals on the fly. \nTwo deployment modes are offered: Static‑Ref (compute/caches a short base‑model prefix once) and Dynamic‑Ref (updates while generating the prefix). \nExperiments on LLaMA‑3.1/3.2 and Qwen‑2.5 (7B/14B) show consistent ROUGE‑Lsum improvements, often with k = 4‑token prefixes and LoRA updates to q_proj and v_proj."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Static‑Ref’s single forward pass per sample (during adaptation) is a practical and well‑motivated design. The cohort‑level, transductive “question‑only” setting matches common multi‑tenant deployments and makes the compute constraints explicit\n- Principled way to sharpen predictions without collapse and to anchor updates near the base model. The mode‑seeking property of reverse‑KL makes it a reasonable “trust‑region” surrogate for generation.\n- Analysis provides actionable choices and relates gains to model post‑training intensity (Qwen vs LLaMA), which will help practitioners."}, "weaknesses": {"value": "- ROUGE/BERTScore/BLEU are surface overlap metrics. For domain QA they correlate imperfectly with factual correctness and safety. The paper lacks human or verifier‑based factuality/faithfulness evaluation (even sampled) and error analysis to ensure gains aren’t mainly stylistic (e.g., matching domain phrasing) rather than correct content.\n- Does not report out‑of‑cohort generalization (e.g., adapt on batch A, test on disjoint batch B from the same target distribution), nor sensitivity to cohort size or streaming/batch arrival patterns\n- All tasks are text‑generation QA/instruction following. Prior TTA results suggest entropy signals behave differently in code, math, or structured reasoning tasks with verifiers"}, "questions": {"value": "- Use paired bootstrap for ROUGE/BLEU and report 95% CIs.\n- Since domains include health/finance, show a toxicity/hallucination sanity check (or cite one) to confirm KL+entropy do not over‑sharpen into confident but wrong outputs.\n- Have you tried adaptive k (e.g., stop once average entropy drops below a threshold)? This may retain the benefits of k=4 while saving tokens when the signal is even earlier.\n- Why do you restrict the lora to just q_proj and v_proj? Does more modules not yield gains?\n- Can you add small‑scale human evaluation or automatic verifier checks (where feasible) to ensure that ROUGE improvements reflect correct content, not just stylistic alignment? Even random 100‑sample audits on Agriculture/Wealth would be informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X2f1eDYjbC", "forum": "FZYtfAlndh", "replyto": "FZYtfAlndh", "signatures": ["ICLR.cc/2026/Conference/Submission18601/Reviewer_rHcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18601/Reviewer_rHcJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997325654, "cdate": 1761997325654, "tmdate": 1762928317346, "mdate": 1762928317346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}