{"id": "ectXDReTI4", "number": 20905, "cdate": 1758311557213, "mdate": 1759896952833, "content": {"title": "GIST: Gauge-Invariant Spectral Transformers for Scalable Graph Neural Operators", "abstract": "Adapting transformers to meshes and graph-structured data presents significant computational challenges, particularly when leveraging spectral methods that require eigendecomposition of the graph Laplacian, a process incurring cubic complexity for dense matrices or quadratic complexity for sparse graphs, a cost further compounded by the quadratic complexity of standard self-attention mechanism.\nConventional approximate spectral methods compromise the gauge symmetry inherent in spectral basis selection, risking the introduction of spurious features tied to the gauge choice that could undermine generalization.\nIn this paper we propose a transformer architecture that is able to preserve gauge symmetry through distance-based operations on approximate randomly projected spectral embeddings, achieving linear complexity while maintaining gauge invariance.\nBy integrating this design within a linear transformer framework, we obtain end-to-end memory and computational costs that scale linearly with number of nodes in the graph.\nUnlike approximate methods that sacrifice gauge symmetry for computational efficiency, our approach maintains both scalability and the principled inductive biases necessary for effective generalization to unseen graph structures in inductive graph learning tasks.\nWe demonstrate our method's flexibility by benchmarking on standard transductive and inductive node classification tasks, achieving results matching the state-of-the-art on multiple datasets. \nFurthermore, we demonstrate scalability by deploying our architecture as a discretization-free Neural Operator for large-scale computational fluid dynamics mesh regression, surpassing state-of-the-art performance on aerodynamic coefficient prediction reformulated as a graph node regression task.", "tldr": "GIST introduces gauge-invariant graph transformers that achieve linear scalability while maintaining spectral representation invariances for both graph learning and mesh-based neural operators", "keywords": ["Graph Transformers", "Neural Operators", "Graph Neural Networks"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/377ee37492c5b77199accf5e8abe9e547c53d44f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Thanks for the submission. The paper suggests an efficient transformer architecture for graph-structured data. The core idea is to use JLT’d spectral features (Laplacian eigenvectors, whose dimensionality is reduced with a random projection) in place of the usual queries and keys, which gives a form of spectral self-attention which is approximately gauge-invariant. These maps can be computed more efficiently than the full diagonalisation using FastRP, a random projection-based truncation algorithm developed separately by Chen et al. (2019)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficient transformers for graph structured data is an important unsolved research problem, and I can see that pure message passing will underperform in the graph neural operator setting because of its finite receptive field.\n2. The paper is broadly well written."}, "weaknesses": {"value": "1. I appreciate the need for memetic titles, but my understanding is that the method isn’t actually gauge invariant for any draw of the random projection matrix R. The authors do acknowledge that the invariance is approximate (it depends on the expectation of $RR^T$ being the identity, with $R$ a random low rank matrix), but in places I think the phrasing is a bit misleading. E.g. see line 298 – isn’t this equation actually wrong without taking the expectation? I appreciate that the JLT preserves the norms and distances between a set of vectors with high probability; it would be even better (but probably difficult) to formulate the approximate invariance property mathematically rigorously.\n2. A note: if you compute regular linear attention in one branch and gauge-invariant attention in another branch and add the results, isn’t this the same as just concatenating the regular queries/keys with your spectral features? (At least up to the different value projection matrix). I’m wondering whether the division into branches is strictly necessary, or whether the paper is really about a new efficient absolute position embedding with some nice approximate invariance properties. \n3. Missing experiments, that are (imo) crucial:  \na) No ablation of removing the spectral attention branch, including just regular linear attention + message passing. This is surely the most crucial acid test in order to assess gains from your algorithm!    \nb) No ablation over different embedding dimensions $r$ – unless you can fix r independently of N and get consistent performance, it’s a bit of a stretch to claim the algorithm is linear in N.  \nc) No demos on any toy tasks that strongly depend on graph structure. The authors do include a few benchmarks (I especially like the neural operator results), but I’m not sure whether Cora/Citeseer/Pubmed really distill out whether your addition helps the transformer better capture graph structure. Something explicitly topological like shortest path distance prediction might be more natural, and a good setting to ablate the spectral branch.  \nd) No time complexity scaling wrt number of graph nodes – wall-clock time or FLOPs."}, "questions": {"value": "1. Please could you clarify if/how you choose the ‘large enough iteration step $K$’? Is this a hyperparameter, or is it chosen in some principled (graph-dependent) way to ensure the estimator is accurate? Doesn’t this implicitly set some finite receptive field? Won’t finite $K$, independent of $N$, cause regressions on very big graphs/high mesh resolutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpX48IBsxy", "forum": "ectXDReTI4", "replyto": "ectXDReTI4", "signatures": ["ICLR.cc/2026/Conference/Submission20905/Reviewer_Eiq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20905/Reviewer_Eiq7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760711320107, "cdate": 1760711320107, "tmdate": 1762938239443, "mdate": 1762938239443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the positional encoding effectiveness of Laplacian matrix eigenvectors in graph transformers, with a particular focus on node classification tasks. It introduces an Energy Spectral Density metric derived from class labels and uses this metric to identify the top-_k_ eigenvectors for encoding. The proposed approach is integrated into several existing graph transformer architectures, leading to consistent improvements in node classification performance across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed ESD metric and corresponding BTS method are simple, intuitive, and easily adaptable to a wide range of graph transformer models.\n2. The paper offers a theoretical analysis of the rationale behind BTS, elucidating its effectiveness in the context of node classification tasks.\n3. The experimental evaluation is thorough, including extensive ablation studies that validate the efficacy of the proposed BTS method."}, "weaknesses": {"value": "1. The contributions of this paper are somewhat limited. The main contribution  lies in the proposed Gauge-Invariant/Equivariant Spectral Self-Attention mechanisms, while the linear-time spectral embedding implementation is largely based on prior work, i.e., FastRP.\n2. The analysis of GIST is insufficient, both theoretically and experimentally. In particular, the paper does not examine how the parameters $r$ and $k$  in Algorithm 1 affect performance and complexity. When the graph spectral radius is close to 1, large $r$ and $k$  may be required, which could significantly increase computational cost.\n3. The experiments are conducted on relatively small graphs. The method should also be evaluated on large-scale graphs with tens of millions of nodes to demonstrate scalability.\n4. The experimental setup is vaguely described, especially regarding the choices of $r$ and $k$ . A more detailed discussion of these parameters, as well as an analysis of the actual computational (time and space) cost, is necessary.\n5. The selection of baselines is outdated. More recent and relevant Graph Transformer models such as Specformer [1] and PolyFormer[2] should be included for comparison.\n6. The overall writing quality could be improved. See the following minor comments for specific issues.\n\n[1] Ma J, He M, Wei Z. Polyformer: Scalable node-wise filters via polynomial graph transformer[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024: 2118-2129.  \n[2] Bo D, Shi C, Wang L, et al. Specformer: Spectral graph neural networks meet transformers[J]. arXiv preprint arXiv:2303.01028, 2023.\n\n\n**Minor comments:**  \n(1) The Introduction section provides only a brief overview of the proposed method and contributions; these should be elaborated further.  \n(2) The mathematical notation throughout the paper is inconsistent — for instance, matrices and vectors are bolded and not in main paper and appendix. Please ensure consistency and follow the ICLR formatting style.  \n(3) Baseline citations should be placed immediately after each method name (e.g., it is unclear which paper GCNIII refers to).  \n(4) In Tables 1 and 2, if standard deviations are unavailable, the “±” should be omitted rather than left blank.  \n(5) In Algorithm 1, the line $P \\leftarrow A D^{-1}$ should be corrected to $P \\leftarrow  D^{-1}A$"}, "questions": {"value": "Please respond to the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Dlm1PHxfHP", "forum": "ectXDReTI4", "replyto": "ectXDReTI4", "signatures": ["ICLR.cc/2026/Conference/Submission20905/Reviewer_t5mP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20905/Reviewer_t5mP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760790592966, "cdate": 1760790592966, "tmdate": 1762938181013, "mdate": 1762938181013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GIST, a graph transformer that steers attention using (projected) spectral embeddings while enforcing gauge invariance (invariance to rotations/sign flips within eigenspaces). The model is a multi-branch block: (i) a local graph-conv/linear-attention branch, (ii) a feature linear-attention branch, and (iii) a gauge branch with gauge-invariant and gauge-equivariant spectral attention. Authors claim linear complexity overall and show competitive results on Planetoid/PPI, mixed performance on Elliptic, and strong results on a large mesh regression task (DrivAerNet)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear conceptual core... use inner products of (projected) Laplacian eigenmaps to steer attention while remaining invariant to eigenbasis gauge; neatly explained and supported.\n\nRandom-feature JL projections to approximate spectral geometry plus linear attention to avoid $O(N^2)$ attention, embedded in a multi-scale block.\n\nCompelling large-graph result in that DrivAerNet shows practical gains on real meshes without regridding."}, "weaknesses": {"value": "The text claims linear end-to-end via the Katharopoulos et al. linear transformer, but the presented gauge-invariant/equivariant algorithms use softmax attention (Alg. 2/3). It’s unclear whether the actual gauge blocks use linear attention kernels (and if so, which feature map) and how gauge invariance is preserved under that kernelization), or only the “feature branch” is linear while the gauge path remains softmax (thus quadratic). Unless I'm missing something this point seems to critically affect the complexity claim.\n\nFor DrivAerNet the authors append Euclidean coordinates and normals to the spectral embeddings. This seems to re-introduce a coordinate-system dependence into the very vectors whose inner products are supposed to be gauge-invariant. Please clarify: are coords/normals entering only the values (feature branch) or also the Q/K used for gauge-invariant attention? \n\nThe Planetoid/inductive tables omit several post-2022 graph-Transformer baselines that are now standard, e.g. GPS-style models with LapPE/RWPE encodings, sparse-attention Exphormer variants, and/or tokenized/CLS readouts such as NAGphormer/TokenGT. Including these would strengthen empirical positioning.\n\nThe text asserts that, in the refinement limit, similarities “recover the continuum Green’s-function kernel” and that self-attention therefore realizes a nonlocal kernel integral. This is interesting, but there is no theorem or convergence experiment (e.g., accuracy vs mesh refinement with fixed parameters) to substantiate discretization invariance beyond heuristic argument.\n\nWith a 3-branch block and added geometric channels, it’s unclear which component yields the improvement. An ablation (remove gauge path / remove local path / use softmax vs linear in each) is needed to credit the proposed gauge mechanism vs generic multi-branch modeling and extra features. \n\nAuthors should provide wall-clock, peak memory, FLOPs, and parameter counts per branch on Planetoid, PPI, and DrivAerNet; confirm O(N) scaling end-to-end."}, "questions": {"value": "Do the gauge blocks use linear attention or softmax?\n\nIf you sample a fresh R' at test time (or per graph), does accuracy remain stable?\n\nDo coordinates/normals ever enter Q/K in the gauge-invariant path? If so, how is invariance preserved?\n\nOperator discretization study: Fix parameters and vary mesh resolution; does error remain flat (up to sampling noise)? \n\nAny empirical support for Green’s-function limit behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gkP1Gc1Lxo", "forum": "ectXDReTI4", "replyto": "ectXDReTI4", "signatures": ["ICLR.cc/2026/Conference/Submission20905/Reviewer_Qtqz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20905/Reviewer_Qtqz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147324526, "cdate": 1762147324526, "tmdate": 1762938172675, "mdate": 1762938172675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}