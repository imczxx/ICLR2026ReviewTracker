{"id": "zBPZeRjfgu", "number": 14690, "cdate": 1758241792097, "mdate": 1759897354762, "content": {"title": "Cautious Optimizers: Improving Training with One Line of Code", "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searched for faster and more stable optimizers with only constrained positive outcomes. In this work, we propose a \\textbf{single-line modification in Pytorch} to any momentum-based optimizer, which we rename cautious optimizer, e.g. C-AdamW and C-Lion.  Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing not only consistent speed-up on LLM pretraining and post-training tasks, but also better results in MAE pretraining, with minimum extra tuning on hyperparameters.", "tldr": "Improving Training with One Line of Code", "keywords": ["Optimizer", "AdamW"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/449aef2d324d58ec40167c5c69456979e2a92fd8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents \"Cautious Optimizers,\" a simple, one-line modification for momentum-based optimizers like AdamW. The method only applies parameter updates when their direction aligns with the sign of the current gradient, preventing counter-productive steps. This change is claimed to accelerate model training (including for LLMs and computer vision) with minimum extra tuning on hyperparameters, all while preserving theoretical convergence guarantees under the Lyapunov analysis."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The change is trivial (a single line of code) and applicable to existing momentum-based optimizers.\n\n- The method boosts performance without the need for costly and time-consuming hyperparameter retuning.\n\n- The intuitive idea is supported by theoretical analysis, ensuring that convergence guarantees are maintained.\n\n- It demonstrates consistent speed-ups in various high-impact domains, including LLMs and image classification."}, "weaknesses": {"value": "N/A"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XxwGEevfyk", "forum": "zBPZeRjfgu", "replyto": "zBPZeRjfgu", "signatures": ["ICLR.cc/2026/Conference/Submission14690/Reviewer_uTNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14690/Reviewer_uTNE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760519495833, "cdate": 1760519495833, "tmdate": 1762925057507, "mdate": 1762925057507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simple modification to momentum-based optimizers called Cautious Optimizers, implemented by masking update directions that disagree in sign with the current gradient. The authors argue that this one-line change ensures monotonic decrease in loss for small step sizes, preserves the Lyapunov/Hamiltonian structure of momentum dynamics, and empirically improves convergence speed and training stability. Experiments on toy problems, LLM pretraining (100M LLaMA on C4 and FineWeb-Edu), and vision tasks (Mini-ImageNet with ViT) confirm that the proposed modification can improve the performance of some popular optimizers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a practically relevant problem.\n\n- The proposed method is very simple (a one-line change in PyTorch) and broadly applicable to many optimizers.\n\n- The experiments show consistent (but modest) gains in LLM pretraining, image classification, and toy settings, with improved stability and slightly better downstream performance.\n\n- The theoretical analysis tries to conceptually connect the masking heuristic to theoretical convergence guaranties."}, "weaknesses": {"value": "1. Theoretical claims are not fully convincing. The results demonstrate that cautious optimizers can reduce the loss more than the original optimizers in a *single step*, but not throughout the full optimization trajectory. In line 84, the authors claim that \"Our theoretical analysis shows that the modified algorithm converges to local optima under mild conditions on the base optimizers\", but the presented results do not establish such convergence.\n\n2. Lack of stochastic analysis (the paper only considers deterministic gradient setting).\n\n3. Empirical improvements are modest. Most improvements (e.g., Table 2) are around $0.1–1$%, which may fall within variance across training runs.\n\n4. The abstract claims \"consistent speed-up\", but training time or throughput overhead is not reported (the results focus on perplexity and accuracy).\n\n5. No results are shown on other architectures (CNNs, RNNs) or non-transformer tasks.\n\n6. Some equations in the appendix overflow the page margins.\n\n7. Minor issues: several typos and grammatical errors (e.g., 'methods normally requires' in line 43, 'The follow is a comparison result' in line 240, 'catuious' in line 390, 'generalit' in line 444),  inconsistent boldface notation (e.g., equation (1)), formatting issues in References."}, "questions": {"value": "1. Could the authors address the concerns above?\n\n2. In Table 1, why were C-AdamW runs with learning rates 1e-4 and 3e-4 omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fek5DrH77f", "forum": "zBPZeRjfgu", "replyto": "zBPZeRjfgu", "signatures": ["ICLR.cc/2026/Conference/Submission14690/Reviewer_G1tn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14690/Reviewer_G1tn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792143334, "cdate": 1760792143334, "tmdate": 1762925056975, "mdate": 1762925056975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes “Cautious Optimizers,” a one-line masking of momentum updates that zeros coordinates where the proposed update and current gradient have opposite signs, optionally rescaled by the active-mask ratio (Alg. 1). The theory is framed via a Hamiltonian+Descent view (continuous time) and per-step comparison results (discrete time). Experiments include a 2-D toy, Mini-ImageNet, and LLM pretraining up to 1.2B parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a simple mechanism (coordinate-wise sign check) that is easy to implement; clear statement that it promotes monotone decrease of the loss for small steps. \n\n\n2. Empirical results cover both vision and language, and generally show small but consistent improvements"}, "weaknesses": {"value": "1. The key idea of masking coordinates where the gradient and velocity have opposite signs is meant to promote descent. But requiring sign consistency on every coordinate feels too strict — usually, it's enough for the update direction and gradient to have a positive inner product. Also, in stochastic settings with noisy gradients, enforcing per-coordinate alignment could hurt rather than help. Plus, the paper motivates this with “monotonic decrease,” but per-step monotonicity isn’t necessary for faster convergence — e.g., Nesterov's method is non-monotone. This raises the question: is this motivation really essential?\n\n2. In Theorem 2.3 and 2.4, the discrete-time analysis relies on certain properties of the masking function (like Δ(vₓ) ≥ 0). But Algorithm 1 uses a non-smooth hard indicator (1(uᵢgᵢ > 0)) and a heuristic rescaling. These look inconsistent. Is this difference purely due to the hard masking being non-smooth? If so, it would be good to clarify the gap between what’s proved and what’s implemented.\n\n3. The toy experiment in §3.1 is just 2D and too simple. The LLM results in §3.2 only go up to 1.2B parameters, which is relatively small. To claim relevance to large-scale pretraining, results on models at 7B scale or above would be much more convincing.\n\n\n4. It feels odd to postpone related work to the very end (§4). This makes §2.1 hard to follow since many readers won’t be familiar with the Hamiltonian perspective. I'd recommend moving related work earlier or giving at least a brief summary in §2."}, "questions": {"value": "See comments 2 of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yBQaInyo57", "forum": "zBPZeRjfgu", "replyto": "zBPZeRjfgu", "signatures": ["ICLR.cc/2026/Conference/Submission14690/Reviewer_4Ez6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14690/Reviewer_4Ez6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837953089, "cdate": 1761837953089, "tmdate": 1762925056607, "mdate": 1762925056607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce cautious optimizers which is a modification applicable to any momentum-based optimizer where the main idea is to update parameters mainly when the proposed update direction and the gradient have the same sign. The authors show theoretically that it preserves the optimizer’s Hamiltonian structure and guarantees monotonic decrease of the loss under sufficiently small steps. Empirically, C-AdamW, C-Lion, etc. yield performance improvements in large-scale pretraining and image classification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is quite elegant can be seamlessly applied to all momentum-based optimizers without introducing new hyperparameters. \n\n- The authors situate the method within the Hamiltonian Descent framework, which were used to analyze Adam and Lion. Theorem 2.1 and Corollary 2.2 offers a clear interpretation of cautious masking as energy-preserving damping.\n\n- The discrete-time results confirm that each cautious step decreases the loss more efficiently than the base optimizer under µ-smoothness.\n\n- The experiments show improvements without re-tuning hyper-parameters — showing that the cautious modification preserves hyperparameter stability.\n\n- Overall, writing in the paper is clear."}, "weaknesses": {"value": "- The claim that cautious optimizers “do not get stuck at non-stationary points even when the update is fully masked out” is only partially justified. The authors state that momentum dynamics will eventually realign updates and gradients, but did not provide no empirical analysis of how long this alignment requires. Maybe exploring the failure cases near saddle points or flat regions (regimes that dominate the modern deep learning optimization) could help?\n\n- Empirically, the improvements are modest and sometimes fall within the noise range of large-scale training. For example, Table 2 reports ≤ 1 % perplexity gains at max. The comparison is also relatively limited i.e., Section 4 mainly discusses related optimizers like AdamW, Lion, but omits several recent and directly relevant baselines such as AdaBelief, Adan and SOAP even when these methods employ simple directional or normalization modifications. \n\n- the effect of the scaling factor introduced in Eq. (1) on the magnitude of updates and on the effective learning-rate distribution is not thoroughly analyzed. Since α directly scales updates in proportion to the ratio, could it change the convergence behavior in anisotropic curvature regions?"}, "questions": {"value": "Please refer to the comments in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EZgGx3ZY2v", "forum": "zBPZeRjfgu", "replyto": "zBPZeRjfgu", "signatures": ["ICLR.cc/2026/Conference/Submission14690/Reviewer_CSiD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14690/Reviewer_CSiD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976349048, "cdate": 1761976349048, "tmdate": 1762925056175, "mdate": 1762925056175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a plug-in method for improving momentum-based optimization methods (such as AdamW) named as Cautious optimizers. This method applies a coordinate-wise mask according to if the proposed update direction aligns in sign with the current gradient. Theoretically, the authors analyze the method in a proposed Hamiltonian/Lyapunov framework. With smoothness-based arguments, they show that cautious optimizers preserve the base optimizer’s Hamiltonian descent, can ensure monotonic decrease of the loss and further accelerates it. Empirically, they evaluate the proposed method first on a 2D toy objective function, then on language and vision tasks. Consistent improvements on convergence rates and training performances are shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes cautious optimizers, a single yet effective trick for improving momentum-based gradient methods. The method is lightweight and easy to implement.  \n2. The authors argue the soundness of their proposed approach from a Hamiltonian/Lyapunov perspective, which might be of individual interest for future optimizer design.  \n3. Performance gains are shown empirically across different tasks, demonstrating the effectiveness and wide applicability of the method."}, "weaknesses": {"value": "1. The LLM pretraining experiments use the same learning rate when comparing with original AdamW and Lion. This might not be a fair comparison since cautious optimizers use a rescaling factor $\\\\alpha$. Although the rescaling factor can be normalized, it is still hard to say if the same learning rate means the same thing for different optimizers.  \n2. The reported performance gains in Table 3 are modest, making interpretation sensitive to data and random noise. Reporting the number of random seeds tried would strengthen the claims.\n\nMinor comments: Typo in line 390: “catuious”."}, "questions": {"value": "Why is weight decay not included in cautious optimizers but performed after it? What if one uses other regularizers instead of the L2 norm, such as the KL regularizer in RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6gkbYSUave", "forum": "zBPZeRjfgu", "replyto": "zBPZeRjfgu", "signatures": ["ICLR.cc/2026/Conference/Submission14690/Reviewer_TurQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14690/Reviewer_TurQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142521218, "cdate": 1762142521218, "tmdate": 1762925055745, "mdate": 1762925055745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}