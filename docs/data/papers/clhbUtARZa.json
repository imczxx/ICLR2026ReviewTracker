{"id": "clhbUtARZa", "number": 5713, "cdate": 1757928532436, "mdate": 1763689637931, "content": {"title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video", "abstract": "Building digital twins of articulated objects from monocular video presents an essential challenge in computer vision, which requires simultaneous reconstruction of object geometry, part segmentation, and articulation parameters from limited viewpoint inputs. Monocular video offers an attractive input format due to its simplicity and scalability; however, it's challenging to disentangle the object geometry and part dynamics with visual supervision alone, as the joint movement of the camera and parts leads to ill-posed estimation. While motion priors from pre-trained tracking models can alleviate the issue, how to effectively integrate them for articulation learning remains largely unexplored. To address this problem, we introduce VideoArtGS, a novel approach that reconstructs high-fidelity digital twins of articulated objects from monocular video. We propose a motion prior guidance pipeline that analyzes 3D tracks, filters noise, and provides reliable initialization of articulation parameters. We also design a hybrid center-grid part assignment module for articulation-based deformation fields that captures accurate part motion. VideoArtGS demonstrates state-of-the-art performance in articulation and mesh reconstruction, reducing the reconstruction error by about two orders of magnitude compared to existing methods. VideoArtGS enables practical digital twin creation from monocular video, establishing a new benchmark for video-based articulated object reconstruction.", "tldr": "We introduce VideoArtGS, a novel approach that reconstructs high-fidelity digital twins of articulated objects from monocular video with accurate joint parameters and part-level geometry.", "keywords": ["Articulated Object Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad46163c177cd4dd051977dcdf386f2e04a07771.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of reconstructing articulated objects from monocular video, which requires simultaneously estimating geometry, part segmentation, and articulation parameters. The challenge lies in disentangling object geometry from part dynamics when both camera and object parts move together. The paper introduces VideoArtGS, which reconstructs digital twins through: (1) a motion prior guidance pipeline that analyzes 3D tracks and initializes articulation parameters, and (2) a hybrid center-grid part assignment module for accurate part motion capture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively addresses articulated object modeling from monocular video by incorporating motion priors from pre-trained tracking models to resolve reconstruction ambiguities.\n\n2. The hybrid center-grid part assignment module combined with the motion prior guidance pipeline offers an end-to-end framework that handles both initialization and accurate part motion modeling."}, "weaknesses": {"value": "1. The VideoArtGS-20 testing set should not be listed as a contribution, as it simply consists of 20 rendered objects from the existing PartNet-Mobility dataset without significant added value.\n\n2. The reconstructions lack smoothness and exhibit really coarse surfaces, particularly evident in real-world examples in Figure 4 (e.g., the laptop and chair). The geometry appears very noisy.\n\n3. Using GPT-4o to predict joint number and type seems inconsistent with your overall approach. Since you leverage motion priors for other components of the pipeline, why not also predict joint number and type using motion priors?\n\n4. Lack of video demonstrations: Since this work focuses on modeling articulated objects from videos, video results showing temporal consistency and motion would strengthen the demonstration. Static images alone do not fully showcase the method's capabilities.\n\n5. Missing texture visualization: Given that you use Gaussian Splatting as your representation with rendering loss for supervision, I would expect to see textured results in your visualizations."}, "questions": {"value": "Please see the weaknesses. I am open to raising my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NcTypq0FOy", "forum": "clhbUtARZa", "replyto": "clhbUtARZa", "signatures": ["ICLR.cc/2026/Conference/Submission5713/Reviewer_PkAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5713/Reviewer_PkAR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491617005, "cdate": 1761491617005, "tmdate": 1762918213036, "mdate": 1762918213036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (1 / 2)"}, "comment": {"value": "We sincerely thank reviewers 4fq7, L1iA, b5x2, and PkAR for their valuable feedback, especially highlighting our contributions on addressing an important challenging problem (4fq7, L1iA),  achieving large improvements over prior methods with strong ablations (4fq7, L1iA, b5x2), and effective technical designs (PkAR). We follow comments and suggestions from all reviewers and revise our manuscript (colored in blue) accordingly as follows:\n\n1. **Reliance on upstream priors.** (Reviewer 4fq7, L1iA, b5x2)\n- **VGGT for depth and pose estimation**\n    - **Standard Modular Paradigm**: Given monocular RGB input, pose and depth estimation are a prerequisite. We regard the use of upstream estimators as a standard practice, analogous to how static scene reconstruction methods universally rely on COLMAP without diminishing their specific contributions. \n    - **Estimator-Agnostic Framework**: We adopt VGGT from SpatialTrackerV2 merely as an off-the-shelf tool due to its robustness in dynamic scenes. However, our framework is agnostic to the specific upstream estimator and can flexibly incorporate any state-of-the-art method (e.g., MegaSaM or Depth-Anything-V3) that provides good estimation.\n\n- **TAPIP3D for motion priors.** \n    - **Rationale for 3D Motion Priors**: We posit that explicit motion priors are essential for disambiguating complex articulated objects from monocular video. We select a native 3D tracker (TAPIP3D) rather than lifting 2D tracks (e.g., CoTracker3) into 3D space. This decision is based on our observation that lifting 2D tracks introduces significantly magnified errors and noise.\n\t- **Tracker-Agnostic & Evolvable**: While TAPIP3D serves as a robust initialization, our framework is tracker-agnostic. It is designed to benefit seamlessly from the rapid advancements in pre-trained tracking models—as upstream trackers improve, our initialization improves naturally.\n\t- **Robustness & Refinement**: We treat the output of TAPIP3D as a noisy initialization rather than ground truth. A key contribution of our pipeline is the ability to filter noise and refine these tracks. Our learnable hybrid center-grid assignment module is optimized via gradient descent, allowing the network to dynamically correct tracking errors during reconstruction. As demonstrated in **Appendix C.2**, our method not only tolerates input noise but also effectively improves the track quality in the final output.\n\n- **GPT-4o for part number and joint types.**  \n    - **Appropriate scope**: Instead of asking for precise geometric parameters, we restrict GPT-4o to predict only discrete structural priors (joint count and types). This strategy leverages the model's semantic understanding while avoiding the inaccuracies typically associated with LLM spatial reasoning.\n    - **Robustness over Heuristics**: Compared to motion-based heuristics (e.g., [1] or ArtGS), which rely on handcrafted assumptions like motion thresholds, GPT-4o demonstrates superior generalization on in-the-wild data.\n    - **Performance Analysis**: As detailed in **Appendix C.5**, this approach is highly effective. On Video2Articulation-S (73 videos), GPT-4o made only 2 errors (misclassified joint types due to subtle motion). On VideoArtGS-20, it had 3 errors (underestimating parts in complex objects). These failure cases are mostly limited to scenarios with ambiguous motion or extreme structural complexity.\n\n- **Clarification on Complexity and Scalability.**\n    - **Modular Design Philosophy**: We clarify that our pipeline is designed as a modular framework rather than a monolithic system tied to specific models. Our pipeline adopts a \"plug-and-play\" modular design. Tools like VGGT, TAPIP3D, and GPT are instantiated components, not hard constraints.\n    - **Automatic Performance Gains**: A key advantage of this design is that our method naturally evolves with these foundation models. For instance, replacing VGGT with \"Depth-Anything-V3\" would immediately boost our reconstruction quality.\n\n[1] Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Cloud.\n\n2. **Static-frame assumption**. (Reviewer 4fq7, L1iA)\n- **A Necessary Trade-off for Quality**: We acknowledge this assumption (discussed in Appendix B). However, we argue that this is a strategic trade-off. Monocular articulated reconstruction is fundamentally ill-posed: disentangling complex articulation from rigid motion and shape deformation without multi-view cues is extremely difficult. \n- **Practical applicability.** Our method is already applicable to self-captured videos where users can easily provide a static initialization, covering substantial use cases in robotics and AR applications.\n- **Comparable to prior work.** Previous methods (Video2Articulation, RSRD) make similar or even stronger assumptions about input constraints.\n- **Future Directions**: We have identified this as a key area for future work and believe integrating generative priors can relax this requirement."}}, "id": "8cCVhSeOFK", "forum": "clhbUtARZa", "replyto": "clhbUtARZa", "signatures": ["ICLR.cc/2026/Conference/Submission5713/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5713/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5713/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763690780263, "cdate": 1763690780263, "tmdate": 1763715819651, "mdate": 1763715819651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VideoArtGS, a method for reconstructing articulated 3D objects in a scene from videos. It starts from depth and motion estimation from VGGT and Tapip3d, and cluster points with similar motion pattern into parts. Then optimizes a gaussian deformation field with rendering and tracking losses. It achieves better results than prior works on the Video2Articulation dataset, and the VideoArtGS-20 dataset introduced by this paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is sound. The classification of articulated motion into 4 predefined motion types makes monocular reconstruction more well-posed. \n- It represent the motion of points as a mix of rigid transformation given the distance to each part, where the static parts are handled separately with \"staticness\" logit. This hybrid assignment is simple and seems effective for separating movable vs. base parts\n- It introduces a new VideoArtGS-20 dataset and reports of large improvements over prior art\n- Strong ablations showing the improvement from motion-prior init and other components."}, "weaknesses": {"value": "- Contributions are slightly incremental. The core modeling designs are similar to ArtGS. Gaussians + deformation deformation fields is established in prior works; most novelty is in initialization of motion type and assignment of points to parts.\n- Heavy reliance on upstream systems (VGGT, TAPIP3D, GPT-4o). This could make the pipeline complex and less scalable."}, "questions": {"value": "- Can authors clarify the use of GPT-4o vs 3D tracking, in terms of identifying the motion type and clustering?\n- The deformation in Eq(1) assigns a soft weighting to each point, while this makes sense from an optimization perspective, the motion most of the objects/furnitures shown in the paper are controlled by a single rigid transformation. \n- How is the occluded part of the scene represented/reconstructed and what happens if some part of the geometry is missing in the input video? e.g., the back of the table\n- Are there motion types that are excluded by the 4 motion types used in the paper, and if so, how to combine those?\n- The reconstruction results look not as high quality as recent 3d object/part generation models. Potentially combining those would improve the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BTeRW2Liam", "forum": "clhbUtARZa", "replyto": "clhbUtARZa", "signatures": ["ICLR.cc/2026/Conference/Submission5713/Reviewer_b5x2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5713/Reviewer_b5x2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524140187, "cdate": 1761524140187, "tmdate": 1762918212701, "mdate": 1762918212701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for reconstructing articulated objects from a single monocular video. The authors address the  problem by using two main components: first, a motion prior guidance pipeline that analyzes and filters 3D tracks from an upstream model (TAPIP3D) to initialize articulation parameters, and second, a hybrid center-grid part assignment module to segment static bases from movable parts. The method utilizes an articulation-based deformation field over 3D Gaussian Splatting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work addresses the challenging problem of articulated object reconstruction from  monocular video.\n2. The method achieves a significant reduction in error compared to baselines and demos are looking good."}, "weaknesses": {"value": "1.  The pipeline's success is critically dependent on the quality of two separate pre-trained models: VGGT for depth/pose and TAPIP3D for 3D tracks. This is a \"garbage in, garbage out\" system, and failures in these upstream models will cascade.\n\n2. Part segmentation is derived entirely from motion clustering. This will inherently fail for parts that are not moved in the video, parts with very subtle motion, or objects with many parts moving simultaneously. Or if the parts are not moved to its maximum extend. There could be more information like semantic priors to be utilized.\n\n3. The method relies on GPT-4O to predict the number of joints and their types (revolute/prismatic). This is a significant external dependency and a potential point of failure that is not analyzed.\n\n4. The motion pattern analysis (Sec 3.2) involves several components (RANSAC, SVD fitting) and thresholds. This pipeline may be brittle and highly tuned. Thus the generalization ability could be a concern. \n\n5. On the new VideoArtGS-20 dataset, the authors compare against methods (like Video2Articulation) that were not designed for multi-part objects and had to be manually adapted, which may inflate the performance gap. Similar to the question1 I raised, I doubt the good performance is highly tuned with its own carefully contructed dataset and might not generalize well.\n\n6. The method requires the video to begin with a static sequence to initialize the canonical 3D Gaussians. This is a major limitation that prevents its application to most in-the-wild videos."}, "questions": {"value": "1. In real world, many articulated objects don't move themselves. The would be humans/robots operate them to open and close the prismatic and revolute joints. This would definately result in occlusion. In the demos, human operators carefully move the objects to prevent occlusion. But if we use web videos where people operates them normally, how would you consider the occlusion problems? As I mentioned in weaknesses 6, the method relies on static start, if the inner part of a prismatic part like a drawer was occluded in the dynamic part, the method would lilely to fail?\n\n2. Since the demo results looks good, I will give my initial score as marginally above the acceptance threshold. But those weaknesses I listed I still got many concerns on the work. I will consider the reviews from other reviewers and might adjust my score if my concerns are not resolved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zDTtmwOzqg", "forum": "clhbUtARZa", "replyto": "clhbUtARZa", "signatures": ["ICLR.cc/2026/Conference/Submission5713/Reviewer_L1iA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5713/Reviewer_L1iA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993387797, "cdate": 1761993387797, "tmdate": 1762918212478, "mdate": 1762918212478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VideoArtGS, a system for reconstructing articulated object digital twins—including geometry, segmentation, and joint parameters—from monocular videos. The method builds upon Gaussian Splatting frameworks and integrates 3D motion priors and hybrid part assignments to recover both geometry and kinematics.\n\nMotion Prior Guidance: It uses precomputed 3D trajectories (via TAPIP3D) and adaptive fitting (line/circle) to distinguish prismatic vs. revolute joints. The fitted parameters (axis, anchor, and joint state) are optimized using bidirectional trajectory consistency (c2o and o2o) to ensure temporal coherence.\n\nHybrid Center–Grid Assignment: The paper proposes a mixed scheme that assigns Gaussians to movable parts based on a center–Mahalanobis distance metric, and to static regions using a voxel-hash grid.\n\nOptimization: The framework assumes the first N frames are static for canonical initialization, then jointly optimizes geometry and motion over the entire sequence using a photometric and consistency loss.\n\nExperiments on Video2Articulation-S and a newly introduced VideoArtGS-20 dataset show strong improvements over Video2Articulation and ArticulateAnything, both quantitatively (two orders of magnitude reduction in axis and position errors) and qualitatively (cleaner reconstructions and more realistic part motion)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Addresses an important real-world problem: reconstructing interactable articulated objects from monocular video.\n\nIntegrative framework: elegantly combines motion priors, kinematic reasoning, and differentiable Gaussian rendering.\n\nSolid empirical improvements: consistent quantitative and qualitative gains over existing baselines.\n\nAblation studies (motion prior and hybrid assignment removal) are provided and clearly demonstrate their necessity.\n\nThe method seems reproducible in principle, suggesting strong implementation work."}, "weaknesses": {"value": "Over-reliance on upstream priors: The system’s success depends on TAPIP3D and VGGT, yet no robustness or substitution tests are conducted (e.g., replacing with weaker or noisier trackers).\n\nUnrealistic static-frame assumption: The model presumes the first N frames are motionless; this limits generalization to natural videos. No adaptive mechanism or failure handling is proposed.\n\nLimited generalization evidence: The “two orders of magnitude” improvement is mainly on synthetic or semi-synthetic datasets; real-world, unconstrained sequences are underrepresented.\n\nInsufficient dataset transparency: Details of VideoArtGS-20’s annotation, coordinate conventions, and release availability are not discussed.\n\nLack of failure analysis: No qualitative examples of difficult cases (occlusion, partial visibility, transparent parts, trajectory drift) are shown.\n\nUnreported runtime and efficiency: The training time, memory footprint, and inference speed are not compared with prior works.\n\nTerminology and claims: The term “digital twin” suggests full physical consistency, but the framework models only geometric and kinematic properties, not physical constraints."}, "questions": {"value": "How does performance change if TAPIP3D is replaced or degraded? Please provide robustness tests under noisy trajectories or alternative trackers.\n\nCan the model relax the static N-frame assumption? For example, by learning canonical frames adaptively or detecting stationary segments automatically?\n\nHow are prismatic vs. revolute joints differentiated numerically—what thresholds or heuristics are used for line/circle fitting, and how sensitive are results to them?\n\nHow are mixed center–grid weights tuned? Are they learned jointly or fixed heuristically?\n\nPlease provide runtime and GPU memory comparisons with Video2Articulation and ArticulateAnything.\n\nAre the reported “two orders of magnitude” improvements averaged across all categories, or do some categories dominate?\n\nWill VideoArtGS-20 be publicly released with ground-truth joint annotations and evaluation code? If not, how can others reproduce the reported metrics?\n\nHow does the method handle complex real-world artifacts (e.g., flexible cables, soft hinges) that break rigid joint assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oBYJhcuh38", "forum": "clhbUtARZa", "replyto": "clhbUtARZa", "signatures": ["ICLR.cc/2026/Conference/Submission5713/Reviewer_4fq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5713/Reviewer_4fq7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762039163426, "cdate": 1762039163426, "tmdate": 1762918212181, "mdate": 1762918212181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}