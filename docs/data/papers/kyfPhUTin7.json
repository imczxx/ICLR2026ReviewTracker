{"id": "kyfPhUTin7", "number": 721, "cdate": 1756777316339, "mdate": 1763038222184, "content": {"title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models", "abstract": "Understanding surveillance video content remains a critical yet underexplored challenge in vision–language research, particularly due to its real-world complexity, irregular event dynamics, and safety-critical implications. In this work, we introduce SurveillanceVQA-589K, the largest open-ended video question answering (VQA) benchmark tailored to the surveillance domain. The dataset comprises 589,380 QA pairs spanning 12 cognitively diverse question types, including temporal reasoning, causal inference, spatial understanding, and anomaly interpretation, across both normal and abnormal video scenarios. To construct the benchmark at scale, we design a hybrid annotation pipeline that combines temporally aligned human-written captions with Large Vision-Language Model~(LVLM) assisted QA generation using prompt-based techniques. We also propose a multi-dimensional evaluation protocol to assess contextual, temporal, and causal comprehension. We evaluate 12 LVLMs under this framework, revealing significant performance gaps, especially in causal and anomaly-related tasks, underscoring the limitations of current models in real-world surveillance contexts. Our benchmark provides a practical and comprehensive resource for advancing video-language understanding in safety-critical applications such as intelligent monitoring, incident analysis, and autonomous decision-making. The dataset is publicly available at: https://anonymous.4open.science/r/SurveillanceVQA-589K.", "tldr": "SurveillanceVQA-589K provides a practical and comprehensive resource for advancing video-language understanding in safety-critical applications.", "keywords": ["multimodal learning; benchmark; surveillance video;QA"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d022d5727124e5af71e89a4e3974eef708d6fa4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the underexplored area of surveillance video understanding within the vision-language research community. To this end, the authors introduce SurveillanceVQA-589K, a large-scale open-ended video question answering (VQA) benchmark specifically designed for the surveillance domain. The dataset is constructed through a semi-automated pipeline that combines human-written captions with outputs from large vision-language models. It encompasses diverse reasoning dimensions, including temporal reasoning, causal inference, spatial understanding, and anomaly interpretation, covering both normal and abnormal surveillance scenarios. Using this benchmark, the authors demonstrate that existing vision-language models exhibit significant limitations in handling real-world surveillance tasks, particularly those involving causal reasoning and anomaly detection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well-organized.\n2. The proposed SurveillanceVQA-589K benchmark seems to extend beyond simple descriptive tasks to include higher-level reasoning such as logical reasoning, causal inference, and complex semantic comprehension of video content.\n3. The authors perform a comprehensive experimental evaluation using a diverse set of LVLMs, providing valuable insights into their performance and limitations in surveillance-related scenarios."}, "weaknesses": {"value": "1. Although the benchmark includes diverse question types such as causal, temporal, and spatial reasoning, these dimensions have already been explored in prior video understanding benchmarks (e.g., VideoMME [1], MVBench [2]). The main novelty thus lies primarily in the surveillance domain rather than in the reasoning taxonomy itself, which somewhat limits the originality of the contribution. To enhance its distinctiveness, the benchmark could introduce evaluation dimensions specifically tailored to abnormal or security-critical scenarios in surveillance contexts (e.g., predictive reasoning about potential incidents or prevention strategies), rather than directly adapting question types from existing datasets.\n\n2. The quantitative differences among evaluated LVLMs are relatively small, raising concerns about the sensitivity and reliability of the proposed evaluation protocol. Incorporating a subset of structured question types, such as multiple-choice or constrained-answer formats, could improve the robustness and interpretability of performance comparisons across models.\n\n3. The paper lacks a thorough analysis explaining why the fine-tuned models yield modest performance gains compared to their zero-shot counterparts, despite being trained on a substantial number of samples. Simply attributing this to overfitting is insufficient without empirical evidence or diagnostic analysis. A deeper investigation-such as examining data distribution shifts, label noise, or domain mismatch between training and evaluation splits-would strengthen the credibility of this claim and provide more meaningful insights into model behavior.\n\n\n[1] Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis\n\n[2] MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"}, "questions": {"value": "1. How much time is required to complete the full annotation pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PSJVUQ0rUh", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Reviewer_DCiy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission721/Reviewer_DCiy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630303448, "cdate": 1760630303448, "tmdate": 1762915589774, "mdate": 1762915589774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Withdraw Statement"}, "comment": {"value": "We appreciate the reviewers’ time, evaluation, and feedback on our manuscript. However, after carefully comparing the review comments with the content of our paper, we find that there are numerous evident inconsistencies between the critiques and the research we have presented. To avoid an unproductive rebuttal process and extended discussion, we have decided to voluntarily withdraw our submission.\n\nWe would like to briefly clarify the core contributions of our work and how it differs from prior research, such as the UCA dataset. SurveillanceVQA introduces substantial innovations in both task formulation and domain modeling. Unlike UCA, which primarily focuses on text captioning, SurveillanceVQA establishes a comprehensive multi-dimensional QA framework containing 589,380 question–answer pairs, designed to satisfy multi-level cognitive understanding needs ranging from normal surveillance scenarios to anomalous events. The benchmark systematically incorporates 12 types of QA tasks, including spatial reasoning, temporal reasoning, and causal reasoning, thereby constructing for the first time a structured QA evaluation framework tailored to surveillance video understanding.\n\nFurthermore, in building the dataset, we employed manual review and multi-stage quality control procedures, with detailed workflows and statistical analyses provided in the appendix. These measures ensure high data reliability and annotation consistency, offering strong support for the validity of our research conclusions. For these reasons, we believe the reviewers’ concerns regarding data quality and insufficient task innovation do not accurately reflect the actual characteristics of our work.\n\nGiven the substantial discrepancies between the review comments and the content of our manuscript, we have decided to withdraw the submission and will further refine and improve the paper in future versions. We remain confident that this research carries unique value for the fields of surveillance video understanding and security-oriented video question answering, and that it will make a meaningful contribution to the development of intelligent surveillance video analysis."}}, "id": "F5oJee52Yo", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission721/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763038192177, "cdate": 1763038192177, "tmdate": 1763038192177, "mdate": 1763038192177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Given the substantial discrepancies between the review comments and the content of our manuscript, we have decided to withdraw the submission and will further refine and improve the paper in future versions. We remain confident that this research carries unique value for the fields of surveillance video understanding and security-oriented video question answering, and that it will make a meaningful contribution to the development of intelligent surveillance video analysis."}}, "id": "orsLcGBG4w", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763038220829, "cdate": 1763038220829, "tmdate": 1763038220829, "mdate": 1763038220829, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SurveillanceVQA-589K, a large-scale benchmark dataset for Video QA designed for the surveillance domains, with 589K QA pairs about 12 diverse question types. The collection is based on a hybrid annotation pipeline which combines human-written captions with LLM assistance to generate the QA pairs at scale. The paper evaluates 12 different models on this new benchmark, notably showing that current models struggle with complex reasoning, especially in identifying the causes of abnormal events and predicting their results. The authors also show that fine-tuning models on this dataset provides some gains, but doesn't solve the core reasoning challenges and can lead to catastrophic forgetting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Large scale of this dataset compared to prior work\n- Evaluation including both multiple open source and proprietary models"}, "weaknesses": {"value": "- Simple baselines like text-only, single-image and human evaluation are not presented and would help assessing the benchmark.\n- The fact that fine-tuning on the data does not help much is worrying and could suggest issue in the generated data (e.g. lack of diversity of the generated QAs)."}, "questions": {"value": "- Is there any analysis on how much the consolidated caption (or ultimately the generated QA) rely on the human or model caption? e.g. can the QA be answered from one of these two captions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This is a paper about surveillance which is a field that can raise some ethical concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lwWtwayF6V", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Reviewer_6V5t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission721/Reviewer_6V5t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932311609, "cdate": 1760932311609, "tmdate": 1762915589659, "mdate": 1762915589659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a VideoQA benchmark for comprehensive surveillance video understanding. The dataset comprises both normal and abnormal video clips and related questions. The questions are AI generated based on video narratives annotated by human or generated by AI Models. It then provides benchmark results of modern large multimodal models and analyzes their strengths and weakness from multiple aspects based on question categories. The findings point to better causal reasoning as well as more data for finetuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tIt constructs a large-scale dataset for comprehensive cross-modal surveillance video understanding. The dataset could be of significant value for social security.\n2.\tIt provides well-classified questions, which would benefit model analysis.\n3.\tIt provides results of SOTA LVLMs, analyze their success and failure points, and gives suggestions for future research."}, "weaknesses": {"value": "1.\tThe dataset seems to be biased to LLaVA and Qwen series of modes, since LLaVA-Video and Qwen-Max are employed for caption and QA generation respectively. The results in Table 3 and 4 show that LLaVA-Video outperform other opensource models, which further confirms my concern.\n2.\tThe QAs are directly generated by Qwen-max, without further human checking for answer correctness. This raises concern about QA quality.\n3.\tSome basic data statistics should be moved to the main text, for example, what is the size (number of videos and QAs) of the train and test set？\n4.\tFigure 4 (b), while the title shows QA example, there is no answer for the question.\n5.\tThe evaluation metric is vague. It is unknow how well the judgement aligns with human reviewers.\n6.\tThe current analyses of model behaviors in Table 3 and 4 are unconvincing. The authors ignore an important fact that the QAs are generated by LLaVA-Video and Qwen-Max, which could give strong priors for LLaVA and Qwen series of models during testing. Also, why not provide the results of Qwen2.5-VL 7B? Additionally, a human performance would be helpful to better understand the model behaviors and the soundness of the evaluation metric.\n7.\tI suggest the authors to provide the average results of each model for better understanding in Table 4.\n8.\tThe suggestion about researching on casual reasoning and domain-adaptive pretraining is not interesting. Many other datasets can support such conclusions. It would be better to summarize more specific challenges and suggestions for surveillance video understanding."}, "questions": {"value": "1.\tIn Table 4, does finetuning with normal video QA data help abnormal video QA? It would be interesting to see such analyses.\n2.\tWhat are the results of API-called models for normal video qa?\n3.\tCan the benchmarked challenge simply be solved by collecting more data for training? If not, what challenges cannot be well solved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0lB2lQkWpq", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Reviewer_T6N1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission721/Reviewer_T6N1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377515024, "cdate": 1761377515024, "tmdate": 1762915589508, "mdate": 1762915589508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on video understanding in surveillance scenarios and introduces a large-scale benchmark dataset called SurveillanceVQA-589K. The dataset includes over 580K open-ended QA pairs covering both normal and abnormal events, aiming to support semantic and temporal reasoning research in surveillance contexts. The authors use a hybrid of human and llm-generated questions, and evaluate model performance through an llm-based scoring system. Several existing multimodal models are benchmarked to demonstrate the task’s difficulty and provide baseline results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A key strength of this paper is its attempt to extend video question answering into the practically important domain of surveillance. The proposed large-scale dataset and the use of open-ended QA with a hybrid generation strategy reflect a degree of originality in task formulation. The paper is well structured, and the methodology and evaluation framework are clearly described. The experimental section includes multiple mainstream multimodal models, offering a broad view of current model performance in this setting. Given the practical significance of surveillance scenarios, this benchmark has potential impact for both research and applied domains."}, "weaknesses": {"value": "This paper has several notable weaknesses. \n\nFirst, the originality and incremental contribution are limited. The main differences from existing surveillance QA datasets, such as UCA, lie primarily in scaling up the dataset and changing the evaluation protocol, rather than introducing any fundamental advances in task design or domain modeling. \n\nSecond, the open-ended QA format places the task awkwardly between captioning and traditional VQA, failing to provide either the strict semantic alignment of captioning tasks or the controllability and diagnostic power of structured VQA. This ambiguity weakens the scientific rigor and interpretability of the task itself. \n\nThird, the evaluation relies entirely on LLM-as-a-judge, which is inherently subjective and highly sensitive to prompt wording and model versions. This undermines reproducibility and makes it difficult to attribute errors or analyze model capabilities meaningfully. \n\nFourth, the work does not effectively exploit the distinctive properties of surveillance videos (such as fixed viewpoints, low resolution, small targets, long temporal spans, and sparse anomalies) to inform task or evaluation design. Thus, the claimed “surveillance-specific” aspect remains superficial. \n\nOverall, the contribution is largely an engineering-scale extension rather than a conceptual or methodological breakthrough. Strengthening task definition, grounding the design in surveillance-specific characteristics, and adopting more robust, structured evaluation strategies would significantly improve the work."}, "questions": {"value": "1) Could the authors clarify the fundamental differences between SurveillanceVQA and existing surveillance QA datasets such as UCA? Beyond scaling up and changing the evaluation protocol, is there any substantial innovation in task formulation or domain modeling?\n\n2) Surveillance videos have distinctive properties such as long temporal spans, low resolution, small targets, and fixed viewpoints. Did the authors consider explicitly incorporating these characteristics into task design or evaluation metrics?\n\n3) Since LLM-as-a-judge is inherently subjective, did the authors conduct any prompt sensitivity or repeated scoring tests to assess its stability and consistency?\n\n4) Have the authors considered introducing more structured or partially objective evaluation components to improve reproducibility and diagnostic power?\n\n5) Is there any analysis or ablation study that examines how surveillance-specific factors (e.g., temporal grounding challenges, small-object difficulty) influence QA performance, to demonstrate the task’s unique surveillance characteristics?\n\nThere are many benchmark papers nowadays, and I do believe such work plays an important role in advancing the field. However, what matters more to me is whether the authors clearly articulate and deeply understand the relationship between data, scenario characteristics, and the capabilities being evaluated. Simply scaling up without capturing these key aspects is less convincing to me. That said, I genuinely look forward to the authors’ responses and deeper reflections on these essential issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rC7hLVeLDx", "forum": "kyfPhUTin7", "replyto": "kyfPhUTin7", "signatures": ["ICLR.cc/2026/Conference/Submission721/Reviewer_4NkE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission721/Reviewer_4NkE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643984743, "cdate": 1761643984743, "tmdate": 1762915589374, "mdate": 1762915589374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}