{"id": "BAOO9H0ehZ", "number": 10322, "cdate": 1758167083471, "mdate": 1759897658842, "content": {"title": "POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization", "abstract": "Lead optimization in drug discovery requires efficiently navigating vast chemical space through iterative cycles to enhance molecular properties while preserving structural similarity to the original lead compound. Despite recent advances, traditional optimization methods struggle with sample efficiency—achieving good optimization performance with limited oracle evaluations. Large Language Models (LLMs) provide a promising approach through their in-context learning and instruction following capabilities, which align naturally with these iterative processes. However, existing LLM-based methods fail to leverage this strength, treating each optimization step independently. To address this, we present POLO (Preference-guided multi-turn Optimization for Lead Optimization), which enables LLMs to learn from complete optimization trajectories rather than isolated steps. At its core, POLO introduces Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning algorithm that extracts learning signals at two complementary levels: trajectory-level optimization reinforces successful strategies, while turn-level preference learning provides dense comparative feedback by ranking intermediate molecules within each trajectory. Through this dual-level learning from every intermediate evaluation, POLO achieves superior sample efficiency by fully exploiting each costly oracle call. Extensive experiments demonstrate that POLO achieves 84% average success rate on single-property tasks (2.3× better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations, significantly advancing the state-of-the-art in sample-efficient molecular optimization. Our code and data are available at https://anonymous.4open.science/r/POLO-6861/.", "tldr": "A multi-turn reinforcement learning  framework with preference learning for lead optimization.", "keywords": ["Lead Optimization", "Reinforcement Learing", "Preference Learning", "Agentic RL"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/465d668c7988c5d1a1839fc14d2fb066584e3e86.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a framework that adapts large language models (LLMs) for sample-efficient molecular lead optimization. Unlike prior approaches that optimize molecules one step at a time, POLO models the process as a multi-turn reinforcement learning (RL) problem, where the agent iteratively proposes modifications to lead compounds. Its core algorithm, Preference-Guided Policy Optimization (PGPO), combines trajectory-level policy optimization with turn-level preference learning, enabling the extraction of O(NT²) training signals from N trajectories of length T, rather than just O(N) scalar rewards. The method leverages oracle feedback for properties such as QED, plogP, and bioactivity, while enforcing similarity constraints to maintain drug-likeness. Experiments on ten optimization tasks show that POLO significantly outperforms baselines—including GA, REINVENT, and chemistry-specific LLMs—achieving up to much higher success rates within a budget of 500 oracle evaluations. Overall, the paper demonstrates that combining multi-turn RL with preference learning yields state-of-the-art sample efficiency and robustness for molecular optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Essential problem domain**: The paper tackles lead optimization in drug discovery, a field of high scientific and practical importance where each oracle evaluation can be very costly. \n2. **Clear motivation and logical presentation**: The authors present a well-structured narrative: they identify the limitations of existing RL and LLM-based methods, motivate the need for multi-turn reasoning, and clearly explain how Preference-Guided Policy Optimization (PGPO) fills this gap. The logic connecting problem setup, method design, and evaluation is easy to follow. \n3. **Interesting method with validation**: The dual-level learning approach—combining trajectory-level PPO with turn-level preference learning—is novel and interesting. The method is validated across multiple tasks. While there are some issues (in Weaknesses 2), the experiments provide convincing support for the claimed benefits."}, "weaknesses": {"value": "1. **Necessity of the RL and preference design**: In this setup, every molecule can be directly assigned a scalar score by the oracle. A straightforward design would be to use these scores as dense signals for training. The introduction of preference-based learning raises the question of whether relative comparisons are strictly necessary here, as they may discard some information from the original scores.\n2. **Evaluation setup**:\n   - **Oracle budget**: The budget of 500 evaluations is arguably too low compared with realistic practice. For example, the PMO benchmark [1] adopts a budget of 10,000 evaluations. The strict budget in this paper may disproportionately disadvantage strong non-LLM baselines such as Reinvent 4.\n   - **Task complexity**: The chosen oracles (QED, plogP, SA, DRD2, JNK3) have been criticized as relatively simple, and pretrained LLMs may already encode prior knowledge about them. More challenging objectives, such as docking scores [2], would provide a more realistic and convincing testbed.\n3. **Variability across leads**: Different initial lead molecules can yield highly variable optimization outcomes. Reporting only average performance risks obscuring this variability. Including statistics such as standard deviations or distribution plots would give a clearer picture of robustness across leads.\n4. **Minor issues**:\n   - Garbled characters appear in Table 1.\n   - Figures 3 and 4 seem to be mislabeled.\n   - A discussion of the limitations of the work would make the paper more balanced.\n\n[1] Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization.\n\n[2] Generative Models Should at Least Be Able to Design Molecules That Dock Well: A New Benchmark."}, "questions": {"value": "1. **SMILES randomization**: How does the method handle randomized (non-canonical) SMILES representations, which are widely used as a form of data augmentation in molecular modeling? Would randomization affect stability or performance during optimization?\n2. **Dependence on prior knowledge**: To what extent does POLO’s performance depend on the prior chemical knowledge already encoded in the pretrained LLM?\n3. **Scaling with number of leads**: In Figure 4 (d), the improvement from increasing the number of lead molecules appears not to have plateaued. Why did the experiments stop at this scale, and what do the authors expect would happen if the number of leads were further increased?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3Oj5u4cq1q", "forum": "BAOO9H0ehZ", "replyto": "BAOO9H0ehZ", "signatures": ["ICLR.cc/2026/Conference/Submission10322/Reviewer_xJaw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10322/Reviewer_xJaw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760897863852, "cdate": 1760897863852, "tmdate": 1762921661531, "mdate": 1762921661531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission focuses on enhancing the ability of large language models (LLMs) on the molecular optimization task. Notably, prior approaches focus on enhancing the model's performance via in-context learning, instead of leveraging the iterative nature of the molecular optimization task. These paradigms lead the LLM to learn from isolated optimization steps instead of the complete trajectories, thereby failing to fully utilize the past experiences. To address this, this submission proposes preference-guided multi-turn optimization for lead optimization (POLO), which introduces preference-guided policy optimization to extract learning signals from trajectory-level (traj-level) and turn-level to fully utilize the reasoning trajectory to enhance the performance. The submission conducts extensive empirical studies and justifies the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of this submission is clear: by combining turn-level and traj-level loss, the model can learn better from past experience to optimize the molecule better.\n- The submission is generally well-written, with clear illustrations and tables.\n- Extensive experiments have been conducted to provide a good insight into the components of the proposed method."}, "weaknesses": {"value": "- The proposed turn-level and trajectory-level losses are interesting but require further justification. The interpolation between on-policy RL (PPO) and off-policy RL (DPO) may introduce gradient bias, which is not theoretically analyzed or empirically verified.\n- The description of the turn-level loss is unclear. The optimization target appears to be defined on the state incorporating previous history, which could lead to double-counting the contribution of earlier states to the final outcome. Moreover, the LLM may not be able to identify which specific actions are responsible for the observed reward improvement, raising concerns about proper credit assignment.\n- The performance comparisons appear unfair. POLO is trained directly on the target properties using the constructed MolOptIns dataset, whereas other LLM-based methods rely only on prompt-based adaptation. Additionally, POLO employs evolutionary search during inference, which provides an extra advantage. It would be more convincing to evaluate other task-specific LLMs under the same inference setup to fairly assess the effectiveness of the proposed turn-level loss. Furthermore, according to the original MOLLO paper, it achieves relatively strong performance, yet in this submission, the success rate on JNK3 is reported as zero, which raises questions about the evaluation setup or implementation details."}, "questions": {"value": "1. What guarantees or empirical evidence show that the mixed objective preserves a valid policy gradient direction?\n2. How does the algorithm prevent redundant gradient accumulation from overlapping state representations? \n3. Discuss the fairness of the experiment setting, or provide additional experiments to justify to effectiveness on POLO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ksgdsHRSh2", "forum": "BAOO9H0ehZ", "replyto": "BAOO9H0ehZ", "signatures": ["ICLR.cc/2026/Conference/Submission10322/Reviewer_LZd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10322/Reviewer_LZd8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566615606, "cdate": 1761566615606, "tmdate": 1762921660999, "mdate": 1762921660999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces POLO, a framework that enables large language models to perform sample-efficient molecular lead optimization by learning from complete optimization trajectories rather than isolated steps. \nIts core algorithm, Preference-Guided Policy Optimization, integrates trajectory-level reinforcement learning, capturing long-term strategies, with turn-level preference learning, which ranks intermediate molecules to provide dense, comparative feedback. \nThis dual-level optimization extracts maximal learning from each costly oracle call, dramatically improving efficiency. \nBuilt on multi-turn reinforcement learning and augmented by similarity-aware instruction tuning, trajectory filtering, and evolutionary inference, POLO claims to reach the state-of-the-art performance surpass existing LLM- and non-LLM-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces the Preference-Guided Policy Optimization (PGPO) algorithm, which unifies trajectory-level reinforcement learning with turn-level preference learning to extract dense supervision from inherently sparse oracle feedback. \nThis paper is well structured. It also presents comprehensive ablation studies and strong empirical validation across both single- and multi-property molecular optimization tasks, reinforcing the robustness of its approach. The results demonstrate remarkable performance improvements—up to 2.3× better than baselines—while preserving chemical validity and structural similarity, highlighting both reliability and reproducibility. The paper clearly illustrates the algorithm and how multi-turn reasoning enhances optimization efficiency and ultimately establishes POLO as a generalizable paradigm for applying agentic LLMs to iterative, data-constrained scientific discovery."}, "weaknesses": {"value": "I would like to raise the following concerns about this paper,\n\n1. *Overclaiming:* The paper claims state-of-the-art performance but does not compare against established molecular or lead optimization methods, including ML-based (e.g., Random Forest), GPT-based, and Bayesian optimization approaches. Conceptually, it is difficult to justify how an LLM-driven lead optimization framework could consistently outperform specialized models designed for multi-objective reward optimization. \n\n2. *Limited novelty:* The integration of preference learning into multi-turn reinforcement learning has already been explored in prior works such as Shani et al. (NeurIPS 2024) [1]. The proposed approach does not clearly extend or theoretically improve upon these frameworks.\n\n3. *Potential Deficit in Algorithm design:* The algorithm simply combines multi-turn preference learning with trajectory-level optimization, but lacks theoretical justification on how this integration benefits policy improvement. In fact, introducing both trajectory-level rewards O(N) and multi-turn preferences O(NT²) could lead to conflicting optimization directions and unnecessary computational complexity within each policy-update step, potentially weakening convergence. [2]  By hacking the \\lambda value could make the result seems ok, but it's not theoretically ground.\n\n4. *Limited theoretical justification:* The PGPO formulation lacks a rigorous analysis explaining why combining trajectory- and turn-level signals enhances sample efficiency or stability.\n\n5. *Missing related work discussion and citations:* Although this paper focuses on LLM-based reasoning, it overlooks several closely related studies on GPT-based molecular optimization. These works are not cited or discussed, leaving the positioning of this study within the existing literature unclear.\n\n6. *Insufficient evaluation diversity:* Experiments are restricted to small-molecule optimization tasks (QED, pLogP, DRD2 on ZINC-250k) without testing scalability to larger, more chemically diverse datasets. Broader evaluations would better demonstrate robustness and real-world applicability.\n\n7. *Heuristic hyperparameter choices:* The weighting coefficient \\lambda is heuristically set. While ablation studies are presented, the sensitivity to key parameters (e.g., \\lambda, similarity threshold \\gamma, trajectory length T) is not thoroughly examined. Such analysis is essential for understanding stability and reproducibility.\n\n8. *Limited comparison to recent baselines:* The study omits relevant preference-based or multi-turn RL frameworks such as RAGEN (arXiv 2025) and LIPO (Liu et al., 2024b).\n\n9. *Weak generalization evidence:* Although the paper claims cross-task generalizability, it provides limited empirical support for transfer to unseen molecular domains. Demonstrating such adaptability would strengthen the claim that POLO constitutes a broadly generalizable paradigm.\n\n[1] Shani, Lior, et al. \"Multi-turn reinforcement learning with preference human feedback.\" NeurIPS 2024\n\n[2] \"Multi-turn Training with basic human feedback helps little on LLM reasoning\",  arXiv 2025"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oufqQ2wE8m", "forum": "BAOO9H0ehZ", "replyto": "BAOO9H0ehZ", "signatures": ["ICLR.cc/2026/Conference/Submission10322/Reviewer_ZM6i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10322/Reviewer_ZM6i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898321479, "cdate": 1761898321479, "tmdate": 1762921660507, "mdate": 1762921660507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}