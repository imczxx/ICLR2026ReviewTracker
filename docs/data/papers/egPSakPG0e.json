{"id": "egPSakPG0e", "number": 4162, "cdate": 1757617161671, "mdate": 1759898049661, "content": {"title": "Beyond Single Views: Achieving Significant Gains in Text Clustering via Informative Diversification", "abstract": "Clustering text into coherent groups is a long-standing challenge, complicated by high-dimensional embeddings, semantic ambiguity, and distributional shifts in unseen data. Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) systems have further underscored the need for robust and scalable knowledge representation methods. In this work, we introduce a novel clustering framework based on informative diversification. Our method applies a set of semantic-preserving transformations to generate multiple views of the data, and then harnesses their collective structure through a spectral consensus process. We prove that consensus clustering achieves an exponentially lower expected error rate compared to any single view, provided the views are diverse and informative. We then propose an iterative co-training procedure that learns a cluster-friendly latent space by jointly minimizing a contrastive InfoNCE loss and a Gaussian mixture negative log-likelihood loss. This training sharpens assignments and pulls embeddings toward their cluster centroids, while dynamically updating cluster assignments to accommodate the evolving latent space. The result is a robust and generalizable model that not only outperforms baselines on benchmark datasets but also maintains strong accuracy on unseen text, making it a powerful tool for real-world knowledge discovery and retrieval-augmented generation systems.", "tldr": "We show that “many views are better than one” — by diversifying embeddings and co-training with consensus, we get clusters that are sharper, provably more accurate, and generalize even to unseen text.", "keywords": ["Informative Diversification", "Consensus Clustering", "Multi-View Embeddings", "Gaussian Mixture Models", "Contrastive Learning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b136f7904785ca4111641953e06b18180b684014.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new multi-view consensus clustering framework for text data, designed to overcome the limitations of single-view clustering methods. It leverages informative diversification—creating multiple, semantically varied versions of embeddings—to achieve lower misclustering error and higher robustness.\n\nThe paper theoretically proves that, in multi-view consensus clustering, the expected misclustering error decays exponentially with the number of views (m), under diversity and informativeness conditions, whereas single-view clustering retains a positive lower bound on the misclustering rate.\n\nThe proposed methodology is three-fold: 1. Multi-view generation;\n2.Consensus clustering, and 3. Latent space learning.  Experimentally,\nthe proposed consensus clustering consistently outperforms baseline\nmethods such as K-Means, single-view GMM, and Spectral Clustering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel approach that integrates multiple\nsemantically diverse embeddings (“views”) into a spectral consensus\nclustering framework. This idea—aggregating information from\ndifferent embedding transformations—represents a creative extension\nof ensemble learning to modern text clustering, improving stability\nand robustness over single-view methods.\n\n\n2. It provides formal proofs showing that the expected misclustering\nerror decreases exponentially with the number of independent,\ninformative views.  The derivations connect clustering performance\nwith statistical guarantees, offering a clear theoretical\njustification for why and when multi-view consensus is superior.\n\n\n3. The method elegantly combines contrastive learning (InfoNCE loss) with\nGaussian Mixture Modeling (GMM) in a joint optimization loop.  This\nhybrid objective balances representation learning and cluster density\nmodeling, yielding embeddings that are semantically rich.\n\n\n4. Experiments show consistent improvements over baseline methods\n(K-Means, GMM, Spectral Clustering) across multiple configurations.\nThe model maintains robust clustering accuracy on unseen data,\ndemonstrating generalization beyond the training set—a key challenge\nin unsupervised learning.\n\n\n5. The algorithms (Algorithm 1 and 2) are clearly described,\nstep-by-step, with well-defined mathematical notation and transparent\ndesign choices (e.g., transformation types, consensus computation).\nThe combination of deterministic and stochastic transformations (like\nPCA, WPT, Gaussian noise) provides practical reproducibility for\nfuture studies."}, "weaknesses": {"value": "1. The evaluation is narrow, using only two clean English datasets\n(DBPedia and Reuters R8). There’s no evidence of scalability to\nlarge, noisy, or multilingual corpora, nor any analysis of\ncomputational cost.\n\n\n\n2. The proof of exponential error reduction assumes that the multiple\nviews are independent and informative. In reality, the generated views\n(e.g., PCA or similar BERT models) are highly correlated, so these\nconditions are unlikely to hold.\n\n\n3. The method is only compared against basic clustering algorithms\n(K-Means, GMM, Spectral Clustering), omitting modern deep or\ncontrastive clustering baselines. There is also no ablation or\nsensitivity analysis to show which components truly drive the\nimprovement. This limitation appears in the short length of the reference list.\nClustering is one of the most extensively studied areas in machine\nlearning, and any new significant proposal should relate to much more\nrecent related work—most of which this paper ignores.\n\n\n\n\n4. \"Informative diversification” is not formally defined or adaptively\nmeasured; the approach relies heavily on pretrained embeddings without\nclarifying how much the gains come from the transformations versus the\nbase models. This weakens interpretability and reproducibility."}, "questions": {"value": "1. The paper only benchmarks against K-Means, GMM, and Spectral\nClustering. How would it perform against modern methods like DEC,\nIDEC, SCAN, or graph-based and contrastive clustering models that\nalready integrate multiple representations?\n\n\n2. The framework requires multiple clustering runs and spectral\ndecomposition steps. How well does it scale with large corpora or\nhigh-dimensional embeddings, especially when the number of views (m)\nincreases?\n\n\n3. Since the model relies heavily on high-quality sentence embeddings\n(e.g., from BERT), are the observed gains primarily due to the\ndiversification strategy, or simply from strong pretrained\nrepresentations?\n\n\n\n4. The theoretical guarantees rely on mutually independent and\ninformative views. However, deterministic transformations (e.g., PCA\nor similar BERT encoders) are highly correlated. Can the claimed\nexponential error decay still hold when view independence is violated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BMEJoPd2ef", "forum": "egPSakPG0e", "replyto": "egPSakPG0e", "signatures": ["ICLR.cc/2026/Conference/Submission4162/Reviewer_Az8T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4162/Reviewer_Az8T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761031947565, "cdate": 1761031947565, "tmdate": 1762917206593, "mdate": 1762917206593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new approach to do unsupervised-text-clustering. Particularly, they introduce the usage of multi-views combined with Gaussian Mixture Model to aggregate the views information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper has clear writing as well as the reasoning process. I also find the claims made in the paper reasonable. While I have not rigorously validated the theoretical claims of the paper (in the appendix), I think they make sense intuitively on a high-level, thus, I believe they are correct."}, "weaknesses": {"value": "First, I find the benchmark is lacking. The paper only evaluates their methods on 2 datasets. Furthermore, as mentioned in the related works, there are many other methods for text clustering and they only compare the proposed method to K-Mean, GMM, and Spectral clustering. Thus, I find the current benchmarking is not satisfiable for ICLR.\n\nSecond, while I believe the theoretical claims are correct, I question the assumptions that the authors use to make it works. For instance, the \"mutual independent\" (line 235) condition is too strong as I do not believe it ever holds in practice for a reasonable number of views m > 2. From my understanding, as all the views are generated on one entity/data point, it is much more likely for the views to be strongly dependent. On the other hand, I find the second condition \"informative\" is stated in a very unintuitive manner (overly formulated to fit the theoretical claim?). I also think it is difficult to validate how the assumption can hold in practice. Thus, I do not think it makes sense to say they are \"mild conditions\" as stated at line 224."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QE6UwDR12A", "forum": "egPSakPG0e", "replyto": "egPSakPG0e", "signatures": ["ICLR.cc/2026/Conference/Submission4162/Reviewer_CU4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4162/Reviewer_CU4P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761071578447, "cdate": 1761071578447, "tmdate": 1762917205900, "mdate": 1762917205900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Briefly summarize the paper and its contributions. You can incorporate Markdown and Latex into your review. See https://openreview.net/faq.\nThe paper proposes a novel clustering scheme based on multi-view consensus clustering and dual-objective latent space optimization. Multi-view consensus is established by 1) first applying a view-specific randomly parameterized transforms, 2) conducting clustering for each view with Gaussian Mixture Model (GMM) clustering, and finally 3) merging the views via spectral clustering to yield the consensus cluster. Dual-objective latent space optimization refines the encoder latent space by minimizing both 1) an InfoNCE objective to sharpen the cluster assignment and 2) a regularization prior to map sample encodings onto the GMM manifold. The entire clustering process is iteratively conducted through alternation between the two processes. The authors further presents mathematical proof based on Hoeffding's inequality to demonstrate exponential error decrease induced by multi-view clustering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "In general, the presentation of the manuscript is good, and the proofs and methods are sound.\n\n(Quality) The authors present theoretical backing to their scheme through discussions on the lowering of expected error caused by multi-view representation.\n\n(Quality) Overall, the alternating optimization scheme proposed is interesting and methodologically sound.\n\n(Quality) The authors demonstrate that their experiment outperforms single-view clustering.\n\n(Clarity) The authors have presented the necessary equations and pseudocode to ensure a clear understanding for the audience."}, "weaknesses": {"value": "In general, the work has issues with novelty and motivation, caused by the lack of a Related Work section and (more importantly) analysis of more recent literature (within the past 3 years).\n\n(Quality) Experiments for clustering on unseen data are insufficient, as no single-view methods are presented for comparison.\n\n(Originality) The work is somewhat limited in originality. Works on multi-view clustering with GMM [1] and InfoNCE [2] are already well-known. Thus, the proposed work incrementally builds upon existing work, with the main contribution being its application to LLM.\n\n(Significance) The motivation of this work is detracted by the references selected. Aside from lacking a Related Works section, most of the references in the introduction are over 5 years old. An analysis of more recent multi-view clustering work (i.e. [3]) is necessary.\n\n[1] Kumar, A., & Daumé, H. (2011). A co-training approach for multi-view spectral clustering. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 393-400).\n\n[2] Oord, A. V. D., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748.\n\n[3] Pattnaik, A., George, C., Tripathi, R., Vutla, S., & Vepa, J. (2024, November). Improving hierarchical text clustering with llm-guided multi-view cluster representation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track (pp. 719-727)."}, "questions": {"value": "1. Table 1 shows several sentence embedding models for generating multi-view representations. But their application is unclear. Does each model replace/merge its representation along with the Sentence-Bert output?\n\n2. Given that multiple transformations are applied, would the cost (i.e. runtime, compute, memory) also increase significantly?\n\n3. How many views m are used for the different multi-view schemes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1fdPRs1mQB", "forum": "egPSakPG0e", "replyto": "egPSakPG0e", "signatures": ["ICLR.cc/2026/Conference/Submission4162/Reviewer_MMDR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4162/Reviewer_MMDR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761596044896, "cdate": 1761596044896, "tmdate": 1762917205393, "mdate": 1762917205393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new text clustering method based on multi-view clustering and information diversification. The authors introduce an iterative framework that alternately refines the clustering step and the text representation learning step by jointly minimizing a combined loss function consisting of a likelihood term and an InfoNCE term. Multiple feature extraction methods and models are employed to obtain diverse views of the texts, and a voting mechanism is used to measure how many views support that any two texts belong to the same cluster. A spectral clustering algorithm is then applied to this voting matrix (an NxN similarity matrix, where N is the number of texts) to produce the final clustering result. The authors also provide a theoretical analysis explaining why multi-view clustering outperforms single-view clustering, based on the minimax risk of the misclassification rate. Experimental results demonstrate that multi-view clustering with information diversification yields significant performance improvements over single-view embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n\n2. The experimental results are positive and provide support for the authors’ claims.\n\n3. The proposed algorithms are simple, reproducible, and conceptually sound."}, "weaknesses": {"value": "1. **Lack of comparison with stronger baselines.** The proposed method is compared only against standard embeddings (e.g., SBERT) combined with conventional clustering algorithms such as KMeans and GMM, as well as its own variants. However, comparisons with existing multi-view clustering methods or ensemble-based approaches are missing.\n\n2. **Theoretical contribution appears limited.** The authors show that multi-view consensus clustering achieves an arbitrarily low minimax risk as the number of views increases, whereas single-view clustering retains a constant lower bound. This result, however, is rather straightforward and well known in ensemble learning theory. Moreover, the assumption that the multiple views are sufficiently diversified to achieve a large effective number of independent views is quite strong, making the comparison with the single-view case somewhat unfair.\n\n3. **Limited novelty.** The main contribution—combining multi-view clustering with contrastive learning for text clustering—appears incremental relative to prior work.\n\n4. **Questionable scalability.** The proposed algorithm involves computing eigenvalues during the spectral clustering step on an N×N matrix, where N is the number of texts. While this matrix can be stored sparsely, the voting mechanism across multiple diversified views may considerably reduce its sparsity, making the spectral clustering step computationally expensive. More discussion on scalability is warranted."}, "questions": {"value": "1. How should Figure 2 and Table 2 be interpreted? In Table 2, performance drops after adding Gaussian noise, whereas Figure 2 suggests the opposite trend. Please clarify this inconsistency.\n\n2. In Table 2 (first row), what exactly are the “original embeddings”? Which SBERT model was used to produce them, or were they obtained by concatenating embeddings from multiple models?\n\n3. Are there other possible ways to combine multiple views besides the proposed consensus approach (voting + spectral clustering)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "22EQc5Eogf", "forum": "egPSakPG0e", "replyto": "egPSakPG0e", "signatures": ["ICLR.cc/2026/Conference/Submission4162/Reviewer_8qY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4162/Reviewer_8qY8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624779664, "cdate": 1761624779664, "tmdate": 1762917204990, "mdate": 1762917204990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a text clustering framework based on informative diversification, supported by a theoretical analysis of its multi-view consensus mechanism. While the reported gains over baselines such as K-Means merit recognition, the work's overall impact is limited by the restrictive nature of its theoretical assumptions and a lack of comprehensive experimental validation, which collectively undermine the validity of its claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Technical Framework: The paper presents a text clustering framework that integrates multi-view consensus clustering with deep representation learning in an end-to-end manner. An iterative co-training strategy is used to jointly optimize view generation, consensus clustering, and representation learning.\n\n2. Theoretical Contributions: Theoretical analysis is provided, including an exponential upper bound on the misclustering error in multi-view consensus. The authors also connect the minimization of InfoNCE loss to the maximization of mutual information, giving a theoretical basis for the objective.\n\n3. Experimental Results: Experiments on DBPedia and Reuters R8 report improvements in NMI and ARI over several baselines. The model shows some generalization capability, and t-SNE visualizations suggest that the learned embeddings form relatively compact clusters."}, "weaknesses": {"value": "1. Limited Theoretical Novelty and Strong Assumptions: Proof 1 restates the known connection between InfoNCE and mutual information, contributing minimal theoretical innovation. Proof 2 relies on the strong—and often impractical—assumption of view independence, yet fails to discuss its validity or consequences in real-world scenarios.\n\n2. Narrow Experimental Scope:  Evaluation is limited to two clean datasets and traditional baselines, lacking tests under noisy conditions or comparisons with recent deep learning-based clustering methods.\n\n3. Insufficient Computational Analysis: The paper does not address the computational cost of multi-view generation or scalability, leaving practical feasibility unclear for larger datasets.\n\n4. Incomplete Ablation and Hyperparameter Analysis: Ablation studies only explore view combinations without justifying core design decisions (e.g., spectral clustering vs. majority voting). Key hyperparameters—such as the number of views, loss weights, and iteration counts—lack systematic analysis, hindering reproducibility and insight.\n\n5. Lack of Experimental Rigor and Presentation Issues: Critical implementation details and hyperparameter settings are inadequately documented. Additionally, Figure 2 uses inconsistent plot types for the same metric, impairing readability, and references are not alphabetized, reflecting a lack of attention to presentation quality."}, "questions": {"value": "1.\tThe independence assumption for multiple views in Proof 2 is critical yet often impractical. How would violations of this assumption (i.e., correlated views) affect your theoretical guarantees, and did you empirically measure the dependence between the generated views?\n2.\tThe experimental comparisons are limited to traditional methods. To firmly establish the advancement of your work, could you include results comparing against recent deep learning-based clustering approaches?\n3.\tCould you provide an analysis of the computational cost (e.g., training time scaling with dataset size and number of views) and include in the appendix the detailed settings for key hyperparameters (e.g., α, β, τ) to ensure reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tHxMgJk1Hv", "forum": "egPSakPG0e", "replyto": "egPSakPG0e", "signatures": ["ICLR.cc/2026/Conference/Submission4162/Reviewer_bHgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4162/Reviewer_bHgA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831661068, "cdate": 1761831661068, "tmdate": 1762917204736, "mdate": 1762917204736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}