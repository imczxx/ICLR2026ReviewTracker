{"id": "Q5mkmW0cUD", "number": 9, "cdate": 1756728065046, "mdate": 1759898278828, "content": {"title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning", "abstract": "Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual question answering, and code generation, yet their ability to reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English.\nThis implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust.\nWe propose M2A, a novel method that combines multi-scale multilingual alignment with language-consistency rewards on machine-translated questions, training models to reason directly and accurately in the target language.\nFurthermore, existing multilingual benchmarks only evaluate on final answers, overlooking whether reasoning occurs in the intended language.\nTo close this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark together with reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai.\nOur results show that M2A significantly enhances multilingual reasoning fidelity in both mathematical and factual reasoning tasks,  highlighting that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization.", "tldr": "", "keywords": ["LLM", "multilingual reasoning", "alignment", "multilingualism", "cross-lingual transfer", "multilingual benchmarks", "multilingual evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e284e24dbdd3f0ebf98ecdf056906cc636a3291.pdf", "supplementary_material": "/attachment/480d0cda0e04bbe6a70db744b1241d6cf81398c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces M2A, a RL–based framework designed to improve the language-consistency of reasoning in multilingual LLMs. And this paper introduces a “Gemini 2.0 distilled” benchmark GEOFACT-X."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe language-consistency of reasoning in multilingual LLMs is an important topic."}, "weaknesses": {"value": "1.\tLine 738: The reasoning traces in GEOFACT-X were generated by Gemini 2.0. Therefore, for the training stage, this process can partially be viewed as data distillation from Gemini 2.0 to other models. Meanwhile, for the benchmarking stage, the ultimate evaluation objective might become distorted—shifting toward mimicking Gemini 2.0’s behavior. The construction of the reasoning traces is a critical issue that deserves more info rather than a brief mention in the appendix.\n2.\tContinuing the previous concern, part of the reported improvements may stem from data distillation effects of Gemini 2.0 and Google Translation.\n3.\tAfter checking the case in Figure 7, I found that the prompt is actually quite complex. This raises the question of whether the errors in cultural contexts is due to difficulties in understanding cultural content or in following the complex instructions given in Figure 7. An error analysis would help clarify this.\n4.\tRegarding Sentence Matching via similarity computation, did the authors verify the accuracy of the matching results? And is mT5 sufficiently robust for this task?\n5.\tThe description of the Step 4-Human Verification in GEOFACT-X benchmark construction is missing, which should have been an important part for quality guarantee. And does the Human Verification involves only the language check? Or both the language check and the reasoning traces quality check?\n6.\tThe experiments are conducted using only a single backbone model, Qwen-2.5-7B-Instruct, without exploring models of other series or different scales. This raises concerns about the generalizability.\n7.\tThe entire paper relies heavily on Google Translation. Apart from Step 4 in Figure 4, were the other parts involving Google Translation also subjected to human verification?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4JHaNTJVnG", "forum": "Q5mkmW0cUD", "replyto": "Q5mkmW0cUD", "signatures": ["ICLR.cc/2026/Conference/Submission9/Reviewer_kd5k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9/Reviewer_kd5k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394185115, "cdate": 1761394185115, "tmdate": 1762915435989, "mdate": 1762915435989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how LLMs default to reasoning in English, even when prompted in other languages. This bias reduces trust and interpretability, especially for speakers of low-resource languages. The authors argue that correct answers alone are not enough. To make sure that the answer is clear and culturally accurate, models must also think in the same language as the question.\n\nTo solve this problem, the authors introduced M2A (Multi-Scale Multilingual Alignment), a new way to train models that teaches them to think directly in the target language. M2A combines supervised fine-tuning with reinforcement learning through group relative policy optimization. It uses three complementary rewards: context alignment for global coherence, reasoning-step alignment for fine-grained logical consistency, and a language-consistency reward that ensures reasoning stays in the question’s language.\n\nThe paper also introduces GEOFACT-X, a new multilingual factual reasoning benchmark that includes step-by-step reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Introduced a novel M2A method for enforcing language-consistent reasoning in LLMs.\n- Presented the GEOFACT-X benchmark to evaluate multilingual reasoning traces.\n- Highlighted clear empirical gains in both reasoning fidelity and mathematical accuracy.\n- Proposed a joint accuracy metric combining reasoning correctness and language fidelity.\n- Explained reasoning and alignment rewards with well-designed figures and concise text."}, "weaknesses": {"value": "- Results rely on a single model family (Qwen-2.5-7B), leaving it unclear how the proposed M2A training generalizes to other architectures.\n- The LLM-as-judge setup risks stylistic bias, as the evaluator (Qwen2.5-72B) may implicitly favor Gemini-like phrasing and structure.\n- Reliance on Google Translate may introduce subtle cultural or contextual distortions."}, "questions": {"value": "- The four-step pipeline (Gemini > MT > Reasoning traces > Human verification) is useful. Could you also release the rule-based filters and annotator guidelines to clarify the cultural and linguistic validity criteria?\n- How does the “joint accuracy” metric handle partial errors? Does a single incorrect token or step make the entire reasoning trace incorrect?\n- Did you try any other translation methods or tools? What led you to choose Google Translate?\n- Depending on the tokenizer, the number of tokens consumed during reasoning can vary significantly, increasing inference costs. Have you considered this factor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jXVEzxCJz4", "forum": "Q5mkmW0cUD", "replyto": "Q5mkmW0cUD", "signatures": ["ICLR.cc/2026/Conference/Submission9/Reviewer_LN1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9/Reviewer_LN1B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761520996037, "cdate": 1761520996037, "tmdate": 1762915435880, "mdate": 1762915435880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem that LLMs often fail to provide reasoning steps in the language of the question/prompt, if that language is not English or Chinese. The paper’s first main contribution is M2A: an alignment method that, in addition to factual accuracy, rewards language consistency. The second main contribution is GeoFact-X: a factual reasoning dataset in five languages (English, Hindi, Japanese, Swahili, Thai) that was automatically generated by an LLM, automatically translated, and then verified by humans. Experiments with Qwen2.5-7B-Instruct show marginal improvements for both mathematical accuracy. Results on language accuracy were already near-perfect in the baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a relevant problem, namely, ensuring reasoning in LLMs in languages beyond English and Chinese.\n2. The paper introduces both a novel method (M2A) and a new LLM-generated dataset (GeoFact-X). GeoFact-X was validated by human annotators, which could potentially be of value to the research community.\n3. The paper is generally written clearly, with Figure 1-4 providing visualizations to explain the methodology further.\n4. The paper includes ablation studies of the reward functions and reward formulations (Section 6.4), providing additional insights into M2A’s effectiveness."}, "weaknesses": {"value": "1. The motivation of the work is flawed. The authors argue that reasoning traces in target languages are essential for trustworthiness (line 34) and to \"ensure interpretability, i.e. users can directly follow the reasoning in their own language\" (line 36-37). At the same time, it has been shown that reasoning steps are not necessarily faithful to the final answer [1, 2, inter alia], and therefore cannot be trusted to provide faithful interpretability for users.\n2. The cultural relevance of the dataset is unclear, while this is central to the paper’s motivation. Human annotators checked the LLM-generated benchmark dataset (GeoFact-X) for \"factual correctness and linguistic clarity\" (line 273-274), but not for cultural relevance. A concrete example: \"What is the name of the process by which a cell divides into two identical daughter cells?\" (\"Mitosis\") is in the dataset as being culturally relevant specifically for the USA.\n3. The LLM-as-a-judge reasoning evaluation is not scientifically sound. The score is computed with Qwen-2.5-75B (while the method used a Qwen-2.5 model of a different size), and this judgement is not checked against human judgement of reasoning scores. Therefore there is no indication that this metric is reliable and in fact, LLM-as-a-judge is heavily criticized [3]. Secondly, the prompt for the metric itself (Fig. 10) is not motivated. For example, how was is decided that 'conclusion validity' should count for 10%, 'factual correctness' as 20%,\n'key insights' as 30% and 'logical structure' as 40%? How well does the model follow these precise instructions? Lastly, the ‘ground truth’ does not always seem informative. A concrete example is the question \"In what year did Thailand launch its first satellite, Thaicom 1?\" with the following reasoning trace: \"<step> The question asks for the year Thailand launched its first satellite, Thaicom 1. <step> Research indicates that Thaicom 1 was launched in 1993. <step> Therefore, the answer is 1993.\" It does not seem useful to score a generated reasoning trace (for 'key insights', etc.) against such a 'ground truth'. All in all, it is not clear what this metric really measures, or how well and consistently it measures this.\n4. Some of the conclusions are not supported by convincing results. For example, the claim that \"M2A outperforms baselines in all metrics on GSM8K ….\" (line 377) does not convincingly follow from Table 2. Firstly, the language accuracy score of the baseline model is already 100% for GSM8K, thus it is not convincingly 'outperformed' by M2A. Secondly, differences with the SFT baseline in math reasoning are marginal (87.2 ±1.6 for SFT, 87.3 ±0 for M2A).\n5. The discussion of results omits key baselines. For example, the claim that M2A \"achieves large gains in joint accuracy on MGSM compared to SFT\" (line 377) disregards the fact that GRPO and the default model achieve similar scores. This is not discussed in section, which paints a more convincing picture of the results than what follows from the table.\n\n[1] Chen et al. (2025). Reasoning Models Don’t Always Say What They Think. https://arxiv.org/pdf/2505.05410\n\n[2] Arcushchin et al. (2025). Chain-of-Thought Reasoning In The Wild Is Not Always Faithful. https://arxiv.org/pdf/2503.08679\n\n[3] Bavaresco et al. (2025). LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks. https://arxiv.org/abs/2406.18403"}, "questions": {"value": "Questions related to the listed weaknesses:\n1. Could you motivate why reasoning traces are important for interpretability, and why language-consistency in reasoning is \"essential for globally inclusive and interpretable AI\" (line 51)? (W1)\n2. What were the annotation guidelines for human assessment of the LLM-generated benchmark dataset, and did you calculate an inter-annotator agreement for this? Currently, it is difficult to assess the quality of the generated dataset at a large scale. (W2)\n3. How reliable is your LLM-as-a-judge evaluation? Could you calculate an agreement with human judges? (W3)\n\nOther questions:\n\n4. Why do you write 'speak locally' in your title, while there is no evaluation of the LLMs for speech tasks?\n5. \"Our work provides a foundation for future work on multilingual reasoning.\" (line 89) What do you define as multilingual reasoning? Do you mean reasoning in a language that is not English, but not simultaneous in multiple languages?\n6. You mention translating your dataset into \"ten typologically diverse languages\" (line 359). Which languages are included and what makes this selection typologically diverse? If it is the languages reported appendix E, then the selection does not seem so diverse, or at least skewed (German, Spanish, French, …). Would this be quantifiable? Otherwise, this claim is not justified. This question also applied to \"five diverse languages\" (line 478).\n7. Can you assume that Gemini 2.0 generates reasoning traces consistently for each of the five languages when you use it to create the GeoFact-X dataset (lines 269-270)? If so, is reasoning language consistency really a prominent issue in LLMs?\n8. \"We use Qwen-2.5-7B-Instruct as the backbone for all experiments on mathematical and factual reasoning.\" (line 316-317): Why did you choose Qwen-2.5-7B-Instruct specifically? (Why) would you expect similar results for different models?\n9. In Table 3 (\"Comparison of model performance on average reasoning score (%), language accuracy (%), and answer accuracy (%)\"), I could not find the language accuracy scores. Why were they omitted?\n10. Why, in Tables 3 and 4, do you specifically evaluate Thai only?\n\nTypos:\n- Table 3 and 4: Qwen-2.5-Insturct\n- Figure 2: Telugu vs. Telgnu"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CPwZxnFujx", "forum": "Q5mkmW0cUD", "replyto": "Q5mkmW0cUD", "signatures": ["ICLR.cc/2026/Conference/Submission9/Reviewer_ZU1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9/Reviewer_ZU1X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655445123, "cdate": 1761655445123, "tmdate": 1762915435733, "mdate": 1762915435733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs ability to reason in low resource languages is still lacking. Even if models accurately answer the question in the low resource languages, the reasoning traces are often in English. The paper introduces M2A, an approach to do multi-scale multilingual alignment along with language consistency rewards which trains the model to reason in the same language as the question. M2A uses SFT + GRPO on ground-truth reasoning traces. The paper also introduces GEOFACT-X, a geography based multilingual factual reasoning benchmark along with reasoning traces in five languages. The authors also introduce a new metric to measure language accuracy since existing multilingual benchmarks assess the final accuracy but overlook the language of the reasoning traces."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a critical issue of interpretability in low resource languages due to lack of language adherence in LLM reasoning traces. Models often reason in English instead of the target language of the question. The paper introduces a reward modeling approach that combines rewards for language consistency, context and reasoning alignment. The paper also introduces a high quality dataset GEOFACT-X, a geography based multilingual factual reasoning benchmark, in five languages:  English, Hindi, Japanese, Swahili, and Thai. The paper has ablations to show why each component of the reward modeling is important."}, "weaknesses": {"value": "1. While M2A shows improvements on Math or language accuracy over SFT and GRPO individually, it doesn't seem to improve MGSM Math or Language accuracy scores over the baseline Qwen-2.5-Instruct model in Table 2.\n\n2. The proposed M2A approach requires ground truth outputs as well as ground truth reasoning traces for Multilingual Context Alignment and Multilingual Reasoning-Step Alignment as described in Section 3. This is a major limitation especially for resource constrained languages. One of the main benefits of online RL recipes such as GRPO is that you don't need ground truth responses."}, "questions": {"value": "1. The baselines and experiment setup is not very clear. What reward model does baseline GRPO recipe use? Does M2A use SFT + GRPO or does it only use GRPO with the reward modeling mentioned in Section 3? \n\n2. Is there a SFT + GRPO (with sparse rewards) baseline for comparison that can be added to Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bOYZEvizz4", "forum": "Q5mkmW0cUD", "replyto": "Q5mkmW0cUD", "signatures": ["ICLR.cc/2026/Conference/Submission9/Reviewer_yDoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9/Reviewer_yDoH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782904394, "cdate": 1761782904394, "tmdate": 1762915435615, "mdate": 1762915435615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}