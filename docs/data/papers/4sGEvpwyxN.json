{"id": "4sGEvpwyxN", "number": 18192, "cdate": 1758284921345, "mdate": 1763662942243, "content": {"title": "High-Probability Bounds for the Last Iterate of Clipped SGD", "abstract": "We study the problem of minimizing a convex objective when only noisy gradient estimates are available. Under the mild assumption that the stochastic gradients have finite $\\alpha$-th moments for some $\\alpha \\in (1,2]$, we show that the last iterate of clipped stochastic gradient descent (Clipped-SGD) achieves high-probability convergence of order $1/K^{(2\\alpha-2)/(3\\alpha)}$ on smooth objectives. Finally, we provide empirical results that support and complement our theoretical analysis.", "tldr": "", "keywords": ["Stochastic Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25efbec810cc03fb3c4d6ff0a8b4dc14e5bc1abc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies high-probability last-iterate guarantees for Clipped-SGD on convex L-smooth objectives under heavy-tailed noise with finite \\alpha-th moments ($\\alpha \\in (1,2]$). It proposes a potential-based analysis with horizon-free step size and a clipping level scaling as $1/\\sqrt{b_k}$, and proves a last-iterate rate $O(K^{-2(\\alpha-1)/(3\\alpha)})$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Addresses a practically relevant “last-iterate” question for clipped methods under heavy-tailed noise; the setup (convex + L-smooth + $\\alpha$-moment) is standard and clearly stated.\n\n- Writing is generally clear; related-work table situates results among expectation and high-probability bounds (SGD / clipped-SGD, average vs last iterate)."}, "weaknesses": {"value": "- Short and narrow in scope. The paper reads like a concise note; the empirical section is minimal (toy problems, limited baselines).\n- Limited contribution vs. prior art. While the shift to high-probability \\emph{last-iterate} guarantees under heavy tails is interesting, the improvement over existing results is incremental and tightly scoped to convex $L$-smooth objectives.\n- Suboptimal rate at $\\alpha=2$. Instantiating the main bound with $\\alpha=2$ yields a last-iterate rate $\\tilde{\\mathcal{O}}(K^{-1/3})$, leaving a \\emph{polynomial gap} to the known $\\tilde{\\mathcal{O}}(K^{-1/2})$ benchmark in the finite-variance setting.\n- Overclaim / framing. Phrases such as “close a long-standing gap” and “first high-probability last-iterate guarantees” are stronger than warranted given the convex-only scope and the $\\alpha=2$ suboptimality; related work is not contrasted with sufficient precision to delimit novelty."}, "questions": {"value": "The paper is short with limited empirical support; the theoretical advance, while neat, is incremental and currently rate-suboptimal at $\\alpha=2$; several claims read stronger than warranted. With strengthened results and a broader empirical study, it could be a good workshop paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AflskNiWI0", "forum": "4sGEvpwyxN", "replyto": "4sGEvpwyxN", "signatures": ["ICLR.cc/2026/Conference/Submission18192/Reviewer_BKzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18192/Reviewer_BKzV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403743747, "cdate": 1761403743747, "tmdate": 1762927939967, "mdate": 1762927939967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides the first high-probability last-iterate convergence rate of Clipped SGD. The authors also conducted numerical experiments demonstrating that the last iterate of Clipped SGD indeed converges."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The result is meaningful, as it fills a gap in the literature."}, "weaknesses": {"value": "1. Line 112, when talking about the heavy-tailed noise, the finite-sum problem is not a proper example. Note that for this kind of problem, a finite $\\alpha$-th central moment directly implies a finite $2$nd central moment.\n\n1. Line 116, missing $\\\\\\{\\\\\\}$ for ${x_k}_{k=0}^K$.\n\n1. Some statements in Section 3 are inaccurate.\n\n    1. Line 142, $\\widetilde{\\mathcal{O}}$ is not necessary and instead should be $\\mathcal{O}$ since the current description doesn't limit to the case of an unknown $K$.\n\n    1. Line 149, missing $)$ after $\\mathbb{E}[...]$. In addition, Fatkhullin et al. (2025) didn't consider exactly the same condition as in the current Assumption 3.\n    \n    1. For the in-expectation convergence under heavy-tailed noise, the authors missed a relevant work [1]. Note that for the non-smooth Lipschitz case, [1] showed both average-iterate and last-iterate convergence. For the smooth case, [1] also showed the average-iterate convergence.\n\n    1. Still in Table 1, if the results in (Nguyen et al., 2023) are better than (Sadiev et al., 2023) from every perspective, then I don't see the point of putting (Sadiev et al., 2023) in it.\n\n    1. Line 192,  none of these four works (Nazin et al., 2019; Gorbunov et al., 2020; Gorbunov et al., 2024; Parletta et al., 2024) consider heavy-tailed noise. (Liu & Zhou, 2024) didn't study clipping methods.\n\n    **Reference** \n\n    [1] Liu, Z. (2025). Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications. arXiv preprint arXiv:2508.07473.\n\n1. In the statement of Theorem 1: \n\n    1. No definition of $d_k$ is provided. \n\n    1. I don't see why $p$ is a function of $L$. Do the authors mean $C$?\n\n    1. The definition of $C$ is also not proper. It involves solving a fixed-point equation. However, the authors didn't discuss whether it is always solvable (or any other related discussion). In addition, since $\\Phi_0=C\\\\\\|x_0-x^\\*\\\\\\|^2/2$, which cannot be evaluated in general due to the unknown value $\\\\\\|x_0-x^*\\\\\\|$.\n\n1. The whole paper relies on the existence of $x^*$, which the authors should state clearly.\n\n1. About numerical experiments:\n\n    1. For all experiments, could the authors also provide the empirical mean and standard deviation for both the average iterate and the last iterate?\n\n    1. In Line 307, the authors said the step-size and clipping level schedules are selected based on the theory. Could the authors provide more details? Especially, for $f(x)=\\ln(1+\\exp(\\langle x,a\\rangle))$, this objective doesn't have an optimal solution on $\\mathbb{R}^d$ (also implying $\\\\\\|x_0-x^\\*\\\\\\|=+\\infty$), which clearly differs from the current theorem. Could the authors provide more details? \n\n1. I appreciate the authors' honesty about the limitations discussed in Section 6. However, I have to repeat that the current rate $\\tilde{O}(1/K^\\frac{2(\\alpha-1)}{3\\alpha})$ seems not optimal.\n\n1. The analysis is somewhat standard, without introducing any new techniques, as far as I can see.\n\n1. The proof is not written in a mathematically rigorous manner. For example, the two inequalities in Lines 893-897 only hold conditionally on the event $E_{T-1}$. This means that one cannot directly invoke Bernstein’s inequality in the following proof. Instead, one should do one more step to introduce some proxy variables, like many prior works (e.g., the indicator variable $\\mathbb{I}[\\Phi_t\\leq 2\\Phi_0\\log(6(t+1)^2/\\delta)]$). Therefore, I believe many parts of the proof should be carefully revised."}, "questions": {"value": "In addition to the weaknesses mentioned earlier, it seems the current proof cannot be directly extended to constrained optimization. Could the authors provide any discussion on this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LuQoRBcPH9", "forum": "4sGEvpwyxN", "replyto": "4sGEvpwyxN", "signatures": ["ICLR.cc/2026/Conference/Submission18192/Reviewer_TuTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18192/Reviewer_TuTb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516702057, "cdate": 1761516702057, "tmdate": 1762927938834, "mdate": 1762927938834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an analysis of SGD for heavy-tailed gradients with bounded alpha-moment and convergence rate 1/T^{2(alpha-1)/(3alpha)} for smooth convex objectives."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I am not aware of other results on this problem, and the execution seems good."}, "weaknesses": {"value": "The convergence rate is not optimal. This makes the paper somewhat incremental because if you allow projection into a bounded diameter domain, you get the optimal rate (without clipping). This is a straightforward consequence of recent results connecting the last iterate of SGD with linear decay schedules and regret analysis, along with the same martingale/bias arguments used to bound error from clipping in this paper (see e.g. https://arxiv.org/abs/2310.07831 and the more general result in https://arxiv.org/abs/2405.15682). Essentially, one needs only show that the linearized regret of projected online gradient descent is bounded in high probability, which I think is not too hard using the standard concentration of clipped values arguments presented here if the gradients are clipped and the iterates are bounded by some projection.\n\nWithout the projection, there is some difficulty and so the present paper does have value. My concern is that this difference seems relatively small.\n\nIf it were possible to achieve the optimal rate without projection, or (even better) show that the optimal rate is NOT achievable without projections, that would be a much better result."}, "questions": {"value": "Please address the weakness above. Happy to revise my opinion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fs9LWQuf0O", "forum": "4sGEvpwyxN", "replyto": "4sGEvpwyxN", "signatures": ["ICLR.cc/2026/Conference/Submission18192/Reviewer_7EUw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18192/Reviewer_7EUw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940765923, "cdate": 1761940765923, "tmdate": 1762927938476, "mdate": 1762927938476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies **Clipped Stochastic Gradient Descent (Clipped-SGD)** under **heavy-tailed noise**—that is, when the stochastic gradient only has a finite α-th moment for α ∈ (1, 2]. The authors provide, for the **first time**, *high-probability* convergence guarantees for the **last iterate** (as opposed to the more typical average iterate) on convex, L-smooth objectives.\n\nTheir main result shows that the last iterate of Clipped-SGD achieves a convergence rate of\n[\nO!\\left(\\frac{\\mathrm{polylog}(K/\\delta)}{K^{2(\\alpha-1)/(3\\alpha)}}\\right)\n]\nwith failure probability ≤ δ.\n\nThey also propose *horizon-free* step-size (γₖ) and clipping-level (λₖ) schedules—meaning they do not require knowing the total number of iterations in advance—and verify their theory empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **First High-Probability Last-Iterate Result under Heavy Tails**\n   Previous work established only *expected* convergence or *average-iterate* guarantees. This paper closes a notable theoretical gap by analyzing the **last iterate** under α-th moment assumptions, a more realistic model for gradient noise in large-scale ML.\n\n2. **Novel Potential-Based Proof**\n   The authors design a custom potential function Φₖ combining function suboptimality and distance to optimum. This enables applying **Freedman/Bernstein inequalities** for martingale control, producing high-probability results without strong boundedness assumptions.\n\n3. **Horizon-Free (Any-Time) Step-Size and Clipping**\n   The method adaptively scales γₖ and λₖ using logarithmic corrections in δ, ensuring convergence without pre-specifying total iterations. This is practical for streaming or online settings.\n\n4. **Solid Empirical Verification**\n   The authors validate theory with synthetic and logistic regression problems under heavy-tailed (Pareto, Student-t) noise, showing that the **last iterate consistently outperforms the average iterate**."}, "weaknesses": {"value": "1. **Limited Empirical Scope**\n   Only simple convex problems (logistic, quadratic) are tested. Including real datasets or large-scale ML examples (e.g., transformer finetuning) would strengthen impact.\n\n2. **Complex Presentation**\n   The notation (γₖ, λₖ, dₖ, bₖ, Φ₀, C, p(Φ₀,L,σ)) is extremely dense. The exposition would benefit from a simplified summary table for parameters and clearer intuition for each scaling choice.\n\n3. **Lack of Sharpness at α=2**\n   When α = 2 (finite variance), the bound leaves a polynomial gap from the optimal O(1/√K) rate. The authors acknowledge this, but it remains an open issue.\n\n4. **δ-Dependence Obscures Practical Interpretation**\n   The interplay between the learning rate γ and the probability parameter δ—key to your question—is not explicitly analyzed beyond asymptotic scaling."}, "questions": {"value": "In your final convergence result, the learning rate $\\gamma$ explicitly depends on the confidence parameter $\\delta,$ i.e., $\\gamma_k \\propto 1 / \\ln^3(1/\\delta).$\nThis dependence between the step size and the high-probability confidence level is quite unusual in the stochastic optimization literature, where $\\gamma$ is typically independent of probabilistic guarantees.\nCould the authors clarify **why this dependence is necessary** in your analysis?\nIs it an intrinsic requirement of the heavy-tailed, high-probability setting, or merely a technical artifact introduced by the proof method (e.g., via union bounds or Freedman’s inequality)?\n\nIf this coupling is indeed required, the authors should provide a **rigorous necessity proof** to justify it, since this assumption significantly restricts the generality of the result.\n Moreover, **if such δ-dependent compression of the learning rate is generally allowed**, then even many divergent dynamical systems — for example,\n$$\nx_{n+1} = x_n - \\gamma_n x_n^2+\\gamma_n x_n^4 + \\gamma_n z_n, \\quad (z_n \\sim \\mathcal{N}(0,1))\n$$\n—could be made to appear *artificially stable* or “high-probability convergent” simply by shrinking $\\gamma_n$ according to a small δ.\nThis type of construction is **well known from standard graduate-level stochastic process exercises taught in mathematics departments**, and therefore **cannot be regarded as a publishable theoretical contribution** unless the authors rigorously prove that the δ-dependence is both mathematically necessary and not a by-product of overly conservative probabilistic bounding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H8FgeL94ve", "forum": "4sGEvpwyxN", "replyto": "4sGEvpwyxN", "signatures": ["ICLR.cc/2026/Conference/Submission18192/Reviewer_Mpxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18192/Reviewer_Mpxc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021112503, "cdate": 1762021112503, "tmdate": 1762927938126, "mdate": 1762927938126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies **Clipped Stochastic Gradient Descent (Clipped-SGD)** under **heavy-tailed noise**—that is, when the stochastic gradient only has a finite α-th moment for α ∈ (1, 2]. The authors provide, for the **first time**, *high-probability* convergence guarantees for the **last iterate** (as opposed to the more typical average iterate) on convex, L-smooth objectives.\n\nTheir main result shows that the last iterate of Clipped-SGD achieves a convergence rate of\n[\nO!\\left(\\frac{\\mathrm{polylog}(K/\\delta)}{K^{2(\\alpha-1)/(3\\alpha)}}\\right)\n]\nwith failure probability ≤ δ.\n\nThey also propose *horizon-free* step-size (γₖ) and clipping-level (λₖ) schedules—meaning they do not require knowing the total number of iterations in advance—and verify their theory empirically."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **First High-Probability Last-Iterate Result under Heavy Tails**\n   Previous work established only *expected* convergence or *average-iterate* guarantees. This paper closes a notable theoretical gap by analyzing the **last iterate** under α-th moment assumptions, a more realistic model for gradient noise in large-scale ML.\n\n2. **Novel Potential-Based Proof**\n   The authors design a custom potential function Φₖ combining function suboptimality and distance to optimum. This enables applying **Freedman/Bernstein inequalities** for martingale control, producing high-probability results without strong boundedness assumptions.\n\n3. **Horizon-Free (Any-Time) Step-Size and Clipping**\n   The method adaptively scales γₖ and λₖ using logarithmic corrections in δ, ensuring convergence without pre-specifying total iterations. This is practical for streaming or online settings.\n\n4. **Solid Empirical Verification**\n   The authors validate theory with synthetic and logistic regression problems under heavy-tailed (Pareto, Student-t) noise, showing that the **last iterate consistently outperforms the average iterate**."}, "weaknesses": {"value": "1. **Limited Empirical Scope**\n   Only simple convex problems (logistic, quadratic) are tested. Including real datasets or large-scale ML examples (e.g., transformer finetuning) would strengthen impact.\n\n2. **Complex Presentation**\n   The notation (γₖ, λₖ, dₖ, bₖ, Φ₀, C, p(Φ₀,L,σ)) is extremely dense. The exposition would benefit from a simplified summary table for parameters and clearer intuition for each scaling choice.\n\n3. **Lack of Sharpness at α=2**\n   When α = 2 (finite variance), the bound leaves a polynomial gap from the optimal O(1/√K) rate. The authors acknowledge this, but it remains an open issue.\n\n4. **δ-Dependence Obscures Practical Interpretation**\n   The interplay between the learning rate γ and the probability parameter δ—key to your question—is not explicitly analyzed beyond asymptotic scaling."}, "questions": {"value": "In your final convergence result, the learning rate $\\gamma$ explicitly depends on the confidence parameter $\\delta,$ i.e., $\\gamma_k \\propto 1 / \\ln^3(1/\\delta).$\nThis dependence between the step size and the high-probability confidence level is quite unusual in the stochastic optimization literature, where $\\gamma$ is typically independent of probabilistic guarantees.\nCould the authors clarify **why this dependence is necessary** in your analysis?\nIs it an intrinsic requirement of the heavy-tailed, high-probability setting, or merely a technical artifact introduced by the proof method (e.g., via union bounds or Freedman’s inequality)?\n\nIf this coupling is indeed required, the authors should provide a **rigorous necessity proof** to justify it, since this assumption significantly restricts the generality of the result.\n Moreover, **if such δ-dependent compression of the learning rate is generally allowed**, then even many divergent dynamical systems — for example,\n$$\nx_{n+1} = x_n - \\gamma_n x_n^2+\\gamma_n x_n^4 + \\gamma_n z_n, \\quad (z_n \\sim \\mathcal{N}(0,1))\n$$\n—could be made to appear *artificially stable* or “high-probability convergent” simply by shrinking $\\gamma_n$ according to a small δ.\nThis type of construction is **well known from standard graduate-level stochastic process exercises taught in mathematics departments**, and therefore **cannot be regarded as a publishable theoretical contribution** unless the authors rigorously prove that the δ-dependence is both mathematically necessary and not a by-product of overly conservative probabilistic bounding"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H8FgeL94ve", "forum": "4sGEvpwyxN", "replyto": "4sGEvpwyxN", "signatures": ["ICLR.cc/2026/Conference/Submission18192/Reviewer_Mpxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18192/Reviewer_Mpxc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18192/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021112503, "cdate": 1762021112503, "tmdate": 1763761964068, "mdate": 1763761964068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}