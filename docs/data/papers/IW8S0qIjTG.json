{"id": "IW8S0qIjTG", "number": 12466, "cdate": 1758208019004, "mdate": 1763088891365, "content": {"title": "MAGTrack: MLLM-Augmented Grounding and Text Refinement for Language-Guided Tracking", "abstract": "Language-guided object tracking aims to locate the target in a video based solely on a natural language description, without any bounding box supervision. While recent methods have made encouraging progress by incorporating language into visual tracking, most treat it as an auxiliary signal rather than a primary driver. This limits their effectiveness in fully language-only scenarios, which remain underexplored despite their user-friendly nature. In this paper, we propose MAGTrack, a novel framework for language-guided object tracking that seamlessly integrates Multimodal Large Language Models (MLLMs) without requiring additional training. MAGTrack tackles key challenges through two plug-and-play modules: the MLLM-based Grounding Module (MGM) and the MLLM-based Text Refinement Module (TRM). MGM leverages MLLM reasoning to achieve accurate initial target localization, even in challenging scenarios with visually similar objects. Complementarily, TRM dynamically updates the textual description based on the current visual context and tracking history. Extensive experiments on four benchmarks—OTB99, TNL2K, LaSOT, and LaSOText\n—demonstrate that MAGTrack consistently improves both first-frame grounding and long-term tracking accuracy, achieving state-of-the-art performance under the language-only setting.", "tldr": "", "keywords": ["Language-guided object tracking；Multimodal Large Language Models；Text Refinement；First-frame localization；"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1883cfdd0d0a088c22e5676a3a6349f862bd2a0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MAGTrack, a training-free framework for language-guided visual tracking that integrates Multimodal Large Language Models (MLLMs) through two plug-and-play components: the MLLM-based Grounding Module (MGM) and the MLLM-based Text Refinement Module (TRM). The proposed method leverages the reasoning and generative capabilities of MLLMs to address key challenges in the language-only tracking setting—namely, initial grounding ambiguity and temporal mismatch between static text and dynamic visual content. Experiments on four benchmarks (OTB99, TNL2K, LaSOT, and LaSOText) show consistent improvements over existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed MGM and TRM are plug-and-play modules that can be seamlessly integrated into existing vision-language trackers (e.g., UVLTrack, JointNLT), demonstrating good generality and transferability.\n- The paper is easy to follow."}, "weaknesses": {"value": "- The proposed method appears to be mainly a combination of existing MLLM-based reasoning modules and conventional tracking models, making it more of an engineering exploration rather than a fundamentally new algorithmic contribution.\n- It is recommended to compare with more recent state-of-the-art methods, such as DUTrack and other strong baselines, to better highlight the advantages of the proposed framework.\n- The paper should include more detailed ablation experiments, especially regarding confidence thresholds, update frequencies, and frame sampling strategies, to analyze the sensitivity and robustness of key design choices.\n- Since the integration of MLLMs into tracking has already been explored in several recent works, the authors are encouraged to restate their motivation more clearly and emphasize the specific novelty or unique perspective of their approach to improve overall clarity and contribution."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1VcjUF6Q52", "forum": "IW8S0qIjTG", "replyto": "IW8S0qIjTG", "signatures": ["ICLR.cc/2026/Conference/Submission12466/Reviewer_iXjB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12466/Reviewer_iXjB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534167213, "cdate": 1761534167213, "tmdate": 1762923346501, "mdate": 1762923346501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wkf0pUOcMB", "forum": "IW8S0qIjTG", "replyto": "IW8S0qIjTG", "signatures": ["ICLR.cc/2026/Conference/Submission12466/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12466/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763088890305, "cdate": 1763088890305, "tmdate": 1763088890305, "mdate": 1763088890305, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAGTrack, a modular framework for language-guided object tracking that integrates multimodal large language models (MLLMs) through two plug-and-play components: an MLLM-based Grounding Module (MGM) for initial localization, and an MLLM-based Text Refinement Module (TRM) for dynamically updating textual descriptions during tracking. The method aims to perform training-free, language-only tracking by combining reasoning from large vision-language models with conventional visual trackers. Experiments are conducted on four benchmarks (OTB99, TNL2K, LaSOT, LaSOText). While the paper is well written and the motivation is clear, I find that the overall novelty is limited, the efficiency is poor, and the empirical validation is insufficient for a top conference like ICLR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and the problem of language-only tracking is clearly motivated. \n2. The integration of Multimodal Large Language Models (MLLMs) into language-assisted tracking is indeed an emerging and promising research direction.\n3. The idea of using MLLMs as plug-and-play components (MGM, TRM) is simple and easy to integrate with existing trackers without a training process."}, "weaknesses": {"value": "1. Low novelty and shallow contribution. Both modules (MGM and TRM) are straightforward combinations of Grounding DINO + MLLM reasoning and text rewriting via prompting. There is no new algorithm, learning strategy, or theoretical insight. The framework is more of an engineering integration than a research innovation.\n2. Extremely high computational cost and low efficiency. Tracking is inherently a real-time task — models that cannot operate at interactive or near-real-time speeds (≥20 FPS) are difficult to justify, regardless of modest accuracy improvements. The reported speed of only 7–17 FPS with large MLLMs (Qwen2-VL-72B/7B) indicates that the proposed framework is far from deployable and serves mainly as a conceptual prototype. The reliance on Qwen2-VL-72B/7B makes deployment impractical, and there are no lightweight or efficient variants.\n3. Incomplete and weak experimental comparison. Some existing methods like MambaVLT[1] and UVLTrack-L[2] are missed in the Table 1. \n4. Limited task scope. A major conceptual limitation of the paper is that it restricts itself to language-only tracking, despite using UVLTrack as the base tracker—a model initially designed to unify language-guided (NL) and box-initialized (NL+BBOX) tracking within a single framework. Given that MAGTrack claims to be training-free and modular, it is unclear why the authors did not extend the framework to both paradigms, or at least demonstrate compatibility with the UVLTrack’s dual-mode capability. This narrow focus on the language-only setting makes the contribution appear incomplete and less general than prior unified methods such as UVLTrack or MambaVLT.\n[1] Liu X, Zhou L, Zhou Z, et al. Mambavlt: Time-evolving multimodal state space model for vision-language tracking[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 8731-8741.\n[2] Ma Y, Tang Y, Yang W, et al. Unifying visual and vision-language tracking via contrastive learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(5): 4107-4116."}, "questions": {"value": "1. Since MAGTrack builds directly upon UVLTrack, which already supports both language-only and language-box tracking, why is the proposed framework evaluated exclusively on the language-only setting? Wouldn’t a unified, training-free framework be more meaningful and aligned with the original UVLTrack motivation?\n2. Have you considered smaller MLLMs or distilled models to achieve real-time tracking?\n3. How sensitive is the system to the prompt templates and trigger threshold? Is performance stable across different prompt wordings and confidence thresholds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Td9p22liss", "forum": "IW8S0qIjTG", "replyto": "IW8S0qIjTG", "signatures": ["ICLR.cc/2026/Conference/Submission12466/Reviewer_3Zi9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12466/Reviewer_3Zi9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727927942, "cdate": 1761727927942, "tmdate": 1762923346079, "mdate": 1762923346079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MAGTrack, a framework for language-guided object tracking that integrates Multimodal Large Language Models (MLLMs) to address key challenges in this domain. The framework consists of two plug-and-play modules, where MLLM-based Grounding Module (MGM) enhances first-frame localization by leveraging MLLM reasoning to handle semantically ambiguous or visually similar objects; and MLLM-based Text Refinement Module (TRM) dynamically updates the initial natural language description during tracking, maintaining alignment between language and visual content."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Innovative Use of MLLMs. \n\nThe framework leverages the reasoning and generative capabilities of MLLMs in a novel way, addressing challenges like semantic ambiguity and temporal misalignment in language-guided tracking.\n\n- Dynamic Text Refinement.\n\nThe TRM module is a strong addition, allowing the tracker to adapt to appearance changes and contextual variations, which is a critical limitation in existing methods.\n\n- Modular Design.\n\nMAGTrack is designed as a training-free, plug-and-play framework, making it theoretically compatible with various existing trackers without requiring retraining."}, "weaknesses": {"value": "- Lack of Novelty.\n\nWhile the integration of MLLMs is interesting, the overall framework does not introduce fundamentally new ideas compared to existing vision-language tracking (VLT) methods. Many components, such as the use of grounding models and language refinement, are incremental rather than groundbreaking. The framework feels like an extension of current VLT methods, such as UVLTrack or JointNLT, rather than a significant departure from them.\nInsufficient \n\n- Experimental Comparison.\n\nThe paper avoids direct comparisons with some of the strong VLT baselines, such as  VLT. This omission raises questions about whether the improvements claimed are genuinely significant. Including these comparisons would provide a more comprehensive evaluation of MAGTrack's performance relative to state-of-the-art methods.\n\n- Limited Practical Applicability.\n\nThe reliance on large-scale MLLMs severely limits the framework's real-world applicability. Even with the smallest model used (7B parameters), the framework cannot achieve real-time performance, which is a critical requirement for tracking tasks. The reported speed of 17 FPS for the 7B model and 7.67 FPS for the 72B model highlights this issue. Given the high computational cost of MLLMs, the framework is unlikely to be practical for real-time or resource-constrained environments, which limits its utility in real-world applications such as robotics or surveillance."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rW2TC1n4hW", "forum": "IW8S0qIjTG", "replyto": "IW8S0qIjTG", "signatures": ["ICLR.cc/2026/Conference/Submission12466/Reviewer_1wzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12466/Reviewer_1wzG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832635141, "cdate": 1761832635141, "tmdate": 1762923345240, "mdate": 1762923345240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}