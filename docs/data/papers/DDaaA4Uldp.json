{"id": "DDaaA4Uldp", "number": 12243, "cdate": 1758206555932, "mdate": 1759897523024, "content": {"title": "XTransfer: Modality-Agnostic Few-Shot Model Transfer for Human Sensing at the Edge", "abstract": "Deep learning for human sensing on edge systems presents significant potential for smart applications. However, its training and development are hindered by the limited availability of sensor data and resource constraints of edge systems. While transferring pre-trained models to different sensing applications is promising, existing methods often require extensive sensor data and computational resources, resulting in high costs and poor adaptability in practice. In this paper, we propose XTransfer, a first-of-its-kind method enabling modality-agnostic, few-shot model transfer with resource-efficient design. XTransfer flexibly uses single or multiple pre-trained models and transfers knowledge across different modalities by (i) model repairing that safely mitigates modality shift by adapting pre-trained layers with only few sensor data, and (ii) layer recombining that efficiently searches and recombines layers of interest from source models in a layer-wise manner to create compact models. We benchmark various baselines across diverse human sensing datasets spanning different modalities. Comprehensive results demonstrate that XTransfer achieves state-of-the-art performance while significantly reducing the costs of sensor data collection, model training, and edge deployment.", "tldr": "This paper proposes a pioneering and scalable method that enables modality-agnostic few-shot model transfer for advancing human sensing on edge systems.", "keywords": ["Human Sensing", "Cross-Modality Few-Shot Model Transfer", "Edge AI"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e58d9d9b417bb113cef9e31b7e116b60f3d263df.pdf", "supplementary_material": "/attachment/2c6bf2eab011ccbb558e55f358c4e828369df237.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework for modality-agnostic, few-shot model transfer tailored for human sensing on edge devices. The core contributions are a splice repair removal pipeline that mitigates modality shift by aligning latent feature distributions of target sensor data with pre-trained source models using an anchor-based loss in a reduced PCA space, and a layer wise search mechanism that efficiently searches and recombines useful layers from single or multiple source models to construct a compact, high-performance target model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Tackles a practical problem at the intersection of few-shot learning, cross-modal transfer, and edge AI. The proposed method's ambition is to leverage readily available pre-trained models from vastly different modalities (e.g., image, text) for specialized sensing tasks. The quality of experimental evaluation is good."}, "weaknesses": {"value": "The method's reliance on mean magnitude of channels and its s-score as the primary metric for guiding layer repairing and selection feels under-justified. While it  is presented as a lightweight metric, its suitability for capturing feature discriminability across drastically different modalities (e.g., vision to IMU) is not intuitively clear, and a more thorough justification or comparison against other feature distribution metrics (e.g., MMD) would strengthen this core design choice. \n\nThe proposed approach is also quite complex, involving multiple components, stages, and hyperparameters (e.g., PCA dimensionality, search parameters), which could pose challenges for reproducibility and practical implementation. \n\nFinally, LWS could face scalability issues as the number and depth of source models increase and the robustness of the proposed $rate^{est}$ model for the pre-search check is not fully explored, especially in highly dissimilar transfer settings."}, "questions": {"value": "The \"Model Repairing\" component centers on aligning feature spaces to minimize MMC shift. Could you elaborate on the intuition for using a channel-magnitude metric like MMC for cross-modality transfer, where feature representations are fundamentally different? Have you experimented with alternative feature alignment techniques, such as adversarial alignment or MMD, and how do they compare?\n\nThe LWS recombines layers sequentially from source models. How does this mechanism handle non-sequential architectural elements, such as the skip connections in ResNet architectures? Are these connections discarded, or does your method have a way to preserve or reconstruct them in the final compact model?\n\nThe pre-search check's efficiency depends on an exponential growth model for the repair rate. How was this specific model form chosen, and how robust is the search process if the actual repair rate for a given source-target pair deviates a lot from assumed exponential trend?\n\nTable 4 shows that in the challenging 3-shot setting, XTransfer's accuracy is slightly below the oracle baseline on some tasks. Does this point to a fundamental limit on the minimum data required for stable alignment, and could this gap be closed by integrating few-shot data augmentation techniques into the SRR pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Lv4JwoKes", "forum": "DDaaA4Uldp", "replyto": "DDaaA4Uldp", "signatures": ["ICLR.cc/2026/Conference/Submission12243/Reviewer_F1Vc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12243/Reviewer_F1Vc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939688743, "cdate": 1761939688743, "tmdate": 1762923185188, "mdate": 1762923185188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a modality-agnostic few-shot transfer learning framework tailored for resource-constrained human-sensing applications on edge devices.\nThe method leverages pre-trained models as sources and combines layer repairing to mitigate modality shift with layer-wise recombination to select only beneficial layers, thereby producing compact, efficient models.\nThe authors evaluated the proposed method across several sensing datasets and showed state-of-the-art accuracy while reducing sensor data requirements, training cost, and deployment resource overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a timely and well-motivated challenge in human-sensing systems and tackles few-shot cross-modality transfer on resource-constrained edge devices.\n- The proposed XTransfer framework integrates a structured SRR pipeline for modality repair with a principled layer-wise recombination strategy. The design addresses both representation alignment and parameter efficiency, demonstrating a thoughtful mechanism for reusing heterogeneous pre-trained models.\n- The study evaluates the approach across multiple sensing modalities, diverse benchmarks, and real edge-device settings. Results consistently show improvements in accuracy-resource trade-offs and training efficiency, providing convincing evidence of the method's scalability and practicality for deployment."}, "weaknesses": {"value": "- The paper would benefit from a discussion of failure cases, sensitivity to noisy or highly heterogeneous sensor data, and robustness under severe domain shifts\n- The SRR and layer-wise search procedures introduce methodological complexity, and the paper does not fully quantify the tuning burden, search overhead under diverse hardware constraints, or potential stability issues when scaling to larger sets of heterogeneous source models.\n- The evaluation focuses primarily on cross-modality human-sensing tasks, and comparison to broader transfer paradigms (e.g., recent foundation-model or prompt-based adaptation techniques) is limited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ikrWWatQpM", "forum": "DDaaA4Uldp", "replyto": "DDaaA4Uldp", "signatures": ["ICLR.cc/2026/Conference/Submission12243/Reviewer_yVS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12243/Reviewer_yVS9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941906822, "cdate": 1761941906822, "tmdate": 1762923184667, "mdate": 1762923184667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces XTransfer, a cross-modal adaptation framework bridging pretrained models and sensing applications through layer manipulations. The authors propose two components, a Spliece-Repair-Removal pipeline that adapts pretrained layers to new sensor modalities using limited sensor data, and a Layer Wise Search that recombines effective layers for a compact, efficient model. The paper conducts thorough evaluations on 8 datasets and shows improved performance and resource efficiency under limited-sample scenarios. The appendix also contains ablation studies to understand the significance of the components proposed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The attempt to reuse pre-trained models from heterogeneous modalities such as images and text to accelerate sensor-domain adaptation is novel. \n2. Extending few-shot learning to a modality-agnostic context is novel and has great applicability.\n3. Evaluation across multiple modalities and domains is comprehensive.\n4. Overall the motivation is strong and the proposed method outperforms the baselines"}, "weaknesses": {"value": "- The paper needs significant work on the presentation for better clarity. Some examples below:\n    - The term channels used during the removal and repair stages was not clarified and could be misleading, given its context in signal processing.\n    - Preliminary motivation is unclear. Figure 3 shows relationships among MMC, accuracy, and other metrics, but fails to specify the details like the models and domains. Moreover, sensing as a modality remains underspecified. Baselines plotted in Figure 3 are never introduced until later sections.\n    - The methods proposed (SRR and LWS) modules are described very densely with poor structures with little intuition or top-down explanation. Figures are overcrowded and fail to clearly depict information flow across stages and are very far away from where it is referenced.\n- The authors should also compare self-supervised methods. Current work seems to be evaluating on supervised pretrained source models. However, self-supervised models already show great generalizability and cross-domain transfer capabilities. This could improve the impact of the work.\n- The authors reported the training-time statistics but do not discuss the convergence rate, especially given the inclusion of a generator-based repair module. The paper does not compare convergence speed with standard SSL or linear-probe methods, which makes it uncertain whether the proposed system actually converges faster or simply trains less data per step.\n- The LWS module is described as a search process for selecting effective layers over NAS, but it lacks a comparison against any established search or pruning methods. So it is hard to determine the significance of prior search works.\n- The newest baselines, SemiCMT, seem to be a self-supervised cross-modal alignment framework that would require paired data. It is confusing how SemiCMT was trained given there is no cross-modal pairs between source and target domains. It is unclear how the baselines are trained for fair comparison\n- It is unclear on the exact number of samples used for each source dataset, reporting only the number of classes and input shapes. Since source data scale strongly affects transfer quality, it is unclear on the cross-modality transfer performance, since image source datasets usually have a larger scale and are likely to have higher transfer performance compared to other modality source datasets. So it i s unclear on the validity of the conclusion in 6.2 Impact of different sources.\n- Most of the baselines are relatively old (19 - 22), the most recent baselines are SemiCMT which was designed for cross-modal alignment that requires multimodal pair and GPT2 which is a generative model not suitable for the downstream classification."}, "questions": {"value": "Please see the weakness for most of the concerns. Some questions for authors to discuss are:\n- Differences between area A and area B trends (where MMC correlates differently with accuracy) are not explained. Can authors provide more clarification on this?\n- Since SemiCMT requires multimodal pairing, how is it adapted to the unpaired cross-domain case where source and target modalities differ completely?\n- What are the scales of the dataset in terms of number of samples?\n- Is there any established search or pruning algorithms (e.g., NAS, lottery-ticket, or L2-pruning) used for comparison?\n- Most of the time the target domain might have more than just 10 samples per class, what happens when there are more target domain samples, would XTransfer still have the competing performance?\n- Can authors elaborate more on comparison against SSL finetune with additional input head and downstream head for cross-modal and cross-domain adaptation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mh8BNC8asi", "forum": "DDaaA4Uldp", "replyto": "DDaaA4Uldp", "signatures": ["ICLR.cc/2026/Conference/Submission12243/Reviewer_aQ89"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12243/Reviewer_aQ89"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959910971, "cdate": 1761959910971, "tmdate": 1762923184217, "mdate": 1762923184217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XTransfer, addressesing the data scarcity and resource constraints of human sensing on edge devices by enabling modality-agnostic, few-shot model transfer. It repurposes pre-trained models for diverse sensing modalities using very few labeled sensor samples. Its core pipeline includes two key components: (1) Model Repairing via a Splice-Repair-Removal (SRR) pipeline—aligning latent feature distributions across modalities; (2) Layer Recombining via Layer-Wise Search (LWS) control—selecting and recombining only useful repaired layers to build compact models. \nExperiments on 8 source datasets (image/text/audio/sensing) and 7 target datasets show XTransfer outperforms SOTA baselines"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel Modality-Agnostic Paradigm: Unlike prior transfer methods (limited to same-modality or paired cross-modal data), XTransfer achieves transferring knowledge from image/text pre-trained models to sensing modalities with few labeled data. This setting resolves the high cost of sensing data collection and leverages public pre-trained models as \"free\" knowledge sources.\n\n- Theoretically Grounded and Empirically Valid Method Mechanism: The SRR pipeline’s design (PCA orthogonal space, anchor-based loss, class pairing) is justified by Transformer layer dynamics. It effectively mitigates modality shift as evidenced by experiments.\n\n- Resource-Efficient Design for Edge Deployment: LWS control’s layer selection and pre-search check reduce model size by 2.4–16.5× in FLOPs vs. source backbones, while maintaining SOTA accuracy. On edge devices, latency is cut by 1.4–21×, making it practical for resource-constrained human sensing."}, "weaknesses": {"value": "- Dependence on PCA for Feature Alignment: XTransfer relies on linear PCA to reduce dimensionality and align features. However, the concern is that PCA fails to capture non-linear relationships between source and target modalities (e.g., text embeddings vs. Doppler radar signals), which may limit performance in highly dissimilar cross-modality scenarios (e.g., text → ECG). \n\n- Brittleness in Extremely Low-Shot Settings: While XTransfer performs well in 5–10-shot scenarios, it struggles with 3-shot settings—e.g., accuracy lags the oracle baseline on HHAR/Gesture datasets. This raises concerns for ultra-scarcity sensing tasks (e.g., rare medical conditions).\n\n- Homogeneous Source Model Assumption: The framework assumes pre-trained source models have homogeneous architectures (e.g., all ResNet variants). Extending to heterogeneous backbones is not fully validated—layer recombination across structurally diverse models (e.g., CNN vs. Transformer) may break MMC shift estimation and layer-wise dependence, limiting scalability to multi-modal source pools.\n\n- the writing can be further improved for clarity. Too many abbreviated terms,such as MMC, may weakean readability."}, "questions": {"value": "- How would XTransfer perform with non-linear feature alignment methods? \n\n- Can XTransfer be extended to ultra-low-shot (1–2-shot) scenarios?\n\n- How does XTransfer handle heterogeneous source models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R0qel6PfsU", "forum": "DDaaA4Uldp", "replyto": "DDaaA4Uldp", "signatures": ["ICLR.cc/2026/Conference/Submission12243/Reviewer_BTLu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12243/Reviewer_BTLu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011723918, "cdate": 1762011723918, "tmdate": 1762923183856, "mdate": 1762923183856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}