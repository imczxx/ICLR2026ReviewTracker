{"id": "YAWpZcXHnP", "number": 8465, "cdate": 1758085012195, "mdate": 1759897782371, "content": {"title": "RigidSSL: Rigidity-based Geometric Pretraining for Protein Generation", "abstract": "Protein design stands as one of biology’s most important frontiers, with the potential to transform medicine, advance human health, and drive sustainability. Protein generation, a central task in protein design, has been greatly accelerated by AI-driven models—such as FoldFlow, MultiFlow, and AlphaFlow that build on residue-wise rigidity–based modeling pioneered by AlphaFold2. Residue-wise rigid-body representations reduce structural dimensionality while enforcing chemical constraints, enabling more efficient and physically consistent protein structure generation than all-atom modeling. Despite these advances, existing models often underutilize the vast structural information available in large-scale protein datasets. This highlights the importance of pretraining, which can provide richer representations and improve generalization across diverse protein design tasks. More importantly, the challenge lies in how to fully exploit abundant, low-cost unlabeled protein datasets using unsupervised pretraining. We introduce RigidSSL, a rigidity-based pretraining framework for proteins. RigidSSL canonicalizes structures into an inertial frame, employs a two-phase workflow combining large-scale perturbations and molecular dynamics views, and applies a rigid-body flow matching objective with Invariant Point Attention to capture global geometry. This enables learning stable, geometry-aware representations that improve downstream protein generation. To evaluate the effectiveness of RigidSSL, we conduct quantitative experiments on the protein generation task. Empirically, RigidSSL outperforms previous state-of-the-art geometric pretraining algorithms, leading to improvements in unconditional generation across all metrics, including designability, novelty, and diversity, for length up to 800 residues.", "tldr": "We proposed a rigidity-based SSL framework to help protein generation.", "keywords": ["Protein design", "Self-supervised Learning", "3D Geometry", "Rigidity", "Flow matching", "SE(3)-equivariance"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/853a4257df9f3537a757bff298db82da64df7a5e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a pretraining method for an unconditional protein backbone structure generation model. Experimental results demonstrate that this approach enhances the performance of FrameDiff and FoldFlow2 in generating long proteins and high-quality proteins that are not purely helical."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem addressed by this method is highly important. A major issue with unconditional protein generation models is that the generated designable proteins are predominantly helical. This method alleviates this problem to some extent through pretraining.\n\n2. The pretraining strategy used in this method is relatively general and trains fast.\n\n3. The experimental section follows established conventions and provides strong support for the claims made in the paper."}, "weaknesses": {"value": "1. Some descriptions are somewhat confusing. For example, it is unclear whether Phase 1 and Phase 2 represent two different strategies or if they are sequentially related.\n\n2. The case study lacks illustrations or examples of designable proteins with a low proportion of helices."}, "questions": {"value": "1. The batch size during the pretraining stage is set to 1, and no analysis related to batch size is provided. Most existing generation models demonstrate that a larger batch size often leads to better training performance. Why a larger batch size was not used in this work?\n2. Can this approach naturally extend to the transformer decoder framework like AlphaFold 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HpchqOHfaN", "forum": "YAWpZcXHnP", "replyto": "YAWpZcXHnP", "signatures": ["ICLR.cc/2026/Conference/Submission8465/Reviewer_1qeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8465/Reviewer_1qeX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652614198, "cdate": 1761652614198, "tmdate": 1762920347562, "mdate": 1762920347562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a rigidity-based geometric pretraining method for protein backbone generation. For perturbation-based view construction on the large-scale AFDB dataset and molecular dynamics trajectories of the ATLAS dataset, they canonicalize proteins into a reference frame and use bi-directional flow matching to pretrain the IPA module, which is shared by FrameDiff, FoldFlow-2."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The SE(3) rigidity pretraining for protein backbone generation is reasonable. Perturbations on rigidity align with the protein's natural conformational fluctuations, which can be interpreted as a masking-like paradigm. The MD snapshots used for pretraining are novel and interesting."}, "weaknesses": {"value": "**W1. It is hard to determine whether the performance gains stem from the introduction of new data (e.g., AFDB, ATLAS) or the proposed rigidity-based geometric pretraining method. (My main concern)**\n\nThe impact of RigidSSL-MD on diversity has been analyzed in lines 408–411 and Section 5, I think the improvements of diversity are attributed to the new data in the ATLAS dataset. \n\nFor RigidSSL-Perturb, both FrameDiff and FoldFlow2 achieved improvements in designability and novelty. However, \n\n(1). FrameDiff was trained only on a small PDB dataset (\\~20K), while RigidSSL-Perturb incorporates new data from the AFDB (\\~432k). \n\n(2). The training length range for FoldFlow2 is 60-384, while RigidSSL-Perturb ranges is 60-512. For evaluations, the generated length is 100-600. This is unfair. \n\nI suggest authors:\n\n(1). Show FoldFlow-2 + RigidSSL-Perturb results for 100-300 instead of 100-600.\n\n(2). Add a comparison: train baselines Framediff and FoldFlow-2 on the same AFDB and ATLAS datasets using their original training objectives. In this case, comparison on 100-600 is OK.\n\n(3). Use a table in the main body of the paper to present data sources, numbers, protein length ranges, training objectives, training time, and other relevant information of various methods (including training, pretraining, and fine-tuning methods). This will make the paper's contributions clearer.\n\n**Note**: In my opinion, if my understanding is correct, previous CV/NLP tasks usually have scarce data labels, so we adopt self-supervised learning to utilize large amounts of unlabeled data. However, the unconditional protein backbone generation task here is different. We can directly utilize data from AFDB for training the original SE(3) flow matching. FoldFlow-2 [1] also validated that directly using new AFDB data (+PDB, \\~160K filtered data in total) for training yields performance improvements. This is why I have the ‘W1’ concern. However, I still believe that the rigidity-based geometric pretraining could be beneficial. Directly learning noise to protein means simultaneously learning the fundamental rules of protein geometry and the complex task of de novo protein design, which may be challenging. RigidSSL decouples these challenges by first pretraining on large-scale data to learn a robust geometric representation that serves as a good initialization. This motivation is reasonable for me, but the authors may need to clarify the specific benefits of RigidSSL according to suggestions (1)-(3).\n\n[1].Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation\n\n**W2. Referring to RigidSSL-Perturb as phase 1 and RigidSSL-MD as phase 2 is confusing. Particularly, they are two independent pre-training methods in the experiment part. What are the results of their combined use? Can you provide further experimental results and analysis?**\n\n**W3. Can you show some results on the conditional generation experiments, e.g.,  motif scaffolding?**\n\n\nI will raise my score if my concerns are resolved."}, "questions": {"value": "**Q1. I'm curious whether the parameters of these equivariant models based on IPA is hard to scale? For rigidity-based geometric pretraining, thanks to dataset like AFDB, we can scale up the data size during the pretrain phase easily. But can we scale the equivariant models based on IPA module like Proteina[1], SimpleFold[2]?**\n\n\n[1].Proteina: Scaling Flow-based Protein Structure Generative Models\n\n[2].SimpleFold: Folding Proteins is Simpler than You Think"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ghmPIK6dBC", "forum": "YAWpZcXHnP", "replyto": "YAWpZcXHnP", "signatures": ["ICLR.cc/2026/Conference/Submission8465/Reviewer_8zoL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8465/Reviewer_8zoL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985360834, "cdate": 1761985360834, "tmdate": 1762920347052, "mdate": 1762920347052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pretraining method for protein generation by viewing each residue structure as a rigid body and using flow matching. Specifically, it first defines an inertial frame as a reference frame for each protein and aligns protein backbone structure with the inertial frame; then, it perturbs each protein's translation and rotation by randomly sampling transforms from an Euclidean space and a special orthogonal group to form two views; next, it samples trajectory segments from a molecular dynamics dataset; finally, it uses flow matching to generate each view from the other views. The experiments are conducted on a protein generation task and the results outperform the state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It utilizes the structural information available in large-scale protein datasets to pretrain a protein generation model in an unsupervised manner.\n\n2. It achieves superior protein generation performance to the compared approaches."}, "weaknesses": {"value": "1. This paper is more engineering-oriented. Though it achieves superior performance across several models on a protein generation benchmark, it seems to contain little new algorithms or architectures. Reference frame definition and flow matching are widely used across many areas, including, but not limited to, machine learning, computer vision, and computational biology.\n\n2. The construction of two different conformation views is a little bit new, but the motivation for such a construction is unclear. We can directly sample two time steps in a dynamic trajectory and generate protein structures at one time step to the other."}, "questions": {"value": "1. In Section 3.2.1, $g^0$ and $g^1$ represent the original state of one protein and the perturbed state by adding random noise; in Section 3.2.2, $g^0$ and $g^1$  represent different conformations sampled at different time steps along a dynamic trajectory. This inconsistency is confusing.\n\n2. In Section 3.2.1, the other view is constructed by adding random noises sampled from Euclidean space and the SO(3) group. How to make sure the perturbed view is biologically valid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H24jmIThg7", "forum": "YAWpZcXHnP", "replyto": "YAWpZcXHnP", "signatures": ["ICLR.cc/2026/Conference/Submission8465/Reviewer_yn5Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8465/Reviewer_yn5Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075691916, "cdate": 1762075691916, "tmdate": 1762920346575, "mdate": 1762920346575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a pre-training procedure for diffusion and flow-matching based protein design methods. The pre-training method consists of two phases. The first phase, RigidSSL-Perturb, adds Gaussian noise to the translation and rotation component of the residual frames of a protein's backbone. The second phase, RigidSSL-MD, uses pairs of structures from the ATLAS dataset, which contains molecular dynamics simulations of proteins."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Incorporating MD simulations into a pre-training step of protein design methods is an interesting and novel contribution."}, "weaknesses": {"value": "The experiments demonstrate that RigidSSL-Perturb outperforms the baselines for designability and novelty, while RigidSSL-MD outperforms the baselines in diversity. However, RigidSSL-MD is not the methods of choice with respect to designability and novelty. These results limit the appilcability of the approach, as there seems no practical advantage of RigidSSL-MD over RigidSSL-Perturb, which is merely a simple data-augmentation of the data with Gaussian noise.\n\nThe examples of generated structures in figure 3 for RigidSSL seem to solely consist of alpha helices, a common bias in generative models for proteins (compare e.g. with Wagner et al. 2024). Please also report alpha helix and beta strand content for each method.\n\nReferences:\nWagner, S., Seute, L., Viliuga, V., Wolf, N., Gräter, F., & Stühmer, J. (2024). Generating highly designable proteins with geometric algebra flow matching. Advances in Neural Information Processing Systems, 37, 77987-78026."}, "questions": {"value": "Is high designability of RigidSSL-Perturb potentially only achieved by a high amount of alpha helices?\n\nThe augmentation of RigidSSL-Perturb, which adds Gaussian noise to the coordinates and rotation, could destroy physical plausibility of the proteins. How is the noise level controlled to prevent this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zbZ0aUvtcc", "forum": "YAWpZcXHnP", "replyto": "YAWpZcXHnP", "signatures": ["ICLR.cc/2026/Conference/Submission8465/Reviewer_LsQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8465/Reviewer_LsQ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8465/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762542947073, "cdate": 1762542947073, "tmdate": 1762920346194, "mdate": 1762920346194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}