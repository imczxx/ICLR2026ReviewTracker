{"id": "MjJrhfAmRG", "number": 5264, "cdate": 1757881938823, "mdate": 1759897984410, "content": {"title": "From Answer to Think: Multidimensional Supervision of Reasoning Process for LLM Optimization", "abstract": "Large language models (LLMs) can develop strong reasoning ability when trained appropriately. Existing approaches are broadly categorized into outcome-level answer supervision and process-level reasoning supervision. \nHowever, the former provides only sparse binary feedback and overlooks intermediate step quality, while the latter scores individual steps but requires task-specific segmentation.\nTo this end, we propose a novel framework that assesses the quality of reasoning process along three dimensions: **Confidence** for uncertainty calibration, **Relevance** for semantic alignment and **Coherence** for logical consistency. \nTogether, these dimensions capture aspects beyond final answer correctness and enable interpretable assessment without requiring ground truth answers.\nOur framework serves as a Dimension-level Reward Model (**DRM**) that assigns scores to reasoning processes and provides supervision signals for both off-policy (e.g., DPO) and on-policy (e.g., GRPO) optimization.\nExperimental results show that DRM provides effective supervision signals, guides the optimization of LLMs and enhances their reasoning ability.\nIn particular, DRM-supervised training achieves consistent gains on both in-distribution and out-of-distribution open-domain tasks, including mathematics, question answering, code execution and puzzles.\nOur findings demonstrate that multidimensional supervision of reasoning process can improve the generalized reasoning ability of LLMs beyond the training distribution.", "tldr": "We provide multidimensional supervision over the reasoning process, thereby enhancing both the reasoning ability and generalization of large language models.", "keywords": ["Large Language Model", "Reinforcement Learning", "Reasoning", "Natural Language Processing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f026de7380052fa4c4989b76d2213cc63f654faa.pdf", "supplementary_material": "/attachment/ae2584e84a089e5422330b28444d9b66f7350627.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DRM , a new framework that supervises large language models’ reasoning processes across three interpretable dimensions—confidence, relevance, and coherence—to more effectively improve reasoning ability and generalization than traditional outcome-only training methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is simple and reasonable. Experiments show the effectiveness of DRM.\n\n- DRM can simple replace what orm did in different algorithm."}, "weaknesses": {"value": "- In table 3, the improvement is limited, not sure whether the improvement only exist in given hyperparameter."}, "questions": {"value": "- In the paper, the weights for the three dimensions are tuned empirically or via grid search. Were these weights optimized separately for each task, or were they fixed across all tasks? Also, did you compare whether different tasks (e.g., math vs. code generation) require different optimal weight configurations?\n\n- The multi-dimensional supervision seems to rely on several external evaluators. Could you clarify how much additional computational overhead or latency this introduces during training and inference?\n\n- Did you conduct any analysis on how DRM affects the reasoning process qualitatively — for instance, does it make the model’s explanations longer, more structured, or more confident?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0a1hB93q1S", "forum": "MjJrhfAmRG", "replyto": "MjJrhfAmRG", "signatures": ["ICLR.cc/2026/Conference/Submission5264/Reviewer_pnbE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5264/Reviewer_pnbE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497921766, "cdate": 1761497921766, "tmdate": 1762917978667, "mdate": 1762917978667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Dimension-level Reward Model (DRM) that scores a model’s reasoning process along three complementary dimensions—Confidence, Relevance, and Coherence—and uses this multidimensional signal to supervise both off-policy (DPO + SFT) and on-policy (GRPO-style) optimization. Unlike answer-only reward schemes (RLVR) that deliver sparse, outcome-level feedback and often reward “correct answer, flawed reasoning,” and unlike process-level reward models (PRMs) that require task-specific step segmentation, DRM delivers dense, interpretable, ground-truth-free rewards over the entire chain of thought."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. DRM directly targets two known gaps—sparse/answer-only rewards and PRM step-segmentation requirements—by shifting to dimension-level scoring that is dense, ground-truth-free, and interpretable.\n\n2. The DRM reward can be integrated with standard training. It supervises off-policy DPO+SFT (pair selection) and augments on-policy GRPO (added advantage).\n\n3. Across diverse tasks and backbones, DRM-supervised models outperform native and strong baselines."}, "weaknesses": {"value": "1. Relevance depends on a reranker and Coherence on an ORM; the paper fixes dimension weights via grid search. While practical, robustness to judge/model choice and weight calibration is under-analyzed. \n\n2. Using log-prob as self-confidence is intuitive, but there’s limited study of calibration across domains/backbones or under distribution shift, and little comparison to alternative confidence estimators.\n\n3. In GRPO combinations, a few reasoning-heavy or knowledge-intensive datasets see slight regressions vs. DRM alone (e.g., MuSR/GPQA), suggesting interaction effects between answer-only and reasoning rewards that merit deeper analysis."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5yveZfO4Lw", "forum": "MjJrhfAmRG", "replyto": "MjJrhfAmRG", "signatures": ["ICLR.cc/2026/Conference/Submission5264/Reviewer_ubnr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5264/Reviewer_ubnr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832722226, "cdate": 1761832722226, "tmdate": 1762917978381, "mdate": 1762917978381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a novel framework that assesses the quality of the reasoning process along three dimensions: (1) Confidence for uncertainty calibration, (2) Relevance for semantic alignment, and (3) Coherence for logical consistency. Through extensive experiments, the authors show that Dimension-level Reward Model (DRM) can successfully provide supervision signals for both off-policy and on-policy optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work innovatively proposes Dimension-level Reward Model for both off-policy and on-policy optimization, and demonstrates the effectiveness of incorporating metrics of reasoning process (e.g., Confidence, Relevance, and Coherence) over vanilla outcome reward.\n2. This work has done extensive experiments on the advantage of DRM, revealing new findings on process reward.\n3. This work introduces a baseline to merge both process and outcome rewards for on-policy optimization, i.e., simply adding the advantage of both rewards. This opens up a new line of research, and is a significant contribution."}, "weaknesses": {"value": "This work does not investigate deeply how to design a good process metric. Although the dimension-level ones (i.e., Confidence, Relevance, and Coherence) are proposed, more design choices should be compared in calculating the process reward metrics. Also, the final results heavily depend on the accuracy of the process metrics. For example, in cases where the Confidence score mistakenly assigns a flawed reasoning process with a high score, the RL training would be negatively affected. From this perspective, the authors should discuss more on how to calculate the scores."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6yfXuq4ewA", "forum": "MjJrhfAmRG", "replyto": "MjJrhfAmRG", "signatures": ["ICLR.cc/2026/Conference/Submission5264/Reviewer_JprV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5264/Reviewer_JprV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5264/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982244384, "cdate": 1761982244384, "tmdate": 1762917978081, "mdate": 1762917978081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}