{"id": "jUrmXDgQct", "number": 20226, "cdate": 1758303876306, "mdate": 1759896989695, "content": {"title": "Video2Reaction: Mapping Video to Audience Reaction Distribution in the Wild", "abstract": "Understanding audience reactions to video content is crucial for improving content creation, recommendation systems, and media analysis. We introduce $\\textbf{Video2Reaction}$, a multimodal dataset that maps short movie segments to the $\\textit{distributional induced emotional reactions}$ of viewers in the wild, as expressed through social media. Unlike most prior datasets that focus on $\\textit{perceived emotions}$—i.e., the emotions portrayed by characters in a movie clip—$\\textbf{Video2Reaction}$ centers on the induced emotions triggered by the movie clip. Additionally, we model these reactions as $\\textit{distributions}$ over categorical emotions, rather than reducing them to a single dominant label, enabling fine-grained learning of collective emotional responses. $\\textbf{Video2Reaction}$ can support a range of applications, including audience reaction prediction for new video content, emotion-aware video retrieval, and content optimization based on expected viewer engagement. By providing a comprehensive benchmark for distributional video-to-reaction modeling, $\\textbf{Video2Reaction}$ advances the study of audience engagement and emotional impact in multimedia content. The dataset is available at https://huggingface.co/datasets/video2reac/Video2Reaction.", "tldr": "We introduce Video2Reaction, a large-scale dataset for modeling the distribution of induced audience reactions to movie clips, enabling research on video-based emotional impact and engagement prediction.", "keywords": ["multimodal", "multimedia", "sentiment analysis", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87b6ee89528805343b1e7b884e4bf00a13c4914d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors pointed out a critical gap in affective computing research—the limited attention to induced emotion recognition, which focuses on understanding how audiences emotionally respond to video content, rather than the perceived emotions conveyed by characters or intended by filmmakers. Shifting the emphasis from the creator’s intent to the viewer’s reaction is essential for advancing content creation, media analysis, and personalized recommendation. To address this gap, the authors introduce Video2Reaction, a large-scale multimodal dataset comprising over 10,000 movie clips, each annotated with audience reaction distributions derived from real-world viewer comments. Unlike prior datasets that rely on single-label or perceived emotion annotations, Video2Reaction is annotated using the distribution of emotional responses, capturing the diversity and nuance of audience reactions. The authors further benchmark a wide range of existing methods to evaluate their strengths and limitations on this new task."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide a comprehensive and well-documented description of the experimental setup and protocol design. The experiments encompass a wide range of methods, effectively establishing strong and diverse baselines.\n2. The benchmark is described in a thorough and well-organized manner, making it easy for readers to follow\n3.  The figures and tables are clear"}, "weaknesses": {"value": "1. A key concern lies in the reliability of the emotion ground truth. The authors note that many viewer comments may reflect personal attitudes toward actors or filmmakers rather than genuine emotional responses to the video content. To mitigate this issue, they employ prompt-based filtering with minimal human intervention. Since this step is critical to preserving the purity and validity of the dataset, it would be beneficial to provide an evaluation of the filtering process to demonstrate its effectiveness. Moreover, the authors should include evidence that the employed agent models are capable of accurately extracting fine-grained emotional cues from comments, as this capability is a core assumption underlying the reliability of the annotations. However, based on the results in Table 6, the foundation VLM appears to lack the ability to capture fine-grained emotions; therefore, it remains unclear why the LLM would possess this capability merely when the video caption is provided.\n\n2. The authors propose to use emotion distributions for comparison; however, it would be beneficial to clearly demonstrate the advantages of this distribution-based approach over traditional single-label annotation. According to the paper, only up to three human annotations are used to assess label validity, which appears insufficient to ensure the reliability of the annotations. Moreover, for the emotion distribution itself, it remains unclear how the proportion of each emotion is determined and validated—this aspect needs further clarification and justification.\n\n3. It would be helpful to include a more explicit comparison between this work and related studies [1]. That prior work also targets viewer-centered emotion recognition, using a combination of three emotions as ground-truth labels to account for emotional mixtures, with video sources collected from social media. In comparison, the current dataset is derived from films, which are typically more scripted and contextually constrained. From this perspective, the previous work may better align with the notion of “in-the-wild” data, as it includes more spontaneous content such as dashcam footage.\n\n4. Another potential concern is the bias in viewer comments, as not everyone is equally likely to leave a comment. Those who do are often younger or more extroverted individuals, which may skew the emotional responses captured in the dataset. Consequently, the collected emotions may primarily reflect the reactions of this particular group of people rather than representing the broader audience.\n\n5. For highly intense emotions such as anger or fear, the emotional trigger often occurs within a very short time frame. It would be helpful to clarify whether representing each scene with a single frame is sufficient to capture such transient emotional expressions.\n\n6. The authors use clip descriptions provided by the channel, which are likely to be general summaries lacking detailed information about specific scenes or objects within each frame. The absence of these critical triggers in the captions makes it difficult to establish a reliable connection between human emotional reactions and the actual video content.\n\n\n[1] Mazeika, M., Tang, E., Zou, A., Basart, S., Chan, J. S., Song, D., ... & Hendrycks, D. (2022). How would the viewer feel? Estimating wellbeing from video scenarios. Advances in Neural Information Processing Systems, 35, 18571-18585."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BXObw2a5Ft", "forum": "jUrmXDgQct", "replyto": "jUrmXDgQct", "signatures": ["ICLR.cc/2026/Conference/Submission20226/Reviewer_AxqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20226/Reviewer_AxqW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514535389, "cdate": 1761514535389, "tmdate": 1762933723331, "mdate": 1762933723331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Video2Reaction, a dataset of 10,348 movie clips paired with audience reaction distributions derived from YouTube comments. The authors claim this is the first dataset capturing distributional induced emotions from cinematic content \"in the wild.\" They use a two-stage LLM-based pipeline to annotate comments and benchmark various Label Distribution Learning (LDL) algorithms on the task of predicting reaction distributions from video features alone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Large scale dataset with 400 hours of video and 800K comments - One of the largest multimodal emotion datasets to date, providing substantial data for model training\n\nAddresses the distinction between perceived and induced emotions\n\nComprehensive benchmarking across multiple algorithm categories. The evaluation spans PT, SA, AA, and foundation VLMs and provides useful baseline comparisons\n\nTwo-stage annotation pipeline with multi-agent voting that is highly scalable and could support future dataset expansion\n\nProvides both full distribution and dominant reaction evaluation metrics \n\nThe paper is clearly written with well-structured sections and informative figures."}, "weaknesses": {"value": "The Validation Protocol for the LLM pipeline is not necessarily convincing. The human verification shows LLM predictions to reviewers before asking if they're \"correct,\" possibly introducing confirmation bias. The reported 86% accuracy is meaningless under this flawed protocol.\n\nThe paper conflates \"audience reactions\" with \"YouTube commenter reactions.\" Commenters possibly represent only 1-5% of viewers and are possibly also self-selected for stronger emotional engagement. The dataset therefore doesn't measure exactly what it claims to measure. (Thought they somewhat address it in the limitations, this could have been discussed more prominently.)\n\nThe related work section could have better contextualized existing datasets that would enable validation of the annotation pipeline.\n\nLack of validation on existing emotion–comment datasets: No attempt to validate the LLM-based annotation pipeline on established datasets (e.g., Cong Xu, Lu Liu, Liang Jin, Guoguang Du, Zhenhua Guo, Yaqian Zhao, Xuanjing Huang, Rengang Li, et al. Infer induced sentiment of comment response to video: A new task, dataset and baseline. Advances in Neural Information Processing Systems, 37:103737–103750, 2024. ) where ground truth emotion labels for comments already exist.\n\nAlthough the dataset is large and thematically somewhat relevant, the methodological flaws (biased validation and unclear representativeness) limit its usefulness for future research."}, "questions": {"value": "Why use a validation protocol that exposes reviewers to LLM predictions rather than blind annotation?\nHow do you justify treating YouTube commenters as representative of the general viewing audience?\nWhy was the annotation pipeline not validated on existing comment–emotion datasets?\nHow do you account for differences in comment count? Was there an artificial cap of 100 comments? (Max value is 100 in table 3a)\nHow did you handle your dataset splits given the dataset imbalance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Justify whether humans should be informed and compensated."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qYRMzMCRhe", "forum": "jUrmXDgQct", "replyto": "jUrmXDgQct", "signatures": ["ICLR.cc/2026/Conference/Submission20226/Reviewer_vNMM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20226/Reviewer_vNMM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815899887, "cdate": 1761815899887, "tmdate": 1762933722598, "mdate": 1762933722598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Video2Reaction, a large-scale multimodal benchmark that maps movie clips to the distribution of audience emotions expressed in social-media comments. Unlike traditional perceived-emotion datasets, it focuses on induced emotions , what viewers feel rather than what characters portray. It contains over 10k clips (400 h) from 1.5 k movies with 800 k comments, annotated through a two-stage multi-agent LLM pipeline and verified by humans. The task is formulated as label-distribution learning (LDL), and the benchmark compares classical LDL, multimodal adaptations, and foundation vision-language models. Results show that specialized LDL algorithms (eg SA-BFGS) perform best, while VLMs lag behind in zero-shot settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Largest known dataset for induced audience emotion in the wild.\nThoughtful two-stage LLM annotation pipeline with partial human auditing.\nComprehensive benchmark with multiple algorithm classes and clear metrics.\nHighlights meaningful gaps between specialized LDL models and general-purpose VLMs."}, "weaknesses": {"value": "Annotation reliability and bias analysis are thin; LLM bias or sarcasm mislabeling could skew distributions.\nDataset remains culturally and platform-biased (YouTube only).\nResults show moderate accuracy; limited insight into failure cases.\nContribution is more engineering than conceptual. Some claims about 'first of its kind' feel overstated given existing induced-emotion datasets (eg CMSV 2024)."}, "questions": {"value": "How does the LLM ensemble handle cross-lingual comments?\nCan future updates dynamically re-label older clips to track temporal drift?\nWere annotators or models ever confused by sarcasm or meme responses?\nCould foundation models be fine-tuned efficiently using this dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lQ8VlWpVnp", "forum": "jUrmXDgQct", "replyto": "jUrmXDgQct", "signatures": ["ICLR.cc/2026/Conference/Submission20226/Reviewer_3vdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20226/Reviewer_3vdS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947080050, "cdate": 1761947080050, "tmdate": 1762933722063, "mdate": 1762933722063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors collect Video2Reaction, a dataset of 10K short movie segments from YouTube.  These segments are paired with a distribution over the categorical emotions that they elicit, derived from LLM-based labeling of their co-occurring comments.  They propose using Video2Reaction as a benchmark whose aim is predicting the distribution of audience reactions to video content.  They evaluate several different label distribution learning methods on the same set of multimodal features and a few video-language models on their benchmark using a selection of metrics they curate for the task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The writing in this paper is very clear – it’s easy to follow what the authors did.\n\nThe author’s chosen problem, modeling the distribution of user emotion reactions, is an interesting one that is relatively understudied in the literature."}, "weaknesses": {"value": "### Synthetic labels as a test set\n\nA significant concern with the proposed benchmark is that the audience emotion distribution is synthetically derived from user comments.  While this would be fine for a training dataset, typically, for a test set, we want to ensure we have gold labels.  The authors perform a manual analysis of the their multi-LLM-predicted labels and find that they agree with 86% of them – what does a disparity of 86% at the label level imply for the noise in the derived audience emotion distributions?  Can the authors quantify that somehow?  What does it mean for the ceiling performance we should expect from models that we evaluate?  \n\nAdditionally, it would be nice to see a confusion matrix for the emotion categories that were manually evaluated as some labels (like anger / embarrassment) were wrong 100% of the time.  That could very well be due to the small number of examples of anger in the manual evaluation set but that is not convincing on its own – would it be possible for the authors to manually annotate comments that reflect a balanced set of predicted emotions?  \n\nAnother convincing result would be seeing that fine-tuning a video model on the training data in Video2Reaction results in a model that adapts well to a different audience emotion dataset with higher quality labels (for example, the work of Xu et al).  Then, we might have greater confidence in Video2Reaction’s derived labels.\n\n### Weak baselines\n\nWhile I am sympathetic to the compute demands of video modeling, a new video understanding benchmark demands evaluation against stronger baselines than those presented in this paper.  The multimodal features chosen are all quite dated / naïve (from 2020 – 2021) and the VLMs are fairly small.  The authors describe being unable to do model fine-tuning on their dataset – could they evaluate proprietary foundation models like Gemini to help us better understand what current ceiling performance is?  As is, it is difficult for researchers/practitioners who are versed in the strengths / weaknesses of the state-of-the-art models to make sense of this benchmark and its results.  \n\n### Lack of qualitative analysis\n\nTypically, a new benchmark should be published with some qualitative analysis of model performance for others looking to pursue the task.  Which cases do models do well on?   Where do they struggle?  What makes this benchmark in particular hard?  This paper would benefit from this kind of analysis.\n\n### Related work\n\nThe related work discusses many different methods for label distribution modeling but as the paper is not actually presenting a new method for label distribution modeling, this strikes me as unnecessary.  Instead, it would be nice to see the inclusion of more video datasets that leverage user comments (for comment generation or learning affective features) as these seem more related to the proposed task (modeling user responses).  Please see the list below:\n- video comment generation: LiveBot (Ma et al, AAAI 2019), VideoIC (Wang et al, ACM 2020), HOTVCOM (Chen et al, ACL 2024), Personalized Video Comment Generation (Lin et al, EMNLP 2024)\n- affective features learned from co-occurring video-comments: Enhancing Multimodal Affective Analysis with Learned Live Comment Features (Deng et al, AAAI 2025)\n\n### Nitpicks\n- when doing inline citations, please use \\citet (for example in lines 316, 321)\n- “foundation” typically refers to large-scale (and often proprietary) models like DeepSeek, GPT5, Claude, Gemini, etc; in this case, the authors are evaluating relatively small 7B parameter models"}, "questions": {"value": "The authors frame their derived emotion distributions as reflecting audience reactions to a particular video and they caveat (appropriately) in their limitations that this may not reflect global audience reactions but can they motivate the degree to which this reflects local audience reactions as well?  I would think there is a strong selection bias in who looks up / watches / comments on a particular video even with the same geographic area.  I don’t think this is a dealbreaker for the work but I do think it requires addressing.  \n\nThe authors describing splitting videos into clips.  How are the comments for the video then associated with the segmented scenes?  Are all comments from the original video mapped to its parts? \n\nWhat is the input to the VLMs you evaluate?  Is it just frames?  Do they get transcript information too?  \n\nTo what degree does modeling reactions require prior movie context?  Viewers have likely seen all of the prior scenes/clips when re-watching a particular one -- their reactions likely reflect that prior knowledge.  Can the authors test this somehow?  Perhaps, if adding Gemini experiments, see if the model predicts the emotion distribution for scene t more accurately if it’s also seeing scenes 1 through t – 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A7sqbYLgnP", "forum": "jUrmXDgQct", "replyto": "jUrmXDgQct", "signatures": ["ICLR.cc/2026/Conference/Submission20226/Reviewer_tQDy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20226/Reviewer_tQDy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20226/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959781269, "cdate": 1761959781269, "tmdate": 1762933721766, "mdate": 1762933721766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}