{"id": "uUtIyxeBUc", "number": 2883, "cdate": 1757297758072, "mdate": 1759898121286, "content": {"title": "LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation", "abstract": "Despite the impressive advancements of Large Vision-Language Models (LVLMs), existing approaches suffer from a fundamental bottleneck: inefficient visual-language integration. Current methods either disrupt the model's inherent structure or introduce long-context computational burdens, severely limiting scalability and efficiency. In this paper, we rethink multimodal integration and present LaVi, a novel LVLM that enables seamless and efficient vision-language fusion through internal feature modulation within the Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token concatenation, LaVi sidesteps long-context expansion by injecting vision-conditioned deltas into the affine parameters of LayerNorm, a ubiquitous component in modern LLMs. This lightweight transformation makes visual input directly modulate the linguistic hidden states, grounding the next-token probabilities in visual evidence. LaVi achieves precise vision–language alignment while retaining the linguistic priors and substantially reducing computation. Across 18 benchmarks covering images, video and language, it attains state-of-the-art multimodal performance without sacrificing linguistic capability. In addition, LaVi delivers significant efficiency gains. Compared to LLaVA-OV-7B, it reduces FLOPs by 94.0%, accelerates inference by 3.1×, and halves memory consumption. These properties make LaVi a scalable and practical framework for real-time multimodal reasoning. Code and models will be released.", "tldr": "", "keywords": ["Large Vision-Language Models", "Feature Modulation", "Efficient"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a14f7b280ddf154a49f51b56871993eca534306.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LaVi, a new framework for building efficient Large Vision-Language Models  through a mechanism called Feature Modulation Injection (FMI), which performs **vision-conditioned modulation** directly within the **LayerNorm** modules of a pretrained LLM. This design achieves minimal architectural disruption and avoids quadratic scaling in sequence length. Experiments show that LaVi attains SOTA performance across 18 benchmarks while reducing FLOPs by up to 94% and inference latency by 3× compared to LLaVA-style models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* **Eelegant integration mechanism**: The proposed **internal modulation** paradigm is conceptually elegant: by modulating existing LayerNorm parameters instead of extending architecture or ccontext length, the model fuses visual information in a manner that is both efficient and minimally intrusive. This is a fresh alternative to the widely used cross-attention or concatenation strategies.\n* **Strong empirical results and efficiency**: The experimental results are thorough, spanning 9 image and 6 video benchmarks (Tables 1–2). LaVi consistently matches or exceeds prior models’ accuracy while drastically reducing FLOPs and latency. The quantitative efficiency gains (e.g., 94% FLOP reduction, 3.1× faster inference) are impressive and well-documented.\n* **Comprehensive albation studies** (including sublayer modulation, modulation parameters, etc) clearly justify design choices and demonstrate robustness."}, "weaknesses": {"value": "* The paper could benefit from more qualitative examples (e.g., failure cases or visualization of token-level modulation effects) to illustrate how visual context influences language outputs.\n* The benchmarks focus on VQA-style metrics; it should be also evaluated on **free-form visual reasoning or caption generation** tasks to confirm broader applicability, as using vision-conditioned deltas to directly modify internal activations maybe very unstable under complex or long-sequence contexts."}, "questions": {"value": "* Have the authors analyzed whether FMI affects the *reasoning depth* or *chain-of-thought consistency* in multimodal tasks? For instance, do FMI models preserve multi-step visual reasoning accuracy compared to in-context injection models?\n* Since vision-conditioned deltas directly modify internal activations, what happens when visual inputs are noisy, unrelated, or adversarial? Is the modulation mechanism robust to such perturbations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BDbibENSZv", "forum": "uUtIyxeBUc", "replyto": "uUtIyxeBUc", "signatures": ["ICLR.cc/2026/Conference/Submission2883/Reviewer_nXfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2883/Reviewer_nXfH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450952864, "cdate": 1761450952864, "tmdate": 1762916429706, "mdate": 1762916429706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LaVi, a Large Vision-Language Model (LVLM) that proposes a new paradigm for vision-language integration by injecting vision-conditioned modulation signals into the affine parameters of LayerNorm modules inside Large Language Models (LLMs). Instead of concatenating visual tokens or modifying model architectures with additional cross-modal layers, LaVi computes visual feature deltas which adaptively modulate internal linguistic hidden states via a Vision-Infused LayerNorm (ViLN) module. The approach aims to minimize structural interference with trained language priors and dramatically reduces computational and memory overhead. Comprehensive experiments are presented across both image and video benchmarks, and the paper analyzes efficiency, performance, and ablation of design choices."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Integration Mechanism: The core FMI/ViLN idea—using vision-conditioned deltas to modulate LN affine parameters inside LLMs—is a creative design that shifts the paradigm away from standard token concatenation or explicit cross-modal layers. This is well-motivated in the text (Section 3, Equation for ViLN, Figure 2(c)), and implementation variants (MLP, Conv, Attention) are systematically explored.\n\n2. Quality of Visuals: The architecture diagrams (Figures 2, 3) and efficiency plots (Figures 8, 9) are well-constructed and directly support the claims."}, "weaknesses": {"value": "1. **Weak Positioning to Most Recent and Directly Related Work**: Several highly relevant contemporary works are missing from the related work and comparative analysis (see below), especially those focusing on efficiency (e.g., ATP-LLaVA [1]), cognitive alignment (Beyond Sight [2]), and alternative integration or scaling approaches (Mono-InternVL [3], LLaVA-CoT [4], etc.). This is a significant shortfall given the rapid evolution in the LVLM landscape. The related work (Section 2) is too generic and does not situate LaVi tightly within the latest field advances.\n\n2. **Insufficiently Nuanced Empirical Comparison with SOTA**: While LLaVA, Qwen-VL, and other baselines are included, several top recent efficient or cognitively-aligned LVLMs are absent in Tables 1 and 2—direct apples-to-apples comparisons to works like ATP-LLaVA, Mono-InternVL, or LLaVA-CoT are missing. This makes it difficult to fully contextualize the claimed efficiency/performance trade-offs.\n\n3. **Limited Novelty in Conditioning Mechanisms**: The three conditioning modules (MLP, Conv, Attention) are almost plug-and-play instantiations and, while ablated in Table 4, largely follow established architectural motifs (e.g., Mixer, Convmixer, cross-attention). No clear theoretical or empirical justification is made for why the proposed combination is optimal or uniquely advantageous in the LVLM context.\n\n4. **Sparse Mathematical Elaboration of Modulation Dynamics**: While the ViLN concept is presented with explicit equations (see Equation for ViLN), the actual impact of the modulation on feature space, information flow, and alignment is only superficially discussed. There is no rigorous analysis of, for example, how the injected deltas interact with the primary LN statistics, whether this approach is robust to different LLM sizes, or under what theoretical conditions it preserves or distorts language priors.\n\n5. **Hyperparameter Sensitivity and Practical Implementation Details Under-Explored**: There is only light discussion of the selection/rationale for layer frequency (Table 6) and minimal details about the computational trade-offs across module types (e.g., attention vs. MLP). Further, choices about which layers to modulate and initialization strategies deserve deeper quantitative justification.\n\n6. **Experimental Scope Leaves Out Edge Scenarios**: While the approach is evaluated with different frame counts for video, there is limited evidence for how well it generalizes to much larger LLMs (beyond 7B), or to tasks requiring more complex or compositional visual reasoning. This is especially important given that scalability is a central claim.\n\n[1] Ye, X., Gan, Y., Ge, Y. (2025): ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models – Focuses on LVLM efficiency, directly related to LaVi’s claims. \n\n[2] Zhao, Y., Yin, Y., Li, L. (2025): Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge.\n\n[3] Luo, X., Yang, X., Dou, W. (2025): Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training.\n\n[4] Xu, G., Jin, P., Wu, Z. (2025): LLaVA-CoT: Let Vision Language Models Reason Step-by-Step – Enhances multimodal reasoning."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6K5gOqXnSp", "forum": "uUtIyxeBUc", "replyto": "uUtIyxeBUc", "signatures": ["ICLR.cc/2026/Conference/Submission2883/Reviewer_5Rmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2883/Reviewer_5Rmb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536221329, "cdate": 1761536221329, "tmdate": 1762916429476, "mdate": 1762916429476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LaVi, a Large Vision-Language Model (LVLM) designed for computational efficiency. The central idea is to inject visual information into a Large Language Model (LLM) by modulating the affine parameters of its internal LayerNorm (LN) modules. The authors claim that this approach achieves state-of-the-art performance on par with existing models like LLaVA, while significantly reducing FLOPs, latency, and memory usage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Addresses a Critical Problem:** The paper tackles the highly relevant and important problem of computational inefficiency in Large Vision-Language Models (LVLMs). As models grow in capability and are applied to longer visual contexts (e.g., high-resolution images, videos), the quadratic complexity of self-attention becomes a major bottleneck. The work's focus on creating a more scalable and practical framework is well-motivated and timely.\n\n2.  **Impressive Efficiency Gains:** The primary strength of the proposed method, LaVi, lies in its remarkable computational efficiency. The paper provides compelling evidence (Table 1, Figure 8, Figure 9) that its approach significantly reduces FLOPs, inference latency, and memory consumption compared to widely-used in-context injection methods like LLaVA."}, "weaknesses": {"value": "1.  **Limited Novelty:** The core idea of modulating normalization layer parameters with external conditioning is not new. This concept is well-established in computer vision, most notably with Adaptive Instance Normalization (AdaIN) for style transfer (Dumoulin et al., 2016) and conditional normalization in generative models (Brock et al., 2018). The paper's primary contribution is the application of this existing idea to the domain of LVLMs. While this extension is acknowledged, it can be viewed as a straightforward, incremental adaptation rather than a fundamental architectural innovation. The paper lacks a deep, principled discussion on *why* LN modulation is a superior mechanism for vision-language fusion over other potential pathways, making the approach feel more like an engineering heuristic than a novel scientific paradigm.\n\n2.  **Unconvincing and Inconsistent Performance:** The paper's central claim of achieving \"state-of-the-art multimodal performance\" is not well-supported by the results. The reported efficiency gains appear to come at the expense of accuracy on several key benchmarks, indicating a classic speed-accuracy trade-off rather than a Pareto improvement.\n    *   **Performance Regression on Key Benchmarks:** In Table 1, when comparing \"LaVi\" against \"LLaVA-OV,\" the average score improvement (+0.5%) obscures significant performance regressions on individual, challenging benchmarks such as VQAv2 (-0.5), ScienceQA (-0.6), and notably MMBench (-1.5). Sacrificing accuracy on established benchmarks for efficiency is a valid trade-off, but it contradicts the claim of achieving SOTA performance.\n    *   **Incomplete Benchmark Suite:** The evaluation is missing many contemporary and challenging benchmarks that are crucial for assessing the true capabilities of modern LVLMs. For instance, fine-grained visual understanding benchmarks (e.g., OCRBench, DocVQA, ChartQA) and advanced reasoning benchmarks in the STEM domain (e.g., MMMU, MathVista, MathVerse) are not evaluated. Similarly, for video, important long-video benchmarks like Video-MME-Long and LongVideoBench are absent, which makes it difficult to assess the model's claimed scalability in truly long-context scenarios.\n\n3.  **Insufficient and Outdated Baselines:** The experimental comparison is narrow and heavily focused on the LLaVA family. This presents a skewed and incomplete picture of the current LVLM landscape.\n    *   **Missing SOTA Models:** The paper fails to compare against numerous stronger and more recent open-source models, such as the InternVL series, Qwen-VL series (e.g., Qwen2-VL, Qwen2.5-VL). Without these comparisons, the \"state-of-the-art\" claim is unsubstantiated.\n    *   **Missing Diverse Architectures:** The comparison also lacks other relevant architectures that explore efficiency and alternative fusion mechanisms. Models like mPLUG-Owl3, VideoChat-Flash, or CrossLMM should have been included to properly situate LaVi's contribution and demonstrate its superiority over a wider range of architectural designs.\n\n4.  **Insufficient Justification and Superficial Analysis:** The methodological choices lack strong justification, and the analysis is not deep enough to be truly insightful.\n    *   The paper introduces three conditioning modules (MLP, Conv, Attention) but offers little intuition for their design or a compelling, principled reason for selecting the attention-based variant beyond a simple empirical comparison.\n    *   The analysis in Section 4.4, while visually appealing, is superficial. For example, observing that nouns and verbs receive stronger modulation (Figure 7) is intuitive but unsurprising, and does not in itself validate the overall approach. The analysis demonstrates *that* the model exhibits certain behaviors but falls short of explaining *why* this behavior leads to a more effective or principled form of vision-language integration.\n\n5.  **Misleading Framing of Contribution:** The paper frames its contribution as achieving top performance *and* top efficiency simultaneously. However, the evidence points to a classic engineering trade-off: the model is faster but measurably less accurate on several tasks and is not compared against the true state-of-the-art. This trade-off is valuable, but it should be presented as such. By claiming \"state-of-the-art performance,\" the paper overstates its contribution and sets unrealistic expectations. A more accurate and honest framing would be \"A New Efficiency-Accuracy Trade-off for LVLMs,\" which would be a solid but less groundbreaking contribution."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZeZNwaFFOt", "forum": "uUtIyxeBUc", "replyto": "uUtIyxeBUc", "signatures": ["ICLR.cc/2026/Conference/Submission2883/Reviewer_u2BS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2883/Reviewer_u2BS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814578419, "cdate": 1761814578419, "tmdate": 1762916429254, "mdate": 1762916429254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript focuses on the vision-language interation in the paradigm of vision-language models (VLMs). Existing strategies for integrating vision information in VLMs either introduces extra learnable parameters within the language model, disrupting architecture consistency, or concatenates vision tokens with language tokens, leading to reduced efficiency. To achieve minimal structural interference and computational scalability at the same time, the manuscript presents a new method LaVi. The core mechanism is to modulate the features through modulating the LayerNorm parameters in the language models, conditioned on vision features.\n\nExperiment shows reduced FLOPs and inference latency, while keeping a competitive performance compared to token concatenation baselines. Ablation studies are conducted on different modulated components, modulation parameters, and modulation frequency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is well-motivated to improve the efficiency of vision-language models by altering the way of injecting vision information into language models. \n\n2. The overall idea and implementation of feature modulation injection is simple. \n\n3. Empirical results show that LaVi is highly-efficient, using less than 10% computation, 50% latency to achieve a similar performance to token concatenation baselines. \n\n4. Ablation studies are conducted to show the effectiveness of the proposed modules."}, "weaknesses": {"value": "1. One of the major flaw of the proposed feature modulation injection is that, similar to the adaLN in diffusion transformers, support only one set of visual input. For multiple visual inputs (multiple input images, not multiple frames or image tiles mentioned in the manuscript), the difficulty lies in choosing which visual input to condition the layernorm. This limits the application of the proposed feature modulation injection to broader applications of VLMs, hence hardly able to be employed in existing VLMs. \n\n2. The experiment mainly focus on general visual understanding benchmarks, while the main advantage of concatenating visual tokens is that it can preserve enough details for fine-grained recognition, such as OCR. The manuscript fail to evaluate the performance of feature modulation injection on such benchmarks. This is especially important when we see the performance advantage diminishes on TextVQA when the resolution of input images increase (LaVi-Image vs LLaVA-v1.5, and LaVi-Image (HD) vs LLaVA-v1.6).\n\n3. What is the advantage of not modifying the architecture of the language model? Does it better preserve the language capability? This is not reflected in the manuscript. \n\n4. The experiment focuses on 7B language model. It is unknown how the method scales with different number of parameters in the language model."}, "questions": {"value": "1. The feature modulation injection seems highly similar to adaLN in diffusion tranformers. It would be better if explanations of differences are provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYOpovrJQU", "forum": "uUtIyxeBUc", "replyto": "uUtIyxeBUc", "signatures": ["ICLR.cc/2026/Conference/Submission2883/Reviewer_tgaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2883/Reviewer_tgaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050014443, "cdate": 1762050014443, "tmdate": 1762916428985, "mdate": 1762916428985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel vision-language model ensemble method, termed Language and Vision Integrator (LaVI), which injects visual information into the affine parameters of Layer Normalization through Vision-Infused Layer Normalization (ViLN), avoiding the context expansion problem of traditional methods. This method significantly improves computational efficiency while maintaining performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. LaVI uses layer-normalized affine parameters, avoiding complexity issues caused by excessively long contexts.\n\n2. LaVI achieves significant efficiency improvements."}, "weaknesses": {"value": "1. Lack of necessary theoretical analysis (detailed in questions).\n\n2. Lack of comparison with some existing work:\n\n[a] Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng. LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token.\n\n[b] Bo Tong, Bokai Lai, Yiyi Zhou, Gen Luo, Yunhang Shen, Ke Li, Xiaoshuai Sun, Rongrong Ji. FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression."}, "questions": {"value": "1. Why can effective visual-language alignment be achieved through affine parameter modulation? There is a lack of in-depth theoretical analysis.\n\n2. Specific sample analyses can be performed to demonstrate the visual perception of LaVI. For example, to specifically describe a complex scene.\n\n3. How does the model perform on fine-grained tasks, such as OCR-related tasks like TextVQA[c], DocVQA[d]?\n\n[c] Singh, Amanpreet and Natarjan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Parikh, Devi and Rohrbach, Marcus. Towards VQA Models That Can Read.\n\n[d] Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar. DocVQA: A Dataset for VQA on Document Images."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "umBKvcTGXZ", "forum": "uUtIyxeBUc", "replyto": "uUtIyxeBUc", "signatures": ["ICLR.cc/2026/Conference/Submission2883/Reviewer_bSwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2883/Reviewer_bSwR"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071129340, "cdate": 1762071129340, "tmdate": 1762916428794, "mdate": 1762916428794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}