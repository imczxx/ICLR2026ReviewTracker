{"id": "gtc8URMymz", "number": 15144, "cdate": 1758248218997, "mdate": 1759897325299, "content": {"title": "Harnessing Large Language Models Locally: Empirical Results and Implications", "abstract": "The increasing deployment of Large Language Models (LLMs) on edge devices, driven by model advancements and hardware improvements, offers significant privacy benefits. However, these on-device LLMs inherently face performance limitations due to reduced model capacity and necessary compression techniques. To address this, we introduce a systematic methodology---encompassing model capability, development efficiency, and system resources---for evaluating on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to 14B parameters and seven post-training quantization (PTQ) methods on commodity laptops, yields several critical insights: \n    1) System-level metrics exhibit near-linear scaling with effective bits-per-weight (BPW). \n    2) A practical threshold exists around $\\sim$3.5 effective BPW, larger models subjected to low-bit quantization consistently outperform smaller models utilizing higher bit-precision. \n    3) As model size decreases, the primary performance bottleneck potentially shifts from computation to communication.\n    4) Determined by low-level implementation specifics power consumption on CPU, computation-intensive operations spend more power than memory-intensive ones.\n    These insights offer practical guidelines for the efficient deployment and optimized configuration of LLMs on resource-constrained edge devices. Our codebase is available at  https://anonymous.4open.science/r/LLMOnDevice/.", "tldr": "This paper systematically evaluates performance and resource utilization of LLMs on resource-constrained edge devices.", "keywords": ["on-device LLM", "evaluation", "inference"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1992c01bb5dbf53033f51be5d76ef8ea81dee0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive empirical study of on-device Large Language Model (LLM) inference, structured around a tripartite evaluation framework covering model capability, deployment efficiency, and system resource utilization. The authors evaluate a wide range of models (0.5B to 14B from Qwen 2.5 and Llama 3) with seven post-training quantization (PTQ) methods from the llama.cpp framework on a commodity laptop. The study produces several practical insights, such as the shift from compute- to communication-bound bottlenecks for smaller models and the observation that low-level operator implementation, rather than model size, can dominate power consumption. The paper concludes by offering actionable guidance for deploying LLMs on resource-constrained devices. The paper is well-executed and practically useful, but its novelty is limited."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and reproducible experimental setup\n2. Comprehensive coverage of quantization formats available in llama.cpp, with consistent benchmarking across multiple model sizes.\nDetailed engineering discussion of quantization implementation effects (e.g., unpacking costs, bit‑width overheads).\n3. Practical observations that may benefit practitioners deploying LLMs to constrained CPU‑based environments."}, "weaknesses": {"value": "1. Lack of methodological novelty: No new quantization algorithm, system architecture, or theoretical contribution is proposed; the framework is essentially a re‑organization of well‑known metrics (accuracy, throughput, resource usage) and standard benchmarks.\n2. Engineering focus over scientific insight: The results largely confirm existing community knowledge (e.g., 4‑bit sweet spot, extreme compression hurts accuracy) without deeper analysis, generalization, or predictive modelling.\n3. Narrow hardware scope: All edge results are on one consumer‑grade x86 laptop; no evidence that conclusions hold on ARM laptops, NPUs, mobile GPUs, or other common edge platforms.\n4. ICLR fit: The paper reads as a well‑documented engineering report/benchmark rather than a research contribution with clear novelty or new understanding; this weakens its competitiveness at a method‑focused venue."}, "questions": {"value": "1. How is the “tripartite framework” fundamentally different from standard evaluation practice in system papers?\n2. Could the observed compute‑bound vs. communication‑bound regime shifts be formalized into a predictive analytical model rather than purely descriptive plots?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0SXzmav8Ut", "forum": "gtc8URMymz", "replyto": "gtc8URMymz", "signatures": ["ICLR.cc/2026/Conference/Submission15144/Reviewer_t4Hh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15144/Reviewer_t4Hh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219036284, "cdate": 1761219036284, "tmdate": 1762925461195, "mdate": 1762925461195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to provide a systematic empirical evaluation framework for on-device large language models (LLMs) along three dimensions: model capability, deployment efficiency, and system resource utilization. The authors benchmark Qwen 2.5 and Llama 3 series models of varying parameter scales (0.5B–14B) with multiple post-training quantization schemes using the llama.cpp framework on commodity laptop hardware. The reported findings include scaling trends in accuracy and throughput, trade-offs between bit width and performance, and observations on compute vs. memory bottlenecks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Comprehensive empirical benchmarking across multiple model sizes, quantization levels, and tasks.\nThorough resource utilization profiling, including CPU, memory, and power consumption."}, "weaknesses": {"value": "The starting point of the paper is relatively basic, using existing datasets and metrics. It focuses on analyzing and summarizing experimental results, summarizing some experiences of deploying large models on the edge. These experiences already exist, and as a form of knowledge-based summary, it is feasible, but there are no particularly original or novel findings.The paper mostly analyzes results from experiments, lacking corresponding theoretical explanations, and the problems addressed do not significantly solve any challenging issues in the industry.The paper contains some errors.In section 4.2 on page 7, the last paragraph of the third line refers to Figure 2 as controlling a certain quantization method, but the paper claims that this figure gives conclusions on various precision quantization methods, which is clearly incorrect."}, "questions": {"value": "1.Which specific pain points in the related field does the final result of this paper directly address?\n2.In the last paragraph of section 4.2 on page 7, how are the relevant results of different quantization methods derived from Figure 2 in the third line?\n3.Are there any novel quantization strategies or algorithmic modifications that emerged from your experiments which could be generalized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpuwDSo9MB", "forum": "gtc8URMymz", "replyto": "gtc8URMymz", "signatures": ["ICLR.cc/2026/Conference/Submission15144/Reviewer_9haL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15144/Reviewer_9haL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707069042, "cdate": 1761707069042, "tmdate": 1762925460599, "mdate": 1762925460599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper conducts an empirical study on deploying large language models (LLMs) directly on consumer-grade devices using the llama.cpp framework. It evaluates multiple models (Qwen 2.5 and Llama 3, from 0.5B to 14B parameters) and seven post-training quantization (PTQ) methods across five benchmarks (GSM8K, HellaSwag, MMLU, HumanEval, and TruthfulQA).\n\nThe authors measure performance along three dimensions — model capability, deployment efficiency, and system resource utilization — using a laptop CPU setup. The study finds that system metrics roughly scale linearly with bits-per-weight, that low-bit quantized large models can outperform smaller high-precision ones, and that performance bottlenecks shift from computation to communication as models shrink. The paper concludes with deployment recommendations for balancing accuracy, speed, and resource efficiency on edge devices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The on-device LLM trend is rapidly gaining relevance due to privacy and cost advantages, making this study contextually important.\n- Evaluates multiple models, quantization methods, and benchmarks. Uses open-source frameworks (llama.cpp, lm-evaluation-harness) and provides reproducibility details and code link.\n- The tripartite evaluation framework (capability, efficiency, resource usage) provides a coherent structure. Identifies performance-power trade-offs, showing how quantization level and model scale interact in edge scenarios."}, "weaknesses": {"value": "1. The results are confined to one hardware setup (a laptop CPU), with no exploration of whether findings generalize to heterogeneous PC CPU archs or hardware (NPU, smartphones, pad, wearable devices, etc.).\n2. It only uses llama.cpp on CPUs. GPU/AI accelerator comparisons, FP8 baselines, or recent quantization methods (GPTQ, AWQ, SmoothQuant, etc.) are missing.\n3. The work mainly reproduces known trends about quantization–accuracy–efficiency trade-offs without introducing new frameworks, or theoretical insights. The paper lacks engagement with prior literature on efficient LLM inference."}, "questions": {"value": "- How were the quantization parameters (scales/zero-points) tuned? Were all methods implemented natively via llama.cpp, or were any custom modifications applied?\n\n- Why were only CPU results reported, even though edge devices increasingly include NPUs or integrated GPUs?\n\n- How does the chosen hardware (16GB RAM laptop) represent a general “AI PC” class—are the conclusions expected to hold for ARM-based chips or mobile SoCs?\n\n- Did the authors compare the quantized models’ latency/energy against GPU or cloud inference baselines to contextualize results?\n\n- Are the conclusions (e.g., 3.5 BPW threshold) robust across different datasets and architectures beyond Qwen and Llama families?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E16sHBWoBd", "forum": "gtc8URMymz", "replyto": "gtc8URMymz", "signatures": ["ICLR.cc/2026/Conference/Submission15144/Reviewer_ka1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15144/Reviewer_ka1w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897544890, "cdate": 1761897544890, "tmdate": 1762925459312, "mdate": 1762925459312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the deployment of large language models (LLMs) on edge devices using llama.cpp. It primarily examines two model families—LLaMA and Qwen—across different model sizes and quantization strategies. The evaluation focuses on model capability, deployment efficiency, and system resource utilization on a Windows laptop, and also analyzes the impact of varying input lengths."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper explores the impact of specific implementation details in quantization strategies on model deployment.\n* It presents several interesting and unexpected results, along with corresponding explanations.\n* The evaluation is more comprehensive than that of similar studies in certain aspects."}, "weaknesses": {"value": "* Most of the results are not particularly surprising.\n* Some of the more interesting results cannot be reliably reproduced.\n* Results obtained from a single hardware platform lack persuasiveness and limit generalizability.\n* The paper does not clearly highlight its key contributions or distinguish its differences from related work."}, "questions": {"value": "* The extra memory overhead of Q4_0 shown in Figure 3 cannot be reproduced, either using the official llama.cpp repository or the open-source code provided by the authors. Which version of llama.cpp was used in the experiments? Can the experiments—excluding those related to model capability—be reproduced on other platforms?\n\n* Given the above issue, the generalizability of the other experimental results is questionable. Furthermore, the main distinction between this paper and similar works lies in its detailed discussion of quantization methods and the associated memory and computational bottlenecks. However, this discussion is scattered across the paper, lacking a clear structure, which makes it difficult to identify the paper's core novelty. It would be better to reorganize the paper around its main contributions, emphasizing new findings rather than reiterating existing conclusions. Nevertheless, given the limited strength of the presented data and the lack of significant differences among different comparisons, it is uncertain whether the current evidence sufficiently supports a full paper. If the authors have a stronger justification, please provide it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "macqfXnwdp", "forum": "gtc8URMymz", "replyto": "gtc8URMymz", "signatures": ["ICLR.cc/2026/Conference/Submission15144/Reviewer_ck71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15144/Reviewer_ck71"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15144/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943238592, "cdate": 1761943238592, "tmdate": 1762925458555, "mdate": 1762925458555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}