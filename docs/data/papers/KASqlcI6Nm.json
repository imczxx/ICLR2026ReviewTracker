{"id": "KASqlcI6Nm", "number": 17938, "cdate": 1758282239057, "mdate": 1763711812966, "content": {"title": "EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning", "abstract": "At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a specific class of priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.", "tldr": "A Bayesian RL algorithm that leverages epistemic uncertainty for principled exploration, achieving near-optimal guarantees and strong performance in sparse-reward environments.", "keywords": ["Bayesian RL", "epistemic uncertainty", "exploration"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61f22b9ece92e8aba14497cd56fcae19b6e6e786.pdf", "supplementary_material": "/attachment/72a70e44e04260c13893c23581bc39c30c3a3af7.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a method for incorporating epistemic uncertainty into a standard Bayes-Adaptive-MDP-solving framework (interact, update belief-posterior, find the optimal policy with respect to the posterior, repeat). The paper includes a comprehensive theoretical analysis of the new method from the perspective of optimality."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "Strong theoretical foundation.\n\nStrong theoretical results.\n\nGood sanity-check experiments.\n\nClear description of theoretical limitations."}, "weaknesses": {"value": "Missing theoretical motivation:\n  1. Why is CAI chosen to formulate the epistemic uncertainty?\n  2. Why is epistemic uncertainty chosen to be formulated in CAI in this way?\n\nMissing additional limitations - the method is most likely limited to tabular settings. Is that correct?\n\nPresentation: multiple comments, described in more detail below."}, "questions": {"value": "**Presentation:**\n1. Algorithm 1 is very simple and very compact, which is excellent. However:\n    1. I think it is important it is at least described in the main paper.\n    2. ValueIteration(b) is never defined, described, or cited, I believe.\n    3. BeliefUpdate(s,r) should also be described in more detail (at least, how the update is done from a practical perspective).\n    4. It seems to me that algorithm 1 is not one of the contributions of the paper, but rather a rather-standard approach for posterior-belief/over-models based methods. Is that correct? If yes, I would rephrase the contributions. Rather than introducing a new algorith, the authors introduce a new method to incorporate uncertainty into the belief / posterior. I do not view this as weakening the contribution of the paper - merely making it easier for the reader to understand the novelty of the paper.\n2. I don't think $\\Epsilon_b$ is ever defined?\n2. The \"related work section\" would work much better after preliminaries (Section 2) than where it currently is, in the end of the paper. It will also make introducing the baselines and understanding the approach much easier in my opinion.\n3. I would cite at least one uncertainty survey for the Uncertainty Quantification paragraph, such as [1].\n4. Section 3.1 should build on citations from the uncertaity quantification paragraphs in the related work (/preliminaries). Unless 3.1 is entirely novel (in which case - this should be emphasized stronger), the prior work it builds on should be cited.\n5. I think lazy chain is incompletely explained. What is the complete action space? Are transitions det. or stoch.? Is there a difference between the left end and the right end? What does solving the problem constitute (ie what is the optimal policy), if the comulative return is zero?\n6. I would also suggest adding more detailed descriptions of chain and deep sea.\n7. Some \\citep should be \\cite (line 464).\n\n**Experiments and baselines:**\n1. I believe PSRL can be tuned to work much better if the \"right\" choice of prior over transition models (/rewards) is made (ie - transitions from any states are to at most two different states, rewards are det / from a normal). Since experiments are already run with EUBRL+ and there's a prior selection discussion, I think these results belong there. What do the authors think?\n\nI'm open to increasing the score, especially since most changes I would like to see are textual. The most major comments from my perspective are the clarity that relates to Algorithm 1 and the missing theoretical motivation.\n\n[1] Hüllermeier, Eyke, and Willem Waegeman. \"Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.\" Machine learning 110.3 (2021): 457-506."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3HipbEz1tj", "forum": "KASqlcI6Nm", "replyto": "KASqlcI6Nm", "signatures": ["ICLR.cc/2026/Conference/Submission17938/Reviewer_kthm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17938/Reviewer_kthm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961916553, "cdate": 1760961916553, "tmdate": 1762927746824, "mdate": 1762927746824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EUBRL, a Bayesian RL algorithm that steers exploration using epistemic uncertainty. At a high level, the per-step reward is interpolated using a learned \"probability of uncertainty,\" $P(U=1 \\mid s, a)$, which balances exploitation (the current mean reward estimate) against exploration (an intrinsic term derived from epistemic uncertainty). The authors prove nearly minimax-optimal regret and sample-complexity bounds for infinite-horizon discounted MDPs under a class of priors. Experiments on Chain, Loop ,DeepSea, and a new LazyChain benchmark compare EUBRL to several baselines and suggest strong exploratory behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.To the best of my knowledge, this is the first work to convert epistemic uncertainty into an explicit guidance weight $P(U=1 \\mid s, a)$. This idea is novel, naturally decouples exploration from uncertain reward estimates, and provides an adaptive interpolation weight of the reward signal.\n\n2.Theoretical guarantees are strong.  The paper gives (i) a regret bound $\\tilde{O}(\\sqrt{S A T} /(1- \\gamma)^{1.5}+S^2 A /(1-\\gamma)^2$ ) that matches known lower bounds when $T$ is large enough, and (ii) a sample-complexity bound that matches lower bounds for small $\\varepsilon$.\n\n3.The paper formalizes a class \n$\\mathcal{C}$ of decomposable/weakly-informative priors and shows nearly-minimax bounds for uniform bounded priors.\n\n4.The experimental section covers both deterministic and stochastic settings with many random seeds, which helps demonstrate robustness."}, "weaknesses": {"value": "1.The algorithmic description is quite high-level. More concrete details and examples would help reproducibility-for instance: how $\\mathcal{E}(s, a)$ is computed in practice; how the on-policy estimate $P(U=1| s, a)= \\mathcal{E}\\_{b}/ \\mathcal{E}\\_{\\max}$ is formed; and how $\\mathcal{E}_{\\max }$ is chosen.\n\n2.Some notation and concepts in the main text need clearer definitions. For example: what is the role of $w$ in Section 3.1? How is a \"prior\" specified precisely (i.e., prior over what objects/parameters)? It would also help to include concrete examples illustrating Definition 1."}, "questions": {"value": "1.Details in Algorithm 1: How do you compute $\\mathcal{E}(s, a)$ exactly in the tabular/prior choices you study? How is $P(U=1| s, a)=\\mathcal{E}\\_{b}/ \\mathcal{E}\\_{\\max}$ estimated on-policy, and how is $\\mathcal{E}_{\\text {max }}$ selected (global constant, rolling maximum, or theoretical bound)? Do you require assumptions on $f$ and $g$ ? How is Valuelteration(b) implemented under the updated belief?\n\n2.Theorem 3 suggests $\\eta$ is important. How do you choose $\\eta$ across tasks, and how sensitive are results to it? \n\n3.$\\widetilde{V}^t(s)$ is introduced to ensure quasi-optimism. Could you give its precise definition and a short intuition for how it drives the regret decomposition?\n\nSuggestions:\n1. If some definitions or derivations are omitted due to space, please add precise references to the appendix, so readers can quickly locate the formal statements and implementation details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6gjwR5sUkU", "forum": "KASqlcI6Nm", "replyto": "KASqlcI6Nm", "signatures": ["ICLR.cc/2026/Conference/Submission17938/Reviewer_Sc6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17938/Reviewer_Sc6X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874805580, "cdate": 1761874805580, "tmdate": 1762927745930, "mdate": 1762927745930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Epistemic Uncertainty directed Bayesian Reinforcement Learning (EUBRL) algorithm which achieves nearly minimax-optimal regret and sample complexity in infinite-horizon Markov Decision Processes (MDPs). EUBRL directs RL exploration by utilising probabilistic inference to model epistemic uncertainty as part of the agent’s objective (i.e., adaptively weighting the mean reward and uncertainty term based on the probability of uncertainty). Nearly minimax-optimal regret bounds and sample-complexity guarantees are established for the class of priors that are decomposable, or weakly-informative, and whose rate of epistemic uncertainty is $\\mathcal{O}(1\\sqrt{n})$. Empirically, EUBRL is shown to outperform frequentist (RMAX, MBIE-EB), sampling-based (PSRL, BOSS), optimism-based Bayesian (BEB, VBRB), and classical Bayesian (BEETLE, Mean-MDP) baselines across tasks with sparse rewards, long horizons and stochasticity (Chain, Loop, DeepSea, and the newly introduced LazyChain)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is eloquently written and introduces a novel theoretical proof which, for the first time (to the best of my knowledge also), achieves nearly minimax-optimal sample complexity in infinite-horizon discounted MDPs without assuming a generative model. This result improves on He at al. 2021 which shows nearly minimax-optimal regret but doesn’t extend to sample complexity. The theoretical results are backed up by convincing empirical results (using multiple seeds, reporting standard errors etc) covering a wide range of appropriate baselines (including frequentist and Bayesian), three environments from the literature, and newly introduced fourth task which targets algorithm “myopia”. In all tasks EUBRL is shown to improve upon prior works.\n\nOverall I think the claims in the paper are both significant and well-supported both theoretically and empirically."}, "weaknesses": {"value": "Towards the goal of disentangling exploration and exploitation, the evidence could be strengthened by e,g., considering an ablation (see questions).\n\nThe accessibility of the paper to a wider audience could also benefit from adding short intuitive summaries after key lemmas in the appendices."}, "questions": {"value": "Would it be possible to compare EUBRL to a variant that uses the exact same uncertainty estimates but applied as an additive bonus? I think such a result would help further empirically support the benefits of disentangling exploration and exploitation versus improved UQ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jdnM5FoiUs", "forum": "KASqlcI6Nm", "replyto": "KASqlcI6Nm", "signatures": ["ICLR.cc/2026/Conference/Submission17938/Reviewer_cKEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17938/Reviewer_cKEm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951471210, "cdate": 1761951471210, "tmdate": 1762927745307, "mdate": 1762927745307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel Bayesian reinforcement learning algorithm, EUBRL, to solve the explore-exploit dilemma, especially in environments with sparse rewards. The paper argues that common \"optimism-based\" exploration (adding an uncertainty bonus) is flawed because it can magnify errors when the agent's reward estimates are unreliable. EUBRL uses an \"epistemically guided reward.\" This is a weighted average that exploits (uses the estimated reward) when the agent is confident, but explores (uses uncertainty itself as an intrinsic reward) when the agent is uncertain. Theoretically, they show that EUBRL achieves nearly minimax-optimal regret and more importantly sample efficiency which hasn't been done in prior work. Empirically, they show that their method leads to imporved sampled efficiency on different tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the fundamental exploration–exploitation dilemma in reinforcement learning by introducing a novel approach termed “epistemic guidance.” The method is well-motivated, and its effectiveness is supported through both rigorous theoretical analysis and comprehensive empirical evaluation.\n\n2. The paper argues that adding an exploration bonus to the reward estimate is a flawed way to do exploration in BAMDPs because the reward estimate can be highly uncertain and can result in a poorly specified reward. Instead they propose to weigh the two rewards (reward estimate and the exploration bonus) by the uncertainty associated with the state-action pair. If the agent is uncertain it uses the exploration bonus if the agent is certain it uses the reward estimate. The paper also proves that this method achieves nearly minimax-optimal regret in both regret and sample complexity."}, "weaknesses": {"value": "1. The paper's analysis is confined to discrete state-action spaces, and it does not address the challenges of integrating its method with deep function approximation. The reliance on maintaining an explicit Bayesian posterior is computationally intractable for the high-dimensional environments where deep RL is typically applied. Therefore, it is unclear how the 'epistemically guided reward' could be effectively approximated to improve sample efficiency in practical deep RL algorithms. \n\n2. The paper suffers from a separation of theory from implementation. While the main text provides a compelling theoretical justification for the \"epistemically guided reward,\" it lacks a clear, procedural description of the full algorithmic loop. To understand precisely how the belief posterior is updated after each transition and how the new policy is extracted, the reader is required to hunt for the algorithm in the appendix.\n\n3. The adaptive weighting scheme is particularly vulnerable to the degeneracy of the uncertainty estimates. If uncertainty degenerates to zero, the agent will become fully exploitative.\n\n4. Similar to the last point, a mis-specified prior will also impact this algorithm a lot more than standard exploration bonus algorithms."}, "questions": {"value": "My questions are based on the practicality of the algorithm for more realistic applications:\n\nExploration bonuses have been shown to work with Deep Reinforcement learning algorithms. Do the authors think that their method can be deployed with a neural-network based policy in a model-based setting. \n\n1. The paper claims \"superior scalability\" but only demonstrates it in tabular, low-state-space environments (DeepSea). Isn't this claim misleading, as the algorithm's tabular, model-based nature makes it computationally intractable for the large-scale, high-dimensional problems where scalability is actually needed?\n\n3. Since maintaining a full Bayesian posterior is impossible, which practical proxy for uncertainty would work with this method? For example, would you use the variance from a deep ensemble or a novelty score from a pseudo-count method?\n\n4. Could this \"guided reward\" concept be adapted to improve sample efficiency in a model-free deep RL algorithm (like DQN or SAC), or is it fundamentally tied to a model-based approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MXnp7itz9C", "forum": "KASqlcI6Nm", "replyto": "KASqlcI6Nm", "signatures": ["ICLR.cc/2026/Conference/Submission17938/Reviewer_12wQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17938/Reviewer_12wQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986921013, "cdate": 1761986921013, "tmdate": 1762927744831, "mdate": 1762927744831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}