{"id": "ZpgyPxdxiL", "number": 16503, "cdate": 1758265255637, "mdate": 1759897236706, "content": {"title": "Where Matters More Than What: Decoding-aligned KV Cache Compression via Position-aware Pseudo-queries", "abstract": "The Key-Value (KV) cache is crucial for efficient Large Language Models (LLMs) inference, but excessively long contexts drastically increase KV cache memory footprint. Existing KV cache compression methods typically rely on input-side attention patterns within a prompt observation window to estimate token importance during the prefill stage. They fail to preserve critical tokens for future generation since these assessments are not derived from the decoding process. Intuitively, an effective observation window should mirror the decoding-stage queries to accurately reflect which tokens the generation process will attend to. However, ground-truth decoding queries are inherently unavailable during inference. For constructing pseudo-queries to approximate them, we find that positional information plays a more critical role than semantic content. Motivated by this insight, we propose decoding-aligned KV cache compression via position-aware pseudo-queries (DapQ), a novel and lightweight eviction framework that leverages position-aware pseudo-queries to simulate the output tokens, thereby establishing an effective observation window for importance assessment. It enables precise token eviction that aligns closely with the actual generation context. Extensive evaluation across multiple benchmarks and LLMs demonstrates that DapQ achieves superior performance, particularly under strict memory constraints (e.g.,  up to nearly lossless performance 99.5\\% on NIAH with 3\\% KV cache budgets).", "tldr": "", "keywords": ["LLM", "KV cache compression", "Long-context"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad512fcca70f8b5e5368f1ae98d812868865ee6c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a decoding-aligned KV cache compression method that estimates important tokens for future decoding by constructing position-aware pseudo-queries. The key finding is that positional encoding contributes more to query similarity than semantic content, shows that the location of a token matters more than its meaning when determining attention patterns. Based on this insight, the authors design DapQ, which appends synthetic pseudo-tokens after the input prompt to simulate future decoding steps. These pseudo-queries are then used to compute future attention to existing prompt tokens, allowing the model to estimate token importance and selectively retain only the most critical tokens in the KV cache eviction framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Novel Insight on Query Similarity.** This work presents a compelling new finding that positional encoding, rather than semantic content, primarily determines query representations. This insight reshapes our understanding of how attention alignment and KV cache compression should be approached."}, "weaknesses": {"value": "* **Fixed Prefill-Only Compression.** DapQ performs token eviction only during the prefill stage. Its efficiency and effectiveness in long-context generation scenarios, such as chain-of-thought reasoning or multi-step tasks, remain unexplored.\n* **Lack of Efficiency Evaluation.** The proposed method introduces additional pseudo-tokens during prefill, effectively enlarging the input length and increasing the Time to First Token (TTFT). As attention computation scales with $O(N^2)$ in the prefill phase, the overhead may become significant for long inputs. Since KV cache compression is mainly designed to improve efficiency, runtime and latency evaluations should be included to demonstrate the actual benefits versus the added cost.\n* **Unclear Generalization Across Positional Embeddings.** The proposed method is designed around RoPE-based models. However, it is unclear how well it generalizes to models using NoPE (e.g., Jamba1.5 [1]) or NTK-scaled RoPE (as LLaMA3-8B and Qwen3-8B provided). The behavior and compatibility of DapQ under different positional encoding schemes require further clarification. \n* **Unexpected Benchmark Results.** In the LongBench `Code` tasks with `LLaMA3-8B-Instruct`, several compression methods unexpectedly outperform the Full KV cache baseline. This phenomenon warrants additional analysis to explain why cache eviction improves over the uncompressed baseline. Moreover, since Qwen3-8B offers an optional reasoning mode, the paper should clarify whether this mode was enabled during evaluation, as it may influence performance outcomes.\n\n[1] Team, Jamba, et al. \"Jamba-1.5: Hybrid transformer-mamba models at scale.\" *arXiv preprint arXiv:2408.12570* (2024)."}, "questions": {"value": "* As noted in the weaknesses, how does DapQ perform on reasoning-oriented models? Could the authors provide comparisons between Qwen3-8B with and without reasoning mode to illustrate potential differences in long-context reasoning performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z9eK4x3ncI", "forum": "ZpgyPxdxiL", "replyto": "ZpgyPxdxiL", "signatures": ["ICLR.cc/2026/Conference/Submission16503/Reviewer_dTuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16503/Reviewer_dTuz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495879007, "cdate": 1761495879007, "tmdate": 1762926596048, "mdate": 1762926596048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DapQ, a KV-cache compression method that constructs position-aware pseudo-queries during prefill to approximate the first N decoding queries and compute token importance for eviction. A short synthetic segment with future positional IDs is appended to the prompt. The model then produces pseudo-queries, whose attention against prompt keys yields scores used to keep the Top-K KV pairs. Afterward, the synthetic segment is discarded, and decoding starts as usual. The central empirical claim is that positional information dominates semantic content in query similarity, which motivates the design of DapQ. Experiments on different LLMs and benchmarks demonstrate gains when compared to previous baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Constructing position-aware pseudo-queries provides a fresh perspective for token importance estimation in KV-cache compression.\n\n2. The illustrations are informative and easy to understand."}, "weaknesses": {"value": "1. The validation of the central claim regarding positional dominance is somewhat limited. The main analysis is conducted only on GovReport with Llama-3-8B, which might not generalize to other models or task domains. It would strengthen the paper to include more diverse evidence supporting this claim.\n\n2. The comparison with existing baselines seems incomplete in Table 2. The authors are suggested to include results for existing baselines StreamingLLM and Lacache on LongBench as well.\n\n3. Latency and memory measurements are not reported. Since the paper claims that DapQ is lightweight and mitigates peak memory issues found in some prior approaches, providing concrete latency and memory usage comparisons for major experiments would help substantiate these claims and demonstrate DapQ’s speedup and memory-saving effect.\n\n4. The fairness of the current comparisons raises some questions. For attention-free baselines like StreamingLLM and Lacache, their advantages mainly lie in faster decoding speed. Comparing only the prefill stage, without actual latency results, may be unfair. Including end-to-end latency measurements would make the comparisons more convincing and fair."}, "questions": {"value": "1. Could the authors provide measurements of the additional computational cost introduced by DapQ (e.g., on LongBench with Qwen3-8B), separately for the prefill and decoding stages?\n\n2. Does DapQ also apply to the decoding stage? If so, could the authors include results for long decoding tasks similar to those used in StreamingLLM and Lacache?\n\nOverall, this paper presents a fresh perspective on estimating importance for KV cache compression. However, the current version would benefit from stronger empirical validation of the main claim, more complete experiments and baseline comparisons, and quantitative evidence on latency and memory efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NyTnlB8F1O", "forum": "ZpgyPxdxiL", "replyto": "ZpgyPxdxiL", "signatures": ["ICLR.cc/2026/Conference/Submission16503/Reviewer_5eTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16503/Reviewer_5eTW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875847598, "cdate": 1761875847598, "tmdate": 1762926595745, "mdate": 1762926595745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets KV-cache compression for auto-regressive LLMs. Observing that (i) positional encodings dominate query representations and (ii) ground-truth decoding queries are unavailable at prefill, the authors propose DapQ: a lightweight prefill-only method that constructs “position-aware pseudo-queries” by appending synthetic tokens whose position IDs match the next N decoding positions. Attention from these pseudo-queries to the prompt keys yields an importance score; Top-K keys are retained. Extensive experiments on LongBench, Ruler, HELMET and Needle-in-a-Haystack (4 models, cache budgets down to 3 %) show DapQ consistently outperforming SnapKV, PyramidKV, H2O, StreamingLLM, etc., often approaching full-cache accuracy (e.g. 99.5 % on NIAH with only 3 % KV budget). The paper claims 4.7× speed-up over prior retrieval-based methods and near-lossless quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Interesting idea that uses position-aware pseudo-queries to simulate the tokens to be generated and predict attention patterns, thereby guiding KV-cache compression."}, "weaknesses": {"value": "- Figure 1 in the paper contains obvious errors, which detract from the overall quality of the manuscript.\n- The paper sets the position IDs of the pseudo-queries to a short span immediately after the prompt, highly close to SnapKV’s observation window. Consequently, DaqQ fails to outperform SnapKV by a noticeable margin, especially in Figures 4(b) and 4(c).\n- Most experiments compare DaqQ against dated baselines such as SnapKV and H2O (published one to two years ago). Although the very recent LaCache is also included, it consistently performs poorly on all benchmarks. Comparing DaqQ with more recent state-of-the-art methods (e.g., AdaKV) would provide a stronger demonstration of its effectiveness."}, "questions": {"value": "1.\tCould the authors add experiments that compare DaqQ with more recent state-of-the-art approaches?\n2.\tCould the authors include ablation experiments to examine the effect of placing pseudo-queries at different positions, not just following prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Qn7K5zhi2", "forum": "ZpgyPxdxiL", "replyto": "ZpgyPxdxiL", "signatures": ["ICLR.cc/2026/Conference/Submission16503/Reviewer_qbfR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16503/Reviewer_qbfR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929483639, "cdate": 1761929483639, "tmdate": 1762926595261, "mdate": 1762926595261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the DapQ method, which generates pseudo-queries to simulate future queries in order to assess the importance of the KV cache. The motivation behind the paper is the observation that the positional information of pseudo-queries plays a more significant role than semantic information. As a result, the method reconstructs more accurate pseudo-queries by re-encoding the positions of tokens in the current context. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper presents a very interesting insight, namely that the positional information of pseudo-queries plays a more significant role than semantic information. This is supported by experiments that clearly demonstrate the validity of this observation.\n\n+ The proposed method is simple yet effective.\n\n+ The experiments are very thorough."}, "weaknesses": {"value": "+ Although this paper uncovers an interesting phenomenon, it is counterintuitive. The idea that positional information is more important than semantic information in attention computation makes sense for short-range tokens, as they are often more strongly related. However, for long-range tokens, this phenomenon seems to require a more robust explanation.\n\n+ Another issue is that the paper evaluates the quality of pseudo-queries based on query similarity, but query similarity doesn't necessarily reflect the similarity of attention scores. Section 3 should include a measure of attention score similarity to provide a more direct evaluation."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OIsaUo4plU", "forum": "ZpgyPxdxiL", "replyto": "ZpgyPxdxiL", "signatures": ["ICLR.cc/2026/Conference/Submission16503/Reviewer_6Tek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16503/Reviewer_6Tek"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000269540, "cdate": 1762000269540, "tmdate": 1762926594797, "mdate": 1762926594797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}