{"id": "nJvgBolRcR", "number": 19035, "cdate": 1758292958005, "mdate": 1759897064764, "content": {"title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "abstract": "This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items of model outputs are then compared with corresponding items of reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 13 popular large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer Gemini-2.5-Pro achieving only a 33.4 F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, but far from correct; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable, reproducible, and low-cost usage.", "tldr": "", "keywords": ["expert-level evaluation", "long-form evaluation", "fine-grained evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05d37cf8f5a61c7ee4c3fdbedc4ec304bd9c7a13.pdf", "supplementary_material": "/attachment/2c3bbf7b17804413a8cfd0f3c896c3e846f0db51.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces `ExpertLongBench`, a long-form benchmark curated by experts across 11 tasks and 9 domains. The authors provide a detailed walkthrough of the benchmark creation process and propose an evaluation framework called `CLEAR`, and evaluate 13 LLMs on `ExpertLongBench` using the `CLEAR` framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well articulated, and I appreciate the long-context benchmark provided in the paper. I strongly resonate with the authors' motivation, as long-context evaluation techniques are indeed understudied. Additionally, the benchmark covers some practical tasks, whose data is not readily available. \n- I also acknowledge the capital and manual effort invested in this benchmark creation process, and I believe this will be a solid addition to the standard evaluation suite for LLMs. \n- I also appreciate the authors' choice to keep the test set private to avoid contamination."}, "weaknesses": {"value": "- I don't find any glaring technical errors in the paper. Not a weakness per se, by most of the content of this paper is about the benchmark creation process using experts-in-the-loop, while the second half discusses a standard benchmarking process on this benchmark following the `CLEAR` framework. The technical takeaways are minimal compared to the data contribution. \n- Again, not a weakness per se, but a massive chunk of the important information is stowed away in the appendix, and it is a hassle to refer back and forth. \n- I do have some comments, suggestions, and queries for the authors below, which I request the authors to clarify. \n- The benchmark might not be extendable to other domains or languages, given the expert-in-the-loop methodology."}, "questions": {"value": "- Since the benchmark is so domain-specific, why not conduct some sort of RAG-based evaluations? I do understand that this makes the evaluation a two-phase process (retrieval evaluation and then generation evaluation, given the retrieval). However, I feel it might be unfair to expect the LLM to know intricate and nuanced domain-specific information, which is arcane to common users, and it might reflect the true abilities of the model.\n- I suggest that some reasoning models also be evaluated on this benchmark, especially given Figure 5. It would be interesting to see if test-time scaling improves the performance on harder problems. I feel this will be an interesting takeaway, which might show the efficacy of reasoning on non-math/code and factual, knowledge-intensive tasks. \n- It would also be interesting to see how the rankings of these LLMs on `ExpertLongBench` compare to `MMLU`/`GPQA` and `LMArena` rankings. The latter is generally treated as the empirical ranking, while MMLU is _(unfortunately)_ reported as a standard benchmark, and many model builders make their choice solely based on it. Hence, a difference in ranking would further motivate the need for domain-specific and long-context evaluations. However, I do acknowledge that most of the questions in MMLU and LMArena might be quite small compared to `ExpertLongBench`, which specializes in long-context queries. This won't be an apples-to-apples comparison. \n- If possible, I urge the authors to add a small study on LLM-evaluator's self-bias, as this benchmark provides an ideal ground for it. In an $N \\times N$ matrix, we can have the performance of LLMs as evaluated by every LLM, including itself. On this hard benchmark, if we notice the diagonal consistently higher, it would be a clear example of self-bias, where the model scores itself higher despite a lack of knowledge of the domain/incorrect answers. I hope this is clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6rAJvI7A3J", "forum": "nJvgBolRcR", "replyto": "nJvgBolRcR", "signatures": ["ICLR.cc/2026/Conference/Submission19035/Reviewer_LZpR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19035/Reviewer_LZpR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585555128, "cdate": 1761585555128, "tmdate": 1762931075078, "mdate": 1762931075078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EXPERTLONGBENCH, a new benchmark designed to evaluate LLMs on expert-level, long-form generation tasks. The benchmark consists of 11 tasks from 9 professional domains that mimic real-world workflows, featuring long contexts (up to 200K tokens) and long-form outputs (up to 5K tokens). The primary contribution is the CLEAR evaluation framework. Instead of direct text-to-text comparison, CLEAR uses expert-designed \"rubrics\" to create a \"checklist\" of required information. It then uses an LLM to extract this checklist of information from both the model's output and a human-written reference, and compares these two checklists item-by-item. The paper benchmarks 13 SOTA LLMs, finding that even the best model (Gemini-2.5-Pro) achieves a very low F1 score (~33.4%), revealing significant gaps in current LLM capabilities for expert tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and widely acknowledged gap in LLM evaluation. Most benchmarks test for factual recall via multiple-choice or short-form answers. This work makes a significant contribution by shifting the evaluation to what experts actually do.\n\n2. The CLEAR framework is a major strength. The idea of not comparing two long texts directly, but instead using an expert-designed rubric to decompose the task into a structured checklist, is a very strong and novel approach to \"grounded\" evaluation. \n\n3. The paper's key findings are highly valuable. The primary result that even SOTA LLMs fail badly is a crucial data point for the community. More importantly, the discovery of a negative correlation between checklist coverage and F1 score is a critical insight, providing concrete evidence that models are adept at generating plausible, structured, but factually incorrect content, which is a key risk."}, "weaknesses": {"value": "1. A weakness is the small number of samples. The benchmark contains 1050 samples total, about 100 samples/each. It is questionable whether 100 samples are sufficient to draw robust conclusions about a model's performance across an entire complex domain like law or medicine. This small scale could lead to noisy results.\n\n2. The paper states that a portion of the benchmark is public. Given the small sample size (50-100 per task) and the highly unique, long-form nature of the prompts and data, these tasks are at a very high risk of being included in the training data of future models, which would quickly render the public benchmark obsolete for evaluation.\n\n3. CLEAR relies on SOTA LLMs to function as \"Checklist Mappers\" and \"Judges.\" This introduces a circular dependency where the evaluation of LLMs is dependent on the capabilities of another LLM. While Section 6 attempts to validate this, it's still a source of potential variance.\n\n4. The benchmark's greatest strength (expert-designed rubrics) is also its greatest weakness. The paper notes that creating the rubric for a single task (T1) took over 10 hours of expert time. This creates a massive bottleneck, making the framework difficult and expensive to scale to new domains or even new tasks within the same domain."}, "questions": {"value": "1. The main concern is the low sample size (100 per task). How did the authors validate that this sample size is statistically sufficient to be representative of the entire domain and to produce stable, reliable model rankings?\n\n2. The CLEAR framework's results are contingent on the LLM used for mapping and judging (e.g., Qwen2.5-72B and GPT-4O). How sensitive are the final F1 scores (Table 2) to this choice? If you were to swap the judge/mapper to a different models (e.g., a Claude or Llama model), do the model rankings remain consistent?\n\n3. Given the high cost of expert rubric design and the high risk of data contamination, what is the long-term vision for maintaining and expanding this benchmark? How can this approach scale without requiring hundreds of hours of domain-expert time for each new task?\n\n4. The finding of high coverage but low F1 is fascinating. Do the authors have a hypothesis why this is happening? Is it a failure of long-context retrieval (the model fails to find the right facts in the 200K token input) or a failure of generation (the model finds the facts but hallucinates or misrepresents them to fit the required checklist structure)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LCEp7jLqMI", "forum": "nJvgBolRcR", "replyto": "nJvgBolRcR", "signatures": ["ICLR.cc/2026/Conference/Submission19035/Reviewer_15Gk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19035/Reviewer_15Gk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883328352, "cdate": 1761883328352, "tmdate": 1762931074473, "mdate": 1762931074473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ExpertLongBench, a benchmark for expert-level generation with long input and long output, and a checklist-based evaluation method called CLEAR. The benchmark covers 11 domains, including law, science, medicine, education, and finance. Each task comes with an expert-written rubric and a reference that is mapped into a checklist, so an LLM judge can test whether a system answer covers each item. The authors provide a public subset, a private subset for held-out testing, code, and an evaluation pipeline. The goal is to measure factual coverage and faithfulness in realistic expert workflows rather than surface fluency.\n\nExperimentally, the paper evaluates a broad set of LLMs on the suite and finds that even leading models perform far from perfect, with average F1 around 30. The tasks include very large inputs and long outputs, and the analysis shows a common pattern of high coverage but lower correctness. Human spot checks confirm the quality of the reference mapping on selected tasks. Judge reliability is studied by comparing GPT-4o with other judges, showing strong agreement and good correlation with an open model judge, and the pipeline supports using open models for the mapping step with similar rankings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors present a benchmark with long inputs and outputs that requires expert-level knowledge, covering 11 domains. This is a valuable resource for the community.\n\n2. The authors conduct comprehensive experiments on ExpertLongBench, including many frontier models, and show that the benchmark remains highly challenging even for top-performing models.\n\n3. The authors devote substantial effort to data quality and model analysis, for example examining potential data contamination and designing an automated, relatively reliable evaluation pipeline."}, "weaknesses": {"value": "1. **Checklist and rubric reliability.** My main concern is the quality of the checklists and rubrics, which the overall evaluation heavily relies on. While I acknowledge the authors’ efforts, many tasks appear to be curated by a single PhD student in a related field, which may be insufficient and could introduce bias. An ideal setup would involve multiple experts for curation, additional experts for independent review, and iterative updates until the experts reach a reliable agreement. I only noticed that T8 mentions experts “reached consensus,” without any quantitative agreement metrics. Introducing a more thorough process and reporting would improve confidence in the benchmark.\n\n2. **“Long-form” positioning.** Although the paper claims a long-form generation benchmark, Table 1 shows that 8 of 11 tasks have inputs under 2k tokens and 9 of 11 tasks have references under 500 tokens. I understand this is already longer than many expert-level datasets, but compared with long-context-focused benchmarks (e.g., LongBench and L-Eval), it is less convincing to label the suite as a long-context benchmark.\n\n3. **Missing references.** I recommend adding a comparison and citation to HealthBench. If the authors intend to position this work as a long-context benchmark, it would also help to compare with and cite LongBench and L-Eval, which include long-form generation tasks."}, "questions": {"value": "See the above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "itKp3kfucf", "forum": "nJvgBolRcR", "replyto": "nJvgBolRcR", "signatures": ["ICLR.cc/2026/Conference/Submission19035/Reviewer_kZKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19035/Reviewer_kZKG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947402055, "cdate": 1761947402055, "tmdate": 1762931074125, "mdate": 1762931074125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EXPERTLONGBENCH, an expert-level benchmark with 11 tasks across 9 domains, focusing on realistic long-form expert workflows (e.g., legal case summarization, clinical note generation) that require outputs exceeding 5,000 tokens. Each task includes an expert-designed/validated rubric and human-written references for grounded evaluation.\nIt also proposes CLEAR, an evaluation framework that extracts checklists from model outputs and references based on task rubrics, then compares checklist items for fine-grained, objective assessment. Open-weight models like Qwen2.5-72B work effectively in CLEAR, enabling low-cost, reproducible evaluation.\nBenchmarking 13 LLMs shows top model Gemini-2.5-Pro only achieves an average F1 of 33.4, revealing significant gaps. Critically, high checklist item coverage (irrespective of correctness) correlates negatively with F1—models often produce content seemingly aligned with domain standards but incorrect, posing misleading risks. This work fills gaps in expert-level long-form LLM evaluation and guides future model improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work fills the gap in expert-level long-form generation evaluation, which appears to be of considerable significance.\n2. This work proposes CLEAR, a method that extracts checklists from reference outputs for comparison. This approach exhibits higher accuracy compared to existing rubric-based methods, and the experiments also demonstrate that models may potentially game such rubric-based approaches."}, "weaknesses": {"value": "1. The long-form evaluation method is excessively costly. As shown in Table 1, many tasks involve input exceeding 100,000 tokens. For a single task with 100 samples, 10 million tokens are required—and this only accounts for input, excluding the cost of completion. The authors put their cost in Section G.3, where the evaluation costs them over $1000.  The practicality of evaluation at this scale is questionable. \n2. In terms of evaluation, while it assesses long-form generation capabilities, it omits consideration of various solutions for long-form text processing, such as RAG (retrieval-augmented generation) approaches or the integration of agentic workflows (e.g., preliminary summarization). In real-world scenarios, it is uncommon to directly feed such long prompts to models for generation. This aspect lacks practical relevance.\n3. While the paper validates GPT-4o’s consistency with another closed model (Gemini-2.0-Flash, Cohen’s Kappa: 0.81–0.89, Section 4.2) and verifies checklist extraction quality via human inspection (T1/T6: 95.12–99.99% faithfulness, Section E.1), it does not compare GPT-4o’s evaluation results with human expert judgments. The authors acknowledge this gap in Section I.3 (Limitations), noting that “LLM-based evaluation can yield erroneous assessments and requires careful human oversight”—which echoes your concern about unvalidated reliance on GPT-4o.\n4. A portion of the data is not publicly available, which hinders the community from conducting tests and follow-up research."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EUrOQebh34", "forum": "nJvgBolRcR", "replyto": "nJvgBolRcR", "signatures": ["ICLR.cc/2026/Conference/Submission19035/Reviewer_ejKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19035/Reviewer_ejKy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244805350, "cdate": 1762244805350, "tmdate": 1762931073455, "mdate": 1762931073455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}