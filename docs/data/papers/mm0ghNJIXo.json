{"id": "mm0ghNJIXo", "number": 20456, "cdate": 1758306357127, "mdate": 1759896976833, "content": {"title": "Kernel Complexity Reduced Graph Contrastive Learning for Noisy Node Classification", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in learning node representations and have demonstrated strong performance on node classification. However, their effectiveness can be substantially compromised by noise in real-world graph data. To address this challenge, we propose Kernel Complexity Reduced Graph Contrastive Learning (KCR-GCL), a principled framework for noisy node classification with a provable transductive generalization guarantee. KCR-GCL introduces a novel KCR-GCL encoder, which incorporates a new KCR self-attention layer that adaptively balances different frequency components of the graph inspired by generalized graph convolution and reduces the kernel complexity for provably improved generalization for transductive learning. The KCR-GCL encoder is optimized with a low-rank regularization term through the truncated nuclear norm (TNN) on the gram matrix of the learned features. The learned low-rank representations are then used to train a linear  classifier for transductive node classification in noisy graph data.\nThe design of KCR-GCL is inspired by the Low Frequency Property (LFP)  widely studied in general deep learning and node-level graph learning, and is further supported by a sharp generalization bound for transductive learning. To the best of our knowledge, KCR-GCL is among the first to theoretically reveal the benefits of low-rank regularization in transductive settings for noisy graph data. Experiments on standard benchmarks highlight the effectiveness and robustness of KCR-GCL in learning node representations under noisy conditions.\nThe code of KCR-GCL is available at \\url{https://anonymous.4open.science/status/KCR-GCL}.", "tldr": "we propose Kernel Complexity Reduced Graph Contrastive Learning (KCR-GCL), a principled GCL framework for noisy classification with kernel complexity reduced self-attention and provable generalization guarantee.", "keywords": ["Kernel Complexity Reduced Graph Contrastive Learning", "Generalization Bound", "Noisy Node Classification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1d305e5fbd30b7a02c00f4ca6e8f94dcece2c3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the low-rank regularization to node representation learning in the graph constrastive learning setting, with theoretical analysis of the generalization bound. The node classification experiments on several homophilious graphs and two heterophilic graphs are performed to demonstrate the merits of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The redundancy of correlations among node features in graph self-supervised learning is explored and mitigated through a Gram matrix–based low-rank regularizer.\n* The problem is well motivated by analyzing eigen-projection and signal concentration ratio changes under label noise. \n* Experimental results demonstrate the effectiveness and robustness of KCR-GCL in learning node representations against random attribute and label perturbations."}, "weaknesses": {"value": "* Only two heterophilic graphs are used for evaluation, and one of them, Chameleon, is problematic due to a large number of duplicate nodes. It is recommended to include more heterophilic graphs for a more comprehensive evaluation.\n\n*  Robustness experiments against structural perturbations (e.g., Metattack or Nettack) are missing.\n\n* It would also be beneficial to evaluate the proposed model on graph classification tasks, which are commonly used benchmarks for graph contrastive learning.\n\n* Although the low-rank regularization is shown to be effective and empirical studies confirm the low-rank property among node features, a deeper explanation of how this regularization contributes to model performance would strengthen the work."}, "questions": {"value": "1) In KCR-GCL, the prototypical contrastive learning (PCL) module is incorporated to align node representations with their corresponding cluster centers. This component may also contribute to the robustness of node representation learning against noise. It would be interesting to analyze the relative importance of PCL and KCR — which component plays a more significant role in achieving robustness?\n\n2) Furthermore, are there any conceptual or empirical connections between the Gram matrix–based low-rank regularization and node clustering? Could the clustering effect of GCL' node embeddings be further enhanced through this low-rank constraint?\n\n3) It also appears that KCR may not be inherently specific to graph contrastive learning (GCL). Would the proposed design remain effective under other learning paradigms, such as supervised learning?\n\n4)Finally, the paper mentions that the “graph” is low-rank (e.g., in the abstract). It would be helpful to clarify whether “graph” here refers to the correlation matrix K that defines node relationships, or to the original input graph structure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j1famL1357", "forum": "mm0ghNJIXo", "replyto": "mm0ghNJIXo", "signatures": ["ICLR.cc/2026/Conference/Submission20456/Reviewer_M5Tm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20456/Reviewer_M5Tm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821090064, "cdate": 1761821090064, "tmdate": 1762933899524, "mdate": 1762933899524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KCR-GCL, which learns robust node representations and tackles noisy node classification. Experiments on various benchmarks highlight the effectiveness and robustness in learning node representations under noisy conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a theoretical analysis that establishes a generalization guarantee for the linear transductive classifier trained on the low-rank node representations.\n\n2. The experimental setup is exhaustive, involving multiple baselines, datasets, and comprehensive analyses."}, "weaknesses": {"value": "1. Inconsistent KCR-GCL scores between Table 5 and Table 7.\n\n2. Complex Hyperparameter Tuning. Multiple hyperparameters (the weighting parameter, the maximum power,and  the rank parameter) require careful tuning, increasing the tuning burden."}, "questions": {"value": "1. Do the selected baselines cover loss correction, sample selection, label denoising, structural regularization, and auxiliary self-supervised methods?\n\n2. Are there any real-world noisy datasets, instead of those created by injecting synthetic noise into clean datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VcuVLwXEEa", "forum": "mm0ghNJIXo", "replyto": "mm0ghNJIXo", "signatures": ["ICLR.cc/2026/Conference/Submission20456/Reviewer_DzfD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20456/Reviewer_DzfD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886438032, "cdate": 1761886438032, "tmdate": 1762933898717, "mdate": 1762933898717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied noise in graph data, included in both node attributes and labels. The theoretical support is to reduce the kernel complexity of the emebeding gram matrix. Therefore, the authors proposed two ways, 1) add low-rank loss, and 2) propagate output embeddings over their similarity matrix. Extensive experiments show the good performances of the proposed model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Studying how to alleviate noise in graph data is a good motivation.\n2. This paper has theoretical support.\n3. The authors did extensive experiments."}, "weaknesses": {"value": "1. The core property, i.e., Low Frequency Property, actually is not new, also the authors acknowledged this point. Two techniques that lowing ranks essentially enhance low frequency components.\n2. The whole pipeline faces a huge computing complexity. 1) B has shape $N\\times N$, 2) $F=BH$ where B is a dense matrix with O(N^2), 3) EVD on $K$ to get eigenvalues with O(N^3). All these three parts are done once at each epoch.\n3. I cannot get why the authors adopt this technique on contrastive learning based models, rather than traditional semi-supervised GNNs. The proposed techniques are not specific to contrastive learning actually.\n4. In Table 1, although the proposed KCR-GCL always performed the best, its performances also decreased with the increase of noise ratio, nearly at the same decreasing rate as baselines. This cannot prove the robustness of KCR-GCL. Robustness means you can perform well at clean, as well as not decrease too much like others at noise."}, "questions": {"value": "According to Theorem 4.1, the loss is upper bounded by L1, L2 and KC. Actually, usage of B can decrease KC, but how can it guarantee a lower L1 and L2? I notice that in Table 3, L1 and L2 were lower for KCR-GCL, but can we have some theoretical intuitions? There may be bias in experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ixI0sMjDLu", "forum": "mm0ghNJIXo", "replyto": "mm0ghNJIXo", "signatures": ["ICLR.cc/2026/Conference/Submission20456/Reviewer_EX6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20456/Reviewer_EX6T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988652792, "cdate": 1761988652792, "tmdate": 1762933897498, "mdate": 1762933897498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, inspired by the spectral insight that clean label information tends to concentrate in low-frequency components, proposes the KCR-GCL method and provides corresponding theoretical support, demonstrating that it can learn more robust and generalizable representations for graph node classification tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The paper provides a generalization bound for learning with noisy data from the perspective of kernel complexity, offering solid theoretical grounding.\n- The designs of TNN and KCR Self-Attention are reasonable, with clear motivation and implementation logic."}, "weaknesses": {"value": "- The assumptions are not sufficiently general. Although informative features often lie in low-frequency components, in sparse or heterophilic graphs, high-frequency features can be critical discriminative signals. Thus, the low-rank–encouraging TNN regularization may have limited generality.\n- The computational cost is high. Calculating the matrix B is inefficient and expensive, making the method difficult to scale to large-scale graphs such as OGB datasets.\n- Although the authors provide some explanations, the theoretical proof is still based on MSE loss and assumes a large number of labeled nodes, which is inconsistent with the cross-entropy loss used in actual training.\n- In my understanding, the hyperparameters $\\tau$ and $r_{0}$ are not easy to determine and may cause training instability."}, "questions": {"value": "- Have you considered conducting experiments on larger-scale datasets?\n- Have you considered using real noisy graph datasets (e.g., NoisyGL) instead of synthetic noise generation?\n- Please address the concerns raised in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMr1UTTEUq", "forum": "mm0ghNJIXo", "replyto": "mm0ghNJIXo", "signatures": ["ICLR.cc/2026/Conference/Submission20456/Reviewer_CsxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20456/Reviewer_CsxG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004801735, "cdate": 1762004801735, "tmdate": 1762933896799, "mdate": 1762933896799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}