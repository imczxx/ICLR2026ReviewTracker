{"id": "Ms9MMN8STj", "number": 8514, "cdate": 1758088983210, "mdate": 1759897779199, "content": {"title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining", "abstract": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (\\textit{i.e.}, intent–trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.", "tldr": "We propose M$^2$-Miner, a Monte Carlo Tree Search-based collaborative multi-agent framework, which could efficiently mine GUI interaction trajectory data, thereby reducing the high cost of manual annotation.", "keywords": ["GUI Agent; Vision-Language Models; Data synthesis; Monte Carlo Tree Search"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e563e745ff8a8cd4d4e715861d569d0819cfc2e.pdf", "supplementary_material": "/attachment/d866938e320313adb2a2a15fe49f4e12a86f54cc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces M-Miner, a low-cost and automated framework for mining high-quality intent–trajectory pairs to train mobile GUI agents. It builds on Monte Carlo Tree Search and uses a collaborative multi-agent setup with InferAgent for exploration guidance, OrchestraAgent for coordination and speedup, and JudgeAgent for trajectory evaluation. An intent recycling strategy increases intent diversity, and a progressive model-in-the-loop scheme improves mining success over time. Experiments indicate that agents fine-tuned on the mined data achieve state-of-the-art on several mobile GUI benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The M-Miner multi-agent framework automatically collects data based on intents and appears to be the first work to do so; it reduces the time and cost of manual annotation.\n2. Models trained on this dataset achieve strong performance, indicating the dataset’s potential value.\n3. The framework demonstrates sufficient novelty, and ablation experiments show that each component plays a sufficient role.\n4. The paper is clearly structured and easy to understand."}, "weaknesses": {"value": "1. The chapter organization needs adjustment. The theoretical preliminaries of MCTS are not difficult for researchers in RL-based agent domains. The authors devote about a page to this, which seems unnecessary. It could be moved to the appendix, freeing space to detail the three-stage intent component and the training procedure. A table-style exposition would suffice rather than a full appendix-level example.\n2. Although the results are SOTA, the gains are quite limited relative to some Qwen2-VL–based methods; see the Questions for specifics.\n3. The approach assumes expansion from an existing dataset rather than starting entirely from scratch. It is unclear why data were not collected from scratch on the same apps. As presented, the method resembles data augmentation.\n4. The quality validation reveals shortcomings in the method, and the paper does not discuss the cost of human quality review."}, "questions": {"value": "1. I think the comparison in Table 1 is unfair and lacks practical significance. Were the time costs considered? Table 2 reveals limitations of the method. How exactly is the human quality review conducted? And its cost?\n2. In Figure 3, the content measured by the acc metric is not defined.\n3. Line 373, the explanation of CAGUI may be problematic. Since the training data are not released, how do you ensure there is no test-data leakage during training? Generalization experiments are generally conducted entirely on unseen apps. \n4. Baselines are missing, e.g., [1] Falcon-UI: Understanding GUI Before Following User Instructions and [2] MobileIPL: Enhancing Mobile Agents’ Thinking Process via Iterative Preference Learning. To my knowledge, these are not concurrent works.\n5. Table 1 mentions AMEX and GUI-Odyssey as standard GUI evaluation datasets. I recommend adding experiments on these datasets (not strictly required during the rebuttal phase, given time constraints).\n6. In Section 4.4, the intent evolution procedure appears not very different from the instruction filtering in [3] DiGIRL: Training in-the-wild Device-control Agents with Autonomous Reinforcement Learning. Is this understanding correct? \n7. M2-Miner-Agent training dataset + other datasets like AITZ do demonstrate the validity of the data, but how can we ensure that there is no training data leakage, because the newly generated instructions are very similar to the original ones, but we all know that the original train and test instructions are also very similar and share the same data distribution.\n8. If the original data is removed from the ablation experiment and only the mined data is used, what will the model results be?\n9. What is the difference between mining data from scratch using brand new instruction templates in your framework, compared to replacing existing instructions with slots? I think a comparison might be necessary, as the paper doesn't explain why it's necessary to mine data on existing data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TkEza4mtMl", "forum": "Ms9MMN8STj", "replyto": "Ms9MMN8STj", "signatures": ["ICLR.cc/2026/Conference/Submission8514/Reviewer_gnp5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8514/Reviewer_gnp5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760775648338, "cdate": 1760775648338, "tmdate": 1762920380554, "mdate": 1762920380554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose M2-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search(MCTS). For better data mining efficiency and quality, the paper present a collaborative multi-agent frame work, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, this paper design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper propose a fully automated framework for mobile GUI agent data mining. By introducing MCTS and designing a collaborative multi-agent framework, the method improve data mining efficiency while enhancing data quality. \n2.The intent recycling strategy further enhances both mining efficiency and intent richness, while the progressive model-in-the-loop training paradigm boosts success rates in both familiar and novel environments.\n3.Extensive experiments show that GUI agents trained on the mined data achieve SOTA performance."}, "weaknesses": {"value": "1. The paper propose an automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search(MCTS). Monte Carlo tree search is a classic algorithm, is its innovation insufficient?\n2.The background knowledge of MOBILE GUI AGENT DATA MING was not sufficiently introduced in the paper writing, making it difficult to understand."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1QDEicSZKI", "forum": "Ms9MMN8STj", "replyto": "Ms9MMN8STj", "signatures": ["ICLR.cc/2026/Conference/Submission8514/Reviewer_xyHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8514/Reviewer_xyHC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740598427, "cdate": 1761740598427, "tmdate": 1762920380046, "mdate": 1762920380046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent framework for mobile GUI agents. By introducing a tree-based structure, it organizes and stores learned operations in a structured and reusable manner. The framework incorporates three specialized agents to optimize the MCTS search process and reward estimation, effectively avoiding inefficient random exploration. Furthermore, it introduces a model-in-the-loop training paradigm that enables continual learning and self-improvement during deployment, expanding the system’s learning capability from a relatively small initial dataset. Experimental results show significant performance improvements, and comprehensive ablation studies validate the effectiveness of each component."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. Strong Experiments. It compares with 13 methods and analyzes the effect of agent numbers and online learning strategies, showing solid and comprehensive evaluation.\n\nS2. Practical Significance. The framework is scalable and adaptable, demonstrating potential for real-world GUI automation and broader mobile applications.\n\nS3. Trajectory Recycling is an interesting and computation-efficient design."}, "weaknesses": {"value": "W1. Writing and Presentation Issues. The paper contains several typos and minor writing problems that affect readability:\n\n1. Line 52: “we presents” ->“we present”\n2. Line 274: “where i denotes the i-th visit to the node” appears twice.\n3. Line 353” “This is crucial when targeting new application scenarios.” is unclear — please specify what scenarios are referred to.\n4. Line 480 “significantly improve” -> “improves”.\n5. Line 484 “an solid foundation” -> “a solid foundation”\n6. Line 485 “Statement of Using LLM” lacks a period at the end.\n\nW2. Outdated Baselines. The comparison does not include the latest agent-based methods, such as AppAgent[1]. The authors should either explain the exclusion or add these newer methods to the experiments.\n\nW3. Clarity of Agent Interaction\n\nThe description of how the three agents (Infer, Orchestra, and Judge) coordinate during the MCTS process remains vague. A clearer explanation of their information flow and role boundaries would improve reproducibility and reader understanding.\n\n[1]. Zhang, Chi, et al. \"Appagent: Multimodal agents as smartphone users.\" *Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems*. 2025.\n\nW4. The novelty of applying MCTS and multi-agent in the GUI data mining is unclear."}, "questions": {"value": "Q1. Consider comparing with more recent agent-based methods, such as AppAgent [1], or provide a clear justification for why these methods were excluded.\n\nQ2. Ablation Study. The ablation study lacks clarity on how different components are decoupled. Specifically, how does the InferAgent function when the JudgeAgent is removed? Please elaborate on how the agents’ dependencies are handled during ablation.\n\nQ3. Experimental Clarity. It is unclear how large language models (LLMs) are wrapped or packaged as agents for GUI interaction in your experiments. Please explain how the LLMs receive GUI state inputs and produce executable actions, and whether environment feedback is included in this loop."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S0mDBQBoHs", "forum": "Ms9MMN8STj", "replyto": "Ms9MMN8STj", "signatures": ["ICLR.cc/2026/Conference/Submission8514/Reviewer_eLqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8514/Reviewer_eLqw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747684413, "cdate": 1761747684413, "tmdate": 1762920379640, "mdate": 1762920379640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M²-Miner, an automated framework for mining mobile GUI agent training data using Monte Carlo Tree Search (MCTS) enhanced with a collaborative multi-agent system. The framework introduces InferAgent, OrchestraAgent, and JudgeAgent to enhance expansion and simulation efficiency, an intent recycling strategy that extracts multiple intent-trajectory pairs from a single search tree, and a progressive model-in-the-loop training approach. Experiments show that GUI agents trained on M²-Miner data achieve state-of-the-art performance on several mobile GUI benchmarks while significantly reducing annotation costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The intent recycling strategy re-evaluates sibling paths to extract multiple intent-trajectory pairs from a single search tree, significantly improving data diversity and mining efficiency without additional exploration costs.\n\nThe progressive model-in-the-loop training implements a three-stage training strategy, allowing agent capabilities to improve progressively in tandem with data complexity, which enhances the mining success rate in unseen scenarios."}, "weaknesses": {"value": "- The ablation study should be expanded: include a baseline using the stronger 72B model for InferAgent and JudgeAgent, but without the model-in-the-loop (MITL) strategy. This is necessary to validate the true effectiveness of MITL.\n\n- The paper mentions using 8 A100-80G GPUs for training and \"retraining for 2 epochs on the full mined dataset at each stage\". These significant computational costs, as well as the API costs for Qwen2.5-VL-72B, seem to be omitted from the 196 total cost claimed in Table 1. It is better to clarify whether the 196 figure covers this computational overhead.\n\n- The dataset partitioning is unclear. Your training set (during warm-up ) and test set both use AC and AITZ. You need to provide more detailed information, such as statistics on the exact splits, to clarify how data overlap is prevented, especially since ICLR appendices have no page limits."}, "questions": {"value": "Did you build a custom framework to run the model-in-the-loop training strategy? If so, please provide more details. If not, please specify the base framework used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y4xg55P6Qo", "forum": "Ms9MMN8STj", "replyto": "Ms9MMN8STj", "signatures": ["ICLR.cc/2026/Conference/Submission8514/Reviewer_WPrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8514/Reviewer_WPrJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980363026, "cdate": 1761980363026, "tmdate": 1762920379187, "mdate": 1762920379187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}