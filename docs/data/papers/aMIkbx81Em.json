{"id": "aMIkbx81Em", "number": 21060, "cdate": 1758313299058, "mdate": 1759896944296, "content": {"title": "A Neurosymbolic Approach to Natural Language Formalization and Verification", "abstract": "Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.", "tldr": "A neurosymbolic framework using autoformalization to validate the logical correctness of text against policies.", "keywords": ["LLM", "Neurosymbolic", "Automated Reasoning", "Constraint Solvers", "SMT solvers"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7e1e66671c6dc80f31081af65429f38bfb519af.pdf", "supplementary_material": "/attachment/dba6c0c6518524285d3f3765bcd57a391dca6b41.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents LOGICAL REASONING GUARDRAILS (LRG), a neurosymbolic framework addressing the stochasticity of Large Language Models (LLMs) in regulated industries (e.g., finance, healthcare) by enabling formalization and verification of natural language (NL) policies. Key contributions include: exceeding 99% soundness (near-zero false positives) on out-of-training datasets, providing auditable logical artifacts and actionable feedback for LLM output refinement, and demonstrating effectiveness on both synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The two-stage neurosymbolic approach is an integration of existing techniques (LLM autoformalization, symbolic reasoning, human-in-the-loop) tailored to policy verification. Also, the authors systematically evaluate key design choices (redundant translation, human vetting) and compare against 10+ baselines, providing robust evidence for LRG’s performance."}, "weaknesses": {"value": "1. While the soundness-recall tradeoff is justified for safety-critical domains, the 15.6% recall (at 99.2% soundness) is notably low. The paper could explore mitigation strategies to reduce false negatives, or hybrid filtering to prioritize high-impact valid cases—without compromising core soundness.\n2. The paper notes that human vetting amortizes over time but provides limited quantitative data on the upfront cost (e.g., hours per policy, number of experts required, error reduction per vetting iteration). \n3. The paper compares LRG to neurosymbolic methods (e.g., Logic-LM) but could more clearly articulate how its two-stage design and policy-specific focus differ from general-purpose logical reasoning frameworks."}, "questions": {"value": "First of all, I am not very specific in this domain. Here are some of my superficial questions. Of course, I will also refer to other reviewers for their scores.\n\n1. For the low recall (15.6% at 99.2% soundness), have you explored adaptive thresholding or combining LRG with lightweight pre-filters to prioritize likely valid cases?\n2. For large policy models, do you have strategies to streamline human vetting?\n3. The feedback mechanism struggles with Ambiguous/Unknown answers due to policy model gaps. How to solve it?\n4. How does LRG perform on policies with implicit temporal/conditional scoping (mentioned as an autoformalization challenge)?\n5. In the real-world case study, did you evaluate LRG’s impact on chatbot deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DEY31Z51sY", "forum": "aMIkbx81Em", "replyto": "aMIkbx81Em", "signatures": ["ICLR.cc/2026/Conference/Submission21060/Reviewer_gsUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21060/Reviewer_gsUz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536100707, "cdate": 1761536100707, "tmdate": 1762940631735, "mdate": 1762940631735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Logical Reasoning Guardrails (LRG), a neurosymbolic framework combining LLMs and symbolic reasoning for formalizing and verifying whether natural language outputs comply with institutional policies expressed in natural language. It proposes a two-stage approach: (1) the Policy Model Creator (PMC), which auto-formalizes documents into formal policy models vetted with optional human oversight, and (2) the Answer Verifier (AV), which uses LLMs to formalize validation statements and an SMT solver to provide logically-grounded verification. Empirical evaluation on extended ConditionalQA-benchmark variants and real-world policies demonstrates soundness above 99%, outperforming neural and neurosymbolic baselines, especially in minimizing false positive rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The LRG system is clearly presented, with a natural motivation for regulated/high-assurance domains where reliability and logical soundness are paramount\n- The framework offers a bi-directional benefit. By attempting to formalize natural language policies, the system can surface ambiguities and inconsistencies within the original source text (as noted in Lines 409-417), effectively using the formalization process itself as a tool for improving the clarity of the natural language policies. This is quite interesting."}, "weaknesses": {"value": "- Feasibility of Formalizing General Natural Language: The paper's central premise rests on the ability to formalize general natural language, but it does not sufficiently discuss the fundamental limitations of this endeavor. While formalization is well-understood for constrained domains like mathematics, its applicability to open-ended natural language is not guaranteed and can face significant, even philosophical, challenges regarding expressiveness and interpretation. The core assumption that a wide range of institutional policies and LLM outputs can be faithfully formalized is not robustly defended or explored.\n\n- Correctness of Formalization: Related to the first point, the evaluation methodology does not directly measure the correctness of the formalization itself. The experiments report on the final classification accuracy of the verification task, which is an end-to-end metric. This setup treats the formalization process as a black box. Without a direct evaluation of how accurately the Policy Model Creator translates natural language into formal logic, it is difficult to ascertain the validity of this crucial intermediate step.\n\n- Scalability and Human Effort Requirements: As acknowledged in Section 5, human vetting remains a practical bottleneck for scaling to long, complex policies with hundreds or thousands of rules. Even with automation, the upfront human review cost is significant. This means the system is not “push-button” for enterprise adoption, which challenges claims of broad applicability. The need for experts to resolve ambiguous or conflicting formalizations is a serious limitation that is not adequately addressed by the current implementation.\n\n- Empirical Evaluation Limitations: While the policy model creation is tested on the ConditionalQA-LOGIC dataset, these are largely concise, synthetic examples. The experiments on genuinely real-world, deeply nested, or cross-referenced policies receive significantly less attention, and the reported real-world policies study is limited in scale and complexity. The paper does not rigorously quantify the performance gap or challenges when moving from smaller synthetic policies to larger, more complex ones.\n\n- Recall-Soundness Tradeoff and Practicality: As shown in Table 1, achieving the highlighted 99.2% soundness comes at a steep cost: recall plummets to 15.6%. While the paper correctly observes that this may be appropriate for highly regulated domains, it substantially limits practical adoption for broader document understanding tasks where coverage is also necessary."}, "questions": {"value": "1. Could the authors elaborate on the theoretical or empirical basis for assuming that formal methods can be effectively applied to a broad spectrum of general natural language policies and outputs?  What are the perceived boundaries or limitations of expressiveness for your formalization approach?\n\n2. How could one design an experiment to directly quantify the correctness of the intermediate formalization step (i.e., the mapping from natural language to the logical model), separate from the final verification task accuracy? This seems critical for validating the core contribution.\n\n3. Considering the acknowledged bottleneck of human vetting and the limited scale of the real-world experiments, what is the envisioned path to demonstrate that LRG can scale effectively to complex, enterprise-level policy documents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qw5vZQshov", "forum": "aMIkbx81Em", "replyto": "aMIkbx81Em", "signatures": ["ICLR.cc/2026/Conference/Submission21060/Reviewer_4hVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21060/Reviewer_4hVN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811954390, "cdate": 1761811954390, "tmdate": 1762940631459, "mdate": 1762940631459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Logical Reasoning Guardrails (LRG), a two-stage neurosymbolic framework for verifying LLM-generated content against formal policy models. The Policy Model Creator (PMC) translates natural language policies into SMT-LIB logic, while the Answer Verifier (AV) validates claims using redundant translation and SMT solving. The authors claim to achieve 99.2% soundness, exceeding existing approaches.\n\nWhile the problem is important and the technical approach is sound, the evaluation is severely limited in scope, missing important LLM+solver baselines and standard benchmarks. The paper evaluates primarily on a single custom dataset, omits comparison with directly competing neurosymbolic methods, and provides insufficient evidence that the 99.2% soundness (at 15.6% recall) justifies the computational cost and human effort required."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Verifiable guardrails for regulated industries are a timely and practically relevant challenge.\n2. The combination of autoformalization, redundant translation, and SMT verification is well-motivated and well-implemented.\n3. 99.2% soundness (2.5% FPR) represents strong performance on the primary metric, important for safety-critical applications.\n4. Human-in-the-loop vetting, multiple finding categories, and actionable feedback demonstrate thoughtful system architecture.\n5. The system produces auditable logical explanations and counter-examples, valuable for debugging and trust.\n6. I liked the honest limitations discussion. Authors acknowledge scalability, cost, and human effort challenges - often missing from LLM+Solver papers."}, "weaknesses": {"value": "In its current form, the paper does not meet the bar to be published as a full paper at ICLR. Please consider the following as constructive feedback on how to improve it during the rebuttal process. I will increase my ratings if you can conduct all these experiments.\n\n1. **Limited Evaluation Scope**: The paper evaluates primarily on ONE custom modified dataset (ConditionalQA-Logic), with only ONE real-world case study (44 test cases).\n   1. The paper is missing evaluation on LogicNLI (Tian et al), StrategyQA (Geva et al.), ProntoQA (Saparov et al.), FOLIO (Han et al.), ProofWriter (Tafjord et al.), EntailmentBank (Dalvi et al.), BIG-Bench, or any established logical reasoning benchmarks for LLM+Solver papers. (**Please evaluate on at least 3 aforementioned datasets**)\n   2. The proposed evaluation on the ConditionalQA-Logic uses systematically manipulated errors (deliberately removing conditions, flipping yes/no) rather than natural LLM mistakes. While this could be justified as a separate contribution, it cannot be the primary result. The authors should benchmark on standard datasets used in the literature mentioned above.\n   3. The authors discuss six real-world policies collected but evaluate only one. Perhaps these could be explored qualitatively in the appendix (Weakness 3)\n   4. Table 2 numbers are not statistically significant. 44 examples are insufficient to claim 100% soundness. Please avoid claims that are not backed up rigorously.\n\n2. **Missing Baseline Comparisons**: Please compare your method via benchmarking against (**benchmarking against a subset is acceptable, but compare your methodology against all to show why you are different/better**) the mentioned papers and other well-known techniques in the LLM+Solver paradigm, namely: (1) LINC (Olausson et al. Cited, not compared.), (2) Proof of Thought library (very similar yet not cited or compared, Ganguly et al. does both JSON and SMT program generation), (3) SAT-LM (Ye et al. cited, not compared), and (4) Verus-LM (Callewaert et al. cited, not compared). This is important because LogicLM is compared in Table 1, where it shows soundness & FPR numbers outperforming 9/10 baselines. I am looking for better answers to the following related questions:\n   1. Without the aforementioned analysis, the claim in Line 58 - \"Exceeds 99% soundness... unattainable by existing approaches\". This is not backed up with sufficient evidence.\n   2. Line 61 claims the method \"outperforms\" on soundness but achieves only 1/3 to 1/6 the recall of alternatives. For example, in Table 1, RefChecker achieves 87.6% recall versus LRG's 15.6% recall. Moreover, at 99.2% soundness, the system achieves only 15.6% recall, rejecting 84.4% of valid content. While this is acknowledged as a trade-off, no analysis shows how this recall can be improved by examining threshold-agnostic measures such as the area under the risk-coverage curve.\n   3. Soundness, with its particular definition as a metric, is proposed for the first time in this paper. Please consider emphasizing on other well-accepted metrics such as False Positive Rate.\n3. **Need for Domain-Specific Analysis**: The paper opens with applications to legal, medical, and financial compliance. Qualitative case studies from two of these domains would significantly strengthen the paper. Please also discuss qualitatively, through multiple autoformalizations using PMC, the inherent uncertainty in outputs of LLM-driven auto formalizations (Ganguly et al. 2025) and how that may impact the correctness of your pipeline in the absence of human review."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "688v4sa6gF", "forum": "aMIkbx81Em", "replyto": "aMIkbx81Em", "signatures": ["ICLR.cc/2026/Conference/Submission21060/Reviewer_N5bu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21060/Reviewer_N5bu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932527735, "cdate": 1761932527735, "tmdate": 1762940631033, "mdate": 1762940631033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LRG (Logical Reasoning Guardrails) — a neurosymbolic framework designed to ensure high-assurance logical accuracy when LLM-generated answers rely on natural-language policy documents. The approach separates: Policy Model Creator (PMC) , Offline autoformalization of NL policies into SMT-LIB logic, with optional human vetting (linting, rule inspection, symbolic test generation).\nAnswer Verifier (AV): Online validation that translates QA content into logical premise-conclusion pairs using multiple LLMs for redundancy and uses an SMT solver to deliver explainable verification based on sound logical inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides logical justification / counterexamples for every verdict\n- Improves LLM responses by iteratively refining answers\n- Standardized logical formalism with SMT-LIB"}, "weaknesses": {"value": "- Recall is low: A Conservative guardrail can block many correct responses\n- Human vetting is time-intensive for long/complex policies\n- Formalization errors in PMC propagate into AV despite validations\n- Numerical tables, implicit temporal constraints, and nested logic are still hard for autoformalization\n\nMy primary concern is that the core technical contributions seem closer to a well-engineered pipeline than a novel methodology advance. The system components, like natural language autoformalization, symbolic consistency checking, redundant translation, and SMT-based verification, are mostly variations of existing neurosymbolic techniques. Much of the contribution lies in integration and workflow design, particularly the human-in-the-loop vetting tools (linting, structured English rules, symbolic test generation), which, although useful, feel incremental from a research innovation standpoint rather than proposing new theoretical insights or algorithmic techniques. \n\nAdditionally, the writing could benefit from a more polished presentation and better articulation of novelty. Certain implementation decisions appear heuristic without rigorous justification or ablation. It would help to more clearly highlight what is conceptually new versus what is assembling existing modules. Finally, while the focus on soundness over recall is reasonable for high-assurance domains, the very low recall raises questions about real usability and practical tradeoffs."}, "questions": {"value": "- How does performance scale with very large enterprise policies (hundreds of pages)?\n- Can policy vetting be partially automated, e.g., with rule clustering or active-learning review?\n- Could reinforcement learning or retrieval improve recall while preserving soundness?\n- What guarantees ensure that the autoformalized policy faithfully captures human legal intent?\n- How sensitive is the system to domain terminology shifts (e.g., health vs. finance policies)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FKHmYwrZmW", "forum": "aMIkbx81Em", "replyto": "aMIkbx81Em", "signatures": ["ICLR.cc/2026/Conference/Submission21060/Reviewer_7Twk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21060/Reviewer_7Twk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065244085, "cdate": 1762065244085, "tmdate": 1762940630600, "mdate": 1762940630600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LRG (Logical Reasoning Guardrails), a two-stage neurosymbolic framework that aims to bring formal logical verification into the workflow of LLMs applied to compliance, legal, and policy-related question answering.\n\nThe framework consists of:\n(1) Policy Model Creator (PMC):\nA module that automatically converts long natural-language policy documents into formal logical models (SMT-LIB rules), while supporting human-in-the-loop review, linting, testing, and auto-repair.\n(2) Answer Verifier (AV):\nA runtime component that translates QA instances into multiple candidate logical assertions using diverse LLMs, checks their semantic equivalence, and verifies each using an SMT solver (Z3). It outputs structured verdicts such as Valid, Invalid, Satisfiable, Impossible, Ambiguous, or Unknown, accompanied by explicit reasoning evidence and counterexamples.\nOn the ConditionalQA-Logic benchmark, LRG achieves 99.2% soundness (false-positive rate 2.5%) under a strict 3/3 consistency threshold, significantly outperforming strong baselines. A real-world case study on airline refund policies further demonstrates that human-assisted policy repair improves soundness to 100%.\nOverall, LRG provides a practical, auditable pipeline for enforcing logical correctness and interpretability in LLM-based systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed LRG framework effectively integrates a human-in-the-loop refinement process in the Policy Model Creator (PMC), enhancing the accuracy and interpretability of autoformalized policy models, which I think should be an essential step for ensuring the soundness of future verification.\n\n- The Answer Verifier (AV) introduces redundant translation and semantic voting across multiple LLMs to improve both syntactic and semantic reliability during natural language-to-logic formalization.\n\n- Unlike binary verification schemes, LRG supports multiple feedback categories (Valid, Invalid, Satisfiable, Impossible, Ambiguous, etc.), providing users with explainable, logically grounded feedback and counterexamples that facilitate iterative refinement.\n\n- The work is supported by extensive experiments across diverse neurosymbolic baselines. The analysis of soundness, recall, and redundancy impact provides a well-rounded assessment."}, "weaknesses": {"value": "- Despite achieving 99.2% soundness, LRG’s recall remains low (15.6%). The experiments primarily involve moderately complex domains (e.g., park admission, airline refund). Evaluating LRG on domains with nested logical dependencies—such as healthcare or financial compliance—would better demonstrate its scalability and generality. Nonetheless, this limitation may represent a natural boundary of current verification paradigms and could be left for future work.\n\n- The comparison with prior methods (e.g., methods in Table 1) focuses primarily on soundness and recall. Further analysis in terms of latency, token cost, and scalability is also necessary (in my view).\n\n- It also remains unclear whether the main novelty lies solely in the fine-grained feedback mechanism and human-in-the-loop formalization, compared with other methods given in the paper, like Logic-LM (as Logic-LM makes formalization fully automated and only reports the deterministic results). The authors should make the discussion about the difference between existing works deeper.  Currently, it is not so clear in the paper. The paper also lacks delineated conceptual distinctions between LRG and other emerging verification approaches, such as RvLLM (which also provides some policy model for verification), proposed earlier this year.\n\n- While the system introduces multiple fine-grained feedback types, concrete examples are sparse. Providing at least one example for each feedback category would help readers understand their practical role in guiding refinement.\n\n- Since this work includes human-in-the-loop for the policy model generation, providing a user study would make the paper more solid.\n\nOther Minor Issues\n- The expression \"Satisfiable ($M\\wedge P\\nvDash C$ and $M\\wedge P\\nvDash\\neg C$)\" is confusing — it seems that \"and\" should be replaced with \"or\"? If this interpretation is correct, would a simpler and clearer notation, such as \"$M\\wedge P$\", be sufficient?\n- Figures 2 and 3 use inconsistent variable or predicate names; aligning them under a single, coherent example would enhance readability and consistency.\n\nI may revise my final score depending on the authors’ response."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7llffJB6QX", "forum": "aMIkbx81Em", "replyto": "aMIkbx81Em", "signatures": ["ICLR.cc/2026/Conference/Submission21060/Reviewer_nyXc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21060/Reviewer_nyXc"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21060/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762866828596, "cdate": 1762866828596, "tmdate": 1762940630319, "mdate": 1762940630319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}