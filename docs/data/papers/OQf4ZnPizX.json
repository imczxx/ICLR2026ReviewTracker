{"id": "OQf4ZnPizX", "number": 24108, "cdate": 1758352815394, "mdate": 1759896781385, "content": {"title": "Zeroth-Order Forward-Only SNN Training Inspiring Neuromorphic On-Chip Learning", "abstract": "The human brain is a biologically instantiated on-device neural system that integrates both learning and inference in a unified architecture, which enables rapid and flexible learning on-the-fly. This extraordinary capability is achieved through non-backpropagation learning mechanisms, whereas backpropagation (BP) is computationally and memory intensive which makes it unsuitable for on-chip edge learning. zeroth-order (ZO) optimization methods, which resemble biologically plausible perturbation-based learning, offer a promising alternative that enables learning with only forward passes and hence can significantly reduce the complexity of on-chip hardware implementation. However, in this work we show that applying ZO methods to spiking neural networks (SNNs) is non-trivial due to the step-function nature of spiking activation (e.g., Heaviside function). We analyze the challenges posed by the step-function activation, and propose a novel subspace-based zeroth-order (SZO) learning method that leverages the intrinsic low-dimensional structure of the SNN optimization trajectory. By learning in a low-dimensional subspace, SZO substantially enhances ZO learning efficacy, achieving accuracy comparable to first-order (FO) methods with faster learning speed than full-space BP. We evaluate SZO on model training from scratch, continual training, and unsupervised adaptation. Experimental results demonstrate that SZO closely approaches FO training performance for the first time while offering fast learning speed. We expect this work to inspire future research on highly efficient and scalable algorithms for neuromorphic on-chip learning.", "tldr": "", "keywords": ["Spiking neural networks", "zeroth-order optimization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfc1aecad61ec44e3114f12870a15e087ee65595.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This papaer propose an parameter-effcient learning zeroth order learning method on SNN, possibly aiming at edge case learning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation. This work studies zeroth-order learning in SNN by first analyzing the variance introduced by the Heaviside function, then proposes multi-sampling and subspace-based learning to reduce it, two typical methods in zeroth-order learning.\n2. Clear problem formulation and notations.\n3. The author provides a dedicated analysis of the efficiency and superiority of the proposed zeroth-order learning method."}, "weaknesses": {"value": "1. The paper lacks adequate discussion of existing methods that combine subspace-based fine-tuning with zeroth-order learning in ANNs [1-4]. The related work section in the appendix appears to **intentionally separate subspace-based learning from BP-free methods.** There is only one line mentioning ZO+subspace learning, stating \"it has been recently shown that ZO optimization can effectively finetune LLMs,\" without explicitly describing how these methods fine-tune LLMs (through subspace learning techniques). There is a section for subspace method but also not connected to how these method is adopted by ZO-based methods. The authors should provide a detailed comparison with these prior works. Currently, the primary distinction appears to be the choice of PCA over LoRA or stochastic projection matrices used in prior ANN work. Especially given the  concerns regarding theoretical contribution (see W.2), a more thorough comparative analysis is needed.\n2. **Assumption 1, Point 1** is problematic and undermines the theoretical foundation. The authors assume that $\\mathcal{L}_x(\\theta)$ is $L$-smooth with respect to $\\theta$, yet the forward pass of SNNs is inherently discrete, even when employing surrogate gradients. This fundamental tension is not acknowledged as an approximation choice or limitation. *While assuming something that **might not** exist is acceptable in theoretical work, assuming something that **demonstrably does not** exist without any justification or discussion is problematic.* Consequently, the theoretical analysis effectively reduces to proving that combining subspace methods with zeroth-order optimization on *ANNs* (where the loss function may genuinely be $L$-smooth) can reduce variance and improve convergence rates—a result extensively studied in the ANN literature [1-4]. These prior works employ similar analytical frameworks assuming $L$-smoothness, exploitable local structure, and bounded gradient variance. I acknowledge that **Proposition 1** genuinely addresses SNNs by characterizing the increased variance when transitioning from continuous activation functions to the Heaviside function. However, this result does not lead to any subsequent analysis that meaningfully respects or leverages the discrete nature of SNNs. The remainder of the paper relies on assumptions appropriate for the ANN case, limiting the theoretical contribution.\n3. Limited practical validation\n    - *Missing Edge Device Evaluation:* Despite targeting edge device training, the paper provides no actual edge device performance evaluation, making it difficult to assess the real-world potential of the proposed algorithm. \n    - *Unrealistic Operating Regimes and Missing Baselines:* In practice, edge deployments may only operate at time steps and batch sizes corresponding to the leftmost region of Figure 5's x-axis. Furthermore, the paper lacks comparison with gradient checkpointing methods that achieve $\\mathcal{O}(\\sqrt{n})$ memory scaling (where $n$ denotes layers or timesteps) with $\\mathcal{O}(n)$ extra recomputation—typically a 1.5× slowdown that significantly reduces memory consumption. Given that SZO-CGE is slower than BP (and possibly comparable to SZO-RGE with larger $q$), especially in training-from-scratch settings, BP with checkpointing (possibly combined with parameter-efficient learning) appears to be a competitive alternative that should be evaluated. \n    - *Mixed-Precision Training Not Addressed:* Edge training typically uses mixed-precision to further reduce memory, but this is not clearly discussed in the main paper. In such settings, parameters and their buffers (first and second-order momentum) are often maintained in full-precision for stability. While ZO methods save activation memory, parameters still occupy consistent memory throughout training. Therefore, the memory reduction may not be as significant as Figure 5 suggests, and this should be addressed.\n4. Lack of justification for subspace selection. The authors choose PCA for parameter-efficient learning with theoretical analysis but provide no empirical justification for this choice. For continued learning scenarios, various parameter-efficient fine-tuning methods exist, such as tuning only the scaling factors in batch normalization layers [5,6] or advanced techniques like LoRA. The proposed method is not inherently limited to PCA: any projection matrix satisfying Point 2 of Assumption 2 could be used. The authors should discuss (though not necessarily experimentally validate) the potential for extending beyond PCA for subspace identification.\n\nOverall, though the paper aims at practical edge case with a fine method, the presentation and emperical study need significant improvement. \n\n**References**:\n\n[1] Malladi, Sadhika, et al. \"Fine-tuning language models with just forward passes.\" _Advances in Neural Information Processing Systems_ 36 (2023): 53038-53075.\n\n[2] Jin, Feihu, Yifan Liu, and Ying Tan. \"Derivative-free optimization for low-rank adaptation in large language models.\" _IEEE/ACM Transactions on Audio, Speech, and Language Processing_ (2024).\n\n[3] Chen, Yiming, et al. \"Enhancing zeroth-order fine-tuning for language models with low-rank structures.\" _arXiv preprint arXiv:2410.07698_ (2024).\n\n[4] Yu, Ziming, et al. \"Subzero: Random subspace zeroth-order optimization for memory-efficient llm fine-tuning.\" (2024).\n\n[5] Kanavati, Fahdi, and Masayuki Tsuneki. \"Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning.\" _Medical Imaging with Deep Learning_. PMLR, 2021.\n\n[6] Li, Yanghao, et al. \"Adaptive batch normalization for practical domain adaptation.\" _Pattern Recognition_ 80 (2018): 109-117."}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1R793ty3Ev", "forum": "OQf4ZnPizX", "replyto": "OQf4ZnPizX", "signatures": ["ICLR.cc/2026/Conference/Submission24108/Reviewer_1k7D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24108/Reviewer_1k7D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492619522, "cdate": 1761492619522, "tmdate": 1762942941098, "mdate": 1762942941098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of zeroth-order online optimization (ZO-OCO), where only function value feedback (but not gradients) is available to the learner in a sequential decision-making process. The authors propose a forward-form zeroth-order algorithm, which replaces the traditional backward or implicit update commonly used in online mirror descent with a forward approximation step."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The study addresses a significant and practical challenge — optimizing online systems when gradients are unavailable or unreliable. Zeroth-order online optimization has direct implications in areas such as reinforcement learning, hyperparameter tuning, and online control, making the problem both important and impactful. The paper offers a solid theoretical foundation, including well-defined regret analysis and clear assumptions. The proofs are logically structured and technically competent. The derived regret bounds align with or slightly improve upon existing methods, indicating meaningful theoretical contribution."}, "weaknesses": {"value": "1. While the paper proposes a “forward” version of zeroth-order online optimization, the conceptual difference from existing frameworks (e.g., zeroth-order mirror descent, bandit feedback optimization) remains unclear. Theoretical analysis largely follows standard regret minimization proofs, and the innovation seems incremental rather than fundamentally new.\n2. The motivation for introducing the forward variant is purely mathematical; there is little intuitive or practical justification. Readers might find it difficult to understand why this direction is useful, beyond being an alternative formulation.\n3. The related work section omits discussion of some recent or highly relevant contributions in zeroth-order online learning and black-box optimization (e.g., gradient-free policy optimization, adaptive bandit feedback methods).\n4. The experiments are limited to synthetic tasks and a small number of real-world benchmarks. There are no tests on large-scale or higher-dimensional problems, where zeroth-order methods often struggle.\n5. The effect of algorithmic hyperparameters (step size, perturbation radius, or sampling dimension) is not explored. This omission limits the reader’s understanding of how robust or tunable the algorithm is.\n6. Although the paper presents regret bounds, it does not provide detailed computational complexity or query complexity comparisons. The number of function evaluations per iteration can be critical in zeroth-order settings.\n7. Since zeroth-order methods are often used in noisy or non-smooth environments, the lack of experiments under stochastic noise weakens the practical claim."}, "questions": {"value": "1. How does the proposed “forward” formulation differ practically from the backward (implicit) update in online mirror descent? Could you provide an example where the forward version performs strictly better?\n2. What is the computational cost per iteration compared to traditional ZO-SGD or bandit OCO methods?\n3. How sensitive is the proposed algorithm to the choice of perturbation radius and step size?\n4. Could your method be adapted to stochastic or partially observable settings?\n5. Would it be possible to extend your theoretical results to non-convex loss functions or dynamic environments with adversarial noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1qR1jNeFnZ", "forum": "OQf4ZnPizX", "replyto": "OQf4ZnPizX", "signatures": ["ICLR.cc/2026/Conference/Submission24108/Reviewer_nGip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24108/Reviewer_nGip"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871324060, "cdate": 1761871324060, "tmdate": 1762942940836, "mdate": 1762942940836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a SZO learning method for training SNNs in a feedforward-only manner. Motivated by the demand for efficient, biologically plausible, and on-chip trainable learning, the authors argue that conventional zeroth-order (ZO) optimization struggles with the discontinuous Heaviside activation inherent to SNNs. To overcome this challenge, they introduce a low-dimensional subspace learning strategy that leverages the intrinsic structure of SNN optimization trajectories to reduce the variance of perturbation-based gradient estimation. Experimental results demonstrate that SZO achieves accuracy comparable to first-order methods while providing faster convergence and superior scalability to large-scale datasets such as ImageNet."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tNovel training paradigm.\nThe paper introduces the first forward-pass-only training method for SNNs. This direction is of high significance for neuromorphic learning, as it potentially enables direct on-chip and energy-efficient training.\n2.\tScalability and practicality.\nThe proposed approach demonstrates scalability to large-scale datasets, showing competitive performance on ImageNet-1K when trained from scratch, which has rarely been achieved by previous zeroth-order or surrogate-gradient-based SNN training methods."}, "weaknesses": {"value": "1.\tUnclear motivation for subspace-based optimization.\nThe motivation section states that the major challenge of zeroth-order SNN training lies in the large variance caused by the non-differentiable Heaviside function. However, it is not clearly explained why the proposed subspace-based method can mitigate such variance. Since the subspace zeroth-order strategy can also be applied to ANNs, the unique advantage of using it specifically for SNNs remains unclear.\n2.\tLimited performance gain and insufficient comparative baselines.\nAlthough the authors claim their method is the first forward-pass-only training scheme that can scale to large-scale datasets, the ImageNet-1K results show only marginal improvement over OPZO, even with deeper architectures. Considering that previous work perform fine-tuning under noisy perturbations, while this work supports from-scratch training, more discussion should be provided on the trade-off between accuracy and the unique advantage offered by SZO over other zeroth-order SNN approaches.\nMoreover, in Figure 4 and Figure 5, the comparisons are only made with BP and BP-Subspace. Including existing zeroth-order SNN training baselines would be essential to better highlight the value and competitiveness of the proposed method.\n3.\tInconsistent observation on CGE vs. RGE.\nThe paper states that CGE performs better than RGEn deeper networks according to ANN literature. However, in the presented experiments, SZO-CGE and SZO-RGE show almost identical performance on ImageNet, while the performance gap appears only in shallower networks (e.g., CIFAR-10/DVS). More theoretical or empirical discussion is required to explain the different behaviors of these perturbation types under spike-based computation.\n4.\tMissing analysis of computational trade-offs.\nThe ablation study reports the accuracy change with different subspace sampling numbers q in RGE. However, it lacks an important analysis of the computational overhead introduced by increasing Q and its trade-off with accuracy improvement.\n5.\tLimited evaluation on sequential datasets.\nBeyond static datasets such as CIFAR and ImageNet, previous works have also evaluated SNNs on longer temporal sequence datasets (e.g., SHD), which better reflect the temporal credit-assignment capability of SNN training algorithms. Including such datasets would strengthen the claim that the proposed method can handle temporally complex tasks."}, "questions": {"value": "1.\tWhat is the conceptual or theoretical link between subspace sampling and the reduction of Heaviside-induced gradient variance?\n2.\tHow does SZO for SNNs differ from the subspace zeroth-order methods previously applied to ANNs, e.g., [1]?\n3.\tIn Figure 3, the SZO method already achieves ~50% accuracy at the first epoch. Does this result from specific weight initialization or pre-training? If so, why is the same initialization not applied to the BP baseline? \n\n[1] Yu, Ziming, et al. \"Zeroth-order fine-tuning of llms in random subspaces.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ucTFfbxsJa", "forum": "OQf4ZnPizX", "replyto": "OQf4ZnPizX", "signatures": ["ICLR.cc/2026/Conference/Submission24108/Reviewer_GrS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24108/Reviewer_GrS7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881235225, "cdate": 1761881235225, "tmdate": 1762942940520, "mdate": 1762942940520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a subspace-based zeroth-order (SZO) method for optimizing spiking neural networks. The authors first examine the low-dimensional structure of the training trajectory of SNNs, and then propose to perform zeroth-order optimization only on the subspace. Theoretical analysis is provided to show the improvement on convergence rate, and experiments on training from scratch, model fine-tuning, as well as unsupervised adaptation demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Zeroth-order optimization is a biologically more plausible and hardware-friendly method for SNNs. This paper largely enhances ZO by leveraging the subspace of parameters, which is promising for on-chip learning.\n2. This paper performs theoretical analysis on the convergence rate of SZO, showing its improvement in convergence rate.\n3. Extensive experiments on three settings demonstrate the promising results of SZO. Particularly, SZO works well for RGE (q=1) even on ImageNet, which may largely advance the efficiency and performance of zeroth-order methods to train neural networks from scratch."}, "weaknesses": {"value": "1. The subspace matrix $P$ requires an additional stage of BPTT training, making it not purely zeroth-order.\n2. SZO may require additional memory $k$ times the parameter amount (for storing the subspace matrix), which would limit the scalability to very large models. It also poses challenges to realize subspace parameterization on hardware for potential on-chip learning."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xQuCnYqUkt", "forum": "OQf4ZnPizX", "replyto": "OQf4ZnPizX", "signatures": ["ICLR.cc/2026/Conference/Submission24108/Reviewer_fmgm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24108/Reviewer_fmgm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24108/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935631206, "cdate": 1761935631206, "tmdate": 1762942940283, "mdate": 1762942940283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}