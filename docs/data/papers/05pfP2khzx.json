{"id": "05pfP2khzx", "number": 13771, "cdate": 1758222291968, "mdate": 1762964082540, "content": {"title": "Self-Correcting Text-to-Video Generation with Misalignment Detection and Localized Refinement", "abstract": "Recent text-to-video (T2V) diffusion models have made remarkable progress in\ngenerating high-quality and diverse videos. However, they often struggle to align\nwith complex text prompts, particularly when multiple objects, attributes, or spatial\nrelations are specified. We introduce VideoRepair, the first self-correcting,\ntraining-free, and model-agnostic video refinement framework that automatically\ndetects fine-grained text–video misalignments and performs targeted, localized\ncorrections. Our key insight is that even misaligned videos usually contain correctly\nrendered regions that should be preserved rather than regenerated. Building on this\nobservation, VideoRepair proposes a novel region-preserving refinement strategy\nwith three stages: (i) misalignment detection, where systematic MLLM-based evaluation\nwith automatically generated spatio-temporal questions identifies faithful\nand misaligned regions; (ii) refinement planning, which preserves correctly generated\nentities, segments their regions across frames, and constructs targeted prompts\nfor misaligned areas; and (iii) localized refinement, which selectively regenerates\nproblematic regions while preserving faithful content through joint optimization\nof preserved and newly generated areas. This self-correcting, region-preserving\nstrategy converts evaluation signals into actionable guidance for refinement, enabling\nefficient and interpretable corrections. On two challenging benchmarks,\nEvalCrafter and T2V-CompBench, VideoRepair achieves substantial improvements\nover recent baselines across diverse alignment metrics. Comprehensive\nablations further demonstrate the efficiency, robustness, and interpretability of our\nframework.", "tldr": "", "keywords": ["Video Generation", "Multi-agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/92074a4083fee85665efd54a5e543a7af3d7095e.pdf", "supplementary_material": "/attachment/d49f3262bd569432cfbb01e316e81fba9e473798.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VIDEOREPAIR, a video refinement framework to correct text-video misalignments. It has three steps: 1. detect misalignment. Finding the issue and region with MLLM. 2. Plan the refinement including preserve the correct parts and construct prompts that could be used to re-generate the target parts. 3. regenerate the incorrect parts. \nThe method is evaluated on two benchmark EvalCrafter and T2V-CompBench on three different text to video models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is intuitive: finding the incorrect part and fix it.\n2. The author demonstrate the performance with different models on different benchmarks"}, "weaknesses": {"value": "1. The video quality in the paper and supplementary material is not of high quality. Although it fixed the alignment issue, the quality is far below sota video generation model. Meanwhile, the method is not tested on sota models.\n\n2. The blending process could be hard for object parts edit. For example, if a shirt or eye color is incorrect, then it is hard for the aggregating pipeline. While the author did not show those results.\n\n3. While the method claim to be model agnostic, but many component like noise initalization is based on diffusion-based model. In other words, it still only fit certain architecture."}, "questions": {"value": "1. what are the cost of the total editing process?\n\n2. For the same cost or the same iteration, if we regenerate the video the same time or the same cost, will that yeild a better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ygO9k7VKw", "forum": "05pfP2khzx", "replyto": "05pfP2khzx", "signatures": ["ICLR.cc/2026/Conference/Submission13771/Reviewer_EeWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13771/Reviewer_EeWe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804812191, "cdate": 1761804812191, "tmdate": 1762924303101, "mdate": 1762924303101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "6dOs45S72Q", "forum": "05pfP2khzx", "replyto": "05pfP2khzx", "signatures": ["ICLR.cc/2026/Conference/Submission13771/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13771/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964080955, "cdate": 1762964080955, "tmdate": 1762964080955, "mdate": 1762964080955, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the challenge that current text-to-video (T2V) models often fail to align with complex text prompts，the authors propose VideoRepair, a training-free, self-correcting, and model-agnostic video refinement framework. VideoRepair automatically detects fine-grained text–video misalignments and performs targeted, localized corrections. The key contributions are as follows:\n- Misalignment detection, which identifies both faithful and misaligned regions within generated videos;\n- Refinement planning, which preserves correctly generated entities, segments their corresponding regions across frames, and constructs targeted prompts for misaligned areas;\n- Localized refinement, which selectively regenerates problematic regions while preserving faithful content through joint optimization of preserved and newly generated areas."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation of the work is clear and well-justified.\n- The experimental evaluation is extensive."}, "weaknesses": {"value": "- The authors point out that existing methods suffer from high computational cost, visual inconsistency, and temporal incoherence. However, they do not provide sufficient analysis or discussion on how their proposed approach addresses these issues.\n- It remains unclear how the proposed method ensures adaptability and interpretability across different prompts.\n- As is well known, large language models (LLMs) are prone to hallucinations. It is unclear how the authors ensure the correctness of the generated results. For example, when the method relies on prompts to obtain 2D coordinates, how do the authors verify that the predicted locations are accurate?\n- It is not clearly stated what type of segmentation model is used to extract the masks. I think the mask is much important for the results in such mask-guided refinement strategy.\n- From the refined video shown in Figure 1, the newly generated person still exhibits noticeable distortions, particularly in the hands, which appear unrealistic. Moreover, the visualizations in Figure 3 seem unfaithful, different frames show significant inconsistencies in object appearance (e.g., the pigs), suggesting a lack of temporal smoothness. The authors also did not provide any video demonstrations to substantiate the claimed temporal consistency. Based on my experimental experience, such a refinement strategy may introduce undesirable side effects, such as noticeable jitter or flickering in the generated videos.\n- The authors claim to provide spatio-temporal feedback signals; however, they do not clearly explain how the spatio-temporally consistent queries are generated or formulated within their framework.\n- In Table 3, it is difficult to assess the effectiveness of the proposed module. For example, the first row should include a comparison between results with and without the evaluation component to demonstrate its contribution. However, the authors do not provide such ablation results, nor do they include corresponding visual examples for validation. As a result, it is hard to convincingly support the effectiveness of the proposed solution."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The core issue is that the reported metrics are insufficient to demonstrate the effectiveness of the proposed method, as the paper lacks extensive visual and video evidence to support its claims."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nfBvAALDzB", "forum": "05pfP2khzx", "replyto": "05pfP2khzx", "signatures": ["ICLR.cc/2026/Conference/Submission13771/Reviewer_hMBu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13771/Reviewer_hMBu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894453064, "cdate": 1761894453064, "tmdate": 1762924302394, "mdate": 1762924302394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the text-video misalignment problem under complex cues in T2V generation by proposing a model-agnostic, training-free, refined framework, VIDEOREPAIR. Its core achieves self-correction through a two-stage process: first, it utilizes a multimodal large model (MLLM) to generate fine-grained spatiotemporal problem detection, identifying misaligned regions and locking in the correct content; then, through region-preserving segmentation and target cue construction, it locally regenerates the problem region and integrates the global content."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces a novel region-preserving self-correction paradigm for T2V refinement. The integration of MLLM-driven spatio-temporal error detection with localized noise re-initialization and prompt decomposition is a creative combination of existing ideas tailored to the unique challenges of video (temporal coherence, frame-wise consistency).\n2. VideoRepair’s training-free and model-agnostic nature makes it easily deployable with existing diffusion-based T2V backbones."}, "weaknesses": {"value": "1. The core misalignment detection and planning steps rely heavily on MLLM's performance.\n2. Iterative refinement design is underdeveloped: The authors briefly mention iterative refinement but provide limited details on its practical utility. For example, there is no analysis of how many iterations are typically needed for different prompt types, whether performance plateaus after a certain number of steps, or how to balance iteration count with inference efficiency.\n3. The motivation is somewhat trivial, only considering the issue of subject identity and using masks and segments to handle it. However, video generation has many other problems, such as poor animation and image degradation, which were not taken into account."}, "questions": {"value": "1. How does VIDEOREPAIR handle complex failure modes such as (a) object occlusion across frames, (b) motion blur leading to ambiguous object boundaries, and (c) conflicting prompt constraints? Could you provide qualitative examples and quantitative metrics for these scenarios, and explain how the current framework addresses (or fails to address) them?\n2. What is the typical number of iterations required for iterative refinement across different prompt categories (count, spatial relations, attribute binding)? Can you show a performance-efficiency curve (alignment score vs. number of iterations vs. inference time) to guide practical use?\n3. Could you clarify how the framework handles prompts with implicit spatial/temporal relations (e.g., \"a cat chasing a mouse from left to right over 10 frames\")"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ycBToBY7Bj", "forum": "05pfP2khzx", "replyto": "05pfP2khzx", "signatures": ["ICLR.cc/2026/Conference/Submission13771/Reviewer_sEDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13771/Reviewer_sEDb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13771/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929961110, "cdate": 1761929961110, "tmdate": 1762924301867, "mdate": 1762924301867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}