{"id": "tuvkrivvbG", "number": 25088, "cdate": 1758363969117, "mdate": 1759896734715, "content": {"title": "Resurfacing the Instance-only Dependent Label Noise Model through Loss Correction", "abstract": "We investigate the label noise problem in supervised binary classification settings and resurface the underutilized instance-_only_ dependent noise model through loss correction. On the one hand, based on risk equivalence, the instance-aware loss correction scheme completes the bridge from _empirical noisy risk minimization_ to _true clean risk minimization_ provided the base loss is classification calibrated (e.g., cross-entropy). On the other hand, the instance-only dependent modeling of the label noise at the core of the correction enables us to estimate a single value per instance instead of a matrix. Furthermore, the estimation of the transition rates becomes a very flexible process, for which we offer several computationally efficient ways. Empirical findings over different dataset domains (image, audio, tabular) with different learners (neural networks, gradient-boosted machines) validate the promised generalization ability of the method.", "tldr": "We resurrect the instance-only dependent label noise model via loss correction that connects the empirical-noisy-risk with the true-clean-risk.", "keywords": ["label noise", "loss correction", "instance-dependence", "risk equivalence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7e631e0cd64257efe5fc847d206077e2909a9d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper looks at label noise in binary classification. It suggests a new way to fix errors using an instance-aware loss correction scheme. This is based on the instance-only dependent (IDN) noise model. The main idea is to model the chance of a label being wrong as $\\rho_x = \\mathbb{P}(Y \\neq \\tilde{Y} | X = x)$. This depends only on the instance $X$, not the true label $Y$. This is different from the instance-label dependent noise model (ILDN). The authors create a new loss function $\\tilde{\\ell}$ that matches the risk of noisy data with clean data, as long as the base loss is classification-calibrated, like cross-entropy. They estimate the transition rates $\\rho_x$ using the distance to the decision boundary to show how hard an instance is. A regularized version $\\tilde{\\ell}_R$ is also introduced to avoid numerical problems, with theoretical support using Rademacher complexity bounds. Main contributions: 1. A loss correction method that works well with instance-dependent noise 2. Highlighting the efficient IDN model (one value per instance instead of a matrix in ILDN) 3. Different ways to estimate $\\rho_x$, like distance-based, clustering, and ensemble methods 4. Testing in different areas (image, audio, tabular) and with different learners (CNNs, MLPs, GBMs) The tests show that this method works as well as or better than 12 other methods on CIFAR-10, and 5 tabular datasets with moderate (28%) and high (44%) noise levels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper connects theory and practice by creating a loss correction method (Proposition 1) and a practical version with guarantees (Proposition 2), even with some limits. \n\n\n\n2. Estimating one flip probability per instance instead of a full matrix is a big practical benefit. The paper argues well for when this simpler model works, challenging the trend of more complex ILDN methods. \n\n\n\n3. Using $|h(x)|$ as a stand-in for distance to decision boundary is smart and costs nothing extra (already done in forward pass). Though not fully explored theoretically, tests (like the matched-noise experiment in Fig. 1) show it works well. \n\n\n\n4. Applied to CNNs, MLPs, and gradient-boosted trees - Tested in different areas (vision, audio, tabular) - Consistently good performance (often top-2) across 10 datasets and different noise levels - Stability across datasets (noted in Section 4.2) is very useful for users \n\n\n\n5. Two noise levels (28%, 44%) for moderate and high noise - Three trials per setup - Compared with 12 baseline methods with different noise assumptions - Study (Appendix A.3) explains joint vs. separate probability estimation - Analysis (Section 3.2.3) gives insight into the changed loss behavior - Code (Appendix A.8) shows it's easy to use (~20 lines) - Hyperparameter analysis (Appendix A.2) shows it's robust"}, "weaknesses": {"value": "1. The paper talks about using the same function $h(x)$ to estimate both $\\mathbb{P}(Y|x)$ and $\\mathbb{P}(\\tilde{Y}|x)$ (see Section 3.2.1). This is based on warm-up epochs, not detailed analysis. When is this estimation accurate? How much does the performance depend on the length of the warm-up? The paper says 4 epochs but does not test this. - The idea that $|h(x)|$ shows the true distance to a boundary is not backed by theory. It works in practice, but we don't know when it is accurate. For very complex boundaries or early in training, it might not work.\n\n2. Proposition 2's conditions on $\\lambda$ depend on dataset-specific factors like $\\Delta_S$ and norms, which are hard to check. The paper uses grid search but doesn't explain how to choose search ranges or give any tips. - There is no advice on which $\\phi$ (probability transformation) or $z$ (difficulty metric) to use for new areas. The paper shows that distance-based + $\\beta$-logistic works well but doesn't explain when other methods (like clustering or autoencoders) might be better. - The warm-up period of 4 epochs seems random, with no explanation or analysis of its impact.\n\n\n\n3. The main experiments (Section 4.2) use ILDN noise injection, not IDN noise, as per Xia et al. 2020. This checks how well the model handles mismatches but doesn't prove that real-world noise is like IDN. - The matched-noise experiment (Section 4.1) is artificial and repetitive—noise is added and then removed using the same model. - There is no real-world data study to see if IDN is enough compared to ILDN. Are there areas where ignoring label dependence affects performance? - The statement that IDN is \"not less powerful than ILDN\" (Abstract) lacks strong support. Tables 1-3 show mixed results when compared to ILDN methods (PTD, BLTM).\n\n\n\n4. Authors recognize this issue, but it limits how useful the method is. There is no plan for extending it to multiple classes, even though it seems like a logical next step. - Small dataset size: Using parts of CIFAR-10 and small tables doesn't show if it works on large datasets. How does it handle big data like ImageNet or millions of examples? - Fairness issues with baseline methods - Some methods for multiple classes (GCE, DMI) might not work well - DivideMix was left out because it didn't perform well, but it's unclear why (was it a problem with how it was done or a limit of the method for two-class tasks?) - Using different optimizers for some methods (like Adam for Coteaching+) makes it hard to compare them.\n\n5. There is no explanation of when $\\mathbb{E}_X[\\rho_x] &lt; 0.5$ is true. There is no talk about how complex the calculations are or how long they take. There is little error analysis: what if $\\rho_x$ is estimated wrong? Sensitivity analysis is only done for hyperparameters $\\lambda, \\beta$, not for the noise model itself. There is no study of when the method fails or does worse than other methods.\n\n6. Equation 4 is hard to understand and needs more steps to explain it. The main difference (IDN vs ILDN) is not highlighted early enough. Some important details (like warm-up and approximations) are mentioned briefly without explanation."}, "questions": {"value": "1. (Section 3.2.1): - You mention that $h(x)$ models both $\\mathbb{P}(Y|x)$ and $\\mathbb{P}(\\tilde{Y}|x)$. How can one function represent both clean and noisy label distributions? - Can you give a theoretical or practical analysis of when this approximation works? - What if you use the separate model for the whole training, not just the warm-up? Appendix A.3 shows worse results, but the separate model used a fixed $\\mathbb{P}(\\tilde{Y}|x)$—what if both networks are updated together?\n\n\n\n2. Can you explain when $|h(x)|$ is a good way to measure distance to the decision boundary? - In neural networks with ReLU activations, the decision boundary can be made of straight-line segments. Does your method still work well in areas far from the training data? - Have you thought about using real distance calculations (like DeepFool) for some samples to check how good the approximation is during training?\n\n\n\n3. Why is the 4-epoch warm-up chosen? Does it affect performance? - Can you show results for 0, 2, 4, 8, and 16 warm-up epochs? - Is there a method to decide the warm-up length, like using the clean sample ratio or when $h$ converges?\n\n\n\n4. Your main experiments use ILDN noise injection. Can you also test with true IDN noise to check the model's assumptions? - When would IDN not be enough, and ILDN be needed? Can you describe the task or data features that decide this? - Tables 1-3 show PTD and BLTM (ILDN methods) sometimes do better than NDX. Does this mean ILDN is better for these cases?\n\n\n\n5. The conditions in Proposition 2 are hard to check in real situations. Can you give practical tips or simple methods for setting $\\lambda$ other than grid search? - How does the best $\\lambda$ depend on dataset features like size, noise, and dimension? - Have you seen cases where the denominator in Eq. 3 gets close to zero during training even with regularization?\n\n6. How does the method work with larger datasets like full ImageNet and more complex problems? - Can you explain how to extend it to multi-class classification? Is it as simple as one-vs-rest, or are there major challenges? - For the tabular LightGBM results, how does the method work with GBM's way of handling noisy data?\n\n7. Are there times when NDX does much worse than the usual methods? What are these situations like? - What happens to performance when the noise model is very wrong (for example, the real noise depends on the class, but you think it is IDN)?\n\n8. In Section 3.2.3, you show that $\\tilde{\\ell}(\\cdot, \\tilde{y})$ becomes $\\ell(\\cdot, -\\tilde{y})$ when $\\tilde{y} \\cdot h(x)$ goes to negative infinity. This means the model trusts itself more than the label. Could this cause the model to repeat its mistakes? How does this compare to methods that reduce the importance of doubtful samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "68WeOiiskr", "forum": "tuvkrivvbG", "replyto": "tuvkrivvbG", "signatures": ["ICLR.cc/2026/Conference/Submission25088/Reviewer_Hbdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25088/Reviewer_Hbdw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810793356, "cdate": 1761810793356, "tmdate": 1762943321078, "mdate": 1762943321078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the label noise problem in binary classification. The authors revisit the **Instance-Only Dependent Noise (IDN)** model, a rarely explored framework where the label corruption depends solely on the instance features. The authors also propose an instance-aware loss correction method based on risk equivalence. At its core, the method models a per-instance noise rate $\\rho_x = P(Y \\neq \\tilde{Y} \\mid X=x)$ instead of a full transition matrix. Several ways for the estimation of transition rates are proposed. Experiments on diverse datasets (image, audio, and tabular) are conducted to verify the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces an unbiased risk that incorporates the per-instance noise rate $\\rho_x = P(Y \\neq \\tilde{Y} \\mid X = x)$.\n2. The authors conducted experiments on several datasets to verify the proposed method."}, "weaknesses": {"value": "1. The proposed method is currently restricted to binary classification, and its extension to multi-class settings remains unexplored. This limits the applicability of the approach to broader real-world scenarios.\n2. The paper does not include experiments on large-scale real-world noisy datasets (e.g., WebVision or Clothing1M). Evaluating on such datasets would provide stronger evidence of the method's scalability and practical robustness.\n\n3. The estimation of the per-instance noise rate $\\rho_x$ relies on heuristic proxies (e.g., distance to the decision boundary). The paper does not provide an in-depth analysis of how the choice of proxy or mapping function affects performance. Different proxies may exhibit different levels of robustness.\n\n4. The paper lacks a clear discussion of its relationship with prior loss-correction methods, such as Natarajan et al. (2013). A more explicit theoretical analysis or empirical comparison would help clarify the novelty and contribution of the paper."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HA4EaJH8LB", "forum": "tuvkrivvbG", "replyto": "tuvkrivvbG", "signatures": ["ICLR.cc/2026/Conference/Submission25088/Reviewer_joei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25088/Reviewer_joei"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905014152, "cdate": 1761905014152, "tmdate": 1762943320839, "mdate": 1762943320839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses learning under label noise in supervised binary classification by introducing a risk-equivalence-based loss correction that ensures robustness when label flips depend only on the input instance. The method derives a corrected loss so that minimizing the empirical noisy risk aligns with minimizing the true clean risk for any classification-calibrated base loss (e.g., logistic).\n\nThe authors propose efficient ways to estimate the per-instance noise rate $\\eta_x$\t​without requiring matrix-valued transition models, extending CCN and ILDN approaches with lower complexity. Experiments on image, audio, and tabular datasets show strong robustness to moderate and high noise, matching or exceeding prior methods.\n\nOverall, the paper offers a principled and efficient framework for instance-dependent noise modeling. While conceptually incremental, it combines theoretical rigor and empirical breadth to provide a valuable step toward scalable, noise-tolerant learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a rigorous derivation of a corrected loss guaranteeing risk equivalence between noisy and clean risks, with solid theoretical grounding (Propositions 1, 2; Lemma 1).\n- Introduces a stable regularized variant with Lipschitz and generalization-bound analysis.\n- Revives the under-studied **instance-only dependent noise (IDN)** model, offering a simpler yet effective alternative to ILDN.\n- Proposes an elegant, computationally efficient estimation of $ \\rho_x$ using $ |h(x)| $ as a difficulty proxy, avoiding complex multi-stage procedures.\n- Demonstrates machine-agnostic applicability across neural networks and gradient-boosted trees.\n- Achieves strong and stable performance across image, audio, and tabular datasets, outperforming or matching competitive baselines under moderate and high noise."}, "weaknesses": {"value": "- Theoretical contributions mainly extend prior loss-correction frameworks (e.g., Natarajan et al., Patrini et al.), with novelty centered on the instance-only dependency rather than new statistical insights.\n- The warm-up assumption for accurate noise estimation is heuristic and may break under severe noise.\n- The formulation $\\rho_x = f(|h(x)|$ is empirically motivated without strong theoretical grounding.\n- Experiments are limited to binary classification; multi-class extension is left for future work.\n- Evaluation relies on relatively small or subsampled datasets, lacking results on large-scale real-world noise (e.g., WebVision).\n- The drastic accuracy drop below 50% in Figure 1 suggests potential tuning or early-stopping issues in baseline comparisons"}, "questions": {"value": "- Can the proposed framework be extended to multi-class classification tasks?\n- How sensitive is the method to the duration of the warm-up stage used for estimating ($P(Y|x)$?\n- How does the estimation of $\\eta_x$ influence performance when no label noise is present? Including results for a 0% noise setting (e.g., in Table 1) would clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "neZH5OARcp", "forum": "tuvkrivvbG", "replyto": "tuvkrivvbG", "signatures": ["ICLR.cc/2026/Conference/Submission25088/Reviewer_nFiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25088/Reviewer_nFiM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969690860, "cdate": 1761969690860, "tmdate": 1762943320601, "mdate": 1762943320601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles label noise in binary classification by reviving the instance-only dependent (IDN) noise model, where label corruption depends solely on the input instance. The authors derive a theoretically consistent loss correction scheme ensuring risk equivalence between noisy and clean risks, and stabilize it via a regularized variant with generalization guarantees. The instance-level noise rate ρₓ is efficiently estimated from model confidence (|h(x)|), enabling online learning. Experiments on image, audio, and tabular datasets show strong robustness under moderate-to-high noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers a solid theoretical foundation: Proposition 1 rigorously establishes risk equivalence, while Lemma 1 and Proposition 2 ensure stability and generalization.\n\nExperiments across image, audio, and tabular domains with diverse models (CNN, MLP, LightGBM) confirm robustness and broad applicability."}, "weaknesses": {"value": "Justification for using P(Y|X)=P(Y'|X) : This is a critical step in the method that needs justification. Without proper justification, it seems that there is a significant gap between the analysis and the actual proposed method."}, "questions": {"value": "Have you tried to training two model approach, one for P(Y|X) and one for rho_x ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "drreYT5atY", "forum": "tuvkrivvbG", "replyto": "tuvkrivvbG", "signatures": ["ICLR.cc/2026/Conference/Submission25088/Reviewer_5k6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25088/Reviewer_5k6V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25088/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182481308, "cdate": 1762182481308, "tmdate": 1762943320328, "mdate": 1762943320328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}