{"id": "0cbUKCyBsH", "number": 11179, "cdate": 1758192238422, "mdate": 1759897602601, "content": {"title": "Influence-Aware Forecasting: Breaking the Self-Stimulation Barrier in Time Series", "abstract": "The field of time series forecasting faces a critical performance plateau, where even billion-parameter foundation models struggle to outperform simple linear baselines. We argue this stagnation stems not from model architecture but from a universally adopted yet flawed 'self-stimulation' assumption, where models ignore the external influences that drive real-world systems by predicting the future using only the historical values of time series. Through a control-theoretic lens, we formally prove that this assumption imposes a hard, mathematical barrier on forecasting accuracy. To break this barrier, we introduce Influence-Aware Time Series Forecasting (IATSF), a new paradigm that reframes the task from correlation-based inference to dynamic system modeling. To operationalize this paradigm, we provide two foundational contributions. First, we introduce a leak-free, temporally-synced benchmark—a critical resource for the community—that incorporates textual influences to capture the qualitative or uncertain dynamics missed by traditional variables. Second, we develop FIATS, a lightweight, principled model engineered to interpret these influences. Its novel channel-aware mechanisms allow it to adjust its sensitivity to both textual signals and historical data in a channel-specific manner. Our results demonstrate that explicitly modeling external influences is not just an incremental improvement but the primary path forward for meaningful progress in time series forecasting.", "tldr": "", "keywords": ["Time Series Forecasting", "Multimodal Forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33d428801b9d7185a4a9a01226b609432f3c4689.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Paper attributes the performance bottleneck of TSF to the “self-excitation” assumption and introduces the IATSF paradigm along with the lightweight model FIATS. By incorporating external influences such as text as conditional inputs, it seeks to lower the theoretical lower bound of prediction error. On the theoretical side, it establishes a lower bound on error when external factors are ignored and proves that “partial influence can also tighten the bound.” On the engineering side, it builds a “time-synchronized, leakage-free” benchmark and implements channel sensitivity and conditional decoding via CASM/CAPS. Experiments on synthetic, meteorological, traffic, and market datasets report improvements over common baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Paper explicitly identify how the “self-stimulation” assumption induces averaging effects and imposes an intuitive lower bound on error, and paper provide a precise expression for this bound. \n\nThe engineering design and lightweight fusion modules (CASM/CAPS) achieve strong results on selected datasets and exhibit a degree of interpretability."}, "weaknesses": {"value": "The selection of exogenous inputs lacks formal criteria; choices are overly subjective and do not employ statistical or causal tests to determine whether an influence is relevant or actionable. There is no assessment of ex ante testability or ex post consistency, making the approach susceptible to selection bias.\n\nThe scope is narrow, being effective primarily for datasets that are directly and strongly driven by external factors; no compelling evidence is provided for weakly correlated settings or systems with complex feedback.\n\nThe chosen exogenous inputs are uniformly idealized, without accounting for the negative impacts of irrelevant external inputs.\n\nEven on the most favorable dataset, the MSE is only 0.003 lower than the second-best result, and no standard deviations are reported, casting doubt on the stability of the findings."}, "questions": {"value": "What is FIATS’s worst-case behavior when external influences are weakly correlated or irrelevant? Can it at least match the performance of a model without external influences?\n\nIs there a principled framework—statistical, causal, or otherwise—that offers theoretical guidance for selecting which external influences to include?\n\nCan the theory be extended to accommodate weakly correlated external influences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EMc1YjUyQa", "forum": "0cbUKCyBsH", "replyto": "0cbUKCyBsH", "signatures": ["ICLR.cc/2026/Conference/Submission11179/Reviewer_BWDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11179/Reviewer_BWDJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761357138220, "cdate": 1761357138220, "tmdate": 1762922335483, "mdate": 1762922335483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents a very interesting study focused on improving time series prediction.\nThe primary motivation for this manuscript is that traditional approaches are based on the \"self-stimulation\" assumption, i.e., predictions are made solely based on previous observations.\nThis manuscript provides two main contributions: (i) a temporally-synced benchmark that incorporates textual influences to capture the qualitative or uncertain dynamics missed by traditional variables; (ii) FIATS, a lightweight, principled model\nengineered to interpret these influences."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main positive aspect of this proposal lies in combining LLM to add new information and improve the performance of predictors. This trend has been observed in several top-tier conferences. As discussed in the manuscript, there are important related works addressing this problem.The evaluation of contributions in time series collected from different sources and with varying complexities is also a positive aspect."}, "weaknesses": {"value": "- The manuscript cites a very important related work: \"Context is key: A benchmark for forecasting with essential textual information\" (Williams et al., 2025). However, this paper should be used as a benchmark.\n- Still related to this work, and others related, the manuscript says: \"the approaches often lack a clear theoretical justification for how influences should be modeled\". However, the theoretical justification presented by the authors has two main issues: mostly based on two propositions; and the covariance error bound does not justify the paper's hypothesis.\n- Experiments only using MSE allow for monitoring just a single type of error. No causal relation was measured, for example.\n- The manuscript proposes the inclusion of new information to improve predictions, but only univariate models without considering, for example, other multimodal models. \n- Another important missing discussion is about the variation of horizon predictions."}, "questions": {"value": "- Why were the most related works not evaluated as benchmarks? For example, (Williams et al., 2025). The comparison presented in the manuscript was performed only considering univariate models. Explain why multivariate and multimodal baselines were excluded, or include them to ensure a fair assessment.\n\n- The main motivation must be more precise. The idea of combining LLM and time series is not new. Therefore, the paper should articulate concrete problems with the current approach and how to solve them. \n- Still about the previous comment, the justification of \"lack a clear theoretical justification\" is not properly addressed by the authors.\n- Considering the sentence \"Proposition 3.1 demonstrates that any measurable influence information reduces forecasting uncertainty, even with incomplete influence knowledge. This motivates our key insight: textual descriptions\nof influences provide viable information for uncertainty reduction, despite non-numeric formats.\" is not addressing the authors' motivation about the lack of a clear theoretical justification. At least, it is not enough to show general variation of the covariance error bound suggested in the manuscript.\n- Please, consider using other regression metrics to better understand the prediction errors.\n- Discuss how the proposal is impacted by short- and long-term predictions.\n- Equations must be revisited to avoid imprecision of definitions: In Line 259, should the last term be $U_f$, considering the equation in Line 105? What's k in Line 306? Same as in Line 313? Was $B_{U_f}$ in Line 270 correctly defined as in Line 259?\n- In Section 6.1, the authors mention \"the limitationS of self-stimulation\". Clarify them, once I've just noticed one limitation.\n- All quotation marks in latex are wrong, especially the single ones."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jvfh57aqZ5", "forum": "0cbUKCyBsH", "replyto": "0cbUKCyBsH", "signatures": ["ICLR.cc/2026/Conference/Submission11179/Reviewer_bRBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11179/Reviewer_bRBF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881598963, "cdate": 1761881598963, "tmdate": 1762922334830, "mdate": 1762922334830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes time-series forecasting from “self-stimulation” (history-only) to Influence-Aware TSF (IATSF), pairing future-relevant external influences with a leak-controlled benchmark and a lightweight model (FIATS) that uses CASM/CAPS for channel-aware modulation. Experiments across synthetic, physics, market, and traffic data show consistent gains, with ablations indicating the improvements come from the influence inputs and channel descriptions rather than model size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem framing and theory: motivates the error floor of history-only forecasting and why conditioning on influences helps.\n- Time-aligned influences to avoid future information leakage.\n-  FIATS with CASM/CAPS offers principled channel-aware conditioning without heavyweight models.\n- Consistent empirical gains: improvements on diverse datasets and horizons, also the interpretable attention surfaces add insight."}, "weaknesses": {"value": "- Most baselines are classic self-stimulated TSF models. It isn’t shown that they’re given equivalent exogenous inputs (e.g., weather forecasts / channel descriptions) or simple text adapters, so some gains may come from extra information rather than architecture. Please add baselines that receive the same influence features (PatchTST + forecast covariates, or text-embedding concatenation), and exogenous-aware TSF baselines. If FIATS is the only model recieving more information that other models, it is not surprising that it performs better than others\n- The model adds CASM/CAPS blocks and a token-wise decoder. It would help to report parameter counts or FLOPs and compare with othe models to get a better sense of performance in terms of model size.\n- IATSF assumes influences are independent of the system state and take effect instantaneously. Real systems (traffic, power demand, weather-to-physics) often have lagged and endogenous effects. The main text does not quantify sensitivity to these violations.\n- Evaluation is mostly MSE. Many TSF applications also use MAE, MAPE, and SMAPE, etc.\n- The paper itself shows that when trained on good text but tested on noisy/misleading text, performance degrades notably—suggesting susceptibility to imperfect inputs that are common in practice. Therefore, it would be limited to only test on synthetic noises. Would benefit from quantifying robustness on real forecast and text errors."}, "questions": {"value": "- Are the text embeddings frozen? What did you do to bridge the domain adaptation? Given OpenAI embeddings appear strongest in Table 3, can you ensure results are reproducible with open models only?\n- It would be good to provide per-channel and per-series metrics to verify that gains aren’t concentrated in a few channels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wQ3Vi3lDAJ", "forum": "0cbUKCyBsH", "replyto": "0cbUKCyBsH", "signatures": ["ICLR.cc/2026/Conference/Submission11179/Reviewer_7V6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11179/Reviewer_7V6c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901189338, "cdate": 1761901189338, "tmdate": 1762922334470, "mdate": 1762922334470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an \"Influence-Aware Forecasting\" framework aimed at breaking through the long-standing limitations of the \"self-exciting\" assumption in time series forecasting. From the perspective of control theory, the authors point out that traditional models overlook external influences, leading to bottlenecks in predictive performance. They introduce a new approach that integrates external intervention information (such as text, images, events, etc.). Experimental results demonstrate that this method significantly outperforms existing mainstream models on multiple real-world datasets, particularly showing greater robustness when dealing with unexpected events or external interventions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Theoretical Innovation: For the first time, this work analyzes the structural bottlenecks in time series forecasting from the perspective of control theory, offering insightful perspectives.\n2.Challenging Mainstream Assumptions: It breaks the \"self-exciting\" paradigm, opening up new directions for time series research."}, "weaknesses": {"value": "1. The details of external influence modeling are somewhat vague: the paper does not elaborate on how to select, process, and integrate multimodal external information, which may affect reproducibility.\n2. Computational costs are not sufficiently discussed: introducing external data could significantly increase training and inference costs, yet the paper does not evaluate its resource consumption.\n3. The choice of comparison methods is somewhat conservative: in some experiments, the baselines used for comparison are relatively weak, and there is a lack of in-depth comparisons with the latest large models (such as TimePro, TimeMixer++, etc.).\n4. The author raises the issue of self-motivation and claims to be able to solve it. However, the text prompts are also part of the currently available information, which actually includes similar information in historical data. Therefore, adding text prompts does not effectively address the problem. This motivation is flawed."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2vO12Laxpx", "forum": "0cbUKCyBsH", "replyto": "0cbUKCyBsH", "signatures": ["ICLR.cc/2026/Conference/Submission11179/Reviewer_GPeT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11179/Reviewer_GPeT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11179/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904856763, "cdate": 1761904856763, "tmdate": 1762922333867, "mdate": 1762922333867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}