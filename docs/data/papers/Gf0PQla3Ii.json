{"id": "Gf0PQla3Ii", "number": 15359, "cdate": 1758250572733, "mdate": 1759897311778, "content": {"title": "When Symbols Speak: Understanding Logo Triggered Texts in Vision-Language Models", "abstract": "Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning, yet they remain vulnerable to hallucinations where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbols, hybrids, and text-bearing logos, as well as the challenging Hard-60 subset, we systematically measure hallucination across leading VLMs. We further probe robustness through nine structured perturbations and show that hallucinations persist even under strong distortions, with occlusion exposing the sharpest weakness. Embedding-level analysis with open-weight LLaVA demonstrates that hallucination is tied to a small subset of projector dimensions, and targeted ablation substantially reduces errors while preserving OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic priors rather than genuine glyph perception, particularly for iconic circular logos, and that projector subspaces play a decisive role in this failure mode. Our work contributes both a novel diagnostic lens and actionable mitigation insights, highlighting projector disentanglement and OCR-guided decoding as promising directions for building more trustworthy multimodal systems.", "tldr": "VLMs hallucinate brand names for text-free logos; we show this bias stems from projector embeddings and propose diagnostics pointing to disentangled projector design as a solution.", "keywords": ["Vision-Language Models", "logo hallucination", "projector embeddings", "multimodal robustness", "text recognition"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4936f018e590d108d807e74659de00cf33685df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces logo hallucination, the phenomenon where Vision-Language Models (VLMs) incorrectly generate brand names or textual outputs from logos that contain no actual text. They apply controlled perturbations and embedding-level diagnostics to demonstrate that hallucination persists under image distortions and originates from specific projector subspaces within the VLM architecture. Targeted ablation of a small number of projector dimensions reduces hallucination by ~30% with minimal OCR accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Combines taxonomy (text/symbol/hybrid logos), controlled perturbations, and embedding diagnostics in a reproducible and well-defined experimental pipeline.\n\n2. Clarity and presentation: The figures, tables, and structure (bias, perturbation, projector) make the argument cohesive and empirically grounded."}, "weaknesses": {"value": "1. The study is constrained to logo datasets. While logos are a clean diagnostic, it’s unclear whether findings generalize to natural images or scene text. This restriction limits the paper’s general significance.\n\n2. The work is primarily diagnostic rather than methodological. It identifies a failure mode (logo hallucination) and analyzes it well, but does not propose or validate a concrete new training or architectural solution. The projector ablation experiment is an insightful analysis tool, not a deployable method.\n\n3. The link between projector dimensions and hallucination is correlational; ablation reduces the symptom but does not establish a principled mechanism or causal model.\n\n4. The discussion on emotional/value hallucination (luxury, elegance) is anecdotal—no quantification or modeling of this axis."}, "questions": {"value": "Does logo hallucination persist in other symbolic domains (flags, icons, traffic signs)? Could this be a broader symbolic-text entanglement issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Yq2FW5Qc89", "forum": "Gf0PQla3Ii", "replyto": "Gf0PQla3Ii", "signatures": ["ICLR.cc/2026/Conference/Submission15359/Reviewer_QuUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15359/Reviewer_QuUv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761467289118, "cdate": 1761467289118, "tmdate": 1762925644731, "mdate": 1762925644731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies \"logo hallucination\" in VLMs — where models generate brand names from purely symbolic logos. Through careful experiments across logo types, image perturbations, and projector analysis, the authors show this issue is widespread, robust to input changes, and linked to specific embedding directions. The work offers both a new diagnostic perspective and a practical mitigation path."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Logo hallucination is a new and relevant failure mode that hasn't been systematically studied before.\n\nThe three-stage framework provides a rigorous, systematic, and multi-faceted investigation into the phenomenon."}, "weaknesses": {"value": "- The paper positions \"logo hallucination\" as a new and overlooked phenomenon. However, the tendency for models to hallucinate highly correlated labels for salient visual concepts is a well-known issue (e.g., \"object hallucination\"). The paper would be strengthened by a more explicit discussion of how the demonstrated \"logo hallucination\" constitutes a meaningfully distinct failure mode, rather than simply being a specific manifestation of the broader object hallucination problem.\n\n- Insufficient Detail on the Hard-60 Subset: More detailed statistics and examples in the main text would help readers understand its composition and the nature of its challenge.\n\n- The current separation of the comprehensive methodology (Section 3) from the experimental results (Section 4) makes the paper occasionally hard to follow. Perhaps restructuring the sections to present each experimental phase alongside its immediate results would improve the narrative flow and readability."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wA6SEfDtQi", "forum": "Gf0PQla3Ii", "replyto": "Gf0PQla3Ii", "signatures": ["ICLR.cc/2026/Conference/Submission15359/Reviewer_t4Mq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15359/Reviewer_t4Mq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667538900, "cdate": 1761667538900, "tmdate": 1762925644322, "mdate": 1762925644322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the phenomenon of logo hallucination in Vision-Language Models (VLMs). The authors discover that VLMs generate brand names even when logos contain no visible text, suggesting that models rely on symbolic priors rather than genuine visual recognition. The study systematically examines this phenomenon, revealing logo hallucination behavior across different models, perturbations, and scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Logo hallucination is an overlooked yet significant security risk in VLMs. This problem has practical implications for applications such as brand impersonation detection and fraud prevention.\n\n2. Experiments are conducted across multiple mainstream VLMs to validate the generalizability of the findings."}, "weaknesses": {"value": "1. Insufficient evidence for causal attribution.\n- The paper claims that hallucinations stem from \"symbolic priors in token embeddings\" rather than visual feature extraction problems, but the evidence is insufficient to support this causal inference.\n- The paper's logic: Ablating k=32 key dimensions → reduced hallucination rate → concludes these dimensions contain \"symbolic priors\" → proves it's a token embedding problem. However, this finding can equally be explained as a visual feature representation issue.\n\n2. Lack of mechanistic explanation. While the paper states that logic hallucination stem from \"symbolic priors rather than genuine glyph perception,\" it does not explain how these priors form during training\n\n3. Limited experimental scale. Hard-60 contains only 60 logo samples, potentially insufficient to support strong generalization claims."}, "questions": {"value": "1. Experiments primarily use English brand names. Do logos with text in other languages (e.g., Chinese, Japanese signage) produce similar hallucinations?\n\n2. How was k=32 determined? Was grid search or cross-validation performed? How would different k values (e.g., k=16 or k=64) affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RYgjkBI54x", "forum": "Gf0PQla3Ii", "replyto": "Gf0PQla3Ii", "signatures": ["ICLR.cc/2026/Conference/Submission15359/Reviewer_G9zL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15359/Reviewer_G9zL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056407811, "cdate": 1762056407811, "tmdate": 1762925643920, "mdate": 1762925643920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}