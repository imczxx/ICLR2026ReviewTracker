{"id": "fHsVNklKOc", "number": 8972, "cdate": 1758105113529, "mdate": 1759897750836, "content": {"title": "Talk, Evaluate, Diagnose: User-aware Agent Evaluation with Automated Error Analysis", "abstract": "Agent applications are increasingly adopted to automate workflows across diverse tasks. However, due to the heterogeneous domains they operate in, it is challenging to create a scalable evaluation framework. Prior work each employ their own methods to determine task success, such as database lookups, regex match, etc., adding complexity to the development of a unified agent evaluation approach. Moreover, they do not systematically account for the user’s role nor expertise in the interaction, providing incomplete insights into agent’s performance. We argue that effective agent evaluation goes beyond correctness alone, incorporating conversation quality, efficiency and systematic diagnosis of agent errors. To address this, we introduce the TED framework (Talk, Evaluate, Diagnose). (1) Talk: We leverage reusable, generic expert and non-expert user persona templates for user-agent interaction. (2) Evaluate: We adapt existing datasets by representing subgoals—such as tool signatures, and responses—as natural language grading notes, evaluated automatically with LLM-as-a-judge. We propose new metrics that capture both turn efficiency and intermediate progress of the agent complementing the user-aware setup. (3) Diagnose: We introduce an automated error analysis tool that analyzes the inconsistencies of the judge and agents, uncovering common errors, and providing actionable feedback for agent improvement. We show that our TED framework reveals new insights regarding agent performance across models and user expertise levels. We also demonstrate potential gains in agent performance of up to 9% on existing metrics and 5% on our proposed metrics after incorporating the identified error remedies into the agent’s design.", "tldr": "User-aware agent evaluation approach with automated error analysis", "keywords": ["agent evaluation", "metric", "LLM agents", "error analysis"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/936ce33ebd78f374a29b0da6630e34f4aded8697.pdf", "supplementary_material": "/attachment/ed883fe5f2a74c6643d8ef6a695525ef08820adf.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework for evaluating agents, referred to as the talk-evaluate-diagnose (TED) framework. The framework operates in three stages: talk is about collecting interaction data, evaluate is about automatic evaluation, and diagnose is about making errors human interpretable. As part of the evaluation phase the authors propose a MaxProgressRate@K metric that accounts for progress rather than just overall success (i.e., uses a more fine-grained reward). The authors demonstrate the use of their framework through examples. They observe, in one case, improvements of agent performance by 5%/9% (depending on metric) following changes based on error suggestions generated by the framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The general sentiment that superficial metrics are insufficient to allow practitioners to build better agents seems well justified.\n2. The proposed framework seems sensible, and follows common practices in building agent benchmarks (i.e. look at the data)."}, "weaknesses": {"value": "1. The writing of this paper could be clearer, parts of the paper can be difficult to follow. In particular the results section could benefit from clearer structuring, e.g. numbering and clearly separating individual insights/observations.\n2. The motivation for the combined three-part approach is unclear to me. Whilst each part on its own seems sensible enough it's unclear to me why the parts need to be introduced together. It's not clear that one part really depends on the other. This disconnect in motivation, in part, makes the paper difficult to follow. It's almost like reading three papers.\n   - Perhaps a paper just focusing on one of the aspects would be easier to follow, and would allow for a more rigorous analysis of that part.\n2. No code or data available at time of submission: since one of the contributions is a benchmark, this lack of code prevents me from fully assessing this part of the work.\n3. Limited evaluation of the tool itself. The reported gains (L461) seem anecdotal, based on improving a single agent based on a single use of their tool, effectively a single datapoint. It is unclear if these results generalise. Potential areas for improvement:\n\t- Evaluation could be improved by focusing more the number of errors found that are missed by other methods\n\t- I realise human studies can be very resource-intense, but a robust study of usage of TED vs competitor tools would be the most convincing.\n\nMinor (no impact on score, no need to respond):\n1. L70 onwards: Is this a proper enumerate environment?\n2. Figure 1: This figure is confusing to follow. Why do the error numbers descend (start 6 going down). How are the arrows to be interpreted? Is Error 5 and 6 on the same datapoint? What are the red rectangles supposed to mean?\n3. Figure 4: Where is error 4? It's in the list but not shown. Also the text is very small, maybe this could be condensed into something easier to read."}, "questions": {"value": "1. How come you decided against releasing the code at the time of submission?\n2. Am I missing an important aspect about the evaluation, that may convince me that there is more than anecdotal evidence of utility?\n3. What are the concrete reasons why the setup needs to be introduced in a single paper? How do steps 2 and 3 necessarily depend on Step 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lPfSLr4sVg", "forum": "fHsVNklKOc", "replyto": "fHsVNklKOc", "signatures": ["ICLR.cc/2026/Conference/Submission8972/Reviewer_EiT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8972/Reviewer_EiT3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823288047, "cdate": 1761823288047, "tmdate": 1762920705142, "mdate": 1762920705142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the challenge of agent evaluation automation, where explicit references are difficult to acquire and apply (due to the stochastic nature of agent responses). The authors critique existing evaluation frameworks for lacking both structural clarity—specifically, the conflation of task instructions with user personas—and deeper behavioral insights, relying mainly on surface-level metrics. To address these gaps, the authors propose the TED framework (Talk, Evaluate, Diagnose), which introduces structured user-aware simulations, natural language subgoal grading with LLM-as-a-judge, and automated error diagnosis. Experimental results suggest that the diagnostic feedback produced by TED can lead to measurable improvements in agent performance under the proposed evaluation metrics. While I find the motivation and direction promising, I have concerns regarding the soundness and presentation rigor of the proposed methodology, as detailed in the Weaknesses and Questions sections."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a timely and important challenge in evaluating conversational or task-oriented agents, where the stochastic and open-ended nature of responses makes it difficult to define reliable ground-truths. I particularly appreciate the exploration of subgoal decomposition and the use of grading notes, which offer a valuable and interpretable approach to fine-grained agent evaluation."}, "weaknesses": {"value": "My main concerns lie in the soundness and clarity of the paper. In particular, Section 3.2—one of the core components—raises questions about methodological soundness, which in turn affects my confidence in the validity of the experimental design. Beyond this, the motivations behind several key design choices are insufficiently explained, and multiple mathematical notations are either ambiguous or undefined. Further details are provided in the Questions/Concerns section."}, "questions": {"value": "1. For Sec.3.2, what do the authors care about multi-turn progression? Aren't the turns independent trials s.t. their orders do not matter? What exactly does the turn-level progression metric tell/reveal?\n2. Why is it necessary to separate user persona and task? If this is the assumption of this work, could the authors clarify the significance with experiments? Essentially answering what's the consequence of NOT decoupling them?\n3. Eqn 5: undefined $success_l$.\n4. The notation of progressive max $max^k_{l=1}$ is somewhat strange and confusing to me, what operation(s) are taken place there? I roughly guess it's somethihng like a \"cumulative max\", but is it exactly? I would advise the authors to clarity for certain, and try to use clearer notations.\n5. Eqn 6: $p(t)$ is contiuous yet for multi-step, steps, are discrete. How would one compute AUC, I'd guess interpolations, but the authors should state clearly what have been done in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qe9cNKD7Rb", "forum": "fHsVNklKOc", "replyto": "fHsVNklKOc", "signatures": ["ICLR.cc/2026/Conference/Submission8972/Reviewer_N12X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8972/Reviewer_N12X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934410853, "cdate": 1761934410853, "tmdate": 1762920704594, "mdate": 1762920704594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors look at the problem domain of agent evaluation. They argue that existing evaluation frameworks do not take into account a variety of users and that the metrics are too domain specific and are not flexible enough. Therefore the authors propose a framework called TED which they sample personas of \"expert\" and \"non-expert\" users and have a conversation with the agent where the agent has to achieve a series of sub-goals or trajectory. They evaluate is a sub-goal is completed by using LLM-as-a-Judge. Afterwards they have a suite of metrics to represent the trajectory in a numerical value which helps to better categorize if an agent is performing well or not. Additionally they have an automated error analysis method that uses the outputs from their LLM-as-a-Judge to cluster incorrect responses from the agent. They then show that one can come up with a prompt that addresses these errors to feed into the system."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1) The general method described is clear and the authors seem to take a principled approach to designing their metrics.\n\n2) It is interesting to see metrics like meanProg give more details into LLM performance when it comes to agents and uncovering those errors are important.\n\n3) It was good to see the authors use the errors discovered to improve the agents. This leaves future work on how to better incorporate the learnings from their error analysis."}, "weaknesses": {"value": "1) Both datasets are in the task-oriented domain. Given that these metrics seem to be domain agnostic it would be good to see how these can be translated to other domains.\n\n2) One thing that still isn't clear to me is that while I understand that these metrics can give more insight to an agent are we able to find some correlation between user experience and these metrics? When it comes to evaluation metrics most work show some correlation between the two."}, "questions": {"value": "Questions\n\n1) You mention that for ToolSandbox you exclude the variants. I understand that these can be simulated by your user proxy but do the variants offer more diverse conversational paths that can be leveraged?\n\nSuggestions\n\n1) For Figure 2 I recommend having a legend or mentioning which colored lines go with which model. It was hard to decipher at first.\n\n2) I suggest you give a brief description of the datasets before getting into detail.\n\n3) I may have missed it but I don't see what LLM you are using for grading. I would make it more clear as to which model you are using for different parts of your pipeline.\n\n4) I like that you have human studies to evaluate user proxy / grading notes. I know that it's in the Appendix but I would give a brief summary of your results in the main paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fdREbM4oh1", "forum": "fHsVNklKOc", "replyto": "fHsVNklKOc", "signatures": ["ICLR.cc/2026/Conference/Submission8972/Reviewer_gKqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8972/Reviewer_gKqD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945870397, "cdate": 1761945870397, "tmdate": 1762920704221, "mdate": 1762920704221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}