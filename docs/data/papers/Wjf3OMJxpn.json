{"id": "Wjf3OMJxpn", "number": 4059, "cdate": 1757593980290, "mdate": 1759898055332, "content": {"title": "Beyond Pass@ 1: Self-Play with Variational Problem Synthesis Sustains RLVR", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.", "tldr": "We propose an online Self-play with Variational Problem Synthesis strategy for RLVR training that iteratively leverages model responses to synthesize variational problems for augmentation.", "keywords": ["LLM Reasoning; Reinforcement Learning; Self-envolving"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc0b7a496f53e585268f21c74658925933c3256e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SVS (self-play with variational problem synthesis) to address the entropy–performance trade-off observed in the RLVR training of LLMs. SVS utilizes the policy’s correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This mechanism effectively mitigates the significant decline in policy entropy during the training phase and achieves performance gains on both challenging and common reasoning benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper deals with an important and relevant problem: the degradation of policy entropy during the RLVR training of LLMs. The proposed SVS method effectively mitigates this issue in the domain of mathematical reasoning. The experiments provide robust validation of the method's effectiveness, demonstrating clear performance gains.\n2. The experimental evaluation is thorough. The authors present extensive comparisons on both challenging and mainstream reasoning benchmarks. Notably, Table 3 provides a fair and direct comparison between the standard RLVR approach and the proposed SVS, clearly highlighting the SVS's advantages.\n3. The paper is well-written and easy to read. The proposed method is described in sufficient detail to facilitate reproducibility. This work represents a valuable contribution to the research community focused on RLVR."}, "weaknesses": {"value": "1. First, the validation criterion for synthetic problems in SVS is based on whether their average accuracy falls within a predefined range. While this heuristic is sensible, its correlation with the actual logical correctness of the synthesized problems needs further discussion. It is unclear whether these \"valid\" problems are genuinely logically sound or if they merely steer the model toward reproducing the original answer. This ambiguity requires further investigation. I believe introducing a component of human evaluation or manual review for these synthetic problems would significantly strengthen the persuasiveness of the experimental validation.\n\n2. Following the previous point, it is not surprising that training on synthetic problems mitigates entropy degradation. What is intriguing, however, is that SVS significantly boosts performance on challenging benchmarks. This raises the question of whether this performance gain is primarily an artifact of the experimental setup, which focuses on only math-related benchmarks. In mathematical reasoning, the solutions (e.g., step-by-step derivations) often contain sufficient, or even complete, information to reconstruct the original problem. This characteristic may not hold true for more general domains. It is important to investigate whether SVS can deliver consistent performance improvements in broader settings, such as code generation, puzzles, or other complex RL scenarios.\n\n3. The problem synthesis task in SVS introduces non-trivial computational overhead. The paper currently lacks a discussion of this additional cost. An analysis of this computational cost(e.g., increased training time or resource utilization) would be helpful for the evaluation of the application value of SVS."}, "questions": {"value": "1. Can the authors provide some analysis on the synthetic problems? \n2. Can SVS be applied to domains aside from math?  To be clear, I am not referring to generalization experiments, but rather to the direct application of SVS-based RLVR training within those domains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cF8wlYbkFT", "forum": "Wjf3OMJxpn", "replyto": "Wjf3OMJxpn", "signatures": ["ICLR.cc/2026/Conference/Submission4059/Reviewer_zAc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4059/Reviewer_zAc9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552196175, "cdate": 1761552196175, "tmdate": 1762917157746, "mdate": 1762917157746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to mitigate the problem of entropy collapse in RL and proposes a solution from the training data perspective. Specifically, the authors use problems with correct solutions under the current policy as a leverage, and diversify the distribution of the training prompts via variational synthesis, which they call Self-play with Variational problem Synthesis (SvS). During training, the current policy first performs rollout (like in any RLVR-flavored algorithm), and selectively augments underperforming problems given the correct responses as context (still under the current policy), given the same-answer constraint. The pool of synthesized prompts are then included for RL training. Experimental results demonstrate stronger performance at Pass@1 and Pass@N, effective prevention of entropy collapse, at the scale of 3B to 32B models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea seems intuitive: variational problem synthesis widens the distribution of the training prompts, or can be considered as a way to \"perturb\" the inputs, hence improving both performance and robustness (i.e., Pass and Average metrics).\n2. The proposed method is simple to implement and can be scalable given sufficient rollout budget.\n3. The proposed approach addresses entropy collapse by design, in an implicit way (by diversifying prompt distribution rather than directly interfering training dynamics, which is cool). This particular way of producing synthetic problems also does not suffer from the \"common pitfalls\" of synthetic data given the strict problem selection mechanism."}, "weaknesses": {"value": "1. While the authors claim to demonstrate the effectiveness of the method on 12 reasoning benchmarks, they are mostly math benchmarks. I also noticed Appendix F.3 does include results from other reasoning domains (e.g., code, knowledge, etc.), but the improvements are marginal compared with those in the main paper, especially given that on some domains it becomes slightly worse. I do realize this might be ood w.r.t. training prompts, but a different question here is that: if we have training prompts from a different domain (or even mixed domains) like code, does it still work well?\n2. The authors only have vanilla RLVR as baseline (correct me if I'm missing something), even though there are already many other algorithm-based entropy collapse prevention method as of paper submission deadline. There is also one simple baseline missing: what would happen if we simply increase the rollout budget to be consistent with the proposed method?\n3. The proposed method further increase the inference overhead of RL, while missing details on how much this overhead is and if this overhead is worth paying considering that there are algorithms which claims to mitigate entropy collapse without additional inference cost.\n4. The method introduces additional hyperparameters like the difficulty window and decisions on which synthetic problems are rewarded and kept. To me this does not seem like an easy choice to make, or has to be made separately given domains of varying difficulty to the current policy."}, "questions": {"value": "1. In Table 2, we seem to miss numbers from the three AIME columns. Why?\n2. How sensitive is the proposed method to accuracy thresholds (during problem selection for augmentation) and how should they be tuned? (And the same question on how to consistently decide which problems to keep.)\n3. In Figure 7, is the y-axis incorrectly labeled as \"Policy Entropy\" while it actually represents response length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FsIsK5oAUy", "forum": "Wjf3OMJxpn", "replyto": "Wjf3OMJxpn", "signatures": ["ICLR.cc/2026/Conference/Submission4059/Reviewer_kCjs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4059/Reviewer_kCjs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928932740, "cdate": 1761928932740, "tmdate": 1762917157538, "mdate": 1762917157538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-play style strategy SvS to generate problem variants, which are not too easy or too hard (by model performance) and have the same answer to the original problem, to keep the diversity of prompts in RL training. It can keep better policy entropy during training compared with vanilla RLVR algorithms, and get better pass@k performance"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear and easy to follow\n2. Experiment results look solid, try different 8B/32B models, and see consistent improvement on policy entropy and pass@k/final performance on a lot of common benchmarks.\n3. This method does not need additional cost in labeling new data, and the generated problem can be prevented to degenerate to too simple cases (adding hints), result in better continue training performance.\n4. The method itself is RL-agnostic, so it's a relative general method"}, "weaknesses": {"value": "1. when using DAPO-17k, the training may overfit to int-style answer, and SvS perform worse on open-ended problems, like OlymE and OlymH. This shows that SvS may be limited to the model capability, and the distribution of provided seed data. So it's hard for some difficult problems or problems out of the distribution of model performance.\n2. Maybe need the base model to be strong enough to give some proper variants of the original problem, so more difficult for RLVR on small models.\n\n\nHowever, I think these problems are not that critical, and the results are still impressive to me."}, "questions": {"value": "1. What's the cost of generating problem variants? What's the filtering rate of generated problem variants? Would the cost increase as model performance increase because model learn more patterns from self-generated variants?\n2. Is it easy to generalize to other domains?\n3. Have you tried training for more steps? Would the acc have similar curve as original RLVR in fig 1 when training step is very large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7pFpP2g4T", "forum": "Wjf3OMJxpn", "replyto": "Wjf3OMJxpn", "signatures": ["ICLR.cc/2026/Conference/Submission4059/Reviewer_rsiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4059/Reviewer_rsiR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762421321232, "cdate": 1762421321232, "tmdate": 1762917157367, "mdate": 1762917157367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a procedure to iteratively synthesize problem that a RLVR pipeline solves, and leads to boosted performance on various test set. The synthetic process consists of a few steps, such as conditioning on failed problems to synthesize new tasks, filtering and resolving those problem variations. The methods generally sound intuitive and improve the baseline significantly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is interesting in that it provides another way to augment data for RLVR, leading to performance improvements. This affirms the knowledge that RLVR might benefit from synthetic data generation, and this helps push up even eval time performance."}, "weaknesses": {"value": "I think overall the idea of synthetic data generation is not novel for RLVR, the idea of self-play and synthesizing new problems with filtering and verifications have been proposed and discussed in the literature. All in all, it feels that the idea is less novel and insights limited from the current paper."}, "questions": {"value": "=== *main takeaway* ===\n\nI think a main feeling after reading the paper is that it introduces yet another way to synthesize and augment dataset in RLVR process, and despite the performance improvements, I do not feel having learned too much insight from the paper, in additional to knowing yet another set of process to augment data for RLVR.\n\n=== *why certain filtering steps* ===\n\nI think a useful question worth answering is why we'd filter in one way vs another and it's useful to show why filtering the synthetic data according to the current criterion lead to performance vs. a more naive approach. I can see that we iterate a few times on the method while developing the paper and it's useful to understand and present initial failure cases and specific training comparison.\n\n=== *baseline* ===\n\nI think the paper lacks comparison to a few baselines. One is the prior work in the space on synthetic data, e.g., a few ideas related to synthesizing data from the initial training set has been proposed in [1], but maybe we should cite this paper + make proper comparisons, otherwise we do not see additional improvements over existing idea. In flagship results in table 2, such comparisons are warranted.\n\nAnother point is that plots like Fig 6, it seems that the x-axis measures the raw training samples experienced in the original dataset - however, the baseline leverages much more additional compute to arrive at a better performance, due to extra filtering etc. It will be useful to account for such additional compute when making such comparisons otherwise it is unfair to the baseline, one way is to compare against e.g. test time compute + baseline method to see if it's possible to make up for the performance gains, or increasing the number of parallel generation (group size) for baseline GRPO algorithm.\n\n[1] https://arxiv.org/abs/2505.03335\n\n=== *pass@1* ===\n\nWhile the paper motivates the writing with pass@1 metric, I don't see the connection between extra data augmentation with pass@1, after all we seek to augment data for which the model finds hard and try to patch such loopholes. It is not immediately clear to me why data augmentation addresses pass@1 issue at test time, it feels more like a generalization issue rather than an issue with metric (pass@1 vs. pass@32)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMK7VwOjmN", "forum": "Wjf3OMJxpn", "replyto": "Wjf3OMJxpn", "signatures": ["ICLR.cc/2026/Conference/Submission4059/Reviewer_yXhe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4059/Reviewer_yXhe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762600710563, "cdate": 1762600710563, "tmdate": 1762917156940, "mdate": 1762917156940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called Self-play with Variational problem Synthesis (SVS) to address the issue of policy entropy collapse in Reinforcement Learning with Verifiable Rewards (RLVR) for LLMs. The core idea is a self-play strategy where the model synthesizes new, variational problems derived from its own correct solutions. This creates a self-improving loop that maintains policy diversity during training, leading to sustained and significant improvements in Pass@k performance, which is a key limitation of standard RLVR approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple yet Effective: The proposed method is intuitive, easy to understand, and demonstrates significant performance gains.\n2. Self-Contained: The approach does not rely on external models or datasets for data augmentation. It cleverly leverages the capabilities of the policy model itself to generate new training instances, which is an elegant design."}, "weaknesses": {"value": "1. The synthesis of variational problems introduces additional computational overhead. The paper does not provide a clear analysis of this cost in terms of training time or resource usage.\n2. Despite mentioning SwS [1] in the related work, the paper lacks a direct experimental comparison with this very similar method. Additionally, other relevant works using data augmentation to mitigate entropy collapse, like CURE [2], are not discussed or compared."}, "questions": {"value": "1. How do you think SVS would perform if combined with other methods that explicitly regulate entropy, such as those proposed in [3, 4]? Could these approaches be complementary?\n2. Could you elaborate on the advantages of SVS over other data augmentation methods for preventing entropy collapse, like SwS [1] and CURE [2]? Since experimental comparisons are absent, a conceptual discussion would be valuable. Is it feasible to combine SVS with these approaches?\n\n[1] Liang, Xiao, et al. \"SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning.\" arXiv preprint arXiv:2506.08989 (2025).\n\n[2] Li, Qingbin, et al. \"Cure: Critical-token-guided re-concatenation for entropy-collapse prevention.\" arXiv preprint arXiv:2508.11016 (2025).\n\n[3] Cui, Ganqu, et al. \"The entropy mechanism of reinforcement learning for reasoning language models.\" arXiv preprint arXiv:2505.22617 (2025).\n\n[4] Cheng, Daixuan, et al. \"Reasoning with exploration: An entropy perspective.\" arXiv preprint arXiv:2506.14758 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fodhQ70Mo0", "forum": "Wjf3OMJxpn", "replyto": "Wjf3OMJxpn", "signatures": ["ICLR.cc/2026/Conference/Submission4059/Reviewer_Snxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4059/Reviewer_Snxh"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762853974274, "cdate": 1762853974274, "tmdate": 1762917156752, "mdate": 1762917156752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}