{"id": "CJHA6a77Yv", "number": 4809, "cdate": 1757770374200, "mdate": 1759898011853, "content": {"title": "Riemannian Fuzzy K-Means on Product Manifolds", "abstract": "In this paper, we address an open problem: how to perform fast clustering on product manifolds.} With the increasing interest in non-Euclidean data representations, clustering such data has become an important problem. However, a naive extension of the classic K-Means algorithm to product manifolds requires $\\mathcal{O}(\\nu \\omega)$ time, where $\\omega$ is the number of alternating iterations and $\\nu $ is the time complexity of each Riemannian optimization. Due to the need for numerous Riemannian optimizations, the naive Riemannian K-Means (NRK) is not suitable for large-scale data. To this end, we propose the Riemannian Fuzzy K-Means (RFK) algorithm for product manifolds, which reduces the time complexity to $\\mathcal{O}(\\nu )$. Importantly, RFK is not a straightforward extension of K-Means or Fuzzy K-Means to manifolds, it leverages the fuzzy relaxation to eliminate alternating updates and achieve a true single-loop optimization. Furthermore, we introduce Radan to accelerate the optimization of RFK. We conduct extensive experiments. RFK and Radan outperform across nearly all metrics in almost every dataset, reaching an impressive level of performance. \\textbf{RFK and Radan have been integrated into several non-Euclidean machine learning libraries, such as here.", "tldr": "", "keywords": ["non-euclidean representation learning", "hyperbolic geometry", "hyperspherical geometry"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83b119ab4d38bf2fa9fe4b1675e9c92f869e6dbd.pdf", "supplementary_material": "/attachment/0fae315cd57cd3e6d0f1ab3ffb3d9068cee30b53.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a Riemannian Fuzzy k-means (RFK) algorithm for fast clustering on product manifolds, extending the conventional fuzzy k-means to Riemannian manifolds. This approach reduces the time complexity compared to the naïve Riemannian k-means (NRK) algorithm, by removing alternating iterations. In addition, the authors introduce Radan, an adaptive optimization algorithm on manifolds, as a generalization of Adan. Experimental results suggest that RFK achieves lower computational complexity than NRK, faster convergence than Riemannian Adam (Radam), and improved clustering performance on both synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed algorithms seem novel and original, though they are relatively straightforward extensions of fuzzy k-means and Adan to the Riemannian setting.\n- The computational efficiency of RFK relative to NRK is clearly established, with experimental results providing convincing evidence of its advantage.\n- Most of the mathematical derivations seem correct.\n- The paper is clearly written and generally well organized."}, "weaknesses": {"value": "- **Limited comparison to Riemannian baselines:** It is unclear whether the compared algorithms are also formulated on Riemannian manifolds. The authors should explicitly state whether the baselines take into account the manifold geometry. To better demonstrate the advantage of RFK, additional comparisons with other Riemannian clustering algorithms beyond NRK are needed.\n\n  To name a few, representative Riemannian clustering methods include Subbarao and Meer (2009), Nonlinear Mean Shift over Riemannian Manifolds (IJCV), Ashizawa et al. (2017), Least-squares Log-density Gradient Clustering for Riemannian Manifolds (AISTATS), and Zhao et al. (2016), Efficient Clustering on Riemannian Manifolds: A Kernelised Random Projection Approach (Pattern Recognition).\n- **Marginal performance gains:**\nIn several cases, the improvement over NRK is limited, and it is not evident why RFK outperforms NRK given their similar objectives. No error bars are provided, and for some datasets (e.g., CiteSeer and Cora), RFK performs worse in terms of NMI and F1. Further analysis and discussion would help clarify these inconsistencies.\n- **Unclear explanations and missing discussions:**\n  - The reason why RFK shows faster convergence than Radam is not well explained.\n  - The paper lacks a discussion of limitations, such as the sensitivity to the number of clusters or convergence to local minima.\n  - Many results are marked as out-of-memory (OM), yet the experimental setup is not described in sufficient detail. It is unclear why NRK suffers from OM—perhaps a memory analysis or ablation would clarify this.\n  - The description of parallel transport omits the specification of the curve along which vectors are transported."}, "questions": {"value": "Please refer to the points raised in the weaknesses section.\n\nIn addition, would RFK be effective if the underlying manifold consists of SPD matrices or other manifolds without closed-form geodesics? Some remarks on these points would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gveCb9fkJb", "forum": "CJHA6a77Yv", "replyto": "CJHA6a77Yv", "signatures": ["ICLR.cc/2026/Conference/Submission4809/Reviewer_nX9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4809/Reviewer_nX9x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641421885, "cdate": 1761641421885, "tmdate": 1762917587303, "mdate": 1762917587303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the open problem of efficient clustering on product manifolds by proposing Riemannian Fuzzy K-Means (RFK) and an accelerated optimizer Radan. By leveraging the fuzzy relaxation of cluster assignments, RFK eliminates the alternating updates required in Naive Riemannian K-Means (NRK), reducing time complexity from O(νω) to O(ν). Extensive experiments demonstrate significant speedups and superior clustering performance across diverse datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.It is practical to use fuzzy relaxation technology to transform the double-loop optimization of NRK into a single loop, reducing the time complexity from O(νω) to O(ν) while avoiding the problem of cluster centers exceeding the manifold.\n\n2.The Adan optimizer is adapted to obtain Radan, which is suitable for product manifolds through strategies such as parallel transport and scalar second-moment maintenance. Regret bounds and convergence proofs are provided to ensure the theoretical reliability of the algorithm.\n\n3.The paper includes extensive experiments across multiple datasets, demonstrating the superiority of RFK and Radan over existing clustering methods."}, "weaknesses": {"value": "1.Lack of Comparison with Recent Clustering Algorithms:\nAlthough the paper compares several methods, it appears to lack a comparison with the latest clustering algorithms specifically designed for non-Euclidean spaces.\n\n2.Unclear Explanations in Several Sections:\nMany parts of the paper lack clarity. For instance, in Chapter 2, since there are multiple isometric models of hyperbolic space, it is essential to explicitly specify which hyperbolic space model is being used. Additionally, in the regret bound proof of Theorem 3.1, the explicit form and range of values of the curvature functionζ(κ, c) are not sufficiently detailed."}, "questions": {"value": "Convergence Proof of Radan (Theorem 3.2) and Fixed Hyperparameters:\nThe convergence proof for Radan (Theorem 3.2) assumes decaying hyperparameters (e.g., β3t= 1 - 1/t), but all experiments use fixed values (e.g., β3= 0.99). The authors claim this is “standard practice”, yet they provide no justification or reference to support that convergence holds under fixed hyperparameters, creating a gap between the theoretical analysis and practical implementation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oa7AaRbUVa", "forum": "CJHA6a77Yv", "replyto": "CJHA6a77Yv", "signatures": ["ICLR.cc/2026/Conference/Submission4809/Reviewer_kwS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4809/Reviewer_kwS4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847649598, "cdate": 1761847649598, "tmdate": 1762917586959, "mdate": 1762917586959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the high time complexity of Naive Riemannian K-Means (NRK), this paper proposes a Riemannian Fuzzy K-Means (RFK) method that relies solely on cluster centers for single-loop updates. This method avoids alternating updates between the membership matrix and cluster centers used in traditional approaches. This method reduces computational complexity. Meanwhile, the authors also introduce a Riemannian Adan (Radan) optimizer, which is designed for product manifolds to further accelerate the convergence of RFK."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear derivation process from NRK to RFK, and also presents a convergence proof for the proposed Radan optimizer.\n2. Experimental results demonstrate that the method indeed improves both clustering efficiency and accuracy."}, "weaknesses": {"value": "1. Although the authors emphasize that RFK is not a simple extension of Fuzzy K-Means, fuzzy clustering in Euclidean space has been extensively studied.\n2. There are some typos. For example, the reference “Lin et al.” on line 1449 is missing the publication year.\n3. Some details should be provided. For example, is there any additional data pre-processing or pre-training before training? For another, how are the cluster centers initialized in RFK, and does this initialization affect the clustering results?"}, "questions": {"value": "1. The convergence proof relies on the convexity assumption on the manifold. Does this assumption still hold on non-convex manifolds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9Gu4qojSuc", "forum": "CJHA6a77Yv", "replyto": "CJHA6a77Yv", "signatures": ["ICLR.cc/2026/Conference/Submission4809/Reviewer_ptyL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4809/Reviewer_ptyL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996775470, "cdate": 1761996775470, "tmdate": 1762917586500, "mdate": 1762917586500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an application of the Fuzzy K-means algorithm using Riemanian distances. They use an indicator parameter that represents cluster belonging and is expressed in a closed form equation. By utilizing distances derived from several manifold representations, the Fuzzy K-means method is applied to the data. The authors presented results on several experiments with Gaussian synthetic data, graph data, and VAE embeddings of several benchmark datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The combination of multiple geodesic distances, assuming Euclidean, Hyperspherical, and hyperbolic manifolds, showed improvements in clustering performance. The application of closed-form expressions for the geodesic distances and class belonging parameters is a main contribution of the paper."}, "weaknesses": {"value": "However, there are several reasons why I cannot recommend acceptance of the proposed algorithm. The authors combined distances from different manifolds, assuming distinct manifold structures, but they did not provide any justification why it is valid to assume multiple manifold types for the same dataset. A more fundamental question is whether clustering performance can be improved even when ground-truth manifold distances are used. K-means can easily fail even with data having no manifold structure.\n\nCan the authors relate their method to spectral clustering or a clustering in a feature space associated with some kernels? It is difficult to identify a significant contribution given that this is a well-studied problem, while the paper does not sufficiently engage with the existing literature."}, "questions": {"value": "I did not examine all details in the Appendix, but it is unclear how the authors derived the closed-form expressions for the distances on each manifold. Did they first project the data onto the respective manifold and then computed the geodesic distances? \n\nThe closed form solution for the cluster belonging parameter u appears to diverge with m=1. In fact, the hyperparameter m may not being meaningful, because any u^m satisfies the same condition as u, and u^m itself could serve as a new parameter u without the need for m."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oAKWCDLahN", "forum": "CJHA6a77Yv", "replyto": "CJHA6a77Yv", "signatures": ["ICLR.cc/2026/Conference/Submission4809/Reviewer_k6An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4809/Reviewer_k6An"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105889603, "cdate": 1762105889603, "tmdate": 1762917586122, "mdate": 1762917586122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary 1: Consensus on Novelty, Impact, and Reproducibility"}, "comment": {"value": "We thank the reviewers for their comments and appreciate the attention our work has received from the AC, PC, and the community. We look forward to reaching a consensus with all reviewers in this discussion.\n\nWe note that each reviewer's summary points out that our contribution lies in transforming the double loop of Riemannian K-Means into a single loop. We believe we can first reach a primary consensus: \n\n$ \\textcolor{red}{\\text{a single-loop solution should be superior to a double-loop one for the problems it addresses.}} $\n\nThis is because our algorithm avoids the computation of the Frechet mean—which itself requires a loop and is extremely time-consuming, making it infeasible for large-scale data—while ensuring improved performance metrics.\n\nFurther, we hope to establish a second consensus: \n\n$ \\textcolor{red}{\\text{RFK is a conceptual improvement over Riemannian K-Means and has potential significant impact.}} $\n\nReviewers nX9x and ptyL currently hold a different view, suggesting that Fuzzy K-Means in Euclidean space is well-studied and our work is merely a simple extension. We offer the following counterarguments:\n\n1.  The direct extension of K-Means to Riemannian manifolds, adopted by numerous Riemannian machine learning papers and libraries [1] [2] [3] before our work, involves alternating optimization of centers and assignments. It is precisely this direct extension philosophy that led to the challenging and time-consuming problem of requiring Frechet means. Our single-loop Riemannian optimization method solves this significant challenge.\n2. We acknowledge that recent work in 2025 has explored using gradient methods to update Fuzzy K-Means in Euclidean space [4]. However, we must point out that the fundamental reason gradient methods outperform alternating updates is that the **curved spatial structure** results in no closed-form solution for the centers. In Euclidean space, the geometric center *does* have a closed-form solution, meaning that Fuzzy K-Means is effectively a single-loop algorithm whether using gradient methods or alternating updates. This does not provide a fundamental speedup.\n3. **We believe the impact of a work should be judged by the changes it brings to the community.** Our work allow the community to realize that Riemannian K-Means only require a single cycle, not a double cycle. This lays an important foundation for large-scale clustering (We have provided a recent anonymous community-driven open-source library to support this). Based on this, we believe our work has significant meaning and major potential impact.\n\nBased on this, we hope to reach this second consensus. (We welcome continued comments from reviewers, the AC, PC, or anyone else.)\n\nThe third consensus we hope to establish is: \n\n$\\textcolor{red}{\\text{Our work is one of the most reproducible submissions for ICLR 2026.}}$\n\nThis is not only because we provide the source code and a community open-source library that can be implemented in just three lines of code. We have also **provided the parameters and random seeds for each dataset as separate files**, allowing for one-click execution to obtain results identical to those in our paper. We are happy to assist anyone in perfectly reproducing our paper's results.\n\n---\n\n### **Summary of Consensus Points**\n\nTo summarize, we hope to establish the following three points of consensus in this summary:\n\n1.  A single-loop solution for Riemannian (Fuzzy) K-Means should be superior to a double-loop solution.\n2.  RFK is a conceptual improvement over Riemannian K-Means and has potential significant impact.\n3.  Our work is one of the most reproducible for ICLR 2026.\n\nReferences:\n\n[1] Geomstats: a Python Package for Riemannian Geometry in Machine Learning\n\n[2] https://geomstats.github.io/notebooks/07_practical_methods__riemannian_kmeans.html\n\n[3] https://www.linkedin.com/pulse/k-means-riemann-manifolds-patrick-nicolas-rpkkc/\n\n[4] Unconstrained Fuzzy C-Means Algorithm"}}, "id": "zpXUH2nCQ4", "forum": "CJHA6a77Yv", "replyto": "CJHA6a77Yv", "signatures": ["ICLR.cc/2026/Conference/Submission4809/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4809/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission4809/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763045865082, "cdate": 1763045865082, "tmdate": 1763097557207, "mdate": 1763097557207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}