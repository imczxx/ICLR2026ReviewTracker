{"id": "L37nYyd22W", "number": 21071, "cdate": 1758313440913, "mdate": 1758894952499, "content": {"title": "An Efficient and Modular Framework for Targeted Harm Mitigation in LLMs", "abstract": "Large language models (LLMs) are powerful zero-shot learners but remain prone to misalignment with human preferences, often producing biased, toxic, or otherwise harmful outputs. Existing alignment methods, while effective, are costly and tightly coupled to the base model, limiting flexibility and scalability. To eliminate harms from misaligned model responses, we propose a modular correction framework that augments pretrained LLMs with Activated LoRA (aLoRA) adapters and a context-aware routing mechanism. Our approach enables expert adapters to activate mid-sequence without invalidating the KV cache, allowing low-latency, targeted correction during generation. Each expert is trained to detect and mitigate specific harms, such as bias or toxicity. A learned router dynamically selects appropriate experts based on the model's intermediate outputs. We demonstrate that our system improves alignment on standard safety benchmarks while preserving task performance and enabling composable, scalable corrections. This work offers a lightweight, efficient path toward safer and more controllable LLM deployments.", "tldr": "", "keywords": ["harm alignment", "mixture of experts", "safety", "alignment", "large language models", "routing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0f793ccbf25e9e336c80a049a58d8f52209fddf6.pdf", "supplementary_material": ""}, "replies": [], "withdrawn": true}