{"id": "qDFegAnCin", "number": 20862, "cdate": 1758311092654, "mdate": 1759896954766, "content": {"title": "Unified Plan Verification with Static Rubrics and Dynamic Policies for Reliable LLM Planning", "abstract": "Large language model (LLM) agents can decompose tasks, call tools, and execute multi-step plans, yet they frequently fail for two reasons: (i) pre-execution plans look plausible but are incomplete, inconsistent, or ill-posed; and (ii) during execution, tool outputs reveal conflicts or policy violations that the agent neither detects nor repairs. Existing \"LLM-as-judge\" scoring is unstable and opaque, while reactive agents lack grounded, learnable control. We introduce \\ours, a VERification-Aware planning infrastructure that inserts explicit checks both before and during execution. First, Static Verification via Rubrics (SVR) instantiates an instance-specific, binary checklist from a general taxonomy (completeness, correctness, executability), yielding auditable, stable decisions and actionable feedback for plan revision. Second, a Dynamic Verification Policy (DVP) enforces run-time control: a prompt-optimized rulebook (learned via MCTS-style discrete search, no weight updates) consumes the step context and tool outputs to emit symbolic actions---e.g., browse more candidates, switch tool, skip, backtrack, or accept. \\ours is representation-agnostic and applies to structured plans with schemas/tools, unstructured conversational plans, and natural-language plans without tools. Across three regimes, \\ours consistently improves task success and constraint satisfaction over strong prompting and agent baselines, reduces temporal/budget and policy violations, and provides rubric-level diagnostics that localize errors. Ablations show SVR (pre-execution screening) and DVP (execution-time control) are complementary; learned rulebooks outperform human-written heuristics with modest extra compute. We release prompts, rulebooks, and evaluation code to facilitate verification-aware agent research.", "tldr": "We propose a verify-then-control framework for tool-using LLM agents: task-specific rubrics statically screen and repair plans (SVR), and a learned rulebook guides execution (accept/next/alt/skip/backtrack) for auditable, reliable results.", "keywords": ["Prompt Optimization", "Large Language Models", "Planning", "Natural Language Processing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af6efe258feb7653d10ce2b5b642c51ca5d89a4e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main contribution of this paper is a two-part planning framework for LLMs -- the first part generates binary checklist rubrics to perform a version of \"static verification\" on plans before executing them; the second part does \"dyanmic verification\" at run-time to help the plan overcome obstacles as it is being executed (e.g., switch from a flight to a bus if the cost runs too high). Experiments compare to a series of recent works on LLM planning over three standard datasets. Results are quite good. Furthermore, there are some other experiments to explore different facets of the proposed approach (E.g,. comparing inter-judge agreement versus an alternative method PlanGen)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The overall approach makes a nice (although maybe slightly incremental?) advance of previous planners. \n\n+ I like the rubric generation aspect of the approach. Based on a human-authored taxonomy, it relies on the LLM itself plus characteristics of the task to generate specific rubrics. The experiments give evidence that these rubrics lead to better rewards that are more stable.\n\n+ The rulebook learning for plan verification appears to be a nice advance (though I will defer to other reviewers on this specific point). Additional experiments show the significance of this component (Figure 4).\n\n+ The headline experimental results over the three benchmarks are quite good, with the proposed approach achieving the best results in all cases and for all metrics."}, "weaknesses": {"value": "- For the dynamic verification, my understanding is there is a need for some training data to kickstart the learning of the domain-specific rulebook. (I may be mistaken). If so, there is some extra overhead in terms of data requirements and compute, as well as a worry about domain adaptation. \n\n- Continuing this point, there's a mention of \"bounded overhead, with cost–performance curves that preserve most gains under a modest verification frequency\" --> is this reported in the paper anywhere?\n\n- For Table 2 (the main results), why not compare the improvement to the next best result? It seems strange to compare to vanilla prompting since there are so many stronger baselines."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SIroLpTLcf", "forum": "qDFegAnCin", "replyto": "qDFegAnCin", "signatures": ["ICLR.cc/2026/Conference/Submission20862/Reviewer_9iHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20862/Reviewer_9iHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693113375, "cdate": 1761693113375, "tmdate": 1762999981480, "mdate": 1762999981480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERA, a framework designed to enhance the reliability of LLM-based planning agents through a unified verification approach. VERA consists of two core components: (1) Static Verification via Rubrics (SVR) for pre-execution plan validation using instance-specific binary checklists derived from a generic taxonomy (completeness, correctness, executability), and (2) Dynamic Verification Policy (DVP) for runtime control that uses prompt-optimized rulebooks to guide execution through symbolic actions. Evaluations on three benchmarks demonstrate that VERA consistently improves success rates and stability over strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The approach of combining pre-execution (SVR) and during-execution (DVP) verification is a novel and intuitive solution to the identified gaps in LLM planning. \n* VERA achieves large improvements and shows generalization capability to Game-of-24\n* The paper includes thorough ablation analyses that effectively demonstrate the individual and complementary contributions of SVR and DVP."}, "weaknesses": {"value": "* The paper mentions \"modest extra compute\" but provides no concrete analysis of computational overhead and no comparisons. SVR uses multiple judges and DVP requires MCTS optimization during training. Time and cost comparison and analysis should be included.\n* The paper ends abruptly after Section 5.3 with no conclusion or discussion of limitations. \n* Some implementation details are missing. For example, how many datapoints are evaluated for each benchmark? How many datapoints are included in the training set of DVP?"}, "questions": {"value": "* I’m curious what are some example input/output of each module in the framework? Especially, since the travelplanner benchmark contains commonsense constraint evaluation, is SVR able to locate and explicitly check for all commonsense constraints? \n* Please include cost and time comparisons and analysis.\n* Failure analysis is missing. When does VERA still fail? What types of errors does SVR make? Are there cases where it generates irrelevant or incorrect rubric items that misguide the verification process?\n* The paper mentions that learned rulebooks outperform human-written heuristics. How is this claim supported? Could the authors provide more analysis?\n* The authors mention “not able to replicate the numbers” for LLM-Modulo. Is it the code is not open sourced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8tvXJsnGMv", "forum": "qDFegAnCin", "replyto": "qDFegAnCin", "signatures": ["ICLR.cc/2026/Conference/Submission20862/Reviewer_Zgj4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20862/Reviewer_Zgj4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931342445, "cdate": 1761931342445, "tmdate": 1762999981313, "mdate": 1762999981313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VERA, a unified framework to improve the reliability of LLM-based planners by addressing failures both before and during execution. The core contribution is a two-part verification system. First, Static Verification via Rubrics (SVR) validates a plan before execution generating an instance-specific, binary checklist (called a rubric) from a general taxonomy (completeness, correctness, executability) and using an LLM-judge to score the plan against it. Second, a Dynamic Verification Policy (DVP) provides runtime control by using a \"rulebook\" (learned via MCTS-style prompt optimization) to consume tool outputs and emit symbolic actions like backtrack or skip_step. The authors show that this combined approach significantly improves task success and constraint satisfaction over a wide range of strong baselines on three diverse planning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting Formulation: The split into two categories for planning failure is interesting, namely: (i) pre-execution (ill-posed plans) and (ii) in-execution (runtime conflicts). I think this has been discussed in papers that do classical planning with LLMs, but I don't recall the same ideas on open-world agentic planning with LLMs.\n\n- The paper is clear (although some parts are dense) and the idea is conceptually well presented. Additionally, the experiments are quite solid and, at least to my judgement, they are quite complete.\n\n- The topic certainly interests the ICLR audience so it is a good fit for the conference."}, "weaknesses": {"value": "My main concern with this paper is its apparent computational cost. VERA introduces several new LLM calls: (1) rubric generation, (2) SVR plan judging, (3) a potential replanning loop, and (4) DVP policy calls at every execution step. The paper mentions \"modest extra compute\" but does not quantify this. A practical LLM agent framework needs to be efficient, and the cost of VERA seems high. There also seems to be other interesting solutions in the literature that are cheaper and where not mentioned --- e.g., Thoughts of Search by Katz et al.\n\nThe process of learning the \"rulebook\" using MCTS-style prompt optimization is underspecified and confusing. The paragraph starting at 219 is quite dense, and I am not sure one could reimplement the idea just from it. Although Algorithm 1 in the Appendix B gives some more information, the specific details were still too vague for me. The paper does not clearly define the state space (I actually don't know how states are encoded, e.g., is the environment state just textual?), reward function used during the rollouts, or the mechanism by which optimal trajectories are synthesized into the final \"rulebook\" prompt. This lack of detail makes it difficult to assess the computational effort, replicability, and generalizability of the learning process across new domains.\n\nLast, but not least, the paper felt somewhat incomplete to me. For example, there's no discussion, or limitations section. I also really disliked that the related work section is in the appendix. I think the discussion of related work (particularly those that are compared against in the text) should be added to the main text."}, "questions": {"value": "Can you provide a quantitative analysis of the computational overhead? For instance, what is the average number of LLM calls and total tokens for VERA per task compared to a strong baseline like PlanGen or ReAct on the TravelPlanner benchmark?\n\nHow was the generic SVR taxonomy (Figure 1) developed? How much (if any) human adaptation of this taxonomy is needed to apply VERA to a new domain?\n\nDo you have results for fresh new domains? Do you have results for weaker models?\n\nThe DVP rulebook is learned using an \"MCTS-like\" optimization. Can you provide more concrete details on the MCTS implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lb3PquPPDB", "forum": "qDFegAnCin", "replyto": "qDFegAnCin", "signatures": ["ICLR.cc/2026/Conference/Submission20862/Reviewer_amiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20862/Reviewer_amiX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971446576, "cdate": 1761971446576, "tmdate": 1762999981325, "mdate": 1762999981325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a verification framework called VERA (VERification-Aware) that introduces explicit check before and during execution of composite, multi-step plans. VERA has two main components: (i) Static Verification via Rubrics (SVR), a pre-execution filtering mechanism that checks plans for completeness, correctness, and executability using instance-specific binary rubrics, and (ii) Dynamic Verification Policy (DVP), a run-time control policy learned through prompt optimisation that guides execution by issuing symbolic actions $\\in$ {`accept`, `next_result`, `alt_tool`, `skip_step`, `backtrack`}. VERA is evaluated on three benchmarks demonstrating performance improvements against several baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive evaluation of the proposed verification framework including ablations for the added value of the pre- (i.e. SVR) and in-execution (i.e. DVP) plan verification pipelines.\n- Strong empirical results showing large gains on diverse benchmarks (i.e. on TravelPlanner, TauBench and NaturalPlans) compared to baselines.\n- Interesting approach for having rubric-based verifications (by SVR), which are interpretable, and provide stable pass and fail decisions over noisy scalar scoring."}, "weaknesses": {"value": "- Some sections appear incomplete or are relegated to the appendices. In its current form, the paper feels unfinished, with certain sections, such as Related Work, moved to the appendix to save space, and others, such as Conclusion or Limitations, missing entirely.\n    - I would be happy to revisit my score if the presentation of the paper can be improved, resulting in a more standalone main body.\n- The paper would benefit from quantitative breakdown of the frequency with which the various DVP actions $\\in$ {`accept`, `next_result`, `alt_tool`, `skip_step`, `backtrack`} are selected during inference across the different benchmarks. Such analysis would provide valuable insights into the actual decision-making behavior of the runtime verification policy and its qualitative impact.\n- While the ablation studies focusing on the relative benefits of SVR and DVP are thorough on the TravelPlanner domain, it would significantly strengthen the paper to include similar analyses on the other benchmarks: TauBench and NaturalPlans."}, "questions": {"value": "- Is there a fallback strategy queries that led to the generation of plans that do not meet the expected $\\theta_{\\text{pre}}$ requirements?\n- It is unclear if the reported results are averaged over multiple runs or just single runs at a fixed sampling temperature (0.7)\n- I believe the figure cited on line 419 is Figure 3 instead of 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D01cF11hW8", "forum": "qDFegAnCin", "replyto": "qDFegAnCin", "signatures": ["ICLR.cc/2026/Conference/Submission20862/Reviewer_jmv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20862/Reviewer_jmv5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132669299, "cdate": 1762132669299, "tmdate": 1762999981537, "mdate": 1762999981537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}