{"id": "P798W8Ag7L", "number": 1472, "cdate": 1756885497592, "mdate": 1763119584842, "content": {"title": "Benchmarking Visual Knowledge in Multimodal Large Language Models", "abstract": "While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This capability, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored gap in current systems.\nTo systematically measure this capability, we present VKBench, a comprehensive video benchmark featuring 1,680 questions in 1,249 videos, covering eight core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Results show that leading models still fall short of human performance, with particularly notable gaps in world-centric visual knowledge.\nTo bridge this gap, we introduce VKQA, a new dataset, and Video-VK+, a baseline model that explicitly incorporates visual knowledge into MLLMs. Video-VK+ follows a structured See–Think–Answer format and adopts reinforcement learning with visual knowledge reward. This approach improves performance on VKBench by 3.7% and surpasses existing models on multiple video benchmarks.\nOur findings highlight visual knowledge as a key component for developing more robust and generalizable MLLMs that can not only see but also truly understand our world.", "tldr": "", "keywords": ["Multimodal Large Language Model", "Video Large Language Model", "Visual Knowledge", "Benchmark", "Datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/186de1c1275cf653df49719895560101bf3f1938.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces VKBench, a benchmark comprising 1,680 questions across 1,249 videos, designed to evaluate eight core dimensions of visual knowledge spanning both world-centric and human-centric domains. It further presents VKQA, a new dataset, and Video-VK+, a baseline model that explicitly incorporates visual knowledge into multimodal large language models, which follows a structured See–Think–Answer paradigm and employs reinforcement learning with visual-knowledge rewards. Experimental results demonstrate that this approach improves performance on VKBench by 3.7% and surpasses existing models on multiple video understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors present not only VKBench, a comprehensive benchmark, but also Video-VK+, a novel method designed to evaluate and enhance visual knowledge in multimodal large language models. Their study provides systematic insights into how models apply world-centric and human-centric visual knowledge.\n\n2. By explicitly incorporating visual knowledge through a structured See–Think–Answer framework and reinforcement learning, Video-VK+ achieves substantial performance gains on VKBench and other video understanding tasks."}, "weaknesses": {"value": "1. Some of the presented examples in VKBench are ambiguous or confusing. See \"Questions\" part.\n\n2. Human-centric tasks in VKBench have already been well studied in prior benchmarks such as Visual Commonsense Reasoning (VCR), which somewhat diminishes the novelty of this contribution.\n\n3. The definition of visual knowledge remains unclear. According to Section 3.1, it encompasses both visual commonsense and expert-level domain knowledge, while the paper mainly focuses on the former. It may therefore be more appropriate to reframe the work around visual commonsense rather than the broader concept of visual knowledge."}, "questions": {"value": "1. Questions about cases in Figure 2:\n\n-  For the Object Affordance and Object Material tasks, the cases appear solvable using only a pair of images rather than a full video. The necessity of providing a video input for these tasks should therefore be further justified.\n\n-  In the Spatial Awareness task, it is unclear how one can infer the spatial relationship between the TV and the sofa solely from three separate images of the stove, TV, and sofa, together with the description “I am standing by the stove and facing the sofa.” The rationale for how these inputs support spatial reasoning should be clarified.\n\n2. Why is it necessary to use video as input for certain tasks? Given that many examples appear solvable with one or few static images, whether single or multi-image inputs (like VCR) would be sufficient to capture the required visual information and reasoning? What additional advantages can temporal continuity in videos provide?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "22lKPdqxkK", "forum": "P798W8Ag7L", "replyto": "P798W8Ag7L", "signatures": ["ICLR.cc/2026/Conference/Submission1472/Reviewer_em2a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1472/Reviewer_em2a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208514730, "cdate": 1761208514730, "tmdate": 1762915778155, "mdate": 1762915778155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wnYZClFk4Q", "forum": "P798W8Ag7L", "replyto": "P798W8Ag7L", "signatures": ["ICLR.cc/2026/Conference/Submission1472/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1472/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119584088, "cdate": 1763119584088, "tmdate": 1763119584088, "mdate": 1763119584088, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the concept of \"visual knowledge\", referring to the intuitive understanding of the physical world and social knowledge that MLLMs lack. To evaluate this ability systematically, the author constructed a VKBench video benchmark, covering 8 tasks such as intuitive physics and subjective intentions. The evaluation found that the leading model lags behind humans by 15% overall, especially in physical reasoning where the gap is significant. To address this issue, the paper introduces the VKQA dataset and Video VK+ baseline model. The model adopts an \"see think answer\" reasoning format and incorporates visual knowledge rewards through reinforcement learning, achieving a 3.7% improvement on VKBench and demonstrating excellent generalization ability on multiple video benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- 1. Carefully designed benchmark construction: The construction process of VKBench is very rigorous, not simply stacking existing data. The progressive filtering pipeline adopted (Minimize Audio Reliance ->Reduce Language Bias ->Enhance Wrong Candidates ->Human Verification) effectively removes language bias and audio dependencies, ensuring that the benchmark truly evaluates the model's ability to extract knowledge from visual signals, rather than its internal language model's prior knowledge.\n- 2. Large scale evaluation and analysis: The paper comprehensively evaluated 23 mainstream open-source and closed-source MLLMs, not only revealing the overall performance gap, but also conducting detailed analysis (such as differences between open-source and closed-source models, model scale effects, and the impact of reasoning version). Especially through correlation analysis, it was found that the world center and human center knowledge form two independent clusters, which verifies the rationality of their classification from the data.\n- 3. The paper shifts the research focus from traditional \"perception\" (object recognition) and \"complex reasoning\" to the intermediate layer of \"visual knowledge\", which is a relatively unexplored but crucial field. This evaluation and training data are meaningful to the MLLM community."}, "weaknesses": {"value": "- 1. The coverage of \"visual knowledge\" may still be incomplete: although the division of 8 dimensions is quite comprehensive, \"visual knowledge\" itself is an extremely rich concept. For example, the assessment of deeper cognitive abilities such as causality, counterfactual thinking, and visual metaphor may not have been fully covered in the proposed benchmark.\n- 2. VKBench only uses multiple-choice questions. This cannot fully evaluate whether the reasoning process behind it truly conforms to logic and is based on correct visual knowledge.\n- 3. During the RL training of the Video VK+model, an external frozen MLLM as the verifier is introduced, which is equivalent to handing over a key evaluation criterion to another potentially flawed model. Meanwhile, this significantly increases the complexity and computational cost of training.\n-4. The optimization goal of the Video VK+model is to make the visual description contain answer information as much as possible, thereby reducing the inference burden of LLM. However, it may introduce new risks: will this suppress the model from performing necessary and complex multi-step inference? Overemphasizing the \"self-contained\" visual knowledge may make models \"lazy\" for deep reasoning.\n- 5. The analysis of some experimental phenomena is relatively general, such as the performance of the InternVL3.5-38B-Think on VKBench, which is actually worse than InternVL3.5-38B due to \"excessive reasoning\". Video VK+performs poorly on Spatial Awareness tasks, even inferior to some baseline models. The paper simply attributes the task to requiring 'long-term visual memory'. These explanations are somewhat vague and deserve a deeper analysis."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IJdeSP28ut", "forum": "P798W8Ag7L", "replyto": "P798W8Ag7L", "signatures": ["ICLR.cc/2026/Conference/Submission1472/Reviewer_gT1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1472/Reviewer_gT1D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883708820, "cdate": 1761883708820, "tmdate": 1762915777836, "mdate": 1762915777836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current MLLMs can see but often don’t really understand the physical/social structure of the world. So they define that missing layer as “visual knowledge”, a zone between pixels and reasoning (gravity, affordances, etc). They build VKBench: 1,249 videos, 1,680 MCQs, 8 dimensions split into world-centric (physics, affordance, material, spatial) and human-centric (event anticipation, mental state, social relation, intention). They very deliberately filter out audio and language shortcuts so models can’t just guess from text. Then they show: even strong video MLLMs are worse than humans overall, and the gap is especially bad on world-centric stuff (intuitive physics, spatial). To show this isn’t hopeless, they build Video-VK+: basically Qwen2.5-VL-7B with a See–Think–Answer format + GRPO RL + a visual-knowledge reward that checks whether your description was good enough to answer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes 8 tasks that map neatly to cognitive/vision literature.\n2. Complete anti-shortcut pipeline.\n3. The papers shows models are OK on human-centric but bad on world-centric.\n4. The benchmark is balanced and well-scoped."}, "weaknesses": {"value": "1. A lot of the difficulty comes from filtering existing datasets, not from filming new, adversarial, physics-centric videos, leading to any upstream video biases it originally uses.\n2. The visual-knowledge reward uses a frozen MLLM as verifier. That’s convenient, but it bakes the verifier’s biases right back into training.\n3. The paper shows correlations, but not what models actually get wrong?"}, "questions": {"value": "Some tasks (event anticipation, social relation) clearly like longer context. Why did you fix at 32 instead of reporting a “long-context” track?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g7iL4rLhwM", "forum": "P798W8Ag7L", "replyto": "P798W8Ag7L", "signatures": ["ICLR.cc/2026/Conference/Submission1472/Reviewer_LbT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1472/Reviewer_LbT3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987254741, "cdate": 1761987254741, "tmdate": 1762915777518, "mdate": 1762915777518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a core gap in MLLM's ability to do human-like visual reasoning, which they term as _visual knowledge_. This refers to intuitive principles which humans use freely to understand the world, like intuitive physics and social cues. Based on this, the paper proposes two main contributions: \n1. A new benchmark, **VKBench**, which is curated from a set of existing datasets (IntPhys 2, PACS, VSI-Bench, VLEP, Social-IQ 2.0, RexTime) to benchmark MLLM's visual knowledge across world and human-centric axes. It consists of 1,680 multiple-choice questions across 1,249 videos, covering eight distinct types of visual knowledge. It was carefully constructed to avoid audio and linguistic biases. \n2. A new dataset, **VKQA**, of visual knowledge video examples, as well as method, **Video-VK+**, demonstrating that visual knowledge can be taught to these models. The authors utilize a \"See-Think-Answer\" format with RL to enforce the model to first visually process the input before making deductions. This is done on Qwen-2.5-VL-7B Instruction backbone, and this generally improves the models by 4.58% on average on their suite of benchmarks.\n\nThe authors benchmark many state of the art models on **VKBench**, and find that models still fall behind human performance (15% at best), especially at Intuitive Physics and Spatial Awareness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "**Well-curated benchmark methodology**: VKBench was collected from existing datasets, so while there is no new contribution of base data on this front, there was consideration to how biased questions could be to audio and language, which are important problems present in recent benchmarks. There was also human validation done on the questions, in addition to shuffling the choices. \n\n**Simultaneous proposal of problem, benchmark, and method**: The authors not only codify a problem, but they also propose a benchmark, dataset, and method to solve it. This is a fair undertaking which shows that the overarching paper was thought out well in advance. \n\n**Nice figures and writing**: The figures are colorful and illustrative, and supplement the main text very well. Overall the writing of the paper is clear apart from minor details not described."}, "weaknesses": {"value": "**Benchmark is not difficult nor prescriptive**: While the benchmark claims to evaluate visual knowledge, it's not clear to me what information I gain by benchmarking my model on VKBench. First, benchmarks currently introduced where SoTA models achieve 71% accuracy is not helpful, as I suspect such a gap can be closed quite quickly (especially as it seems to be implied that this is a knowledge issue from the methods section). Given that random chance is so high for many questions, I find it surprising the authors didn't add in more options or leave the questions as short answers. This to me also points to how the current division of visual knowledge axes is too coarse, as models are somewhat uniform across them, and I can't tell where a model specifically lacks when it performs poorly. I would like more feedback on where models tend to fail, or _how they do_, rather than attributing it to being a knowledge or processing issue. \n\nRather than looking at Pearson correlations within the benchmark itself, you should compare to other benchmarks to see how well being good at X task correlates with having a strong world simulator in your model, or social understanding (of which there exist other benchmarks already). This would provide stronger validation that this benchmark is indeed prescriptive. \n\n**Weak method contributions beyond GRPO-Zero**: From what I can tell, it seems that the majority of the contribution of the method lies with GRPO(-Zero), which is prior work. For instance, MVBench, Video-MME, and MMVU are very similar between GRPO-Zero and Video-VK+, while I suspect VKBench may be more knowledge-based as a benchmark, which is why it improves better when incorporating extra data from VKQA. In fact, the See-Think-Answer SFT does not improve VKBench at all, while only providing modest contributions for the rest of the benchmarks."}, "questions": {"value": "1. It's not quite clear to me from the text if the QAs are also pulled from the existing datasets, or if only the annotations are used to then synthesize new QAs. Clarification on this would be helpful. \n\nI think the dataset needs to go under significant revisions for this to be helpful for the community. While there clearly has been a large amount of detailed effort invested, I don't think that this benchmark is of critical value given the already immense number of video benchmarks which exist. It seems to be in a niche which is already covered by other benchmarks (given that the questions are collected from other popular datasets), and even the filtering done does not have validation to show that this provides a meaningful improvement over alternatives. Therefore I recommend a reject based on my current review of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U8VEkHIb2d", "forum": "P798W8Ag7L", "replyto": "P798W8Ag7L", "signatures": ["ICLR.cc/2026/Conference/Submission1472/Reviewer_XciW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1472/Reviewer_XciW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122624837, "cdate": 1762122624837, "tmdate": 1762915777102, "mdate": 1762915777102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}