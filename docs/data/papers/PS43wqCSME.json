{"id": "PS43wqCSME", "number": 13794, "cdate": 1758222721612, "mdate": 1759897412007, "content": {"title": "Through the Stealth Lens: Attention-Aware Defenses Against Poisoning in RAG", "abstract": "Retrieval-augmented generation (RAG) systems are vulnerable to attacks that inject poisoned passages into the retrieved context, even at low corruption rates. We show that existing attacks are not designed to be stealthy, allowing reliable detection and mitigation. We formalize a distinguishability-based security game to quantify stealth for such attacks. If a few poisoned passages control the response, they must bias the inference process more than the benign ones, inherently compromising stealth. This motivates analyzing intermediate signals of LLMs, such as attention weights, to approximate the influence of different passages on the response. Leveraging attention weights, we introduce the **Normalized Passage Attention Score** (NPAS) and a lightweight **Attention-Variance Filter** (AV Filter) that flags anomalous passages. Our method improves robustness, yielding up to $\\sim$ **20\\%** higher accuracy than baseline defenses. We also develop adaptive attacks that attempt to conceal such anomalies, achieving up to **35\\%** success rate and underscoring the challenges of achieving true stealth in poisoning RAG systems.", "tldr": "Poisoning and prompt-injection attacks on RAG are not stealthy, as they leave anomalies in LLM attention. We detect these outliers via attention scores and propose AV Filter, a defense that removes poisoned passages and improves robustness.", "keywords": ["Retrieval-augmented Generation", "Safety", "Poisoning Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51865361681a3fa71fd8f656a7c74cfe6279bb33.pdf", "supplementary_material": "/attachment/a1a9264ec85e07133b5f1a4203b7b95f7a06c486.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Attention-Variance Filter (AV Filter), a novel defense against poisoning attacks in Retrieval-Augmented Generation (RAG) systems. The core idea is that malicious passages, in order to successfully poison the output, must attract a disproportionate amount of the model's attention. The authors formalize this using a Normalized Passage Attention Score (NPAS) and show that the variance of these scores across passages is a reliable signal for detecting corruption. The proposed AV Filter iteratively removes passages with the highest attention scores until this variance drops below a threshold. The paper evaluates this defense against several content-poisoning and instruction-poisoning attacks, demonstrating its effectiveness in reducing the Attack Success Rate (ASR) while claiming to maintain high accuracy in benign settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Defense Signal: The use of attention variance as a signal for detecting poisoned context is novel.\n\n2. Thorough Experimental Setup: The authors conduct a comprehensive set of experiments across multiple datasets (RQA, NQ, HotpotQA), LLMs (Llama2, Mistral, Llama-3.1, Deepseek-R1, GPT-4o), and attack types (Poison, PIA, MA, Paradox).\n\n3. Baseline Comparison: The paper includes a comparison against the state-of-the-art \"Certified Robust RAG\" (Keyword and Decoding) baselines, providing a clear benchmark for its performance."}, "weaknesses": {"value": "1. Weak Reranker Baseline: The comparison to reranking methods in Appendix D.5 (Table 11) may have some problems. The paper's baseline \"removes the passage ranked highest in relevance,\" which is the opposite of a reranker's intended use. A proper baseline would use the reranker to re-order the passages and pass the most relevant ones to the LLM. Furthermore, the paper uses relatively weak rerankers (ColBERTv2, T5). Even against this  baseline, Table 11 shows that for GPT-4o, the reranking method outperforms AV Filter in 4 out of 9 setups. A comparison against modern, stronger rerankers  using a correct methodology is necessary to prove the utility of AV Filter.\n\n2. Inconsistent Performance Against \"Keyword\" Baseline: The \"Keyword\" baseline from Xiang et al. (2024) proves to be a very strong competitor. As seen in Table 2, the Keyword baseline delivers significantly better Robust Accuracy (RACC) and lower Attack Success Rate (ASR) than AV Filter in several key scenarios, particularly against Prompt Injection Attacks (PIA) on stronger models like Deepseek-R1 and GPT-4o. This makes the practical necessity of the more complex AV Filter unclear.\n\n3. Remaining Accuracy Drop: In many other cases, the AV Filter does result in a drop in clean accuracy (e.g., an average of 4-6% noted by the authors). While this may be an acceptable trade-off for some, when combined with the inconsistent performance against baselines, it weakens the overall proposal."}, "questions": {"value": "1. Explanation of Baselines: The \"Keyword\" and \"Decoding\" baselines are central to your evaluation, how do they work? what are the specific configurations for these two methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4PvyXKdfiV", "forum": "PS43wqCSME", "replyto": "PS43wqCSME", "signatures": ["ICLR.cc/2026/Conference/Submission13794/Reviewer_JgSt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13794/Reviewer_JgSt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788936076, "cdate": 1761788936076, "tmdate": 1762924328652, "mdate": 1762924328652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Stealth Attack Distinguishability Game (SADG) to formally characterize the trade-off between stealthiness and detectability of poisoning attacks against Retrieval-Augmented Generation systems. To analyze this relationship, the authors leverage attention signals to quantify the influence of each retrieved passage on the model’s final answer. Building on this observation, they design a lightweight anomaly detector called the AV Filter, which identifies and removes suspicious passages based on abnormal variance in normalized passage-level attention scores. Experiments demonstrate that the AV Filter significantly reduces attack success rates across multiple datasets and language models, while maintaining high clean performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper analyzes the influence of poisoned passages through generation-time attention signals, providing an intuitive and interpretable view of which retrieved segments drive the model’s responses. Building on this insight, the authors design a lightweight and effective defense algorithm, making the motivation both clear and convincing.\n2. By proposing the Stealth Attack Distinguishability Game (SADG), the paper introduces a quantitative framework that defines “stealth” as a game-theoretic metric. This formalization contributes a principled foundation for future theoretical and adversarial analyses of RAG security.\n3. The experiments are thorough, covering multiple datasets, various large language models, and different types of poisoning attacks, including both content-based and instruction-based threats. Moreover, the authors design a novel adaptive attack to stress-test their defense, further demonstrating the robustness of the evaluation."}, "weaknesses": {"value": "1. The proposed defense assumes that only a small fraction of retrieved passages are poisoned. If an attacker manages to contaminate the majority of retrieved entries, especially in private or small-scale knowledge bases, the variance signal used by the AV Filter can be diluted, leading to failure of detection. This limitation poses a realistic concern for practical deployments.\n2. The AV Filter is primarily designed to defend against attacks that alter the content of generated answers in QA-style RAG tasks. Its effectiveness in other non-QA or generative contexts, such as summarization, dialogue, or decision-making tasks, remains untested.\n3. The paper introduces numerous formulas and symbols that are not strictly necessary for understanding the core ideas. This abundance of notation may reduce the overall readability."}, "questions": {"value": "Please refer to my comments on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GfCQsFNacL", "forum": "PS43wqCSME", "replyto": "PS43wqCSME", "signatures": ["ICLR.cc/2026/Conference/Submission13794/Reviewer_WDZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13794/Reviewer_WDZX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815329008, "cdate": 1761815329008, "tmdate": 1762924328277, "mdate": 1762924328277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an attention-aware defense mechanism designed to mitigate the impact of knowledge attacks on Retrieval-Augmented Generation systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and straightforward to understand.\n\n2. The authors perform experiments to demonstrate the effectiveness of the proposed defense method."}, "weaknesses": {"value": "1. The threat model assumed in the paper is strong.\n\n2. The proposed approach may fail when $\\epsilon$ exceeds 0.5.\n\n3. Several existing attack and defense methods are not included in the evaluation."}, "questions": {"value": "1. The paper assumes that the defender has full access to the model’s internal attention matrices and can compute multi-layer attention scores. This assumption is impractical for many real-world RAG systems that rely on closed-source APIs (e.g., GPT-4, Claude). Although the authors mention experiments with a “closed-source” setup, the proposed defense still depends on internal attention data, which limits its applicability in practical deployments.\n\n2. In the threat model, the authors assume that the attacker can inject only a few poisoned passages into the top-k retrieval results, with $\\epsilon < 0.5$. This is a strong assumption. Prior works [a][b] generally assume that the attacker can poison only a small portion of the overall knowledge database. For example, in [a], for each targeted query, five poisoned passages are injected while considering the top-5 retrieval results, meaning that the fraction of poisoned passages $\\epsilon$ can exceed 0.5 or even reach 1.0 (all retrieved passages are poisoned).\n\n3. In the experiments, the authors evaluate the defense under a mild adversarial setting where the corruption rate $\\epsilon = 0.1$ and top-10 results are retrieved. This setting is too weak. Following the previous comment and the setups in [a][b], it would be more convincing to report results when five poisoned passages are injected per query and top-5 results are retrieved in the RAG framework.\n\n4. Several recent attacks specifically targeting RAG systems have been proposed, such as [b][c][d]. The authors should also evaluate the robustness of their defense against these more advanced attacks.\n\n5. Numerous new defenses have also been developed. The authors should include comparisons with these recent defense methods, such as [e][f], to provide a more comprehensive evaluation.\n\n6. As discussed earlier, the corruption rate $\\epsilon$ can exceed 0.5 in top-k retrieval results. Therefore, for Figure 5, it would be helpful to also include experimental results for larger $\\epsilon$ values, such as 0.7, 0.9, or 1.0.\n\n\n\n[a] PoisonedRAG Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models.\n\n[b] Practical Poisoning Attacks against Retrieval-Augmented Generation.\n\n[c] FlippedRAG Black-Box Opinion Manipulation Adversarial Attacks to Retrieval-Augmented Generation Models.\n\n[d] Machine Against the RAG Jamming Retrieval-Augmented Generation with Blocker Documents.\n\n[e] Certifiably Robust RAG against Retrieval Corruption.\n\n[f] Traceback of Poisoning Attacks to Retrieval-Augmented Generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pSlu0U4pYP", "forum": "PS43wqCSME", "replyto": "PS43wqCSME", "signatures": ["ICLR.cc/2026/Conference/Submission13794/Reviewer_oHcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13794/Reviewer_oHcu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213684647, "cdate": 1762213684647, "tmdate": 1762924327663, "mdate": 1762924327663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates defenses against data poisoning attacks in retrieval-augmented generation (RAG) systems. The key observation is that existing poisoning attacks tend to lack stealth: tokens in the target responses specified by such attacks exhibit disproportionately high correlations with the poisoned input passages. Building on this observation, the paper introduces the Normalized Passage Attention Score (NPAS), which measures each retrieved passage’s contribution to the response based on the attention weights within the LLM component of the RAG system. By analyzing the variance of NPAS across retrieved passages, they formulate a Stealth Attack Distinguishability Game (SDAG) that identifies potentially corrupted or poisoned passages and further proposes an attention-variance filter to filter out those corrupted passages. Experiments demonstrate the effectiveness of the proposed defense against the state-of-the-art defense baseline across five attacks on four QA benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-presented overall, with the problem setup and attack model clearly defined and formalized, which significantly aids reader comprehension of both the context and the proposed defense.\n\n2. Developing defenses against data poisoning attacks in retrieval-augmented generation (RAG) is a problem of high practical relevance.\n\n3. While the general idea of leveraging attention scores to detect correlations between poisoned inputs and target outputs as a defense method is not entirely new, its adaptation to the RAG defense setting could potentially be interesting."}, "weaknesses": {"value": "1. Several aspects of the proposed defense method described in Section 4 require further clarification (see the specific questions below). \n\n2. The experiment results in terms of attack success rate (ASR) do not seem to be strong. In particular, Table 2 compares the proposed defense with the state-of-the-art baseline, Certified Robust RAG [Xiang et al., 2024], but the results indicate that the proposed method yields higher ASR values in most settings, suggesting weaker defense effectiveness. For example, when defending against the Poison attack, across five models evaluated on three datasets, the proposed defense achieves an equal ASR compared to the baseline in only one case."}, "questions": {"value": "Regarding Section 4 on the proposed defense:\n\n1. In the subsection “Discriminating Bbetween Corrupted and Benign Retrievals via Attention”, how are the two sets of retrieved passages constructed? Is the assumption that one set contains only benign passages while the other contains corrupted ones? What if both sets are corrupted?\n\n2. In lines 312-313, the paper assumes that “the attention distribution over passages is approximately uniform with a slight recency effect.” How valid is this assumption for extractive QA queries, where attention typically concentrates on the passage region containing the correct answer? Would such cases undermine the proposed discriminative mechanism?\n\n3. In line 332, it is stated that sorting passages by attention scores helps reduce the recency effect. Could the authors clarify why and how this sorting step mitigates the effect?\n\n4. How to choose the $\\alpha$ threshold to compute NPAS in practice?\n\n------\n\nOther questions:\n\n5. In Table 1, the Clean Accuracy of the proposed method with Deepseek-R1 and Llama-3.1 exceeds that of the Vanilla RAG on the RQA and NQ datasets, respectively. Would it be possible to explain why this is the case here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KSPL9kw4Fm", "forum": "PS43wqCSME", "replyto": "PS43wqCSME", "signatures": ["ICLR.cc/2026/Conference/Submission13794/Reviewer_sWjW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13794/Reviewer_sWjW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762555050686, "cdate": 1762555050686, "tmdate": 1762924327060, "mdate": 1762924327060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}