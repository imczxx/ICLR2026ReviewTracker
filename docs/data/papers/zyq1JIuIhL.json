{"id": "zyq1JIuIhL", "number": 13342, "cdate": 1758216785294, "mdate": 1762964929140, "content": {"title": "DoMiNO: Down-scaling Molecular Dynamics with Neural Graph Ordinary Differential Equations", "abstract": "DoMiNO: Down-scaling Molecular Dynamics with Neural Graph Ordinary Differential Equations", "tldr": "", "keywords": ["Multi-scale Modeling", "Molecular Dynamics", "Neural ODE"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/99983c740e057ab5240b1e4426d5c4a9fe111da6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a hierarchical neural graph ode to simulate molecular trajectories on K different time scales, resulting in a reduced time complexity scaling, from O(T) down to O(KT^1/K) with K=3 in practice. The model uses K distinct graph neural networks that are integrated in time, and the latent features of the previous coarsest time scales feed into the features of the finer timescale. The authors show reduced mean-squared errors in forward time step prediction compared to a handful of time series prediction models on some small-scale molecular benchmarks of single-molecule trajectories."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors tackle an important problem with a creative and, in principle, intuitive idea. The reduced scaling from O(T) down to O(KT^1/K) is significant and the empirical results are encouraging"}, "weaknesses": {"value": "While the idea has potential and initial results are encouraging, i am currently not convinced that the results generalize to practical settings:\n\nOnly small-scale single-molecule trajectories are used, and the model only evaluates MSE. This leaves several open questions:\n1. How does the model transfer between molecules?\n2. How does the model generalize to larger more interesting systems?\n3. Can the model successfully predict transitions between different conformations not seen during training? Can the model, for example, simulate the folding of proteins or reactions of molecules?\n4. Currently, there is no temperature or initial velocity input. The velocity/temperature is implicitly inherited from the training data. This is a very limited setting, in particular for non-biological tasks where we dont want to retrain models for each new temperature setting.\n5. As there is no notion of an energy function, the learned ode is not enforced to be conservative. Therefore, there are no guardrails against energy drifts, and the model sampling energetically inaccessible states.\n6. No real-time comparisons or memory measurements are given. This makes it unclear how significant the time savings really are\n7. No thermodynamical observables are calculated. For example, the radius of gyration and time correlation functions would be easy out-of-the-box observables that could be reported\n8. The models are trained with 10% of MD17 frames, which amounts to tens to almost a hundred thousand training examples if the pytorch geometric datasets are being used. The dataset authors specify that not more than 1000 samples should be used to avoid data leakage: https://archive.materialscloud.org/records/pfffs-fff86\n9. The only baseline model that is explicitly designed for the prediction of large time steps in molecular systems is ITO. However, ITO is a distribution-level model; it tries to predict distributions at a time in the future, not individual samples. Comparing MSE of individual samples is therefore not very meaningful and inflates ITO's error. A fair comparison would need to sample some initial distribution, push the ensemble of states forward in time with DoMiNO and the distribution with ITO, and then compare the models on statistical divergence metrics, not MSE."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vzanZOtJ1N", "forum": "zyq1JIuIhL", "replyto": "zyq1JIuIhL", "signatures": ["ICLR.cc/2026/Conference/Submission13342/Reviewer_LGqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13342/Reviewer_LGqt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761106077723, "cdate": 1761106077723, "tmdate": 1762923997449, "mdate": 1762923997449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "On behalf of all co-authors, we respectfully withdraw this submission. After internal discussion about our dissemination plan, we decided to consolidate this line of work and pursue a venue whose format is better aligned with the scope of the contribution. We remain confident in the work and sincerely thank the reviewers and area chair for their time and engagement."}}, "id": "oYkdURb7W6", "forum": "zyq1JIuIhL", "replyto": "zyq1JIuIhL", "signatures": ["ICLR.cc/2026/Conference/Submission13342/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13342/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964928341, "cdate": 1762964928341, "tmdate": 1762964928341, "mdate": 1762964928341, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a framework for learning molecular dynamics trajectories. The core idea is to use multiple Neural ODEs that target different time step sizes. The initial state (atom positions) is encoded and solved by the coarsest Neural ODE (largest timesteps). The resulting latent states are then passed on to the Neural ODEs at finer levels (smaller timesteps). The latents at each level are combined using attention to output the final predictions. Each Neural ODE consists of EGNN layers. The authors evaluate on MD17 and MD22 molecules."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors report a significant reduction in MSE (although it is unclear to me of what, see weaknesses below)\nComposing neural graph ODEs that specialize on different time step sizes is interesting and novel"}, "weaknesses": {"value": "1. Clarity of the experiments: \n- In table 1, what is the MSE of? Average in the difference in atom positions over a rollout of a certain length?\n- The main motivation of the paper is that MD with MLIPs are slow and accumulate error. This claim is not properly verified or compared to their method. Inference time is not compared, and the error drift experiments lack a proper MLIP and do not mention important parameters like time step size\n2. I am unsure how technically sound the idea of multiple Neural ODEs at different time step sizes is. Shouldn’t Neural ODEs be implicit representations of the continuous dynamics?\n3. The approach requires training data from trajectories, while MLIPs can be trained with unordered samples from e.g. MC?"}, "questions": {"value": "1. Can you show experiments comparing the and drift error over the steps of you method vs MD with a SOTA MLIP like Mace/Equiformer/LeftNet? EGNN is a long outdated architecture from over four years ago\n2. How does the inference time of your method compare to the baselines (or just an MLIP+MD)?\n3. What time step size did you use for MD? This should strongly affect the drift error\n4. How does the training time of the methods compare? Training the neural ODE should be significantly more expensive\n5. Do the hierarchy levels improve results even if a single hierarchy model is matched in terms of training compute and memory (i.e. increasing the parameters)?\n6. There is a broken citation of MD22 in line 713"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TYx95I3uei", "forum": "zyq1JIuIhL", "replyto": "zyq1JIuIhL", "signatures": ["ICLR.cc/2026/Conference/Submission13342/Reviewer_STZw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13342/Reviewer_STZw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834655886, "cdate": 1761834655886, "tmdate": 1762923996870, "mdate": 1762923996870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DoMiNO, a hierarchical neural ODE framework for molecular dynamics (MD). The key idea is to decompose molecular motion into multiple temporal scales, each modeled by an E(n)-equivariant Graph ODE that captures dynamics from slow global motions down to fast local vibrations. These levels are fused via an attention mechanism to reconstruct molecular trajectories. The authors evaluate DoMiNO on standard MD datasets like MD17 and alanine dipeptide, showing large gains in both prediction accuracy and long-term stability compared to baselines like EGNN, EGNO, and ITO. Ablations highlight the importance of hierarchical decomposition and local time normalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-The paper clearly identifies and tackles a central challenge in MD — the multi-scale nature of atomic dynamics — and provides a coherent neural ODE-based solution.\n- The hierarchical ODE formulation is conceptually elegant, with each level operating in its own local time scale. This is a nice balance between coarse- and fine-grained temporal modeling.\n- The model is physically grounded, maintaining SE(3) symmetry through equivariant GNNs and decoding steps.\n-The empirical results are strong and comprehensive, covering both small molecules and larger systems. The performance improvements (especially on benzene/toluene) are quite impressive.\n-The authors provide detailed ablations and implementation details, which improves credibility and reproducibility."}, "weaknesses": {"value": "-While the decomposition idea is solid, the connection to physical timescales (e.g., mapping levels to specific frequencies or normal modes) remains largely heuristic. There’s no explicit analysis linking learned scales to real physical processes.\n- The paper is heavy on architectural details but light on intuition for why attention fusion is the right way to combine scales. It could use some visualization of learned weights or interpretability results beyond benzene/toluene.\n- The evaluation, while broad, focuses mainly on predictive accuracy. There are no experiments showing practical utility for sampling, free-energy estimation, or integration with existing MD workflows.\n- Computational cost is mentioned, but no direct runtime comparison versus baselines is shown.\n- It’s unclear how well DoMiNO generalizes across molecular systems, or whether it needs retraining for every molecule type."}, "questions": {"value": "1: How sensitive is the model to the choice of number of hierarchical levels or the relative time intervals between them?\n2: Can the learned timescales be interpreted physically — e.g., do they correspond to specific vibrational or conformational modes?\n3: Could the authors compare computational efficiency (e.g., wall-clock time) with neural ODE baselines or generative MD methods like ITO?\n4: Since only a single initial frame is used, how robust is the model when initial configurations are noisy or sampled from different conditions?\n5: Could this method be combined with energy conservation constraints or learned force fields to improve physical fidelity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GzQvz15a1A", "forum": "zyq1JIuIhL", "replyto": "zyq1JIuIhL", "signatures": ["ICLR.cc/2026/Conference/Submission13342/Reviewer_JFsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13342/Reviewer_JFsL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846791164, "cdate": 1761846791164, "tmdate": 1762923996331, "mdate": 1762923996331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hierarchical multi-scale Neural Graph ODE framework for simulating molecular dynamics: an equivariant encoder projects atomic states to latents,  evolve dynamics in the coarse latent space via neural ODE and evolve at a local time-scale for finer level space, and then an attention fuser merges all levels to reconstruct coordinates. The design aims to resolve the small-timestep vs large-timestep dilemma by letting each level specialize in a characteristic timescale. Benchmark shows competitive MSE and slower error growth on datasets like MD17, alaine dipeptide."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and easy to follow.\n\n* The paper proposes to use an SE(3)-equivariant encoder/decoder with different spatio-temporal levels of graph neural ode to capture different scale of physics and achieve good computational effciency.\n\n* On MD17/ALA2 and larger systems, DoMiNO shows slower error growth than baselines across extended trajectories, indicating better stability beyond short-term fit."}, "weaknesses": {"value": "As MD dynamics are chaotic, matching a single deterministic trajectory quickly becomes ill-posed; long-horizon MSE on coordinates is therefore not a meaningful objective beyond short transients. What typically matters are ensemble/statistical properties (e.g., RDFs, energy drift, diffusion constants, autocorrelation times, free-energy landscapes) and long-term stability. An example of more practical evaluation is Fu et al. [1]—running sustained simulations and assessing thermodynamic/kinetic statistics with appropriate confidence intervals—rather than emphasizing single-trajectory reconstruction.\n\n[1] Fu, Xiang, et al. \"Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations.\""}, "questions": {"value": "Can the method retain its accuracy and long-horizon stability on substantially larger, multi-molecule systems (e.g., solvated biomolecules with periodic boundaries)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EfU6BEo5WF", "forum": "zyq1JIuIhL", "replyto": "zyq1JIuIhL", "signatures": ["ICLR.cc/2026/Conference/Submission13342/Reviewer_MiTQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13342/Reviewer_MiTQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991158934, "cdate": 1761991158934, "tmdate": 1762923995994, "mdate": 1762923995994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}