{"id": "sRDCzh6gVw", "number": 20624, "cdate": 1758308310613, "mdate": 1759896967285, "content": {"title": "Weight-Aware Meta Auxiliary Learning", "abstract": "Auxiliary Learning (AL) is a form of Multi-Task Learning in which a model leverages auxiliary tasks to improve performance on a primary task. AL has boosted performance across multiple domains, including navigation, image classification, and natural language processing. One of the main weaknesses of AL is the need for labeled auxiliary\ntasks, which can require human effort and domain expertise to generate. Furthermore, it has been shown that not all auxiliary tasks are equally beneficial to aid primary task performance. Therefore, deciding how to weight an auxiliary task or sample during training is also a hard problem. Recent work addresses the task-creation problem by learning auxiliary labels using Meta Learning approaches, often via bi-level optimization. However, these methods assume uniform weighting across data points. Other works present selecting weights for known tasks. In this work, we propose Weight-Aware Meta Auxiliary Learning (WAMAL), a novel framework that jointly learns both auxiliary labels and per-sample auxiliary loss weights to better guide the main task. Our method improves upon existing approaches by allowing more nuanced and adaptive task supervision. Across multiple benchmarks WAMAL surpasses both handcrafted auxiliaries and prior meta-auxiliary baselines. On CIFAR-100 (20 super-classes, VGG16) it reaches 80.2\\% test accuracy (+5.6 pp over human-designed auxiliaries; +2.8 pp over weight-unaware meta-learning). When fine-tuning ViT-B/16 on Oxford-IIIT Pet, WAMAL improves accuracy by 0.62 pp. These results underscore the importance of learning both which auxiliary tasks to use and how strongly to weight them at the sample level. Code repo will be released after submission. Anonymized version: \\url{https://anonymous.4open.science/r/wamal-66EF/README.md}.", "tldr": "", "keywords": ["Meta Learning", "Auxiliary Learning", "Sample Weighing", "Multi task learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a69e5b8032c7d6797dd7c6b0fa958c7c008eee7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper considers Auxiliary Learning (AL), a method for improving a model’s primary task by using additional auxiliary tasks for extra supervision. It proposes weight-aware meta auxiliary learning (WAMAL), which jointly learns auxiliary labels and per-sample weights through bi-level optimization. The paper focuses on image classification tasks and provides experiments on several datasets, which show improved performance compared to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is easy to read.\n1. WAMAL introduces the first approach that explicitly learns both auxiliary labels and per-sample weights.\n1. Experiments show the method outperforms the single-task and MAXL baselines."}, "weaknesses": {"value": "This paper presents several notable weaknesses in terms of novelty, experimental depth, and presentation quality. Overall, the work feels too preliminary.\n\n1. My concern is the paper’s novelty: The conceptual and technical novelty beyond prior work, particularly MAXL (Liu et al., 2019) and AuxiLearn (Navon et al., 2020), appears modest. The proposed WAMAL framework primarily extends MAXL by introducing learned per-sample auxiliary weights, but the underlying optimization paradigm remains largely unchanged. Furthermore, AuxiLearn has already shown that learning task weights is equivalent to per-datum adaptive weighting, and experimented with learning both auxiliary tasks and their respective weights. \n1. Experimental evaluation is limited for a pure empirical paper: \n    - Task diversity: All experiments are confined to image classification. The work would benefit from evaluations on additional modalities (e.g., text) or tasks (e.g., semantic segmentation, depth estimation etc.).\n    - Baselines:  In the main text, WAMAL is compared only to single-task learning and MAXL, i.e., a single auxiliary-learning baseline. This is insufficient for a fair empirical study. While the appendix briefly includes AuxiLearn, comprehensive comparisons should appear in the main text and across all datasets. Please add additional AL baselines to the main text.\n1. The related work section omits several important and conceptually relevant works:\n     - Auxiliary Learning with Joint Task and Data Scheduling, ICML 2022.\n     - Auxiliary Learning as an Asymmetric Bargaining Game, ICML 2023.\n     - Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting, NeurIPS 2019. (for learned per-sample weighting)\n     - And some more classic AL papers like: Adapting Auxiliary Losses Using Gradient Similarity (2018), Adaptive auxiliary task weighting for reinforcement learning (NeurIPS 2019), etc."}, "questions": {"value": "Other Concerns:\n1. Paper structure: Some implementation details (for example, the section on the Python wrapper) are better suited for the appendix, freeing space in the main text for more substantial empirical results and discussion.\n1. Presentation: Important experimental results appear only in the appendix (with relevant text in the main paper) and should be moved to the main paper for clarity and impact.\n1. Computational cost: The proposed framework introduces computational overhead (additional network and bi-level optimization). The paper does not report or analyze this additional cost, which may limit the method’s practicality and scalability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Authors discuss ethics-related concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GVYJuic0Kn", "forum": "sRDCzh6gVw", "replyto": "sRDCzh6gVw", "signatures": ["ICLR.cc/2026/Conference/Submission20624/Reviewer_BxVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20624/Reviewer_BxVj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760561455882, "cdate": 1760561455882, "tmdate": 1762934027248, "mdate": 1762934027248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Weight-Aware Meta Auxiliary Learning (WAMAL), a bi-level optimization framework that simultaneously generates auxiliary labels and per-sample weights for auxiliary learning. The method is evaluated on standard image classification datasets including CIFAR-100-20, SVHN, Oxford-IIIT Pet, Food-101, and CUB-200 using VGG16, ResNet50, and ViT-B/16 architectures. The authors report consistent accuracy improvements over single-task training and meta-auxiliary baselines. An open-source wrapper is provided to convert existing image classifiers into WAMAL-ready networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. Consistent but small accuracy gains on CIFAR-100-20, SVHN, Pets, Food-101 when trained from scratch or fine-tuned."}, "weaknesses": {"value": "1. Motivation. Auxiliary learning method is designed to decide which label to use (mentioned in motivation 1), I don't this method can interpret the question better than previous auxiliary methods.\n2. The designed method is simple, thus it lacks novelty.\n3. In the experiment part, the authors only choose two baselines to be compared, which makes the result not convincing.\n4. In Figure 3, the authors paint high/low weight picture in different datasets, but it is hard to tell why the top ones should have low weight while the bottom ones should have high weights."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0uBAMotsu1", "forum": "sRDCzh6gVw", "replyto": "sRDCzh6gVw", "signatures": ["ICLR.cc/2026/Conference/Submission20624/Reviewer_eejB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20624/Reviewer_eejB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751353649, "cdate": 1761751353649, "tmdate": 1762934026769, "mdate": 1762934026769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes weight-aware meta auxiliary learning, which simultaneously learns auxiliary labels and the auxiliary loss weight for each sample to enhance the performance of the main task. This method addresses the reliance of existing auxiliary learning approaches on manually annotated auxiliary tasks and the uniform weighting of auxiliary losses across all samples. By enabling more fine-grained and adaptive supervision, the proposed approach can effectively guide the main task and achieve improvements over previous methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed two-level optimization framework and sample-adaptive weighting can effectively enhance the performance of the method, achieving relatively good results.\n- The paper is relatively well-organized and easy for subsequent work to follow."}, "weaknesses": {"value": "- The abstract devotes a large portion to describing the background and issues of existing methods. However, it provides insufficient description of the innovations and contributions of this work, making it less compelling.\n- Figure 1 appears somewhat confusing; it does not clearly convey the interaction between the weights of the label network and the main network. How does it function? The current single arrow without any explanation is unclear. In addition, some extra explanation should be added to aid understanding of the label generation process.\n- It is not surprising that adding an extra weight from the samples to the network would improve performance. The authors experimentally demonstrate the effectiveness of distinguishing the number of categories through parameter settings, but is there any theoretical support for this? The authors should clarify.\n- In the auxiliary weight supervision, how is the weight obtained? How is it ensured that this weight accurately reflects the importance of the sample?"}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GNZtxGWAvT", "forum": "sRDCzh6gVw", "replyto": "sRDCzh6gVw", "signatures": ["ICLR.cc/2026/Conference/Submission20624/Reviewer_qd7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20624/Reviewer_qd7C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830695731, "cdate": 1761830695731, "tmdate": 1762934025993, "mdate": 1762934025993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary\n\nThis paper falls within the area of Auxiliary Learning, a subfield of Multi-Task Learning (MTL) aimed at improving the generalization of a main task by leveraging auxiliary tasks.\nThe authors propose Weight-Aware Meta Auxiliary Learning (WAMAL) to address two challenges:\n\nMitigating negative interference from auxiliary tasks.\n\nHandling the lack of labeled data for auxiliary tasks.\n\nThe approach combines sample-level weighting, which is known to be more robust to noise than task-level weighting, with the Meta Auxiliary Learning (MAXL) algorithm.\nExperiments compare WAMAL against MAXL and single-task learning baselines on two datasets, including fine-tuning analyses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\nThe idea of combining sample-level weighting and auxiliary-task label generation is novel.\n\nThe reported results are favorable for WAMAL, showing improved generalization over single-task learning and MAXL.\n\nThe authors include some ablation studies that analyze hyperparameter ranges and optimizer choice."}, "weaknesses": {"value": "Weaknesses\n\nThe connection to prior weighting algorithms is underdeveloped. MTL methods often use gradient similarities to guide task weighting; this relationship should be clearly discussed.\n\nThe rationale for sample-level weighting is unclear. The design motivation and computation of these weights need to be explicitly justified.\n\nBenchmarking is limited, with comparisons to only MAXL. Including additional auxiliary-learning baselines (both in datasets and weighting schemes) would make the evaluation more convincing.\n\nThe structure of the paper is confusing. Related work and methodology are interwoven, making it hard to identify what is new relative to MAXL.\n\nWriting and notation require refinement. For example, the paper switches between upper- and lowercase L  (possibly total vs. sample-level weights) without clear definition or aggregation details.\n\n\nWhile the method’s motivation is reasonable, the design decisions are weakly justified and the contribution is largely compositional.\nThe paper does not convincingly explain why these specific components (sample-level weighting and MAXL) belong together or how they interact.\nGiven the modest novelty, limited evaluation, and writing issues, the contribution currently lacks the rigor needed for acceptance."}, "questions": {"value": "Questions for the Authors\n\nWhat is the reasoning behind the sample-level weighting method? How does it relate to other sample-level weighting approaches in single- or multi-task learning?\n\nIs it surprising that the model learns an approximately normal distribution of weights? Would randomly sampled weights yield similar results, as sometimes observed in MTL?\n\nCan you quantitatively verify that lower weights correspond to noisier samples?\n\nWhat are the exact differences between MAXL and WAMAL beyond the second head predicting sample-level weights? Please clarify this in Section 3.1.\n\nWhat is the theoretical motivation for Equation (4)?\n\nHow do bounded weights prevent dominance or vanishing of the auxiliary term? Can this be shown empirically?\n\nCould the framework easily incorporate alternative weight generation mechanisms?\n\nAdditional Feedback\n\nThe writing requires improvement: the abstract and literature review are too limited and occasionally repetitive. The tone could be more formal.\n\nThe structure should be revised to clearly separate related work from methodology.\n\nThe repository link was not functional.\n\nThe paper should explicitly list its key contributions and clarify which components are new.\n\nThe literature review lacks nuance: sample-level weighting is not a standard practice in auxiliary learning and should not be presented as such."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T62ZQ4fzVG", "forum": "sRDCzh6gVw", "replyto": "sRDCzh6gVw", "signatures": ["ICLR.cc/2026/Conference/Submission20624/Reviewer_xXBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20624/Reviewer_xXBi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924451178, "cdate": 1761924451178, "tmdate": 1762934025119, "mdate": 1762934025119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}