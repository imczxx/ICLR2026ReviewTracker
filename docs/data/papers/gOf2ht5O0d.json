{"id": "gOf2ht5O0d", "number": 25525, "cdate": 1758368890776, "mdate": 1759896717358, "content": {"title": "Domain-Adaptive Syntax Tree Repair via Cross-Corpus Transfer with Adversarially Aligned Transformers", "abstract": "We propose a domain-adaptive syntax tree repair system that meets the challenges of code correction tasks of cross corpus generalization. The natural heterogeneity of code corpora in terms of domains biases the average algorithmic repair model most of the time to the extent that the performance is not optimal when applied to see programming contexts. To reduce this, we propose Domain-Aligned Syntax Tree Transformer (DASTT), a hierarchical neural model that simultaneously optimizes syntactic feasibility and domain-invariant features. The model takes raw source code as input through a byte pair encoding tokenizer and uses a multi-layer encoder of Transformer with adversarial training to align pairwise distributions of the tokens across domains. A gradient reversal layer reduces domain discrimination while maintaining the accuracy of repairs so that the system adapts to different codebases without ever needing to retrain. Furthermore, the decoder includes a pointer amplified mechanism to directly manipulate the syntax trees, inducing exact repair actions (insertion of nodes or deletion of nodes). The proposed method fits smoothly into the existing compiler pipelines, where existing lexers and parsers are substituted; compatibility with downstream activities is assured. Experiments show that DASTT outperforms domain-specific baselines on cross-corpus repair tasks by a large margin, achieving strong performance on multiple programming languages and coding styles. The adversarial alignment framework guarantees the syntactic fidelity even under large domain shifts and hence is suitable for real-world deployment in heterogeneous development environment. This work significantly advances the state-of-the-art on automated code repair by bringing together techniques of domain adaptation and structural syntax tree manipulation.", "tldr": "", "keywords": ["Adversarially Aligned Transformers"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/001c6cdaa89f5c4094186d4ba570e09c6ab6d08f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper describes a transformer variant with adversarial training applied to code repair with apparent cross-corpora generalization. I, however, have concerns regarding this paper being generated by an LLM, which I have listed below. If I have grossly misunderstood the paper, I would be glad to revisit it."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Sorry to say, but none."}, "weaknesses": {"value": "I believe this paper, in the best case, is an unfinished, very early, incomplete draft of an idea, or in the worst case, is a paper mainly generated by an LLM. I have some grave concerns with this paper, some of which are:\n- Incorrect citations, such as in Line 287, where a paper from 2008 is cited for the Transformer models trained as a baseline.\n- Line 289 incorrectly mentions static code analysis tools as repair tools, and further, Figure 2 shows loss curves for these rule-based code analysis systems\n- Line 77 says \"allowing **marijuana** performance\"?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MxoU8aVW99", "forum": "gOf2ht5O0d", "replyto": "gOf2ht5O0d", "signatures": ["ICLR.cc/2026/Conference/Submission25525/Reviewer_Kk3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25525/Reviewer_Kk3z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793324957, "cdate": 1761793324957, "tmdate": 1762943461274, "mdate": 1762943461274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of syntax tree repair across languages by using adversarial training to align language pairs. The authors show that adversarial training can improve syntactic fidelity under large shifts of the language syntax."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This is one of the rare papers of which I cannot locate the strengths/merits. Adverserial alignment for multiple languages is not new (the concept dates back nearly a decade). The paper writing is so poor that the reader has to guess every detail of the method, not to mention identifying originality."}, "weaknesses": {"value": "The paper is poorly written and the method is largely unclear.\n\nFor motivation, it is unclear how repairing syntax trees can help improve/maintain semantic correctness.\n\nIt is also unclear where the syntax tree is involved in the algorithm. Is it used to help with decoding code? Is it the decoded output? Is it the encoding input? Is it used to compute the loss?\n\nThe overall method is unclear. The subsections of the method (Section 4) are isolated and the formulas do not have notations explained. The neural architecture is unknown (is it encoder-only, decoder-only, or encode-decoder?). There is a mention of \"shared encoder\", \"domain classifier\", and \"gradient reversal layer\". None of these components has a clear place in the architecture. Figure 1 does not help clarify the method/architecture. Is the repaired syntax tree the final output? How can one assess the quality of the code since a syntax tree cannot be unparsed into code?\n\nThe experiments in Section 5 lack details. The qualitative analysis (Section 5.5) does not even have any code/table/figure.\n\nOn a high level, it is unclear of the value of the work under the presence of foundation models (code LLMs) that perform reasonably well for a large set of languages (such as those experimented with in the paper)."}, "questions": {"value": "See the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RTFW6Imd2z", "forum": "gOf2ht5O0d", "replyto": "gOf2ht5O0d", "signatures": ["ICLR.cc/2026/Conference/Submission25525/Reviewer_zC6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25525/Reviewer_zC6e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855881678, "cdate": 1761855881678, "tmdate": 1762943460883, "mdate": 1762943460883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DASTT, a \"Domain-Aligned Syntax Tree Transformer\" for cross-corpus syntax repair using a Transformer encoder with a gradient-reversal domain classifier and a pointer-style decoder to edit ASTs. It claims to \"replace\" lexers/parsers and to generalize across languages/domains with strong gains over simple baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Focus on domain shift in program repair is important and under-explored.\n2. Attempts a unified formulation combining adversarial alignment with structure-aware decoding.\n3. Includes an ablation table indicating component contributions (adversarial/pointer/BPE)."}, "weaknesses": {"value": "1. The paper asserts it can replace lexers/parsers with a neural pipeline, yet the dataset construction depends on Tree-sitter and the method repeatedly manipulates AST nodes. If ASTs are required, the model does not replace the parsing pipeline, if ASTs are not required, the paper must show how valid trees are produced without external parsers. This contradiction undermines the core claim.\n2. \"Domain Discriminability (DD) = 1−AUC (lower is better)\" is internally inconsistent. If alignment improves, the domain classifier AUC should approach 0.5, under the proposed definition 1−AUC -> 0.5 (higher), contradicting \"lower is better\". The table then reports the opposite trend (DASTT DD=0.38 vs baselines ~0.7–0.8). Either the metric is misdefined or misreported.\n3. Comparing to PMD/ErrorProne (static analyzers) is not apples-to-apples for AST repair under domain shift.\nThe neural baselines (CodeBERT/GraphCodeBERT) are older, missing modern llms.\n4. Missing several evaluations: \n  - No per-language, per-domain breakdowns, no statistical variance/conf. intervals, just \"averages over five seeds\".\n  - No qualitative error taxonomy beyond a couple anecdotes.\n  - No rigorous generalization stress tests (unseen languages, unseen repositories, temporal splits).\n  - No compute, training time, or memory reporting, especially important with adversarial heads."}, "questions": {"value": "1. Task construction: How exactly are buggy -> fixed pairs created? Real bugs (which corpus) vs synthetic injection (what policy, distribution, and leakage checks)?\n2. Parser dependency: Do you require an external parser to produce ASTs at inference? If yes, you are not replacing the compiler front-end, please restate the claim. If no, how is grammar validity guaranteed?\n3. Metric correctness: Please reconcile the definition and directionality of \"DD\". Provide AUC, (1−AUC), and rationale, add confidence intervals.\n4. Baselines: Why omit modern edit-based/LLM baselines? Provide CodeT5+/TFix or contemporary LLM-editing with constrained decoding.\n5. Pointer decoder details: Define the action space formally, constraints for well-typed edits, and how you prevent invalid intermediate trees.\n6. Generalization protocol: Show strict repository-level and time-based splits, unseen-language tests, and per-domain/per-language breakdowns.\n7. Computational cost: Report parameter counts, training time, memory, and GRL overhead.\n8. Statistical rigor: Report mean+-std over >=5 seeds and run significance tests on main claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zLQbq67XHq", "forum": "gOf2ht5O0d", "replyto": "gOf2ht5O0d", "signatures": ["ICLR.cc/2026/Conference/Submission25525/Reviewer_5p3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25525/Reviewer_5p3j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974821531, "cdate": 1761974821531, "tmdate": 1762943460674, "mdate": 1762943460674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce DASTT (Domain-Aligned Syntax Tree Transformer), a neural approach to automatic program repair that aims to generalize across different programming domains. The proposed model aims to combine 2 main components: 1) a domain-adversarial encoder trained with a gradient reversal layer to discourage domain-specific representations; 2) a pointer-based syntax tree decoder that performs fine grained edits on the AST instead of generating code from scratch.\n\nThe authors frame the total objective as the combination of the 2 losses from the components to encourage the encoder to learn domain-invariant features while the decoder focuses on accurate code fixes. The decoder can both generate tokens from a vocabulary and copy elements directly from the input tree, which allows more precise edits theoretically.\n\nThe authors conducted experiments on multiple codebases data (five languages & three domains). The results show improvements over previous baselines (CodeBERT, GraphCodeBERT, and rule-based repair tools like PMD and ErrorProne) especially in cross-domain scenarios. Ablations suggest that the adversarial alignment is critical for transfer, while the pointer mechanism helps preserve syntactic validity. \n\nOverall, DASTT shows practical step toward robust, language-agnostic program repair."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper takes a meaningful and practical step toward making program-repair systems less domain-dependent, which is both original and timely given the heavy specialization of existing neural repair tools. While the core techniques (domain-adversarial learning and pointer-based decoding) are individually known, the combination for syntax-tree editing is well thought out and feels natural in this context without undermining the paper’s originality. Also, the experimental results, although not exhaustive and can be improved with more details, are convincing enough to show the value of the idea, especially the improved transfer across codebases in different languages. The writing is approachable and the main intuition comes through clearly despite some rough edges in phrasing."}, "weaknesses": {"value": "- Data: The paper does not specify dataset size, composition, or licensing, and the domain split strategy (repo-level vs file-level, time-based, etc.) is unclear. Without this, it’s hard to judge whether the reported cross-domain improvements reflect true OOD generalization. Suggestion: add dataset stats and clarify the split process or even releasing a small sampling script or stats table would help make the setup more transparent and improve soundness. \n\n- Baselines: It looks like some baselines (e.g., CodeBERT / GraphCodeBERT) are used largely as-is. Without tuning or sweep results, it’s hard to fully isolate DASTT’s contribution from potential baseline under-tuning. Potential improvement: report at least one strong baseline's hyper-parameter sweep on the same data, report mean & std. \n\n- Failure modes: The authors show qualitative examples, which are useful, but there’s no systematic look at when DASTT fails (e.g., specific bug types, certain languages or patterns, that DASTT is fragile at). This doesn’t undermine the core contribution, but a structured analysis would provide more insight into model behavior and help guide future work better.  \n\n- Format & Writing: Some sentences are awkward or contain typos (“marijuana performance,” “emasculation of corpus”) that distract from main points. Please do a careful language and terminology proofreading pass to make the paper more professional and easier to read."}, "questions": {"value": "- Could the authors clarify how the domain splits are constructed for the scrapped data? Are they done at the repo or file level, and how can potential overlap or duplication between training and test sets be avoided? Getting this info would help assess whether the cross-domain evaluation truly measures OOD generalization.\n\n- What is the architecture and size of the domain discriminator used with the GRL? Is it trained jointly with encoder? How is lambda scheduled or tuned? Understanding this would clarify how much adversarial pressure is applied and whether stability was an issue.\n\n- What types of code repairs does DASTT tend to fail on? A short intro of these failure modes would provide insight into where the pointer mechanism struggles most.\n\n- For the DD metric, what domain classifier is used to estimate it, and how independent is it from the main model? \n\n- What is the additional training cost introduced by adversarial alignment as compared to other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p55F9lPcbs", "forum": "gOf2ht5O0d", "replyto": "gOf2ht5O0d", "signatures": ["ICLR.cc/2026/Conference/Submission25525/Reviewer_pqKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25525/Reviewer_pqKW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220014261, "cdate": 1762220014261, "tmdate": 1762943460421, "mdate": 1762943460421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}