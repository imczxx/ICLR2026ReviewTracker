{"id": "xjDgITlk7a", "number": 11985, "cdate": 1758205024416, "mdate": 1759897541175, "content": {"title": "Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning", "abstract": "Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose a vulnerability analysis framework for Safe RL policies via inverse constrained reinforcement learning (ICRL). Our approach only requires a set of expert demonstrations to learn both the safety constraints and a learner policy, which are then used to generate adversarial attacks capable of inducing safety violations in Safe RL policies. Theoretical analysis establishes  the feasibility and provides bounds for our attack  method. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach.", "tldr": "Explore vulnerabilities of safe reinforcement learning applications under the minimal knowledge assumption via Inverse Constrained RL", "keywords": ["Safe reinforcement learning", "inverse constrained reinforcement learning", "adversarial attack", "vulnerability analyses"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55679d5287e15bc80e35fe96b1e04e6eb3c5b060.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a state-perturbation attack method to increase the safety cost of an RL agent subject to a safety constraint. The approach first applies the inverse constrained reinforcement learning (ICRL) method recently proposed in [Kim et al., 2024] to learn the cost function from a set of clean trajectories and the agent's policy and reward function. It then applies the standard projected gradient descent (PGD) method to identify state perturbations that maximize the agent's instantaneous cost in each time step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows that it is possible to perturb an agent's states to increase its safety cost even without knowing the cost function a priori, as long as the attacker has access to a set of clean trajectories and the agent's policy and reward functions."}, "weaknesses": {"value": "1. The paper applies the ICRL method in [Kim et al., 2024] to estimate the agent's cost function and then leverages the standard PGD attack. The technical contribution is low. \n2. While the paper claims its approach to be gradient-free, it looks like it still needs the gradient information. In Algorithm 1, line 4, the PGD method requires evaluating the gradient with respect to s', which involves the gradient of pi_E. \n3. The attacker's objective is confusing. According to (4), it appears the attacker simply maximizes the instantaneous cost at each time step, which is inconsistent with the cumulative cost constraint in the safe RL formulation (1). Further, in this case, the attacker's problem is essentially the same as the traditional reward minimization problem considered in the literature for unconstrained MDPs. \n4. The evaluation is unfair by comparing the proposed method with baselines developed for different objectives other than (4). Given the myopic objective (4), PGD is the most direct approach to optimize it. It makes better sense to just compare the same PGD attack with and without ICRL. \n5. Another big issue with the evaluation is that it completely ignores defenses. As the attacker in the paper only considers the cost objective, previous defenses for unconstrained MDPs can be easily adapted to the scenario considered in this paper.  \n6. The paper has multiple typos and missing/inconsistent definitions. \n - The constraint function phi is undefined. It looks like phi should just be the cost function c in the safe MDP formulation, although that is never made clear in the paper. \n - Similarly, it looks like the so-called system identification function f is simply the transition dynamics p. \n - The definition of stealthness is not standard. In adversarial machine learning, one typically captures stealthiness based on the attack outcome (intensity, frequency, etc.), not the attacker's capability (black-box, white-box, etc.). \n - In the safe MDP definition, the cost function c relies on the current state, the current action, and the next state, but in Algorithm 1 and Eq. (4), it only depends on the next state. Then, in Lemma 1, it only depends on the current perturbed state and action, which is very confusing.\n7. The related work section misses some important recent studies on state perturbations against RL, such as\n- Li et al., Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error, ICML 2024.\n- Sun et al., Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations, ICLR 2024.\n- Yang et al., DMBP: Diffusion Model-Based Predictor for Robust Offline Reinforcement Learning against State Observation Perturbations, ICLR 2024.\n- Liu et al., Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies, ICLR 2024."}, "questions": {"value": "Please refer to the discussion on weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MG3pFE0XPi", "forum": "xjDgITlk7a", "replyto": "xjDgITlk7a", "signatures": ["ICLR.cc/2026/Conference/Submission11985/Reviewer_ofMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11985/Reviewer_ofMg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631982428, "cdate": 1761631982428, "tmdate": 1762922979478, "mdate": 1762922979478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for analyzing the vulnerabilities of Safe Reinforcement Learning (Safe RL) agents using inverse constrained reinforcement learning (ICRL). The authors show that even safety-constrained policies remain susceptible to adversarial manipulation without requiring gradient access to the victim model. By reconstructing implicit constraints from expert demonstrations, the proposed approach systematically reveals hidden weaknesses in Safe RL algorithms, offering both theoretical insights and empirical validation across benchmark environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introducing an inverse-learning formulation to analyze Safe RL vulnerabilities is original and conceptually strong.\n\n2. The method works under limited access assumptions, making it applicable to realistic black-box attack settings.\n\n3. This work provides a theoretical analysis of the attack performance bound, which strengthens the work's contribution."}, "weaknesses": {"value": "1. The methodology is relatively trivial by combining ICRL to learn the constraint policy and an approximate agent policy to conduct gradient-based attack. \n\n2. The experiment was only conducted on two toy environments, which is not general enough to state the effectiveness of the proposed method. I would like to see results on more environments.\n\n3. The experiment only compares with other attack methods. It does not include defense baselines in the experiment."}, "questions": {"value": "1. The proposed method will rely on how accurately the approximate safety constraint function and learner policy are learned through expert trajectories. Could the author provide analysis or ablation studies on how many trajectories are needed?\n\n2. For the question refer to the experiment, please refer the Weakness 2 and 3.\n\n3. Will the method scale to an image-based RL agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MltvEdcmom", "forum": "xjDgITlk7a", "replyto": "xjDgITlk7a", "signatures": ["ICLR.cc/2026/Conference/Submission11985/Reviewer_yhh7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11985/Reviewer_yhh7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767905314, "cdate": 1761767905314, "tmdate": 1762922979033, "mdate": 1762922979033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the vulnerability assessment of Safe RL in Constrained Markov Decision Processes. existing Safe RL methods often assume benign environments, limiting their real-world applicability. To address this, they propose a method to evaluate the maximum cost under realistic adversarial attacks. In their threat model, the adversary has access to the victimâ€™s demonstrations but not to the policy gradients and constraint functions. Under this setting, the authors propose a method that combines Inverse Constrained Reinforcement Learning (ICRL) with gradient-based adversarial perturbations. Specifically, they estimate a differentiable constraint function via ICRL and optimize perturbations to maximize this estimated constraint. Experiments show that their method achieves higher attack performance than attacks assuming even stronger adversarial capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a practical and realistic vulnerability assessment of Safe RL by considering adversaries closer to real-world scenarios. This perspective enhances the practical value of robustness evaluation for Safe RL methods."}, "weaknesses": {"value": "The novelty of the proposed method is limited. It is essentially a straightforward combination of ICRL and standard gradient-based adversarial optimization. The theoretical analysis in Section 4.3 also lacks originality. Theorem 1 can be almost trivially derived from [1], and Lemmas 2, 3, and 4 follow directly from their assumptions. Although Remark 1 claims the effectiveness of estimating the Lipschitz constant, this claim is not empirically validated.\n\nMoreover, the proposed method is computationally expensive. It requires estimating both the cost function via ICRL and the system dynamics surrogate $\\hat{f}$. These estimations require a very large computational cost. Consequently, its applicability to high-dimensional state or action spaces may be severely limited.\n\nMinor comment:\nThe descriptions of FGSM Attacker (L2) and Gradient-based Attacker (L2) appear almost identical except for the citations. Clarifying the difference would strengthen the experimental section.\n\n[1] Konwoo Kim, Gokul Swamy, Zuxin Liu, Ding Zhao, Sanjiban Choudhury, and Steven Z Wu. Learning shared safety constraints from multi-task demonstrations. Advances in Neural Information Processing Systems, 36, 2024."}, "questions": {"value": "- Why does the proposed method outperform the Max-Cost Attacker, which directly uses the cost critic and should act as an oracle?\n- How computationally expensive is the cost estimation process in the proposed method, and is it feasible for high-dimensional environments?\n- To what extent does the quality of the system dynamics surrogate $\\hat{f}$ affect attack performance, and how costly is this estimation?\n- The experiments use 100 epochs of expert trajectories, which is relatively large. How does the number of demonstrations correlate with attack performance?\n- Equation (4) appears to require the gradient of $\\pi_E$, but this is not permitted under the threat model. Is this a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fRiSyrEC24", "forum": "xjDgITlk7a", "replyto": "xjDgITlk7a", "signatures": ["ICLR.cc/2026/Conference/Submission11985/Reviewer_jVQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11985/Reviewer_jVQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891829606, "cdate": 1761891829606, "tmdate": 1762922978493, "mdate": 1762922978493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}