{"id": "BN4vhB5IRy", "number": 24176, "cdate": 1758353684347, "mdate": 1759896778567, "content": {"title": "Exploration for Deployment-Efficient Reinforcement Learning Agents", "abstract": "Reinforcement learning (RL) provides a rich toolbox with which to learn sequential decision making policies. Notably, the ability to learn solely from offline interaction data has been a highly successful modality for training real-world policies. However, a gap exists in this paradigm when the offline dataset does not cover all the behaviors necessary to extract optimal policies. Naively, one can pre-train a policy using offline RL and fine-tune it using online RL; this can lead to catastrophe in settings like healthcare and autonomous driving, where deploying an unverified policy is irresponsible. Deployment efficient learning is a potential solution, where the number of distinct data collection policies is relatively low compared to the number of updates to the policy. We argue that safely improving a dataset requires a deployment efficient algorithm with a carefully constructed data collection policy. We introduce a framework with a stationary exploration policy that aims to reduce out-of-distribution uncertainty while maintaining strong returns. We establish theoretical guarantees of this exploration framework without finetuning and demonstrate our method on a large-scale supply chain environment with real-world data.", "tldr": "We present a novel exploration framework for offline reinforcement learning agents deployed in the real world that emphasizes the need to reduce uncertainty through a carefully constructed data collection policy.", "keywords": ["Real-World Exploration", "Deployment Efficient RL", "Real-World RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96c0afc63f63b87b90069aedfb12250ed862ba49.pdf", "supplementary_material": "/attachment/81edd68c8e8fe0b6113045e954f97fb0b26a8dc0.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on offline-to-online RL by designing a stationary policy to improve exploration. This paper proposes an exploration policy that aims to reduce the uncertainty in the offline data while staying within the known safe regions. Empirical results on the gridworld and supply chain environment are reported."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "(+) The paper is written clearly."}, "weaknesses": {"value": "(-) The analysis in this paper is not well justified, e.g., Assumption 5.3.\n\n(-) The experiment section lacks standard baseline methods. Only naive baselines are compared."}, "questions": {"value": "Q1. How is the uncertainty $u$ computed in Def 5.1?\n\nQ2. In Line 208, what are the references for the following statement: \"This assumption is also made by most offline RL algorithms using Equation (1) as their objective\"?\n\nQ3. Can you provide a comparison with standard offline RL algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MZBoJ5yjvE", "forum": "BN4vhB5IRy", "replyto": "BN4vhB5IRy", "signatures": ["ICLR.cc/2026/Conference/Submission24176/Reviewer_MGgf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24176/Reviewer_MGgf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761697085972, "cdate": 1761697085972, "tmdate": 1762942977293, "mdate": 1762942977293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies deployment efficient reinforcement learning, where the number of distinct data collection policies is relatively low compared to the number of updates to the policy. The authors argue that safely improving a dataset requires a deployment efficient algorithm with a carefully constructed data collection policy. They introduce a framework with a stationary exploration policy that aims to reduce out-of-distribution uncertainty while maintaining strong returns. They establish theoretical guarantees of this exploration framework without finetuning and demonstrate their method on a large-scale supply chain environment with real-world data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The setting of deployment efficient RL is important.\n2. The paper is technically solid, the proof looks correct.\n3. There are experimental results supporting theoretical results."}, "weaknesses": {"value": "1. The setting is not clear and seems restrictive. It is not clear whether the setting is tabular or continuous. According to the definition of $u(s,a)$, it seems that the state-action space must be discrete, which is very restrictive in real-world applications. Is it possible to extend the method to more general function approximation? How is $u(s,a)$ rigorously defined there?\n\n2. Given Assumption 5.3, Theorem 5.4 seems to be a straightforward result. Bounding the difference between the real MDP and a pessimistic absorbing MDP is a standard approach in various previous works.\n\n3. The selection of the policy $\\pi_{exp}^\\star$ is according to the standard approach of maximizing the reward (here is the uncertainty measure $u$) with a KL constraint. Could you please explain the novelty here?"}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OyGCCBvMlL", "forum": "BN4vhB5IRy", "replyto": "BN4vhB5IRy", "signatures": ["ICLR.cc/2026/Conference/Submission24176/Reviewer_XRiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24176/Reviewer_XRiF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962161972, "cdate": 1761962161972, "tmdate": 1762942976953, "mdate": 1762942976953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies deployment-efficient exploration for offline-to-real RL: when continual online updates are infeasible, deploy a small number of stationary exploration policies to enrich data, then retrain offline. It introduces an uncertainty-weighted, KL-regularized explorer and an MPC variant that plans with a shaped reward  A theoretical bound links performance gaps to optimal-policy visitation of dataset “unknowns,” motivating targeted exploration. Experiments on a toy gridworld and a supply-chain simulator suggest better data collection (and comparable evaluation returns) than simple ϵ-greedy/UCB baselines under limited deployments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The formalization is reasonable, but the main bound (Theorem 5.4) hinges on strong assumptions (offline optimizer finds the optimal policy for the pessimistic MDP; the KL on visitation replaced by a KL on actions), and the experiments do not fully validate the conditions under which the framework is guaranteed to help. The supply-chain evaluation uses an indirect, multi-simulator protocol with heuristic uncertainty and limited ablations."}, "weaknesses": {"value": "1. Assumptions behind the bound are strong and under-tested.\nTheorem 5.4 assumes the offline learner finds the optimal policy for the pessimistic MDP and then replaces a visitation-level divergence with a policy-level KL. The paper does not empirically probe when these surrogates are tight (e.g., via measuring actual state-visitation drift vs. action KL), nor does it test sensitivity of performance to that approximation.\n2. Baselines are weak for the stated setting.\nIn the deployment-constrained regime, there exist stronger comparators than ϵ-greedy/naive UCB: deployment-efficient MBO (e.g., model-based data collection à la prior work cited), conservative online fine-tuning with verification/shielding, uncertainty-aware behavior cloning with data-selection, or “collect-once” behavior-regularized explorers. The paper cites several lines but does not instantiate competitive versions under the same deployment budget."}, "questions": {"value": "Metricization of deployment efficiency.\nCan you report: (a) number of distinct deployed explorers, (b) trajectories per deployment, (c) per-deployment collect-return and regret, and (d) any safety/constraint violation proxies during exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hJu9Sm6FDL", "forum": "BN4vhB5IRy", "replyto": "BN4vhB5IRy", "signatures": ["ICLR.cc/2026/Conference/Submission24176/Reviewer_TFxf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24176/Reviewer_TFxf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143659598, "cdate": 1762143659598, "tmdate": 1762942976423, "mdate": 1762942976423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a deployment-efficient exploration framework for offline-to-online reinforcement learning in safety- or cost-sensitive domains. It defines dataset suboptimality via visits to uncertain state-action pairs, derives a bound linking missing coverage to return loss, and constructs stationary single- and multi-step exploratory policies that stay near the dataset policy while targeting uncertain regions, with theory and experiments to validate effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper gives a clear suboptimality characterization: $J\\left(\\pi^{\\star}, M\\right)-J\\left(\\pi_D^{\\star}, M\\right) \\leq \\frac{2 R_{\\max }}{(1-\\gamma)^2} d_{\\pi^*}\\left(U_D\\right)$, which cleanly isolates \"missing states of the optimal policy\" as the only thing exploration needs to fix. This is a useful, actionable target for data-collection policies.\n\n2. The exploratory policy is derived from a principled divergence-regularized objective so exploration is explicitly balanced against staying close to a verified baseline policy - exactly what deployment-efficient RL needs.\n\n3. The paper provides two constructive approximations (single-step exponential reweighting and multistep MPC-style planning with an uncertainty bonus) that show the abstract objective can be instantiated in both discrete and continuous/large action spaces without online policy finetuning."}, "weaknesses": {"value": "1. The key quantity $u(s, a)$ (\"likelihood of being outside the dataset\") is only heuristically estimated from counts/GMMs; the guarantees rely on it tracking the true unknown set $U_D$, but the paper does not give an error-to-performance translation for misspecified $u(s, a)$.\n\n2. The bound and the construction assume the offline policy $\\pi_D^{\\star}$ is already optimal on the pessimistic MDP $M_D$; this is a strong assumption that pushes difficulty into the offline learner and is not relaxed in the main theorem.\n\n3. The multi-step exploratory policy requires a learned dynamics model $\\hat{T}$ and reward $\\hat{r}$; the method does not analyze model-bias accumulation in the planning rollout, so it is unclear how far from the dataset support the MPC variant can safely explore."}, "questions": {"value": "1. The objective $\\max_\\pi \\mathbb{E}_{d^*}[u(s, a)]-\\beta D_{\\mathrm{KL}}(\\pi \\| \\pi_D^*})$ is motivated via a visitation-level KL; can the authors formalize when the policy-level KL is no longer a good surrogate (e.g., when dataset-induced state marginals differ a lot)?\n\n2. The suboptimality bound depends on $d_{\\pi^*}\\left(U_D\\right)$, which is not observable. Is there a practical upper bound in terms of the learned $u(s, a)$ that could be monitored to decide when to stop deploying the exploratory policy?\n\n3. For the multi-step version (Eq. (5)), how sensitive is the exploration target to model errors in $\\hat{T}$ near the boundary of the dataset support, and can a conservative backup (e.g., HALT transition) be added without breaking improvement?\n\n4. The framework fixes $\\pi_{\\exp }$ during a deployment window to satisfy verification constraints. Could the analysis be extended to a piecewise-stationary schedule and still retain the same form of the suboptimality reduction bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yxr8Uga2l6", "forum": "BN4vhB5IRy", "replyto": "BN4vhB5IRy", "signatures": ["ICLR.cc/2026/Conference/Submission24176/Reviewer_6xLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24176/Reviewer_6xLW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208385588, "cdate": 1762208385588, "tmdate": 1762942976171, "mdate": 1762942976171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for deployment-efficient exploration in reinforcement learning—motivated by real-world constraints where policy updates require verification and redeployment is costly. The setting lies between offline and online RL: the agent can collect additional data through limited, pre-approved deployments but cannot update its policy mid-deployment.\n\nThe authors suggest learning an offline conservative policy, then constructing an exploration policy that tilts the behavior distribution toward uncertain regions, $\\pi_{\\text{exp}}(a|s) \\propto \\pi_D(a|s)\\exp(u(s,a)/\\beta)$, where u(s,a) is an uncertainty function. The idea is to collect “useful but safe” new data for later offline retraining. They present several theoretical results linking coverage to sub-optimality and conduct experiments in a Gridworld domain and a supply-chain simulator."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem setting—safe data collection under limited deployment budgets—is realistic and practically relevant, especially for industrial RL applications. The high-level motivation of optimizing exploration to improve future offline training is a good idea, though this is well studied in the area of reward-free RL (e.g. Jin et al. 2020, Wang et al. 2020, Chen et al. 2022, Wagenmaker et al. 2022, Amortila et al. 2024). The inclusion of \"real-world\" experiments in a supply-chain simulator demonstrate some potential, though the method used in Section 6.2 does not seem related to the methods proposed in the rest of the paper. \n\nCitations: \nJin et al. 2020: https://arxiv.org/pdf/2002.02794\nWang et al. 2020: https://arxiv.org/pdf/2006.11274\nChen et al. 2022: https://arxiv.org/pdf/2206.10770\nWagenmaker et al. 2022: https://arxiv.org/pdf/2201.11206\nAmortila et al. 2024: https://arxiv.org/pdf/2403.06571"}, "weaknesses": {"value": "**Theoretical novelty and correctness**\n\nThe theoretical developments seem to mostly build on Kidambi et al. 2021 (including the main theoretical bound), but somehow without its formality or correctness. The new developments are sloppy and in parts incorrect. Many equalities are handwaved completely and/or incorrect as stated (e.g. Equation 1: \\hat{p} is undefined, not specified which (s,a) is being considered), many approximate equalities $\\approx$ that are informal and unclear). The main theoretical result (Theorem 5.4) is almost verbatim a restatement of known results from Kidambi et al., 2021 on coverage-based sub-optimality bounds in for pessimistic offline RL. Fundamental quantities for the method to work, such as a definition for the uncertainty function u(s,a), are left under specified. Since u(s,a) drives the entire exploration method, leaving it abstract makes the method non-operational. In fact, obtaining a proper notion of uncertainty is well-studied and one of the defining challenges in reward-free RL (e.g. Amortila et al., 2024) and bonus-based exploration (Bellemare et al. 2016, Pathak et al. 2017, Pathak et al. 2019, Ash et al. 2022). And as mentioned before, though finding exploratory policies that cover the distribution of $\\pi^\\star$ is a good idea (since this would allow for subsequent offline RL), this is not a novel idea.\n\n**Experimental limitations**\n\nThe experiments are limited to a toy gridworld and a supply-chain simulator, with very weak baselines (greedy and ε-greedy policies). No comparisons are provided to standard offline-to-online RL, reward-free methods, or bonus-based exploration as mentioned above. Without these, it is impossible to judge whether the proposed method adds practical value. Furthermore, the experiments do not seem to actually measure the deployment efficiency of their method. The results themselves seem modest over epsilon-greedy and not accompanied by ablations on the different choices of the uncertainty function. \n\nOverall, the paper’s motivation is good, but the theoretical development is largely a repackaging of prior results, the new contributions are handwavy and left undefined, and the experiments lack strong baselines, ablations, and a quantitative measure of the claimed deployment efficiency. \n\nCitations: \nBellemare et al. 2016: https://arxiv.org/pdf/1606.01868\nPathak et al. 2017: https://arxiv.org/pdf/1705.05363\nPathak et al. 2019: https://arxiv.org/pdf/1906.04161\nAsh et al. 2022: https://arxiv.org/pdf/2110.11202\nJin et al. 2020: https://arxiv.org/pdf/2002.02794\nWang et al. 2020: https://arxiv.org/pdf/2006.11274\nChen et al. 2022: https://arxiv.org/pdf/2206.10770\nWagenmaker et al. 2022: https://arxiv.org/pdf/2201.11206\nAmortila et al. 2024: https://arxiv.org/pdf/2403.06571"}, "questions": {"value": "- How should one compute the uncertainty function u(s,a) in practice outside of tabular domains?\n- What differentiates Theorem 5.4 from prior results such as those in Kidambi et al. (2021)?\n- How does your exploration method compare to prior reward-free methods, ensemble disagreement methods, curiosity-driven methods, or count-based exploration on the same tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zHxC3NjAV2", "forum": "BN4vhB5IRy", "replyto": "BN4vhB5IRy", "signatures": ["ICLR.cc/2026/Conference/Submission24176/Reviewer_LM7e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24176/Reviewer_LM7e"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762451616339, "cdate": 1762451616339, "tmdate": 1762942975854, "mdate": 1762942975854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}