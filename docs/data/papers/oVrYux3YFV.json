{"id": "oVrYux3YFV", "number": 10671, "cdate": 1758179203092, "mdate": 1759897636363, "content": {"title": "Generative Hints", "abstract": "Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78 \\% top-1 accuracy improvement (0.63\\% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286 \\% on the CheXpert X-ray dataset.", "tldr": "Using generative hints to learn property of a target function to outperform data augmentation in vision.", "keywords": ["Hints; Generative Models; Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c65ae83e55b8400aa04f919e138959a3b70c820.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Generative Hints, a new training methodology that directly enforces known functional properties (called hints) in machine learning models using generative models. The key idea is to sample virtual examples from a generative model trained on the dataset and apply hint transformations (e.g., spatial or flip invariance) to them. These unlabeled virtual examples are used in a semi-supervised setup, combining the main classification loss with a hint loss that enforces the desired invariance (via symmetric KL divergence or MSE).\n\nUnlike traditional data augmentation, which only indirectly encourages invariance through transformations on labeled data, generative hints explicitly regularize model behavior over the entire input space. The method reformulates a fully supervised task as semi-supervised learning without requiring unlabeled real data. Experiments across fine-grained visual classification datasets show the effectiveness of these methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea is original. Reinterpreting generative augmentation as functional constraint learning rather than sample diversity is innovative.\n\n2. The formalism of hints, virtual examples, and invariance objectives is elegant and well presented.\n\n3. The effectiveness of the method has been demonstrated across multiple architectures (transformers and CNNs) and tasks (vision, medical imaging).\n\n4. Experimental studies include ablation on generative model quality and analysis of FID–correlation relationship."}, "weaknesses": {"value": "1. Improvements over strong baselines (e.g., ViT/Swin with augmentation) are small (<2%), which may limit practical impact.\n\n2. Results are constrained to medium-sized datasets; it remains unclear how this method performs on large-scale datasets (e.g., ImageNet).\n\n3. The success of generative hints relies heavily on having a capable generative model. The cost and feasibility of training StyleGAN3 for every dataset is nontrivial.\n\n4. There is no ablation on hint type: It would be helpful to understand how sensitive results are to the choice of hint function, weighting parameter, or temperature T."}, "questions": {"value": "1. How does the method scale computationally when training high-quality generative models on large datasets? Could pretrained generative models (e.g., diffusion models) be reused effectively?\n\n2. How sensitive are results to the choice of α and T? Would an adaptive scheduler materially change outcomes?\n\n3. What happens when the generative model’s FID is worse than 15–20? Does the method ever degrade performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "crZ79jjZrI", "forum": "oVrYux3YFV", "replyto": "oVrYux3YFV", "signatures": ["ICLR.cc/2026/Conference/Submission10671/Reviewer_HxYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10671/Reviewer_HxYR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761374269918, "cdate": 1761374269918, "tmdate": 1762921921703, "mdate": 1762921921703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Hints, a novel approach to data augmentation. The authors train a generative model (StyleGAN3) and use its output, together with a transformation function h(x), to train a downstream classifier. The reported experiments demonstrate consistent improvements in classification accuracy across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new data augmentation method that leverages generative modeling.\n2. The approach consistently improves classification performance across several benchmark datasets."}, "weaknesses": {"value": "I find two major weaknesses in the current version of the paper. The first is that **the problem formulation contains significant ambiguity**, resulting in unclear or even contradictory statements throughout the paper. For example, in Section 3, Definition 3 fails to clearly define the generative model G. While I understand that the authors may refer to common generative models such as GANs or diffusion models, such assumptions should be explicitly stated for clarity and self-containment. Similarly, in Definition 4, the usage of the symbol “\\approx” is never properly defined. Moreover, in Definition 7, if p and q represent probability density functions, then p/T is no longer a valid density function when T \\neq 1, since \\int p(x) dx / T \\neq 1. These issues are not isolated but appear multiple times, making the overall formulation mathematically inconsistent and conceptually vague.\n\nMy second concern relates to the experimental section, which **lacks sufficient ablation studies to support the claimed performance improvements**. To be convinced that the proposed approach itself is responsible for the reported gains, I would expect to see experiments that vary hyperparameters, dataset scales, and the transformation function h, as well as analyses of failure cases. Without such analyses, it is difficult to attribute the improvement to the proposed idea rather than to specific settings or chance factors. Therefore, although the numerical results are encouraging, they are not yet convincing enough to demonstrate the effectiveness of the proposed method. I should also note that I am not an expert in image classification, so please disregard this comment if I have underestimated the technical contribution of the work.\n\n_LLM Usage Disclosure:_\nNote that this review was polished with assistance from a large language model (OpenAI GPT-5). The scientific assessment, judgments, and all substantive comments reflect my own independent evaluation."}, "questions": {"value": "I have no further questions about this paper. Please address the weaknesses I mentioned above, especially those concerning the mathematical ambiguity and lack of ablation studies. Once these points are clarified, I will be happy to adjust the score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nFaX56GWAF", "forum": "oVrYux3YFV", "replyto": "oVrYux3YFV", "signatures": ["ICLR.cc/2026/Conference/Submission10671/Reviewer_G6Vz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10671/Reviewer_G6Vz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572005810, "cdate": 1761572005810, "tmdate": 1762921921320, "mdate": 1762921921320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Hints, a semi-supervised training framework that leverages generative models to impose functional invariances on visual classifiers. The authors argue that conventional data augmentation only captures invariances locally within the training distribution, while a high-fidelity generative model can synthesize a broader set of plausible samples for enforcing invariance more effectively.\n\nThe proposed method trains a classifier using both (1) a supervised classification loss on labeled data and (2) a hint loss computed on generated, unlabeled images. The hint loss enforces consistency of model predictions under transformations corresponding to known invariances (e.g., horizontal flip, translation, small rotation). Empirical evaluations on fine-grained recognition benchmarks (CUB-200-2011, Stanford Cars, FGVC Aircraft, Oxford Flowers) and on CheXpert show consistent but moderate accuracy improvements over standard training with data augmentation."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptually clear formulation of “hints.”\nThe paper formalizes the notion of a hint as a constraint linking inputs that should yield similar outputs, providing a unifying view that bridges data augmentation, regularization, and semi-supervised learning.\n\n2. This  paper Novel use of generative models for invariance enforcement.\nRather than simply augmenting labeled samples, the approach explicitly uses a pretrained generator (StyleGAN3) to sample the data manifold and apply transformations in this synthetic domain, extending the effective training support."}, "weaknesses": {"value": "1. Limited performance gains.\nReported improvements are relatively small (average ≈ +0.6 percentage points, maximum ≈ +1.8 points top-1 accuracy). While consistent, the benefits may not justify the additional computational overhead of training or maintaining a generative model.\n\n2. Dependency on generator quality.\nThe approach relies heavily on the fidelity of the generator. The paper notes that when FID > 11, generative hints become ineffective, restricting applicability to domains with strong generative models."}, "questions": {"value": "The paper formalizes hints as invariance constraints, but how does this differ in principle from standard consistency regularization used in semi-supervised learning (e.g., Mean Teacher, FixMatch)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n0i3n0hshU", "forum": "oVrYux3YFV", "replyto": "oVrYux3YFV", "signatures": ["ICLR.cc/2026/Conference/Submission10671/Reviewer_2eDL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10671/Reviewer_2eDL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967584740, "cdate": 1761967584740, "tmdate": 1762921920985, "mdate": 1762921920985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The idea of the paper is to present a generative data augmentation setup referred to as generative hints, which allows generative models to create extra images modeling the input distribution (but not being the same) and augmentations, that would allow capturing various properties of the target distribution. \n\nIn order to show the effectiveness of the idea, the paper utilizes StyleGAN3 to generate extra data that models the input distribution, and demonstrates two hint functions: Flip Invariance and Spatial Invariance (Translation and Rotation) -- modeling invariance properties between the input and target distributions. The invariance is imposed using a KL and a MSE loss. \n\nImproved performance is demonstrated on the Stanford Cars, Caltech Birds (CUB), FGCV Aircraft and Oxford Flowers datasets, over the ViT-Base and Swin-Base models. Results are also shown on the CheXpert dataset using a ResNet50. In both cases, the baseline used is the standard version of the model; however, exact training specifics (aka data augmentation used on the training dataset, if any) are not provided."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea of creating \"hints\" or functions that capture diverse properties of the target distribution is strong and would help the community. However, the paper only presents invariance-based hints/transformations, which are already standard in the community."}, "weaknesses": {"value": "The two main pillars of the presented paper are commonly existing ideas in the ML community, namely: generative (data) augmentation and data augmentation (for invariances). In my understanding, while the paper starts by planning to go beyond these ideas to present \"other properties\" (aka non-invariant properties like the authors specify for tabular data -- monotonicity), the proposed methodology and experiments do not go beyond existing knowledge to demonstrate any additional ideas or further analysis.\n\nWhile the paper presents and claims a novel generative data augmentation paradigm, the experimental studies do not compare against any baselines beyond the authors' standard model training. Good baselines would include both existing generative [1,2,3,4] and non-generative data augmentation [5,6,7] techniques, as well as augmentations that impose transformation invariance [9, 10]. \n\nFurthermore, the paper claims lines 139-140 —\"Applying hints directly on training data can lead to overfitting, where the model memorizes the hints with respect to specific training examples rather than learning the underlying property\"—contradicts common knowledge of invariance-based input-data augmentation [9, 10], but fails to provide any evidence in support of this statement. I find the following additional baselines/ablations necessary for such a claim and good analysis of the reasons for improvement:\n1. Input-data only invariance transformations\n2. No invariance transformations, only generative data augmentation to the training set (class-based supervised loss)\n3. No invariance transformations on input data; generative hints [proposed methodology]\n4. Invariance transformations on all data (both input and generated)\n\nThe related work section misses out on numerous relevant works (not limited to the ones I mentioned) in the fields of:\n1. Data Augmentation and Invariance: only two papers from 2017 and 2019 are cited, while the field has moved far ahead since. [5,6,7,8]\n2. Generative Data Augmentation: only one paper from 2022 has been cited, while there has been immense progress in the field.[1,2,3,4]\n3. Invariance transformations on the input data. [9,10,11].\n\n[1] Bansal, H., & Grover, A. (2023). Leaving reality to imagination: Robust classification via generated datasets. arXiv preprint arXiv:2302.02503.\n\n[2] Zheng, C., Wu, G., & Li, C. (2023). Toward understanding generative data augmentation. Advances in neural information processing systems, 36, 54046-54060.\n\n[3] Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., & Fleet, D. J. (2023). Synthetic data from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466.\n\n[4] Rahat, F., Hossain, M. S., Ahmed, M. R., Jha, S. K., & Ewetz, R. (2025, February). Data augmentation for image classification using generative ai. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (pp. 4173-4182). IEEE.\n\n[5] Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412.\n\n[6] Kim, J. H., Choo, W., Jeong, H., & Song, H. O. (2021). Co-mixup: Saliency guided joint mixup with supermodular diversity. arXiv preprint arXiv:2102.03065.\n\n[7] Müller, S. G., & Hutter, F. (2021). Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 774-782).\n\n[8] Wang, L., Zhan, Y., Ma, L., Tao, D., Ding, L., & Gong, C. (2025). Splicemix: A cross-scale and semantic blending augmentation strategy for multi-label image classification. IEEE Transactions on Multimedia.\n\n[9] Hounie, I., Chamon, L. F., & Ribeiro, A. (2023, July). Automatic data augmentation via invariance-constrained learning. In International Conference on Machine Learning (pp. 13410-13433). PMLR.\n\n[10] Liu, Y., Yan, S., Leal-Taixé, L., Hays, J., & Ramanan, D. (2023). Soft augmentation for image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16241-16250).\n\n[11] Quiroga, F., Ronchetti, F., Lanzarini, L., & Bariviera, A. F. (2018, January). Revisiting data augmentation for rotational invariance in convolutional neural networks. In International conference on modelling and simulation in management sciences (pp. 127-141). Cham: Springer International Publishing."}, "questions": {"value": "Referring to my understanding of the paper (in summary), and why I think it's not novel and lacks good baseline comparisons (detailed in weaknesses). My ratings reflect this understanding of this work. If the authors can explain the novelty of their work beyond generative/non-generative data augmentation and invariance transformations (as in papers I've previously mentioned), I am open to a detailed discussion and to reevaluating my assessment of the paper.\n\nThere is also a lack of comparison with existing methods, no ablation study to back the claims in the paper (e.g., on which data subset should invariance or other transformations be applied), and the related work section is not thorough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RSexJE0duN", "forum": "oVrYux3YFV", "replyto": "oVrYux3YFV", "signatures": ["ICLR.cc/2026/Conference/Submission10671/Reviewer_Es7H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10671/Reviewer_Es7H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10671/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990690789, "cdate": 1761990690789, "tmdate": 1762921920622, "mdate": 1762921920622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}