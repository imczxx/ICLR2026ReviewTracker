{"id": "PAS2SV70xU", "number": 9728, "cdate": 1758136850761, "mdate": 1759897701900, "content": {"title": "Echoes of the Visual Past: Test-Time Prompt Tuning with Multi-Scale Visual Memory", "abstract": "Test-time prompt tuning (TPT) aims to adapt pre-trained vision-language models (VLMs) to various downstream tasks by learning textual prompts using unlabeled data at test time. However, existing TPT methods exhibit a performance gap compared to a line of prompt-engineering-based methods that leverage hand-crafted or LLM-generated prompts for VLM adaptation. We attribute this gap to a core limitation of previous TPT approaches: they learn prompts from only limited class-specific visual knowledge derived from a single test image. As a result, the learned prompts underperform compared to hand-crafted and LLM-generated prompts enriched with diverse, class-specific knowledge. To address this limitation, we propose $\\textbf{T}$est-time $\\textbf{P}$rompt $\\textbf{T}$uning with $\\textbf{M}$ulti-scale visual $\\textbf{M}$emory ($\\text{M}^2\\text{TPT}$). Specifically, the memory is constructed to store past seen class-relevant image patches as multi-scale visual descriptions for each class. For each test image, we use it to query the memory and learn the textual prompt using both the test image and the retrieved class-relevant visual memory. Additionally, we introduce holistic visual memory to better handle holistic visual recognition tasks that require global image-level context, and an irrelevance suppression strategy to mitigate the impact of noisy memory entries at test time. We evaluate our method on 15 commonly used benchmark datasets and show that it outperforms existing TPT methods. Furthermore, our framework can incorporate human-designed prompts and achieves state-of-the-art performance compared to recent VLM adaptation methods that use hand-crafted or LLM-generated prompts.", "tldr": "", "keywords": ["test-time prompt tuning", "vision-language models", "foundation models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c831601c5619c35392517733480e2f6a99319617.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work extends test-time prompt tuning (TPT) for vision-language models by introducing a multi-scale visual memory mechanism that stores class-relevant patch features from past test samples and uses them to guide prompt adaptation. Experiments on 15 datasets show consistent gains over previous TPT and competitive performance with hand-crafted/LLM prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It pinpoints a clear weakness in existing TPT methods, relying on a single image, and offers a conceptually reasonable solution via memory augmentation.\n- Extensive experiments on 15 benchmarks, both in-distribution and OOD, with clear ablation."}, "weaknesses": {"value": "- With regard to multi-scale visual memory, what does the term \"multi-scale\" refer to, and how is it initialized?\n- The idea of maintaining a memory of past features (e.g., HisTPT, DynaPrompt) is not new. The main difference here lies in multi-scale patch granularity and explicit cross-promotion between memory and prompt, which is incremental rather than conceptually ground-breaking. It's better to compare with HisTPT and DYnaPrompt if possible.\n- Since the memory is class-relevant, it costs 18.96G for ImageNet, so the scalability and memory efficiency are limited.\n- Since only two of the compared methods were published in 2024 or 2025, are there any other recent related works?\n-  Visualization of retrieved patches and how they influence prompt tokens would strengthen the paper’s interpretability."}, "questions": {"value": "See the comments in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WsysxnEHOa", "forum": "PAS2SV70xU", "replyto": "PAS2SV70xU", "signatures": ["ICLR.cc/2026/Conference/Submission9728/Reviewer_r18P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9728/Reviewer_r18P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761336782306, "cdate": 1761336782306, "tmdate": 1762921227626, "mdate": 1762921227626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a TPT method that differs from prior approaches relying solely on a single test image and its augmented views for prompt optimization.  By introducing multi-scale visual memory, holistic memory, and an irrelevance suppression mechanism, the method aims to achieve more effective TPT. Extensive experiments across 15 datasets demonstrate competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The evaluation is comprehensive, including two experimental settings across 15 diverse datasets.\n- The overall writing is clear and easy to follow."}, "weaknesses": {"value": "- The method suffer from practical inefficiency. Test-time prompt tuning introduces substantial inference latency and computational overhead due to full back-propagation and multi-step forward inference. On top of this burden, the proposed approach further maintains a memory queue, which can significantly increase the computational cost, especially in tasks with a large number of classes. I am seriously concerned about the deployability of the method in real-world scenarios.\n- The novelty is limited and some strongly relevant works are missing. The idea of introducing a memory mechanism into TPT is not new, as HisTPT [1] has already explored similar concepts. BoostAdaptor [2] incorporates augmented views of test images as multi-scale information in memory, which is closely related to the “multi-scale memory” proposed here. Moreover, recent approaches such as DPE [3] and GS-Bias [4] have shown more efficient test-time learning via prototype or bias updates. In addition, compared with the latest training-free method MCP [5], the proposed approach does not show clear performance advantages.\n\n[1] Historical Test-time Prompt Tuning for Vision Foundation Models. NIPS2024\n\n[2] BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping. NIPS2024\n\n[3] Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models. NIPS2024\n\n[4] GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models. ICML 2025\n\n[5] Multi-Cache enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models. ICCV 2025"}, "questions": {"value": "- The final results in Eq (8) are not solely obtained through prompt tuning, making it difficult to determine whether the method’s effectiveness primarily stems from prompt optimization.\n- The tuned prompts may potentially have negative effects.  For example, TPT has been observed to decrease performance on the Pets dataset.  Such negative effects could also compromise the quality of memory samples.\n- The paper does not report how many steps of prompt tuning were used, leaving unclear the computational cost and convergence behavior of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QQ8DS7uhxK", "forum": "PAS2SV70xU", "replyto": "PAS2SV70xU", "signatures": ["ICLR.cc/2026/Conference/Submission9728/Reviewer_Lgso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9728/Reviewer_Lgso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660795269, "cdate": 1761660795269, "tmdate": 1762921226944, "mdate": 1762921226944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M²TPT, a test-time prompt tuning method that enhances vision-language models by incorporating a multi-scale visual memory of past class-relevant image patches, allowing prompts to be learned from richer, accumulated visual context rather than just a single test image. By jointly optimizing prompts and updating memory in a mutual promotion loop—supplemented by a holistic memory for global context and an irrelevance suppression mechanism to filter noise—it outperforms existing test-time methods and even rivals performance of hand-crafted or LLM-generated prompt approaches, without requiring prior knowledge of test datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a novel test-time prompt tuning framework that bridges the performance gap between TPT and hand-crafted/LLM-generated prompts by incorporating multi-scale visual memory of past class-relevant patches.  \n- Achieves SOTA results on 15 benchmark datasets, outperforming prior TPT methods and prompt-engineering approaches even without human-designed prompts.  \n- Maintains computational efficiency with minimal overhead compared to existing TPT methods, making it practical for deployment without requiring backpropagation through the full VLM.  \n- Rigorous evaluation across in-distribution and out-of-distribution settings, with reproducible implementation and clear ablation studies."}, "weaknesses": {"value": "- The method relies on pseudo-labels for memory update and retrieval, making it vulnerable to early prediction errors that can accumulate and degrade performance over time.  \n- Memory requires storage of visual features across the test stream, which may not be feasible in memory-constrained or real-time deployment scenarios.  \n- The approach assumes a fixed, known set of classes beforehand, limiting applicability to dynamic or open-category settings where new classes emerge online.  \n- The irrelevance suppression mechanism introduces additional complexity and hyperparameters (γ, α, β) with limited analysis of their robustness across diverse tasks or domains."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SyYhJPsxe8", "forum": "PAS2SV70xU", "replyto": "PAS2SV70xU", "signatures": ["ICLR.cc/2026/Conference/Submission9728/Reviewer_Emou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9728/Reviewer_Emou"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9728/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896484752, "cdate": 1761896484752, "tmdate": 1762921226385, "mdate": 1762921226385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}