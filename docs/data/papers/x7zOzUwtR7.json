{"id": "x7zOzUwtR7", "number": 23148, "cdate": 1758340245256, "mdate": 1759896830298, "content": {"title": "Structure-Aware Graph Hypernetworks for Neural Program Synthesis", "abstract": "We study the neural program synthesis of $\\textit{parameterized}$ function families through the lens of meta-learning with hypernetworks. Given a user intent $U$, a meta-learner $M_{\\phi}$ produces a full weight set $\\hat{\\theta}=M_{\\phi}(U)$ for a target neural network with fixed architecture $S$, and the instantiated network $m_{S,\\hat{\\theta}}(X)\\to Y$ realizes the behavior intended for $U$. Classical hypernetworks typically $\\textit{ignore the target network’s structure}$ and emit a flat list of weights; as a consequence, they fail to account for $\\textit{neuron-permutation symmetry}$—many distinct parameterizations of $S$ implement the same function—so equivalent solutions are treated as different targets, fragmenting supervision and hurting out-of-distribution generalization. To address this, we propose $\\textit{Meta-GNN}$, a hypernetwork that constructs a $\\textit{neural graph}$ from the target architecture $S$ and applies $\\textbf{structure-aware}$ message passing with parameter-tied encoders and decoders. This design reduces the search space during learning by collapsing equivalent classes of target networks, without loss of expressivity. Empirically, across modular arithmetic ($\\textit{AddMod}$-$p$), array operations ($\\textit{SumFirst}$-$n$), and inverse-rule tasks from 1D-ARC, $\\textit{Meta-GNN}$ substantially improves learning and $\\textbf{out-of-distribution generalization}$ compared to classic hypernetworks and direct $(U,X)\\to Y$ baselines. Mechanistic analyses reveal $\\textit{what is learned}$: on $\\textit{AddMod}$-$p$ the synthesized Transformers recover the canonical clock representation and admit a compact closed-form map $U\\mapsto\\theta$. These results demonstrate that structure-aware Meta-GNNs enable reliable generalization to $\\textit{unseen program parameterizations}$, providing a critical advance for the nascent field of neural program synthesis.", "tldr": "We show that a structure-aware hypernetwork can directly generate full neural-network weights—treating weights as a continuous program modality—and outperform baselines with strong zero-shot generalization across unseen tasks.", "keywords": ["neural program synthesis", "weight-space learning", "meta-learning", "permutation-equivariant graph networks", "zero-shot generalization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d451cadf9fb7367e31b87fe24721a6039227aa28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Traditional program synthesis methods that search within a template space rely heavily on handcrafted templates and incur substantial computational costs as template size increases. To bridge these gaps, the authors introduce the lens of meta-learning with hypernetworks, defining a neural program (NeuroP) as a differentiable program modality that enables continuous, gradient-based optimization. The authors then propose Meta-GNN, a structure-aware hypernetwork capable of generating the complete set of weights for a target architecture given a user intent U. Experimental results show that Meta-GNN demonstrates strong out-of-distribution (OOD) generalization compared to all baselines. It is notable that on the ADDMOD-p task, Meta-GNN learns a canonical clock representation, indicating that it captures underlying, generalizable computational regularities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method makes meaningful contributions across multiple prior research directions and demonstrates clear originality:\n\n   - Leveraging hypernetworks for program synthesis is an innovative idea that mitigates the dependence on handcrafted DSLs present in neuro-symbolic approaches.\n\n   - The proposed Meta-GNN employs message passing and group-tied encoders and decoders to address the permutation symmetry problem in traditional hypernetworks.\n\n   - Out-of-distribution (OOD) generalization is a critical challenge in both meta-learning and hypernetwork research, and the paper shows that Meta-GNN achieves strong OOD generalization performance.\n\n2. The paper is well-structured and comprehensive, with clear formal definitions and thorough explanations of the key design components of Meta-GNN."}, "weaknesses": {"value": "1. The claimed benefits of structure-awareness in improving generalization and performance are supported only by theoretical explaination, lacking ablation studies. Specifically, while the selected baselines partially demonstrate that greater structure-awareness correlates with better generalization, Meta-GNN, as a graph neural network, differs topologically from the MLP-based baselines. It would be valuable to further investigate the contribution of Meta-GNN’s own components.\n  \n2. In the experimental section, each task domain employs only a single target network configuration. As shown in Table 1, all target networks are shallow (mostly one-layer) , which limits evidence for scalability and cross-architecture generalization.\n  \n3. The paper lacks direct empirical comparisons with traditional methods, such as neuro-symbolic baselines, which would strengthen the claim of superiority over existing approaches."}, "questions": {"value": "1. What other tasks are commonly studied in traditional program synthesis, and could this method be compared directly with classical or symbolic approaches on those tasks?\n  \n2. In Tables 2 and 3, Meta-GNN does not achieve the best generalization across all tasks. What might explain these cases?\n  \n3. Beyond demonstrating strong OOD generalization, does the structure-aware design offer additional advantages, such as smaller model size, reduced training cost, or improved efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zynHGXUc0j", "forum": "x7zOzUwtR7", "replyto": "x7zOzUwtR7", "signatures": ["ICLR.cc/2026/Conference/Submission23148/Reviewer_D2KG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23148/Reviewer_D2KG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642807681, "cdate": 1761642807681, "tmdate": 1762942532819, "mdate": 1762942532819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is in the field of hypernetworks. Given a user intent, it generate specific weights for another (target) neural network to complete the task. The main contribution is that the proposed architecture considered neuron-permutation symmetry (that is, for example, the order of neurons in a layer does not influence the functionality of the network). The architecture of the target network is transformed to a graph in which nodes are biases and edges are weights. While swapable nodes/edges are assigned different positional encodings to differentiate them, they are processed by the same encode function. $K$ message-passing processes are done on the graph, which include a permutation-invariant aggregation to merge all the informations from neighbors. Finally, every node and edge obtains an embedding which is then decoded as a scalar."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- As far as I know, this is the first hypernetwork that considered neuron-permutation symmetry, and the idea do make sense\n- Three variants (Meta-MLP/gMLP/GNN) are proposed and experimented, which help illustrate the effectiveness of the proposed apporach\n- A mechanistic analysis is provided, which shows strong evidence that the proposed architecture learns a generalizable solution"}, "weaknesses": {"value": "- While the tasks in the experimental section are illustrative, all of them are rather simple without significant practical usage"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tx9WOAN43C", "forum": "x7zOzUwtR7", "replyto": "x7zOzUwtR7", "signatures": ["ICLR.cc/2026/Conference/Submission23148/Reviewer_rkyC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23148/Reviewer_rkyC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950896033, "cdate": 1761950896033, "tmdate": 1762942532452, "mdate": 1762942532452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of program synthesis. The authors refer to a neural network that approximates a symbolic program as a “neural program” (“NeuroP”) and explore whether a hypernetwork can be effective at converting a program specification (given by “user intent”) into a neural program. They note that a neural program has many neuron-permutation symmetries that can hinder a hypernetwork’s performance. To address this, the neural program’s structure is represented as a graph, that is divided into weight subsets. The hypernetwork then outputs a neural program in 3 steps: 1) A subset-specific encoder transforms the user intent to a corresponding latent vector. 2) The graph structure is utilized, along with message passing, in order to ensure the permutation equivariance inside the necessary weight subsets. 3) A subset-specific decoder is used to generate the weights of a neural program.\n\nThe paper claims three contributions: establishing NeuroP as an alternative to symbolic program synthesis; providing empirical evidence that using structure-aware hypernetworks can lead to strong OOD generalization across diverse program families; releasing the code and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Appears to provide a novel approach (in the program synthesis literature) to generating neural programs using a hypernetwork.\n\nProvide an interesting comparison between the performance of group-aware and group-and-structure aware hypernetworks.\n\nTheir experiments cover an MLP as well as the transformer architecture, demonstrating that the method is effective for both."}, "weaknesses": {"value": "Limited evaluation - experiments and baselines. It’d be good to see how it compares to other neural program synthesis methods.\n\nLimited novelty. From the three listed contribution, I would argue that the first (introducing the concept of a neural program) is not a contribution, while the third (releasing the code and benchmark) is of limited significance. The second listed contribution is “empirical evidence that intent-to-weight synthesis is feasible, and structure-aware meta-learners achieve strong OOD generalization across diverse program families.” However, in my opinion, this statement is not supported by the limited evaluation. It is possible that the way the hypernetwork is constructed is novel, but if so it should be explicitly states as a contribution.\n\nRelated work section appears to be lacking. The hypernetwork section discusses the original hypernetworks paper and then goes into MAML, while I imagine there are other important works that are related to this one (could be mistaken). Also, this section needs to incorporate other neural program synthesis approaches that represent the target program as a neural network (I don’t have a reference off the top of my head, please correct me in case all such references are already covered)."}, "questions": {"value": "The approach appears to be applicable to any problem where hypernetwork are useful. Why was program synthesis (of neural programs) chosen as the target domain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W7sHgX2Qwp", "forum": "x7zOzUwtR7", "replyto": "x7zOzUwtR7", "signatures": ["ICLR.cc/2026/Conference/Submission23148/Reviewer_fYXg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23148/Reviewer_fYXg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162159562, "cdate": 1762162159562, "tmdate": 1762942532016, "mdate": 1762942532016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}