{"id": "3tUSHgohi5", "number": 2248, "cdate": 1757041190785, "mdate": 1763730840679, "content": {"title": "ScaleCap: Scalable Image Captioning via Dual-Modality Debiasing", "abstract": "This paper presents ScaleCap, a scalable image captioning strategy that generates\ncomprehensive and detailed image captions. The key challenges of high-quality\nimage captioning lie in the inherent biases of LVLMs: multimodal bias resulting in\nimbalanced descriptive granularity, offering detailed accounts of some elements\nwhile merely skimming over others; linguistic bias leading to hallucinated de-\nscriptions of non-existent objects. To address these issues, we propose a scalable\ndebiased captioning strategy, which continuously enriches and calibrates the caption\nwith increased inference budget. Specifically, we propose two novel components:\nheuristic question answering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate hallucinations\ncaused by linguistic biases. With increased inference cost, more heuristic questions\nare raised by ScaleCap to progressively capture additional visual details, generating\ncaptions that are more accurate, balanced, and informative. Extensive modality\nalignment experiments demonstrate the effectiveness of ScaleCap. Annotating\n450K images with ScaleCap and using them for LVLM pretraining leads to consis-\ntent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap\nshowcases superb richness and fidelity of generated captions with two additional\ntasks: replacing images with captions in VQA task, and reconstructing images\nfrom captions to assess semantic coverage.", "tldr": "We present ScaleCap, a scalable image captioning framework.", "keywords": ["Large Vision Language Model", "Image Caption"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2b591287b1589687209074b75748b8219ded366.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes ScaleCap, a detailed image captioning pipeline that uses multiple models, including an LLM and an LVLM. It first generates an initial caption and identifies golden sentences, highly likely non-hallucinatory sentences within the generated caption, based on contrastive sentence ratings. Contrastive sentence rating is a strategy for evaluating the factual precision of a sentence by comparing output token probabilities between multimodal decoding and language-only decoding. Then, ScaleCap further obtains visual information by iteratively requesting detailed descriptions for each object mentioned in the golden sentences (referred to as heuristic question-answering module). By alternating between contrastive sentence rating and heuristic question answering, ScaleCap collects reliable sentences and finally summarizes them using an LLM. The authors demonstrate the effectiveness of ScaleCap through reconstruction and pretraining experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clear and well-organized overall.\n2. The problem addressed in the paper is interesting and timely."}, "weaknesses": {"value": "1. The authors use image reconstruction and pretraining experiments to demonstrate the effectiveness of the proposed method. However, there are still many evaluation approaches remaining for assessing detailed image captions. For example, GPT-based methods [1,2] and QA-based methods [3] could be leveraged. I strongly encourage the authors to incorporate these additional evaluation metrics.\n2. Several previous studies have proposed detailed image captioning systems that involve multiple deep learning models [3, 4]. Comparisons and discussions with these works are needed to better position the proposed method within the existing literature.\n\n[1] Petryck et al., \"ALOHa: A New Measure for Hallucination in Captioning Models\"  \n[2] Chan et al., \"CLAIR: Evaluating Image Captions with Large Language Models\"  \n[3] Lee et al., \"Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage\"  \n[4] Get et al., \"Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation\""}, "questions": {"value": "oes the proposed method outperform existing methods across multiple evaluation metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BnXEgSlSlV", "forum": "3tUSHgohi5", "replyto": "3tUSHgohi5", "signatures": ["ICLR.cc/2026/Conference/Submission2248/Reviewer_VAQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2248/Reviewer_VAQo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761027459986, "cdate": 1761027459986, "tmdate": 1762916160296, "mdate": 1762916160296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ScaleCap, a pipeline designed to generate comprehensive and detailed image captions for pretraining VLM. The authors identify multimodal bias and linguistic bias as key challenges. ScaleCap addresses these through Heuristic Question Answering and Contrastive Sentence Rating. Using ScaleCap, the authors create the ScaleCap-450K dataset  and show that pretraining LVLMs on this dataset consistently improves performance compared to using datasets like ShareGPT4V and DenseFusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is generally well-written and clearly structured, making the proposed pipeline and experiments easy to follow.\n\n2.The work proposes a complete pipeline aimed at improving the quality, detail, and factuality of LVLM-generated captions.\n\n3.Experiments effectively demonstrate that the ScaleCap-450K dataset, generated by the proposed pipeline, leads to superior LVLM pretraining outcomes compared to existing large-scale caption datasets like ShareGPT4V-450k and DenseFusion-450k."}, "weaknesses": {"value": "1.The core components of ScaleCap, namely Heuristic Question Answering and Contrastive Sentence Rating, appear to have limited novelty, primarily combining or refining existing techniques rather than introducing fundamentally new concepts.\n\n2.The proposed ScaleCap pipeline suggests a high computational cost for generating each caption, involving multiple model inference stages including initial captioning, filtering, question generation, iterative question answering, answer filtering, and final integration using a large LLM. This multi-step process appears resource-intensive, potentially limiting its practical scalability."}, "questions": {"value": "1.Could the authors quantify the computational cost—such as average time per caption or total GPU hours required to generate a single caption using the full ScaleCap pipeline, and how does this compare to baseline caption generation costs?\n\n2.While the paper compares pretraining benefits against other datasets, how does the quality of ScaleCap captions or the performance of models pretrained on them compare against models directly improved using training-based hallucination mitigation techniques like RLAIF-V or LLaVA-RLHF?\n\n3.Considering the introduction's potentially restrictive view on tool-based captioning , could the ScaleCap pipeline itself benefit from integrating specific tools, perhaps for grounding question generation or verifying answers, rather than solely relying on the VLM and CSR"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZxTqULvmLI", "forum": "3tUSHgohi5", "replyto": "3tUSHgohi5", "signatures": ["ICLR.cc/2026/Conference/Submission2248/Reviewer_jUSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2248/Reviewer_jUSR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488830417, "cdate": 1761488830417, "tmdate": 1762916160060, "mdate": 1762916160060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces *ScaleCap, a scalable image-text paired data distillation pipeline designed to produce long, instance-complete, and visually grounded captions from LVLM without external detectors/tools. ScaleCap has two core components:\n(1) Heuristic Question Answering (HQA): an LLM generates targeted, object/attribute/position questions from an initial caption; a LVLM answers them to surface missing details under a controllable budget.\n(2) Contrastive Sentence Rating (CSR): an offline filter that compares token probabilities with vs. without the image and retains sentences whose “critical tokens” are better supported by the image than by language priors, aiming to suppress hallucinations.\nUsing this pipeline, the authors build ScaleCap-450K, a 450k-image caption dataset sourced mainly from LAION and ShareGPT4V images with resolution/complexity filtering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The two identified blockers map neatly to HQA (add missing visual facts) and CSR (filter unsupported text). The mechanism is easy to reason about and implement with standard LVLM/LLM primitives.\n2. The observation that 7B-class LVLMs are often sufficient for perception while larger LLMs help during long-context integration provides concrete guidance for cost-constrained systems.\n3. Improvements appear across multiple LVLM backbones/settings, suggesting the dataset’s benefits aren’t model-specific."}, "weaknesses": {"value": "1. The core claim that balanced, instance-complete detail drives the gains isn’t cleanly isolated from caption length or sheer verbosity. Controlled studies (equal length across methods; fixed token budgets redistributed among object/attribute/position details) are missing.\n2. CSR accepts sentences based on a max-over-critical-tokens Δ probability threshold. This may be unstable (single-token spikes; POS-tagging noise). Robustness analyses (pooling variants, τ-sweeps, cross-dataset calibration) are not provided."}, "questions": {"value": "1. For frontier models, does the way ScaleCap introduces more prior information help the LVLM itself in the pipeline, such as fine-tuning the LVLM with generated data?\n2. If CSR is computed with a different LVLM than the one used to answer HQA (or than the pretraining backbone), do conclusions hold?\n3. A clear pseudocode algorithm will be more helpful for reading and understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m7WftiUAcL", "forum": "3tUSHgohi5", "replyto": "3tUSHgohi5", "signatures": ["ICLR.cc/2026/Conference/Submission2248/Reviewer_x2KT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2248/Reviewer_x2KT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721001765, "cdate": 1761721001765, "tmdate": 1762916159810, "mdate": 1762916159810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ScaleCap, a captioning pipeline that iteratively enriches captions while reducing hallucinations by combining (i) heuristic question answering and (ii) contrastive sentence rating. A tunable scale budget controls how many questions are asked, trading cost for detail. Using ScaleCap, the authors build ScaleCap-450K and show consistent gains across 11 benchmarks, plus benefits in Prism-style perception tests and an image-reconstruction study. The author argues that the approach improves informativeness and alignment even with smaller LVLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, modular method that targets two real failure modes. The offline contrastive, sentence-level rating is a neat way to down-weight language priors without destabilizing decoding.\n- Strong empirical coverage and breadth. Pretraining with ScaleCap-450K beats ShareGPT4V-450K and DenseFusion-450K on most of the 11 benchmarks.\n- Practical efficiency levers. The pipeline uses a small LVLM for perception (object/position Q&A) and a budget N to scale detail, giving users cost–quality control."}, "weaknesses": {"value": "- Practical efficiency levers. The pipeline uses a small LVLM for perception (object/position Q&A) and a budget N to scale detail, giving users cost–quality control.\n- Prompt dependence in question generation. The method hinges on a “powerful LLM” to craft good object/position prompts; robustness across domains or weaker LLMs isn’t deeply probed."}, "questions": {"value": "Refer to the weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WXDbl3YWNS", "forum": "3tUSHgohi5", "replyto": "3tUSHgohi5", "signatures": ["ICLR.cc/2026/Conference/Submission2248/Reviewer_wjkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2248/Reviewer_wjkS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2248/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762276883301, "cdate": 1762276883301, "tmdate": 1762916159653, "mdate": 1762916159653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}