{"id": "XQv3jegXqt", "number": 18652, "cdate": 1758289743845, "mdate": 1759897089558, "content": {"title": "Grokked Models are Better Unlearners", "abstract": "The phenomenon of $\\textbf{grokking}$, where deep neural networks achieve delayed but strong generalization long after fitting training data, challenges traditional views of model generalization. While previous work has shown that grokked models exhibit enhanced robustness, we establish a novel connection: grokked models are fundamentally better at machine unlearning—the process of removing specific data influences without full retraining. We provide comprehensive empirical evidence across CNNs and ResNets on CIFAR datasets, and transformers on text datasets. State-of-the-art unlearning algorithms (gradient ascent, SCRUB, Fisher forgetting, and fine-tuning) achieve significantly more efficient data removal when applied to grokked models. Critically, unlearned grokked models retain higher performance on remaining data and exhibit enhanced robustness compared to non-grokked counterparts. Our analysis reveals that grokking restructures internal representations, creating more disentangled knowledge that facilitates selective forgetting with minimal collateral damage. These findings establish the first systematic connection between grokking and machine unlearning, suggesting that grokking-induced training dynamics can be leveraged for more practical and robust privacy-preserving unlearning methods.", "tldr": "", "keywords": ["machine unlearning", "grokking", "Deep Learning", "Generalization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40d78ccfb5fac60f4682151d850b794d7bea153b.pdf", "supplementary_material": "/attachment/ebe65b7bc8d6ec93e8168f9b847aadb49b43a63d.zip"}, "replies": [{"content": {"summary": {"value": "The authors demonstrate that grokking machine learning models leads to better unlearning without changing the unlearning methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of investigating the effects of grokking on unlearning is highly interesting and provides novel insights in how seemingly unrelated aspects of training can have a massive impact on post-hoc operations."}, "weaknesses": {"value": "W1\nPlease check for missing references/typos e.g., As illustrated in Figure ?? on page 1\n\nW2\nThe selection of unlearning methods is quite outdated (e.g., Fisher from 2020 instead of any newer similar methods, and both student-teacher models from 2023)\n\nW3\nUnlearning scenarios are only full class for section 3.1. It would be especially interesting to see how results change in subclass and random selection scenarios which are trickier to unlearn (as done in prior works of Chundawat etc.).\n\nW4\nIs it really grokking or just better models that lead to better unlearning? What if we change the training process to achieve a model with the same accuracy that did not grokk?\n\nAt the same time “locally grokked” samples seem to just be easy samples that the model learns easily while the other samples are harder samples. Hereby unlearning works better on samples on which the model performs well → the same as a model with high accuracy (when looking at it at model scale vs sample level).\n\nWhile I find the insights of this paper interesting, I am not fully convinced that the results point to grokking being the reason for better unlearning and not simply that better models are easier to unlearn.\n\nW5\nThe datasets and models are very small. It would be interesting to see results for larger datasets (and models). E.g., Imagenette vs CIFAR\n\nW6\nMissing related literature on unlearning difficulty. As an example: \nZhao, Kairan, et al. \"What makes unlearning hard and what to do about it.\" Advances in Neural Information Processing Systems 37 (2024): 12293-12333."}, "questions": {"value": "Q1 My central concern is if it is really about grokking or just models with higher performance. In Table 1, the accuracy difference is 7% between the base models which is an enormous difference. In the LLM case, see W4 \"easy samples\".\n\nQ2 How do the results change on scenarios that are not full class? (W3, e.g. subclass unlearning or random samples unlearning)\n\nQ3 Locally grokked just seem to be easy samples. Please elaborate on your reasoning - I am happy to be convinced otherwise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dfOTal20nJ", "forum": "XQv3jegXqt", "replyto": "XQv3jegXqt", "signatures": ["ICLR.cc/2026/Conference/Submission18652/Reviewer_ooE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18652/Reviewer_ooE1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651927545, "cdate": 1761651927545, "tmdate": 1762928359832, "mdate": 1762928359832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether models that have undergone grokking exhibit superior machine unlearning (MU) properties compared to pre-grokking checkpoints. Using vision models (CNNs/ResNets on CIFAR-10) and language models (Phi-1.5 on TOFU), the authors claim that grokked models enable (i) faster unlearning convergence, (ii) better retention of retain-set performance, and (iii) more stable updates. They attribute this to modular representations and reduced gradient alignment in post-grokking states."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper attempts to connect two emerging phenomena—grokking and machine unlearning—offering a novel angle on representation learning dynamics.\n- The observation on grokked model may provide some insight on representations that what would help the machine unlearning."}, "weaknesses": {"value": "1. The problem setting is fundamentally impractical. \n\nMachine unlearning is motivated by real-world scenarios where a pretrained model must selectively forget data upon request (e.g., GDPR). However, the paper assumes access to a post-grokking checkpoint of the same model, which is not feasible in practice. Grokking typically requires extreme overfitting (e.g., 50k+ epochs on CIFAR-10), far beyond standard pretraining regimes. There is no mechanism proposed to induce grokking-like features in pretrained models to exploit the advantage claimed in this manuscript. \n\n2. There are severe flaws in experiments.\n\n - On CIFAR-10 with ResNet (presumably ResNet-18), the original test accuracy is ~80% (Table 1), whereas standard training achieves >90%. Even after grokking, performance plateaus below expected levels.\n\n- No retrain-from-scratch baseline provided, which is a core MU evaluation metric is comparison against retraining on the retain set. This is absent entirely.\n\n- After unlearning, test accuracy (TA) drops to 16% in some cases. The authors claim \"consistent improvement\" in (L250) in forgetting, but while destroying the model. This is not unlearning; the RA and TA must be retained.\n\n- Figure 1(a): The claimed \"delayed generalization\" is not visible. Train accuracy has not converged, so this is not grokking by definition.\n\n- Figure 1(b): Unclear whether trajectories are from forking checkpoints or sequential unlearning from $\\theta_{\\text{pre}}$. No clarification of unlearning target or procedure.\n\n3. The Language model experiment may be cherry-picked and misleading.\n\n- The concept of \"local grokking\" is an undefined, unconventional term. The authors construct forget sets by selecting examples that partially grok at a given checkpoint, which is explicit cherry-picking to favor their models.\n\n- Table 2 shows identical original performance for grok/ungrok models. I think this should be clarified. \n\n- L289~290 need additional clarification why the grokking is unavailable. According to literature, the first grokking was observed with the language model trained on arithmetic. The authors may use it as a baseline and consider the MU problem.\n\n- minor) acronym KL, PO, etc are used w/o citation or explanation. \n\n4. results on gradient correlation and local complexities may not related to grokking or unlearning\n\n- Referring Table 3, lower correlation in grokked models is claimed as evidence of modularity. However, pre-grokking models have high train loss and large gradients, while grokked models are near-stationary. Reduced alignment is expected at convergence, not a grokking-specific phenomenon.\n  - Observation on gradient magnitudes or angular distances across each retain/forget set may reinforce the claim in manuscript.\n\n- LC differences between grok/pre-grok are trivial: grokking implies generalization, so lower LC is expected. No link to unlearning is established."}, "questions": {"value": "- Need mutiple clarifications:\n  - which resnet is used for th experiment? The Resnet18, most basic resnet usually shows much better test accuracy(>90) but yours is about 80%.\n  - Which MU algorithm were used in section 4? and why did you select it?\n  - Figure 1(b): Are unlearning trajectories from independent forks or sequential updates? \n  - Table 2: why original(w/o MU) has identical scores on grok/ungrok models?\n\n- What is the definition of \"local grokking\"? Why should we choose locally grokked samples as a forget dataset? \n\n- Why retraining is omitted from all results? retraining the model with retain dataset only, until the grokking behavior appears, can be reasonable baseline for the experiment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qXRj9Z9QSj", "forum": "XQv3jegXqt", "replyto": "XQv3jegXqt", "signatures": ["ICLR.cc/2026/Conference/Submission18652/Reviewer_aDw8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18652/Reviewer_aDw8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724699713, "cdate": 1761724699713, "tmdate": 1762928358874, "mdate": 1762928358874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper finds that applying unlearning methods to a model after it has grokked is significantly more effective than applying them immediately after the model has only memorized the training set (the pre-grok phase). The grokking transition not only improves the test accuracy but it represents a fundamental structural reorganization of the model's internal knowledge which helps in effective unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Bridges two important phenomena—grokking and machine unlearning—with clear empirical results, offering an important insight into creating robust unlearning.\n\n- The core finding is consistent across diverse models and data modalities (vision and language), significantly increasing confidence in the generality of the approach.\n\n- The paper is written well and easy to follow. \n\n- They demonstrate consistent emprical performance across Unlearning methods post grokking."}, "weaknesses": {"value": "- The paper lacks a deep explanation of why post-grok models unlearn better. It is unclear if the improved unlearning efficiency is due to structural changes in the representation space (e.g., features and localized memory influence) or changes in the loss landscape (e.g., a transition to a wider, shallower basin).\n\n- Grokking requires significant additional compute time (extended training) well past the point of initial data fitting. Without a quantitative analysis, the computational cost of reaching the post-grok state may negate the efficiency savings gained during the unlearning process, challenging its practical recommendation.\n\n- Findings rely on simple algorithmic tasks; whether it extends to non-algorithmic feature spaces of LLMs is unclear. Apply the method to a large LLM fine-tuned on a specific, real-world dataset."}, "questions": {"value": "- Provide a quantitative analysis of the cost-benefit trade-off. How much extra wall-clock or compute time is required to reach the post-grok state, and how does this cost compare to the total computational savings gained in the unlearning process?\n\n- Provide a more thorough analysis of the underlying mechanism linking the structural changes inherent to grokking with the model’s increased flexibility for unlearning. Determine if the cause is primarily the Representation Space or the Loss Landscape or both. Studying this will be very valuable.\n\n- The current explanation feels like a correlation: grokking happens, then unlearning works better, and the solution is flatter. We don't know if flatness is the cause or the effect of generalization. Additional experiment will be to take a Pre-Grok (overfitted) model checkpoint. Apply an external optimization technique (like sharpness-Aware Minimization) specifically designed to flatten the loss minimum without further wasting the additional compute required fr grokking. If the model's unlearning efficiency improves despite not having grokked, it proves that flatness is the direct cause of efficient unlearning. \n\n- The feature space shift from 'entangled' to 'disentangled' is not directly quantified. Use Centered Kernel Alignment (CKA) to compare final layer representations for $D_{\\text{forget}}$ vs. $D_{\\text{retain}}$.Validate the mechanism.\n\n- Minor writing mistakes: Missing reference to figures in some places."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XjJWORL8jl", "forum": "XQv3jegXqt", "replyto": "XQv3jegXqt", "signatures": ["ICLR.cc/2026/Conference/Submission18652/Reviewer_SMr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18652/Reviewer_SMr1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931052816, "cdate": 1761931052816, "tmdate": 1762928358405, "mdate": 1762928358405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Grokking is the phenomenon of delayed generalization after initial overfitting. This paper looks into the unlearning ability of “grokked” models, which refers to removing certain knowledge from the model, while preserving the rest. The authors compare standard unlearning procedures applied to checkpoints taken before and after the grokking , across vision (CNNs/ResNets on CIFAR variants) and language tasks. The experiments show that starting from a grokked model results in faster forgetting and better preserving of the remaining data. These findings provide insights on the potential of grokked models in machine unlearning.\n\nAlthough this paper provides interesting findings, more experimental results and theoretical analysis are needed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper reveals a novel and interesting observation, which is that grokking can result in a model with better unlearning abilities.\n \n* Evaluation is done on both vision and language domain, and several forgetting algorithms are included."}, "weaknesses": {"value": "* As the authors have acknowledged in the paper, the experiments are limited to CIFAR10/100 and ResNet/CNNs in vision, and one LLM on TOFU in the language domain. Since this paper focuses on empirical results, more datasets (such as ImageNet-100) and architectures need to be evaluated to verify the consistency of the current findings. \n\n* The authors provide an analysis on gradient correlation and local complexity to explain the findings. However, a theoretical analysis on why grokked models show lower correlation and complexity, and why these help with forgetting should be explored. \n\n\nMinor issues:\n\n*  Typo in Table 1 (“Orginal”).\n\n* A figure reference is missing (line 52)"}, "questions": {"value": "* What are the practical implications of this finding? From a practical standpoint, is it guaranteed that the model can achieve a grokked state, regardless of the data or architecture?\n\n* How much is the training time overhead to achieve grokking in this setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IS9eafG592", "forum": "XQv3jegXqt", "replyto": "XQv3jegXqt", "signatures": ["ICLR.cc/2026/Conference/Submission18652/Reviewer_bqHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18652/Reviewer_bqHc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18652/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981588637, "cdate": 1761981588637, "tmdate": 1762928357965, "mdate": 1762928357965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}