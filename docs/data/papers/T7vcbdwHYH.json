{"id": "T7vcbdwHYH", "number": 25228, "cdate": 1758365476915, "mdate": 1759896729213, "content": {"title": "CL2GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "abstract": "The growing demand for automated writing assistance in scientific domains highlights the need for robust Chinese Grammatical Error Correction (CGEC) systems that can adapt across disciplines. However, existing CGEC research lacks dedicated benchmarks for academic writing and overlooks continual learning as a solution to handle domain-specific variation. To fill this gap, we introduce CL2 GEC, a Continual Learning benchmark for Chinese Literature Grammatical Error Correction, designed to evaluate adaptive CGEC across multiple academic fields. Our benchmark includes 10,000 human-annotated sentences spanning 10 disciplines, each exhibiting distinct linguistic styles and error patterns. We evaluate large language models under sequential tuning, parameter-efficient adaptation, and representative continual learning strategies, using both standard GEC metrics and continual learning metrics adapted to task-level variation. Experimental results show that regularization-based continual learning methods, such as OGD and GEM, outperform replay-based and sequential approaches in both grammatical accuracy and knowledge retention. These findings underscore the feasibility and importance of integrating continual learning into CGEC and position our benchmark as a foundation for future research on adaptive scientific writing assistance.", "tldr": "CL2GEC is a new benchmark for Chinese academic GEC across 10 disciplines; results show that regularization-based continual learning significantly outperforms replay and sequential tuning in both grammatical accuracy and knowledge retention.", "keywords": ["Chinese Grammatical Error Correction;Benchmark Evaluation;Continual Learning;Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4aa6dc313ba60e31d9724d0bd7aa27a688e9a3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CL2GEC, the first benchmark and evaluation suite for continual learning in Chinese Grammatical Error Correction (CGEC) across 10 academic disciplines. The benchmark consists of 10,000 manually curated and annotated sentences, each associated with up to three human references, drawn from diverse domains such as law, science, and art. CL²GEC enables controlled assessment of catastrophic forgetting and cross-domain transfer using both standard GEC metrics and continual learning metrics adapted for task-sequence settings. The paper provides extensive empirical results for large language models (LLMs) under various adaptation and continual learning strategies, including sequential fine-tuning, LoRA, replay-based, and regularization-based continual learning methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Comprehensive Academic Chinese GEC Benchmark**: The CL²GEC dataset is not only substantial in size (10,000 sentences) but also features rigorous multi-stage curation, leveraging both automatic detectors and in-domain human experts. The annotated data covered multiple research fields, which ensure the diversity of the benchmark.\n\n**First Chinese GEC benchmark for continue learning**: To the best of my knowledge, this is the first work that study the scenario of continue learning for Chinese GEC. **However**, the target of continue learning for such a GEC task is ambiguous.\n\n**Rich Experimental Settings**: The authors provide detailed experimental setups, including both random and semantically informed task sequences, ablation on replay buffer size, and systematic results across multiple LLMs and adaptation strategies.\nIn-depth Evaluation and Metrics: The use of both standard GEC metrics (Precision, Recall, F₀.₅) and continual learning-specific metrics (Backward Transfer, Average Task Performance) brings nuance to the empirical analysis. Equation formalizations are transparent and align with continual learning literature (see Page 4, loss and optimization formulations)."}, "weaknesses": {"value": "**Missing Annotation Principles**: The paper did not describe the annotation principles for GEC task. The grammatical error can be corrected in multiple ways. In most previous works, the principle of minimal edit is applied in the data annotation. Without annotation principles or annotation guidelines, the quality and the consistency of labeled data can not be ensured.\n\n**Problematic Data Filtering** In section 3.2 Data Annotation, the authors described \"Only sentences flagged consistently by all  6 grammatical error detectors are kept\".  However, the sentences that are judged as erroneous sentences by all models are simple samples, while the difficult samples may be excluded by such a principle. This will significantly influence the distribution of the benchmark.\n\n**Limited Research Significance for Continue Learning**: GEC task is not a long-context task and the dataset has only 10000 samples in total, which means the train cost on CL2GEC is not that unaffordable. Why not just shuffle the dataset randomly and directly train the model using all training data? The application of continue learning seems meaningless in such a task.\n\n**Limited Theoretical Insights into Catastrophic Forgetting**: While the empirical results are substantial, there is little in-depth mathematical or theoretical exposition regarding why certain CL strategies succeed or fail in this linguistic, multi-domain context. For example, the paper lacks a formal analysis of error distribution shift or domain overlap between disciplines.\n\n**Ambiguous Treatment of Semantic Task Ordering**: The computation of semantic similarity (Appendix A.1.2) is described, but the similarity is a metric for every pair of sub-dataset. The paper did not describe how to organize them into a absolute sorted list. From the Appendix A.1.2, I cannot know the specific rules for the 3 groups and how to sort them.\n\n**Typos**: In line 462, there is reference error (Figure ??)"}, "questions": {"value": "- Could you please provide the annotation guidelines?\n- How do you ensure the data quality?\n- Could you please clarify the purpose of continue learning in GEC task? Can the continue learning reach a higher performance than direct training using all data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ENflVyV5h1", "forum": "T7vcbdwHYH", "replyto": "T7vcbdwHYH", "signatures": ["ICLR.cc/2026/Conference/Submission25228/Reviewer_a1hc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25228/Reviewer_a1hc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809547915, "cdate": 1761809547915, "tmdate": 1762943372675, "mdate": 1762943372675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the CL^2GEC dataset, which contains 10,000 high-quality, human-annotated samples for grammatical error correction, featuring a diverse range of error patterns. In addition, the paper conducts continual learning experiments on the CL^2GEC dataset and establish solid baselines for future work."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A large-scale, high-quality GEC dataset has been constructed.\n\n2. Comprehensive analytical experiments are conducted to evaluate the performance of existing models."}, "weaknesses": {"value": "1. The motivation for applying continual learning (CL) in the GEC domain is not clearly justified. Since grammatical errors across different domains of the same language share certain common patterns, it is unclear whether adopting a CL framework provides practical value.\n\n2. As GEC dataset annotation is inherently challenging, the paper lacks consistency metrics to demonstrate the reliability of human annotations."}, "questions": {"value": "1. As mentioned above, considering the commonality of error patterns in GEC, the necessity of CL in this domain still requires further validation. Adding a full-data fine-tuning result in Table 2 would make the argument more convincing.\n\n2. Typo issue: there is a citation error between lines 43–46."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "558OzSo5jN", "forum": "T7vcbdwHYH", "replyto": "T7vcbdwHYH", "signatures": ["ICLR.cc/2026/Conference/Submission25228/Reviewer_nRwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25228/Reviewer_nRwe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993960846, "cdate": 1761993960846, "tmdate": 1762943372369, "mdate": 1762943372369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces $CL^{2}GEC$, the first continual learning (CL) benchmark for multi-disciplinary academic writing. The benchmark comprises 10,000 human-annotated sentences from 10 academic fields, designed to evaluate models in a sequential, domain-incremental setting. The authors benchmarked large language models using sequential fine-tuning, parameter-efficient adaptation, and four CL algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper first proposes the Continual Learning benchmark for Chinese Literature Grammatical Error Correction, which could comprehensively evaluate the continual learning ability of LLMs in the chinese literature grammatical error correction task."}, "weaknesses": {"value": "1. This paper only re-implement classic continual learning methods, which are not specifically designed for LLMs' continual learning. This paper should report the newest LLM continual learning methods[1,2] and provide convincing experiments to demonstrate the value of the dataset and the pros and cons of different methods.\n\n2. It is unclear what its core differences from other datasets are. The dataset is built with grammatical error data from 10 different domains, but the connection between this domain categorization and continual learning is not immediately obvious.\n\n3. As detailed in ICLR call for papers, the main text should be 9 pages or fewer, and additional pages are only allowed for the bibliography/references. Thus, the limitations should be controlled in 9 pages.\n\n4. This paper only evaluates two common large language models, it fails to meet the acceptance standards for ICLR in terms of evaluation comprehensiveness, dataset indispensability.\n\n[1] He, Jinghan, et al. \"Continual instruction tuning for large multimodal models.\" arXiv preprint arXiv:2311.16206 (2023).\n\n[2] Smith, James Seale, et al. \"Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2dkhNOvUFq", "forum": "T7vcbdwHYH", "replyto": "T7vcbdwHYH", "signatures": ["ICLR.cc/2026/Conference/Submission25228/Reviewer_tXfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25228/Reviewer_tXfj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139260281, "cdate": 1762139260281, "tmdate": 1762943371986, "mdate": 1762943371986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **CL2GEC**, the first continual learning benchmark for **Chinese Grammatical Error Correction (CGEC)** across academic disciplines. It studies how large language models adapt to new domains sequentially, highlighting the issue of **catastrophic forgetting**. While the benchmark assumes that models cannot perform **multi-task training** or access all domains simultaneously, this constraint may be unrealistic for modern LLMs that already demonstrate strong **cross-domain generalization and in-context learning**. Experiments on Qwen2.5 and LLaMA3 show that **regularization-based continual learning methods** outperform naive fine-tuning and replay strategies. Overall, CL2GEC provides a valuable research framework for studying lifelong adaptation, though its sequential learning assumption may limit its real-world applicability."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a **novel multi-disciplinary Chinese Grammatical Error Correction (GEC) dataset** with a **comprehensive and well-structured evaluation framework**.\n\n2. Experiments on Qwen2.5 and LLaMA3 show the proposed method outperforms naive fine-tuning and replay strategies."}, "weaknesses": {"value": "1. The paper assumes that large language models (LLMs) can only acquire multi-domain GEC capabilities through **continual learning**, without comparing other plausible approaches such as **multi-task fine-tuning, retrieval-augmented generation (RAG), or in-context learning**. Given the strong generalization ability of LLMs, continual learning may not be strictly necessary in this setting.\n\n2. The model comparison is limited; it should include **more open-source models with stronger Chinese capabilities**, such as **different sizes of the Qwen2.5 series**, to provide a fairer evaluation.\n\n3. The study **does not compare smaller encoder–decoder models**, for which continual learning might actually be **more relevant and effective** than for large instruction-tuned LLMs."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IwVMAlFcdE", "forum": "T7vcbdwHYH", "replyto": "T7vcbdwHYH", "signatures": ["ICLR.cc/2026/Conference/Submission25228/Reviewer_cLmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25228/Reviewer_cLmW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162535841, "cdate": 1762162535841, "tmdate": 1762943371407, "mdate": 1762943371407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CL2GEC which is a benchmark for Chinese Literature Grammatical Error Correction across multiple disciplines. In particular, CL2GEC is designed to support continual learning. The paper explores various continual learning algorithms on this dataset, and the authors perform various experiements to highlight important dynamics (e.g., task ordering, backbone differences) that practitioners should be aware of. Evaluation is done using both standard GEC metrics, as well as continual learning metrics.\n\n## Dataset\nDataset was crawled from China National Knowledge Infrastructure, with 10 disciplines: Law, Management, Education, Economics, Science, History, Agriculture, Literature, Art, and Philosophy. Random sample of 1000 questions per discipline to form 10k questions in total. Then some cleaning steps are done, such as sentence extraction, noise removal, and anonymization. Finally, the data is annotated both by LLMs (initial filter), then manually reviewed by expert annotators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- baselines are thorough\n- it's a big effort to collect 10,000 human annotated sources, so this is a valuable resource\n    - Thorough data collection process, with manual human reviews, so the dataset is likely high quality\n- experiments had good coverage of various methods.\n- ablations are interesting. For example, I liked the section on task order"}, "weaknesses": {"value": "- This is a very narrow domain and not easily generalizable to some of the bigger topics that the community really cares about. I imagine the subset of researchers who care about Chinese Literature GEC might not be that large.\n- I would have wanted the authors to flesh out more what makes this task special from other GEC tasks. Are there any nuances specific to this task that are less common in other GEC tasks?\n- I don't fully see how this dataset itself is connected to continual learning. It somehow feels like the authors just stitched two somewhat disjointed topics together (GEC + continual learning)."}, "questions": {"value": "- How much or how little did you ablate on the different filtering steps? For example, in the \"Noise Removal\" step, did you iterate much on the parameters here? Or did you just use standard reasonable assumptions? I'm quite curious on how some of these filtering parameters may affect the performance.\n- curious on how this will transfer to other architectures (e.g. state sapace models), and also other model sizes (No need for extra experiments! Just curious if this is something you've done)\n- How do you ensure annotators are annotating with the same metric/criteria in mind?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hYGW98Rlqp", "forum": "T7vcbdwHYH", "replyto": "T7vcbdwHYH", "signatures": ["ICLR.cc/2026/Conference/Submission25228/Reviewer_2STk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25228/Reviewer_2STk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762258363329, "cdate": 1762258363329, "tmdate": 1762943371213, "mdate": 1762943371213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}