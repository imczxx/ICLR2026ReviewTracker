{"id": "7FM0GBFhe5", "number": 3974, "cdate": 1757577752134, "mdate": 1759898060157, "content": {"title": "PRKV:Page Restruct KV Cache for High Accuracy and  Efficiency LLM Generation", "abstract": "As the key-value(KV) cache size scales with context length, accessing large KV\ncache each step and substantial GPU memory demand challenge us to deploy\nLLMs with long contexts.Various sparse attention methods have been proposed\nand offloading-based KV retrieval preserves entire KV cache in CPU memory\nand dynamically retrieves most relevant KV pairs for each decoding step, which\nperforms higher quality and effectively reduces GPU memory consumption than\nother line works. However, exiting KV retrieval performs page-level to reduce\nestimation overhead, which introduces inaccurate KV selection and significant re-\ntrieval overhead. We propose PRKV, a framework that both-optimizes algorithm\nand system for page-level KV retrieval with KV offloading. On the algorithm side,\nPRKV introduces hybrid KV selection that combines both static and dynamic KV\nselection strategies. On the system side, PRKV employs contiguous memory in-\ndexing and batched transfer optimizations to improve retrieval efficiency. Exper-\niments demonstrate that PRKV improve accuracy across various scenarios and\nmodels, delivering up to 6.75× speedup compared to SOTA KV retrieval methods.", "tldr": "Hybrid KV selection for achieving high accuracy", "keywords": ["Long-Context LLM Inference", "KV Cache Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e4255b105f65f0b90443f91cb92f2473d11e29c.pdf", "supplementary_material": "/attachment/4a20777460d75c8c874f1c4e30433e886a9f0250.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PRKV, a hybrid key value retrieval framework for long context LLM inference that combines token level reuse with dynamic page level selection.\nPRKV periodically reuses top k tokens from the previous step and a local window, then fills the remaining budget with page selection guided by compact page representations.\nIt restructures the cache so the reused tokens are stored contiguously, which simplifies indexing and reduces memory traffic.\nOn the systems side, PRKV uses a head major layout and a batched copy pipeline from CPU to GPU to cut transfer overheads.\nExperiments on retrieval and reasoning benchmarks show accuracy close to full cache and up to 6.75x end to end speedups over prior offloading methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper propose simple and effective hybrid design which combines prior-step token reuse with dynamic page selection to close the recall gap of page-only methods while keeping estimation overhead low.\n* HND layout and batched KV transfer are broadly reusable optimizations that materially reduce TPOT and drive the reported speedups.\n* Broad and convincing evaluation across multiple models and benchmarks with clear wins in both accuracy and latency."}, "weaknesses": {"value": "* The paper relocates static tokens to the front and refreshes every T steps (Alg. 1), but the overhead (CPU time and additional PCIe traffic) of reordering KV pages/indices is not reported.\n* Results stop at 14B‑parameter general models and 8B for reasoning. It remains unclear how PRKV behaves for larger size model, and models with different attention architecture (e.g. DeepSeek).\n* The LongBench evaluation setup seems a bit tricky: The evaluation truncates inputs longer than the model’s context by taking half from the beginning and half from the end, then concatenating. This setup disadvantages the “full cache” baseline and can artificially make PRKV look better than “full attention” because PRKV can still select tokens from the entire document while the baseline never sees the middle. Could you elaborate on the setup?"}, "questions": {"value": "* Additionally, the score in the needle-in-a-haystack seems under-specified, making it hard to interpret results. Could you specify how you measure the success, why use Kimi API as the judge model (what about other models as the judge results)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rO6V8YNmqE", "forum": "7FM0GBFhe5", "replyto": "7FM0GBFhe5", "signatures": ["ICLR.cc/2026/Conference/Submission3974/Reviewer_Vcit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3974/Reviewer_Vcit"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896042251, "cdate": 1761896042251, "tmdate": 1762917121665, "mdate": 1762917121665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRKV, a framework for page-level KV retrieval that combines static reuse and dynamic selection through a “hybrid KV selection” strategy. The authors also propose system optimizations, namely contiguous memory indexing using the HND layout and batched KV transfers to improve retrieval efficiency. PRKV reports up to 6.75× speedup and near-full-cache accuracy across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clarity and completeness:\nThe paper is reasonably well written and provides a comprehensive experimental evaluation across several standard long-context benchmarks.\n2. Incremental algorithmic variation:\nThe proposed periodic static token update introduces a minor variation to existing retrieval schemes. While not particularly innovative, it reflects an effort to stabilize long-sequence performance by refreshing the static token set."}, "weaknesses": {"value": "1. Limited algorithmic novelty:\nThe proposed method largely concatenates two existing paradigms of KV-cache compression, static dropping and dynamic retrieval, without introducing a fundamentally new principle. As a result, the algorithmic contribution is incremental and lacks clear conceptual advancement.\n2. Questionable system design and justification:\nThe claimed system optimizations are not well substantiated. Although the HND layout may simplify head-wise indexing, it is unclear how this layout integrates with modern GPU attention kernels, which typically assume NHD for coalesced access and contiguous memory traversal (as used in FlashAttention and vLLM). Similarly, the batched KV transfer optimization resembles standard engineering practice rather than a substantive research contribution."}, "questions": {"value": "1. Please justify how the HND layout efficiently supports GPU attention kernels. Most state-of-the-art implementations (e.g., FlashAttention, FlashInfer) assume NHD layout for coalesced tensor access. Has PRKV been benchmarked with real kernel implementations to confirm compatibility?\n2. The proposed “static token selection” is periodically updated. Does this imply that the entire KV cache must still be retained in CPU memory to enable re-selection? If so, how does this affect the memory footprint compared to fully static methods such as SnapKV?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iZluMKod7P", "forum": "7FM0GBFhe5", "replyto": "7FM0GBFhe5", "signatures": ["ICLR.cc/2026/Conference/Submission3974/Reviewer_k1cZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3974/Reviewer_k1cZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930230397, "cdate": 1761930230397, "tmdate": 1762917120118, "mdate": 1762917120118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PRKV, a hybrid sparse-attention framework for long-context LLM decoding with KV offloading. It combines a static token-level set built from an observation window (reused and periodically updated) and a dynamic page-level selection. And it reorganizes the KV layout and uses batched transfers to reduce PCIe traffic and kernel launch overhead. Experimental results report state-of-the-art quality under fixed KV budgets and impressive end-to-end speedups over prior offloading baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a practical bottleneck in long-context serving: reducing KV retrieval latency and GPU memory pressure during decoding with CPU offloading, while maintaining quality.  ￼\n- Well-motivated hybrid design by reusing prior top-k with local window to boost attention recall\n- System optimizations are concrete and codesigned with the proposed algorithm"}, "weaknesses": {"value": "- Some method knobs are central but their sensitivity and generalization across models or tasks are not fully explored   ￼  ￼\n- A few writing issues slightly obscure otherwise solid ideas.  ￼"}, "questions": {"value": "Thank you for the submission. I like the paper overall, the hybrid selection insight is compelling, and the system side is thoughtfully engineered.  However, several descriptions are still vague or underspecified. Clarifications that would strengthen the paper:\n- The authors show higher recall with a hybrid split and reuse and local window. Could you provide principled guidance for choosing S and the local window size across different models/tasks? Extra sensitivity curves beyond the current S-only ablation will be appreciated to better estimate the impact of the proposed method.  ￼  ￼\n- The algorithm periodically recomputes the static set and restructures pages. What is the measured overhead of this reorganization?   ￼\n- In some scenarios, PRKV matches or even beats full attention. What is the reason for this?  ￼\n- The HND layout speeds up head-wise indexing, and the batched-transfer path reduces kernel launches. Is there any restriction to apply this layout? ￼"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Thank you for the submission. I like the paper overall, the hybrid selection insight is compelling, and the system side is thoughtfully engineered.  However, several descriptions are still vague or underspecified. Clarifications that would strengthen the paper:\n- The authors show higher recall with a hybrid split and reuse and local window. Could you provide principled guidance for choosing S and the local window size across different models/tasks? Extra sensitivity curves beyond the current S-only ablation will be appreciated to better estimate the impact of the proposed method.  ￼  ￼\n- The algorithm periodically recomputes the static set and restructures pages. What is the measured overhead of this reorganization?   ￼\n- In some scenarios, PRKV matches or even beats full attention. What is the reason for this?  ￼\n- The HND layout speeds up head-wise indexing, and the batched-transfer path reduces kernel launches. Is there any restriction to apply this layout? ￼"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sxljjy3Mba", "forum": "7FM0GBFhe5", "replyto": "7FM0GBFhe5", "signatures": ["ICLR.cc/2026/Conference/Submission3974/Reviewer_g8ut"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3974/Reviewer_g8ut"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3974/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978417486, "cdate": 1761978417486, "tmdate": 1762917119859, "mdate": 1762917119859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}