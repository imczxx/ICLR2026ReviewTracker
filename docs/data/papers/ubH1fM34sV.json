{"id": "ubH1fM34sV", "number": 2722, "cdate": 1757223113226, "mdate": 1759898131437, "content": {"title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping", "abstract": "GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.", "tldr": "RULER tokens provide explicit position-to-pixel mapping and I-MRoPE balances spatial encoding, transforming GUI grounding from regression to referencing with strong high-resolution generalization.", "keywords": ["Vision-Language Models (VLMs)", "GUI Grounding", "Spatial Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebb27a9e9d4fa60a5d053c8bd86ca6334209d1c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces two methods—RULER tokens and I-MRoPE—to enhance the visual grounding capability of MLLMs. The approach is intuitive and theoriatical. Experimental results on mainstream benchmarks show consistent gains in grounding accuracy, suggesting that the proposed techniques are effective. However, the underlying motivation is not sufficiently developed, and the paper lacks deeper analysis or interpretability studies to clarify why these methods lead to improved localization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a more intuitive visual grounding strategy by positional encoding from the perspective of spatial awareness, which offers a clearer interpretation of how models localize objects.\n\n2. The authors identify inherent limitations in existing positional encoding schemes and present targeted enhancements that effectively improve localization capability.\n\n3. Extensive experiments on established benchmarks demonstrate the empirical effectiveness of the proposed methods."}, "weaknesses": {"value": "1. The paper lacks a compelling and intuitive research motivation, making the introduction of the two proposed strategies feel abrupt and insufficiently grounded in the broader challenges of multimodal grounding. Stronger design intuition and pre-experiments would help clarify why these particular techniques are necessary and meaningful. This issue is especially pronounced for I-MRoPE, where the rationale behind the method is under-developed and its connection to real-world grounding failures remains unclear.\n\n2. The experimental findings feel somewhat superficial and lack sufficient breadth. First, recent advances in grounding are rapidly evolving, and the paper does not fully situate its results within the latest leaderboard trends or analyze whether the proposed strategies meaningfully reduce existing performance gaps. Second, there are methods that outperform the proposed approach, yet the paper does not investigate the reasons behind these differences, limiting the reader’s understanding of the technique’s strengths and weaknesses. Third, the evaluation on MLLM foundation models remains limited. For example, newer multimodal systems such as mini-CPM exhibit strong grounding ability, would they experience the same deficiencies identified here, and would the proposed strategies still lead to improvement? \n\n3. The results in Table 3 indicate that the proposed strategy does improve the localization capability of MLLMs; however, the contribution of I-MRoPE appears marginal. This raises questions about the necessity and justification for combining the two methods as presented. Additionally, fine-tuning results show notable gains, yet the paper lacks interpretability analysis, such as attention behavior or grounding heatmaps, to substantiate how and why these improvements occur. Without such evidence, the effectiveness of each component and the underlying mechanism behind the observed performance changes remain insufficiently explained.\n\n4. Grounding enhancement is often motivated by its impact on downstream tasks such as GUI tasks (AndroidControl, AndroidWorld, OSWorld). The current experimental setup does not demonstrate whether the proposed improvements translate into measurable gains in these more realistic use cases. Additional downstream evaluations would therefore significantly strengthen the practical relevance and efficacy of the proposed approach.\n\n5. It would be valuable to investigate scaling behavior with respect to model size and data volume. Establishing scaling laws could provide deeper insight into the effectiveness and limitations of the proposed strategies, and help clarify whether improvements persist as models and datasets grow.\n\n6. The paper would benefit from a more intuitive, step-by-step case study to clarify the end-to-end workflow of the proposed method. In particular, it would be helpful to illustrate how RULER tokens are extracted, how the I-MRoPE module is instantiated, and how these components jointly contribute to the final grounding prediction. Such a detailed example would greatly improve the accessibility and interpretability of the approach.\n\n7. Since the proposed approach introduces additional tokens, it would be valuable to provide a more intuitive analysis of the computational implications, for example by measuring the impact on inference latency. \n\n8. What advantages does this approach offer, compared to directly providing the object locations from OCR parsing and analysing the layout via CoT, by offering additional image token references?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DcMRMu6QMg", "forum": "ubH1fM34sV", "replyto": "ubH1fM34sV", "signatures": ["ICLR.cc/2026/Conference/Submission2722/Reviewer_n3e7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2722/Reviewer_n3e7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761469450746, "cdate": 1761469450746, "tmdate": 1762916344765, "mdate": 1762916344765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the failure of VLMs in GUI grounding, specifically their inability to generalize to high-resolution displays unseen during training. The authors identify the root cause as the model's reliance on implicitly regressing coordinates from visual features. They propose two innovations: 1) RULER tokens, which provide explicit coordinate references to transform grounding into a robust \"reference-and-adjust\" mechanism, and 2) I-MROPE, a balanced positional encoding that corrects frequency bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper excels at identifying a critical and well-defined weakness in existing models (the instability of implicit coordinate regression). The proposed RULER token mechanism is an intuitive solution\n2. The method shows its significant gains on the SS-Pro, which features high-resolution displays and a domain shift from the training data. This provides strong evidence for the authors' core claim that explicit coordinate referencing is more robust than existing works."}, "weaknesses": {"value": "1. The paper's comparison in Table 1 is a significant weakness. The authors' best from-scratch model (32.1%) significantly underperforms established baselines like UI-TARS-7B (35.7%) and GUI-Actor-7B (44.2%). While the tuned model (37.2%) is more competitive, it still trails GUI-Actor. The authors attribute this to differences in training data, but this is a critical point that need further explanations.\n2. The core concept of this paper seems to be a variation of explicit tokens or coord embeddings that have been explored in other visual grounding (namely gui-actor) or object detection contexts.\n3. Minor: The paper's strength lies in framing it as a \"reference-and-adjust\" mechanism inspired by induction heads, but it lacks direct analysis (e.g., attention probing) to prove the model actually learns this mechanism rather than simply using the tokens as a stronger signal.\n4. Please refer to my question."}, "questions": {"value": "1. The models are trained *exclusively* on the UGround dataset (web) but evaluated on SS-Pro, which features professional desktop applications (CAD, Scientific, etc.). The authors frame this as a test of generalization, but it introduces a massive domain / resolution shift?\n2. The ablation study in Table 1 suggests that the primary performance gain comes from RULER tokens, not I-MROPE. The performance difference between `LLaVA-NeXT + MROPE` (29.2) and `LLaVA-NeXT + I-MROPE` (29.4) is marginal. So, does that mean I-MROPE is an incremental, minor fix that adds complexity for a negligible benefit, and that this work should focus almost entirely on RULER as the core contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BXMFur8OL1", "forum": "ubH1fM34sV", "replyto": "ubH1fM34sV", "signatures": ["ICLR.cc/2026/Conference/Submission2722/Reviewer_j9wR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2722/Reviewer_j9wR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999348992, "cdate": 1761999348992, "tmdate": 1762916344592, "mdate": 1762916344592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to addresses the problem of patch-to-pixel mapping in the GUI grounding task. The key contributions of the paper lie in (1) proposing RULER tokens to function as explicit coordinate markers; (2) proposing Interleaved MRoPE (I-MRoPE) to address the asymmetry of standard positional schemes. The experiments on three benchmarks illustrate the effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well motivated, focusing on a novel problem in grounding tasks.\n2. The proposed methods RULER and I-MRoPE are interesting and makes sense."}, "weaknesses": {"value": "1. It is unclear whether the proposed method can generalize to other grounding tasks beyond GUI scenarios. Also, how does the method perform when applied to GUI Agent tasks (e.g., AndroidWorld, WebArena) ?\n2. It lacks experiments on whether training with these methods can affect the performances on the general ability of VLMs.\n3. GUI-Actor also tackles on the similar research question while RULER fails to give comparable or superior performances. Also, the comparisons on ScreenSpot series benchmarks lack many recent strong baselines. These weaknesses question whether the proposed method is truly effective and competitive ?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ljTmUKeWYZ", "forum": "ubH1fM34sV", "replyto": "ubH1fM34sV", "signatures": ["ICLR.cc/2026/Conference/Submission2722/Reviewer_WRrH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2722/Reviewer_WRrH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094892281, "cdate": 1762094892281, "tmdate": 1762916344464, "mdate": 1762916344464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The topic is interesting\n2) The writing is good\n3) The experiments show the effectiveness of the proposed method."}, "weaknesses": {"value": "1) The introduction of new tokens and spatial encoding methods adds complexity to the model architecture, which may require more resources and time for development and training.\n2) Dependency on Training Data: While the method improves performance on high-resolution displays, it may still be limited by the quality and diversity of the training data used, potentially impacting performance on very novel interfaces.\n3) Potential Overfitting: The focus on explicit mappings might lead to overfitting on specific tasks or resolutions, limiting the model's adaptability to other types of GUI environments not represented in the training data."}, "questions": {"value": "1) The introduction of new tokens and spatial encoding methods adds complexity to the model architecture, which may require more resources and time for development and training.\n2) Dependency on Training Data: While the method improves performance on high-resolution displays, it may still be limited by the quality and diversity of the training data used, potentially impacting performance on very novel interfaces.\n3) Potential Overfitting: The focus on explicit mappings might lead to overfitting on specific tasks or resolutions, limiting the model's adaptability to other types of GUI environments not represented in the training data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cGGGv5UU91", "forum": "ubH1fM34sV", "replyto": "ubH1fM34sV", "signatures": ["ICLR.cc/2026/Conference/Submission2722/Reviewer_Jm9N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2722/Reviewer_Jm9N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152424309, "cdate": 1762152424309, "tmdate": 1762916344231, "mdate": 1762916344231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}