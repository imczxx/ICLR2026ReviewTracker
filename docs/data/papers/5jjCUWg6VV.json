{"id": "5jjCUWg6VV", "number": 12416, "cdate": 1758207635550, "mdate": 1763706460074, "content": {"title": "Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning", "abstract": "Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.", "tldr": "", "keywords": ["Reinforcement Learning", "Optimal Control", "Deep Reachability"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f5b64d7036dea6f73132ff3b1a4aa93b51d2634.pdf", "supplementary_material": "/attachment/06fa3b1bb291977b927a88b10059b678cacc4ddc.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an exploration method for identifying a subset of feasible initial conditions in the parameter-robust avoid problem. The core component is a conservative feasibility classifier that incrementally expands the estimated feasible region from collected samples. The policy is updated by approximately solving a robust policy optimization problem and then executed to gather new data for further classifier refinement. Across MuJoCo and other control benchmarks, FGE achieves solid safe coverage compared to standard baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It's interesting how the paper connects feasible-set estimation with robust policy optimization. The formulation naturally follows from the parameter-robust avoid problem and builds a nice bridge between reachability analysis and safe RL. \n\n- The proposed method is empirically validated on several benchmarks, showing consistent improvements in safe coverage. Theoretical claims are somewhat idealized but conceptually sound (see Weaknesses for details).\n\n- Overall, the paper offers a useful approach for safe exploration that explicitly tackles feasibility estimation, even if its scalability to high-dimensional parameter spaces remains unexplored."}, "weaknesses": {"value": "- Theorem 1’s guarantees (zero false positives and controllable false negatives) hold only for a fixed policy $\\pi$. But, FGE continuously updates $\\pi$ using PPO, making the rollout distribution $\\rho$ and the conditional success probability non-stationary. This violates the theorem’s assumptions and undermines the claimed guarantee. \n\n- Since FGE only samples parameters classified as infeasible, any false positive (infeasible but predicted feasible) is never revisited. This can cause the algorithm to miss meaningful parts of the parameter space and under-cover the true feasible set.\n\n- If a single rollout happens to succeed by chance (e.g., due to disturbances or randomness), that parameter is permanently added to $D_f$. These accidental successes can cause the estimated feasible set to become larger than it actually is, especially near boundaries where the system is sensitive to small variations.\n\n- All experiments use low-dimensional parameter spaces. Performance in higher-dimensional or stochastic settings remains untested."}, "questions": {"value": "- How do you justify using Theorem 1 although $\\pi$ and $\\rho$ are continuously updated during training? Have you considered an online learning or bounded drift variant of the guarantee?\n\n- Did you observe blind spots caused by false positives in practice? Could uncertainty-based or boundary-based focused sampling (e.g., entropy or ensemble variance) alleviate this issue?\n\n- How sensitive is performance to occasional mislabeling from accidental successes? Have you tried multi-rollout or confidence-based labeling?\n\n- Have you tested FGE in higher-dimensional parameter spaces (at least in 5D)? What are the computational or sample-efficiency challenges as the parameter dimension grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WKQy5LV3Fi", "forum": "5jjCUWg6VV", "replyto": "5jjCUWg6VV", "signatures": ["ICLR.cc/2026/Conference/Submission12416/Reviewer_kNnA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12416/Reviewer_kNnA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895080651, "cdate": 1761895080651, "tmdate": 1762923308977, "mdate": 1762923308977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a special case of a safety problem in the context of decision making, that the authors call `parameter-robust avoid` problem. The goal is to identify a subset of parameters/initial conditions, which lead to the existence of a safe policy. More specifically, a safe policy is the policy that avoids an unsafe set for *all* parameter values. \n\nThe authors first reformulate the problem as a maximisation of the subset of the parameters subject to constraints: first hitting time of the avoid set and dynamics constraints. Then the authors provide a two-stage algorithm common for robust parameter optimization problems, where they optimize over a safe policy and maximise the support of the safe parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is solving a complex problem with an interesting solution, and good results\n* There’s a good ablation study\n* The exposition of results is great! I wish more papers would present the results this way. There’s a thesis of what algorithm does better than competitors and the supporting evidence."}, "weaknesses": {"value": "* The paper’s flow can be improved as it is not easy to understand the problem and the contribution at the first read. While it’s not easy to write a complicated contribution in an easy way, some steps could be taken:\n     * Provide a concrete example of the problem - why is it important? \n     * Try to avoid a bottom up approach to explain the solution as much as possible. When there’s a lot of steps to get from point A to point B, the reader may lose the thread of these explanations. For instance, \n          - I think formulation (6) can be explained intuitively without Lemmas and Theorems, which will give the reader an idea of what’s going to happen. Then explaining how we get there, will give the reader the context (if the reader is interested).\n          - The same is true for Section 4.1. In this case, for example, the regularizer `\\psi` is not even defined when introduced, and it’s not entirely clear how this algorithm would work. Is there iteration, stopping conditions, do we sample different `\\theta` on every step? \n* It would be good to discuss  the problem definition in lines 121-124 in a bit more detail. What are the assumptions on \\Theta, h, f. Why is this problem hard?\n* Why mention saddle-point convergence if the convergence guarantees are very restrictive? The authors mention that actually, but it all becomes confusing fast. I’d suggest having this discussion in conclusions/limitations. \n* What’s the main idea of the parameter set estimation algorithm? Overall, I found this section hard to understand.\n* It is not clear what the RL formulation(s) of the problem is. What formulations were used for baselines?\n* I don’t quite understand what coverage is. It seems this metric depends on the algorithms, which is not easy to understand. Do we have access to the ground truth safe parameters? Is it possible to estimate the ground truth set of safe parameters?"}, "questions": {"value": "* I find it confusing how the authors define reachability. I looked in Tomlin et al 2000, and they have a more common definition of reachability: the set of states that the dynamical system can reach with a control policy. Safety is typically not a requirement for reachability definition. I would also point out that Tomlin et al 2000 study discrete-event systems and their safety definition is not directly applicable to any dynamical system. I suggest simply formally providing a definition of the authors want to study, even if these concepts are inspired by Tomlin et al 2000. \n* Could you explain intuitively Section 4.2\n* How is policy optimization performed? It seems it’s just a gradient descent algorithm. Did the authors use PPO, Reinforce or something custom here?\n* Line 470. This is a bit confusing, I read it first as your method outperforms NSF just marginally. But this doesn’t seem to be the case! I’d recommend a more direct language here: your approach outperforms significantly both NSF and PLR.\n* Small issues:\n    * Lemma 1. Technically, in Lemma 1 equation 4 is not describing a sublevel set for V(x, \\theta) but a level set. \n    * Eq 8. `\\Psi` is not defined"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EoPPeYMKaQ", "forum": "5jjCUWg6VV", "replyto": "5jjCUWg6VV", "signatures": ["ICLR.cc/2026/Conference/Submission12416/Reviewer_6h75"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12416/Reviewer_6h75"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967871107, "cdate": 1761967871107, "tmdate": 1762923308608, "mdate": 1762923308608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method (called FGE) for solving an initial-states-robust avoid (reachability) problem when feasibility of the initial states set is unknown. To achieve this, it jointly learns (a) a classifier that identifies which parameters (defining initial states) admit a safety-preserving policy and (b) trains a robust policy over that discovered feasible subset by alternating approximate saddle-point / best-response updates and a replay/rehearsal scheme. FGE adapts the reset (initial-state) distribution as a mixture of a base distribution, an explore distribution (via rejection sampling against the feasibility classifier) to expand the feasible set, and a rehearsal buffer to stabilize robust optimisation. The paper then provides theoretical guarantees on the classifier and empirical evidence across several MuJoCo and vehicle/aircraft tasks that FGE yields substantially larger safe (feasible) initial state sets than baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Theoretical grounding:** The paper gives a clear problem formulation, shows equivalence between its indicator-style objective and a reachability formulation (Lemma 1), and analyzes properties of the learned feasibility classifier (Theorem 1), which underpin the empirical design choices\n- **Clear, practical algorithmic pipeline:** FGE is presented as an algorithm that can be dropped on top of any on-policy method (PPO in the experiments), combining feasibility learning, rejection-sampling exploration, and a rehearsal buffer. This is an appealing and implementable recipe. Algorithmic pseudocode and reset-distribution mechanics are provided. \n- **Several empirical evaluations and useful metrics:** Experiments cover multiple domains (ToyLevels, Dubins, Hopper, HalfCheetah, FixedWing) and a broad set of baselines. The coverage metrics and derived diagnostics (Coverage Gain vs DR, Coverage Loss vs DR, etc) are insightful, especially the Coverage Gain vs DR plots that highlight FGE’s ability to find rare hard initial states. Several ablations (e.g. explore / rehearsal / classifier choices) are also provided and are informative.\n- **Significance for safe RL:** The paper tackles an important, well-motivated gap between reachability/robust objectives and the sampling of initial states in RL: with unknown feasibility of those initial states. The proposed formalization may be a useful addition to the safe-RL literature."}, "weaknesses": {"value": "- **Notation and readability:** \n  - The abstract and introduction are hard to read because of vague sentences. For example, there is repeated use of terms like \"initial conditions\" and \"initial parameters\" throughout without defining what they mean in the context of this paper (this is only done in the next section). I initially thought \"initial conditions\" meant the initial conditions of an optimisation process, and not the distribution/set of initial states. Similarly, I thought initial parameters meant the initial parameters of the policy neural network, but instead it meant some parameters that define the deterministic dynamics function and specific initial state for an episode.\n  - The problem formulation was also extremely confusing. The paper uses notations from control theory instead of RL (despite the RL focus), and uses the usual RL notations for something else. For example, A and S are not action and state spaces but instead avoid states and safe initial states respectively. Also the value function is not the RL value function (it is not an expectation of cumulative rewards and does not necessarily satisfy the Bellman equations). This just makes readability extremely more difficult than it has to be as the paper progresses. It would have been much clearer if the authors just used standard RL notations and terminology. For example the problem setting seems to be essentially a deterministic contextual MDP with an almost-surely safe RL objective (the agent needs to avoid unsafe states with probability 1).\n  - Minor: Lots of typos and mathematical impreciseness. E.g.\n    - Inconsistent use of = and := (both are being used for definitions). The general convention is using = to claim equality and := to define a function. \n    - Equation 1 $f_{\\theta}$ and $x_0$ are undefined (explicitly say what they are). \n    - Lines 130 and 144 $V(x_0;\\theta)$ should be $V(x;\\theta)$. \n    - $T_{\\theta}$ should depend on $x$.\n    - Line 1348 \"that that\".\n\n- **Scope limitations (method assumptions):** \n  - The approach is only applicable to episodic deterministic environments where the agent have access to the reset function (can chose the initial state of each episode). \n  - The paper defines the dynamics and policy as being conditioned on the parameter $\\theta$ (which is fixed thoughout an episode), but the experiments appear to only change the start sates with $\\theta$. It is unclear what happens when the environment dynamics also change with $\\theta$, as in most contextual MDPs.\n  - It is also unclear how FGE scales to high-dimensional observations such as RGB pixels (the experiments seem to use state vectors). The feasibility classifier and rejection sampling may become challenging in very high-dim $\\theta$ or when observations are high-dimensional. \n\n- **Theory–experiment alignment:** The theoretical setup conditions (convexity/concavity, access to best-response oracles, deterministic dynamics) needed for convergence claims are not satisfied in the empirical setups; while the paper acknowledges this, it should more explicitly describe the gap and, if possible, include analyses showing sensitivity to the violated assumptions (some ablations exist but more explicit discussion is needed).\n\n- **Experimental reporting gaps (Major):**  \n  - The paper does not show standard reward / success/failure training curves in the main results (only coverage metrics are emphasized). For a safe-RL paper, plots showing rewards and success/failure rates (or explicit returns vs constraint violations) would help assess whether policies genuinely optimize task objectives while avoiding unsafe states. A natural concern regarding the proposed method is that the oversampling of hard initial states may compromise policy learning for the actual task.\n  - The paper also does not clearly report the reward functions used in each environment (how goal reward and avoid/safety labels are combined). Given the task descriptions, it seems like it is not just something as simple as the negative of the indicator function in Equation 3. Because the RL objective and evaluation hinge on both avoiding unsafe regions and achieving task goals, omission of explicit reward definitions (and corresponding training plots) makes it hard to judge whether FGE is improving safety, task performance, or both (which is their main motivation/claim for improving the initial states distribution).\n\n- **Baselines and comparisons (Major):**\n  - The choice of baselines leans heavily on robust RL, curriculum, and UED methods (instantiated with PPO). However, no standard safe-RL baselines are used nor cited (such as the ones in the OmniSafe benchmark, like PPO-lagrangian and CPO). More importantly, it is also unclear why the authors did not compare against works that are also focused on maximally safe policies (the setting of this paper), like Saute-RL and ROSARL. \n  - This is important because (a) FGE explicitly uses the safety label / safety function to train its feasibility classifier and to guide resets  (information that many baselines may not use) and (b) the paper's stated goal is to produce safe policies. The absence of safe-RL baselines and citations to directly related safe-RL literature weakens the claims substantially."}, "questions": {"value": "Please see the weaknesses above. Mainly:\n\n1. How is the reward function combined with the safety constraints in each environment? If not, how do the current baselines \"see\" the safety constraints?\n2. Could the authors provide reward and/or success-rate plots to show how the improved initial states distributions affect the safety constraints and return maximization (which is the main motivation for this work)?\n3. Why were safe RL baselines (e.g., PPO-Lagrangian, CPO, Sauté-RL, ROSARL) omitted from the comparison? Would including them alter the conclusions?\n4. How does FGE scale to high-dimensional observation spaces (e.g., RGB pixel inputs)?\n5. In the theoretical formulation, both dynamics and policy depend on $\\theta$, but the experiments vary only the start distribution. What happens when environment dynamics change with $\\theta$?\n6. Please fix minor presentation issues throughout (e.g., undefined variables in Eq. 1 and incorrect state variables in definitions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CL5SXJM7vD", "forum": "5jjCUWg6VV", "replyto": "5jjCUWg6VV", "signatures": ["ICLR.cc/2026/Conference/Submission12416/Reviewer_CPF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12416/Reviewer_CPF8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12416/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762458464415, "cdate": 1762458464415, "tmdate": 1762923308189, "mdate": 1762923308189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}