{"id": "kGNtM8NteM", "number": 18751, "cdate": 1758290633945, "mdate": 1759897083175, "content": {"title": "Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay", "abstract": "Biological neural networks (like the hippocampus) can internally generate \"replay\" resembling stimulus-driven activity.\nRecent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate.\nReplay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description.\nWe re-examine noisy RNN replay as sampling to understand or improve it in three ways:\n(1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay.\n(2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay.\n(3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling and short-term facilitation, and show that, when combined with adaptation, it counters slowness while maintaining exploration.\nWe verify our findings via path-integration of 2D paths in T-maze and triangular environments and of high-dimensional paths of synthetic rat place cell activity.", "tldr": "Without inputs, path-integrating RNNs (like the hippocampus) generate “replay” activity. We describe this as non-stationary Langevin sampling, and examine existing and our own methods of biasing it via modified RNN dynamics.", "keywords": ["RNN", "replay", "hippocampus", "path integration", "predictive", "generative model", "Langevin sampling", "gradients", "score", "adaptation", "underdamped", "momentum", "compression", "exploration"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6981612def180fc72c14825fdfa010bedceb9942.pdf", "supplementary_material": "/attachment/95c90b194f7c2dcb86a6ab93d7aef4e860041cd2.zip"}, "replies": [{"content": {"summary": {"value": "This paper builds on existing theory of replay in path-integrating RNNs to provide mechanisms that explain phenomena known to exist in vivo: temporal compression and exploration.\n\nTheir first analysis is refuting an assumption from prior work, which assumes that p(r(t)) is stationary, and reveals the role of leakage in helping path integration.\n\nThe second analysis proposes two mechanisms (underdampening and adaptation) to explain phenomena of replay observed in vivo, related to the speed of replay: one slows it down and the other one quickens it."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. A principled, quantitative approach to the phenomenon of replay.\n\nS2. Great related works.\n\nS3. The figures from the experiments are very clear and comprehensive. The experiments themselves seem sound to illustrate and support the theoretical results."}, "weaknesses": {"value": "W1. The motivation behind the proposed mechanisms is difficult to follow. Maybe it is clearer to a neuroscience audience, but for an ICLR audience it needs to be explained. The authors refute the stationarity of p(r(t)) but I had trouble following *why* this assumption must be violated for their framework to hold. \n\nSimilarly, why this choice of underdamping and adaptation mechanisms? While they indeed modulate the temporal dynamics (e.g., the speed of replay), the paper does not convincingly argue (or I missed it) why these mechanisms were chosen over other plausible alternatives. \n\nW2. The intended scope of the framework is a bit unclear. Is the work primarily aimed to model biological neural circuits (in which case novel, testable predictions should be articulated, and claims should be supported by biological experiments) or to propose computational mechanisms relevant for RNNs (in which case implications for model training or performance should be discussed)? Clarifying this distinction would help position the contributions.\n\nW3. Section 2 is hard to follow. The sequence of Definition, Assumption, Lemma, Theorem is a bit abrupt and difficult to parse. It could benefit from a higher-level narrative or schematic overview or illustrative toy examples. As written, it's hard to follow how each statement supports the main argument.\n\nW4. The claim that the proposed mechanisms “improve hippocampal RNN replay” is currently unsubstantiated. The paper presents qualitative examples but lacks quantitative metrics or statistical comparisons with experimental data.\n\nThe use of the term “hippocampal” seems too specific given the lack of biological validation. Replace with \"path integration\" o?Using more neutral terminology would avoid overinterpretation of the biological relevance."}, "questions": {"value": "How many RNNs (different seeds) were trained each time?\n\nThe definition of RNN in eqn (4) is very generic. What function f is used in practice (leaky current? leaky firing rate?)  and what is the biological faithfulness of that choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "DdYHxIyhzq", "forum": "kGNtM8NteM", "replyto": "kGNtM8NteM", "signatures": ["ICLR.cc/2026/Conference/Submission18751/Reviewer_FE8C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18751/Reviewer_FE8C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867680357, "cdate": 1761867680357, "tmdate": 1762928474382, "mdate": 1762928474382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to improve the understanding of replay in RNNs from the perspective of stochastic sampling and dynamical systems theory. The authors re-examine how specific network mechanisms, such as leakage, adaptation, and momentum, shape the resulting replay dynamics. They first show that the score function driving replay is time-varying and difficult to estimate directly, motivating the inclusion of hidden-state leakage as a stabilizing inductive bias that improves both training and replay quality. Next, they show that adaptation, modeled as negative feedback, encourages exploration in the RNN’s internally generated activity but results in slower replay. To address this, the authors introduce hidden-state momentum, connecting it to underdamped Langevin dynamics, which yields faster and more temporally compressed replay that more closely mirrors hippocampal replay observed in the brain. Experiments in simulated 2D navigation and synthetic place-cell environments show that combining adaptation with underdamped dynamics produces replay that is both diverse and efficiently time-compressed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper provides a deeper understanding of replay dynamics in RNNs under a set of strong but well-motivated assumptions. It effectively bridges neuroscience and machine learning, which machine learning draws inspiration from. They are seemingly the first to introduce an RNN framework that exhibits temporally compressed replay, resembling replay observed in the hippocampus. The experiments, though conducted in simplified environments, are thoughtfully designed and demonstrate how mechanisms such as leakage, adaptation, and underdamped dynamics influence replay."}, "weaknesses": {"value": "While this paper helps further the understanding in the study of replay dynamics, several aspects could be improved for clarity and empirical depth. Some figures. such as Figure 1, could more clearly distinguish functions such as s(t) and r(t) to improve readability. The discussion linking the underdamped mechanism to short-term facilitation is intriguing but remains speculative without experimental validation or quantitative comparison to biological data. Additionally, the claim that the proposed mechanisms improve replay is supported primarily by qualitative evidence; including more quantitative analyses would help strengthen the paper’s conclusions."}, "questions": {"value": "Can similar conclusions be made for different architectures like gated RNNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ft0wLXo8bc", "forum": "kGNtM8NteM", "replyto": "kGNtM8NteM", "signatures": ["ICLR.cc/2026/Conference/Submission18751/Reviewer_Qmcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18751/Reviewer_Qmcp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948122802, "cdate": 1761948122802, "tmdate": 1762928472872, "mdate": 1762928472872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors study RNNs in the context of hippocampal replay, in which RNNs are first trained in the presence of task-relevant inputs and later their neural activations are sampled without inputs. Authors show that the score function of the resulting probability distribution for the neural activities is not stationary, which corrects an incorrect assumption in the literature, and incorporate three biologically relevant mechanisms under one common framework."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Authors present a convincing unified theoretical framework of RNN replay that incorporates membrane voltage decay, adaptation, and STSP.\n\n- The paper identifies the invalidity of a key assumption made regularly in the literature.\n\n- Authors explain modulation of replay speed, which to my understanding was not explained with theoretical models before."}, "weaknesses": {"value": "- Presentation-wise, I find the current manuscript very dense. For instance, what does it mean for an RNN to perform Langevin sampling? What is sampling, what is being sampled? A more comprehensive background for the broader audience may be desirable here. Also, how can an RNN sleep? Do the authors refer to running RNNs without any inputs? If so, what are the initial conditions? As a computational neuroscientists not immediately working on these topics, I was at first very confused about the content of the work. For publication in ICLR, which is not a specialized journal, a lot less jargon would have ben desirable.\n\n- My major concern on the results is that authors are rediscovering the fact that discretized RNNs, with discretization time being equal to the neural decay constant $Δ t = \\tau$, are actually not a good model of continuous neural dynamics. Allow me to elaborate. The case with no leakage is actually not an accurate approximation to the continuous RNN dynamics $\\tau \\dot r = -r + f(r(t),u(t))$, where $f(r(t),u(t))$ is the flow map. If we discretize this with Euler's discretization, we arrive at $r(t+Δt) = (1-Δ t/ \\tau) r(t) + (1-Δ t/\\tau) f(r(t),u(t))$. Now, the line of research that authors are citing simply assumed $Δ t = \\tau$, which is certainly not reasonable since the discretization only makes sence if $Δ t \\ll \\tau$. Now, the authors once again are finding that \"leakage\" is important for path-integration, which simply means $\\tau$ needs to be set properly based on task demands. In other words, scientifically, this simply means one should let the task decide the timescales. There is no \"leakage\" in continuous systems, but the time scale of the decay $\\tau$."}, "questions": {"value": "Can you please address the two weaknesses above? \n\nMy overall assessment is as follows: This work is interesting, and does have the novelty and potential impact that ICLR seeks. Some reframing of the results is needed (e.g., leakage...), and presentation needs to be cleaned up significantly. Once these changes are made, I would feel comfortable recommending an acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D3gUrqwSvH", "forum": "kGNtM8NteM", "replyto": "kGNtM8NteM", "signatures": ["ICLR.cc/2026/Conference/Submission18751/Reviewer_WDVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18751/Reviewer_WDVo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953766705, "cdate": 1761953766705, "tmdate": 1762928469474, "mdate": 1762928469474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the noisy recurrent neural networks (RNNs) from the perspective of biological neural networks (like the hippocampus). Since hippocampus generate the “replay” stimulus — internally-generated sequences of activity during quiescent periods (e.g., rest or sleep) that resemble trajectories during active behavior, many works have started to analyze noisy RNNs which mimic this behavior by path-integration. This work focuses on RNNs that implicitly learn to acts as generative models over a fixed distribution of input sequences. More specifically, how noisy RNNs that are trained to path-integrate their inputs implicitly learn statistics that produce Langevin sampling of the input distribution when no input is given. Earlier works have assumed that the distribution of RNN activity p(r(t)) is stationary and thus the score function depends only on r(t). This work refutes this assumption and argues in favor of the role of hidden state leakage in path-integration. This work shows that hidden state adaptation (negative feedback) allows for exploration in replay, but results in non-Markov sampling along with slow down in replay. Further, it proposes model for temporally compressed replay in noisy path-integrating RNNs through hidden state momentum term resulting in underdamped Langevin sampling. Numerical experiments are conducted for path-integration to verify the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This works shows that path-integration in noisy RNNs results in score functions which are time-variant and difficult to estimate even for simple distributions such as Gaussian. It demonstrates a useful inductive bias of hidden state linear leakage.  \n- This paper shows that Adaptation (negative feedback) induces exploration in replay. It results in non-Markov second-order Langevin sampling destabilizing attractors, results in diversification/exploration in replay as well as slowing down replay process. \n- The proposed method of adding momentum to induce underdamped Langevin sampling induces fast replay resulting in temporal compression replay and overcomes replay slow-down induced by adaptation while still maintaining replay diversity/exploration."}, "weaknesses": {"value": "- It is unclear if the proposed under-dampening mechanism with momentum is biologically plausible?\n- The theoretical analysis has been done under the Gaussian assumption, it’s not clear if the same would hold true when this assumption breaks down. \n- While the proposed work focuses on the effects of two counteracting terms : adaptation (negative feedback) and underdampening (momentum). Since adaptation slows replay along with adding  diversity/exploration in the replay. In contrast, under-dampening accelerates replay due to momentum term. These two terms are at odds with each other and while carefully balancing helps solve some of the easy tasks presented in this work, it’s unclear what a systematic approach would encourage. It would have been better to explore this aspect in a bit more detail in this work."}, "questions": {"value": "- In Fig, 2, any clues as to why leakage is more helpful for T-maze problem compared to Triangle? Since it’s clear with increasing the masking difficulty (k), the convergence is better for T-maze with leakage but there’s less gap in Triangle setup. \n- What is the behavior of the experiment in Fig.2 without masked training ? Does the leakage term still helps in this case? \n- How biologically plausible is the proposed under-dampening mechanism of added momentum term $(1-\\lambda_v) v(t)$ ?\n- How does one extend the proposed insights from path-integration tasks to richer tasks like planning and policy learning?\n- In Fig. 4, underdampening constant $\\lambda_v$ is greater than 0.5, what are the effects when this constant goes down?\n- Although most of the experimental setup uses ReLU RNNs, would the empirical analysis change drastically in the presence of Gated RNNs or RNNs with other non-linear activation functions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSXQyXOnmc", "forum": "kGNtM8NteM", "replyto": "kGNtM8NteM", "signatures": ["ICLR.cc/2026/Conference/Submission18751/Reviewer_MAde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18751/Reviewer_MAde"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001621427, "cdate": 1762001621427, "tmdate": 1762928468198, "mdate": 1762928468198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}