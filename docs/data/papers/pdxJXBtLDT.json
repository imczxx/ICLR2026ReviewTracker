{"id": "pdxJXBtLDT", "number": 8591, "cdate": 1758092041396, "mdate": 1763772635211, "content": {"title": "Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models", "abstract": "Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, LN and Pre-LN are inefficient due to repeated statistical calculations and suffer from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented, normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT achieves improved stability and efficiency on pretraining. Besides, BHyT enhances robustness across various downstream language understanding and reasoning benchmark tasks. Our code is available at: \\url{https://anonymous.4open.science/r/BHyT}", "tldr": "Pre-LN destabilizes deep LLM training and limits scaling benefits. We introduce BHyT, which bounds Tanh inputs to ensure stable gradients, enabling faster and more robust pretraining as well as better supervised fine-tuning.", "keywords": ["Layer Normalization", "Large Language Models", "Pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56eee7d67326c757e49e86f87c22aefafff65827.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Peri‑LN was proposed to mitigate layer‑wise gradient and variance growth in Transformers but it introduces notable computational overhead. This paper takes an alternative path: it replaces the normalizer with a tanh‑based formulation that explicitly constrains inputs to a preset range, which keeps activations away from saturation and curbs depth‑wise drift. The authors provide theoretical guarantees for stability and show that their formulation does not require the extra normalizing stages used by Peri‑LN while retaining compatibility with standard Transformer blocks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Conceptually clean formulation that directly bounds activations and targets the underlying cause of depth‑related instability.\n* Solid theory that upper‑bounds gradient amplification and supports the intended stability properties.\n* Empirical results broadly align with the theory and indicate improved stability of layer statistics across depth."}, "weaknesses": {"value": "* Throughput is reported as higher than Peri‑LN, yet training loss at a fixed number of steps is slightly worse. It remains unclear whether the proposed method would surpass Peri‑LN under equal training compute. Time‑to‑target‑loss or equal‑FLOPs comparisons would make the efficiency claim more convincing.\n* The paper should analyze where Peri‑LN’s inefficiency comes from. Is the bottleneck inherent to the algorithmic cost of repeated reductions and memory movement, or chiefly an implementation artifact such as unfused CUDA kernels or suboptimal scheduling? Kernel‑level profiling or theory‑backed complexity analysis would clarify this point.\n* The final downstream comparison emphasizes overall averages, but absolute MMLU scores are low. Since a random baseline is 25% on four‑choice tasks, small differences at this scale may be hard to interpret for models trained with limited tokens.\n* With the current experimental setup it is difficult to conclude that BHyT is definitively more effective than Peri‑LN. That said, the gains over the RMSNorm baseline are sizable, which suggests BHyT is a strong and practical alternative to Peri‑LN."}, "questions": {"value": "* Not a question but just a minor editing issue: `\\citep` should be used in places where citation on points is intended. (e.g., line 094, 095, ...)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b43qWRy9n9", "forum": "pdxJXBtLDT", "replyto": "pdxJXBtLDT", "signatures": ["ICLR.cc/2026/Conference/Submission8591/Reviewer_fZC6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8591/Reviewer_fZC6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804725898, "cdate": 1761804725898, "tmdate": 1762920437850, "mdate": 1762920437850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bounded Hyperbolic Tanh (BHyT) as a drop‑in replacement for Pre‑Layer Normalization (typically RMSNorm) in Transformer blocks. The key idea is to pair a tanh nonlinearity with explicit, per‑instance input bounding so activations stay in a non‑saturating regime while avoiding repeated statistic computations.  Empirically, BHyT is evaluated by pretraining 1B/3B Llama‑style models from scratch on C4, then (optionally) SFT on LIMA‑1k and Commonsense‑170K, with downstream evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 Creative combination of normalization‑free activations with explicit, data‑aware input bounding and a block‑level variance approximation; the latter gives a neat knob to keep overhead small while preserving stability\n\n2 Clear derivations, concise definitions, readable pseudocode,\n\n3 If the approach scales, it could reduce normalization overhead in large LMs without sacrificing training stability"}, "weaknesses": {"value": "Please see the questions."}, "questions": {"value": "1 Since the paper introduces  BHyT∗ first, could you please add an ablation directly comparing BHyT∗ and the practical BHyT under the same setup, reporting training loss/validation perplexity, downstream accuracy, and throughput.\n\n2  If the variance can be estimated cheaply, could you apply this estimator to RMSNorm (e.g., replace one normalization with the estimate) and report whether similar speedups and performance are achieved?\n\n3 Please include validation perplexity results of pretraining.\n\n4  Please specify hardware (GPU/TPU model & count), precision, parallelism strategy (DP/TP/PP/ZeRO), global batch size/sequence length, and the measurement protocol.\n\n5  Since LNS is a strong baseline here and LNS reports BoolQ, could you add BoolQ results for parity with the tasks in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DzdMIw1roY", "forum": "pdxJXBtLDT", "replyto": "pdxJXBtLDT", "signatures": ["ICLR.cc/2026/Conference/Submission8591/Reviewer_5S5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8591/Reviewer_5S5H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845223392, "cdate": 1761845223392, "tmdate": 1762920437364, "mdate": 1762920437364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the BHyT which replaces Pre-LN with \"input bounded with probability guarantee + tanh\" and only precisely calculates the variance once per Transformer block, with the rest using lightweight approximation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Combine the speed advantage of \"unnormalized\" with the stability of \"bounded non-saturated\" to explicitly avoid tanh saturation with probability limiting.\n- The method is simple and effective."}, "weaknesses": {"value": "- Only validated on 1B/3B, C4, and a small amount of SFT datasets; did not cover deeper layers or longer contexts. I understand that it is not realistic to do such a thing with limited resources, but perhaps training a narrow and deep model might be feasible?\n- The \"uniform\" attention assumption is least tenable in which training stages/tasks? If the second moment of the real attention weights replaces the uniform assumption, what are the approximate errors and throughput losses?"}, "questions": {"value": "see weekness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IHl6ShXTS0", "forum": "pdxJXBtLDT", "replyto": "pdxJXBtLDT", "signatures": ["ICLR.cc/2026/Conference/Submission8591/Reviewer_d4bW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8591/Reviewer_d4bW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901000113, "cdate": 1761901000113, "tmdate": 1762920437082, "mdate": 1762920437082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Bounded Hyperbolic Tanh (BHyT), a normalization-free alternative to Pre-Layer Normalization designed to improve both stability and efficiency in large language model training. It combines a tanh nonlinearity with an explicit, data-driven input bounding mechanism that constrains activations within a stable, non-saturating range, preventing variance explosion across depth.\nBHyT theoretically guarantees gradient stability by bounding the Jacobian norm relative to RMSNorm and uses a lightweight variance approximation to reduce normalization overhead. Empirically, it aims to maintain training stability comparable to heavily normalized methods like Peri-LN while achieving higher computational efficiency. Overall, the paper positions BHyT as a practical replacement for Pre-LN, balancing stability and throughput for large-scale Transformer models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation.\n- Conceptually simple yet theoretically grounded design.\n- Lightweight variance approximation for efficiency.\n- Empirical evidence of improved stability."}, "weaknesses": {"value": "- **Inadequate reporting and questionable generality of Peri-LN throughput results.** Figure 4(b) claims that Peri-LN achieves strong accuracy but suffers from the slowest throughput, positioning BHyT as the best trade-off. However, the paper does not specify the environment under which throughput was measured. All experiments were conducted in Llama-Factory rather than in standard large-scale frameworks (e.g., Megatron-Core or NeMo) that officially support Gemma-style Peri-LN. Without such metadata or a cross-framework check, it is unclear whether the reported slowdown reflects an intrinsic limitation of Peri-LN or merely framework-specific overhead.\n\n- **Under-trained experimental regime and limited statistical reliability.** The 1B and 3B models were trained on only 1.64 B and 1.97 B tokens, respectively—orders of magnitude below standard compute-optimal ratios. These under-saturated runs make it difficult to assess convergence, generalization, or stability trends. No results are averaged over multiple seeds or accompanied by confidence intervals. Reported downstream improvements (Tables 1–4) therefore lack statistical robustness, and the unusually large MMLU jump (36.54 % for BHyT) raises concerns about configuration consistency and reproducibility.\n\n- **Theory–practice gap and missing validation of assumptions.** Theoretical guarantees apply only to the idealized variant BHyT* with exact statistics and zero-mean inputs. The implemented BHyT uses an approximated variance under strong assumptions—uniform attention, Gaussianity, and linearized tanh—but the paper offers no quantitative error analysis of this approximation (e.g., deviation across layers or attention-entropy regimes). As a result, the claimed Jacobian-norm bound and variance-stability guarantees remain unverified in realistic conditions.\n\n- **Missing baselines and scale comparisons.** At the 3B scale, BHyT is compared only with DyT, omitting RMSNorm, LNS, and Peri-LN under identical configurations. This prevents a fair evaluation of scalability and undermines the claim that BHyT preserves stability at greater depth."}, "questions": {"value": "stated in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sw1O766v6J", "forum": "pdxJXBtLDT", "replyto": "pdxJXBtLDT", "signatures": ["ICLR.cc/2026/Conference/Submission8591/Reviewer_iv9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8591/Reviewer_iv9T"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762511918561, "cdate": 1762511918561, "tmdate": 1762920436698, "mdate": 1762920436698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}