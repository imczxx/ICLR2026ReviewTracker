{"id": "dwgtYTuSaS", "number": 12497, "cdate": 1758208214207, "mdate": 1759897505827, "content": {"title": "Continuous Online Action Detection from Egocentric Videos", "abstract": "Online Action Detection (OAD) tackles the challenge of recognizing actions as they unfold, relying solely on current and past frames. However, most OAD models are trained offline and assume static environments, limiting their adaptability to the dynamic, user-specific contexts typical of wearable devices. To address these limitations, we propose Continuous Online Action Detection (COAD), a novel task formulation in which models not only perform online action detection but also continuously learn and adapt on-the-fly from streaming videos, without storing data or requiring multiple training passes. This paradigm naturally fits egocentric vision on wearable devices, given its highly dynamic, personalized, and resource-constrained characteristics. We introduce a large-scale egocentric OAD benchmark dataset (Ego-OAD) and develop training strategies that enhance both adaptation to individual users and generalization to unseen environments. Our results on Ego-OAD demonstrate continuous learning from streaming videos improves adaptation to the user’s environment by up to 20% in top-5 accuracy, and improves generalization to new scenarios by up to 7%, advancing the development of personalized egocentric AI systems.", "tldr": "We tackle egocentric online action detection by enabling on-device, single-pass training on continuous video streams, improving adaptation and generalization without storing data. We also release a new benchmark from Ego4D.", "keywords": ["Egocentric Vision", "Online Action Detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50ad0bbefc7225213325c394104bcb7106e8125a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles continuous egocentric action detection and introduces Continuous Online Action Detection (COAD), emphasizing the need for models that operate on long, unsegmented video streams rather than isolated clips. To address gradient noise and distribution shifts across continuous sequences, the authors leveraged gradient orthogonalization, a Non-Uniform loss, and persistent RNN hidden states, achieving up to 6% mAP improvement on Ego4D and comparable gains on EPIC-KITCHENS. Through detailed ablations, they show that gradient orthogonalization and Non-Uniform loss are the most critical components, underscoring the difficulty of adapting exocentric models to egocentric settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors carefully designed ablation studies that show each of their modifications to a continuous (non-IID) training pipeline is effective (perhaps except for state-continuity)\n\nThe authors demonstrate a significant improvement over naive IID-pretraining across two egocentric datasets specifically for the continuous Online Action Detection task."}, "weaknesses": {"value": "In Figures 7 and 8 from the Appendix, it appears that some categories (ex., Cutting/trimming grass, Using the phone, etc.) are both prevalent and long-lasting. If this is the case, wouldn’t reporting the per-frame mAP and Top-5-Recall, computed over all action classes, lead to an unbalanced view (by label) of the model’s performance? Wouldn’t it be fairer to weight each action class proportionally to the total number of seconds present in the out-of-stream subset? Please provide some clarifications.\n\nBeyond merging temporal annotations from multiple reviewers and manually grouping semantically similar action classes, it is unclear what modifications the authors have made to the EGO4D MQ subset to state that their EGO-OAD dataset is a “curated” version."}, "questions": {"value": "What are the dimensions of the embedding layer used in the RNN? Are they identical to the output embedding dimensions of the visual backbone? If so, could this alignment explain any performance differences between TSN and TimeSFormer beyond the temporal modeling effects reported in Table 4?\n\nRegarding the ablation in Table 3, what would happen if the hidden states were not continuous and the training procedure did not involve continuous adaptation? In other words, if we maintained an IID training setup for both the visual backbone and the RNN, but retained gradient orthogonalization and the Non-Uniform loss, would we still observe comparable gains in mAP and Top-5 Recall?\n\nIt remains unclear to the reviewer that the out-of-stream setting outperforms the in-stream setup by such a large margin (over 25% mAP) for the noun classification task in EPIC-KITCHENS? If this gap is attributed to the fine-grained nature of actions and annotations in EPIC-KITCHENS, why do we not observe a similar disparity for actions and verbs in Table 2?\n\nAdditionally,1,177 videos (approximately 62%) were reserved for the in-stream subset in EGO-OAD, while only 202 videos (around 31%) were used for the in-stream subset in EPIC-KITCHENS. Is this discrepancy related to video length or another dataset-specific factor that should be clarified for the reader?\n\nFinally, there are some other relevant methods not reported in Tables 1 and 2 for baseline comparison. For example, including some of the top-performing approaches from the Ego4D leaderboard (https://eval.ai/web/challenges/challenge-page/1626/leaderboard/3913\n) could provide readers with a broader context, highlighting that continuous online action detection remains a challenging problem even for methods specifically designed for egocentric OAD."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6SPgJZ0vQM", "forum": "dwgtYTuSaS", "replyto": "dwgtYTuSaS", "signatures": ["ICLR.cc/2026/Conference/Submission12497/Reviewer_31JR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12497/Reviewer_31JR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796844085, "cdate": 1761796844085, "tmdate": 1762923370569, "mdate": 1762923370569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces continuous online action detection, a new task that combined traditional online action detection with learning from continuous video streams. The paper targets this setting in egocentric video proposing a new Ego online action detection benchmark based on the Ego4D moment queries annotations. To tackle this task the paper combines orthogonal gradients with state continuity and a non-uniform loss."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a timely problem: enabling models to learn continuously from streaming egocentric video. This is well motivated and relevant for wearable AI systems.\n***\n\n- The proposed method does seem to have benefit across in-stream and out-of-the-settings for both in-stream and out-of-stream settings, at least on Top-5 recall (little improvement on mAP)  \n***\n\n- The Ego-4D based online action detection benchmark could be useful for future works"}, "weaknesses": {"value": "- Novelty and Positioning\n- The proposed Continuous Online Action Detection (COAD) task does not appear fundamentally new. It largely combines elements of continual (or continuous) learning and online action detection, without providing a clear distinction from existing formulations.\n- The paper does not clarify how much adaptation is actually required to extend existing continuous video learning methods (e.g., Carreira et al., 2024b; Han et al., 2025) to this setting, or whether these methods could serve as direct baselines.\n***\n\n- Comparison to Prior Work\n- The evaluation lacks state-of-the-art baselines from both online action detection (e.g. An et al. 2023) and continuous video learning (e.g. Carreira et al. 2024b and Han et al. 2025). Without these comparisons, it is not possible to assess whether the proposed approach meaningfully advances the field. \n- The main results (Table 1) instead focus more on the benefit of using egocentric data in training rather than the benefit of the proposed method itself.\n- The comparison to prior work is particularly important method appears conceptually similar to earlier recurrent or memory-based OAD models, but these connections are not made explicit.\n- The ablation study shows that state continuity has minimal impact, and that improvements come primarily from the non-uniform loss and orthogonal gradient components, both of borrowed from prior work. Orthoganal gradient comes from continious video work Han et al. 2025 and the non-uniform loss comes from online action detection work An et al. 2023.\n***\n\n- Dataset Contribution (Ego-OAD)\n- While the dataset could be useful for future works, the contribution is relatively small since it is derived from the Ego4D momemnt queries task and annotations\n***\n\n- Evaluation Considerations\n- There is no analysis of the computation–accuracy trade-off, which is critical for the proposed on-device learning setting. The cost of continuous adaptation compared to standard offline or inference-only OAD is not quantified.\n- The concept of “out-of-stream” data is poorly defined. It is introduced briefly at the end of the method section and later described in the results as “held-out data reserved for evaluation only,” without a clear explanation of its role or relevance."}, "questions": {"value": "Task Definition\n- In what precise way does Continuous Online Action Detection (COAD) differ from simply combining continual learning and online action detection?\n- What assumptions make COAD a distinct and necessary formulation rather than an application of existing paradigms?\n***\n\nRelation to Prior Work\n- How much modification of existing continuous video learning methods (e.g., Carreira et al., 2024b; Han et al., 2025) is required to adapt them to the OAD setting?\n- Why does the paper not include these prior works as baselines or points of comparison?\n- How does the proposed approach differ from earlier recurrent or memory-based OAD models (e.g. An et al. 2023) and why are these works not compared to?\n***\n\nMethod and Analysis Details\n- Why does state continuity have such a limited impact compared to the non-uniform loss and orthogonal gradient components?\n- Are these components directly adopted from prior work, or have they been substantially modified for this setting?\n***\n\nEvaluation Considerations\n- What is the computational overhead of continuous adaptation during inference compared to standard OAD models?\n- How significant is the accuracy–efficiency trade-off for on-device deployment?\n- How is \"out-of-stream\" data defined and used in the experiments, and how does it differ from standard held-out test data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IFUPApixW7", "forum": "dwgtYTuSaS", "replyto": "dwgtYTuSaS", "signatures": ["ICLR.cc/2026/Conference/Submission12497/Reviewer_KtAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12497/Reviewer_KtAP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816532414, "cdate": 1761816532414, "tmdate": 1762923369617, "mdate": 1762923369617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new problem setup called Continuous Online Action Detection (COAD) for egocentric video. \n\nUnlike standard Online Action Detection (OAD), where models are trained offline and only infer online, COAD aims to both learn and adapt continuously from streaming input, in a single-pass, without replay or multiple epochs.\n\nThe authors introduce a benchmark called Ego-OAD (a modified subset of Ego4D’s Moment Queries split) and combine three ideas to deal with the new configuration:\n1. state continuity in RNNs,\n2. orthogonal gradient updates (to decorrelate consecutive gradients), and\n3. non-uniform loss computed only on the last frame of a window.\n\nThey claim this setting improves in-stream adaptation and out-of-stream generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The proposed setup is relatively simple to implement."}, "weaknesses": {"value": "1. **Almost no technical novelty**:\nThe three main components (orthogonal gradient, non-uniform loss, RNN state continuity) are all borrowed from prior work.\nThe proposed orthogonal gradient is directly taken from prior work on streaming learning (e.g., CVPR 2025), the non-uniform loss has already appeared in OAD literature (e.g., MiniROAD), and state continuity is essentially an inherent property of RNN-based models.\nOverall, the claimed “new task and strategy” appears to be more of a reapplication or repackaging of existing ideas rather than a genuinely novel methodological contribution.\n\n2. **Motivation is not persuasive**: \nCOAD assumes frame-level or interval-level action labels—extremely expensive to obtain in egocentric settings.\nLabeling continuous egocentric streams for action intervals is one of the most labor-intensive tasks in video research.\nIf the method truly aims at continuous online learning, it should address label sparsity, delayed labels, or weak/self-supervised alternatives.\nAs written, it still assumes dense supervision, which fundamentally contradicts the “continuous deployment” scenario it claims to model.\nIn addition, the proposed formulation gives the impression of directly extending the streaming-learning setup of Carreira et al. (CVPR 2024a) into the OAD domain, without sufficiently reconciling the differing supervision assumptions and data requirements between the two tasks.\n\n3. **Missing comparison across diverse OAD architectures**:\nThe paper evaluates COAD primarily on a minimal RNN-based configuration, without comparing across existing architectural paradigms.\nHowever, most prior OAD works—such as LSTR, OADTR, MA-Transformer, GateHub, MiniROAD, and TeSTra—are fundamentally architectural contributions, largely orthogonal to the continuous-learning setup proposed here.\nFor the study to be experimentally complete, it is therefore important to demonstrate how COAD behaves when integrated with or compared against these architectures under a consistent backbone and feature extraction setting."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4rT6a9Nl14", "forum": "dwgtYTuSaS", "replyto": "dwgtYTuSaS", "signatures": ["ICLR.cc/2026/Conference/Submission12497/Reviewer_JxkE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12497/Reviewer_JxkE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889391303, "cdate": 1761889391303, "tmdate": 1762923368950, "mdate": 1762923368950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional Online Action Detection (OAD) models are trained offline and assume static environments. This limits their adaptability to dynamic, personalized context especially in wearable devices like smart glasses. These devices capture egocentric (first-person) video streams in real time, where users, environments, and tasks vary continuously. The paper introduces Continuous Online Action Detection (COAD) to address this gap. COAD enables models to not only detect actions in real time but also learn and adapt continuously from streaming video without storing data or retraining offline. It curates a large-scale benchmark from Ego4D’s Moment Queries split for egocentric OAD with multi-label, temporally grounded annotations. It proposes training strategies of orthogonal gradient projection to reduce update redundancy, state continuity via RNNs to maintain long-term memory, and a non-uniform loss to align training with inference dynamics. In the experimental evaluation, COAD improve top-5 accuracy by 20% for in-stream adaptation and generalization performance by 7% on unseen data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a well-motivated problem of moving beyond static, offline-trained OAD models to methods that can continuously learn and adapt in dynamic, egocentric environments.\n2. The curation of the Ego-OAD benchmark provides a new, large-scale dataset for this specific task, derived from the existing Ego4D dataset."}, "weaknesses": {"value": "1. The method/architecture novelty of the paper seems to be minimal. The proposed COAD method is a combination of pre-existing components. The orthogonal gradient technique is applied from [1] , and the non-uniform loss is applied from [2]. The third component of state continuity seems to be the default behavior of an RNN during inference. Can the authors emphasize the contributions in terms of architecture, if any?\n2. The main results in Table 1 show that COAD method improves out-of-stream generalization (26.0 mAP) but worsens in-stream adaptation (36.8 mAP) compared to the \"w/o COAD\" baseline (25.5 mAP and 39.0 mAP, respectively, with Ego pretraining). Can the authors discuss why the performance in generalization task improves but seems to be less for in-stream data?\n\n[1]. Han, Tengda, et al. \"Learning from Streaming Video with Orthogonal Gradients.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n[2]. An, Joungbin, et al. \"Miniroad: Minimal rnn framework for online action detection.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."}, "questions": {"value": "Suggestions:\nThe text in Section 5.3 refers to Table 1 for Epic-kitchens-100 results but these results are located in Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gbC7USfWrh", "forum": "dwgtYTuSaS", "replyto": "dwgtYTuSaS", "signatures": ["ICLR.cc/2026/Conference/Submission12497/Reviewer_9Gn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12497/Reviewer_9Gn1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12497/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047766562, "cdate": 1762047766562, "tmdate": 1762923368524, "mdate": 1762923368524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}