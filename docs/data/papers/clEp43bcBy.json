{"id": "clEp43bcBy", "number": 2828, "cdate": 1757265170402, "mdate": 1759898124605, "content": {"title": "Automating Environments For Measuring Agentic Learning", "abstract": "Humans naturally adapt to diverse environments by learning underlying rules through direct interaction with the real world, which naturally contains all environments as subsets.\nFor agents to achieve human-like adaptability, they must develop the ability to discover and adapt to unknown environmental rules through exposure to diverse environments. \nHowever, current agents rely on limited human-provided environments with human-specified learning strategies.\nTo address environment scarcity, we propose AutoEnv, an automated framework that generates diverse environments with distinct rule distributions, achieving 90\\% execution success at \\$4.12 per environment. We construct AutoEnv-36, a dataset of 36 heterogeneous environments with 358 levels with varied reward, observation, and state dynamics.\nTo address learning strategy limitations, we formalize the agentic learning process as discrete, composable components including selection strategies, optimization signals, and target components. \nExperimental validation across 7 language models and 8 optimization methods confirms the quality of AutoEnv-36 while demonstrating that environment-adaptive learning significantly improves performance: 14\\% improvement when using only two learning methods, and 32\\% improvement when using eight methods. This finding establishes the critical importance of environment-specific learning adaptation and validates the synergy between diverse environments and adaptive learning strategies for developing scalable agent capabilities.", "tldr": "", "keywords": ["Learning; Language Agent"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b5af2770ddf5a4ee44cb8eb09ed59383b9ee65a.pdf", "supplementary_material": "/attachment/5fd94e07333cade34d7f30d3558fe799c0712a6b.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AUTOENV framework to automatically generate different envrioments for evaluating and training agentic language models. The author states that current agents lack the ability to generalize across different rule systems due to the limit and human-designed environments with fixed policies. Thus, the proposed AUTOENV decompose environments intro BaseEnv, ObsEnv, and SkinEnv abstract layers and provide variation in reward structures, state dynamics, and partial observability. This paper also formalizes agentic learning as modular components including selection strategies, optimiztion signals, and target components. Extensive experiments shows the effectiveness of this framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The proposed AUTOENV automantes the envrionment generation for agentic learning. While it builds upon exisiting ideas from benchmark construction and meta-learning, the automated environment creating with structured learning strategy is well-motivated.\n2. Quality: The technical execution is solid. The three-layer abstraction is clearly presented and the working pipeline capable of producing validated and executable environments is demonstrated. \n3. Clarity: The manuscript is well orgnanized and systematically written.\n4. Significance: This work addresses an important problem of the lack of scalable and diverse environments for evaluation.By automating envrionment creation and framing agentic learning as composable components, this paper shows potential contributions to future research in generalist and adaptive AI."}, "weaknesses": {"value": "While this paper shows credible contribution, several weaknesses limit its overall impact.\n1. Despite that the automated envrionment creating with structrued learning strategy is well-motivated, the conceptual advancement lies in combining the automated benchmark generation and meta-learning components rather than introducing fundamentally new solutions. Therefore, the contribution is well-motivated yet incremental.\n2. Although this paper reports that the AUTOENV-36 contains 36 validated heterogeneous envrionments, the analysis is largely based on decriptive rather than quantitative. It is unclear how distinct these envrionment are in terms of rule distribution, learning dynamics, and transfer difficulty. \n3. The reported 74.7% consistency validation rate may suggest that nearly 25% generated environments may exhibit unstable or inconsistent reward and rule behaviors, which may raise concerns about the robustness.\n4. The experiments exclusively evaluate LLM agents without comparison to non-LLM or classical RL baselines. This makes it difficult to asses whether the claimed improvments are due to the adaptive learning or simple model capacity differences.\n5. Besides, the experiments also evaluate closed-sourced LLMs without open-sourced LLMs after finetuning. It is unclear whether the introduced complexity from the integrating of diverse envrionments still challenges the current fine-tuned LLMs, which weakens the influence of the contributions of this work."}, "questions": {"value": "Based on the previous description, I have some questions listed below.\n1. Could the author provide some quantitive measures of diversity among the generated envrionments beyond categorical features. How do AUTOENV ensure that the generated envrionments differ meaningfully  in their underlying mechanics instead of just parameter variations?\n2. This paper reports a 74.7% consistency validation rate. Can you elaborate on what kinds of inconsistencies occur during the rest 25%?\n3. Due to the fact that the envrioments are generated in closed loops by large language models, are there bottlnecks such as model inference time or code validation loops that might limit the scalability of larger envrionment amounts needed?\n4. How does AUTOENV-36 compare quantitatively to other benchmark collections such as GG-Bench or OSWorld in terms of diversity, difficulty, and adaptability?\n5. Are there safeguards to prevent overfitting or environment leakage during generation and validation such as environments unintentionally reusing the same rule patterns?\n6. The paper briefly mentions possible future directions such as multimodal and embodied scenarios and the chosen primary area is applications to robotics. According to the demonstrations in the Appendix, the envrionments genreated are not complex. Could the authors outline what modifications would be needed to extend AUTOENV beyond text-based settings, for example, to 3D embodied or visual environments?\n7. Is the AUTOENV able to generate sceanarios with more envrionment non-stationarities, such as competitive envrioment with diversity opponent strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ItkAmLABwO", "forum": "clEp43bcBy", "replyto": "clEp43bcBy", "signatures": ["ICLR.cc/2026/Conference/Submission2828/Reviewer_FGMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2828/Reviewer_FGMv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622115086, "cdate": 1761622115086, "tmdate": 1762916392601, "mdate": 1762916392601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the limitation of human-provided environments by proposing an automated environment generation method to prompt LLMs to produce code for constructing environments with distinct rule distributions and build a dataset of 36 heterogeneous environments with 358 levels.\n\nThen the paper addresses the limitation of static learning strategies in different environments by formalizing the agent learning process as composable components and introducing selection strategies, optimization signals, and target components for learning adaptation analysis. \n\nExperiments across 7 language models and 8 learning strategies demonstrate the quality of generated environment datasets and highlight the necessity of environment-specific learning strategy selection (different environments correspond to different optimal learning strategy configuration)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses two core problems in current LLM agentic learning: (1) insufficient diversity of existing agent environments, and (2)  lack of exploration of different learning strategies in different environments. Solving these problems are crucial to develop scalable and optimized LLM agents in dynamic interactive environments.\n2.  The paper constructs a dataset (AUTOENV-36) comprising 36 heterogeneous environments with 358 levels with varied reward, observation, and state dynamics.\n3. The paper proposes a structured framework to analyze what learning strategies can be combined optimally in different environments, establishing a foundation for methodical evaluation of different learning methods across diverse environments.\n4. Experiments demonstrate the quality of generated environments compared to human-supervised datasets, the usefulness of AUTOENV-36 to evaluate LLMs in agent capabilities, and the necessity to dynamically adapt and combine learning strategies in different environments (however, the evaluation metrics used in experiments are not well explained, raising concerns about the the credibility of the experimental results. See weaknesses below.)."}, "weaknesses": {"value": "1. The major concern about this paper is that it **lacks many details** to help understand the proposed concepts, metrics, and implementaion details. Here are some issues: \n\n(1) Lack of specific calculation formula of evaluation metrics, such as the average generation cost per environment (cost in table 4), and  the \"execution cost of optimized candidates in USD\" in line 328, as well as \"error rates across generation phases including execution errors, validation errors, and consistency errors and the overall success rates\". By the way, what is USD? These raise concerns about the credibility and authenticity of experimental results.\n\n(2) Lack of explanation and calculation formula of \"theoretical reward upper bound\".\n\n(3) For optimization units, lack of detailed explanation and examples of \"agent implementations and models\"? The paper mentions somewhere that the agent may be \"agent code\" or \"agent memory\", which is confusing.\n\n(4) Lack of explanation of \"inverse semantics\".\n\n(5) Lack of descriptions and examples of \"self-repair tasks\"?\n\n(6) No descirption of how to do \"human review of LLM-generated requirements\" concerning table 2, and how to ensure the consistency and reliability of the human review.\n\n(7) No description of how to do \"comprehensive feature analysis\" to select environments to build AUTOENV-36 dataset. What features are analyzed and why chooses these features.\n\n(8) Please provide specific temparature values set to different models in section 5.1.\n\n2. The second major concern is **the practicality of generated environments.** \nAs shown in Table 1, the generated environments have an average of 6.10 available actions, which is a small action space compared to many realistic agentic tasks (e.g, web search/deep research/embodied interaction), especially open-ended environments with infinite action spaces (common in text-based environments such as dialogue). \nAnd it is better to provide detailed description for each of the 36 generated environments (such as its state/action space and reward functions) and visualized examples for some of the environments to validate their practicality.\n3. Another concern is the **representativeness of the 8 optimization methods** evaluated in experiments. \nIt seems that these optimization methods all belong to prompt engineering, however, post-training approaches such as SFT and RL have been widely used for agent learning in dynamic environments. What if these training approaches are incorporated into the learning adaptation analysis?\n3. Since claude-4-sonnet is used as the optimization model in table 4, it should be compared as one of the baselines.\n4. Lack of agent structures (code) before optimization in environments in the appendix.\n5. Desciptions or reference of some figures or tables are missing in the main paper, such as Figure 1 and Table 4."}, "questions": {"value": "1. How to calculate the results of \"overall\" in Table 2? It seems that it is not an average of automated and supervised.\n2. Why do inverse semantic environments often yield higher scores than aligned semantic environments across most models?\n3. Does Oracle Learning Selection in table 3 refer to combining Dynamics + Prompt and Instruction + Prompt methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3YUOGr1Cc4", "forum": "clEp43bcBy", "replyto": "clEp43bcBy", "signatures": ["ICLR.cc/2026/Conference/Submission2828/Reviewer_RJQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2828/Reviewer_RJQ1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624152905, "cdate": 1761624152905, "tmdate": 1762916392381, "mdate": 1762916392381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **AUTOENV**, a framework for (i) automated environment generation via a three-layer abstraction (BaseEnv/ObsEnv/SkinEnv), (ii) a formalization of agentic learning as a compositional loop, and (iii) the **AUTOENV-36** benchmark. The authors report low-cost environment generation, clear performance stratification across LLMs, and gains from environment-specific learning strategies."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated framework with practical utility**  \n   The three-layer environment abstraction combined with a domain-specific language (DSL) enables scalable, low-cost environment generation ($4.12 per environment, 90% success rate). This addresses a real bottleneck in agent development where manual environment creation is expensive and limited in diversity.\n\n2. **Compelling empirical evidence for environment-specific learning**  \n   The core finding that Oracle Learning Selection (selecting the best learning method per environment) achieves 14% improvement with 2 methods and 32% with 8 methods—strongly demonstrates that no single learning strategy works universally. This challenges common assumptions in the field and is well-supported by experiments across 36 diverse environments."}, "weaknesses": {"value": "1. **Unclear Baseline Definitions**: \nThe baseline used in the experiments is not clearly defined. The reported \"14% improvement over baseline\" and \"32% improvement over baseline\" are ambiguous without specifying the exact baseline configuration. Clarification is needed on whether the baseline refers to a fixed learning setup or the best performing existing model baseline.\n2. **Unknown Capability Coverage of AUTOENV-36**: \nAUTOENV-36 lacks a systematic capability taxonomy. It is unclear what specific cognitive skills each automatically generated environment tests (e.g., planning depth, spatial reasoning, memory requirements). How does AUTOENV-36's capability coverage compare to existing benchmarks?\n3. **Insufficient Analysis of Component Improvement Mechanisms**: \nAlthough Appendix E.2 provides qualitative examples of learned prompts and agent code, the analysis of the learning process remains insufficient. It is unclear what percentage of iterations modify Prompt vs. Agent Code, whether certain environment types favor specific components, and how these modifications correlate with performance gains."}, "questions": {"value": "**Q1 (Baseline Definition):** Can you explicitly define the baseline corresponding to each reported improvement?\n\n**Q2 (Capability Taxonomy):** Can you provide a mapping from environments to cognitive capabilities beyond the features in Table 1? How would you categorize the 36 environments along dimensions like planning depth, memory demands, or exploration difficulty? This would help readers understand what agent competencies the benchmark actually measures.\n\n**Q3 (Component Analysis):** Can you provide detailed analysis of the learning process: (a) What percentage of learning iterations modify Prompt vs. Agent Code? (b) Is there correlation between environment features and effective components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k14kqvSw0U", "forum": "clEp43bcBy", "replyto": "clEp43bcBy", "signatures": ["ICLR.cc/2026/Conference/Submission2828/Reviewer_KP3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2828/Reviewer_KP3t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794063755, "cdate": 1761794063755, "tmdate": 1762916392209, "mdate": 1762916392209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a modular pipeline for automatically generating diverse RL-like environments and presents AUTOENV-36, a benchmark to evaluate agentic learning. The problem is timely and the engineering contribution is useful for the field, with clear motivation and reproducibility support. The work provides a promising benchmark direction, but clearer explanation of key concepts and more principled evaluation of learning strategies would strengthen its impact."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The paper identifies a timely problem in agentic-learning - the lack of diverse, automatically generated environments and the need for adaptive learning strategies rather than fixed training pipelines. \n* The authors provide detailed code snippets and algorithm descriptions that can help reproducibility."}, "weaknesses": {"value": "While the motivation and system design are strong, I found the paper hard to follow. Several concepts are introduced without intuitive grounding or examples. For instance:\n* Aligned vs. inverse semantics (Lines 223–229): These are referenced but not clearly explained; a concrete example would help clarify their role and importance.\n* Selection strategies, optimization signals, and components (Line 269): Although briefly described later (Lines 299–309), the rationale behind the specific choices is not well justified. It is unclear what properties the chosen strategies are intended to probe, or why alternative agent-learning techniques were not explored.\n* Execution model vs. optimization model (Lines 380–383): Their roles are not clearly defined; a short explanation of how these components interact would improve clarity.\n\n* The paper also claims environment diversity but does not clearly describe how diversity is quantified (Line 371). Providing explicit metrics or qualitative analyses would strengthen the claim that AUTOENV-36 spans meaningfully different environment classes.\n\n* A more detailed description of environments and model failure modes would make the paper easier to parse."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CWZD1hrZnP", "forum": "clEp43bcBy", "replyto": "clEp43bcBy", "signatures": ["ICLR.cc/2026/Conference/Submission2828/Reviewer_GLTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2828/Reviewer_GLTJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975090912, "cdate": 1761975090912, "tmdate": 1762916392073, "mdate": 1762916392073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}