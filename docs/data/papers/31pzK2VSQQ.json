{"id": "31pzK2VSQQ", "number": 12934, "cdate": 1758211832960, "mdate": 1759897475594, "content": {"title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "abstract": "arge Multimodal Models have achieved remarkable progress in integrating vision and language, enabling strong performance across perception, reasoning, and domain-specific tasks. However, their capacity to reason over multiple, visually similar inputs remains insufficiently explored. Such fine-grained comparative reasoning is central to real-world tasks, especially in mathematics and education, where learners must often distinguish between nearly identical diagrams to identify correct solutions.  To address this gap, we present VisioMath, a curated benchmark of 1,800 high-quality K–12 mathematics problems in which all candidate answers are diagrams with subtle visual similarities. A comprehensive evaluation of state-of-the-art LMMs, covering both leading closed-source systems and widely adopted open-source models, reveals a consistent decline in accuracy as inter-image similarity increases. Analysis indicates that the dominant failure mode stems from image–text misalignment: rather than grounding reasoning in textual cues, models often resort to shallow positional heuristics, resulting in systematic errors. We further explore three alignment-oriented strategies, spanning training-free approaches and finetuning, and achieve substantial accuracy gains. We hope that VisioMath will serve as a rigorous benchmark and catalyst for developing LMMs toward deeper diagram understanding, precise comparative reasoning, and grounded multi-image–text integration.", "tldr": "We introduce VisioMath, the first benchmark for evaluating mathematical reasoning with image-based answer options, revealing that current large multimodal models struggle with fine-grained visual distinctions and achieve limited accuracy.", "keywords": ["Figure-based Mathematical Reasoning", "Large Multimodal Models", "Mathematical Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d530c73995c006ea9f37f0f9b0793614b9ebac04.pdf", "supplementary_material": "/attachment/f075d69bb5ce0e3f2416ec590a235a2f2a3705cc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces VisioMath, a curated benchmark comprising 1,800 K–12 mathematics problems in which all answer options are diagrams that are highly visually similar. VisioMath aims to rigorously evaluate Large Multimodal Models (LMMs) on fine-grained, comparative mathematical reasoning tasks, particularly focusing on challenges where multiple image options differ only subtly. The authors present an in-depth evaluation of various state-of-the-art LMMs in both closed- and open-source families, reveal systematic weaknesses related to image–text misalignment, and propose alignment-oriented strategies that yield meaningful performance improvements. The benchmark, dataset construction pipelines, error analyses, and strategic interventions are all discussed in detail, with extensive experimental results and visualizations supporting the analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. VisioMath fills an under-explored niche by targeting figure-based multiple-choice mathematical reasoning where distractors are intentionally visually similar.\n2. offers insightfully categorizes failures (image–text misalignment predominates) and connects these with systematic bottlenecks in LMM architectures. The controlled shuffling and ablation study setting is quite nice, offering insights to advance the multimodal reasoning domain\n3. OK coverage of current vision or multimodal models with regards to evaluation and with perturbation setting to gauge the robustness of eval results"}, "weaknesses": {"value": "1. Limited Scope and Generalization: K12 and a quite narrow domain (geometry, algebra, etc.). The conclusions regarding LMMs’ comparative reasoning ability may not generalize to other STEM domains such as physics diagrams or scientific reasoning in general. If the authors want to target K12 for its pedagogical value maybe there should be a clear case made for that in analysis etc.\n\n2. Comparison with related work: more multimodal benchmark that explores vision-dependency of reasoning can be compared and discussed like SeePhys, MMSciBench, VisAidedMath etc. that goes beyond the current limited scope to position this work better in the landscape of multimodal reasoning, the authors could further strengthen this work by comparing the vision-based approach in these related works. The current first para of related work on LMM is really not that necessary and cannot possibly cover all LMMs due to the nature of that field's progress.\n\n3. Human baseline? Are these similarities only confusing for models or for humans as well? I also am a bit confused as to the necessity of classifying open-source LMMs into single vs. multi-image input? If they cannot process multiple images then they cannot process all the options correctly right?\n\nPS the current abstract entry on OpenReview has a entry typo \"Large\" missing an \"L\""}, "questions": {"value": "1. Many of the tables are too dense and hard to read (eye-straining already) or grasp key points e.g. by color highlights or optionally move something into the appendix.\n\n2. Wording can be improved, \"question stem length\" is confusing, \"visual-option\" should perhaps be \"image-option\" because otherwise I can easily understand it as \"visual-optional\" so overall the presentation needs to be improved, a lot of the wording are highly confusing and will benefit significantly from more polishing in phrasing.\n\nIn general I think this design is really nice for testing vision-based reasoning capability for LMMs and the shuffing/strategy experiments are quite nice as analysis. Although the overall presentation could be significantly improve to aid readability of this manuscript I think this paper has merited a OK contribution to this field."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CZqgdlT33l", "forum": "31pzK2VSQQ", "replyto": "31pzK2VSQQ", "signatures": ["ICLR.cc/2026/Conference/Submission12934/Reviewer_4GQo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12934/Reviewer_4GQo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760519461850, "cdate": 1760519461850, "tmdate": 1762923697778, "mdate": 1762923697778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisioMath, a new benchmark designed to address the evaluation gap in the fine-grained comparative reasoning abilities of current Large Multimodal Models when handling multiple, visually similar options. It presents VisioMath, a benchmark comprising 1,800 high-quality K-12 mathematics problems. Its unique characteristic is that all candidate answers are visually highly similar diagrams, compelling models to perform fine-grained visual discrimination and comparative reasoning, rather than simple pattern matching. Through extensive experiments on a series of SOTA models, the paper reveals a common deficiency in current models on this task, with the root cause being primarily image-text misalignment. Finally, the paper explores and validates three effective alignment strategies that significantly improve model performance. Its core contribution lies in providing a new, challenging evaluation tool, while also offering deep insights and practical directions for enhancing the multimodal alignment capabilities of LMMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies and addresses a critical problem in the field of LMM evaluation by focusing on visually similar diagrammatic options.\n\n- Through the quantification of visual similarity and an innovative \"option shuffling\" experiment, it uncovers that the core weakness of current LMMs lies in precise image-text alignment.\n\n- It proposes three concrete and viable performance enhancement strategies ranging from training-free methods to lightweight fine-tuning and experimentally demonstrates their effectiveness, offering the community a clear roadmap to tackle this issue."}, "weaknesses": {"value": "- The definition of visual similarity relies entirely on a single model (Qwen multimodal-embedding-v1). Different models' visual encoders may have varying interpretations of \"similarity.\" A brief discussion or experiment demonstrating the correlation or discrepancy with other metrics (such as CLIP embeddings or DINO scores), or one that further justifies this choice, would make this core metric more robust.\n\n- The fine-tuning experiment for Strategy 3 was conducted on only a single model (Qwen2.5-VL-3B). Applying this method to another model family with a different architecture and observing similar results would provide stronger evidence for the generality of this conclusion.\n\n- The analysis of \"Vision Recognition Errors\" lacks depth. At 34%, this error type represents a significant proportion. Are these errors due to the model's inability to recognize basic geometric shapes (e.g., confusing a triangle with a pyramid), or its failure to understand more abstract symbols (such as the monotonicity of a function's graph)? A more fine-grained error classification would offer greater insight into the limitations of current visual encoders in diagram understanding.\n\n- The dataset is sourced exclusively from Chinese exams. Although mathematics is a universal language, the phrasing of the problems and the style of the diagrams may still carry certain cultural or curricular imprints.\n\n- In the \"option shuffling\" experiment, when faced with misaligned labels, do the models tend to guess randomly, or do they systematically follow the new (but incorrect) positional cues (e.g., always choosing the image corresponding to the new position of label 'A')? This could provide deeper insight into the nature of the heuristic rules the models rely on."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lAWpFxSrpZ", "forum": "31pzK2VSQQ", "replyto": "31pzK2VSQQ", "signatures": ["ICLR.cc/2026/Conference/Submission12934/Reviewer_bDac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12934/Reviewer_bDac"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627861148, "cdate": 1761627861148, "tmdate": 1762923697459, "mdate": 1762923697459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisioMath, a new benchmark to evaluate the fine-grained, multi-image comparative reasoning of Large Multimodal Models (LMMs). The authors identify a critical gap in existing benchmarks: LMMs are rarely tested on problems common in K-12 mathematics, where all answer options are visually similar diagrams (e.g., graphs, geometric figures).\n\nThe VisioMath benchmark consists of 1,800 high-quality, curated K-12 math problems sourced from real-world exams. Its defining feature is that all answer options are diagrams, and the dataset includes a metric to quantify the (often high) visual similarity between them.\n\nThrough a comprehensive evaluation of state-of-the-art LMMs (e.g., GPT-4.1, Gemini 2.5 Pro, Qwen2.5-VL), the paper demonstrates:\n\n* A consistent decline in accuracy as the visual similarity between answer options increases.\n* The dominant failure mode (36% of errors) is image-text misalignment. Models rely on shallow positional heuristics (e.g., assuming the first image corresponds to option A) instead of true semantic grounding.\n* This reliance on heuristics is proven by a controlled option shuffling experiment, where permuting the text labels (e.g., \"A, B, C, D\" → \"B, C, D, A\") while keeping images in the same order caused a significant drop in accuracy (e.g., -8.7% for Gemini 2.5 Pro).\n\nFinally, the authors propose and validate three strategies to mitigate this: two training-free (consolidating images into a single layout and adding explicit visual-textual anchors) and one training-based (alignment-oriented CoT finetuning), which achieved a +12.6% accuracy gain on Qwen2.5-VL-3B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Originality and Significance: The paper identifies and rigorously tests a novel, practical, and highly significant problem. The task of \"figure-based option\" reasoning, especially with high-similarity distractors, is a ubiquitous real-world scenario (especially in STEM education) that has been almost entirely overlooked by existing benchmarks.\n* Benchmark Quality: The VisioMath benchmark is meticulously constructed and serves as an excellent diagnostic tool, not just a leaderboard. The curation from real exams ensures representativeness. Key quality steps include enforcing a one-image-per-option rule, balancing the answer distribution to prevent bias, and, crucially, quantifying the visual similarity of options to enable controlled analysis.\n* Analytical Depth: The paper's primary strength is its deep analysis, which goes far beyond reporting accuracy. The option shuffling experiment (Fig 4b) is a simple but brilliant controlled study that provides definitive proof of the model's reliance on positional heuristics. This insight is arguably more valuable than the benchmark itself. The error categorization (Fig 4a), which pinpoints \"image-text misalignment\" as the dominant failure, is also a key contribution.\n* Clarity: The paper is very well-written, with clear and effective figures. Figure 1 instantly communicates the problem, Figure 2 shows the careful data pipeline, and Figure 4 clearly presents the core analytical findings."}, "weaknesses": {"value": "* Apparent Contradiction in Concatenation Strategy: There is a confusing contradiction in the results. The baseline evaluation (Table 2) shows that single-image LMMs, which use a \"composite image concatenation strategy,\" perform at random-guess levels (e.g., LLaVA-v1.6 at 24.4%). However, \"Strategy 1,\" which also uses a \"consolidated single image layout\" (concatenation), *improves* the performance of multi-image LMMs (e.g., +6.4% for Seed1.6-Thinking). The paper does not adequately explain why concatenation fails so catastrophically for one set of models but serves as an effective enhancement for another.\n* Limited Scope of Finetuning (Strategy 3): The finetuning result (Strategy 3) is very promising, yielding a substantial +12.6% gain. However, this experiment was only conducted on a single, small model (Qwen2.5-VL-3B). This makes it difficult to assess if this is a generally effective strategy or if the gains would be as pronounced on larger, more capable models (e.g., 72B or closed-source models).\n* Benchmark Sourcing: The dataset is sourced exclusively from Chinese high school and college entrance examinations. While the authors translate the problems, this single-source origin may limit the benchmark's global representativity, as it may contain cultural or pedagogical biases specific to that education system.\n* Similarity Metric Rationale: The visual similarity for a question, Sim(Q), is defined as the *minimum* pairwise cosine similarity between options. This choice is not fully justified and could be fragile. For example, a question with three identical options and one complete outlier would receive a very low similarity score, even though it would be a very \"confusing\" set for a model. An average or median pairwise similarity might be a more robust measure of overall inter-option confusability."}, "questions": {"value": "1. Could you please clarify the discrepancy between the failure of single-image LMMs using concatenation (Table 2) and the success of Strategy 1 (also concatenation) for multi-image LMMs (Fig 4b)? Is this difference due to (a) the models' pre-training (single-image vs. interleaved multi-image), (b) the specific concatenation method used (e.g., zero-padding), or (c) the inherent architectural differences in how these models process a composite image?\n\n2. The +12.6% gain from your alignment-oriented CoT data is very impressive. To demonstrate the scalability of this approach, have you attempted to finetune other models in the Qwen2.5-VL family (e.g., the 7B or 72B versions) to see if this substantial gain is consistent across model scales?\n\n3. What was the rationale for choosing the *minimum* pairwise similarity as your Sim(Q) metric? Have you analyzed how the results would change if you used the *average* or *median* pairwise similarity, which might be more robust to a single outlier option?\n\n4. The gains from Strategy 2 (explicit anchors) vary wildly, from +9.8% for QwenVL-plus to only +0.9% for Gemini 2.5 Pro. What is your hypothesis for this large variance? Does it suggest that more advanced models like Gemini 2.5 Pro are already attempting this alignment (and thus don't benefit much from the explicit hint), whereas weaker models are not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PPE493oSPY", "forum": "31pzK2VSQQ", "replyto": "31pzK2VSQQ", "signatures": ["ICLR.cc/2026/Conference/Submission12934/Reviewer_9Krr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12934/Reviewer_9Krr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876750037, "cdate": 1761876750037, "tmdate": 1762923697176, "mdate": 1762923697176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, *“VisioMath: Benchmarking Figure-Based Mathematical Reasoning in LMMs,”* presents a new benchmark designed to evaluate **fine-grained visual reasoning in mathematical contexts**, particularly when **multiple highly similar diagrams** are presented as answer choices.\n\nThe authors construct **VisioMath**, a dataset of **1,800 high-quality K–12 math problems** collected from real entrance exams (2002–2023). Each problem provides **four visually similar diagrammatic options (A–D)**, with some including **images in the question stem**. Such *diagram-based answer formats* are indeed rare among existing math benchmarks, making this a **valuable and innovative dataset contribution**.\n\nThe benchmark evaluates a wide range of **open- and closed-source large multimodal models (LMMs)**, and key findings include:\n\n- **Multi-image reasoning remains a major weakness** — accuracy often drops to near chance level when multiple diagrams must be compared.  \n- **High visual similarity sharply reduces performance** — all models degrade significantly as image similarity increases, indicating weak fine-grained perception.  \n- **Positional bias exists** — models show uneven accuracy across option positions, especially when the question stem includes images.  \n- **Image–text misalignment dominates failure cases** — models frequently rely on superficial spatial cues rather than grounded reasoning.\n\nTo mitigate these issues, the authors explore **three alignment-oriented strategies**:  \n(1) merging all visuals into a **consolidated single layout**,  \n(2) overlaying **explicit textual anchors** on images, and  \n(3) fine-tuning with multi-image chain-of-thought (CoT) samples.  \nThese approaches lead to **notable performance gains**, demonstrating that targeted alignment interventions can substantially improve multimodal reasoning in visually complex mathematical tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel Benchmark Design for Visual Reasoning in Math**  \n   The benchmark uniquely incorporates **both image-based question stems and diagrammatic answer options**, a setting that is rarely seen in prior math benchmarks.  \n   This integration of text and diagrams offers a more realistic and cognitively demanding setup, and I believe such *interleaved visual–textual formats* represent an important future direction for multimodal reasoning benchmarks.\n\n2. **High-Quality, Real-World Data Source**  \n   The dataset is constructed from **authentic K–12 entrance exam problems**, ensuring **pedagogical validity**, **realistic problem difficulty**, and alignment with real-world educational reasoning tasks.  \n   This grounding in real exam materials enhances the dataset’s credibility and practical relevance.\n\n3. **Effective Improvement Strategies**  \n   The paper proposes **three lightweight yet effective alignment-oriented interventions** — layout consolidation, explicit textual anchors, and multi-image CoT fine-tuning.  \n   These strategies are simple to implement but lead to **clear and interpretable performance gains**, demonstrating practical value for improving multimodal alignment in large models."}, "weaknesses": {"value": "1. **Restricted Evaluation Format (Multiple-Choice Only)**  \n   Every question in VisioMath follows a **four-choice multiple-choice format**, which simplifies the reasoning process and limits evaluation to **discrete answer selection**.  \n   This design makes it difficult to assess open-ended or step-by-step reasoning ability, which is essential for understanding the full reasoning depth of large multimodal models.\n\n2. **Lack of Failure Case Analysis Beyond Accuracy**  \n   As a benchmark paper, the analysis would be more insightful if it included **qualitative examples of failure cases**.  \n   For instance, showing what a model’s **chain-of-thought output looks like when cross-image attention fails** would help readers better understand the specific weaknesses and reasoning breakdowns of current LMMs."}, "questions": {"value": "Overall, this is a very solid and well-executed paper.\nI do have one question: when evaluating on the benchmark, why didn’t the authors include specialized math-oriented multimodal models for comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zDj1H9xqdk", "forum": "31pzK2VSQQ", "replyto": "31pzK2VSQQ", "signatures": ["ICLR.cc/2026/Conference/Submission12934/Reviewer_ABAJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12934/Reviewer_ABAJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12934/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945502634, "cdate": 1761945502634, "tmdate": 1762923696707, "mdate": 1762923696707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}