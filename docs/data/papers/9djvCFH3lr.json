{"id": "9djvCFH3lr", "number": 23303, "cdate": 1758341921076, "mdate": 1759896821979, "content": {"title": "Group Contrastive Learning for Weakly Paired Multimodal Data", "abstract": "We present GROOVE, a semi-supervised multimodal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our approach integrates an on-the-fly backtranslating autoencoder with a novel group-level contrastive loss, GroupCLIP, which leverages shared labels to enforce group-level consistency. This approach encourages cross-modally entangled representations while maintaining group-level coherence within the shared latent space. Importantly, GroupCLIP bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal Supervised contrastive learning. Next, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. Across simulated datasets with varying degrees of modality coupling and two real single-cell genetic perturbation datasets, GROOVE is the only method to show consistent outperformance in downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multimodal representation learning in scenarios where only weak pairing is available.", "tldr": "A multi-modal semisupervised representation learning framework for weakly paired multi-modal perturbation data", "keywords": ["multimodal learning", "weakly paired data", "contrastive learning", "single-cell genomics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a72aea9f010507732ad2344a74df337a11531e49.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GROOVE, a semi-supervised representation learning method for weakly paired multimodal data. The approach integrates an on-the-fly backtranslating autoencoder with a group-level contrastive loss, GroupCLIP, which leverages shared labels to enforce cross-modal consistency within a shared latent space. The authors also propose a combinatorial evaluation framework that tests representation learners against various optimal transport (OT) alignment algorithms. The method is evaluated on cross-modal matching and imputation tasks across simulated and two real single-cell perturbation datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses the important and difficult problem of \"weakly paired\" multimodal data. This is a common scenario in fields like single-cell biology, where destructive measurement techniques prevent collecting coupled omics data.\n\n* The paper provides a robust evaluation framework, when compared to state of the art. By testing representation learners against various alignment algorithms, the authors decouple these two choices and provide a thorough assessment."}, "weaknesses": {"value": "* While the problem addressed in this work is important, and the results seem solid and insightful to me, the novelty of the method appears  limited. On the one hand, it's simplicity is a strength, in my opinion, but the proposed methodology is not as preponderant as the effort the authors put in the experimental evaluation.\n\n* Although the evaluation section is solid, in my opinion, it lacks several benchmarks established in the literature (see questions below)."}, "questions": {"value": "* The GROOVE method was mainly evaluated on ATAC, protein, and count data. How does it handle other kinds of modalities, such as images? The study by (Xi, 2024) [1] uses different benchmarks, including CITE-seq Data (NeurIPS 2021 challenge) and PerturbSeq/Single Cell Images, which provide more diversity in modality type. Would extra experiments on the datasets studied in [1] provide stronger empirical evidence of the method's generalizability? Is there time for the authors to assess the performance of the proposed method on such additional datasets?\n\n* The ablation study in Table 5 is central to the claim that GroupCLIP is the key performance driver justifying the overall method performance. However, the \"No GroupCLIP\" model is effectively just the backtranslation framework, which the authors admit may be suboptimal without a pre-trained encoder. Can you elaborate more on how the ablation support the claim of GroupCLIP being so central to the performance of the proposed method?\n\n* In [1], it is argued that any model with a reconstruction loss is forced to learn modality-specific noise, which is \"counterproductive to matching\". Could the authors discuss this claim in the context of GROOVE's autoencoder and backtranslation components? Furthermore, could this theoretical conflict explain your paper's own observation that \"better matching performance does not guarantee optimal downstream task performance\"? For instance, is it possible that the reconstruction-based losses (which the work in [1] argues are bad for matching) are actually necessary or beneficial for the downstream imputation task, thereby creating a trade-off between the two objectives?\n\n[1] Xi, Johnny, et al. \"Propensity score alignment of unpaired multimodal data.\" Neurips (2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FcGK1JPihe", "forum": "9djvCFH3lr", "replyto": "9djvCFH3lr", "signatures": ["ICLR.cc/2026/Conference/Submission23303/Reviewer_JKiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23303/Reviewer_JKiR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569581467, "cdate": 1761569581467, "tmdate": 1762942597436, "mdate": 1762942597436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper tackles multimodal settings where examples from two modalities share only a group/label (e.g., perturbation ID) rather than instance-level pairs, and proposes a group-aware contrastive objective (“GroupCLIP”) that pulls together all samples from the same label across modalities while pushing apart different labels. \n- The method combines this group-contrastive objective with a reconstruction pathway and on-the-fly “backtranslation” between modalities, yielding a shared embedding space that supports both matching and imputation. \n- The paper argues that evaluating only instance-level matching is inadequate for weakly paired data, and introduces a combinatorial evaluation protocol that factors out aligner choice using labeled variants of entropic OT, GW-OT, and COOT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a realistic regime where modalities cannot be co-measured on the same cell, making group-level supervision both natural and necessary. \n- The paper is well written and easy to follow\n- The evaluation design separates representation learning from alignment by sweeping multiple labeled OT variants, which reduces confounding and yields a more credible comparison across methods. \n- The ablations are clear and show that removing the group-contrastive term degrades performance consistently across metrics, which supports the central claim about the importance of group-aware contrast."}, "weaknesses": {"value": "- CLIP’s web pairs are often weak and effectively many-to-one (e.g., many different dog images paired with near-identical captions), so large-scale CLIP training already approximates a group-level supervision regime rather than strict instance pairing. Thus it is important for the paper to show clear advantages in regimes where per-instance captions carry little unique information beyond a coarse label. The key claimed difference is that GROOVE does not need any per-instance pairing at all, whereas CLIP still relies on a text associated with each image; however, this difference is only compelling if the method outperforms strong CLIP-style baselines constructed to mimic weak pairs.\n- The group-level objective risks collapsing within-label diversity because it contracts all samples of a label toward each other across modalities, which can be harmful when a perturbation has heterogeneous cellular responses; this risk is amplified by balanced per-label batching that repeatedly couples the same aggregate label sets. \n- The backtranslation path adds complexity but appears to contribute less than the group-contrastive term in ablations, which raises questions about whether the extra module is necessary relative to a streamlined group-contrastive-only baseline with stronger decoders. \n- The approach assumes label sets are perfectly aligned across modalities during training, but real datasets often contain partial, missing, or mis-specified label mappings; the paper does not evaluate robustness to label noise or missing labels, which is central to the weakly paired setting it targets. \n- The proposed evaluation leans heavily on OT-based aligners, and while the authors sensibly sweep variants, the method’s ranking changes across datasets, which suggests sensitivity to the aligner choice. The paper does not analyze why particular aligners pair best with GROOVE or how to choose them reliably without oracle tuning."}, "questions": {"value": "see weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OLRYGdp6GK", "forum": "9djvCFH3lr", "replyto": "9djvCFH3lr", "signatures": ["ICLR.cc/2026/Conference/Submission23303/Reviewer_8e99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23303/Reviewer_8e99"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774325376, "cdate": 1761774325376, "tmdate": 1762942597196, "mdate": 1762942597196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an important problem in single-cell biology: learning representations from technically unpaired multi-modal data with shared functional labels such as perturbations (making it “weakly” paired). It introduces GroupCLIP, a novel contrastive loss extension for cross-modal representation alignment based on group-level supervision. This loss is combined with backtranslating autoencoders for higher-quality pseudo-pair generation. In an extensive evaluation with various optimal transport methods for matching, the method shows improved performance in cross-modal matching and imputation on small single-cell perturbation sets compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper tackles the important and practical challenge of learning from weakly paired multimodal data (where only group labels connect modalities), a common scenario in biological perturbation screens\n\n2) The core contribution, the GroupCLIP loss effectively bridges the gap between cross-modal contrastive learning and uni-modal supervised contrastive learning for this specific weakly paired setting\n\n3) The proposed method, GROOVE, outperforms the most comparable methods on real single-cell data for both matching and imputation\n\n4) The paper includes both an ablation study showing that GroupCLIP is the main driver of the performance, as well as a thorough evaluation framework for all-against-all (learner and OT method) comparison.\n\n5) The paper is generally well-written, clearly motivated, and provides substantial methodological detail (extensive appendices on architecture, sampling, baselines, and simulations)"}, "weaknesses": {"value": "1) Many of the performance differences reported in the simulation results (Table 1 Bary. FOSCTTM, Table 2) appear small and likely not statistically significant given the overlapping standard errors. The authors should provide a statistical test to show significant improvement.\n\n2) The paper doesn't provide any analysis on a) sensitivity to hyperparameters alpha and beta that balance the GroupCLIP and reconstruction/backtranslation losses. It's unclear if the chosen values generalize or require dataset-specific tuning. b) the effectiveness of a balanced undersampling strategy is proposed, there's no analysis showing its effectiveness or the limits of imbalance the method can tolerate.\n\n3) The discussion section focuses primarily on future directions for the community rather than critically analyzing the limitations of the proposed method itself (e.g., potential failure modes, hyperparameter sensitivity, unclear value of backtranslation).\n\n4) The real-world datasets used are relatively small subsets derived after significant feature selection and focusing on specific experimental conditions. While understandable for computational reasons (OT scaling) and somewhat sufficient for demonstration on more homogeneous data, it leaves open the question of whether the method scales and performs well on larger and more heterogeneous datasets that are of interest for real-world screens.\n\n5) The abstract's claim of \"consistent outperformance in downstream cross-modal matching and imputation tasks\" [lines 023f] is slightly overstated, as GROOVE did not significantly outperform the other methods on the simulations. Also, while GroupCLIP is novel, the overall GROOVE architecture relies heavily on adapting an existing backtranslating autoencoder framework from unsupervised machine translation. The ablation results (Table 5) also suggest the backtranslation component itself adds minimal value over a standard autoencoder in this setup, making the main effective innovation primarily the GroupCLIP loss. But since this is in an applications track and the results are outperforming alternative methods, this might be less important but it should be acknowledged in abstract/discussion.\n\n6) While correctly identifying a gap for weakly paired supervised CLIP, the background could acknowledge existing supervised/semi-supervised CLIP extensions that use few perfect pairs (e.g., S-CLIP, SemiCLIP)."}, "questions": {"value": "1) The ablation description for \"Autoencoder only\" vs. \"No GroupCLIP\" could be clearer. Does \"Autoencoder only\" include GroupCLIP?\n\n2) The results show mostly superior performance when using label-constrained OT methods. Could you briefly discuss why leveraging labels during the OT alignment step provides such a significant boost compared to standard OT, even after label-aware representation learning with GroupCLIP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iQTybT0CDS", "forum": "9djvCFH3lr", "replyto": "9djvCFH3lr", "signatures": ["ICLR.cc/2026/Conference/Submission23303/Reviewer_dmZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23303/Reviewer_dmZz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777131999, "cdate": 1761777131999, "tmdate": 1762942596984, "mdate": 1762942596984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GROOVE, a multimodal representation-learning approach for weakly paired data. In particular, GROOVE addresses challenges in single-cell perturbation analyses, where, while valuable, multimodal analysis with measurements from different modalities but the same cells can be infeasible to obtain. The proposed method combines a novel group-level contrastive loss (GroupCLIP), which leverages shared labels (across perturbations) to enforce consistency across modalities. It also leverages an on-the-fly backtranslating autoencoder to enforce well-mixed shared representations between modalities. The authors benchmark GROOVE against two standard baselines and compare representation learners across multiple optimal transport aligners. Experiments across simulated datasets (with varying degrees of modality sharing) and two real single-cell perturbation datasets show that  GROOVE (with an appropriate OT aligner) can lead to performance improvement in downstream cross-modal matching and imputation tasks. The ablation studies show that GroupCLIP is the key component for the performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- GroupCLIP is an extension that combines SupCon (using supervised class-labels for contrastive learning) and CLIP (for cross-modal alignment). Technically, it is a straightforward extension, but I find this simplicity a strength rather than a weakness.\n\n- The motivation of the work is clear and addresses an important gap; multimodal methods that can leverage weakly paired data are crucial for biological applications where true paired measurements are experimentally infeasible\n\n- A well-motivated experimental design evaluating different OT aligners."}, "weaknesses": {"value": "- W1: The contributions of the paper are minimal. Besides GroupCLIP, the second contribution is the backtranslating autoencoder. This, however, doesn’t seem to have any positive effect on GROOVE’s performance.\n- W2: The experimental analysis is limited to only two baselines. The authors discuss many more methods in the related work, but it’s unclear how these methods differ from GROOVE and why they weren’t chosen for benchmarking (for instance, Samaran et al, 2024)\n- W3: Inconclusive findings wrt design choices of similarity metrics and OT aligners"}, "questions": {"value": "- The ablation study shows that removing GroupCLIP causes the largest performance drop, while backtranslation alone performs similarly to a standard autoencoder. Does this suggest that a simpler architecture employing GroupCLIP with a standard autoencoder would be equally effective? Have you evaluated this at different shared portion settings and real data?\n- How does GROOVE scale computationally as the number of perturbations increases? The paper mentions ~20 perturbations, but this number could be much larger. Does the undersampling strategy become inefficient when there are many rare labels? Would this also affect the labeled OT aligners?\n- How sensitive is GROOVE to the temperature parameter τ and the loss weights λ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KtuW9PKs58", "forum": "9djvCFH3lr", "replyto": "9djvCFH3lr", "signatures": ["ICLR.cc/2026/Conference/Submission23303/Reviewer_uFXB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23303/Reviewer_uFXB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915510297, "cdate": 1761915510297, "tmdate": 1762942596704, "mdate": 1762942596704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}