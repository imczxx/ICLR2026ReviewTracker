{"id": "rs6rRCEixQ", "number": 21910, "cdate": 1758323458938, "mdate": 1759896897261, "content": {"title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent Generated Code in Real-World Tasks", "abstract": "Vibe coding, the practice of letting LLM agents complete complex coding taskswith little human supervision, is increasingly used by engineers, especially beginners. However, is it really safe when the human engineers may have no abilityor intent to examine its outputs? We propose SUSVIBES, a benchmark consistingof 200 software engineering tasks from real-world open-source projects, which,when given to human programmers, led to vulnerable implementations. Whenfaced with these tasks, widely adopted open-source coding agents with strongfrontier models perform terribly in terms of security. Although 47.5% of the tasksperformed by Claude 4 Sonnet are functionally correct, only 8.25% are secure.Further experiments suggest that inference scaling and LLM-as-a-judge mitigatethe issue to some extent, but do not fully address it. Our findings raise seriousconcerns about the widespread adoption of vibe-coding, particularly in securitysensitive applications.", "tldr": "", "keywords": ["Vibe Coding", "Code Security", "Agentic System"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c6a055929385aa0e8440de0d7f7fe73a2f911e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SusVibes, a benchmark consisting of 200 software engineering tasks derived from real-world open-source projects, designed to evaluate the functionality and security capabilities of coding agents in vibe coding. In addition, the paper introduces an automatic curation pipeline that constructs repository-level coding tasks equipped with a runtime evaluation environment for end-to-end assessment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Scope: The benchmark focuses on large-scale repositories with complex file structures, emphasizing realistic, repository-level coding environments.\n\n2. Evaluation dimensions: It jointly assesses functional correctness and security correctness, ensuring that models are tested not only for task completion but also for safe and secure implementation.\n\n3. Coverage: The dataset spans 77 CWE categories, capturing diverse vulnerability types across real-world projects.\n\n4. Design intuition: The ideas behind Constraint I (Patch Enclosure) and Constraint II (Security Implications) are well-motivated and provide an intuitive foundation for constructing security-aware tasks.\n\n5. Findings: Coding agents often achieve functional correctness but fail security checks on some tasks. Simple mitigation strategies—such as security-themed prompting, CWE self-identification, or even oracle CWE hints—do not reliably close this gap and often lead to a trade-off between functionality and security."}, "weaknesses": {"value": "1. Unclear source of test cases:\nThe paper does not clearly specify where the functional and security test cases originate from—whether they are collected from real project test suites, generated by LLMs, or synthesized during the curation pipeline.\n\n2. Pipeline clarity and reproducibility:\nWhile the three-stage pipeline (masking, problem generation, verification) is novel and interesting, its description remains conceptually vague. The paper lacks concrete examples or visual case studies showing how a real repository sample passes through each stage.\n\n3. Quality of generated masks:\nIt is difficult to ensure the mask quality, particularly whether masking removes or alters critical contextual information needed for code understanding and security reasoning.\n\n4. Inaccuracy in mask ratio control:\nThe mask ratio (α) is controlled by the Stage-1 agent, which might not consistently or accurately capture the right functional boundary—especially across large repositories with complex dependencies.\n\n5. CWE scope ambiguity:\nThe paper does not make clear whether each task corresponds to a single CWE or involves multiple overlapping CWE types."}, "questions": {"value": "1. Where do the functional and security test cases come from?\n\n2. Can the authors provide a concrete case study demonstrating the three-stage pipeline process (Stage 1–3) on a real vulnerability example?\n\n3. Does each task map to a single CWE, or can it belong to multiple CWEs simultaneously?\n\n3. How is mask-ratio control validated? Has any quantitative analysis been done to verify that the Stage-1 agent accurately identifies proper masking boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LysDF9rVMp", "forum": "rs6rRCEixQ", "replyto": "rs6rRCEixQ", "signatures": ["ICLR.cc/2026/Conference/Submission21910/Reviewer_b85Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21910/Reviewer_b85Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451142912, "cdate": 1761451142912, "tmdate": 1762941977496, "mdate": 1762941977496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SUSVIBES, a benchmark of 200 repository-level coding tasks derived from real-world security vulnerabilities, to evaluate the security of LLM-based coding agents. The study finds that while frontier models achieve reasonable functional correctness, the vast majority of their solutions contain security vulnerabilities, and simple prompting strategies fail to mitigate this issue while degrading functional performance."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Highly relevant and timely problem: SUSVIBES operates at the repository level, requiring cross-file edits and build/test interactions, a major step beyond existing single-file benchmarks, also, from the reviewer’s point of view, vibe coding is one of the timely topics, making this research critically important for the current AI-assisted programming landscape.\n\nNovel benchmark construction: The three-stage automated pipeline systematically constructs tasks from real vulnerabilities, ensuring scalability and authenticity, which can also be extended to other applications\n\nGood evaluation and insights: This work identifies strong evidence that current agents are functionally competent but insecure with case studies such as Django password verification examples, clearly demonstrates timing-based vulnerability reproduction by LLMs, providing the readers with a straightforward and clear idea of the task of benchmarking vibe coding on LLMs."}, "weaknesses": {"value": "Data imbalance: One of the concerns is from Fig 3, among all 10 categories, the data distribution is imbalanced, which may affect the effectiveness of the evaluation with SUSVIBES.\n\nPotential dataset bias: The benchmark heavily depends on public vulnerability repositories, which may favor well-documented projects and miss obscure bug classes.\n\nInsufficient mitigation approach: This work has insights into how insecure the current vibe coding is, while proposing some potential mitigation approaches beyond prompt based techniques will make this paper more solid\n\nAnalysis can be improved: From the reviewer’s aspect, security correctness in this work is defined by test outcomes; weak or incomplete tests could underestimate latent vulnerabilities. Also, only a few models such as Claude, Gemini and Kimi are tested, which I think is not sufficient. How’s other model family, for example, Qwen, GPT etc. performs on these tasks."}, "questions": {"value": "Can you please justify why swe-agent was chosen to generate the data? Have you tested other agent systems? How good are they?\n\nAlso back to the weakness on analysis, why did you choose Claude, Kimi and Gemini but not other models?\n\nAre there plans to release a leaderboard or ongoing benchmark updates as new vulnerabilities or projects emerge?\n\nHow do you ensure the difficulty distribution across 200 tasks is reasonable? Could some tasks be too trivial or too difficult, affecting the benchmark's discriminative power?\n\nFig. 4 shows different models excel at different CWE types. I am curious if this work also analyzed which CWE types are inherently harder to detect or fix? What are the root causes of these differences?\n\nWhat specific mitigation strategies or best practices would you recommend for engineering teams wanting to use vibe coding in production, as it was proposed in the weakness?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "It would be better to discuss 1. How should people avoid abusing vulnerable code from Vibe Coding 2. Benchmark tasks are from real-world projects; are they properly cited or credited? Thus, I think it may have security and terms of use problems."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l0kYU7Ji0K", "forum": "rs6rRCEixQ", "replyto": "rs6rRCEixQ", "signatures": ["ICLR.cc/2026/Conference/Submission21910/Reviewer_3D9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21910/Reviewer_3D9P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501132751, "cdate": 1761501132751, "tmdate": 1762941977044, "mdate": 1762941977044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds a repository-level secure code generation benchmark with an automatic pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It extends secure code generation benchmarks to the repository level, which is more realistic and closer to real-world development scenarios.\n- It provides an automatic pipeline for constructing tasks, which has potential to extend to more projects."}, "weaknesses": {"value": "- The masking process is done by an LLM. Did the authors manually examine the masked results to ensure they are reasonable?\n- In Section 3.1.1, the paper mentions 'a test patch T, likely examining security.' How does the automatic pipeline determine whether a test is related to security, even if it appears in the same commit as the patch?\n- Since the entire problem is generated by LLM, how do the authors justify that each task is reasonable and not vague or missing critical information? For example, could a senior human software engineer complete the task and produce secure code with the provided context?\n- The benchmark only includes 200 tasks, despite the authors claiming to have collected ~20,000 open-source vulnerability records over the last 10 years. This raises concerns about the success rate of the pipeline."}, "questions": {"value": "- I understand the security issue and the need for a fake runtime in the Django example, but could the authors further explain how this example relates to 'spam emails or junk messages'?\n- Could the authors provide more task examples (like Figure 6) or release the entire dataset? Having access to more examples would help better assess the quality of the benchmark.\n- The authors claim the benchmark consists of 100% cross-file edits. Could they explain why the Django example is considered a cross-file edit, given that it appears to modify only one function in a single file?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KOEAiqi2Ps", "forum": "rs6rRCEixQ", "replyto": "rs6rRCEixQ", "signatures": ["ICLR.cc/2026/Conference/Submission21910/Reviewer_DRLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21910/Reviewer_DRLX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987335463, "cdate": 1761987335463, "tmdate": 1762941976717, "mdate": 1762941976717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SUSVIBES, a novel benchmark that evaluates the security of \"vibe coding\" by asking agents to complete repository-level tasks. The authors propose a 3-stage agent pipeline for task creation by masking a feature and auto-generating a natural-language \"feature request\".  The benchmark is grounded in two sets of human-written unit tests, one for correctness and the other for security. The findings reveal that agents are functionally capable but highly insecure. The authors also show that simple prompting-based mitigations fail, often worsening the overall success rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a critical, timely, and practical problem, as \"vibe coding\" represents a rapidly growing phenomenon.\n- This work introduces an LLM-assisted pipeline for constructing large-scale data to quantify security risks. The repository-level context, 100% cross-file-edit requirement, and dynamic execution environment present practical and complex challenges. The inclusion of human-written unit tests provides reliability for the \"ground truth” of correctness and security. \n- The core finding, demonstrating that functional correctness and security are frequently in opposition for LLM agents, represents an important insight. The subsequent findings that simple mitigation strategies (such as prompting) fail and even exacerbate the overall success rate (Figure 5) constitutes a contribution."}, "weaknesses": {"value": "- Curation Pipeline Reliability: Given the finding that current frontier LLM agents perform poorly at implementing security, how do the authors ensure reliability when using these agents for security-critical benchmark construction? Specifically:\n  - Can LLM agents generate a \"Patch-enclosing feature mask\" that accurately captures the necessary security context without over- or under-masking?\n  - As the checker in \"Security Implication Verifier\" (Stage III) is also an LLM agent, it might suffer from the same security blind spots as the agents being evaluated.\n\nMore clarification on the reliability of this agent-driven curation process, especially regarding the security implication verification, would strengthen the paper.\n\n- Evaluation Proxy is Narrow: The benchmark's definition of \"secure\" is to pass the specific human-written tests  that were added to fix the known vulnerability. However, a solution could pass this one test while introducing new vulnerabilities that are not tested for.  This limitation is compounded by the small average of 4.1 security test cases in the dataset.  More evaluation methods such as static and dynamic analysis can be used for vulnerabilities detection. \n\n- Potential Source Data Bias: The benchmark is derived from publicly-identified, fixed, and test-covered vulnerabilities. The benchmark may not represent the full landscape of security issues. The vibe coding agents might be introducing more subtle, novel, or unknown flaws that have no historical precedent and are not in datasets like  (Wang et al., 2024; Akhoundali et al., 2024).\n\n- Missing discussion of some related work, e.g., [1,2,3]\n\nReference:\n- [1] RedCode: Risky Code Execution and Generation Benchmark for Code Agents, NeurIPS 2024 Datasets and Benchmarks Track\n- [2] CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models, SATML 2024\n- [3] Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models, 2023"}, "questions": {"value": "- Models used for dataset construction: In Section 3.1.1, the authors utilize a 3-stage agent-flow pipeline (Masking, Problem Generation, Verification) to construct the benchmark tasks. Could you please clarify which models/agents were used for these stages?\n- CWE Assignment: The paper mentions the dataset is built from existing datasets (Wang et al., 2024; Akhoundali et al., 2024). Could the authors please clarify the specific process used to assign ground-truth CWE categories to each task in the SUSVIBES dataset? For instance, was this an automated mapping from the source datasets, or did it involve manual verification or annotation?\n\n- CWEs per Task: Please clarify if each SUSVIBES task is linked to a single or multiple CWEs. Including statistics on the distribution (e.g., average CWEs per task, percentage of tasks with multiple CWEs) would help in understanding the complexity of the security challenges within the benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vWiisd1MBr", "forum": "rs6rRCEixQ", "replyto": "rs6rRCEixQ", "signatures": ["ICLR.cc/2026/Conference/Submission21910/Reviewer_9B9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21910/Reviewer_9B9S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168108237, "cdate": 1762168108237, "tmdate": 1762941976455, "mdate": 1762941976455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}