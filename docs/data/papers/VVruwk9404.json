{"id": "VVruwk9404", "number": 2550, "cdate": 1757143400030, "mdate": 1759898141653, "content": {"title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure", "abstract": "Low-rank architectures have become increasingly important for efficient large language model (LLM) pre-training, providing substantial reductions in both parameter complexity and memory/computational demands. Despite these advantages, current low-rank methods face three critical shortcomings: (1) compromised model performance, (2) considerable computational overhead, and (3) limited activation memory savings. To address these limitations, we propose **C**ross-layer Low-**R**ank residual **Net**work (**CR-Net**), an innovative parameter-efficient framework inspired by our discovery that inter-layer activation residuals possess low-rank properties. CR-Net implements this insight through a dual-path architecture that efficiently reconstructs layer activations by combining previous-layer outputs with their low-rank differences, thereby maintaining high-rank information with minimal parameters. We further develop a specialized activation recomputation strategy tailored for CR-Net that dramatically reduces memory requirements. Extensive pre-training experiments across model scales from 60M to 7B parameters demonstrate that \\textit{CR-Net} consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.", "tldr": "We propose a low-rank framework for LLMs pre-training named CR-Net which leveraging cross-layer activation residuals to enhance model efficiency while maintaining performance, reducing computational/memory costs.", "keywords": ["Parameter-efficient", "LLMs pre-training", "cross-layer low-rank", "low-rank pre-training."], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34c9b534f805f35571bbe9282c0ec5675d03d010.pdf", "supplementary_material": "/attachment/1edd39284a3682bde1fda03d1687e3b44c0e7c20.zip"}, "replies": [{"content": {"summary": {"value": "Paper presents a method to change the linear layers in LLMs into a low-rank (AB) decomposition plus residual activations from the corresponding matrix in the previous (l-1) layer in the transformer, together with a learnable interpolation factor per each layer. It reduces the training time, the training memory footprint, and sometimes reduces the perplexity."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This is an interesting method which reduces pre-training time significantly. \n* Reduces memory significantly"}, "weaknesses": {"value": "* Table 4 shows improvement over baseline (Adam) for 7B model, but only in table 10 (in the appendices) we discover that the method is less good in the case of 13B. For fairness, I think the two tables should be adjacent so the reader can see them both and get the whole picture easily. So we understand the benefits in reducing training time and memory, but it should be clear that as the model size gets bigger, the quality (perplexity) of CR-Net gets worse than the baseline.\nIn line 1225 it says “The evaluation perplexity shown as Table 10 illustrates the performance of CR-Net in 13B model, which demonstrates that CR-Net exhibits a comparable performance than full-rank training with 8-bit Adam. Thus can be scaled to models with 13B parameters.”\nI think that “slightly worse” is more appropriate than “comparable”, and the assertion “Thus can be scaled” is wrong. There is no evidence for it. When we analyze the trend of what happens in larger models, we see that the CR-Net edge decreases (ppl-wise), so it feels like in larger models it would become worse. Anyhow, without evidence, giving such statement is probably a bit too far-reaching \n\n* Line 196, “Different from…” should be checked for grammar errors.\n* Line 204, “approached”. Do you mean “approaches”?"}, "questions": {"value": "* Figure 1, ROPE -> RoPE\n* Table 4 description mentions that the “10K, …” are the number of training steps.\n* Table 3, the reader is also interested in 7B. I assume in order to fit in memory you can only do it with “re-computation”, so I suggest mentioning that, and to refer the reader to table 4.\n* Theorem, (44), I assume a quantifier “for each i” should be added."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LGpaPJUCA5", "forum": "VVruwk9404", "replyto": "VVruwk9404", "signatures": ["ICLR.cc/2026/Conference/Submission2550/Reviewer_WfvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2550/Reviewer_WfvT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710956236, "cdate": 1760710956236, "tmdate": 1762916278780, "mdate": 1762916278780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Unlike prior works, which focus on low-rank approximations of activations or the weights directly, CR-Net proposes to use the previous layers activations to enable a lower reconstruction error. I believe this is a very interesting approach which the authors verify empirically and also theoretically to some extent.\n\n\nThe authors found that naively training with this alone is not sufficient and will lead to poor stability. To address this, they introduce a learnable parameter beta for dynamically adjusting the contribution of the previous activation and the current low-rank output."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The experimental evaluation is very thorough and of sufficient scale to highlight the practical utility of CR-Net. The idea is simple, makes a lot of sense, and is proven to work very well. If the authors are able to provide reproducible code, then extending this to other architectures is a very natural progression of this work.\n\nThe computational cost in terms of FLOPs is quite a complex expression. This is not a problem in itself and the authors provide a discussion on the various regimes for L355:358, which is good. Furthermore, the fact that CR-Net can enable much smaller values of r to compensate for the first layer cost, while also enabling improved training performance, really highlights the benefit of CR-Net."}, "weaknesses": {"value": "Although I do not have any major concerns, there are a few points which I believe the authors should address.\n\nMissing important related work [1] that uses very low rank projections of the intermediate activations to save memory. However, in contrast to this proposed work, they use random projection (no SVD init) of sub-tokens and an optimal reconstruction during the backwards pass. It seems CR-Net outperforms both [1] and [3], but including their results in table 3 would be good to see. Finally, there is missing a discussion on QLoRA, and finally how does CR-Net perform when the activations are also quantised? \n\n[1] VeLoRA : Memory Efficient Training using Rank-1 Sub-Token Projections. NeurIPS 2024\n\n[2] Flora: Low-rank adapters are secretly gradient compressors. ICML 2024\n\n[3] Qlora: Efficient finetuning of quantized llms. NeurIPS 2023\n\nSec 2.2 Related works would be much better before the preliminary section. This RW section does not use the preliminary notation anyway and it seems to just break up the flow of reading from preliminary notation -> method (section 3).\n\nMinor comments\n\n\"inputs and outputs of linear layers at the position P, where P stands for the detailed position in the transformer layer.\" the second part (after the comma) describing what P is again seems a bit redundant.\n\neqn (2) should the activation not be a swiglu? for consistency with the start of this paragraph, which is presenting CR-Net with the LLaMA architecture.\n\n\nenlarging the text in figure 1. At the moment it is very hard to read without having to zoom in a lot.\n\n\n\"making CR-Net NOT a simple extension of existing low-rank frameworks for pre-training.\" - I understand the intent, but I would encourage the authors to remove the bolding and capitalising of \"NOT\".\n\n\n\"Whether does the learnable scaling factor βPl benefit the model convergence?\" grammar mistake -> \"Does the learnable scaling factor βPl benefit model convergence?\"\n\nL464: \"ovservation\""}, "questions": {"value": "Does the first layers activations have to be full-rank for good performance? It would be interesting to see an ablation on this. It makes sense for this decision to be important, but I think highlighting and showing why this is needed could be nice to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TuXSZIsv5b", "forum": "VVruwk9404", "replyto": "VVruwk9404", "signatures": ["ICLR.cc/2026/Conference/Submission2550/Reviewer_BJ5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2550/Reviewer_BJ5z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761056769080, "cdate": 1761056769080, "tmdate": 1762916278536, "mdate": 1762916278536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new parameter-efficient pre-training framework for LLMs, termed CR-Net. Specifically, CR-Net is designed to exploit the inherent low-rank structures (indicated by Figure 2) of the difference between activations of adjacent layers. It replaces the full-size weight parameters at each layers with low-rank parameterized matrices and learnable linear combination factors that reconstruct the activation of current layer from that of the previous layer (Equation 6). The authors also proposed a novel activation-recomputation strategy (Algorithm 1) to further reduce the memory footprint of the model in propagation, which is essential for large-scale pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors evaluate the CR-Net's performance on the C4 dataset with LlaMA-2 models scaling from 60M to 7B parameters (Table 3 and Table 4), showing that CR-Net achieves a comparable or better perplexity than the full-rank model, with a significant reduction in the number of parameters and the amount of computation. \n\n- The authors also conduct systematic analysis on the FLOPs and parameter complexity of CR-Net and several related baselines (Table 1 and Table 2)."}, "weaknesses": {"value": "In general, the logic flow of this paper is clear, and the experiment results are convincing. I am considering raise my score accordingly if my concerns are properly addressed.\n\n\n**Concern 1. Comparison with the pretraining method that leverages fully low-rank weights [1].**\n\nWhile CR-Net proposed to replace the full-size weight parameters at all but the first layers with low-rank factors, I aware that a pretraining method that leverages fully low-rank weight parameters has been proposed in [1]. What is the most essential benefit of CR-Net over this method, which also leverages fully low-rank weights? Moreover, including this baseline in Table 3 would be helpful for understanding the state of low-rank pretraining methods. \n\n\n> [1]. Mo, Z., Huang, L.-K., & Pan, S. J. (2025). Parameter and memory efficient pretraining via low-rank Riemannian optimization. In The Thirteenth International Conference on Learning Representations (ICLR 2025). \n\n-----\n\n**Concern 2. How does CR-Net overcomes the difficulty of low-rank pretraining?**\n\nAs discussed in Table 2 of [2], pretraining models with fully low-rank weights from scratch is difficult and usually suffers from severe numerical instability. How does CR-Net overcome this difficulty, while it also trains models with a lot of low-rank weights?\n\n> [2]. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., & Tian, Y. (2024). Galore: Memory-efficient LLM training by gradient low-rank projection. arXiv preprint arXiv:2403.03507.\n\n-----\n\n**Concern 3. Run-time efficiency of CR-Net.**\n\nAs modern GPUs are optimized for large-scale parallel computation, some low-rank computations with lower FLOPs does not necessarily lead to better run-time efficiency (e.g., token per second). Therefore, I recommend the authors to provide additional runtime efficiency experiments between CR-Net and the full-rank model in inference and training."}, "questions": {"value": "See **Weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E8wvE8rjBg", "forum": "VVruwk9404", "replyto": "VVruwk9404", "signatures": ["ICLR.cc/2026/Conference/Submission2550/Reviewer_DNpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2550/Reviewer_DNpk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571987348, "cdate": 1761571987348, "tmdate": 1762916278379, "mdate": 1762916278379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors empirically discovered that the low rank reconstruction error for the *difference* from the previous layer's activations is less than that for the layer's activations, relatively, suggesting that we should do low rank modeling for this difference instead. The scale of the previous layer's activation $Y_{l-1}^P$ going into the current layer $Y_l^P$ is a learnable parameter, balancing between whether it relies heavily on the low rank residual term or refines from the previous activation. Moreover, the first layer uses full rank.\n\nThe paper proposes activation-efficient re-computation to address the challenges of new dependencies for gradient checkpointing.\n\nExperiments provided for pretraining Llama 2 with varying sizes up to 7B on the C4-en dataset and comparing with methods with similar low rank complexity or full rank with similar optimizer memory overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple but works well. The flow of the paper is intuitive, starting from an empirical observation to developing the full method.\n- The authors propose beta_0 as a scaling factor and provide a theoretical insight that under the assumption of high cosine similarity between activations, we can select a beta_0 such that the error for eq (3) is less than the regular low rank approximation, which seems reasonable to me.\n- The paper includes complexity analysis"}, "weaknesses": {"value": "- The communication overhead from the residual is high in multi gpu training, although the authors argue that the computation time outweighs that. However, it is unclear if the communication volume would affect real-life large scale distributed training."}, "questions": {"value": "- In the experiment leading to the observation, what is the value of beta_0 and how did you choose it? Do results hold with varying selections of beta_0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zZRFW33UtY", "forum": "VVruwk9404", "replyto": "VVruwk9404", "signatures": ["ICLR.cc/2026/Conference/Submission2550/Reviewer_kvx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2550/Reviewer_kvx9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073372400, "cdate": 1762073372400, "tmdate": 1762916278238, "mdate": 1762916278238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}