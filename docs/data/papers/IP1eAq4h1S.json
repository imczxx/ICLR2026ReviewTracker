{"id": "IP1eAq4h1S", "number": 15132, "cdate": 1758248061636, "mdate": 1763106829570, "content": {"title": "Mind the Gap: Transferring Labels Across Datasets with Divergent Annotation Protocols", "abstract": "Combining multiple object detection datasets offers a path to improved model generalisation but is hindered by inconsistencies in class semantics and bounding box annotations.cSome methods to address this assume shared label taxonomies and address only spatial inconsistencies; others require manual relabelling, or produce a unified label space, which may be unsuitable when a fixed target label space is required. We propose Label-Aligned Transfer (LAT), a label transfer framework that systematically projects annotations from diverse source datasets into the label space of a target dataset. LAT begins by training dataset-specific detectors to generate pseudo-labels, which are then combined with ground-truth annotations via a Privileged Proposal Generator (PPG) that replaces the region proposal network in two-stage detectors. To further refine region features and address pseudo-label noise, a Semantic Feature Fusion (SFF) module injects class-aware context and features from overlapping proposals using a confidence-weighted attention mechanism. This pipeline preserves dataset-specific annotation granularity while enabling many-to-one label space transfer across heterogeneous datasets, resulting in a semantically and spatially aligned representation suitable for training a downstream detector. LAT thus jointly addresses both class-level misalignments and bounding box inconsistencies without relying on shared label spaces or manual re-annotation. Across multiple benchmarks, LAT demonstrates consistent improvements in detection performance, achieving gains of up to +8.4 AP over baseline methods.", "tldr": "", "keywords": ["label transfer", "dataset alignment", "object detection"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b4185acc1d6c7c3a21c7835bee59f624becccc3e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses annotation transfer for object detection in the autonomous driving domain, a task that involves adapting annotations from source datasets to a target one. The authors propose PPG, which is a RPN trained on both ground-truth and pseudo-labels. Their framework also incorporates a module named SFF, which leverages the attention mechanism to fuse features between ROI. The authors report that their experiments demonstrate the effectiveness of proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The task of transferring annotations from existing large datasets to specific real-world scenarios is interesting and also meaningful in a practical sense"}, "weaknesses": {"value": "**Title**\n- The paper title sets a broader expectation than the paper's specific focus. It is titled “transferring labels across datasets with divergent annotation protocols”, suggesting a general solution, however, the work itself focuses specifically on object detection for autonomous driving. A more specific title would be more appropriate.\n\n**Presentation**\n\nThe paper's writing and presentation require significant improvement, as the current version is difficult to follow. \n\n- One primary concern is the mathematical formalization and notation, which is often unclear and non-standard.\nFor example, several notational choices hinder readability:\n  - The formalization of the problem as $L_{−N} \\rightarrow L_N$ is not clear. The notation $L_{−N}$, presumably used to represent a set excluding elements related to N, is non-standard. Its meaning must be inferred from the context, which is not ideal. An explicit definition of this notation is crucial for clarity.\n  - The notation $PL^n_{-n}$ appears to represent a single, simple object. However, based on the context, it seems to be a structured set associated with multiple pseudo-labels from different label spaces. This complexity should be clarified, and the notation should be revised to better reflect its structure.\n  - $I_n$ and $GT_n$ should be explicitly denoted as sets, as they represent sets of images and ground truths for dataset $n$, respectively.\n- Line 201: The paper states that an upstream model can generate pseudo-labels in the label spaces of other datasets. This is unclear to me considering a model optimized on a specific label space ($L_N$) should only be able to predict labels within that same space. \n- Line 257-260: The text here describes the PPG outputs class labels, which is confusing, as PPG is introduced as a region proposal module. Standard RPNs are class-agnostic and only generate objectness scores and bounding box coordinates, not class-specific labels. While I understand the authors may be describing how class labels for the final head are formulated, attributing this function directly to an RPN-like module is misleading. \n- Line 296: The dimension M of the attention matrix A is not defined , which is important for reader to understand the inputs and mechanics of the specific attention module.\n\n**Experiments**\n- The experiment section lacks detailed explanations of the setup. For example, regarding Table 1, it is unclear what training data was used when evaluating on nuImages. The authors must provide these details for each experiment to clarify what is being measured and to ensure the results are interpretable.\n- As shown in Table 1, the mean-teacher based semi-supervised approach surprisingly lowered performance, despite using the same quantity of annotations and a much larger pool of unlabeled data. However, the effectiveness of semi-supervised methods have been well proven. For instance, SSDA3D [1] demonstrated that leveraging Waymo annotations in a semi-supervised manner significantly improves a 3D detector's performance on nuScenes when compared to using only the nuScenes training set.\n\n**Contributions**\n- Mismatch between claimed contribution and methodology: The proposed PPG appears to be a combination of pseudo-labeling and model ensembling to train the region proposal component. The model aggregates pseudo-labels from various detectors trained on different datasets. However, there seems to be a gap between this approach and the paper's main claimed contribution: transferring annotations across datasets with divergent protocols. As illustrated in Figure 1, these divergences include key challenges such as: (1) The same entity being assigned different category labels across datasets (e.g., \"pedestrian\" vs. \"person\"); (2) Different bounding box granularities for the same entity (e.g., annotating a \"cyclist\" and \"bicycle\" separately versus as a single \"cyclist\" entity). The paper lacks a clear discussion or evidence demonstrating how the proposed approach specifically resolves these two fundamental annotation conflicts. The authors need to explicitly present their method focusing on these challenges and show how it harmonizes or transfers such divergent annotations.\n- Most importantly, **the differences between Domain Adaptation and the proposed label transferring task are not clear, as both tasks aim to use annotations from other datasets or domains. The paper offers no discussion or comparison.**\n\n**References**\n\n[1] Wang, Yan, et al. \"Ssda3d: Semi-supervised domain adaptation for 3d object detection from point cloud.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 3. 2023."}, "questions": {"value": "- For the first row of Figure 1, I suggest the authors add category annotations near the bounding boxes to make the figure more self-contained."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dKgOyLdTkS", "forum": "IP1eAq4h1S", "replyto": "IP1eAq4h1S", "signatures": ["ICLR.cc/2026/Conference/Submission15132/Reviewer_aLf5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15132/Reviewer_aLf5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760683325037, "cdate": 1760683325037, "tmdate": 1762925448962, "mdate": 1762925448962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "48Ttlmrr7l", "forum": "IP1eAq4h1S", "replyto": "IP1eAq4h1S", "signatures": ["ICLR.cc/2026/Conference/Submission15132/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15132/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763106828796, "cdate": 1763106828796, "tmdate": 1763106828796, "mdate": 1763106828796, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of label space inconsistency across different object detection datasets. Unlike prior work, the proposed approach tackles both class-level and box-level inconsistencies. The method consists of two main components: a privileged proposal generator that leverages ground truth labels within each dataset, and a semantic feature fusion module that refines proposals in a class-aware manner."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Addresses an important and underexplored problem in data-centric research.\n- Effectively integrates class information into the transfer model, leading to large improvements on downstream tasks.\n- Demonstrates strong generalization across different detection models (Faster R-CNN, RT-DETR, and YOLO, as shown in the appendix).\n- Provides thorough ablations on downstream training strategies (Table 3) and class-aware attention (Table 4).\n- Presents compelling qualitative results."}, "weaknesses": {"value": "- No discussion or evaluation on synthetic object detection datasets—what is the reason for their exclusion?\n- Missing comparison with Liao et al. (2024) or at least with the experimental setup/scenario presented in that work."}, "questions": {"value": "- In Line 258 (“To maintain label … identical names”), does this mean that even if two datasets share the same class name, they are treated as distinct? If so, could this unintentionally encourage the model to differentiate datasets based on domain rather than class semantics (e.g., when the class “bicycle” is consistent across datasets A and B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7KONQEIoVi", "forum": "IP1eAq4h1S", "replyto": "IP1eAq4h1S", "signatures": ["ICLR.cc/2026/Conference/Submission15132/Reviewer_tCdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15132/Reviewer_tCdV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715980075, "cdate": 1761715980075, "tmdate": 1762925448635, "mdate": 1762925448635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle multi-dataset object detection without requiring shared label taxonomies and consistent spatial annotations. To this end, the authors introduce Label-Aligned Transfer (LAT), a data-centric framework comprising a privileged proposal generator and a semantic feature fusion module. The key idea is to project the label space of source datasets to target dataset, thereby generating target-aligned pseudo annotations that facilitate downstream detector training. Experiments on multiple object detection benchmarks show that LAT achieves state-of-the-art results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles a practical and challenging problem in multi-dataset object detection, where source and target datasets do not share the same label space and annotation styles.\n* Label-aligned transfer is proposed to tackle label and annotation inconsistency between source and target datasets, which enables joint training on heterogeneous datasets.\n* Evaluations on two label transfer settings demonstrate that the proposed approach achieves promising results."}, "weaknesses": {"value": "* Unjustified claim regarding label taxonomy. The introduction mentions that the proposed method does not require shared taxonomies between source and target datasets. However, the benchmarks used in the paper still exhibit class overlap (e.g., common categories such as pedestrian and vehicle). Intuitively, this claim should be justified by using source and target datasets with non-overlapped object classes.\n* The reported results in Table 1 are not entirely convincing. First, the authors mention that only 3,000 images are sampled for each dataset. Given that the nuImages dataset contains over 60,000 training images, the use of 3,000 samples may lead to biased results. Second, the comparison with Plain-DET is unfair. This paper adopts DINOv2 as backbone, whereas Plain-DET uses ResNet-50.\n* Missing comparisons with a pseudo label baseline under RT-DETR setting. It is important to validate that the proposed method outperforms naïve pseudo label baseline.\n* High computational cost. The proposed method adopts a multi-stage training pipeline that scales quadratically with the number of datasets. Specifically, for N datasets, the first stage trains N individual detectors, and the second stage requires each model to generate pseudo-labels on the remaining N−1 datasets, resulting in $O(N^2)$ inference passes. The approach may become prohibitively expensive as N grows, limiting its scalability in large-scale multi-dataset scenarios.\n* It would be better to evaluate the method under a strict multi-dataset setting, where  the total training iteration remains the same when the number of datasets increases. (similar to Table 1 in Plain-DET)."}, "questions": {"value": "* Does the pseudo label baseline in Tables 2 and 3 incorporate filtering strategy? Table 8 shows that pseudo label with filtering significantly outperforms naïve pseudo label approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YId9Mb8gPi", "forum": "IP1eAq4h1S", "replyto": "IP1eAq4h1S", "signatures": ["ICLR.cc/2026/Conference/Submission15132/Reviewer_GUUF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15132/Reviewer_GUUF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823162611, "cdate": 1761823162611, "tmdate": 1762925448318, "mdate": 1762925448318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes Label Aligned Transfer (LAT), a method to translate object detection annotations with varying protocols to the label space of a target dataset. The method first fuses pseudo-labels with ground truth annotations via a \"privileged proposal generator”, and then refines region features with a class-aware semantic feature fusion module. The proposed approach is shown to significantly improve performance on several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "– Studies a challenging problem with real-world utility\n\n– The paper is clear and well-organized, and the novelty over prior work is outlined clearly\n\n– The experimental results validate the effectiveness of the proposed method\n\n– The method ablations clearly justify the role of each component in improving performance\n\n– The qualitative results included in the appendix are compelling"}, "weaknesses": {"value": "– LAT seems to match but not exceed the performance of LGPL – since code is unavailable, it would still be good to describe in more detail the distinctions between the two methods and relative pros/cons of each\n\n– It would be good to also benchmark a frontier model (eg. SAM-2), to confirm that the label transfer task still has practical utility\n\n– The paper would be strengthened by a more fine-grained performance analysis – which classes benefit/degrade the most from this type of transfer, and why (size, frequency, location, something else?)?"}, "questions": {"value": "Please address the weaknesses listed above, especially around comparison to LGPL and missing comparisons and analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zBj4bEB2oG", "forum": "IP1eAq4h1S", "replyto": "IP1eAq4h1S", "signatures": ["ICLR.cc/2026/Conference/Submission15132/Reviewer_TBvP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15132/Reviewer_TBvP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937646629, "cdate": 1761937646629, "tmdate": 1762925447917, "mdate": 1762925447917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}