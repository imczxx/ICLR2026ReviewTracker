{"id": "g53Mu7ix8b", "number": 5762, "cdate": 1757932654024, "mdate": 1763645634683, "content": {"title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "abstract": "Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed signals into components (base patterns) and their concentrations (intensity), playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified number of components, usually unknown in real data. Once data or component number increases, the scalability of these MCR approaches face significant challenges. This study reformulates MCR as a data generative process (gMCR), and introduces an Energy-Based solver, EB-gMCR, that automatically discovers the smallest component set and their concentrations for reconstructing the mixed signals faithfully. On synthetic benchmarks with up to 256 components, EB-gMCR attains high reconstruction fidelity and recovers the component number within 5% at 20dB noise and near-exact at 30dB. On two public spectral datasets, it identifies the correct component number and improves component separation over MF-based MCR approaches (non-negative variants [NMFs], ICA, MCR-ALS). EB-gMCR is a general solver for fixed-pattern signal unmixing (components remain invariant across mixtures). Domain priors (non-negativity, nonlinear mixing) enter as plug-in modules, enabling adaptation to new instruments or domains without altering the core selection learning step.", "tldr": "A new solver for signal unmixing that scales to large numbers of components and large datasets.", "keywords": ["Signal unmixing", "Multivariate curve resolution", "Generative modeling", "Energy-based model", "Dictionary learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/566bf6483467a65b18ee608873608c12f46c8279.pdf", "supplementary_material": "/attachment/0fdad2f45d9cd38c921f6bcdf400823dfda73277.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes EB-gMCR (Energy-Based Generative Modelling for Signal Unmixing and Multivariate Curve Resolution). This framework reformulates classical multivariate curve resolution (MCR) as a generative process rather than a matrix factorisation problem. The authors introduce an energy-based adaptive gating module (EB-select) that automatically selects a minimal subset of source components while maintaining reconstruction fidelity. This approach aims to overcome limitations of conventional MCR methods, such as the need to predefine the number of components, lack of scalability, and difficulty in incorporating domain constraints.\nExperiments on synthetic benchmarks (up to 256 components) and two real spectral datasets demonstrate strong reconstruction accuracy and correct component-count recovery compared with NMF, sparse-NMF, ICA, and MCR-ALS baselines. The authors also provide detailed convergence proofs, ablations, and reproducibility statements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Recasts MCR as a generative problem, conceptually unifying signal unmixing and energy-based learning.\n\n* Introduces a differentiable hard-selection mechanism (EB-select) enabling data-driven component pruning.\n\n* Provides comprehensive theoretical derivations and convergence analysis.\n\n* Demonstrates strong empirical results on spectral datasets and includes full reproducibility materials.\n\n* A meaningful step forward for scalable unmixing and spectral decomposition within chemistry and materials analysis."}, "weaknesses": {"value": "*  The claimed novelty is modest; the paper applies energy-based modelling (EBM) and sparse gating techniques, both well-studied concepts. For example, the recent review â€œHitchhikerâ€™s guide on Energy-Based Modelsâ€ (Carbone, 2024) surveys EBMs and situates them among VAEs, flows and GANs, showing energy-based methods are mature and widely applied. Meanwhile, sparse gating or mixture-of-experts style selection mechanisms are well established (see e.g., Mixture-of-Experts architecture surveys).  As a result, the contribution here is primarily domain adaptation (spectral unmixing) rather than a fundamentally new generative modelling algorithm or gating mechanism.\n\n* Although the paper claims plug-in adaptation across domains, validation is confined to chemical spectra. There are no results for real hyperspectral imaging datasets or other modalities.\n\n* The method claims to handle thousands of components, yet experiments only reach 256 latent sources.\n\n* Metrics focus on ð‘…2 and component count recovery; no generative metrics (likelihood, uncertainty, sample diversity) or comparisons to VAE/score-based baselines are reported.\n\n* Despite claims of â€œhands-freeâ€ component discovery, Î» and temperature schedules require manual tuning.\n\n* Dense notation, minimal intuition for EB-select and gating dynamics."}, "questions": {"value": "* How sensitive is EB-gMCR to the choice of the Î» coefficients and temperature decay schedule? Can the authors provide quantitative ablation on these hyperparameters?\n\n* Can the proposed framework be extended to nonlinear or multimodal unmixing tasks (e.g., image or audio mixtures)?\n\n* How does EB-gMCR perform under correlated component spectra or severe noise (>30 dB)?\n\n* Could the authors clarify how EB-select differs fundamentally from previous sparse or Concrete-gated EBMs beyond its application domain?\n\n* Would evaluating on benchmark generative datasets (e.g., MNIST mixtures, speech sources) help demonstrate generality"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oZt9RIEkhE", "forum": "g53Mu7ix8b", "replyto": "g53Mu7ix8b", "signatures": ["ICLR.cc/2026/Conference/Submission5762/Reviewer_TaiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5762/Reviewer_TaiB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728046038, "cdate": 1761728046038, "tmdate": 1762918244738, "mdate": 1762918244738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(Author Comment) Foundational Clarifications on gMCR Framework."}, "comment": {"value": "Thank you to all reviewers for your thoughtful feedback. Before addressing individual concerns, we clarify the core problem to bridge potential understanding gaps.\n\nWe provide a toy example to illustrate what MCR is. In short, MCR decomposes observed superposition signals into underlying components and their concentrations. Consider this simple example:\n\n**Given observations:**\n$$\n\\\\begin{aligned}\n\\\\text{Sample 1:} \\\\quad &[1, 1, 0, 0] = 1 \\\\times C_1 \\\\\\\\\n\\\\text{Sample 2:} \\\\quad &[0, 0, 1, 2] = 1 \\\\times C_2 \\\\\\\\\n\\\\text{Sample 3:} \\\\quad &[1, 1, 1, 2] = 1 \\\\times C_1 + 1 \\\\times C_2\n\\\\end{aligned}\n$$\n\n**The decomposition is:** $C_1 = [1,1,0,0]$, $C_2 = [0,0,1,2]$ (not normalized for visual clarity) with concentrations $[[1,0], [0,1], [1,1]]$.\n\n**Why is this \"generative\"?** Each observation is generated by: mixture $= \\\\sum (\\\\text{concentration} \\\\times \\\\text{component})$. Real-world signals with fixed patterns can be combined in more complex non-linear ways, so we use an aggregation function $\\\\Phi$ instead of simply assuming linear addition. This compositional structure is the natural description of how superposition signals form. We model this generation process to perform decomposition (conditional inference), not to synthesize new mixtures.\n\n**Traditional MCR's main weakness is lack of scalability.** This manifests in two ways: scaling to large datasets, and decomposing new samples efficiently. Traditional methods initialize fixed-size matrices: concentration $C \\\\in \\\\mathbb{R}^{M \\\\times N}$ and components $S \\\\in \\\\mathbb{R}^{N \\\\times d}$, where $N$, $M$, $d$ are predefined component number, collected sample size, and data dimension. As the toy example shows, we can easily collect more data from the same signal source and expand the dataset:\n\n$$\n\\\\begin{aligned}\n\\\\text{Sample 1:} \\\\quad &[1, 1, 0, 0] = 1 \\\\times C_1 \\\\\\\\\n\\\\text{Sample 2:} \\\\quad &[0, 0, 1, 2] = 1 \\\\times C_2 \\\\\\\\\n\\\\text{Sample 3:} \\\\quad &[1, 1, 1, 2] = 1 \\\\times C_1 + 1 \\\\times C_2 \\\\\\\\\n\\\\text{Sample 4:} \\\\quad &[2, 2, 0, 0] = 2 \\\\times C_1 \\\\\\\\\n\\\\text{Sample 5:} \\\\quad &[0, 0, 2, 4] = 2 \\\\times C_2\n\\\\end{aligned}\n$$\n\nIntuitively, $C_1$ and $C_2$ remain the main components, but traditional MCR must recompute everything when any new sample is added. This toy example uses clean signals, but recomputation amplifies noise effects, causing numerical instability.\n\n**For scaling to large component numbers or dataset sizes:** Specifying $N$ before running the algorithm means we must search over candidate $N$ values and rerun the algorithm for each one. This becomes prohibitively expensive when $N$ and $M$ are large. Moreover, we found that traditional MCR methods cannot maintain effective decomposability even when given the correct $N$. They fail to preserve reconstruction quality when $N$ exceeds several hundred. While we tested up to $N=256$, EB-gMCR maintains better decomposability as $N$ increases (Fig. 2f and 2h: near-ground-truth component counts at N=256).\n\n**Regarding the convergence proof:** We aim to justify why and how the complex loss function (Eq. 13) can reach an endpoint through end-to-end training. The proof does not claim to prove why this framework and loss function work for signal unmixingâ€”the framework is heuristic and the design comes from physical intuition. We hope this clarification helps reviewers better understand the problem we address and the roadmap this paper provides for solving signal unmixing challenges.\n\nWe address specific reviewer questions in individual responses below."}}, "id": "Nrv4e6j1lg", "forum": "g53Mu7ix8b", "replyto": "g53Mu7ix8b", "signatures": ["ICLR.cc/2026/Conference/Submission5762/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5762/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5762/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763645861124, "cdate": 1763645861124, "tmdate": 1763645861124, "mdate": 1763645861124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generative model for signal unmixing that includes a generic aggregation function, concentration generator, and component patterns, all of which are implemented by deep learning networks. The model is trained via minimization of an energy function that accounts for correlation of the signal components via a kernel embedding, component usage, and selection energy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The generic nature of the components of the mixing model makes the approach amenable to a variety of applications."}, "weaknesses": {"value": "The approach uses a combination of heuristics (sparsity prior, component activations, $\\ell_1$ norm penalty on component weights) but does not provide much motivation for the choices. Furthermore they are sourced from existing approaches.\n\nThere are multiple instances of notation not being defined (as detailed in \"Questions\").\n\nA comparison of computation time with unmixing baselines that do not require training is missing."}, "questions": {"value": "What is $\\omega$ in eqs. (2-3)? It does not appear to be considered anywhere else.\n\nIn lines 201 and later, what is $X_o$/$X_0$?\n\nIn line 226, is $|{\\mathbf E}\\|_2^2$ supposed to be $\\|{\\mathbf E}(\\omega)\\|_2^2$?\n\nWhat is $E_{f_e}$ in eq. (6)? Is it different than ${\\mathbf E}(\\omega)$?\n\nWhat is the \"coefficient of determination $R^2$\"? Why is it used as a metric in Section 5? What is the \"EC\" metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MJk69LNeNT", "forum": "g53Mu7ix8b", "replyto": "g53Mu7ix8b", "signatures": ["ICLR.cc/2026/Conference/Submission5762/Reviewer_NBQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5762/Reviewer_NBQX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873791273, "cdate": 1761873791273, "tmdate": 1762918244513, "mdate": 1762918244513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EB-gMCR, an approach that reformulates the classical MCR problem as a data generative process (gMCR). The main innovation is an energy-based solver that automatically discovers the minimal component set needed for signal reconstruction without requiring a pre-specified number of components. The method starts with an oversized pool of candidate components and uses a differentiable \"EB-select\" gating network to retain only necessary components while estimating their concentrations. The authors validate their approach on synthetic benchmarks with up to 256 components and two real spectral datasets (carbohydrate Raman and NIR bioethanol)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Overall, the writing is quite clear and logically structured (except for the mathematical formalism; see below).\n* The reformulation of MCR as a generative process (gMCR) is innovative. Traditional MCR is typically framed as matrix factorization requiring a user-specified number of components, while this work provides a principled way to learn both the component set and their mixing patterns simultaneously.\n* The ability to handle pools of 1000+ candidates is impressive and addresses real-world needs.\n* The convergence analysis (Theorem B.6 and supporting lemmas) provides theoretical grounding for the two-phase learning dynamics, though the presentation could be clearer.\n* The plug-in of domain constraints (non-negativity, nonlinear mixing) without requiring solver redesign is a  practical advantage over existing methods."}, "weaknesses": {"value": "* Only two real datasets are tested, both relatively simple (N=3 and N=2 components). The method's performance on more complex real-world mixtures isn't shown.\n* The method introduces several hyperparameters (Î» weights, temperature Ï„, RÂ² bands for checkpointing) whose selection process and sensitivity are not thoroughly discussed. Sensitivity or instability w.r.t. those parameters could be neck breaking for many more complex problems.\n* The mathematical formalism is quite sloppy and therefore sometimes difficult to follow. E.g. what kind of elements are D, C, S and E? Assumptions and conclusions aren't strictly separated (see questions below)."}, "questions": {"value": "* All mathematical statements in the appendix contain proof sketches. Does that mean you have a checked proof for those statements, but you're not spelling out the proof? If so, why aren't you showing the full proof?\n* Fig. 2 and 3: Are the R2 numbers on test or train data?\n* Eq. 3: is this an assumption or a condition that is required? It seems like this does not follow from E. 2 in general.\n* Eq. 5: Is the approximation an assumption or why would this be true? The expectation is in [0, 1] in general whereas the function maps to {0,1}."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PgukQAWBHF", "forum": "g53Mu7ix8b", "replyto": "g53Mu7ix8b", "signatures": ["ICLR.cc/2026/Conference/Submission5762/Reviewer_zgrb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5762/Reviewer_zgrb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914231436, "cdate": 1761914231436, "tmdate": 1762918244219, "mdate": 1762918244219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an eneger-based method, EB-gMCR, to tackle multivariate curve resolution. \nEB-gMCR automatically discovers the minimal set of latent components needed to reconstruct mixed signals by using a hard selection mechanism (EB-select) and additional regularizations to prune an initially oversized component pool during training. The approach shines when to hundreds of candidate components and integrates chemical priors naturally. Experiments on synthetic datasets (up to 256 components) and real spectral mixtures show that EB-gMCR accurately recovers component numbers and achieves strong reconstruction fidelity, outperforming classical MCR-ALS, NMF variants, and ICA particularly in high-component regimes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed formulation is conceptually natural for the problem, and as a result the method exhibits favorable scaling compared to traditional matrix-factorizationâ€“based approaches.\n- The framework is flexible, allowing domain knowledge to be incorporated. \n- The empirical results are strong: the approach performs well in both synthetic benchmarks and real spectral datasets."}, "weaknesses": {"value": "- The exposition is generally unclear. The core components of the method are spread across Section 4, with implementation details and conceptual justification interleaved, making it difficult to follow the full pipeline end-to-end. The presentation would benefit from consolidating the algorithmic steps (e.g., perhaps through a dedicated algorithm block or overview subsection) and then discussing each module in isolation in separate sections.\n- In addition, there are several minor writing and notation issues, e.g. $\\lambda_{\\text{amb}}$ appears for the first time in Eq. 13 without a clear linkage to the ambiguity regularizer in Eq. 9, and $X_o$ is not introduced prior to line 201. A few other are listed below, but they are frequent enough that a careful pass is warranted. \n- One of the main benefits of the method is it's computation efficiency as number of components grow. However, the experiments on real datasets involve only 2-3 components. Could the authors demonstrate performance of EB-gMCR in a real, large-component setting?"}, "questions": {"value": "- In line 240, why $\\tau = 0.9999994$?\n- In line 302, what is PL? \n- In line 202, do you mean $X_o$, as opposed to $X_0$\n- Could the authors provide wall-time comparison between EB-gMCR and baselines, particularly when component count is high."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nAUP1dwbga", "forum": "g53Mu7ix8b", "replyto": "g53Mu7ix8b", "signatures": ["ICLR.cc/2026/Conference/Submission5762/Reviewer_Ljcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5762/Reviewer_Ljcq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5762/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986511633, "cdate": 1761986511633, "tmdate": 1762918243955, "mdate": 1762918243955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}