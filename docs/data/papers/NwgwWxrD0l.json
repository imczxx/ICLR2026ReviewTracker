{"id": "NwgwWxrD0l", "number": 16147, "cdate": 1758260613382, "mdate": 1759897258339, "content": {"title": "CO3: CONTRASTING CONCEPTS COMPOSE BETTER", "abstract": "We propose to improve multi-concept prompt fidelity in text-to-image diffusion\nmodels. We begin with common failure cases—prompts like “a cat and a clock”\nthat sometimes yields images where one concept is missing, faint, or colliding\nawkwardly with another. We hypothesize that this happens when the diffusion\nmodel drifts into mixed modes that over-emphasize a single concept it learned\nstrongly during training. Instead of re-training, we introduce a corrective sampling\nstrategy that steers away from regions where the joint prompt behavior overlaps\ntoo strongly with any single concept in the prompt. The goal is to steer towards\n“pure” joint modes where all concepts can coexist with balanced visual presence.\nWe further show that existing multi-concept guidance schemes can operate in unstable \nweight regimes that amplify imbalance; we characterize favorable regions\nand adapt sampling to remain within them. Our approach, CO3, is plug-and-play,\nrequires no model tuning, and complements standard classifier-free guidance. Experiments \non diverse multi-concept prompts indicate improvements in concept\ncoverage, balance and robustness, with fewer dropped or distorted concepts com-\npared to standard baselines and prior compositional methods. Results suggest that\nlightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.", "tldr": "", "keywords": ["Composition", "classifier-free guidance", "diffusion", "text2image"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f26f029581c87c7b34e43d94e02f1aa70f342414.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed the CO3, Contrasting concepts compose better, to improve muti-concept prompt fidelity in text-to-image diffusion models. Instead of re-training, the authors introduced a corrective sampling strategy, to steer away from regions where the joint prompt behavior overlaps strongly with any single concept in the prompt. Specifically, they analyze and show that composition through weighted sum of Tweedie-means in the Tweedie denoised space, offers a more general framework. Experiments on diverse multi-concept prompts demonstrate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)\tThe paper is well-written, and the structure is well-organized. \n2)\tThe proposed CO3 is plug-and-play, model-agnostic, and gradient-free. And the results shows stronger semantic alignment to prompts. \n3)\tCombing the strengths of correction-based approaches and composable diffusion seem reasonable and effective."}, "weaknesses": {"value": "1)\tSome very latest and more relevant works are not compared, such as, Magnet (We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function), TWEEDIEMIX ICLR 2025, ConceptWeaver, etc. \n2)\tIt is confusing that why using arbitrary weights in (10) (i.e., Liu et al. 200) does not lead to a valid Tweedie-mean. There seems a lack of theoretical analysis or empirical validation.\n3)\tHow to connect the equation (12) to (13). On other words, why (13) is representing the samping from the unnormalized probability distribution in Eq. (12).\n4)\tMore experiments are tested on two concept prompts. Can this model generalize to the scenario with the prompts of three or more concepts?"}, "questions": {"value": "1)\tSome very latest and more  closely relevant works are not compared, such as, Magnet (We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function), TWEEDIEMIX ICLR 2025, ConceptWeaver, etc. \n2)\tIt is confusing that why using arbitrary weights in (10) (i.e., Liu et al. 200) does not lead to a valid Tweedie-mean. There seems a lack of theoretical analysis or empirical validation.\n3)\tHow to connect the equation (12) to (13). On other words, why (13) is representing the samping from the unnormalized probability distribution in Eq. (12).\n4)\tMore experiments are tested on two concept prompts. Can this model generalize to the scenario with the prompts of three or more concepts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "chKQkj46AW", "forum": "NwgwWxrD0l", "replyto": "NwgwWxrD0l", "signatures": ["ICLR.cc/2026/Conference/Submission16147/Reviewer_agn3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16147/Reviewer_agn3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558625616, "cdate": 1761558625616, "tmdate": 1762926314939, "mdate": 1762926314939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed a method for better prompt-aligned multi-concept image generation. Specifically, existing models often neglect or mix multiple concepts in a single generation scenario. The proposed method tends to squeeze the distribution, hindering the generation process from walking through some problematic regions. The probability normalization approach is designed for this purpose."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The diagnosis of the multi-concept image generation problem is interesting—specifically, the overlap of individual concept distributions and the original multi-concept distribution's regions.\n2. The distribution correction idea is theoretically intuitive and good."}, "weaknesses": {"value": "1. A known post-hoc guidance is called in a new name, “correction guidance”, for a better sound of the paper. It sounds unnatural.\n2. Only Subjective metrics were used. What about objective ones, such as CLIP-T, DSG (Davidsonian Scene Graph), or simple classification metrics?\n3. The proposed method is based on a modern and relatively stronger model, SDXL, while it is often compared with older methods with weaker diffusion models (2023 methods, such as Attend and Excite or Divide-Bind with SD 1.5 or 2.0). Also, many other modern methods are neglected in the paper ( Not a fair comparison.\n4. In the ablation study, the corrector’s contribution is somehow questioned: a) What is the performance of SDXL+Corrector (w/o Re-sampler)? b) In all 6 metrics, it has gains only in 3 metrics, out of 6. The other 3 are either the same or even worse.\n5. There is no limitation section.\n6. In Table 3, it has the worst speed performance, and even the metrics are not great compared with ToMe.\n7. Limited color diversity in qualitative results in Figure 3.\n8. No qualitative Ablation study.\n9. It drops the image quality in Figure 5."}, "questions": {"value": "1. Isn’t \\epsilon have a similar formulation as \\tilda(\\epsilon) in Lemma 1 204-205 (CFG, with Lambda)? Is it a typo?\n2. What is the \"x\" in Lemma 1, a) 209? is it x_{t}?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2fhI06Lss1", "forum": "NwgwWxrD0l", "replyto": "NwgwWxrD0l", "signatures": ["ICLR.cc/2026/Conference/Submission16147/Reviewer_Md3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16147/Reviewer_Md3g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915189793, "cdate": 1761915189793, "tmdate": 1762926314554, "mdate": 1762926314554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of multi-concept compositional generation in text-to-image diffusion models. The authors hypothesize such models struggle with compositional generation because the learned joint distribution for composite prompts overlaps significantly with individual concept distributions, leading to dominance by a single concept. To address this, they propose CO3, a plug-and-play corrective sampling method that suppresses individual concepts by re-weighting Tweedie means of each concept, rather than directly re-weighting in the noise (or score) space. Experiments on SD1.5, SDXL, and Pixart-$\\Sigma$ show improved prompt alignment and concept coexistence."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CO3 requires no retraining or gradient computation and can be applied to any diffusion model. This makes it broadly applicable and practical.\n2. The paper provides an intuitive theoretical explanation showing that directly performing weighted averaging of scores does not guarantee a valid Tweedie mean or consistent classifier-free guidance formulation; by instead re-weighting in the Tweedie-denoised space, CO3 ensures mathematical validity.\n3. Novelty. While not the first to explore Tweedie-denoised space reweighting for concept composition [1], CO3 offers a clear theoretical formulation and a practical two-stage approach, early resampling followed by later correction, that together provide consistent empirical improvements and strong interpretability.\n\n[1] Kwon & Ye, TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation, ICLR 2025."}, "weaknesses": {"value": "My main concern lies in the evaluation, particularly regarding benchmark coverage and baseline selection.\n\n1. The evaluated prompts are relatively limited, focusing mainly on compositions involving animals and objects. Incorporating commonly used benchmarks such as T2I-CompBench, which include more diverse attribute and object combinations, would strengthen the empirical validation.\n2. A relevant baseline is missing [1]. In addition, comparisons with recent **LLM-guided generation methods** [2, 3, 4] would be beneficial to quantitatively assess the relative performance and effectiveness of CO3 in enhancing compositional fidelity. If the proposed method is orthogonal to LLM-guided approaches, it would be particularly valuable to demonstrate how CO3 complements them when combined.\n3. Including comparisons with recent **state-of-the-art text-to-image models**, such as **Stable Diffusion 3.5** or **Flux**, would improve the paper’s relevance and help readers better understand the practical significance of the proposed method.\n\n[1] Yu et al., Improving Compositional Generation with Diffusion Models Using Lift Scores, ICML 2025.\n\n[2] Yang et al., Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms, ICML 2024.\n\n[3] Lian et al., LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models, TMLR 2024.\n\n[4] Hu et al., ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment, arXiv preprint."}, "questions": {"value": "Please refer to weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lEmc9CTdtl", "forum": "NwgwWxrD0l", "replyto": "NwgwWxrD0l", "signatures": ["ICLR.cc/2026/Conference/Submission16147/Reviewer_3fep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16147/Reviewer_3fep"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917463232, "cdate": 1761917463232, "tmdate": 1762926314141, "mdate": 1762926314141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of multi-concept prompt fidelity in text-to-image diffusion models. The authors propose CO3, a lightweight, re-training-free corrective sampling strategy that adjusts inference to maintain balanced representation among multiple concepts. By analyzing the instability of existing compositional guidance, the method identifies stable composition-weight regions and steers sampling toward them. CO3 integrates seamlessly with classifier-free guidance and demonstrates improved concept balance, coverage, and robustness across diverse prompts compared with prior compositional baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Correction‑based compositional guidance with theoretical grounding and broad evidence.\n- The method unifies ideas from correction‑based approaches and composable diffusion into a single re‑training‑free sampling framework, and provides a reasonable theoretical justification for composition‑weight choices and stability regions. The paper offers extensive quantitative and qualitative comparisons that demonstrate improvements in multi‑concept fidelity while remaining compatible with standard classifier‑free guidance."}, "weaknesses": {"value": "1. Comparison on multi‑concepts is incomplete.\n- The paper focuses on improving multi‑concept prompt fidelity, especially when one concept is rare or easily dominated by another. A direct theoretical and empirical comparison with R2F [1], ideally on RareBench, would clarify relative strengths on the rare‑concept regime where fidelity typically degrades most.\n\n2. Model coverage could be broadened beyond SDXL.\n- To strengthen the claim of generality, evaluations on additional recent open‑source text‑to‑image diffusion models, for example FLUX, would make the comparisons more persuasive and reduce the risk that gains are specific to one backbone.\n\n\nReference\n\n[1] Park et al., Rare‑to‑Frequent (R2F): Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance."}, "questions": {"value": "1. Compatibility with low‑step and high‑order solvers.\n- Main experiments use SDXL with a 50‑step DDIM sampler. Can CO3 be applied effectively with high‑order solvers such as DPM++ at around 10 steps, and with distilled text‑to‑image models that operate at 4 steps or fewer? Any guidance on tuning composition weights in these low‑step regimes would be helpful.\n\n2. Human evaluation.\n- ImageReward and BLIP‑VQA are useful proxy metrics, but has a human evaluation been conducted to assess concept presence and balance under multi‑concept prompts? If not, a small user study could substantiate the claimed improvements in perceived fidelity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6oADp6QXj8", "forum": "NwgwWxrD0l", "replyto": "NwgwWxrD0l", "signatures": ["ICLR.cc/2026/Conference/Submission16147/Reviewer_Qf6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16147/Reviewer_Qf6F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997557001, "cdate": 1761997557001, "tmdate": 1762926313679, "mdate": 1762926313679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}