{"id": "Z2SRrwSS9O", "number": 4629, "cdate": 1757729911813, "mdate": 1759898022730, "content": {"title": "FOE-RL: Flexible Online Reinforcement Learning for Efficient Inference in Large Language Models", "abstract": "Recent advancements in large reasoning models have significantly enhanced their reasoning abilities. However, recent studies have shown that these models often experience \"overthinking,\" even when handling relatively simple questions. In this paper, we propose a flexible online reinforcement learning method that estimates the difficulty of a problem in real-time and predicts an appropriate output length. Based on this, we design a length reward function and a flexible reward trend monitor, which dynamically activates or deactivates the length reward according to smoothed correctness rewards. Experimental results demonstrate the effectiveness of our approach. Compared to training methods that rely solely on correctness rewards, our approach significantly improves model accuracy while substantially reducing the average response length. On the MATH dataset, our method reduces the output token count by over 40% and increases accuracy by more than 4%. Across multiple testing benchmarks, it maintains or even enhances model performance while consistently lowering token usage. Furthermore, we observe that the method exhibits a self-regulating output length capability: depending on the model’s own capacity and question difficulty, it automatically converges toward an optimal output length range, achieving higher accuracy in the process.", "tldr": "", "keywords": ["Reinforcement Learning; Efficient Reasoning; LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce7db9e2622eae0d1cb018d6c868a8abae5f77ae.pdf", "supplementary_material": "/attachment/4aa54d9902cacfb805b869fcb32a7c3ae5b9c2bd.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the \"overthinking\" problem in large reasoning models (LRMs), where models produce unnecessarily long chains of thought, increasing latency. The authors propose FOE-RL to adaptively encourage concise reasoning. FOE-RL reduces tokens (often >40%) and sometimes improves accuracy versus a correctness-only baseline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Overthinking in LRMs is timely and well-motivated .\n2. The Gaussian length reward around $L_{\\text{expect}}$ is intuitive and easy to add to GRPO-style pipelines.\n3. The paper correctly identifies a common failure mode of length-based rewards: the model's accuracy can collapse as it \"over-optimizes\" for shortness (reward hacking).\n4. For Qwen3-1.7B on MATH-TEST, FOE-RL improves accuracy from 72.10 → 76.10 while cutting tokens 1976 → 1158; similar on MATH500 (73.40 → 77.20, 1961 → 1138) ."}, "weaknesses": {"value": "1. Defining $L_{\\text{expect}}$ from within-batch correctness with the rule  $L_{\\text{expect}}{=}p^2L_{\\min} + (1{-}p^2)L_{\\max}$ and determine $p$ based on correct rate is heuristic and could be brittle . No sensitivity analysis is provided (and also the choice of Gaussian kernel in reward term). The only ablation removes the scheduler (-OL); there is no study isolating $p$ scaling, the Gaussian vs. alternatives, or real-time difficulty estimation strategies .\n2. The monitor relies on EMA and linear-fit slopes across windows, toggling the length reward when all slopes fall below a threshold $k_d$ , but the paper does not specify window sizes, $k_d$, or robustness to noise.\n3. Some Experimental setups are questioning:\n    - Single epoch, single seeds, and no variance/confidence intervals (common point but still needs to point out here).\n    - Hyperparameters $k{=}3$, $\\alpha{=}0.01$  are fixed without tuning studies.\n    - The ACU metric is unconventional and mixes scale with architecture size, complicating cross-model interpretation; justification is brief (since I don’t see any scaling law on model size? Only math reasoning with two small models (0.6B/1.7B)).\n    - The training/eval length caps (256/3072) may confound gains by truncating prompts/outputs .\n4. no results on code, science QA or other important domain despite claims that the paradigm generalizes (If generalizes, please give more illustrations here)."}, "questions": {"value": "1. Justify the Gaussian length reward centered at $L_{\\text{expect}}$ versus simpler Huber/piecewise penalties; provide sensitivity or AUC-style comparisons and discuss gradient shapes at both “too short” and “too long” extremes.\n2. Precisely specify the EMA coefficient(s), window sizes, linear fit protocol (weighted vs. plain OLS), and the **threshold** $k_d$ used to toggle the length reward; currently these are under-specified.\n3. The variance over random seeds for Table 1/2 results may be added.\n4. If you have the computation resources, does FOE-RL transfer to non-math reasoning (code, GSM8K, scientific QA) or larger models (7B or more)?\n5. More details on efficiency testing may be needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dy5sgOvJnY", "forum": "Z2SRrwSS9O", "replyto": "Z2SRrwSS9O", "signatures": ["ICLR.cc/2026/Conference/Submission4629/Reviewer_ErGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4629/Reviewer_ErGi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634955189, "cdate": 1761634955189, "tmdate": 1762917478151, "mdate": 1762917478151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FOE-RL, a method designed to mitigate “overthinking’’ in large reasoning models by adaptively controlling their output length during training. The approach estimates task difficulty in real time and predicts an appropriate response length, introducing a Gaussian-based length reward combined with the correctness reward. A reward trend monitor dynamically enables or disables the length reward based on smoothed reward trends to prevent over-shortening in later training stages. Experiments on several mathematical reasoning benchmarks show that FOE-RL reduces token usage while improving or maintaining accuracy, demonstrating faster convergence and adaptive optimization of reasoning length."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n\n2. The experimental results, despite some evaluation concerns, still show certain improvements in efficiency and accuracy."}, "weaknesses": {"value": "1. The proposed method is largely heuristic. Both the mapping between expected length and difficulty (Eq. 10) and the Gaussian-shaped length reward (Eq. 11) are ad-hoc choices without theoretical/empirical justification, and the reward trend monitor is similarly heuristic. These designs also introduce many hyperparameters, reducing the method’s general applicability.\n\n2. The theoretical formulation in Section 3.1 aims to minimize the reasoning length L under an accuracy constraint, yet Section 3.2 shifts to aligning L with $L_{expect}$ instead of directly minimizing it. This disconnect weakens the consistency and clarity of the overall framework.\n\n3. The experimental setup is limited: using only 0.6B and 1.7B models makes it difficult to draw strong conclusions. This limitation is partially understandable given computational constraints, but evaluating on AMC23, AIME24, and AIME25 with a single sample per question still weakens the credibility of the conclusions, especially for such small models.\n\n4. The analysis focuses mainly on accuracy and token count. The paper lacks deeper quantitative or qualitative evidence showing that the proposed mechanism genuinely improves token budget allocation or reasoning efficiency beyond shorter outputs."}, "questions": {"value": "1. A fundamental question: while it makes sense that smaller p (lower accuracy) should correspond to larger L, the reverse may not always hold—when a question is simple but inherently verbose, L may remain long even if p is high. How does the proposed mapping account for such cases?\n\n2. Why must Equation (10) take a specific quadratic and concave form? Could other functional forms (e.g., linear or convex) work equally well? Is there any theoretical or empirical justification for this particular choice?\n\n3. In line 474, the paper mentions that the proposed method “could not support long-term training effectively.” Could the authors clarify what this means? If the method becomes unstable or ineffective under extended training, how can it be considered a practical or scalable solution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e7q8wsD9al", "forum": "Z2SRrwSS9O", "replyto": "Z2SRrwSS9O", "signatures": ["ICLR.cc/2026/Conference/Submission4629/Reviewer_SW4q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4629/Reviewer_SW4q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864272431, "cdate": 1761864272431, "tmdate": 1762917477825, "mdate": 1762917477825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach to optimize reinforcement learning for LLMs by dynamically estimating the ideal response length using real-time reward signals. This method effectively reduces response length and enables more efficient and effective inference on reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is novel.\n* The presentation and organization of the paper are clear and easy to follow."}, "weaknesses": {"value": "The main weakness of this paper is that the limited experiments reduces the reliability of its conclusions. The proposed method introduces several hyperparameters, yet it is evaluated on only one training dataset, which affects its generalizability. Moreover, the performance of GRPO often fluctuates across different random seeds. Considering that the authors trained for only one epoch, the absence of error bars further limits the reliability of the experimental results."}, "questions": {"value": "* Is temperature = 1 a commonly used setting in GRPO or related works? \n* For models at the 1.5B scale, GRPO typically requires more than five epochs to converge. Could the authors provide results from additional training epochs to verify its convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2ooFUexfsT", "forum": "Z2SRrwSS9O", "replyto": "Z2SRrwSS9O", "signatures": ["ICLR.cc/2026/Conference/Submission4629/Reviewer_4FTW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4629/Reviewer_4FTW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018032844, "cdate": 1762018032844, "tmdate": 1762917477517, "mdate": 1762917477517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of overthinking when training large language models with rl on mathematical reasoning tasks. It proposes two techniques: a length-based reward and a reward trend monitor to mitigate this issue."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The focus on concise training is meaningful and relevant for improving efficiency in reinforcement learning–based fine-tuning of language models."}, "weaknesses": {"value": "Using a length reward to penalize long responses is not a new idea. Moreover, the description of the reward trend monitor is unclear, making it difficult to understand how it functions in practice. The contribution and novelty appear limited, and the empirical experiments are not sufficiently strong to establish the paper’s effectiveness."}, "questions": {"value": "1. DAPO (DAPO: An Open-Source LLM Reinforcement Learning System at Scale) also uses an overlength penalty to discourage overly long responses. What are the specific advantages of your design compared to theirs?\n2. Could you clarify Section 3.3? Some definitions are unclear — for example, what are $RC_n$ and $S_t$? Also, why does the length reward become zero after setting $k_d$?\n3. The experiments are too simple. In Table 2, the FOE method shows a much larger accuracy improvement over NL (26.67 vs. 13.33) on aime 24. This result seems inconsistent; I would expect similar accuracy but shorter responses for FOE, rather than such a large performance gap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qwQiOOEneX", "forum": "Z2SRrwSS9O", "replyto": "Z2SRrwSS9O", "signatures": ["ICLR.cc/2026/Conference/Submission4629/Reviewer_fSfS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4629/Reviewer_fSfS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167619783, "cdate": 1762167619783, "tmdate": 1762917476928, "mdate": 1762917476928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}