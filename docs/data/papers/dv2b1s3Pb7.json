{"id": "dv2b1s3Pb7", "number": 6797, "cdate": 1757996160697, "mdate": 1759897893040, "content": {"title": "Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits", "abstract": "Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.", "tldr": "", "keywords": ["Autonomous Driving", "Large Language Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35ef5253ccfb19939573dde80c26c464260513c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes LLM-RCO, a large-language-model-guided control override system for autonomous vehicles operating with partial perception failures. Instead of relying on conventional “fail-safe” policies that fully stop the vehicle, LLM-RCO leverages multimodal LLM reasoning to infer possible hazards and generate cautious, short-term motion plans. The framework consists of four modules—hazard inference, short-term motion planning, action-condition verification, and safety-constraint generation—and is fine-tuned on a new dataset (DriveLM-Deficit). Experiments in CARLA show that LLM-RCO can produce more proactive driving behaviors than purely risk-avoidant baselines when parts of the environment are occluded or missing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem is clearly important: handling partial perception failure is a major open challenge for real-world autonomous driving systems. The idea of using LLMs for commonsense reasoning to mimic human-like fallback behavior under uncertainty is novel at a conceptual level and well-motivated. The authors also make a useful contribution by releasing a domain-specific dataset (DriveLM-Deficit) and performing systematic simulation studies across multiple types of perception loss."}, "weaknesses": {"value": "- [Lack of formal safety guarantee] Although the paper consistently emphasizes “safety” and “risk-averse control,” the proposed system provides no formal safety assurance—only heuristic checks (e.g., YOLO-based proximity ratios and rule-based thresholds). In safety-critical domains such as autonomous driving, existing frameworks can guarantee constraint satisfaction and safe motion through verifiable methods such as control-barrier-function (CBF) safety filters [1], Hamilton-Jacobi based runtime-assurance/Simplex architectures [2], and reachability-based tracking and shielding [3]. These approaches maintain provable safety envelopes while allowing the vehicle to continue moving, whereas LLM-RCO offers no theoretical bound on risk or constraint violation. Thus, despite its empirical promise, the method cannot be considered safe in the formal sense.\n\n\n- [Limited novelty and incomplete related-work coverage] Technique-wise, the approach is mainly a domain-specific fine-tuning of a vision-language model combined with rule-based gating, which does not introduce new learning formulations or theoretical insights. Moreover, the paper ignores major prior lines of research on verified policy switching and degraded-mode control, such as the Simplex and runtime-assurance frameworks [2][4] and fail-safe motion planning with online verification [1][3]. These works already address the same goal—maintaining safe, limited operation under perception or planner uncertainty—using provable mechanisms rather than heuristic LLM reasoning. The omission weakens the novelty claim and gives an incomplete view of the state of the art.\n\n\n[1] Magdici, Silvia, and Matthias Althoff. \"Fail-safe motion planning of autonomous vehicles.\" 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2016.\n\n[2] Wabersich, Kim P., et al. \"Data-driven safety filters: Hamilton-jacobi reachability, control barrier functions, and predictive methods for uncertain systems.\" IEEE Control Systems Magazine 43.5 (2023): 137-177.\n\n[3] Herbert, Sylvia L., et al. \"FaSTrack: A modular framework for fast and guaranteed safe motion planning.\" 2017 IEEE 56th Annual Conference on Decision and Control (CDC). IEEE, 2017.\n\n[4] Mehmood, Usama, et al. \"The black-box simplex architecture for runtime assurance of autonomous CPS.\" NASA formal methods symposium. Cham: Springer International Publishing, 2022."}, "questions": {"value": "Please discuss the weaknesses above, i.e., how the safety guarantees and how to position the paper in relation to a long line of Simplex/runtime-assurance frameworks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uxJzOXUzHh", "forum": "dv2b1s3Pb7", "replyto": "dv2b1s3Pb7", "signatures": ["ICLR.cc/2026/Conference/Submission6797/Reviewer_LKNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6797/Reviewer_LKNa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566654862, "cdate": 1761566654862, "tmdate": 1762919068092, "mdate": 1762919068092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLM-RCO (LLM-Guided Resilient Control Override), a risk-averse control framework for autonomous driving under partial perception deficits. The central idea is to use the commonsense reasoning capabilities of large language models (LLMs) to guide safe, proactive driving decisions when perception sensors (e.g., cameras) fail to detect critical objects.\n\nThe architecture includes four modules:\n\n> Hazard Inference, predicting potential unseen hazards from past frames;\n\n> Short-Term Motion Planner, generating conditional action sequences (“move” vs “stop-observe-move”);\n\n> Action Condition Verifier, ensuring consistency and real-time safety;\n\n> Safety Constraint Generator, setting adaptive control bounds based on traffic, weather, and daylight.\n\nThe authors introduce DriveLM-Deficit, a dataset of 53,895 clips simulating perception deficits of safety-critical objects, derived from DriveLM-GVQA. They fine-tune Qwen2-VL-2B-Instruct using LoRA on object, motion, and planning tasks, and evaluate the system using CARLA with TransFuser and InterFuser agents. Results show improved driving scores and reduced infraction rates across deficit scenarios, with detailed ablation and sensitivity analyses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Addresses an important and realistic challenge: decision-making under partial perception loss.\n\n+ Proposes a well-structured and interpretable control architecture (LLM-RCO) that bridges reasoning and motion planning.\n\n+ Introduces DriveLM-Deficit, a valuable dataset for studying perception-deficit resilience.\n\n+ Solid experimental validation across multiple agents and deficit types, with clear ablation and sensitivity analyses.\n\n+ The “risk-averse but proactive” design philosophy is conceptually appealing and practically relevant."}, "weaknesses": {"value": "- Limited learning novelty: The contribution lies in integration, not in representation or algorithmic innovation.\n\n- Venue fit: The work aligns better with CoRL or ICRA, where system robustness and simulation performance are primary evaluation metrics.\n\n- No real-world validation: Results are confined to CARLA; the framework’s performance on real sensor data or unseen environments is unknown.\n\n- No analysis of learned representations: The paper does not examine what the fine-tuned LLM actually learns about occlusion, hazard inference, or uncertainty.\n\n- Incomplete treatment of latency and scalability: The claimed reduction in inference frequency is not quantified.\n\n- Safety guarantees: The LLM-generated safety constraints lack formal validation or calibration analysis."}, "questions": {"value": "> How sensitive is the system to prompt formulations and the choice of LLM backbone (e.g., GPT-4 vs Qwen2-VL)?\n\n> How does the Action Condition Verifier handle rapidly changing deficits (e.g., intermittent occlusions)?\n\n> Could the authors clarify whether LLM confidence or uncertainty estimates play a role in replanning frequency or safety margins?\n\n> What would be required to extend this method to multi-modal perception loss (camera + LiDAR)?\n\n> Have the authors considered learning-based uncertainty modeling (e.g., conformal prediction or Bayesian calibration) as a complement to rule-based verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J96lTeuTq7", "forum": "dv2b1s3Pb7", "replyto": "dv2b1s3Pb7", "signatures": ["ICLR.cc/2026/Conference/Submission6797/Reviewer_dJ99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6797/Reviewer_dJ99"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933315458, "cdate": 1761933315458, "tmdate": 1762919067359, "mdate": 1762919067359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LLM-RCO, a modular system that uses multimodal LLMs to (1) infer hazards in occluded/deficit image regions, (2) produce short-term action–condition plans (move vs stop-observe-move), (3) verify conditions with rule checks, and (4) generate “loose” safety constraints. The authors release/construct a DriveLM-Deficit dataset (53,895 short clips) for fine-tuning Qwen2-VL and evaluate closed-loop in CARLA with Transfuser and Interfuser agents, reporting improvements on CARLA metrics under synthetic perception masking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem motivation: perception deficits matter and default stops are suboptimal. \n2. System modularization (hazard inference, planner, verifier, safety generator) is sensible and well described."}, "weaknesses": {"value": "1. Dataset realism. DriveLM-Deficit is built from DriveLM-GVQA while occlusions are produced by mask/occlude operations driven by YOLO tracking, it's unclear whether the deficits represent real sensor failures (noise, glare, partial occlusion) or adversarial attacks. No transfer or real sensor experiments are provided.\n\n2. Action mapping and control realism. The high-level action tokens map deterministically to throttle/brake parameters (Appendix Table 4). This discretization may be brittle and hides control instability introduced by wrong LLM plans (e.g., sudden deceleration to zero mapped to brake=0.8). No comparison to lower-level continuous controllers or MPC is shown.\n\n3. Important experimental details are missing: how exactly masks are applied, how often LLM planning is called, timing of replanning, and hyperparameter settings for thresholds and LoRA ranks are scattered in appendix snippets rather than clearly summarized.\n\n4. Potential data leakage and weak supervision. DriveLM-Deficit derives “move vs stop” labels from final-frame braking annotations and uses privileged simulator information (DriveLM-GVQA privileged labels). This trains the LLM on simulator privileged signals instead of realistic human commonsense; it risks overfitting to simulator artifacts. \n\n5. Metric manipulation. The authors modify Infraction Score (IS) by excluding red-light and stop-sign violations in the deficit scenarios (they explicitly say they excluded those violations), which directly affects the Driving Score (DS = RC × IS) and makes comparisons unfair. Also the CARLA \"game time\" excludes model inference time while \"system time\" includes it in a second setting. This inconsistent accounting hides the practical latency cost of LLMs. \n\n6. Heuristic thresholds and unverified safety logic. Several critical thresholds are arbitrary such as immediate-hazard ratio = 0.05, deficit spatial shift threshold. There is no formal safety verification which is unacceptable for safety claims."}, "questions": {"value": "1. Labeling: Exactly how are move/stop labels created? Do they use privileged ground-truth object positions or only per-frame sensor images?\n2. Metrics: Why were red-light/stop-sign infractions excluded from IS in deficit experiments? Provide results with the standard CARLA IS and with latency fully accounted for in all runs. \n4. Statistical strength: Provide results over ≥10 randomized runs per scenario with confidence intervals and statistical tests.\n5. Safety verification: How can you guarantee that LLM-generated safety constraints won’t increase risk in edge cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BtU71l7J0S", "forum": "dv2b1s3Pb7", "replyto": "dv2b1s3Pb7", "signatures": ["ICLR.cc/2026/Conference/Submission6797/Reviewer_ESrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6797/Reviewer_ESrJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995680643, "cdate": 1761995680643, "tmdate": 1762919066967, "mdate": 1762919066967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a risk-averse framework leveraging LLM to integrate commonsense into autonomous systems with perception deficits. It includes a few key components: hazard inference, short-term NP, action condition verifier for re-planning, and safety constraints generation. The authors implemented a comprehensive experiment sets to validate the framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow, and overall well-written\n2. The risk-averse framework is novel to me, and it really provides the hint to solve real-world long-tail headache problems of autonomous driving. \n3. it addresses partial perception deficits by LLM — a real but under-studied AV safety problem\n4. The experiments look solid and comprehensive.\n5. The authors contribute to a new dataset, which could be valuable."}, "weaknesses": {"value": "1. It is not clear to me that when this risk-averse framework will be triggered. AV perception systems have two failure modes, FN and FP, which can be told by humans, but not by the perception models. Please elaborate more on how and whether the trigger happens in what form with FN and FP. \n2. It is unclear the runtime latency with LLM in the AV software loop for perception and planning. \n3. The experiment is on CARLA, not on real-world vehicles. \n4. Rule-Heavy Safety ComponentsAlthough pitched as LLM-based, the system still relies on: rule thresholds (e.g., 5% image area for hazard) and heuristics for action check. \n5.If LLM outputs unsafe plan and verifier misses it, consequences unclear."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UVzR6sDJsF", "forum": "dv2b1s3Pb7", "replyto": "dv2b1s3Pb7", "signatures": ["ICLR.cc/2026/Conference/Submission6797/Reviewer_D8mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6797/Reviewer_D8mg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032061804, "cdate": 1762032061804, "tmdate": 1762919066651, "mdate": 1762919066651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}