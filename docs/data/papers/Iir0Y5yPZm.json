{"id": "Iir0Y5yPZm", "number": 19553, "cdate": 1758297235909, "mdate": 1759897032997, "content": {"title": "Uncertainty Quantification for Regression: A Unified Framework based on Kernel Scores", "abstract": "Regression tasks, notably in safety-critical domains, require proper uncertainty quantification, yet the literature remains largely classification-focused. In this light, we introduce a family of measures for total, aleatoric, and epistemic uncertainty based on proper scoring rules, with a particular emphasis on kernel scores. The framework unifies several well-known measures and provides a principled recipe for designing new ones whose behavior, such as tail sensitivity, robustness, and out-of-distribution responsiveness, is governed by the choice of kernel. We prove explicit correspondences between kernel-score characteristics and downstream behavior, yielding concrete design guidelines for task-specific measures. Extensive experiments demonstrate that these measures are effective in downstream tasks and reveal clear trade-offs among instantiations, including robustness and out-of-distribution detection performance.", "tldr": "We propose a new framework for uncertainty quantification in regression, based on proper scoring rules and kernel scores", "keywords": ["Uncertainty quantification", "supervised regression", "scoring rules"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8e071303f041581762879f8530163bc87a022da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for quantifying uncertainty in regression models using kernel scoring rules. It aims to provide a principled way to separate total, aleatoric, and epistemic uncertainty by generalising existing measures such as variance and entropy through kernel-based formulations. The authors introduce two estimators: one based on Bayesian model averaging and another on pairwise comparisons, and show how different kernel choices affect properties like robustness and sensitivity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper are:\n- The paper elegantly connects different uncertainty measures (variance-, entropy-, and energy-based) under the general framework of kernel scoring rules, offering a consistent mathematical view of total, aleatoric, and epistemic uncertainty.\n- The decomposition into total, aleatoric, and epistemic components is clearly formalised.\n- I like the justification that uncertainty metrics should align with proper scoring rules!"}, "weaknesses": {"value": "The weaknesses of this paper are:\n- I am left confused, in definition 4.1 the authors state that the framework assumes a non-negative kernel. But then in section 5, the Gaussian kernel is negative. Why does this clash exist in the paper? Have I missed something?\n- A key weakness of the paper is the narrow scope of its empirical evaluation. All comparisons are made between different scoring rules (S_SE, S_ES, S_log, and Energy Score) applied to the same underlying ensemble models, meaning the study tests only uncertainty measures rather than fundamentally different uncertainty estimation methods. The absence of broader baselines is poor for this venue.\n- Similarly, only evaluating upon two benchmarks is poor for this venue."}, "questions": {"value": "- In Definition 4.1, the framework assumes a non-negative kernel, yet Section 5 defines the Gaussian kernel as negative. Could the authors clarify this apparent contradiction? Is the Gaussian example consistent with the theoretical assumptions on conditional negative definiteness and properness of the score?\n- The empirical section compares only different scoring rules applied to the same ensemble predictor. Why were no other uncertainty estimation methods?\n- How well does the proposed framework generalise to other domains such as high-dimensional vision, non-stationary time series, or real-world safety-critical regression problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xEnNxdwHBV", "forum": "Iir0Y5yPZm", "replyto": "Iir0Y5yPZm", "signatures": ["ICLR.cc/2026/Conference/Submission19553/Reviewer_RG8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19553/Reviewer_RG8j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760870853121, "cdate": 1760870853121, "tmdate": 1762931437170, "mdate": 1762931437170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical framework for uncertainty estimation in regression based on proper scoring rules induced by kernels. The authors show a connection between the convex order of the respective distributions, kernel properties, and certain properties of the underlying uncertainty measures. Overall, the work provides clear explanations and a rigorous framework for constructing principled uncertainty estimates. It offers an original perspective on uncertainty, as prior works have explored related ideas of constructing scores from proper scoring rules but have not addressed the regression setting. Such a framework is indeed needed and could have significant practical applications. The authors conduct extensive experiments to validate their proposed approach and explore different application scenarios, covering several interesting use cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work presents a solid theoretical framework and addresses an interesting problem that could have intriguing implications for downstream applications. It also removes previous limitations related to uncertainty scoring in classification tasks.\n2. The mathematical soundness of the paper is strong. The propositions, results, and connections to prior work are solid.\n3. The authors did a good job conducting extensive experiments to evaluate the proposed functions and covering various use cases in their experimental \n4. Overall, the text is very clear and well written."}, "weaknesses": {"value": "Overall, the paper represents solid work; however, I have a few minor remarks.\n1. There is a missing space on line 147 in “However,since.”\n2. The begging of Part 5 is interestingly formulated. Personally, the notation on lines 232–239 introduced some confusion for me. Why not first define the convex order between any two measures and then incorporate it directly into the proposition? For example, explicitly state in Proposition 5.1 that $Q_1 \\leq_{cvx}^2 Q_2$ and that $P_1 \\leq_{cvx} P_2$. Or maybe make it explicit that $P_1, P_2, Q_1, Q_2$ from introduction of the notations are re-used in the propositions later.\n3. I find it somewhat peculiar that you first claim the heuristic for selecting the bandwidth is sufficient and yields good empirical results for the Gaussian kernel, yet later perform an experiment to tune the bandwidth to better align with the active learning objective."}, "questions": {"value": "1. You mention in your work that a similar approach based on Bregman divergences has been used to construct scoring rules for classification tasks. It appears that using kernels restricts the class of measures that can be produced (you note that the log-score is not a kernel score). What, then, is the advantage of using kernels compared to Bregman divergences? Could the approach based on Bregman divergences be adapted to the same use case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WCcmiBcFbO", "forum": "Iir0Y5yPZm", "replyto": "Iir0Y5yPZm", "signatures": ["ICLR.cc/2026/Conference/Submission19553/Reviewer_NoVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19553/Reviewer_NoVr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760103230, "cdate": 1761760103230, "tmdate": 1762931436757, "mdate": 1762931436757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a unified framework for quantifying total, aleatoric, and epistemic uncertainty in regression tasks. The framework is built upon proper scoring rules, with a specific focus on kernel scores. By selecting different kernels, the framework can instantiate various uncertainty measures (e.g., based on squared error, energy score, or Gaussian kernels) with controllable properties such as robustness and translation invariance. The paper provides a theoretical analysis of these measures and their properties, and validates them on several downstream tasks, including out-of-distribution detection and active learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   The paper tackles the important and less-explored problem of principled uncertainty quantification for regression\n*   The proposed framework, based on kernel scores, is elegant\n*   The theoretical connection between kernel properties (e.g., boundedness, translation invariance) and the behavior of the resulting uncertainty measures (e.g., robustness via influence functions, ordering properties) is a valuable contribution \n*   The experiments are comprehensive and well-designed, covering qualitative out-of-distribution (OOD) assessment, a quantitative robustness analysis, and an interesting study on  active learning"}, "weaknesses": {"value": "My main concerns are regarding the positioning of the work with respect to recent literature and some aspects of the experimental evaluation.\n\n*   **Related Work:** The authors seem to have missed the highly related work of Gruber & Buettner (ICML 2024), who introduce a bias-variance-covariance decomposition of kernel scores to assess generative models. While the application domain is different (generative models vs. regression), the core idea of using kernel scores to derive uncertainty measures is very similar. Gruber & Buettner derive a \"kernel entropy\" and a \"distributional variance\" from their decomposition, which are conceptually analogous to the aleatoric (AU) and epistemic (EU) uncertainty measures proposed here. Specifically, this paper's AU is the expectation of the kernel entropy H_k over the posterior, and its EU is the expected pairwise MMD, which is a measure of spread related to Gruber & Buettner's distributional variance. A thorough discussion and comparison to this work is crucially missing to properly situate the paper's contribution. How do the authors see their framework (based on the entropy/divergence decomposition of scoring rules) relating to the bias-variance decomposition of the kernel score itself?\n\n*   **Experimental Evaluation:**\n    *   Regarding the robustness analysis in Section 6.2, the experimental setup seems somewhat indirect. An outlier is introduced by corrupting the training target of one ensemble member . This conflates the training dynamics with the intrinsic robustness of the uncertainty measure at test time. A more direct evaluation would be to introduce an outlier prediction into a trained ensemble at test time (e.g., a Gaussian with a very large variance). Furthermore, the analysis is limited to aleatoric uncertainty (AU). It would be important to also analyze the robustness of epistemic uncertainty (EU), which is arguably more sensitive to model disagreement.\n    *   In the task adaptation experiment (Section 6.3, Figure 4), the paper shows that larger gamma values for the Gaussian kernel score lead to better active learning performance . This is an interesting result, but it lacks interpretation. A larger gamma makes the kernel less sensitive to small distances. Why is this beneficial for this specific active learning task? Is there an optimal gamma, or does performance saturate? A more in-depth discussion on the link between the kernel hyperparameter and the downstream task performance would strengthen this section.\n    *   The qualitative assessment in Figure 2 is illustrative, but the claims about the superiority of kernel-based measures for OOD detection are subjective. The color scales for the different measures are not standardized, making a fair visual comparison difficult. Could the authors provide a quantitative comparison, for instance, by computing the correlation between EU and the land-sea mask?\n\n*   **Minor Points:**\n    *   The paper states that the pairwise estimator (P) is used for the experiments because it \"admits closed-form solutions\" . However, it also comes at a higher O(M^2) cost. A brief justification for why this trade-off is acceptable or necessary for the conducted experiments would be helpful.\n    *   The choice of the Gaussian kernel bandwidth gamma is presented inconsistently. In the introduction to Section 6, the median heuristic is mentioned as working well, but in Section 6.3, gamma is presented as a key tunable parameter. Could the authors clarify the recommended practice? Was the median heuristic used for the experiments in 6.1 and 6.2?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "00lQ6OKyDW", "forum": "Iir0Y5yPZm", "replyto": "Iir0Y5yPZm", "signatures": ["ICLR.cc/2026/Conference/Submission19553/Reviewer_9q6x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19553/Reviewer_9q6x"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947132015, "cdate": 1761947132015, "tmdate": 1762931436338, "mdate": 1762931436338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "There is a large amount of literature on how to obtain second-order distributions that represent estimated epistemic, aleatoric, and total uncertainty, which is still an active field of research (A). The submission follows a different path (B): Instead of improving second-order distributions, it studies uncertainty measures that summarize the estimated second-order distribution into three scalar values EU(x), AU(x), and TU(x). The submission argues that this summarization step (B) obtained much more attention for classification than for regression. Therefore, this submission focuses on studying this step (B) for regression. There are multiple different uncertainty measures available for this step (B), and the submission unifies these uncertainty measures, studies them theoretically and experimentally, and suggests ways to design new approaches for this step (B).\n\nFigure 1 nicely shows how these different uncertainty measures can be different in a synthetic example.\n\nThe abstract and the conclusion are framed as if studying the difference of these complexity measures from step (B) has a significant impact on downstream performance, and as if the paper provides useful guidelines for task-specific uncertainty measures for practitioners. However, from my perspective, all the experiments on real-world data rather convey the opposite message: For the real-world experiments in this submission, there is no substantial difference in downstream performance across the different complexity measures in step (B). Therefore, the choice of uncertainty measure seems rather irrelevant for downstream performance in practice.\n\nThis mismatch between claims and experiments leads me to a score of 2-3.\n\nIf you more honestly reformulated the paper, that no major effect of the choice among these uncertainty measures on downstream performance was found, I would raise it to 3-4.\n\nIf you significantly extend the number of datasets to show reliably among multiple different datasets that the choice among these uncertainty measures can reliably be ignored, I would raise it to 3-5, depending on the quality/rigor/quantity of the experiments. This could encourage the community to keep focusing on (A) rather than on (B).\n\nIf you show that the negligible effect of the choice among these uncertainty measures on downstream performance for the considered datasets was rather an exception, but for many other datasets/down-stream tasks the choice among these complexity measures is really important for downstream performance, then I would raise my score to 3-7 (depending on how big the difference is, how extensive and convincing the experiments are [quality, rigor, number of experiments, number of datasets, number of downstream tasks, ablation studies], and depending on how practical the guidelines are how to pick the right uncertainty measure in practice). But to be honest, I think this is outside the scope of this revision, but rather for a future resubmission. It would be a very interesting result if you manage to show that (B) is (almost) as important as (A), while the majority of the literature focuses on (A). However, the current experiments do not hint towards this direction.\n\n(These scores are based on the assumption that the mathematical mistake will be fixed during the revision.)\n\n**Language-wise clarification:**\n\nThe submission clearly distinguishes between two different tasks:\n\n* (A) Obtaining a representation of uncertainty via second-order distributions, and\n* (B) summarizing a second-order distribution into three scalar values EU(x), AU(x), and TU(x).\n\nThis distinction is explained well, very reasonable and very clear. However, I don’t think that the terms used to describe them are in accordance with the literature:\n\nThe submission refers to (A) as “uncertainty representation”, and (B) as “uncertainty quantification”. I think a big part of the literature refers to “uncertainty representation” as a general umbrella term that also includes (A). For example, Wikipedia says: “Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real-world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.” I think a second-order distribution is also a quantitative characterization of uncertainty, and second-order distributions try to determine how likely certain outcomes are if some aspects of the system are not exactly known.\n\nFor me, an uncertainty representation is a language to express or communicate uncertainty. One representation of uncertainty is a second-order distribution; another representation of uncertainty is the 3 numbers EU(x), AU(x), and TU(x), and there are other representations of uncertainty, e.g., via sets instead of distributions. To my understanding, (B) is about translating/summarizing one representation of uncertainty (concretely, second-order distributions conditioned on x) into another representation of uncertainty (concretely, 3 numbers EU(x), AU(x), and TU(x)).\n\nSo in some sense one could argue also that the naming should be done exactly the other way around: (A) to which the paper refers as “uncertainty representation” is also about “uncertainty quantification” and (B) to which the paper refers to as “uncertainty quantification” is about changing the “uncertainty representation”.\n\nWithin the submission, (A) is clearly and consistently referred to as “uncertainty representation”, and (B) as “uncertainty quantification”. However, it would be nice to use different (maybe new) terms that are not that overloaded yet. Or at least write explicitly that you are following a specific subculture of the literature that uses the terms in this way, while explicitly acknowledging that there are other subcultures in the literature that use these terms differently."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Aleatoric, Epistemic, and total uncertainty are important topics for many applications, which require further research.\n\nI like the idea of a simple post-processing step for an already trained model to potentially improve the downstream performance. Figure 1 nicely visualizes why changing the uncertainty measure could theoretically be such a step with theoretically high potential for impact. I totally see that it was very well motivated to start this project to find out if this can actually have a significant impact on downstream tasks. If clear evidence was found during this project that this choice is actually impactful, this would have been a much stronger paper. However, I think there is also value in publishing “negative” results about theoretically promising ideas that have not (yet) turned out to be particularly useful for practice.\n\nFigure 1 provides good intuition about the mathematical difference between these uncertainty measures.\n\nThe paper provides a nice overview of different uncertainty measures by explaining how multiple different uncertainty measures can be seen as special cases of a general formalism. (If I understand it correctly, none of these formalisms are new, however?) (I am not familiar enough with the literature to judge how complete this overview is.)\n\nThe theoretical results on certain properties of complexity measures seem plausible. However, I did not check the proofs, and I think the novelty of these results is overstated due to a mathematical mistake in the argument. \n\nThe main idea of the experimental setup of Section 6.3 is reasonable.\n\nThe potential to extend this beyond regression to other output modalities by using appropriate kernels seems interesting. However, no experiments were conducted in this direction."}, "weaknesses": {"value": "**1 Major weakness: The mismatch between the overall storyline and the actual experimental results on real-world data.** The overall storyline is formulated as if (B) is very important for downstream performance (while the majority of the literature only focuses on (A)). However, I don’t see any big impact of (B) on any downstream task in this submission. Let’s go together through all the experiments:\n\n1a Experiment of Section 6.1 QUALITATIVE ASSESSMENT OF UNCERTAINTY QUANTIFICATION\n\nI think you are interpreting too much into Figure 2: When I look at the four different plots of AU for different choices of step (B), I see 4 qualitatively extremely similar plots. Each of these plots shows more aleatoric uncertainty in high-altitude regions and lower aleatoric uncertainty in flat regions. IF you compare Figure 2 to Figure 6 in the appendix, then the impact of step A (the difference between Figure 2 and 6 and the difference for different lamdas) seems much larger than the impact of step (B) in Figure 2. In Figure 6 in the appendix, step (B) seems to make a difference too, but it seems to be in favor of S_log, which is kind of the default in the literature and cannot be expressed via Kernel Scores, so it does not really motivate why one should study Kernel Scores.\n\nAlso, for the EU, all 4 uncertainty measures in Figure 2 behave qualitatively similarly in the sense that they all assign higher epistemic uncertainty to the water than to the land. The scales are differently distorted. Maybe taking the log of the log for the color scale of the EU for S_log would make the picture look more similar to the others.\n\nFurthermore, I criticize that while a pretty plot for 1 dataset is very nice, it is not sufficient to make general claims. The aleatoric uncertainty in Figure 2 nicely aligns with your intuition that in more mountainous areas, the aleatoric uncertainty should be higher. However, without quantitatively measurable performance metrics for relevant downstream tasks, this qualitative assessment of the plot is not enough, especially when the qualitative assessment is so tight as here. I don’t really see which of these uncertainty measures is better or worse in Figure 2.\n\n1b Experiment of Section 6.2 ROBUSTNESS ANALYSIS\n\nThis experiment seems quite artificial. In practice, there is no reason to add artificial noise to the training data of one ensemble member. For practically used ensemble members, there is usually some model selection step where poorly performing ensemble members are sorted out. This would already eliminate the distorted ensemble. I do like the experiment, as it shows a clear result that is exactly as theoretically expected. But is this a common scenario in real-world applications that one of the ensemble members is outlying in this specific way? When people talk about robustness, they often think about outliers in the training data, which can occur in practice, but how often do you have an outlying ensemble member? Real-world examples of this would be a valuable addition.\n\nThis experiment also misses any downstream performance. Do I want the outlying model ensemble to have an influence on the estimated aleatoric uncertainty, or not, to improve downstream performance, if the outlier was not artificially added but might actually carry some important information? How do I decide for which downstream tasks I want this type of robustness for and which I don’t? What kind of downstream tasks are affected by this? How are they affected by this? How much? I like this experiment, but I don’t think that it is sufficient to claim that step (B) is particularly important for downstream tasks.\n\n1c Experiment of Section 6.3 TASK ADAPTATION OF MEASURES\n\nThis is the only section with experiments on downstream tasks. However, it only studies 1 dataset, and for this dataset, step (B) seems to be quite irrelevant for downstream performance. There are two subexperiments, one in Figure 3 and one in Figure 4.\n\nFigure 3:\n\nLine 401-402 correctly describes “relatively minor variation between individual measures” as the different uncertainty measures from step (B) perform visually indistinguishably. At least for these specific downstream tasks, I would say that the choices made in step (B) are basically irrelevant. Figure 3 shows a much stronger difference for the choice of loss functions during training (i.e., step (A)).\nIn addition to Figure 3, some quantitative performance metrics would be helpful. E.g., in the spirit of abstention (http://arxiv.org/abs/2404.10960), plot on the x-axis the percentage of rejected datapoints and on the y-axis the average metric over the non-rejected datapoints: For example, for x=20%, you plot the average score of the 80% least uncertain datapoints. Then lower curves correspond to better uncertainty measures than the higher curves. This would be less noisy than the current Figure 3.\n\nFigure 4:\n\nFigure 4 shows a well-defined downstream task with a clearly defined performance metric. When plotted on a typical scale (i.e., the left plot), the different uncertainty measures from step (B) are visually indistinguishable. Only on the close-up one can see that, depending on the step (B), the CRP varies between 0.74 and 0.76 after 8000 training instances. This does not seem to be a big difference. It is absolutely not clear if this difference is statistically significant. (You repeated each gamma for 3 runs. Can you get statistical significance from this?) To get this difference, you need to vary gamma over quite a wide range. Do people actually use such extreme values of gamma? To me, the results look rather inconsistent: For less than 3500 training instances, gamma>1 seems to perform best; for more than 4000 training samples, gamma>1 seems to perform best. I think the statement: “The results clearly demonstrate systematic task adaption: larger values of γ consistently yield lower CRPS (highlighted by the color gradient), indicating better model performance.” strongly overstates the result. The submission only evaluated this one single dataset, and there the differences are tiny and not consistent over the number of training instances. I would summarize the same results as “The results **don’t** demonstrate clear systematic task adaptation. There might be a tendency that larger values of gamma yield lower CRPS in the case of more than 4000 training samples for this specific dataset. However, this tendency is not visible for fewer than 4000 training samples.” What should a practitioner learn from this experiment? Larger values of gamma are always better? Or gamma=1.6 is typically a good choice. Or one should always do a very expensive hyperparameter optimization (HPO) to find out which gamma works best (however, if one does the HPO based on the first 3000 samples, the resulting gamma is not particularly good for the performance after 8000 samples)? Or the choice of gamma does not really matter for downstream performance; instead, one should rather spend more time optimizing step (A). I don’t think that any of these claims is well substantiated based on this one experiment on one dataset. In the discussion, the submission claims “Our analysis highlights how specific properties of kernel scores directly translate into distinct characteristics of the induced uncertainty measures, offering practical guidance for their selection and adjustment” [Lines 468-471]. What exactly is the practical guidance? I don’t think that there are any conclusive results on this in this submission.\n\nI would recommend you also compare with the other uncertainty measures for the experiment in Figure 4. Maybe this comparison could reveal a larger difference.\n\n**2 Mathematical Mistakes and Overstated Novelty**\n\nIn lines 251-252, the submission claims: “However, our notion is more general, as every mean-preserving spread implies a convex order, but not vice versa.” I think the second part of this statement is wrong. I think it does also hold vice versa. Convex order is equivalent to mean-preserving spread:  https://en.wikipedia.org/wiki/Mean-preserving_spread#Relation_to_expected_utility_theory. If you think that I am wrong, please explain and/or provide a counterexample of any 2 random variables whose distributions are in convex order but cannot be expressed as a mean-preserving spread. If you want, I can also give you a more detailed mathematical proof of why I think they are equivalent.\n\nThe submission claims that one of the main novelties of the submission’s theory over the existing theory by Wimmer et al. (2023), Sale et al. (2023a) is that the submission generalizes the results from mean preserving spread to convex order [Lines 251-252 and 257]. However, I don’t agree that the submission's results are more general, since convex order is equivalent to mean-preserving spread. Therefore, Propositions 5.1 and 5.2 are not novel but rather an equivalent reformulation of the results by Wimmer et al. (2023); Sale et al. (2023a). Please correct me if you disagree.\n\n**3. Minor Weakness:**\n\nThe usage of the terms \"uncertainty representation\" and \"Uncertainty quantification” to distinguish between (A) and (B) is not aligned with the literature. These terms are introduced as if this specific usage of these terms was widely accepted across the literature, but it is not.\n\n**4. Presentation issues:**\n\nLines 41-42: “This choice of measure is crucial, as it directly influences both the decision-making process and the performance of downstream tasks.” This is strongly overstating how crucial this is. The experiments don’t show any evidence (not even any real-world example) where the choice of uncertainty measure crucially influences the performance of any relevant downstream task.\n\nLine 133: S(P,P) might be more concise than the integral.\n\nLines 147-148: “However, since the BMA distribution generally differs from the true predictive distribution, this estimator can be misleading (Schweighofer et al., 2023).” More details would be interesting here. What do you mean by “true predictive distribution”? How does BMA deviate from this?\n\nLine 160-161: “From now on, we refer to the two different methods with index B and P for BMA and pairwise estimation, respectively.” But then you don’t use this notation anymore. Does Section 5 hold for both B and P or only for one of them?\n\nLine 168: “negative definite kernel”. I think negative definite kernels are less known than positive definite kernels. Maybe a quick definition would be useful here, but I leave it up to you to decide if this is necessary.\n\nLine 205: “U-statistic (Gretton et al., 2012)” might not be widely known in the ML community.\n\nLine 213-214: “The energy score (Gneiting & Raftery, 2007)” was not defined until here. You define it 2 pages later. Maybe consider defining it directly here?\n\nLine 244: It would be helpful to explicitly mention that you are still assuming $Q_1\\leq_{cx}^2 Q_2$ here. You can simply add “for all $Q_1\\leq_{cx}^2 Q_2$” here without any additional line break.\n\nLine 252: If you actually believe that “but not vice versa” holds, then you need to explain why. I am very sure that “but not vice versa” is wrong.\n\nSection 6.2 is hard to read, mainly because in Line 366, it was not clear to me what “predictions” you are talking about. I think by “predictions” you actually mean “estimates of the aleatoric uncertainty uncertainty measure AU”? I also think that the notation $\\hat{y}$ is too overloaded in Line 359. In Line 359, I would recommend writing $\\tilde{y}$ instead of $\\hat{y}$. In lines 363-367, you might want to write $\\hat{a}$ instead of  $\\hat{y}$."}, "questions": {"value": "Q1: Lines 222-224: “For instance, kernel scores allow for comparing (almost any) arbitrary distributions with an unbiased estimator, which can be important, for example, for mixture-of-expert models, where each expert issues a prediction in a different format.” I think many more details would be needed here to understand your thoughts on this. About what kind of mixture-of-experts (MoE) are you talking about? The same MoEs that are used in LLMs? To my understanding, each expert in a MoE, as they are used in LLMs, outputs a tensor of the same dimensions, which I would consider the same format. What do you mean by different formats? Where does this occur in practice and how?\n\nQ2: Figure 2: You only show the AU over the land, but not over the water? Why? If so, you need to clearly specify this in the caption or at least somewhere in the text, or as an absolute minimum, somewhere in the appendix. Or does AU happen to always have the white value over the water?\n\nQ3: In line 377, what does “task loss” refer to? Is the task loss the loss that is used as training loss and as evaluation loss? This was quite confusing when reading the submission for the first time.\n\nQ4: In Line 398, you mention “the model”. Is it actually just one individual model or an ensemble of models? If it’s an ensemble, I would write “the ensemble” instead to avoid any ambiguity.\n\nQ5: How do you suggest doing the HPO for Figure 4 in practice? In practice, the main motivation for active learning is that obtaining labels is very expensive. In Figure 4, you collect different labels for each different value of gamma. However, only after collecting these 8000 labels you see which gamma performed best. But once you have already collected the 8000 labels for each gamma, there is no point anymore in predicting which gamma would have been best for collecting 8000 labels. How would you imagine a pipeline that takes advantage of the different performance of different gammas?\n\nQ6: In the Discussion, in Lines 469-472, the submission says: “Our analysis highlights how specific properties of kernel scores directly translate into distinct characteristics of the induced uncertainty measures, offering practical guidance for their selection and adjustment.”. How practical is this guidance? Can you give a practical example where this guidance practically improves a practically relevant metric of a practically relevant downstream task?\n\nQ7: In Line 1025-1027 of Appendix C: What is the actual input to the DRNs? Is it only the mean prediction of the ECMWF integrated (ensemble) forecast system (IFS)? Or is it actually everything that is mentioned in Section C.1? In other words, is the input only the predicted temperature or all the covariates listed in Section C.1? Does ECMWF predict all the covariates listed below or only the temperature?\n\nQ8: In Appendix C.1, Deep evidential regression seems to perform very poorly compared to the Deep ensemble. Do you agree?\n\nQ9: Do you see any interesting connections to optimal transport (e.g., the Wasserstein metric between probability measures)? Can you also fit them into this framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tv17ISb7mr", "forum": "Iir0Y5yPZm", "replyto": "Iir0Y5yPZm", "signatures": ["ICLR.cc/2026/Conference/Submission19553/Reviewer_tJPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19553/Reviewer_tJPo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964317418, "cdate": 1761964317418, "tmdate": 1762931435912, "mdate": 1762931435912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}