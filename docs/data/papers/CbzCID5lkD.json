{"id": "CbzCID5lkD", "number": 10877, "cdate": 1758183928956, "mdate": 1759897622936, "content": {"title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment", "abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition.\nAs challenges remain in extracting 3D modal features and bridging the gap between different modalities,\nwe propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment.\nExtensive experiments on various datasets demonstrate the state-of-the-art performance of \\method in multiple tasks.\nOur source code will be made publicly available.", "tldr": "", "keywords": ["Multimodal", "Gaussian Splatting", "Contrastive Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5adc6473af105cf892bd1097372ed7c31ac2bd4d.pdf", "supplementary_material": "/attachment/1e5cd3d3839b7ddd36a63dcacae7b4b07240d41a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces TIGAUSSIAN, a novel framework for aligning text, image, and 3D modalities, specifically using 3D Gaussian Splatting (3DGS) representations. The core contributions are threefold: 1) a multi-branch 3DGS tokenizer that disentangles and separately encodes geometric, appearance, and morphological attributes of Gaussian primitives to create a more effective latent representation; 2) a diffusion-enhanced multi-view fusion mechanism for image-3D alignment that generates multiple views from a single image to create a more holistic, 3D-aware feature representation, mitigating single-view bias ; and 3) a 3D-text projector module that uses a query transformer to map 3D features into the text embedding space for improved alignment. The authors demonstrate through extensive experiments on datasets like Objaverse, ABO, and SUN RGBD that TIGAUSSIAN achieves state-of-the-art performance on various downstream tasks, including zero-shot classification, text-3D retrieval, and image-3D retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Writing**: This paper is clear and easy to follow.\n2. **Cross-Modal Alignment Strategies:** The paper introduces sophisticated strategies for both image-3D and text-3D alignment that address key challenges. The diffusion-enhanced multi-view fusion for image-3D alignment is a clever way to overcome the ambiguity and information loss inherent in single-view representations without requiring actual multi-view data during inference. Similarly, the 3D-text projector is a principled approach to bridge the modality gap, which is a persistent challenge in this area.\n3. **Empirical Results:** The method is thoroughly evaluated on multiple benchmarks (Objaverse, ABO, SUN RGBD) and across a variety of tasks (zero-shot classification, cross-modal retrieval, few-shot learning)."}, "weaknesses": {"value": "1. **Lack of Large-Scale Dataset Validation:** The primary weakness of this work is the scale of the datasets used for the primary training and validation. While the mentioned Objaverse, with 146k objects, is a respectable size. The current experiments, while strong, do not fully demonstrate the scalability of TIGAUSSIAN on entire Objaverse with 800k objects. Training and evaluating on a significantly larger dataset would provide more convincing evidence of the model's robustness and generalizability.\n2. **Comparisons to other methods:** It will be better to report the performance comparisons to ReCon, ReCon++ of ShapeLLM, TAMM on used datasets for completeness.\n3. **Complexity and Computational Cost:** The proposed framework introduces several new components. This adds complexity and likely significant computational overhead compared to simpler baseline models. While the performance gains are clear, a discussion on the trade-offs in terms of computational cost, training time, and memory usage would be beneficial. For instance, UniGS report the ablation study on computational cost."}, "questions": {"value": "1. Regarding the scalability: The authors mention that Objaverse contains 146k objects. Have the authors considered or attempted to pre-train TIGAUSSIAN on larger-scale dataset (full objeaverse) to test the limits of the proposed architecture? \n2. Regarding the 3DGS tokenizer: The paper mentions using FPS to downsample to 1024 Gaussians. How sensitive is the model's performance to the number of sampled Gaussians? Does the performance saturate at 1024, or could further improvements be gained with more Gaussians at the cost of computation?\n3. Regarding the multi-view fusion: The framework generates 6 views using a diffusion model. What was the rationale for choosing 6 views? Is there a performance trade-off with the number of generated views? For instance, would using 3 views significantly degrade performance, or would 12 views provide a notable boost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DewmiJRuTi", "forum": "CbzCID5lkD", "replyto": "CbzCID5lkD", "signatures": ["ICLR.cc/2026/Conference/Submission10877/Reviewer_yocs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10877/Reviewer_yocs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812246848, "cdate": 1761812246848, "tmdate": 1762922090237, "mdate": 1762922090237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to train a unified text-image-3D features by decoulping the parameters of 3D gaussian. It introduces a multi-branch 3DGS tokenizer to fuse the 3D features within various perspectives. In addition, it proposes a tri-modal alignment strategy to interact with multi-view generated images. The experiments prove that it achieves impressive improvements across retrieval and scene understand tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper achieves impressive improvement across various tasks.\n\n- The writing is easy to understand and mask sense."}, "weaknesses": {"value": "- Lack of comparisons. One of the core contribution is 3D-aware image feature fusion. However, the usage of multi-view rendering images has been proposed in the JM3D [1], which mitigates the contribution. The authors should either discuss the differences or include a comparison.\n\n- Lack of the experiments of perception. Whether JM3D or ULIP has experiments about 3D Res or Object detection to support the ability in sparse perception. The paper needs the similar experiments.\n\n- Contribution. In Tab.4, the comparison between the second and fifth lines shows that the 3D-text projector did not perform well, which impacts the validity of the author's claimed significant contribution.\n\n[1] Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation, MM 23"}, "questions": {"value": "- Why use a single image generation method to obtain multi-view images instead of directly rendering from the model?\n\n- What backbone does ULIP2 use in Tab. 1?\n\n- Appendix Table 2 shows that the pre-trained point cloud encoder has a significant impact on the results, which weakens the overall contribution of the method. What is the significance of using 3D Gaussian to compare point clouds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Tf06nHVTAm", "forum": "CbzCID5lkD", "replyto": "CbzCID5lkD", "signatures": ["ICLR.cc/2026/Conference/Submission10877/Reviewer_Xq4U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10877/Reviewer_Xq4U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911406670, "cdate": 1761911406670, "tmdate": 1762922089729, "mdate": 1762922089729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TIGAUSSIAN, a framework for text-image-3D Gaussian (3DGS) multimodal alignment, is proposed in this research. The main contribution is to solve the problems of single-view bias and attribute entanglement encoding that exist in current multimodal approaches for 3D settings. To make up for the shortcomings in single-view 3D sceneries, they have developed a diffusion-enhanced multi-view fusion module and a multi-branch 3DGS tokenizer module that decouples from the spatial, appearance, and morphological aspects of 3DGS. Finally, an engineered 3D-text projection module is used to align 3D features with text features. Tests on datasets including Objaverse, ABO, and SUN RGBD show that this approach performs noticeably better than baseline approaches like CLIP2, Uni3D, and UniGS in tasks like open-scene identification, text-3D/image-3D retrieval, and zero-shot categorization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The TIGAUSSIAN framework proposed in the article is highly targeted. It tackles the fundamental issues of attribute entanglement encoding and single-view bias in 3D multi-modal alignment tasks by creating multi-modal processing modules like the multi-branch 3DGS Tokenizer. It offers a fresh approach to text-image-3DGS multi-modal alignment, surpassing the drawbacks of current techniques.\n\n2. They rigorously validate the effectiveness of the method, and conducts verification on three major datasets, including Objaverse, by designing four core person scenarios such as zero-shot classification. The baseline approach covers popular 3D representation techniques and uses ablation tests to demonstrate the need for each module design.\n\n3. With a new design, a well-defined experimental procedure, and promises to make the source code publicly available, this approach offers a reproducible reference for future 3D multimodal related work and actively fosters the growth of related disciplines in 3D multimodal research."}, "weaknesses": {"value": "1. The article omits information about the experiment's parameter base and indicators, such as the number of multi-view generations (6) and the 3D-text projection module's parameter sensitivity, which have not been confirmed by tests. Additionally, when it is actually implemented, it does not report the pertinent information (such the inference delay on A100 GPU).\n\n2. It is unclear if baseline models (such UniGS and Duoduo-CLIP) have implemented the same preparation procedures as TIGAUSSIAN (like preprocessing in the downsampling stage) when compared to baseline approaches. The persuasiveness of the outcome is affected by the comparing conditions.\n\n3. Its specific performance in multi-objective, real-world outdoor, and other complicated 3D settings cannot be verified due to the absence of generalization testing in complex scenarios. Additionally, it is not stated how diverse the 3DGS data sources were used in the trials, which makes it impossible to completely illustrate how generalizable the strategy is in arbitrary situations."}, "questions": {"value": "1. How do the issues with the TIGAUSSIAN framework presented in this paper specifically manifest themselves in the generalization testing in complex scenes (like occluded multi-targets and real outdoor scenes) and the rationality verification of important parameters (like the number of Transformer layers in the 3D-text projection module and the number of multi-view generations)? What fixes exist for the issues described above?\n2. The experiment's Objaverse, ABO, and SUN RGBD datasets are all openly accessible. Do any self-constructed datasets exist? Will they become open-source if that is the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T3J9XxdPr7", "forum": "CbzCID5lkD", "replyto": "CbzCID5lkD", "signatures": ["ICLR.cc/2026/Conference/Submission10877/Reviewer_hpbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10877/Reviewer_hpbf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922152606, "cdate": 1761922152606, "tmdate": 1762922089353, "mdate": 1762922089353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TIGaussian, a new text-image-3D contrastive learning paradigm. Based upon the previous work UniGS, it introduces three core modules: (1) decoupled 3DGS tokenizer to avoid entangled 3D representation; (2) mulit-view image fusion to avoid degraded 3D representation; (3) a 3D-text projection module to adapt the 3D latent space for better text-3D alignment. Experiments on Objaverse and ABO show improved performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured and easy-to-follow. It focuses on improving different designs of current 3DGS-based text-image-3D contrastive method and shows clear improvement.\n\n2. The multi-vew image fusion idea makes intuitive sense to me. Forcing the alignment of a 3D object and a single-view 2D image is clearly suboptimal."}, "weaknesses": {"value": "1. The benchmark comparison is insufficient. Only three baseline methods are listed in Table 1/2. Strong competitors like ULIP [1], OpenShape [2], and MixCon3D[3] should also be included. For example, OpenShape and MixCon3D score a top-1 accuracy of 46.8 and 52.5 on Objaverse-LVIS, which is significantly better than the 41.76 top-1 accuracy of TIGaussian, making one wonder the effectiveness of the proposed approach.\n\n2. Similarly, results on other important benchmarks like ModelNet40 and ScanObjectNN should be reported, as in OpenShape.\n\n3. The image fusion idea is not new. MixCon3D has already shown the effectiveness of aggregating multi-view image features before alignment, and thus make the contribution of this work less novel.\n\n\n**References**\n\n[1] Xue, Le, et al. \"Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[2] Liu, Minghua, et al. \"Openshape: Scaling up 3d shape representation towards open-world understanding.\" Advances in neural information processing systems 36 (2023): 44860-44879.\n\n[3] Gao, Yipeng, et al. \"Sculpting holistic 3d representation in contrastive language-image-3d pre-training.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "1. Why use a pre-trained multi-view diffusion model to generate multi-view images from a single-view perspective? If the data contains multiple text-image-3D triplet of a single object, isn't it more straightforward to fuse the representations of multi-view **real** images? Seems to me the introduction of a diffusion model might also introduce hallucination and might be a potential bottleneck of this method.\n\n2. What is the core difference between 3DGS-based 3D contrastive learning approaches and point-cloud-based or mesh-based approaches? The authors are encouraged to add related discussions and show performance comparisons across different approaches in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4jcTAYOBDr", "forum": "CbzCID5lkD", "replyto": "CbzCID5lkD", "signatures": ["ICLR.cc/2026/Conference/Submission10877/Reviewer_BqVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10877/Reviewer_BqVr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762498764330, "cdate": 1762498764330, "tmdate": 1762922088807, "mdate": 1762922088807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}