{"id": "S7jGYTATSw", "number": 18946, "cdate": 1758292246502, "mdate": 1759897071397, "content": {"title": "LoRDQ: activation-aware Low-Rank Decomposition and Quantization for Large Language Model Compression", "abstract": "Large language models (LLMs) deliver high performance but remain prohibitively expensive to deploy in resource-constrained environments. Post-training quantization (PTQ) is widely used to reduce memory and compute, while it often degrades sharply in the ultra-low-bit regime. Although recent PTQ methods incorporate weight sensitivity for further improvement, the sensitivity analysis is often conducted at the element-, row-, or vector-wise level within the original weight matrix, which can limit robustness at very low bitwidths. We instead operate at the \\emph{subspace} level by deriving an activation-aware low-rank factorization of each weight matrix (for a given layer/block). The key idea is to represent each weight matrix by a small set of activation-aware components that retain most output energy, and to solely quantize these factors, enabling higher precision per stored parameter under the same budget and improving accuracy in the low-bit regime. We thus propose \\textbf{LoRDQ}, an activation-aware low-rank decomposition and quantization scheme that provides a closed-form factorization minimizing layer-output reconstruction, and incorporates two complementary techniques to mitigate the loss from quantizing low-rank factors, including a block-wise greedy decomposition and an intra-block compensation step. Simulations demonstrate that LoRDQ can achieve \\(\\sim\\!10\\times\\) lower perplexity in comparison with existing methods such as GPTQ and AWQ. Moreover, leveraging our analytical results, we provide a {theoretical explanation} for these gains by connecting them to the spectrum of the output Gram matrix \\(WXX^\\top W^\\top\\), clarifying when low-rank structure preserves critical model behavior.", "tldr": "", "keywords": ["Model compression", "post training quantization", "low rank decomposition", "weight-only quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a41a1679132e005bdc250a969350dac812476b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors formulate an activation-aware low-rank decomposition that minimizes layer output error and derive a closed-form solution for any valid square-root factorization of the Gram matrix of layer inputs, which leads to proposing LoRDQ. LoRDQ involves two complementary techniques to mitigate the loss from quantizing low-rank factors, including a block-wise greedy decomposition and an intra-block compensation step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors combines a block-wise greedy decomposition with an intra-block quantization compensation strategy to mitigate quantization-induced performance degradation under aggressive bit-width constraints.\n2. They conduct extensive experiments across multiple LLMs under 2-bit or 3-bit quantization schemes."}, "weaknesses": {"value": "1. In Table 1, the baselines (only GPTQ and AWQ) are too limited to justify the effectiveness of the proposed method under 2-bit quantization. Given that QuIP is effective for 2-bit quantization, it is necessary to compare the proposed method with QuIP.\n2. In Table 1, LoRDQ shows better performance than GPTQ and AWQ, but the ppl of LoRDQ is still too high, which seems not practical at all. \n3. Most LLMs used in Table 1-3 are old-fashioned. Considering that low-bit quantization favors under-trained LLMs [1], it is required to see whether the proposed method is also effective for recent models such as Llama 3.2, Qwen2.5, or Qwen3.\n4. Either PPL or CSR accuracy is reported. So, it is hard to determine whether LoRDQ performs well or not. It would be more beneficial if the authors also explore more challenging tasks (e.g., MMLU) and generation tasks (e.g., IFEval, GSM8K) with recent LLMs mentioned in Weakness 3.\n5. As the bit-width to preserve the performance of the original FP model is also important, can the authors provide the experimental results under 4-bit quantization? \n6. In Table 3, it is hard to understand why intra-block quantization compensation is sometimes helpful and sometimes not. Can the authors provide more detailed analysis about this?\n\n[1] Low-Bit Quantization Favors Undertrained LLMs, ACL 2025."}, "questions": {"value": "In Table 3, the performance goes up as the number of blocks increases. Then, when does the performance saturate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sVjxcvUo0Z", "forum": "S7jGYTATSw", "replyto": "S7jGYTATSw", "signatures": ["ICLR.cc/2026/Conference/Submission18946/Reviewer_6H1S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18946/Reviewer_6H1S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742775703, "cdate": 1761742775703, "tmdate": 1762931003530, "mdate": 1762931003530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose a way to compress weights for large language models by performing joint low rank decomposition and quantization. The paper is well written. But it does not fairly characterize the contributions in comparison to baselines. For example, the activation aware low rank decomposition is similar to ASVD, the residual compression is similar in to ResSVD. What if I create a new baseline which applies GPTQ/AWQ over ASVD or ResSVD? Is that application trivial? How does LoRDQ compare with such a baseline. Further, the evaluations in this paper are a little weak and does not consider complex tasks or hardware numbers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "LordQ outperforms GPTQ and AWQ baselines at 2-bit average per weight and Omni-quant, QUIP baselines at 3-bit average bits per weight."}, "weaknesses": {"value": "1. The source code is not provided.\n2. The baselines considered are relatively old. The paper lacks comparison with more recent baselines. \n3. The paper lacks larger open source model evaluations (eg. Llama-3-70B). \n4. The paper lacks recent evaluation benchmarks (eg. code completion, long context reasoning, mathematical understanding, etc.)\n5. The paper lacks hardware evaluations/speedup with the technique. \n6. The paper does not report time/computational resources required to compress LLMs using LordQ."}, "questions": {"value": "1. How beneficial is using Cholesky decomposition ($XX^T = LL^T$) ? Paper mentions that computation complexity is reduced from $O(N^3)$ to $O(N^3/3)$. How big of an issue is the computation complexity? Since it is a post training compression approach, this is a one time cost.\n2. What if I apply GPTQ/AWQ quantization over ASVD or ResSVD low rank decomposition, how would LordQ compare with such a baseline.\n3. How is the process of inter/intra block compression different from what GPTQ does for quantization. \n4. What is the impact of calibration data on the compressed model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fxZCvqomU7", "forum": "S7jGYTATSw", "replyto": "S7jGYTATSw", "signatures": ["ICLR.cc/2026/Conference/Submission18946/Reviewer_d9MU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18946/Reviewer_d9MU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761784595374, "cdate": 1761784595374, "tmdate": 1762931002779, "mdate": 1762931002779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LoRDQ, a post-training compression framework for large language models that combines activation-aware low-rank decomposition with efficient quantization. This paper proposes LoRDQ, a post-training compression framework for large language models that combines activation-aware low-rank decomposition with efficient quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper demonstrates high originality by creatively combining low-rank decomposition and quantization into a unified, activation-aware framework, a novel synthesis beyond existing isolated approaches. \nThe quality is robust, featuring rigorous theoretical proofs (e.g., optimality of the Cholesky-based solution) and extensive empirical validation across major LLMs. \nThe clarity is excellent, with a logically structured narrative that effectively explains the complex interplay between decomposition and quantization. \nIts significance is substantial, delivering state-of-the-art performance in the challenging ultra-low (2-bit) precision regime, which is critical for the practical deployment of LLMs on resource-constrained devices."}, "weaknesses": {"value": "This paper has several notable weaknesses. \n\nFirst, its performance drops significantly at 3-bit, where it is outperformed by GPTQ/AWQ. The authors attribute this to \"extra quantization noise\" from compressing two matrices, but this undermines the method's general applicability and suggests it is primarily a solution for extreme, sub-2-bit scenarios. \n\nSecond, a major omission is the lack of hardware efficiency analysis. The computational overhead of the Cholesky decomposition, block-wise SVD, and intra-block compensation is not discussed, leaving its practical deployment cost unknown. \n\nFinally, the comparison to hybrid methods like QLoRA or QuIP# is insufficient; benchmarking against these would better situate its novelty in the rapidly evolving field of LLM compression."}, "questions": {"value": "(1) The paper emphasizes the computational efficiency of the Cholesky decomposition over SVD but does not report any latency, throughput, or memory footprint measurements during inference. Could you provide a comparative analysis of the end-to-end inference speed and memory usage of a model compressed with LoRDQ versus strong baselines like GPTQ or AWQ?\n\n(2) The performance drop in the 3-bit regime is a significant limitation. You note this is due to quantizing two matrices, but this suggests LoRDQ is only optimal for ultra-low-bit scenarios. Have you considered a hybrid or adaptive strategy where the method dynamically chooses between a) applying LoRDQ or b) falling back to a standard PTQ method like GPTQ on a per-layer basis, perhaps based on the spectral concentration metric you analyzed? Demonstrating such an adaptive system would greatly enhance the method's generality and practical utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K7FMLfRkl8", "forum": "S7jGYTATSw", "replyto": "S7jGYTATSw", "signatures": ["ICLR.cc/2026/Conference/Submission18946/Reviewer_JYD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18946/Reviewer_JYD3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808243496, "cdate": 1761808243496, "tmdate": 1762931002213, "mdate": 1762931002213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LoRDQ, an activation-aware low-rank decomposition & quantization framework for post-training compression of large language models (LLMs). Unlike conventional post-training quantization (PTQ) methods that handle weights directly or assign precision heuristically, LoRDQ first derives a closed-form activation-aware low-rank decomposition based on the Cholesky factorization of the input covariance matrix, then quantizes the residuals again and again via a block-wise greedy decomposition and intra-block quantization compensation. Theoretically, the authors prove that their decomposition is equivalent to the optimal truncated SVD of the layer outputs without explicit whitening. Empirically, LoRDQ demonstrates consistent gains over GPTQ and AWQ in 2-bit quantization across LLaMA-2/3 and OPT models, achieving up to ~10× lower perplexity and higher zero-shot accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.LoRDQ shows substantial performance advantages in ultra-low (2-bit) regimes, outperforming GPTQ/AWQ by large margins in perplexity and accuracy on multiple benchmarks and models.\n2.Efficient and interpretable design: The Cholesky-based projection yields a closed-form and computationally efficient solution."}, "weaknesses": {"value": "In general, the idea using covariance matrix to do SVD is not new, for example https://proceedings.neurips.cc/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf\nThe author extend it to blockwise. However,  the study of block-wise is not clear. How K change in terms of error? \n\nI think there is an important baseline missing in the experiments, which is the pure low rank,$W ≈\\sum_{k=1}^KPQ$ where P,Q is low rank by svd, as the paper trying to sell the activation aware. \n\nOthers:\n1. The paper rigidly fixes the calibration setup to 128 samples of 2048 tokens from the C4 dataset. However, since the covariance estimation directly affects the activation-aware decomposition, the choice of calibration data could significantly impact performance. I would like to see ablations over different dataset sources, sample counts, and sequence lengths, as well as authors’ recommendations on selecting appropriate covariance datasets for practical deployment.\n\n2. Experiments focus on LLaMA and OPT families. Including more diverse architectures (e.g., Qwen-like, DeepSeek-like) or other quantization baselines (e.g., OmniQuant#) would better validate generality.\nThe rank r and bitwidth settings for P and Q appear to be selected empirically to match a 2-bit average without clear justification or adaptive strategy."}, "questions": {"value": "1. LoRDQ’s decomposition explicitly depends on the input covariance How sensitive is the method to the choice of calibration dataset and token distribution? Can you do some ablation on it?\n\n2.The paper focuses on 2- and 3-bit quantization, but the proposed method can be independently of the quantization. Better application needed to be found. \n\n3. The results show that QuIP and OmniQuant outperform LoRDQ in ARC-E and ARC-C for LLaMA-3 models. Could the authors analyze the cause of this discrepancy—e.g., differences in dataset calibration, rank allocation, or architecture compatibility?\n\n4. Does the two-factor representation (P and Q) increase matrix multiplication overhead during inference?\nIn intra-block compensation (Algorithm 1), how are the quantization errors redistributed—does this step significantly increase computational cost during compression?\n\n5. Have the authors tested LoRDQ on reasoning-heavy or multilingual benchmarks (e.g., GSM8K, BBH, XWinograd), which might prevent inference speed up? Also I want to see more general benchmark results like MMLU, etc. Not only the 4 selected benchmark.\n\nWhat’s ALDQ in Figure3.(a)\nminor:\nline 391 reach -> reaches\n757 “better in in” -> in in\nAll 2bit -> 2-bits"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aLa2FwLfOp", "forum": "S7jGYTATSw", "replyto": "S7jGYTATSw", "signatures": ["ICLR.cc/2026/Conference/Submission18946/Reviewer_jpvA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18946/Reviewer_jpvA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762747566872, "cdate": 1762747566872, "tmdate": 1762931001863, "mdate": 1762931001863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}