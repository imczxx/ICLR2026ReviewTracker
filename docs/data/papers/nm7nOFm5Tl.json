{"id": "nm7nOFm5Tl", "number": 12655, "cdate": 1758209320976, "mdate": 1759897495795, "content": {"title": "MARKED INDUCING POINT CASCADED SDES FOR NEURAL MANIFOLD LEARNING", "abstract": "The manifold hypothesis suggests that high-dimensional neural time series lie on a low-dimensional manifold shaped by simpler underlying dynamics. To uncover this structure, latent dynamical variable models such as state-space models, recur-rent neural networks, neural ordinary differential equations, and Gaussian process latent variable models are widely used. We propose MIP-CSDE (Marked Inducing Point Cascaded SDE), a novel cascaded stochastic differential equation model that balances computational efficiency with interpretability and addresses key limitations of existing approaches. Our model assumes that a sparse set of trajectory samples suffices to reconstruct the underlying smooth manifold. The manifold dynamic is modeled using a set of Brownian bridge SEDs, with points–specified in both time and value–drawn from a multivariate marked point process. These Brownian bridges define the drift of a second set of SDEs, where their trajectories are mapped to the observed data. This yields a continuous, differentiable latent process capable of modeling arbitrarily complex time series as the number of inducing points increases. For MIP-CSDE, we derive efficient training and inference procedures, demonstrating that its computational complexity of inference per iteration scales as O(P · N ), exhibiting linear dependence on the observation data length N , where P is the number of particles. We then show in both synthetic data and neural recordings that our proposed model can accurately recovers the underlying manifold structure and scales effectively with data dimensionality.", "tldr": "We introduce MIP-CSDE, a cascaded SDE model with marked inducing points that efficiently uncovers low-dimensional neural manifolds from high-dimensional time series.", "keywords": ["Neural manifold learning", "Cascaded stochastic differential equations (SDEs)", "Marked inducing points"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9635083d035bb9fd43a360c65e85f13814c60aff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Assuming that many high-dimensional time series are governed by simpler dynamics on a low-dimensional manifold, this work presents a time series model designed to extract this low-dimensional structure, advertised as a sweet spot between interpretability and expressive power. Specifically, the model uses a cascade SDE model that is fit to data using expectation maximation (or alternatively variational inference).\nSeveral theoretical properties of the model are studied, i.e., universal approximation capabilities and computational costs. \nEmpirically, the model is evaluated in 4 experiments or tasks, (i) reconstruction of a chirp signal, (ii) a Lorenz system, (iii) decoding animal spacial trajectories from brain data, and (iv) inference of a manifold representation of neural activity on the MC_Maze neural latents benchmark dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The expressivity of the presented method is studied theoretically and a universal approximation theorem is proven.\n- The method scales with O(sampled time points (N) times sampled trajectories (P)), whereas competitors can scale with O(N$^3$)\n- Empirically, the model is able to reconstruct the studied signals, slightly outperforming the baselines."}, "weaknesses": {"value": "- **Clarity.** It was quite difficult for me to understand the descriptions and arguments in the article, and I don't think I was successful.\n This includes the proofs and setting of the theorems, the model-fitting procedure and the setting and outcome of the experiments. More specifically, ...\n    - Experiments. The overall setting of the experiments is insufficiently described, and as a result I am still not sure of what the goal was. From my understanding (but I might be completely wrong) it seems that the input to the model is a (very finely) sampled time series of latent states $X_t$ and that the model output is a sample path of a an estimated stochastic process $\\hat Z_t$ that is compared to ground truth observations $z_t$. It also seems that the model is fit on a single time series (instead of a corpus of time series) and that only one trajectory pair $(x_t, z_t)$ is available for model fitting, i.e. spread information to fit the stochastic process is not directly accessible.\n    - In the proof of the Theorem, the construction of the $x_t^{d,N}$ and their relation to the marks $m_i$ is unclear. Therefore, also Eq. 8 (and arguments build upon it) is unclear.\n    - It seems that the proof of the theorem assumes that the time series can be observed arbitrarily often and at equally spaced time points, but this is not stated as an assumption.\n    - Model fitting, it is stated that model fitting *\"involves updating several sets of parameters\"*, it would be very beneficial to the reader to specify the model parameters and Likelihood function here.\n\n- The model is advertised for extracting low dimensional representations of time series, suggesting that obtaining high quality representations is the goal and strength of the model. However, the evaluation of the obtained representations is quite limited. Only on one dataset (experiment 3.5),  insights are obtained from the representations.  \nSpecifically, the authors observe that  *\"within each trial, the trajectories during the reach and preparation phases were similar, but they differed across trial\"*. However, what is meant with *\"similar\"* is not specified, so to me it remains unclear, in which way they are similar or how I can see it in the plots (Figure 4 also misses a legend entry for the blue trajectories).\n- I might have misunderstood the experiment design, but the tasks seem quite easy. Reconstructing the chirp signal is based on 500 samples and the added gaussian noise is quite small (variance 0.1). Still, the model is unable to reconstruct the signal near its peaks.   \nFor the rat-hippocampus data, we even have 30 observations per second for more than 120 seconds and use 80% of that for model fitting. If only reconstruction is the goal, then, given the amount of available data, this should be quite easy. While one might argue that constructing the rat movement only from the spiking activity of cortex cells is more difficult (if this is what is done in experiment 3.4), it should be noted that the movement data is (likely) already used during training.\n- Limited comparison with baselines. The model is mainly evaluated in a qualitative way using plots of the predicted latent states and observations. For these qualitative evaluation, no baselines are compared with. The only quantitative comparison is done in terms of reconstruction MSE on the two synthetic toy datasets. On the two real world datasets, no baseline results are presented. This is quite surprising, as one of the datasets (MC_Maze) is an established benchmark dataset with standardized evaluation protocols, and the other dataset is presented to address a *\"longstanding benchmark task for SSMs\"*.\n- The Corollary is imprecisely stated and suggest something false. It states that \"*any* multidimensional time series can be characterized through [the] proposed model\". Assuming that \"characterized\" is meant to say that the approximation error can become arbitrary small, this is not sufficiently supported by the proof. For example, the the preceding theorem and its proof make certain assumptions on the time series, including continuity."}, "questions": {"value": "- In experiment 3.4 you state that 80% of observations are used for training. What fractions are used in the other experiments? How are the 80% selected?\n- For the quantitative reconstruction analysis (experiment 3.3), do you evaluate on all observations or only on those that were not seen during training? Moreover, how do simple strategies such as interpolation and constant continuation with the last known observation perform?\n- For decoding you often use a large number of particles, e.g. 10k in experiment 3.4, which suggests a large spread, especially as you only report the mean later on. Is this correct?\n- Related: Can you comment on the selection of datasets, specifically on why they are best modeled using stochastic processes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hPiM20cVfi", "forum": "nm7nOFm5Tl", "replyto": "nm7nOFm5Tl", "signatures": ["ICLR.cc/2026/Conference/Submission12655/Reviewer_z9Qw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12655/Reviewer_z9Qw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581011169, "cdate": 1761581011169, "tmdate": 1762923496194, "mdate": 1762923496194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIP-CSDE, a cascaded stochastic differential equation model that uses a sparse set of adaptively placed \"inducing points\" to learn low-dimensional manifolds from high-dimensional neural time series. Its key contributions are an architecture that does not rely on pre-defined kernels; a computationally efficient inference procedure with linear $\\mathcal{O}(P \\cdot N)$ scaling; a theoretical guarantee of universal approximation capability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well structured and the overall idea is easy to follow. \n* The cascaded SDE framework with marked inducing points can be regarded as a novel approach, supported by a theoretical guarantee (universal approximation property)."}, "weaknesses": {"value": "*   **Motivation & Conceptual Foundation:** The rationale for the proposed cascaded SDE structure is underdeveloped. The paper would be strengthened by a more explicit motivation for this specific architecture, explaining why a two-layer SDE driven by a marked point process is a natural or necessary solution to the limitations of existing models.\n\n*   **Clarity of Inference Procedure:** The description of the learning and inference algorithm is insufficient for a reader to fully grasp the methodology. Relying on a high-level algorithm pseudocode and appendices is inadequate for the main text. Key details, such as the precise form of the proposal distribution `π` or the handling of particle degeneracy, should be elaborated upon to improve clarity and reproducibility.\n\n*   **Empirical Evaluation & Support for Claims:** The experimental results, while extensive, do not fully or clearly substantiate several key claims:\n    *   The superior performance on the Lorenz system is measured in **observation space**, which does not directly demonstrate the claimed ability to accurately \"recover the underlying manifold structure.\" A latent space analysis or a dedicated manifold recovery metric is needed to support this.\n    *   Figure 3's caption claims superior accuracy, yet the figure itself only visualizes the proposed method's results, providing no point of comparison for the reader.\n    *   The paper mentions comparisons against models like Latent ODEs, and DiGP (and SING?) in the text, but these results are absent from the main experimental section and Table 1, creating a discrepancy.\n    *   The rat hippocampus and monkey reaching task experiments lack benchmarking against established methods, making it difficult to assess the proposed model's performance relative to the state-of-the-art in these specific applications.\n\n* There are also many typos and acronym issues in the main text, careful proofreading is needed"}, "questions": {"value": "1.  What is the specific advantage of the two-layer SDE cascade over a single SDE with a flexible drift network?\n2.  How does the model's performance depend on the choice of priors for the waiting times and marks (Gamma, Normal)? Do we need to perform sensitivity analysis?\n3. For the latent $\\boldsymbol{x}_t$ and $\\boldsymbol{y}_t$, according to Eqs. (2) & (4), it seems that each dimension is independent. What is the benefit of this assumption, and how does this assumption affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rI7MtsLDfW", "forum": "nm7nOFm5Tl", "replyto": "nm7nOFm5Tl", "signatures": ["ICLR.cc/2026/Conference/Submission12655/Reviewer_2jDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12655/Reviewer_2jDe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623165494, "cdate": 1761623165494, "tmdate": 1762923495695, "mdate": 1762923495695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors consider a SDE for representing observations from a point process. The authors suppose that a latent dynamical system exists which is driving the observations in the point process space. The authors then describe an expectation maximization algorithm that finds the optimal latent process that corresponds to the observations. The authors also describe a variational inference technique within the appendix that is used to complement the EM algorithm described in the main text."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method empirically performs very well compared to the chosen baselines. \n\nThe method provides a nice connection between a latent continuous process and observed point process, which appears in many disciplines."}, "weaknesses": {"value": "This line of work (e.g. filtering type problems) is well studied and the authors do not really do a good job of referring to existing work. \n\nThe baselines are a bit weak and do not consider a variety of point process approaches or other filtering approaches. \n\nExisting work relates point processes to latent SDEs, which should be discussed: \n\n[1] Hasan et al, Inference and Sampling of Point Processes from Diffusion Excursions, UAI 2023\n\n[2] Jaiswal et al, Variational inference for diffusion modulated Cox processes"}, "questions": {"value": "Can the authors please explain the significance of the theoretical results?\n\nWhat guarantees do you have on the properties of the recovered stochastic process? The paper makes multiple motivating claims on interpretability but it’s unclear under what conditions these are actually satisfied. \n\nCan the authors please include some more discussion to traditional filtering algorithms? \n\n\nMinor:\nSED —> SDE in abstract."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zKOPIOtnsv", "forum": "nm7nOFm5Tl", "replyto": "nm7nOFm5Tl", "signatures": ["ICLR.cc/2026/Conference/Submission12655/Reviewer_Y7i2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12655/Reviewer_Y7i2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883107131, "cdate": 1761883107131, "tmdate": 1762923495279, "mdate": 1762923495279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a probabilistic framework called Marked Inducing Point Cascaded Stochastic Differential Equation (MIP-CSDE) for modeling neural population activity continuously. A marked temporal point process generates inducing points and consecutive inducing points are connected via a Brownian-bridge SDE to form an intermediate latent process, which derives the evolution of another SDE. Model parameters are trained with an SMC-EM algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of integrating a marked temporal point process with continuous-time latent dynamics is interesting. \n2. The paper includes detailed mathematical derivations and algorithmic procedure.\n3. The computational cost of the proposed model scales linearly with data size, outperforming other non-parametric methods."}, "weaknesses": {"value": "1. The paper does not discuss the necessity of using two SDEs. From the formulation, the first-SDE variable X appears to play the role of a velocity term, yet the motivation describes learning dynamics that lie on a low-dimensional manifold, which would naturally correspond to positional representations of positions rather than velocities. Moreover, the two-layer SDE structure itself is a standard controlled differential equation (CDE) formulation, and the paper does not clarify how it differs conceptually or empirically from existing latent CDE/SDE frameworks. \n2. The paper does not specify how the number of inducing points is determined. This omission leaves unclear how the inducing-point resolution affects model capacity and computational cost. \n3. Necessary details are lack when first time using some abbreviations, such as GP."}, "questions": {"value": "Please see the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MWLcGvA9wu", "forum": "nm7nOFm5Tl", "replyto": "nm7nOFm5Tl", "signatures": ["ICLR.cc/2026/Conference/Submission12655/Reviewer_PAza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12655/Reviewer_PAza"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897833321, "cdate": 1761897833321, "tmdate": 1762923494586, "mdate": 1762923494586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}