{"id": "NW8ebJTDON", "number": 18201, "cdate": 1758285097023, "mdate": 1759897120113, "content": {"title": "Augmented Mixup Procedure for Privacy-Preserving Collaborative Training", "abstract": "Mixup, introduced by Zhang et al., is a regularization technique for training neural networks that generates convex combinations of input samples and their corresponding labels. Motivated by this approach, Huang et al. proposed InstaHide, an image encryption method designed to preserve the discriminative properties of data while protecting original information during collaborative training across multiple parties. However, recent studies by Carlini et al., Luo et al., and Chen et al. have demonstrated that attacks exploiting the linear system generated by the mixup procedure can compromise the security guarantees of InstaHide. To address this vulnerability, we propose a modified mixing procedure that introduces perturbations into samples before forming convex combinations, making the associated linear inverse problem ill-conditioned for adversaries. We present a theoretical worst-case security analysis and empirically evaluate the performance of our method in mitigating such attacks. Our results indicate that robust attack mitigation can be achieved by increasing the perturbation level, without causing a significant reduction in classification accuracy. Furthermore, we compare the performance of our approach with that of InstaHide on standard benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100.", "tldr": "We analyze three major attacks on InstaHide and identify a common vulnerability: attackers can solve the system of equations induced by InstaHide. To address this, we propose a modified mixup procedure that makes the problem ill-posed for attackers.", "keywords": ["privacy", "mixup", "InstaHide"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c26e85e29a8e362b74af77e32d10edf123fdf4e1.pdf", "supplementary_material": "/attachment/b4fd5cd2ccdf3c1313d7f046c9f87c6fef9f13fc.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a security flaw in InstaHide, a privacy-preserving method for collaborative machine learning. The authors demonstrate that InstaHide's process of mixing private and public images can be reversed by an adversary. As a solution, they propose a new method called Singularized Mixup. The core contribution is a procedure where a user's private image is mixed with a weighted average of other *noisy* private images from their own dataset. This deliberate injection of coupled noise before mixing creates a mathematically ill-conditioned inverse problem, making it computationally infeasible for an attacker to reconstruct the original data, thereby securing privacy while largely preserving the model's training accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a mathematically simple privacy-preserving mechanism that does not rely on external public datasets.\n- The algorithm maintains high model utility, demonstrating only a minor drop in accuracy in exchange for strong privacy guarantees."}, "weaknesses": {"value": "The paper shows empirically that accuracy is preserved, but lacks a theoretical justification for *why*. Could the authors provide a formal proof explaining how gradients can effectively de-mix the feature signals during backpropagation, ensuring the model learns true features rather than spurious correlations from the noise and other mixed images?\n\nThe mixup procedure's core assumption of a linear data manifold can fail for complex datasets. Could the authors justify why their method is robust to this? Specifically, how does it prevent the model from learning erroneous features when interpolations between points fall into semantically meaningless regions of the feature space?\n\nThe hyperparameters `k` and `r` are currently tuned empirically. Could the authors provide a principled guideline for selecting these parameters for an arbitrary new dataset? A framework that connects a dataset's geometric properties to the choice of `k` and `r` to achieve a predictable privacy-utility trade-off would be a significant contribution.\n\nThe paper assumes the adversary is limited to linear attacks. It completely ignores the possibility of more sophisticated, non-linear attacks. For example, a powerful adversary could potentially train a dedicated neural network (like a GAN or a diffusion model) to \"de-noise\" or \"de-mix\" the images.\n\nThe security analysis doesn't answer a crucial question: For a given image size and a chosen noise radius r, what is the minimum computational cost for an attacker to achieve a certain reconstruction accuracy? Without this, it's hard to anoint the system as \"secure\" in a practical cryptographic sense; it's more of a demonstration of computational difficulty.\n\nThe evaluation on CIFAR and MNIST is promising, but doesn't show scalability. Could the authors evaluate their method on ImageNet to demonstrate if the privacy-utility trade-off holds on a larger, more complex benchmark without a prohibitive drop in accuracy?\n\nThe paper contains numerous typos and formatting errors (e.g., `[?]`, \"ALgorithm,\" \"miuxp\"). A thorough proofread is strongly recommended."}, "questions": {"value": "* Could the authors provide a formal proof for how gradients de-mix feature signals during backpropagation to preserve model utility?\n* How does the algorithm prevent learning erroneous features when the linear interpolation between two data points falls into a semantically meaningless region of the feature space?\n* Can the authors provide a principled guideline for selecting the hyperparameters `k` and `r` on a new dataset to achieve a predictable privacy-utility trade-off?\n* How robust is the algorithm against a non-linear adversary who uses a generative model to learn the de-mixing function?\n* What is the minimum computational cost for an attacker to achieve a meaningful reconstruction of a private image?\n* Would the stated privacy-utility trade-off hold if the method were evaluated on a large-scale, complex benchmark like ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3NWJXTs7M1", "forum": "NW8ebJTDON", "replyto": "NW8ebJTDON", "signatures": ["ICLR.cc/2026/Conference/Submission18201/Reviewer_9hNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18201/Reviewer_9hNT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210221479, "cdate": 1761210221479, "tmdate": 1762927947443, "mdate": 1762927947443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a procedure for training on private data using mixup. Specifically, they identify that prior works that attempt that leverage mixup were broken by follow up works due to the linear system generated by the mixup procedure. To mitigate this vulnerability, the paper proposes a novel procedure called \"Singularized Mixup\". This method modifies the standard mixup process significantly: each generated sample is a convex combination containing exactly one 'clean' private image. The remaining $k-1$ components are constructed by taking other images from the private dataset and adding substantial, independently sampled noise vectors to them before performing the weighted summation. Crucially, the noise added to a sample is scaled by the same weight used for the sample itself in the mix, coupling the noise structure to the mixing process. Through theoretical and experimental analysis, they show that this change makes it harder for adversaries to recover the private inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow\n2. I like the security analysis that assumes a strong attacker who has access to the mixing matrix."}, "weaknesses": {"value": "1. **Experiments:** Experiments are mostly limited to toy datasets (MNIST, CIFAR-10, CIFAR-100). Utility of the method for more realistic datasets remain unknown.\n2. **Potential adaptive attack:** The authors correctly assume a strong adversary model where the mixing matrix $W$ is known (Section 4). This knowledge trivially allows the adversary to perfectly cluster all mixed samples $\\tilde{x}$ that contain a specific original image $x_i$, regardless of the added noise $e$. While the paper argues that the \"Singularized\" nature prevents recovery by averaging (as the average includes other clean images $x_a, x_b, ...$), this overlooks the possibility of a two-stage attack:\n\n    a. Noise Reduction via Averaging: The adversary can average the identified clusters. This averaging process will reduce the variance of the explicit noise components ($e$) due to their independent sampling across different mixups, even if it creates new, complex mixtures of the other clean images.\n\n    b. Blind Source Separation (BSS): The result of Stage 1 is a new set of mixed observations where the explicit noise is significantly attenuated, leaving primarily mixtures of the original image signals. This precisely forms a Blind Source Separation problem. An adversary could potentially apply standard BSS techniques (e.g., Independent Component Analysis - ICA) to this \"denoised\" system to recover the original images $X$.\n\n    The paper's security analysis focuses on the difficulty of solving $\\tilde{X} = WX + E$ directly due to the large, coupled noise $E$. However, it does not appear to address the possibility that an attacker might first effectively remove $E$ through averaging clusters (enabled by knowing $W$) and then solve the remaining BSS problem. This potential attack vector warrants further investigation and discussion by the authors."}, "questions": {"value": "1. Can you address the weakness regarding adaptive attacks? \n2. How well does the proposal work for more realistic datasets like imagenet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jaBZlewIoy", "forum": "NW8ebJTDON", "replyto": "NW8ebJTDON", "signatures": ["ICLR.cc/2026/Conference/Submission18201/Reviewer_h7YU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18201/Reviewer_h7YU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515855516, "cdate": 1761515855516, "tmdate": 1762927946685, "mdate": 1762927946685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Singularized Mixup (S-Mixup), a modification of Mixup/InstaHide that adds per-term noise before forming convex combinations so that each encoded sample includes exactly one private image and k−1 noisy components, aiming to make the linear inverse problem ill-conditioned for attackers. The authors analyze security under an isotropic Gaussian assumption and evaluate on MNIST/CIFAR-10/100."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation: repeated private-sample reuse in InstaHide is a key attack vector, and S-Mixup avoids that by design.\n- The paper is well structured in terms of explaining the motivation and the vulnerabilities of the prior methods.\n- There are sufficient experiments to demonstrate the properties of the proposed method."}, "weaknesses": {"value": "## Weaknesses\n- The major issue with this method would be the significant classification accuracy dropping when reasonable noise levels are used. For mf=1,2,4 the recovered images could be simply denoised by a learned image restoration network to obtain good results. For mf=8,16,32 the performance drop is significant. This severely limits cases in which the proposed method can be used.\n- The theorem has too strict assumptions (gaussian for data etc.) and the connection to the experimental work is left a bit tenuous. I am not sure it provides meaningful insight in understanding or evaluating the proposed method. Given the assumptions on the data don't hold for the experimental part, the only value could be in understanding the characteristics of the method which is also not very well explained. The connection between the theory, proposed method, and empirical results should be strengthened and explained more clearly.\n\n## Minor Issues\nThere are a couple of grammatical issues throughout the paper.\n- Typo in line 330 \"ALgorithm\"\n- Typo in line 330 \"miuxp\"\n- Line 580 \"coorsinates\"\n    etc.\n\nLine 294 the reference is not rendered."}, "questions": {"value": "- In a multi-party scenario where each party has k (for example 1000) private labeled images, when would using S-Mixup across n parties improve performance over training with standard Mixup on private data? This seems like an interesting case to analyze. This would demonstrate an interesting tradeoff between secure training on more data versus using minimal clean data.\n- Can the matrix W be constructed to make the system more ill-conditioned, rather than sampled at random?\n- The paper states that S-Mixup makes the inverse problem ill-conditioned. Can one estimate or bound the condition number as a function of the noise level?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i3mroAbhJE", "forum": "NW8ebJTDON", "replyto": "NW8ebJTDON", "signatures": ["ICLR.cc/2026/Conference/Submission18201/Reviewer_xaSY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18201/Reviewer_xaSY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18201/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937556135, "cdate": 1761937556135, "tmdate": 1762927946308, "mdate": 1762927946308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the vulnerabilities of image encryption methods, such as InstaHide, to adversarial attacks through a simple but effective modification to the mixup procedure. The contribution is based on the insight that introducing noise before the mixing operation makes private data reconstruction significantly less effective compared to adding it after the mixup operation. Through a thorough review of systematic attacks to which InstaHide is susceptible, the authors demonstrate that their method is resilient and provide a formal proof of its security."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the vulnerabilities of image encryption methods, such as InstaHide, to adversarial attacks through a simple but effective modification to the mixup procedure. The contribution is based on the insight that introducing noise before the mixing operation makes private data reconstruction significantly less effective compared to adding it after the mixup operation. Through a thorough review of systematic attacks to which InstaHide is susceptible, the authors demonstrate that their method is resilient and provide a formal proof of its security."}, "weaknesses": {"value": "1. In page 8, the paper claims ”for more complex data, such as CIFAR-10\nand CIFAR-100, a noise factor of mf = 8 is sufficient to prevent the recovery\nof the original image.” This seems to be based on the results of Figure 2\nrather than theory. This makes it difficult for users to apply their method\nto new datasets, as it’s hard to determine what the appropriate mf may\nbe sufficient to prevent reconstruction of the original data. Can the authors please clarify? \n\n2. The experiments only explored the image domain.\n\n3. The experiments covered MNIST, CIFAR-10, and CIFAR-100, but do not\nexplore dealing with medium to large datasets (IMAGENET), so it is empirically unclear if the method works for these larger systems. CIFAR-100, the hardest of the three datasets, has the largest drop in accuracy with increasing scaling factor mf. Would this be more pronounced with even more difficult datasets to the extent that their algorithm becomes impractical?\n\n4. Figure 2, page 9: Wouldn’t the effect of mf be more apparent if every\nrow contained the same image? Is it possible that the effect of mf varies\ndepending on the individual characteristics of each image?\n\n5. In page 7, line 330, capitalization of L in ”ALgorithm. ” Missing citation in page 6, eq. 9."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8i3ao60r7b", "forum": "NW8ebJTDON", "replyto": "NW8ebJTDON", "signatures": ["ICLR.cc/2026/Conference/Submission18201/Reviewer_L4sq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18201/Reviewer_L4sq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18201/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762961488789, "cdate": 1762961488789, "tmdate": 1762961488789, "mdate": 1762961488789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}