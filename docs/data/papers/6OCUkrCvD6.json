{"id": "6OCUkrCvD6", "number": 18218, "cdate": 1758285298146, "mdate": 1759897118601, "content": {"title": "Trajectory First: A Curriculum for Discovering Diverse Policies", "abstract": "Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima.\nIn this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel.\nHowever, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity.\nTo improve diversity optimization in RL, we therefore propose a two-stage curriculum.\nThe key idea of our method is to leverage a spline-based trajectory prior as an inductive bias to generate diverse, high-reward behaviors in the first stage, before learning step-based policies in the second.\nIn our empirical evaluation, we provide novel insights into  shortcomings of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.", "tldr": "", "keywords": ["Reinforcement Learning", "Exploration", "Diversity", "Robotics"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce50879a6c47a73b416fe7c21fe120869722d3a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes learning diverse skills based on a constrained diversity objective. Notably, the diversity maximization follows from optimizing the parameters of an open-loop trajectory that enables exploration on a broader state space. Subsequently, a closed-loop policy is distilled from the learned trajectories to embed the diversity into a high-performing policy. Quantitative and empirical results show that the proposed method can learn diverse skills and perform well."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an interesting and important field for learning robot policies. \n- Generally, the paper is well motivated. The reader can understand why the proposed method is important to the field and follow the structure to easily understand the underlying idea."}, "weaknesses": {"value": "- For me, the usage of the term curriculum learning is unclear. The paper states that the first stage of learning diverse skills is the curriculum, however, from my understanding, the curriculum learning is to start from easier tasks and gradually increase the task complexity (see Questions). \n- Although the work discusses relevant related works, it lacks a discussion of a field of works that learn diverse behaviors based on the pure maximum entropy RL framework [1,2]. How does the proposed method compare to those methods, and why is it beneficial to follow the constrained Diversity optimization framework? As also stated in the paper, the work in [2] also considers learning skills on a trajectory-level as proposed in this paper. \n\n[1] T. Haarnoja, et al. Latent Space Policies for Hierarchical Reinforcement Learning. ICML 2018.\n\n[2] O. Celik, et al. Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts. ICML 2024."}, "questions": {"value": "- How does the method generalize to varying initial positions? From my understanding, the learned skills in the first stage are related to fixed initial positions, such that for every new initial position, a set of diverse skills must first be learned. How does the proposed method react to this situation?  \n\n- From my understanding, the diversity is discovered in the trajectory level by measuring how far single features/states are away from each other. How is it guaranteed that diversity arises from this measurement? I understand that this is an effective measurement, for example, on the cylinder avoidance task, where the trajectories are represented in the XY-plane of the end-effector. However, in the case where the robot is controlled by joint trajectories, is it guaranteed to discover diversity? The robot might have redundancies that do not necessarily lead to diverse behaviors. Is it required to define a task-specific diversity measure in this case?\n\n- Related to the prior question, how does the diversity measurement actually scale? From Eq. 3 and Eq. 7, it is clear that a set of reference trajectories needs to be saved. Although the paper states that only one reference trajectory is saved to compare the distance, this reference trajectory needs to be found by running the nearest neighbors method, which again requires calculating the distances over a set of trajectories.  \n\n- How exactly is the policy extraction done? It would be good for the reader to provide explicit objective functions for optimizing the reactive policy. \n\n- Where exactly is the curriculum learning? Curriculum learning in RL (see e.g., in [1,2,3]) has already been proposed in the literature, also in the diverse skill discovery setting, both in RL [4] and Imitation Learning [5]. In these cases, the curriculum mostly represents tasks that change in difficulty. \n\n\n [1] C. Florensa, et al. Reverse curriculum generation for reinforcement learning. ICLR 2017.\n\n [2] C. Florensa. et al. Automatic goal generation for reinforcement learning agents. ICML 2018.\n\n [3] P. Klink, et al. Self-Paced Deep Reinforcement Learning. NeurIPS 2020.\n\n [4] O. Celik, et al. Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts. ICML 2024.\n\n [5] D. Blessing, et al. Information maximizing curriculum: A curriculum-based approach for learning versatile skills. NeurIPS 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TdHy4ghtxL", "forum": "6OCUkrCvD6", "replyto": "6OCUkrCvD6", "signatures": ["ICLR.cc/2026/Conference/Submission18218/Reviewer_p23b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18218/Reviewer_p23b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649417591, "cdate": 1761649417591, "tmdate": 1762927961078, "mdate": 1762927961078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of discovering a diverse set of high-performing policies in complex reinforcement learning (RL) tasks, such as robotic manipulation. The authors identify a key limitation in existing constrained-diversity RL methods (e.g., Domino): they often ​​under-explore​​ in high-dimensional spaces, leading to a lack of genuine behavioral diversity as policies collapse to a few local optima. The core contribution is a novel ​​two-stage curriculum​​ that introduces a strong inductive bias for exploration. The curriculum's key idea is to decouple exploration from policy learning:\n\n​​- Stage 1 (Trajectory-First Exploration)​​: A method called ​​Constrained Novelty Search (CNS)​​ uses an evolution strategy (CMA-ES) to optimize a population of open-loop, spline-parameterized trajectories. CNS maximizes the diversity of these trajectories (measured by a particle-based entropy estimator on state features) under a performance constraint enforced by Lagrange multipliers.\n\n​​- Stage 2 (Policy Distillation)​​: The diverse, high-reward trajectories from CNS are used to warm-start the training of multiple skill-conditioned policies using an off-policy RL algorithm (a modified version of Domino). The paper provides specific recommendations for stabilizing this offline-to-online phase, such as symmetric sampling and a balanced update-to-data (UTD) ratio for the policy and critic networks.\n\nEmpirical evaluation on robotic tasks (cylinder avoidance, cube pushing, button pressing) demonstrates that this curriculum enables the discovery of significantly more diverse behaviors than baseline methods, including vanilla diversity optimization, action/parameter noise, DIAYN, and RND."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "​​1. The two-stage \"trajectory-first\" approach is the primary strength. It is a conceptually clean and effective way to tackle the exploration problem in high-dimensional diversity search.\n\n2. ​​The paper is backed by extensive experiments, including compelling qualitative visualizations (Figures 2, 3) and solid quantitative comparisons across multiple tasks and seeds (Figure 4).\n\n3. ​​The analysis goes beyond simply proposing a new method. It offers deep insights into whyit works, through ablations on the CNS objective, the necessity of the diversity objective in stage 2, and practical training recipes (UTD ratio).\n\n​​4. The use of spline-based trajectories is a well-justified and effective bias that makes the exploration problem tractable."}, "weaknesses": {"value": "1. ​While the method is intuitively well-motivated and empirically validated, a more formal theoretical discussion on the guarantees of the two-stage curriculum (e.g., conditions under which trajectory-space diversity leads to policy-space diversity) would strengthen the foundation.\n\n2.The trajectory parameterization works well for the evaluated tasks, but its scalability to problems requiring much longer horizons or more complex, hierarchical behaviors is not discussed. An analysis or discussion of the limitations regarding the dimensionality m×uof the trajectory space would be helpful.\n\n​​3. Lack of investigation of related work. Actually, there are recent works of QD-RL that seems to address the similar problem as yours. For example: [1] and [2] both facilitate learning a diverse set of high-performing policies via reinforcement learning or inverse reinforcement learning. Acknowledgement, discussion or baseline comparison might be necessary.\n\n[1] Wan, Z., Yu, X., Bossens, D. M., Lyu, Y., Guo, Q., Fan, F. X., ... & Tsang, I. Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity. In Forty-second International Conference on Machine Learning.\n\n[2] Batra, S., Tjanaka, B., Fontaine, M. C., Petrenko, A., Nikolaidis, S., & Sukhatme, G. (2023). Proximal policy gradient arborescence for quality diversity reinforcement learning. arXiv preprint arXiv:2305.13795."}, "questions": {"value": "1. ​​The choice of a B-spline for trajectory parameterization is a key inductive bias. How sensitive are the results to the choice of the number of control points m? Did you explore alternative parameterizations, and what are the theoretical or empirical guidelines for choosing mto balance expressivity and optimization efficiency for a new task?\n\n​​2. The current CNS explores open-loop trajectories. For tasks that require long-horizon reasoning or hierarchical strategies (e.g., \"grasp then place\"), could the trajectory-first approach be extended? For instance, by having a higher-level CNS plan a sequence of sub-goals for lower-level policies?\n\n​​3. The diversity objective relies on a fixed random projection of states. Did you experiment with using features learned in an unsupervised manner (e.g., from successor features or other representation learning methods) instead of the random projection? Could a learned feature space potentially capture more semantically meaningful diversity, especially in pixel-based tasks?\n\n4. There are recent works of QD-RL that seems to address similar problem as yours. For example: [1] and [2] both facilitate learning a diverse set of high-performing policies via reinforcement learning or inverse reinforcement learning. What is the difference of settings between your work and these works? If the settings are the same, you may need to incorporate these methods as baseline for comparison. If not the same, then discussion or acknowledgement would be necessary.\n\n[1] Wan, Z., Yu, X., Bossens, D. M., Lyu, Y., Guo, Q., Fan, F. X., ... & Tsang, I. Diversifying Robot Locomotion Behaviors with Extrinsic Behavioral Curiosity. In Forty-second International Conference on Machine Learning.\n\n[2] Batra, S., Tjanaka, B., Fontaine, M. C., Petrenko, A., Nikolaidis, S., & Sukhatme, G. (2023). Proximal policy gradient arborescence for quality diversity reinforcement learning. arXiv preprint arXiv:2305.13795.\n\n\nGlad to raise my score if these concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "851OdOPhEm", "forum": "6OCUkrCvD6", "replyto": "6OCUkrCvD6", "signatures": ["ICLR.cc/2026/Conference/Submission18218/Reviewer_Zexh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18218/Reviewer_Zexh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714154259, "cdate": 1761714154259, "tmdate": 1762927960677, "mdate": 1762927960677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a novel approach for learning diverse policies. The approach consists of two main steps. First, they generate diverse behaviors using spline-based trajectory priors, and then optimize diverse step-based policies using reinforcement learning in the second step."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper deals with diversity in policy learning, and that is an important topic. The approach proposes a novel combination of spline-based exploration and learning step-based policies using reinforcement learning. Overall, the method is well-supported mathematically."}, "weaknesses": {"value": "The authors evaluated the proposed approach on only a smaller 3 tasks. Moreover, the value of diversity for two of the tasks is questionable, namely cube push and button press.\n\nAdditionally, the results presented in Figure 4, show returns of their method lower than the baselines. While diversity is useful, their method losses out on performance as it achieves lower returns. Also in ablation presented in Figure 5b, without the diversity, the method achieves better mean diversity but worse return. However, both in confidence intervals.\n\nWhile the authors use step-based RL in the second phase of their approach, it would also be possible to use episode-based RL. In that case, it would be useful to compare it to existing methods, which the authors mentioned in the related works. Moreover, the authors claim that those methods have drawbacks in practical domains such as robotics. However, this claim is not further supported and if it is used to contrast with their approach, it is somewhat hard to make since the authors have not evaluated the proposed method on a real-world robotics task. This comment refers to text and works in Line 250.\n\nThe authors do not provide code."}, "questions": {"value": "Please address the points raised in Weaknesses.\n\nIs the Cylinder avoidance task the same as the Maze task? Based on the writing, it is, but the naming is different in Figures 2 and 4.\n\nHave you considered diffusion models? Recent works have managed to train diffusion policies using RL, and they inherently have diverse behaviors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YfmGHIYJUF", "forum": "6OCUkrCvD6", "replyto": "6OCUkrCvD6", "signatures": ["ICLR.cc/2026/Conference/Submission18218/Reviewer_HQFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18218/Reviewer_HQFJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936053717, "cdate": 1761936053717, "tmdate": 1762927960225, "mdate": 1762927960225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage method to train reinforcement learning agents that can solve a task in many different ways. In the first stage, the authors search for diverse and high-reward trajectories by optimizing smooth spline-based motion paths with an evolutionary algorithm. This produces a set of distinct ways to complete the task. In the second stage, these trajectories are used to train a single skill-conditioned policy that can reproduce and refine those diverse behaviors while still meeting performance constraints. The method, called trajectory-first constrained diversity optimization, is tested in simulated robot manipulation tasks such as obstacle avoidance, cube pushing, and button pressing. It achieves higher diversity and comparable task performance to previous diversity-driven RL approaches, demonstrating that exploring in trajectory space before policy learning can lead to richer and more varied solutions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Reframing diversity exploration to trajectory space with B‑splines and population‑wise Lagrange multipliers is a clean, compelling bias. The  UTD recipe for constrained diversity is a practically relevant training insight.\n\nStrong ablations substantiate design choices: Lagrangian vs scalarized novelty, with/without diversity in the RL stage. Clear comparisons to multiple good baselines\n\nThe results show substantial diversity gains on the experiments at high return, and the qualitative behaviors are convincing.\n\nThe high‑level pipeline is well conveyed; the objective and training signals are mathematically well specified."}, "weaknesses": {"value": "All tasks are compact, deterministic simulations. The curriculum’s value for contact‑rich, stochastic manipulation and real‑world execution is not assessed.\n\nFigures show end‑to‑end policy behaviors, but I could not find a stand‑alone characterization of CNS outputs before RL. I would like to see its performance quantitavely on all tasks and visually on avoidance task.\n\nNo code is provided, which increases reproducubility concerns.\n\nThe sensitivity of the results to alpha value should be shown in the paper.\n\nMethod section is hard to follow because of the engineering load to stabilize the training."}, "questions": {"value": "How exactly are v* and alpha estimated per stage and over time?\n\nPlease provide performance and diversity of CNS trajectories alone (pre‑RL).\n\nYour Stage‑1 uses spline‑parameterized open‑loop trajectories with CMA‑ES under a diversity/near‑optimality objective\nCould you explicitly position this against:\n\nQuality‑Diversity (QD) methods such as MAP‑Elites that evolve archives of high‑performing diverse solutions (often over policy or primitive parameters), why is CNS‑over‑splines preferable, and would a MAP‑Elites baseline on the same trajectory parameterization close the gap? \n\nMovement primitives optimized by evolutionary methods, e.g., DMPs and ProMPs (classical low‑dimensional trajectory representations widely used with evolutionary search): in what ways is your spline representation and Lagrangian near‑optimality constraint genuinely different or stronger than running QD/ES directly over DMP/ProMP parameters (and then distilling)? Please consider an ablation swapping splines for DMP/ProMP under the same CNS/constraint setup.\n\nReward‑Conditioned Neural Movement Primitives : both methods use population‑based search. What is the key advantage of CNS over RC‑NMP’s reward‑conditioned latent‑space generation + crossover/mutation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "milh0acl7R", "forum": "6OCUkrCvD6", "replyto": "6OCUkrCvD6", "signatures": ["ICLR.cc/2026/Conference/Submission18218/Reviewer_VTZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18218/Reviewer_VTZ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991592453, "cdate": 1761991592453, "tmdate": 1762927959753, "mdate": 1762927959753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}