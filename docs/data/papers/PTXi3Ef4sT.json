{"id": "PTXi3Ef4sT", "number": 3290, "cdate": 1757395327961, "mdate": 1759898097513, "content": {"title": "Don’t Pass@$k$: A Bayesian Framework for Large Language Model Evaluation", "abstract": "Pass@$k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass@$k$ and average accuracy over $N$ trials (avg@$N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass@$1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT, and BrUMO, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass@$k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass@$k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit.", "tldr": "We propose a Bayesian evaluation framework that replaces Pass@k with stable, uncertainty-aware metrics for reliable and compute efficient LLM evaluation.", "keywords": ["Large Language Models", "Evaluation Metrics", "Bayesian Methods", "Uncertainty Quantification", "Model Ranking", "Reasoning", "Statistical Significance"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6fd9c35dba1a385def6815ca772d99be030cf05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes replacing the widely-used Pass@k metric for LLM evaluation with a Bayesian framework that models evaluation outcomes as categorical distributions under a Dirichlet prior. The approach yields closed-form posterior means and credible intervals, enabling principled uncertainty quantification and stable model rankings. The authors demonstrate that under a uniform prior, their Bayesian estimator is order-equivalent to avg accuracy, which helps explain avg@N's empirical robustness while adding explicit uncertainty. Through simulations with known ground-truth and experiments on math reasoning benchmarks (AIME'24/'25, HMMT'25, BrUMO'25) with 11 LLM models, they show their method converges faster to stable rankings than Pass@k variants and naturally extends to graded, rubric-based evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a genuine pain point in LLM evaluation: Pass@k's instability with limited samples creates unreliable rankings and makes it difficult to determine when performance differences are meaningful versus noise. The principled Bayesian treatment is a natural and well-motivated solution.\nThe equivalence proof showing that Bayes@N with uniform prior produces rankings identical to avg@N (Appendix B) is reasonable and explains why average accuracy has been empirically robust. The extension to categorical outcomes via Dirichlet-multinomial conjugacy provides a unified framework for both binary and graded evaluation.\nThe framework provides actionable guidance through credible intervals: only declare ranking differences when intervals don't overlap. This \"interval-aware protocol\" reduces leaderboard churn and over-interpretation of small gaps."}, "weaknesses": {"value": "While the Bayesian framework is principled, the key finding that Bayes@N with uniform prior is order-equivalent to avg@N means the main contribution is essentially formalizing average accuracy with better uncertainty quantification. The paper would be strengthened by deeper exploration of informative priors, which are mentioned but not systematically investigated. When and how should practitioners incorporate prior knowledge? What are the risks of prior misspecification?\n\nAll experiments cap at N=80 trials. Given computational costs are claimed to be \"negligible,\" why not demonstrate convergence at N=200 or N=500 to better understand asymptotic behavior? Is N=80 a result from cherry-picking? The paper acknowledges this: \"more extensive evaluations may be constrained by computing and academic budgets\", but at least one large-N experiment would strengthen claims about efficiency gains."}, "questions": {"value": "You mention incorporating prior evidence from past runs or domain knowledge (Section 2.4, Conclusion), but this is never demonstrated. Can you provide examples showing how informative priors affect convergence speed? What safeguards prevent gaming the system through carefully chosen priors?\n\nWhat principled approach do you recommend for choosing among the 12 categorical schemas? Should practitioners report rankings under multiple schemas? How sensitive are conclusions to schema choice?\n\nYou claim \"negligible overhead\" but provide no timing experiments. Can you quantify the computational cost relative to simply computing Pass@k or avg@N? For a benchmark with M=100 questions and N=80 trials, what is the actual runtime difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NWIAmG4zl9", "forum": "PTXi3Ef4sT", "replyto": "PTXi3Ef4sT", "signatures": ["ICLR.cc/2026/Conference/Submission3290/Reviewer_4SJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3290/Reviewer_4SJp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828915856, "cdate": 1761828915856, "tmdate": 1762916652197, "mdate": 1762916652197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a principled Bayesian evaluation method called Bayes@N that replaces Pass@k and avg@N. The proposed method is to use posterior estimation using a Dirichlet prior. This method also produces confidence intervals; when the difference in scores between two models is within the confidence interval, the comparison should be considered as a tie. In the special case of a uniform prior, the method is equivalent to avg@N."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Bayes@N is well-principled.\n- The method allows computing confidence intervals, which allows determining when a pairwise comparison should be considered as a tie.\n- The paper demonstrates that Pass@K leads to ranking instability, and that the ranking fails to converge.\n- The proposed Bayes@N approach is more stable and converges quickly.\n- The experiments use a range of real-world models and datasets, and show that the method works well in practice."}, "weaknesses": {"value": "- The proposed method is cost-prohibitive in practice. In particular, Bayes@N requires the number of trials N = 10 to achieve Tau > 0.95, which is more trials than is usually done in practice. For instance, HELM (Liang et al., 2022) used N = 3. In the paper, the method is applied to mathematical reasoning datasets, which increases the cost further because mathematical reasoning typically requires long reasoning outputs.\n- Although the paper mentions that priors can be incorporated into the method, it does not present a use case where a prior would be useful in practice.\n- Appendix D Categorical has an interesting discussion of incorporating categorical features. However, this was not well-incorporated into the main narrative, and the argument for this technique's usefulness is not that strong.\n- Nit: In Appendix D, when `finish_reason` is `stop`, the instance is annotated as `repeated_pattern`, but this can occur for reasons other than a repeating pattern (e.g. the model continues reasoning for too long because it fails to arrive at the right answer)."}, "questions": {"value": "- What was the approximate cost in tokens or GPU hours for running the experiments? Please consider including this information in Section F.3 Reproducibility.\n- Is there an example of a real-world use case where incorporating a non-uniform prior would be useful?\n- Could you elaborate on the real-world usefulness of the categorical schemata method that discussed in Appendix D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5fuXQVVXNd", "forum": "PTXi3Ef4sT", "replyto": "PTXi3Ef4sT", "signatures": ["ICLR.cc/2026/Conference/Submission3290/Reviewer_68QZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3290/Reviewer_68QZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933928270, "cdate": 1761933928270, "tmdate": 1762916651372, "mdate": 1762916651372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present a bayesian framework for LLM evaluation, especially on the reasoning tasks. The main target is on Pass@k metric, which was claimed to be with high variance, slow convergence. It considers per-item outcomes as categorical with a Dirichlet prior. Under a uniform prior, they aimed for a principled uncertainty. It shows faster convergence and greater rank stability than Pass@k and variants in simulations on real math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "> The paper contributes on an interesting problem, which is about the unstable rankings from limited samples.\n\n> It provides a straightforward derivation linking to Bayesian approach, and has provided basic test on a few math datasets. \n\n> The interval-based decision rule is easy to understand and could help reduce some ranking noise."}, "weaknesses": {"value": "- The focus is narrow. It has only covered math reasoning tasks with a small set of models, leaving its broader applicability untested.\n\n- I don't quite understand the strong claim with the i.i.d trial assumption. It may not hold for common sampling methods, potentially skewing results. In particularly, the paper mentions informative priors but doesn’t explore their practical use or risks, sticking mostly to uniform ones.\n\n- This work doesn't provided thorough comparisions with other uncertainty methods or diverse rubrics. In this case, the implementing categorical rubrics might add complexity, and scalability isn’t well addressed."}, "questions": {"value": "1. As a particular interesting finding in the work, the i.i.d samples appear to be the focus. However, many modern LLM evaluation strategies, such as chain-of-thought prompting or self-consistency sampling, introduce correlations between trials that violate the i.i.d. assumption. Given that these methods are prevalent (e.g., in math reasoning tasks like AIME), how would this work handle these scenarios?\n\n2. In this paper, the experiments are primarily confined to math benchmarks, but LLMs are widely used in diverse domains (e.g., HumanEval for coding, or safety benchmarks like TruthfulQA). Testing on these could reveal domain-specific limitations, such as handling partial correctness in code or nuanced harm categories in safety, which might require different rubric designs.\n\n3. I wonder for the informative priors, what are the strategies for the authors to ensure bias is not introduced in the process? In general, it risks cherry-picking or overfitting. Especially, when the test was done with roughly 30 questions, what will be the outcome in real-world applications covering hundreds of questions or complex rubrics. \n\n4. For the experiment, it lacks details. For example, what are the details on setting up categorical rubrics and verifiers? It may also need to consider how the framework perform under noisy or adversarial data conditions? Moreover, what is the minimum sample size N required for reliable rankings across all tested datasets? How do you justify the potential overfitting to the gold standard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kGEz0dnQfn", "forum": "PTXi3Ef4sT", "replyto": "PTXi3Ef4sT", "signatures": ["ICLR.cc/2026/Conference/Submission3290/Reviewer_4Ua4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3290/Reviewer_4Ua4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987301429, "cdate": 1761987301429, "tmdate": 1762916650500, "mdate": 1762916650500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified Bayesian evaluation framework that models LLM results as categorical outcomes with a Dirichlet prior, yielding closed-form posteriors and credible intervals for both binary and complex, rubric-based metrics. Empirical evidence shows that the proposed approach chieves faster convergence and greater rank stability than Pass@k and recent variants, enabling reliable comparisons at smaller sample counts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well written and easy to follow.\n\n- It offers a principled alternative to Pass@k.\n\n- The ability to save compute and provide statistically sound model comparisons is highly valuable."}, "weaknesses": {"value": "- The paper only focuses on math benchmarks and functional correction domain. It would be nice to see application is diverse domains and tasks.\n\n- There is a lack of discussion on prioi choice. Can an informative prior reduce the number of trials? If so, how would that go?"}, "questions": {"value": "- How does runtime scale with large M and N (e.g., thousands of problems and multiple categories)?\n\n- How well does the proposed framework handle tasks where correctness is subjective (e.g., summarization, dialogue safety)?\n\n- How sensitive are rankings and credible intervals to poorly chosen priors? The authors use an informative prior but what happens if you change that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i8708uYRBy", "forum": "PTXi3Ef4sT", "replyto": "PTXi3Ef4sT", "signatures": ["ICLR.cc/2026/Conference/Submission3290/Reviewer_ma3N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3290/Reviewer_ma3N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993112974, "cdate": 1761993112974, "tmdate": 1762916649981, "mdate": 1762916649981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}