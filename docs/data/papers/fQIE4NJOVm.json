{"id": "fQIE4NJOVm", "number": 4333, "cdate": 1757664195718, "mdate": 1759898038964, "content": {"title": "Tight Bounds and Achievable Upper Bounds of Minimal Dimensions for Embedding-based Retrieval", "abstract": "This paper studies the minimal dimension required to embed subset memberships into vector spaces. \nThe lower and upper bounds are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities\", including $\\ell_2$ metric, inner product, and cosine similarity.\nOur results suggest no fundamental differences between those metrics in terms of Minimal Embeddable Dimension (MED).\nIn addition, we conduct experiments in the achievable setting, where we find that we can easily realize the logarithmic dependency between the MED and the number of objects to embed.\nOur results also align well with existing practices in large language models, vector databases, and other related fields.", "tldr": "", "keywords": ["representation learning", "embedding-based retrieval"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65dbcdbc1993a8478148564aed036c9426df6c19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper provides theoretical bounds for the so-called minimal embeddable dimension, which is the smallest dimension for which some configuration of m points with a given functional family can be k-shattered. It shows that both lower and upper bounds are independent of the number of points, and only depend on k, in the special case where the functional family is given by the 3 standard scoring functions: inner product, cosine, and L2 distance. \n\nThe paper also defines so-called minimal achievable embeddable dimension, where k-shattering is replaced by k achievable-shattering, defined by evaluating a scoring function evaluated at the centroid of k nearest points. Here nearest just means highest scores. Then the paper uses a union bound to show that the O(k^2 log m) dimension is sufficient to find a configuration of m points with the k achievable-shattering property. Finally the paper runs a simulation to verify that the true relation between achievable configuration and dimension is indeed logarithmic as opposed to cubic in a referenced paper."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The context of the problem being addressed is of significant interest in the ML community. For instance, in K nearest neighbor retrieval, we need to find the right kind of dimension to ensure most if not all query embeddings can find its nearest k item embeddings simply using dot product, L2 distance, or cosine distance (dot product with item embeddings L2-normalized).\n\nThe construction using the moment curve is interesting mathematically. \nThe proof of the achievable upper bound is also standard and reasonable. The simulation result also supports its general order of magnitude."}, "weaknesses": {"value": "The exposition of the paper is quite cryptic sometimes. I will list some examples\nl070-075: looks like 3 things are being compared here: MED, MAED, and real life practical situation. The first sentence says MAED is weaker than real life, but the second sentence then concludes that MAED upper bounds MED. The logic simply doesn’t follow.\nIt would be helpful to tabulate the results for the 3 kinds of scoring functions to make their relationship more transparent. \n\nThere are many typos in the paper, including\nl293: “We use optimize m embeddings randomly initialized”\nl028-029: \"retrieving the top-k answers of top-k largest scores\" should be rephrased as \"retrieving the answers with the k largest scores\".\n\nl019 (abstract): \"Our results also align well with existing practices in large language models, vector databases, and other related fields.\" seem irrelevant.\n\nl097: C_k definition doesn't need min\n\nThe paper can also make the result statements as well as the proofs more accessible to general ML practitioners not familiar with proof heavy literature, involving VC dimensions. \n\nThe upper bound result of the MED is pretty contrived. It doesn’t really show that the dimension n doesn’t depend on the number of points, but only that for any m, one can find a configuration of m points in R^n with the property that the k nearest points of any point can be separated by one of the scoring functions. This makes the result of little practical value. \n\nEven in the MAED case, the so-called achievable query, where it’s required to be the centroids of its k nearest neighbors, seems rather special. In addition, it’s again not saying the dimension upper bound works for all configurations, but rather one can find some configuration with the achievability constraint under the upper bound, even though it should work for almost all cases. I think it’s very much worth highlighting the limitations of these results, and try to make some attempt connecting the results to practical cases, such as in a unsupervised KNN learning task.\n\nThe setup for the simulation requires more detailed explanation as well as motivation. The use of gradient descent to search for an achievable k-shattering configuration does not guarantee optimality, but is sufficient for the upper bound. Some references would be useful to compare against other such simulation work for such a theoretical result."}, "questions": {"value": "Overall I think the paper has some interesting probabilistic results. But it needs to explain the results in a more accessible manner. There should be plenty of space left to add more details in the main text.\nI would like to see the paper much better polished in terms of writing style and motivation. Focus on the main claims, namely the 2 upper bounds and 1 lower bound, as well as the special treatment for each scoring function, and making sure the setup, definitions, and proof strategy are completely transparent. Leave some of the propositions to the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lnrdGkSSEk", "forum": "fQIE4NJOVm", "replyto": "fQIE4NJOVm", "signatures": ["ICLR.cc/2026/Conference/Submission4333/Reviewer_Dok3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4333/Reviewer_Dok3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544059277, "cdate": 1761544059277, "tmdate": 1762917304539, "mdate": 1762917304539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the *minimal embeddable dimension* (MED) problem, where given a set of $m$ objects\nand a pairwise scoring function $f$, we want to know the minimum embedding dimension $n$ such that\nwe can perfectly recover the top-k (object, query) results according to $f$. The authors consider\nthe following score functions: Euclidean distance, cosine similarity, and a related inner product.\nThey also study a so-called \"achievable\" setting (MAED) where query vectors are of the form\n$1/|S| \\sum_{i \\in S} \\mathbf{x}_i$ for some set $S \\subseteq X$ of size $k$. They give a clean\nproof of the lower and upper bounds for MED (tight up to a factor of 2). They also give\n$O(k^2 \\log m)$ upper bounds for MAED and some synthetic experiments to demonstrate this result."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors introduce the study of the achievable setting.\n- The MED and MAED problem statements and analysis are built on a clean definition of *$k$-shattering*.\n- The cyclic polytope example in Section 3.1 is instructive and succinct."}, "weaknesses": {"value": "- Manuscript is quite unpolished.\n- Experiments (Section 4.2) are very interesting but incomplete. It would be\n  good in a future version of this paper to strengthen these results (e.g.,\n  revisiting the cyclic polytope as a warm up)."}, "questions": {"value": "**Questions**\n\n- What are the lower bounds for MAED (Table 1)?\n\n**Misc**\n\n- [049] Typo: \"can be found in Section ??\"\n- [081] Typo: two different capitalization styles for list items, i.e., \"Standard setting\" and \"achievable setting\"\n- [081] Typo: \"simulation results on the .\"\n- [095] Nit: How do we handle having the same vector for two different items given the notation $\\{\\mathbf{x}_{i}\\}_{i=1}^m$?\n- [102] Nit: Inconsistent use of normal and boldface letters for scalars and vectors.\n- [246] Typo: \"top-k\" --> \"top-$k$\"\n- [281] Sugestion: Add some horizontal space between the captions of Figure 1 and Figure 2 so it's more clear they're separate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ALetFM29a2", "forum": "fQIE4NJOVm", "replyto": "fQIE4NJOVm", "signatures": ["ICLR.cc/2026/Conference/Submission4333/Reviewer_KFqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4333/Reviewer_KFqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747618472, "cdate": 1761747618472, "tmdate": 1762917304200, "mdate": 1762917304200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the minimum dimension of the vector space required for embedded retrieval systems, aiming to challenge the view on the \"vector bottleneck\" in the field. The authors introduce two core settings to analyze this problem:\n\n1. Standard Setting. Under this theoretically idealized setting, the paper proves that the Minimum Embedding Dimension (MED)—required to perfectly retrieve all queries with no more than k answers—has a linear relationship only with $k$ (i.e., $\\Theta(k)$) and is independent of the total number of objects $m$ in the corpus.\n2. Achievable Setting. Under this more practically relevant setting, query vectors are constrained to be the centroid of the answer set vectors. The paper theoretically derives and experimentally verifies that the Minimum Achievable Embedding Dimension (MAED) required for this constructive method has a logarithmic relationship with the total number of objects $m$ (i.e., $d=\\text{O}(k^2\\text{log}m)$)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical proof on the $\\Theta(k)$ bound for the Minimum Embedding Dimension provides an entirely new, more optimistic perspective for understanding the theoretical limits of embedded retrieval. On top of that, the paper successfully reframes the \"vector bottleneck\" problem, shifting it from a seemingly immutable hardware constraint (space dimension) to an optimizable software issue (embedding construction method)."}, "weaknesses": {"value": "1. The experimental validation for the \"achievable\" $O(\\log m)$ bound is not truly achievable, as its training method requires checking all $\\binom{m}{k}$ combinations, which is computationally unfeasible for large $m$.\n\n2. The experimental comparison to prior work [Weller et al., 2025a] is misleading because it compares results from two different paradigms (MAED vs. MED). A fair comparison would require the authors to re-run experiments under the same (e.g., MED) setting to prove their optimization method is truly better.\n\n3. The centroid query method proposed in the paper cannot handle complex compositional queries, which limits the applicability of the achievable method. It is important to more honestly define the scope of applicability of their method.\n\n4. There are several minor but noticeable typographical and notational issues in the manuscript."}, "questions": {"value": "1. Most importantly, to truly support the \"achievable\" claim, you should demonstrate that this O(logm) bound can also be reached using a scalable, practical training algorithm (e.g., one based on negative sampling or contrastive loss) rather than the full-combination check.\n\n2. To make a more robust claim, you should run your optimization method under the same MED (Standard Setting) as Weller et al. This would provide a true \"apples-to-apples\" comparison and prove that the difference is due to your superior optimization, not the change in settings. Alternatively, provide a clear theoretical proof or new experimental evidence within your paper demonstrating that the MAED (centroid query) is indeed a harder problem than MED (free query).\n\n3. You should more clearly define the limited scope of the MAED model. It would be valuable to discuss the gap between this \"centroid query\" model and a more realistic \"independent query\" model, and what new challenges the latter might introduce.\n\n4. There are several minor but noticeable typographical and notational issues in the manuscript. It is recommended that the authors carefully proofread the paper to improve readability and consistency. In particular:\n    - There are some cross-reference errors, including \"Section ??\"  and \"on the \\space(a space)\" in Section 1.\n    - Some misspellings are present, such as \"MEAD\" in Section 2.3 and \"k-shuttering\" in Section 3.2.\n    - Equation 10 in A.2 should be $\\langle v_1, \\sum_{u\\in S} u\\rangle - \\langle v_2, \\sum_{u\\in S} u\\rangle$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oob2LjPwwd", "forum": "fQIE4NJOVm", "replyto": "fQIE4NJOVm", "signatures": ["ICLR.cc/2026/Conference/Submission4333/Reviewer_txVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4333/Reviewer_txVp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888479507, "cdate": 1761888479507, "tmdate": 1762917303866, "mdate": 1762917303866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of finding the appropriate dimensionality to embed data in vector spaces. In contrast with recently published work, the formal findings in this paper show an encouraging picture for embedding based retrieval. First, for common similarity measures, the minimal number of dimensions does not depend on the cardinality of the set to embed. Second the minimal dimensionality is a low degree polynomial of the number k of retrieved vectors: between k and 2k in the general case and quadratic in the \"achievable\" setting. Considering that the number of retrieved vectors is a small number in practice, these theoretical bounds paint a positive picture for vector search."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The topic is of great relevance in practice, even though the results are of a very theoretical nature.\n- The dimensionality bounds are novel and they bring a much needed formal understanding to the important area of retrieving unstructured data.\n- These small bounds also highlight that more work is needed in the embedding models and that such practical work is not a lost cause.\n- To the best of my knowledge, the proofs are correct and the level of rigor exhibited is appropriate for ICLR."}, "weaknesses": {"value": "- The discussion of the achievable setting in the introduction (line 70) feels a bit lacking and its description in the contributions (lines 81 to 84) too vague. The authors should position the achievable setting more clearly in a \"hardness of embbedability\" scale.\n- How tight are the MAED bounds? Although the authors state that this bound may not be tight, I would have appreciated a more detailed discussion of what this bound means in practice.\n- How important is the fact that random vectors are used to get the MAED bound? What would happen in a different setting? Would the bound get worse? It is important to clearly state whether the authors are covering a best or worst case scenario here (or neither and it is just a particular one). This should be more clearly stated in the introduction and abstract, because those section seem to implicitly indicate the MAED bound is general."}, "questions": {"value": "- In the abstract, it is unclear what the authors mean by achievable. It would be useful to have a succint definition.\n- In the third paragraph  of the introduction, too little context is given when taking about the work by Weller et al. It is not clear what the query-relevance matrix and rank_{+/-} mean. Adding a couple of sentences might help. Alternatively, the authors should up-level that discussion to a more intuitive explanation.\n- There is an undefined symbol at the end of the third paragraph (line 49).\n- In the contributions, there is an unfinished sentence in the item about the \"achievable setting\" (line 81).\n- After Definition 2.11, \"that if MAED\" -> \"that MAED\" (line 181).\n- After Proposition 2.12, \"MEAD\" -> \"MAED\" (lines 186 and 187)\n- Please add a horizontal space between Figures 1 and 2 as the captions are hard to read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f8whvVSApw", "forum": "fQIE4NJOVm", "replyto": "fQIE4NJOVm", "signatures": ["ICLR.cc/2026/Conference/Submission4333/Reviewer_JMqo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4333/Reviewer_JMqo"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947267867, "cdate": 1761947267867, "tmdate": 1762917303610, "mdate": 1762917303610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission looks at the problem of determining the minimum dimension needed to encode an arbitrary set $X$ of $m = |X|$ objects into $\\mathbb{R}^n$ such that a retrieval query on $X$ with $k$ answers in the set is perfectly retrievable. This is done by way of a combinatorial argument:\n1. $k$-shattering is used to define the minimum embeddable dimension (MED)\n2. A simple VC-dimension bound falls out of the definitions, parametrized by $m$ and $k$, and works for an arbitrary scoring function \"family\" $\\mathcal{F}$\n3. For specific scoring functions (dot products, $\\ell_2$, cosine sim.), one can get tight bounds on the MED of $\\Theta(k)$\n4. A minimum achievable dimension (MAED) is designed to model practical scenarios and provides an upper bound on MED\nThere is also an upshot given by empirical results, which suggest that how we construct the embeddings (or generate them with a neural net) matters much more than the available dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The bounds on MED are quite surprising and make for a great result.\n2. The contrasting optimism for low-dimensional dense retrieval to the prior work of Weller, et al. will make for interesting and important discussion on the limits of vector search in the AI landscape.\n3. Careful effort is made to reconcile empirical results with the theoretical results in an intuitive manner. There is also a clean comparison with the prior work, which makes it easier to reconcile the position of this work with existing results."}, "weaknesses": {"value": "1. Typos (e.g. lines 49, 81, 283): some are quite substantial, definitely get these fixed\n2. Considering that this submission aims to contradict earlier work, some more discussion about the earlier work, what is acking in it, and motivation to pursue this approach in place of the prior work should appear earlier on in the manuscript.\n3. The MAED discussion is perhaps oversimplifying the practical scenarios that it tries to represent. While it does model the in-distribution setting of vector search, it fails (at the admission of the authors) to capture the nuance that comes with embeddings generating with a neural network. This leads to an empirical section that is, I feel, lacking. To complement the existing results, there should be experiments based on real data with neural network based embeddings that support the paper's results and an effort to quantify the issues that come with such a setting."}, "questions": {"value": "1. It's common in practice to retrieve a larger number (than $k$) of candidates, then rerank them down using a stronger similarity function into $k$ final results. Is there a way to model that setting with this framework? If we were to naively apply the theoretical results to this method, it's possible we would see poor results (as we rely on $k << m$), but this seems to work remarkably well in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CBy4xBy2FE", "forum": "fQIE4NJOVm", "replyto": "fQIE4NJOVm", "signatures": ["ICLR.cc/2026/Conference/Submission4333/Reviewer_KFwt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4333/Reviewer_KFwt"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission4333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982333691, "cdate": 1761982333691, "tmdate": 1762917303179, "mdate": 1762917303179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}