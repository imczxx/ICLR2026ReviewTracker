{"id": "2G1PoH1YHQ", "number": 9342, "cdate": 1758119510725, "mdate": 1763556462685, "content": {"title": "Evaluation-Aware Reinforcement Learning", "abstract": "Policy evaluation is often a prerequisite for deploying safety- and performance-critical systems. Existing evaluation approaches frequently suffer from high variance due to limited data and long-horizon tasks, or high bias due to unequal support or inaccurate environmental models. We posit that these challenges arise, in part, from the standard reinforcement learning (RL) paradigm of policy learning without explicit consideration of evaluation. As an alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in which a policy is trained to maximize expected return while simultaneously minimizing expected evaluation error under a given value prediction scheme—in other words, being “easy” to evaluate. We formalize a framework for EvA-RL and design an instantiation that enables accurate policy evaluation, conditioned on a small number of rollouts in an _assessment environment_ that can be different than the deployment environment. However, our theoretical analysis and empirical results show that there is often a tradeoff between evaluation accuracy and policy performance when using a fixed value-prediction scheme within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an assessment-conditioned state-value predictor alongside the policy. Empirical results across diverse discrete and continuous action domains demonstrate that EvA-RL can substantially reduce evaluation error while maintaining competitive returns. This work lays the foundation for a broad new class of RL methods that treat reliable evaluation as a first-class principle during training.", "tldr": "We present a paradigm for reinforcement learning that trains policies to ensure low evaluation error with respect to a performance estimator, while improving the expected return.", "keywords": ["Reinforcement Learning Development", "Policy Evaluation", "Safe Deployment", "Evaluation-aware Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2719262ccf46acf6c49638d9035f0820957e2efb.pdf", "supplementary_material": "/attachment/a37a2670fafc188f396f5fc0dfac7f57f9a8fb9f.zip"}, "replies": [{"content": {"summary": {"value": "This paper challenges the conventional reinforcement learning paradigm, wherein policy learning proceeds without explicit consideration of its subsequent evaluation. The authors argue that verifiable policy evaluation is an essential prerequisite for deploying systems in safety and performance-critical applications.  To address this limitation, the paper proposes a novel framework named Evaluation-Aware Reinforcement Learning (EvA-RL). This framework introduces a dual objective: the policy is trained not only to maximize the expected return but also to minimize the expected evaluation error relative to a given value prediction scheme. The paper substantiates this framework with several key theoretical insights. These include an analysis of the problem's structure, demonstrating the convexity of a relaxed formulation and the existence of multiple optimal value functions. Additionally, the authors formally characterize the inherent tradeoff between policy performance and evaluation accuracy, explore the relationship between hard and soft-constrained versions of the problem, and establish a formal bound on the performance estimation error."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel framework that incorporates evaluation as a part of the learning process, which ensures safety in deploying systems (intuitively).\n- Systematic theoretical analysis gives a deeper insight into problem formulation and solution. (Haven't been able to verify the proofs)\n- Strong and Diverse Empirical Results"}, "weaknesses": {"value": "- Assumption about an available safe assessment environment is a strong one, in my opinion. The paper doesn't provide a strong argument or evidence for how to design a good assessment environment, or how to trust that it's predictive of the real world. \n- While the error is reduced, there appears to be less insight on the trade-off with respect to samples and computation, which is crucial to understand the practical use of this method.\n- While the paper uses standard complex benchmarks (like HalfCheetah, Reacher, and Ant), it fails to test a true \"reality gap.\" The \"assessment environment\" and \"deployment environment\" are explicitly set to have identical dynamics (Page 7, lines 319-321). A much stronger test of the framework's utility would be to use a clean simulator as the \"assessment environment\" and a more complex, noisy, or physically different version as the \"deployment environment\" to see if the method can handle the domain mismatch central to its own motivating examples.\n- The paper critiques OPE for failing on long-horizon tasks but then uses very short assessment horizons (10-25 steps {on page 7: Assessment environment design}). It's unclear if this \"cheap\" evaluation is still predictive when the deployment task is thousands of steps long. An experiment on a truly long-horizon task would be needed to validate this.\n- The paper repeatedly motivates itself with \"safety-critical\" applications but never evaluates safety. The experiments only measure return (performance) and Mean Absolute Error (evaluation accuracy). A key missing evaluation would be to show that an EvA-RL policy is provably safer or easier to certify, perhaps by showing it avoids certain catastrophic states, even if a standard RL policy gets a slightly higher score."}, "questions": {"value": "- Given such low-cost or free-to-evaluate environments like simulation, could other approaches or baselines benefit from this?\n- How does this work compare to traditional RL, which, if given the ability to explore each (s,a) infinitely often, can converge to an optimal policy in all possible scenarios.\n- Given that assessments can be carried out only on a few easy-to-evaluate scenarios, when is it useful to use this approach compared to other baselines that do not rely on this condition?\n- What happens when the deployment scenario is not easy to evaluate? \n- Could the authors elaborate on why modifying the policy to be 'easy to evaluate' (EvA-RL) is a more promising direction than the alternative of developing more powerful, general-purpose evaluators (like more robust OPE techniques) that could handle complex policies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FHhQfhzEPu", "forum": "2G1PoH1YHQ", "replyto": "2G1PoH1YHQ", "signatures": ["ICLR.cc/2026/Conference/Submission9342/Reviewer_3SWF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9342/Reviewer_3SWF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760906572414, "cdate": 1760906572414, "tmdate": 1762920972560, "mdate": 1762920972560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "EvA-RL adds a term that explicitly penalizes value-estimation error to the usual return objective, jointly training a transformer-based critic and the policy. Theory and experiments on standard benchmarks show it cuts evaluation error versus prior methods while keeping policy performance competitive."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors provide a mathematical formalization and theoretical proof of EvA-RL\n- The idea of employing estimator from an related environment to aid the policy of learning in the target environment are somewhat interesting, but it is not actually validated."}, "weaknesses": {"value": "- The phrase “Evaluation-Aware RL” may over-state the contribution. RL agents are intrinsically driven by an evaluation signal (the return); the paper’s focus is narrower—improving the value-function estimator. A title that highlights “value-estimation aware” or “uncertainty-driven value estimation” might reflect the content more accurately.\n- The manuscript employs off-policy evaluation tools such as doubly-robust (DR) estimators inside an otherwise on-policy algorithm. Please clarify why these estimators are preferable to standard TD-critic updates when fresh on-policy data are available. Classical theory suggests that TD should yield lower-variance estimates in this regime; a principled justification (e.g., bias-variance trade-off, partial observability, or sample reuse) would strengthen the narrative.\n- The current experiments show only marginal gains over strong actor–critic baselines. The whole story of using an off-policy estimator instead of an on-policy optimization for the value function is very confusing, and questionable."}, "questions": {"value": "see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3XvxIoeyQt", "forum": "2G1PoH1YHQ", "replyto": "2G1PoH1YHQ", "signatures": ["ICLR.cc/2026/Conference/Submission9342/Reviewer_EFMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9342/Reviewer_EFMK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709885082, "cdate": 1761709885082, "tmdate": 1762920972236, "mdate": 1762920972236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an evaluation-aware RL, which jointly optimizes for high return and low evaluation error under a value-prediction model. The authors show the existence of a tradeoff and mitigate it by co-training a transformer-based state-value predictor alongside the policy. \n\nThe paper’s motivation is weak, and its claimed analytical novelty is highly questionable. The authors introduce two separate MDPs, one for policy evaluation and another for policy deployment, without providing any meaningful theoretical or practical connection between them. Even more puzzlingly, their analysis later assumes these MDPs are identical and deterministic, which undermines the entire premise of the proposed framework. Much of the exposition is obscured by dense jargon and vague formulations that fail to convey clear technical insight. Overall, the conceptual foundation is inconsistent, the theoretical contribution is minimal, and the results add little value beyond existing work. I see no feasible path to a meaningful revision, and therefore I strongly recommend rejection."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "NA"}, "weaknesses": {"value": "- Weak motivation, especially when compared to existing work in robust learning.\n- Theoretical analysis is superficial and rests on unrealistic assumptions (e.g., deterministic identical MDPs).\n- Writing is polished in form but often obscure in substance: overly dense, filled with jargon, and difficult to follow."}, "questions": {"value": "- What is the rationale for introducing two separate MDPs (assessment and deployment) if the theoretical analysis ultimately assumes they are identical and deterministic?\n\n- How does this framework differ from the extensive literature on robust reinforcement learning, where discrepancies between simulation and deployment environments are explicitly modeled? In what sense is your formulation novel relative to those approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zcJzYqVM7J", "forum": "2G1PoH1YHQ", "replyto": "2G1PoH1YHQ", "signatures": ["ICLR.cc/2026/Conference/Submission9342/Reviewer_vrTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9342/Reviewer_vrTj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926235641, "cdate": 1761926235641, "tmdate": 1762920971767, "mdate": 1762920971767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all the reviewers"}, "comment": {"value": "We thank all reviewers for their careful reading and constructive feedback. Reviewers n3me and 3SWF highlight the conceptual novelty of explicitly incorporating evaluatability into policy learning; reviewers n3me, EFMK, and 3SWF appreciate our theoretical analysis; reviewer 3SWF finds the empirical results strong and diverse; and reviewer n3me finds the writing generally clear.\n\nThe main concerns focus on (i) the determinism assumptions in the theory, and (ii) the practicality and role of assessment vs. deployment environments. We have addressed these concerns in the revised draft (changes denoted with blue color) as detailed below.\n\n---\n### On the determinism assumption in the theoretical section:\n\n**Response:** We have updated the draft to address this concern:\n\n- **Main text (Section 3, Method)**: Added a forward reference explaining that the deterministic analysis naturally extends to stochastic settings by conditioning on assessment state-values rather than single returns.\n\n- **Appendix A.7 (new subsection)**: Added a complete \"Extension to Stochastic Environments and Policies\" that rigorously shows how the predictor generalizes to stochastic settings using assessment state-values. We explain how multiple assessment rollouts can approximate these values at low cost, and how empirical feeding of returns into the transformer serves as a single-sample approximation.\n\nThis removes the highly restrictive determinism assumption while preserving all theoretical insights.\n\n---\n\n### Practical considerations on assessment and deployment environments\n\n**Response:** We have strengthened the discussion\n in the revised draft:\n\n- **Main text (Section 3, Method)**: Added a new subsection \"Practical construction of assessment environments\" that explains:\n  - Real-world examples: CARLA for autonomous driving, ODE-based virtual diabetic patients\n  - Conceptual view as \"unit tests\" analogous to driver's tests\n  - Common setting where assessment shares dynamics but has different initial-state distribution\n\nThis clarifies how assessment environments are constructed in practice and their role in EvA-RL.\n\n\n## New Additions to the Draft\n\n### 1. Game-theoretic analysis of EvA-RL with co-learned predictor\n\n**In response to reviewer 3SWF's question about characterizing behavior under extensive exploration:**\n\nWe have added a complete game-theoretic analysis to the draft:\n\n- **Main text (Section 3, Method)**: Added Theorem 1 stating that co-learning converges to an equilibrium with (i) near-optimal performance and (ii) near-perfect evaluation, with both guarantees holding as optimization iterations increase (independent of β).\n\n- **Appendix C (new subsection)**: \n  - Added \"Game-Theoretic Analysis of EvA-RL with Co-Learned Predictor\" modeling the framework as a Stackelberg game (policy as leader, predictor as follower)\n  - Provided formal proof showing convergence to Nash equilibrium with the desired properties\n  - Added discussion explaining why co-learning enables the predictor to track the evolving policy\n\nThis provides rigorous theoretical justification for the empirical effectiveness of co-learning.\n\n---\n\n### 2. Safety application: Deployment gating via gridworld case study\n\n**In response to reviewer 3SWF's request for elaboration on safety applications:**\n\nWe have added a complete case study demonstrating deployment gating:\n\n- **Main text (Section 4.3, new subsection)**: Description of the safety application with figure showing the deployment (4×4 gridworld) and assessment (2×2 gridworld) environments, along with forward reference to appendix for full details\n\n- **Appendix G (new section)**: Complete \"Safety Application: Deployment Gating via Gridworld Case Study\" including:\n  - 4×4 deployment gridworld with start state S=(3,0), goal G=(0,3) [+1], and lava L=(0,1) [-10]\n  - 2×2 assessment gridworld with three start states A1, A2, A3, goal G=(1,1) [+1], and lava L=(1,0) [-10]\n  - Assessment horizon of 1 for cost-effective evaluation\n  - Three safe training policies used to fit a linear value predictor\n  - Two test policies: one safe (correctly approved with $\\hat{J}=+1$) and one risky (correctly rejected with $\\hat{J}≈-2.67$)\n  - Demonstrates that assessment behavior successfully gates deployment to exclude high-risk policies without additional deployment rollouts\n\nThis case study demonstrates EvA-RL's practical utility for AI safety through efficient deployment gating.\n\nThe remaining responses are provided to individual reviewer's comment.s"}}, "id": "ervFQqFWNE", "forum": "2G1PoH1YHQ", "replyto": "2G1PoH1YHQ", "signatures": ["ICLR.cc/2026/Conference/Submission9342/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9342/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9342/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763556913782, "cdate": 1763556913782, "tmdate": 1763557299014, "mdate": 1763557299014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Evaluation-Aware Reinforcement Learning (EvA-RL), an approach that trains policies not only for high return but also for reliability under value prediction.\nThe idea is to make learned policies easier to evaluate by jointly optimizing the policy and an auxiliary evaluator that measures prediction error.\nTheoretical analysis explores the trade-off between performance and evaluation accuracy, and experiments on several control benchmarks aim to show that the approach yields more stable and predictable outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel conceptual direction: incorporating evaluation reliability directly into policy optimization.\n\n2. The theoretical section is mathematically consistent and illustrates a clear trade-off between performance and predictability.\n\n3. Writing and figures are generally clear."}, "weaknesses": {"value": "1. The analysis assumes deterministic transitions and reward, deterministic policies, linear evaluators with non-negative features, and uniform finite evaluation distributions.\nThese choices make the proofs tractable but confine the theory.\n\n2. Although $\\pi$ is formally defined as stochastic ($\\pi:\\mathcal{S}\\to\\Delta(\\mathcal{A})$), the derivations later treat $\\pi(s)$ as a deterministic action variable — an inconsistency that should be clarified.\n\n3. The introduction claims that “In either case, OPE assumes that the support of the behavior policy fully\ncovers the support of the evaluation policy”, yet recent work shows that a relaxed, transition-weighted support condition suffices: \n$$ \\mu(a|s)=0\\implies \\pi(a|s)q(s,a)=0$$ as proved in Theorem 1 of (1) and Lemma 2 of (3).\nThe manuscript should update this discussion and clarify which support notion its argument actually relies on.\n\n4. The paper omits the growing line on behavior-policy design for OPE, which explicitly seeks to optimize data collection to satisfy relaxed support conditions and improve evaluation reliability.\nThis direction is directly relevant to EvA-RL’s motivation and should be discussed (e.g., (1), (2), (3)).\n\n(1) Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design.\nShuze Daniel Liu, Shangtong Zhang. (ICML)\n\n(2) Data-efficient policy evaluation through behavior policy search .\nJosiah P. Hanna, Philip S. Thomas, Peter Stone, Scott Niekum. (ICML)\n\n(3) Doubly Optimal Policy Evaluation for Reinforcement Learning.\nShuze Daniel Liu, Claire Chen, Shangtong Zhang.(ICLR)"}, "questions": {"value": "1. Does the theoretical analysis extend to genuinely stochastic policies, or is it specific to deterministic settings?\n\n2. In Figure 5, only the Reacher bar appears to include a standard-deviation (error) bar, while HalfCheetah and Ant do not.\nCould the authors clarify whether the variance was computed for all three environments and, if so, why the corresponding uncertainty bars are omitted from the plot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AK29JmZTOL", "forum": "2G1PoH1YHQ", "replyto": "2G1PoH1YHQ", "signatures": ["ICLR.cc/2026/Conference/Submission9342/Reviewer_n3me"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9342/Reviewer_n3me"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977953178, "cdate": 1761977953178, "tmdate": 1762920971337, "mdate": 1762920971337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}