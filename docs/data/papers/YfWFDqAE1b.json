{"id": "YfWFDqAE1b", "number": 9016, "cdate": 1758107355216, "mdate": 1759897747996, "content": {"title": "From Forgetting to Robustness: Robust Class-Incremental Learning with CLIP", "abstract": "Class-Incremental Learning (CIL) aims to enable model to continuously recognize new categories without forgetting previously learned noes. While most existing methods focus on alleviating catastrophic forgetting, they largely overlook the vulnerability of CIL models to adversarial perturbations, which poses a critical threat to their reliability in real-world applications. Motivated by this oversight, we formalize a new problem setting, Robust Class-Incremental Learning (RCIL).  To address the conflict between adversarial robustness and class-incremental learning, we propose Selective parameter optimization for Adversarial training with GEometric constraint (SAGE), which selectively updates critical parameters to protect knowledge learned from previous tasks. Beyond parameter efficiency, SAGE introduces a theoretically grounded geometric constraint together with a contrastive loss to preserve structural relationships among features. This design enables stable and robust learning across tasks under adversarial attacks. Extensive experiments demonstrate that SAGE effectively improves adversarial robustness while mitigating catastrophic forgetting, leading to more reliable and practical CIL models. The code is provided in the supplementary material.", "tldr": "", "keywords": ["Robust Class-Incremental Learning; Class-Incremental Learning; Adversarial Robustness"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0be27345453e95e8f239bcbf4fc9637a6a8f1e6f.pdf", "supplementary_material": "/attachment/508d52ea0e4152e3dea2ac36f018a46fa788e9c0.zip"}, "replies": [{"content": {"summary": {"value": "This paper reinterprets the concept of combining Adversarial Training (AT) and Class-Incremental Learning (CIL) by leveraging the CLIP framework, extending ideas originally explored in TABA and FLAIR. While the prior works approached AT + CIL through conventional vision backbones (like ResNet), this paper introduces the notion of RCIL (Robust Class-Incremental Learning)—performing AT + CIL directly within CLIP. The proposed method, SAGE (Selective parameter optimization for Adversarial training with GEometric constraint), selectively updates only the most critical parameters during adversarial learning while enforcing a geometric constraint to align clean and adversarial embeddings. By applying the CLIP-based contrastive paradigm to incremental and adversarial settings, SAGE bridges robustness and continual learning more effectively than earlier AT + CIL frameworks, yielding a more practical solution for AT + CIL.\n\nAfter reviewing the main paper, supplementary material, and attached code, I find the work to be generally well-supported and leaning toward acceptance; however, several weaknesses and open questions remain, which I outline below."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "**1. Well-structured and comprehensive framework**\n\nThe proposed framework — AT + CIL on CLIP (referred to as RCIL) — is well constructed, with a solid experimental setup and clearly chosen baselines. The authors include reasonable comparative methods such as AT fine-tuning approaches on CLIP (e.g., TeCoA, FARE), and CLIP-based CIL variants (e.g., R-SG). Moreover, previous AT + CIL methods such as FLAIR are systematically reinterpreted within a unified RCIL framework, extending their original residual-family formulation to a CLIP-based setting. Overall, the experiments are formalized and carefully structured, providing a consistent and standardized evaluation of the proposed framework.\n\n**2. Simple yet effective and computationally efficient design**\n\nThe proposed method is simple yet effective, offering a practical solution to the high computational burden typically associated with AT. By selectively updating only the most important parameters, SAGE reduces unnecessary computation while maintaining both robustness from forgetting and robust learnability in the RCIL setting. \n\n**3. Reproducibility and implementation clarity**\n\nThe paper provides clear implementation details and includes well-structured pseudo-code in Algorithm 1, along with the released code in the supplementary materials. After examining both the text and the attached code, I find that the overall training pipeline is logically organized and reproducible in principle. (Note: I did not directly execute the provided code to verify the numerical results in the tables.)\n\nConsidering these three strengths, the paper establishes a solid and credible foundation for future research on AT + CIL. In this sense, it could serve as an important baseline and reference point for subsequent studies."}, "weaknesses": {"value": "## Major Weaknesses\n**1. Conceptual limitations in the Observation section**  \nWhile the Observation section (Sec. 4.1) offers an intuitive theoretical framing, its formulation in Eq. (8) oversimplifies the notion of robustness. The objective assumes that minimizing the input-gradient term \n$\n\\|\\nabla_x f_{\\theta_t}(x_t)\\|^2\n$ \nleads to improved robustness. However, this assumption only holds if the model already produces correct predictions on clean samples at task $t$. If the model fails to learn the clean data properly, reducing its input gradients merely enforces local flatness around an incorrect decision region, offering no true robustness improvement. More importantly, Eq. (8) is introduced as a conceptual formulation but is never explicitly optimized or referenced later in the method.\n\n\n**2. Potential implementation inconsistency in Table 2 results**  \n\nTable 2 reports nearly zero clean accuracy and zero BWT for methods such as R-LwF-MC and FLAIR on S-CIFAR100 and S-TinyImageNet. A BWT of 0 combined with almost 0 final accuracy indicates that the model likely failed to learn the first task at all, rather than preserving previous knowledge. The paper attributes this to the use of BCE loss instead of CE, but this explanation appears insufficient. In the original FLAIR paper, experiments under ResNet-18 did not exhibit such collapse. \nIt appears that the unexpected behavior might be related to how BCE is implemented in the attached code. BCE is applied after feature normalization within the embedding space, which could potentially reduce the logit magnitude and make optimization unstable. While this is only a conjecture, clarifying whether feature normalization is related to the BCE computation would help ensure interpretability of the reported results. \n\n## Minor Weaknesses \nThe following points are relatively minor and do not significantly affect my overall evaluation or score. If the relevant experimental data are not already available, there is no need to conduct additional experiments.\n\n**1. Adversarial strength selection in training and evaluation**  \nThe choice of $\\epsilon = 1/255$ for adversarial training appears rather weak compared to common settings in similar CLIP-based robust learning studies (e.g., FARE uses $\\epsilon = 4/255$). In Table 3, it is also unclear whether the results averaged across $\\epsilon = \\{1/255, 2/255, 4/255\\}$ are all obtained from models trained with $\\epsilon = 1/255$, or if separate models were trained for each setting. Training and evaluating models under matched $\\epsilon$ values (e.g., $1/255$–$1/255$, $2/255$–$2/255$, $4/255$–$4/255$) and reporting each table separately would make the robustness analysis clearer and more interpretable.\n\n**2. Backbone limitation**  \nAll experiments are conducted solely on the ViT-B/32 backbone. It would be informative to evaluate whether the proposed SAGE framework maintains similar robustness-forgetting trade-offs on larger or stronger backbones such as ViT-L/14, as done in prior work like FARE in the context of CLIP robust finetuning. \n\n**3. Numerous typos and inconsistent terminology**  \nExamples include:  \n- “noes” → should be “ones” (Abstract, 012)  \n- “FLIAR” → inconsistent with the main text’s “FLAIR” (Related Work, 064)\n- “text prompt prompt” → redundant phrase (Training details, 359)  \n- “Similarity, the vector b ...” → should be “Similarly, the vector b ...” (Appendix C, 754)\n- “derived form” → should be “derived from” (Appendix H.1.4, 966)  \n- “adversarail” → should be “adversarial” (Appendix H.3.2, 1082)  \n- “us applied” → should be “is applied” (Appendix H.3.2, 1092)  \n- “sigmod” → should be “sigmoid” (Appendix H.4.2, 1143)\n- Notation inconsistencies in equations, such as the inconsistent use of BCE in Eq. (39)"}, "questions": {"value": "1. Is there a specific reason for using 2 iterations with a step size of 1/255 (equal to the attack budget) for training attacks, and 10 iterations with the same step size for test attacks? This setup seems somewhat different from the conventional AT practice, where 10–20 PGD steps with a smaller step size than the attack budget are typically used.\n\n2. In the original FLAIR paper, data augmentation was reported to improve performance (FLAIR+). Does this observation also hold for SAGE, or have you noticed a similar benefit when applying data augmentation?\n\n3. Robust fine-tuning methods for CLIP like FARE exhibit a forgetting of “natural” features. Since SAGE also fine-tunes a pre-trained CLIP, could you comment on whether similar forgetting occurs at the first task? In particular, how much does the zero-shot CIFAR-10 clean accuracy of the pre-trained CLIP drop after training only the first task with SAGE?\n\n4. If SAGE were initialized not from vanilla CLIP but from a model that has already been robustly fine-tuned on ImageNet (via FARE), would you expect further gains?\n\n5. Since SAGE is built upon CLIP, have you explored or considered its zero-shot accuracy or robustness, similar to prior CLIP-based robust finetuning like FARE?\n\n6. In Equation (28) in Section H.1.2, it seems that the loss function of FARE may be incorrectly formulated. Shouldn't the adversarial input be fed into the current task model? \n\n7. “This is a photo of {}” is the commonly used CLIP prompt as noted in L359. However, you set the prompt to \"a good photo of a {}\" at L1093 and to \"a bad photo of {}\" at L1114. Is there a specific reason to prefer the “good/bad photo” templates in these sections? Have you checked how sensitive the results are to the prompt template—e.g., replacing them with the standard \"This is a photo of {}\"? \n\n\nOverall, I am leaning toward accept, as the paper proposes a clear and meaningful contribution.\nHowever, several technical and clarity concerns remain.\nIf these issues are adequately addressed during the rebuttal, it may lead me to adjust my score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Dcw4LAeR8n", "forum": "YfWFDqAE1b", "replyto": "YfWFDqAE1b", "signatures": ["ICLR.cc/2026/Conference/Submission9016/Reviewer_j5be"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9016/Reviewer_j5be"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760585673768, "cdate": 1760585673768, "tmdate": 1762920741201, "mdate": 1762920741201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAGE (Selective parameter optimization for Adversarial training with GEometric constraint), a novel framework for Robust Class-Incremental Learning (RCIL) based on CLIP. The authors formalize RCIL as a new problem setting that jointly tackles catastrophic forgetting and adversarial robustness. SAGE selectively updates important parameters identified by gradient–weight products while enforcing a geometric-constraint-based contrastive loss to preserve feature structure and mitigate forgetting. Extensive experiments on CIFAR-10/100, STL-10, and Tiny-ImageNet demonstrate that SAGE achieves superior robustness and lower forgetting compared with both classical and CLIP-based CIL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formally defines the RCIL problem for the first time and provides theoretical insight into the intrinsic conflict between adversarial robustness and knowledge retention.\n\n2. SAGE integrates two elegant ideas: (1) selective parameter optimization based on gradient–weight importance to control stability-plasticity trade-off, and (2) a geometry-constrained symmetric contrastive loss that preserves inter-task feature consistency. \n\n3. The authors conduct comprehensive experiments across four datasets and multiple baselines (e.g., FARE, R-LwF, FLAIR). The results are consistently strong, with up to 15–20% improvement in PGD/AA robustness and substantially reduced backward transfer (BWT). Ablation studies clearly demonstrate the contribution of each module, showing a careful empirical validation.\n\n4. The use of Taylor expansions to derive the trade-off between robustness and stability gives the method a solid theoretical grounding, adding interpretability and rigor often missing in CIL literature."}, "weaknesses": {"value": "1. The framework relies on PGD-based adversarial training, which is computationally expensive and may not scale to large-scale CLIP variants or datasets such as ImageNet-1K. No efficiency comparison or training-time analysis is provided.\n\n2. Experiments are all performed on standard small-scale benchmarks (CIFAR/STL/Tiny-ImageNet). It remains unclear whether SAGE generalizes to realistic continual learning settings with large-scale, noisy, or multimodal data.\n\n3. There is no comparison with recent lightweight adversarial training or robustness transfer methods that reduce computational overhead (e.g., free/fast adversarial training). Including such baselines would enhance the claim of practicality."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2bJ5gNEdeE", "forum": "YfWFDqAE1b", "replyto": "YfWFDqAE1b", "signatures": ["ICLR.cc/2026/Conference/Submission9016/Reviewer_ZojM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9016/Reviewer_ZojM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447541389, "cdate": 1761447541389, "tmdate": 1762920740630, "mdate": 1762920740630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies robust class-incremental learning (RCIL), which jointly tackles class-incremental learning and adversarial robustness. It formulates RCIL as optimizing two objectives—mitigating forgetting and improving robustness to input perturbations—and proposes a novel method named SAGE to address this problem. The method has two components: selective parameter optimization, which updates only high-importance parameters based on an importance score, and a geometric constraint that introduces a loss jointly optimizing the two potentially conflicting RCIL objectives. The effectiveness of SAGE in both class-incremental performance and adversarial robustness is demonstrated across four datasets."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Formalizes the RCIL problem and explicitly states the trade-off between mitigating forgetting and enhancing adversarial robustness.\n- The proposed method-consisting of a contrastive loss and selective parameter optimization-is easy to adopt without complex implementation.\n- Specifically, the geometric-constraint guided contrastive loss is theoretically well-grounded and effectively addresses both robustness and forgetting in RCIL jointly. The ablation study on contrastive loss clearly demonstrates the effectiveness of the loss."}, "weaknesses": {"value": "- **Questionable novelty of the problem formalization** The paper presents itself as the first to formalize Robust Class-Incremental Learning; however, the cited literature seems to indicate that closely related settings have been explored. So it remains somewhat uncertain whether the current formalization represents a clear contribution.\n- **Observation section** The points presented in the observation section appear to reflect widely accepted views in CIL and adversarial robustness when considered separately, and presenting them together as Eq. (8) may not, on its own, constitute a distinct observation of the paper. In addition, the claim that the two terms in Eq. (8) are *contradictory* and involve a trade-off would need more detailed justification.\n- **Experimental Setup** is the primary weakness of the paper.\n  1. **Weak PGD configurations**  Training uses only PGD-2 and evaluation uses PGD-10, which are relatively mild settings and insufficient to establish robustness claims.\n  2. **Too small attack strength**  Similarly, The main tables evaluate at an attack strength of $\\epsilon = 1/255$, with only limited reporting at $\\epsilon \\in \\{1/255, 2/255, 4/255\\}$ (e.g., in Table 3). These budgets are much smaller than the $\\epsilon = 8/255$ commonly used in related work on this problem, making it difficult to conclude that the method is adversarially robust.\n  3. **Unexpectedly low baselines on S-CIFAR100/S-TinyImageNet**  Several methods show abnormally low accuracy and BWT on CIFAR100 and TinyImageNet. Notably, FLAIR is reported as 3.05% (clean) on CIFAR-100 with BWT = 0.00, despite the relativeness of CIFAR-10 and CIFAR-100. These anomalies raise concerns about the correctness of the baseline implementations or evaluation protocol for those settings."}, "questions": {"value": "1. Could the authors clarify in what sense the two terms in Eq. (8) are contradictory? Or is this claim primarily supported by empirical evidence (e.g., Appendix B)?\n2. Could the authors explain the choice of attack strength $\\epsilon=1/255$? Reporting results at a stronger budget ($\\epsilon=8/255$) would substantially strengthen the soundness of the paper.\n3. Could the authors comment or explain the unexpectedly low baseline results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ewuACTf7r1", "forum": "YfWFDqAE1b", "replyto": "YfWFDqAE1b", "signatures": ["ICLR.cc/2026/Conference/Submission9016/Reviewer_dSoa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9016/Reviewer_dSoa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897910096, "cdate": 1761897910096, "tmdate": 1762920740148, "mdate": 1762920740148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper highlights a neglected issue in class-incremental learning (CIL): vulnerability to adversarial perturbations. It formalizes **Robust Class-Incremental Learning (RCIL)** and introduces **SAGE**—Selective parameter optimization for Adversarial training with GEometric constraint. SAGE selectively updates only critical parameters to preserve knowledge from previous tasks, offering parameter efficiency while resisting forgetting. A theoretically grounded geometric constraint, paired with a contrastive loss, preserves structural relationships among features, enabling stable, robust learning across increments under attack. Extensive experiments show SAGE improves adversarial robustness and simultaneously mitigates catastrophic forgetting, yielding more reliable, practical CIL models. Code is included in the supplementary material."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and easy to follow.\n2. The experiments are fairly comprehensive and demonstrate the effectiveness of the proposed SAGE algorithm under the RCIL setting."}, "weaknesses": {"value": "1. The importance and novelty of RCIL are not sufficiently justified. From my perspective, given prior CIL work, introducing RCIL feels natural—even trivial—and appears to be a simple combination of CIL with conventional adversarial robustness.\n2. The baselines used to compare against SAGE are not described in adequate detail.\n3. Much of the key notation is undefined—for example, $\\mathcal{L} _{t}^{R}$ and $\\mathcal{L} _{t}^{CIL}$ around lines 158–160.\n4. Figure 1 conveys no information beyond what is already in its caption.\n5. The attack strengths of 1/255, 2/255, and 4/255 are too small and are not standard perturbation radii in the adversarial robustness literature."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k2R2itR7G2", "forum": "YfWFDqAE1b", "replyto": "YfWFDqAE1b", "signatures": ["ICLR.cc/2026/Conference/Submission9016/Reviewer_z3NJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9016/Reviewer_z3NJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909405814, "cdate": 1761909405814, "tmdate": 1762920739471, "mdate": 1762920739471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Robust Class-Incremental Learning (RCIL) method, requiring a CIL model to not only remember previous categories but also maintain adversarial robustness against malicious input perturbations across all classes learned so far. The proposed framework uses a CLIP-based approach to anchor the model in a naturally robust feature space, easing the conflict between learning new concepts and retaining robust knowledge of the old ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Successfully formalizing and addressing the critical intersection of continuous learning (CIL) and AI security.\n\n- Proposing a novel framework that systematically tackles the known challenge that Adversarial Training (AT) often leads to increased forgetting in CIL by leveraging the robust, fixed feature space of CLIP and employing a selective mask.\n\n- Demonstrating extended sets of experiments and good empirical performance against the compared baselines."}, "weaknesses": {"value": "-  Adversarial Training drastically increases training time compared to standard CIL baselines (like LwF or iCaRL) that use only clean data. This makes the method's practical application questionable for very large incremental tasks. Conduct a direct comparison of the total wall-clock training time required for the proposed RCIL method versus representative compared baselines (e.g., LwF, iCaRL, and the strongest integrated CIL+AT baseline) across the full sequence of tasks to understand the computational efficiency. Also, report the time complexity and discuss how the computational overhead introduced by the adversarial attacks and the selective mask scales with the number of classes and the number of incremental steps. or different attacks. This is critical for assessing the method's scalability. \n\n- How much of the strong performance of the RCIL framework may be primarily due to the fixed, highly robust and generalizable features provided by the large, pre-trained CLIP encoder, rather than solely the selective parameter updates of the robust incremental learning method itself is unknown.\n\n\n\n- Performance under black box attacks is unknown. Evaluate the final CIL model's adversarial robustness using the AutoAttack benchmark suite."}, "questions": {"value": "- Conduct ablation experiments to study the mask. Study the evolution of mask across tasks. What specific layers or parameters are selected for adversarial robustness and clean accuracy. Does adversarial robustness primarily rely on protecting low-level features (early layers) while forgetting (clean performance) is more sensitive to high-level features (late layers)? This provides crucial insight into the mechanism of RCIL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yfzmdlg042", "forum": "YfWFDqAE1b", "replyto": "YfWFDqAE1b", "signatures": ["ICLR.cc/2026/Conference/Submission9016/Reviewer_BwZM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9016/Reviewer_BwZM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992717908, "cdate": 1761992717908, "tmdate": 1762920739039, "mdate": 1762920739039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}