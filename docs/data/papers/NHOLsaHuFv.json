{"id": "NHOLsaHuFv", "number": 21602, "cdate": 1758319502340, "mdate": 1763742487879, "content": {"title": "LABEL-FREE MITIGATION OF SPURIOUS CORRELATIONS IN VLMS USING SPARSE AUTOENCODERS", "abstract": "Vision-Language Models (VLMs) have demonstrated impressive zero-shot capabilities across a wide range of tasks and domains. However, their performance is often compromised by learned spurious correlations, which can adversely affect downstream applications. Existing mitigation strategies typically depend on additional data, model retraining, labeled features or classes, domain-specific expertise, or external language models posing scalability and generalization challenges. In contrast, we introduce a fully interpretable, zero-shot method that requires no auxiliary data or external supervision named DIAL (Disentangle, Identify, And Label-free removal). Our approach begins by filtering the representations that might be disproportionately influenced by spurious features, using distributional analysis. We then apply a sparse autoencoder to disentangle the representations and identify the feature directions associated with spurious features. To mitigate their impact, we remove the subspace spanned by these spurious directions from the affected representations. \nAdditionally, for cases where prior knowledge of spurious features in a dataset are not known, we introduce DIAL+ which can detect and mitigate the spurious features. We validate our method through extensive experiments on widely used spurious correlation benchmarks. Results show that our approach consistently outperforms or matches existing baselines in terms of overall accuracy and worst-group performance, offering a scalable and interpretable solution to a persistent challenge in VLMs.", "tldr": "We propose an interpretable zero-shot approach to remove spurious features from vision language model representations.", "keywords": ["Spurious Correlations", "Vision-Language Models", "Interpretability", "Sparse Auto Encoders"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c6a6d0277a7b3d8882f5c5a9d12b66ee084ddec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DIAL, the motivation is to achieve robustness of zero-shot classification of VLM like CLIP. For the disentanglement, the author proposes the use of a spare autoencoder to decompose the attribution in the column space of the decoder. For identification, the author proposes the attribution score such that it can align the feature vector with the spurious concept without using spurious labels. To remove the spurious feature, the author first finds the space spanned by the spurious vector, then projects the visual embedding to this space and removes such components. The author conducts benchmark dataset evaluation on the Group robustness and overall performance. The results show it surpasses the existing SOTA, like TIE or Orth-Cali."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I found this paper quite interesting.\n\n**1** It combines the recent trend of explainability in LLM that use a sparse autoencoder to disentangle the feature vector, and align with a post-hoc explanation to find spurious feature direction. \n\n**2** The paper is well presented, and the logical flow of the paper is good. The overall soundness of the paper is good for me. \n\n**3** The author conducted the benchmark evaluation, which is at least comparable with the existing work."}, "weaknesses": {"value": "1. In Figure 4, I don't quite understand why the average Acc can be even lower than WGA  in FMOW. \n\n2. To my knowledge, sparse autoencoders (SAE) are often used in the explanation of the transformer encoder's FFN layer. I don't know how the performance would be that migrates the SAE to explain the latent representation. \n\n3. Correct me if I am wrong. I found the SAE uses pre-trained weights. Would there be any distribution shift to align with the spurious vector in the CLIP models? Why don't we train on the specific dataset?\n\n4. Equation at line 208, I think both parts could be sort of the attribution score. For the first term, it shows how the activation of the feature towards the positive spurious concept. For the second term, it shows how the feature vector aligns with the spurious text prompt. Then my question is, why do we need both terms multiplied together? Have you tried just using a single term in ablation? \n\n5. In line 239, why do we use the reconstructed embedding to remove the spurious vector, not the original embedding?"}, "questions": {"value": "(1) Related to weakness 3, I would like to see the dataset-specific SAE on the outcomes. \n\n(2) I don't quite get how you evaluate the explaniblity, as you mentioned this method is fully interpretable.\n\n(3) Figure 2 is a good motivation figure. I would also want to see the heatmap after applying this method.\n\n(4) I am also curious how the alignment between the spurious vector you found based on SAE and the spurious vector found by TIE using the spurious text prompt?\n\n(5) Can the method find the novel spurious feature? Like in ISIC, there are multiple spurious features. Can we apply DIAL to find and mitigate such spurious features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't have any ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NsriL8sDJ2", "forum": "NHOLsaHuFv", "replyto": "NHOLsaHuFv", "signatures": ["ICLR.cc/2026/Conference/Submission21602/Reviewer_TSFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21602/Reviewer_TSFp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624819611, "cdate": 1761624819611, "tmdate": 1762941851407, "mdate": 1762941851407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel, zero-shot method named DIAL to mitigate spurious correlations in Vision-Language Models. DIAL uses the VLM's own zero-shot predictions to create pseudo-labels for identifying samples likely affected by a known spurious attribute and leverages a pre-trained Sparse Autoencoder to decompose VLM embeddings to gain more disentangled, interpretable features that correspond to the spurious attribute.\nThe authors validate DIAL on five benchmark datasets using multiple VLM backbones. The results show that DIAL consistently improves worst-group accuracy over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive Empirical Evaluation:** The experimental setup is thorough and convincing.\nThe use of five standard and diverse benchmark datasets, including challenging medical and real-world scenarios, demonstrates the method's broad applicability.\n\n2. **Clarity and Presentation**: The paper is well-written, logically structured, and easy to follow."}, "weaknesses": {"value": "1. **Dependency on Pre-trained SAEs**: The method's effectiveness is largely affected by the quality of a pre-trained SAE for the given VLM backbone. The paper does not discuss the sensitivity of DIAL to the SAE's quality (e.g., degree of disentanglement, reconstruction error, sparsity level). If a high-quality, pre-trained SAE is not available for a particular VLM, the contribution of DIAL is limited.  Plus, I'm quite curious about which kinds of SAE models[1,2,3] are most suitable for spurious tasks. \n\n2. **Requirement of a Spurious Concept Description:** While label-free, the method still requires a user to provide a high-level textual description of the spurious attributes (e.g., \"Male\", \"Female\"). This assumes that the source of spurious correlation is known, which means that the method cannot discover unknown or hard-to-describe spurious features (e.g., a subtle imaging artifact without a common name). This limitation should be explicitly stated.\n\n3. **Potential for Negative Interference:**  The orthogonal projection forcefully removes any information in the direction of the spurious subspace. If a genuinely causal feature is closely aligned with a spurious one in the embedding space, this process could inadvertently harm model performance by removing useful information. I believe the paper should give a more detailed discussion or analysis of this potential failure mode.\n\n[1] BatchTopK SAE: Bussmann, Bart, Patrick Leask, and Neel Nanda. \"Batchtopk sparse autoencoders.\" arXiv preprint arXiv:2412.06410 (2024).\n\n[2] JumpReLU SAE: Rajamanoharan, Senthooran, et al. \"Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders.\" arXiv preprint arXiv:2407.14435 (2024).\n\n[3] SAE + Contrastive loss: Wen, Tiansheng, et al. \"Beyond matryoshka: Revisiting sparse coding for adaptive representation.\" arXiv preprint arXiv:2503.01776 (2025)."}, "questions": {"value": "The paper's utilization of SAE with KNN for identifying useful sparse representations for downstream tasks is a technique that has been explored in prior literature. For example, the following papers should be discussed in the related work:\n\n[1] Tian, Zhihua, et al. \"Sparse autoencoder as a zero-shot classifier for concept erasing in text-to-image diffusion models.\" arXiv preprint arXiv:2503.09446 (2025).\n\n[2] Wen, Tiansheng, et al. \"Beyond matryoshka: Revisiting sparse coding for adaptive representation.\" arXiv preprint arXiv:2503.01776 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Fr8eoLRlK2", "forum": "NHOLsaHuFv", "replyto": "NHOLsaHuFv", "signatures": ["ICLR.cc/2026/Conference/Submission21602/Reviewer_bU24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21602/Reviewer_bU24"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760963673, "cdate": 1761760963673, "tmdate": 1762941851051, "mdate": 1762941851051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VLMs often rely on spurious correlations, which can affect downstream tasks. The authors introduce DIAL, a zero-shot method that finds and mitigates spurious correlations. DIAL operates by (1) filtering representations that might be disproportionately influenced by spurious features, (2) applying a sparse autoencoder to disentangle the representations and identify feature directions associated with spurious feature, and (3) removing the subspace spanned by the spurious directions from the representations. Results across several benchmarks demonstrate the utility of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses an important problem - finding and mitigating spurious correlations learned by vision-language models\n- Results show performance improvements when compared to several baselines, suggesting utility of the approach. The method also works well across domains (i.e. general domain as well as medical domain)."}, "weaknesses": {"value": "- **Insufficient analysis:** Section 4.4 provides overall metrics across various datasets, but does not provide sufficient fine-grained analysis of results. Ablations are also limited. Ultimately, it is not clear to me *why* the method works better than baselines.\n- **Need for attribute labels:** The proposed method requires a set of candidate spurious attributes, which may not always be known ahead of time and might limit utility of the method in real-world settings.\n- **Choice of sparse autoencoder:** The authors consider one off-the-shelf pretrained sparse autoencoder for their analyses. How robust are the results to different choices of the autoencoder?"}, "questions": {"value": "Questions are listed above under weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8QEKiTmi4D", "forum": "NHOLsaHuFv", "replyto": "NHOLsaHuFv", "signatures": ["ICLR.cc/2026/Conference/Submission21602/Reviewer_aziT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21602/Reviewer_aziT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976810500, "cdate": 1761976810500, "tmdate": 1762941850759, "mdate": 1762941850759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DIAL, a label-free and zero-shot method to mitigate spurious correlations in vision-language models (VLMs). The approach uses sparse autoencoders to disentangle image embeddings and identify feature directions associated with spurious attributes. These directions are then removed via orthogonal projection to produce debiased representations. The method is evaluated on several benchmark datasets and compared against existing zero-shot debiasing techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is fully zero-shot and does not require labeled data, retraining, or external models, which improves scalability.\n2. The use of sparse autoencoders for disentangling representations is well-motivated and contributes to interpretability."}, "weaknesses": {"value": "1. Limited novelty: The core idea—removing spurious directions via projection—is conceptually similar to prior work. The use of sparse autoencoders is incremental and not fundamentally new in the context of representation disentanglement.\n\n2. Low practical impact: The spurious correlation issues addressed (e.g., background bias in Waterbirds, gender bias in CelebA) are well-known and have been extensively studied. The paper does not convincingly demonstrate that these issues remain critical in modern VLMs.\n\n3. Outdated model focus: The analysis centers on older VLMs like CLIP ViT-B. It remains unclear whether the same spurious correlation problems persist in newer models such as SigLIP, OpenCLIP, or multi-modal transformers trained with more diverse data.\n\n4. Assumption-heavy candidate selection: The method relies on pseudo-labels and centroid-based heuristics to identify biased samples, which may be unreliable in real-world settings or for more complex tasks.\n\n5. Lack of generalization evidence: The paper does not explore whether the proposed mitigation transfers across tasks (e.g., retrieval, captioning) or domains beyond the selected benchmarks."}, "questions": {"value": "Can your method be extended to mitigate spurious correlations in text embeddings or multi-modal fusion layers?\n\nIs there any evidence that your projection-based debiasing improves downstream task performance beyond classification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "63jGKlsno7", "forum": "NHOLsaHuFv", "replyto": "NHOLsaHuFv", "signatures": ["ICLR.cc/2026/Conference/Submission21602/Reviewer_ddFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21602/Reviewer_ddFq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989119408, "cdate": 1761989119408, "tmdate": 1762941850576, "mdate": 1762941850576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}