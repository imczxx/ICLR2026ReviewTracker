{"id": "7KluEfmiXG", "number": 2549, "cdate": 1757143211432, "mdate": 1763638917710, "content": {"title": "Plug, Play, and Fortify: A Low-Cost Module for Robust Multimodal Image Understanding Models", "abstract": "Missing modalities present a fundamental challenge in multimodal models, often causing catastrophic performance degradation. Our observations suggest that this fragility stems from an imbalanced learning process, where the model develops an implicit preference for certain modalities, leading to the under-optimization of others. We propose a simple yet efficiency method to address this challenge. The central insight of our work is that the dominance relationship between modalities can be effectively discerned and quantified in the frequency domain. To leverage this principle, we first introduce a **F**requency **R**atio **M**etric (FRM) to quantify modality preference by analyzing features in the frequency domain. Guided by FRM, we then propose a **M**ultimodal **W**eight **A**llocation **M**odule, a plug-and-play component that dynamically re-balances the contribution of each branch during training, promoting a more holistic learning paradigm. Extensive experiments demonstrate that MWAM can be seamlessly integrated into diverse architectural backbones, such as those based on CNNs and ViTs. Furthermore, MWAM delivers consistent performance gains across a wide range of tasks and modality combinations. This advancement extends beyond merely optimizing the performance of the base model; it also manifests as further performance improvements to state-of-the-art methods addressing the missing modality problem.", "tldr": "", "keywords": ["Modality Missing", "Plug-and-play module", "multimodal imbalanced learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cbcdee3a09779d1f200dfeb42180869a240d04a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel approach to multimodal learning that applies the Discrete Cosine Transform (DCT). \nThe purpose of using DCT is to extract the low- and high-frequency components of each modality in the frequency domain, thereby capturing the intrinsic characteristics of each modality. \nThis approach is employed in two key modules:\n\n- **Frequency Ratio Metric (FRM)**: Measures the dominance of each modality.\n- **Multimodal Weight Allocation Module (MWAM)**: Dynamically allocates modality weights within each batch to determine the relative contribution of each modality.\n\nThis design leads to improved performance in several experimental settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Strength 1**: Preprocessing input data in the frequency domain can enhance efficiency in capturing the power and meaningful information of each modality within the input space.\n- **Strength 2**: Since this approach focuses on extracting informative representations from the input space, it can be broadly and effectively applied to various vision tasks, such as segmentation, classification, and object detection."}, "weaknesses": {"value": "- **Weakness 1**: Although this paper focuses on multimodal learning, its proposed approach is primarily applicable to vision-based tasks. It might seems difficult to extend the method to other modalities, such as text, where applying DCT directly is not feasible. Additional processing steps would be required to make DCT applicable to such modalities.\n- **Weakness 2**: The method involves a large number of hyperparameters that can significantly influence the results. For example, variations in $\\alpha$, $\\beta$, $\\lambda$, $\\gamma$, and $\\sigma$ in Eq.(4) and (5) may lead to substantially different outcomes. This complexity makes it challenging to determine which hyperparameters are critical or not. Although the authors provide ablation studies in Table 9, a clearer justification of the importance of each hyperparameter would strengthen the work. Otherwise, it would be better to change the reflected sigmoid function used in the weight allocation process.\n- **Weakness 3**: The performance analysis lacks clarity in certain aspects. For instance, in Table 2, results for the vanilla GSS model are missing, even though GSS + MWAM achieved the best overall performance. The authors compare this with LS3M; however, since GSS and LS3M are fundamentally different approaches, this comparison may not be entirely fair. It would be more appropriate to compare Original vs. Original + MWAM to more fairly and effectively highlight the contribution and novelty of MWAM. Same comments for other Tables.\n- **Weakness 4**: The overall structure of the paper and writings could be improved for better coherence. For instance, in Section 3.1, the authors present empirical analyses on the influence of different frequency components, while all theoretical discussions are deferred to the appendix. Given that one of the paper’s novelty is the theoretical and empirical validation of modality dominance, I think it is required to include at least a concise summary or key results of the theoretical validations within the main text, even if the detailed derivations remain in the appendix.\nAdditionally, I would recommend that important equations and their explanations be formatted like Definitions, Lemmas, and Theorems. (e.g., Appendix A.1)\n\n\nMinor Issues:\n\nWords change at Abs \\& Conclusion: **efficiency** method $\\Rightarrow$ **efficient** method\n\nTypo at line 296: show the sensitivity **o** these factors  $\\Rightarrow$ show the sensitivity **of** these factors"}, "questions": {"value": "- **Question 1**: What is the motivation for applying the Discrete Cosine Transform (DCT) instead of other frequency-domain transforms such as the Discrete Fourier Transform (DFT) or Fast Fourier Transform (FFT)? In vision tasks, DFT/FFT captures both magnitude and phase information, whereas DCT focuses on real-valued frequency components and is often used for compression. It would be helpful to understand why DCT was specifically chosen in this context and how it benefits the representation of visual modalities.\n- **Question 2**: What are the main differences between this paper and other frequency-domain deep learning approaches? Previous studies have already applied Fourier transforms either between layers [1] or at the input space [2]. From the paper, the primary difference seems to be the use of DCT instead of FFT. Since both operate in the frequency domain, this substitution alone does not convincingly demonstrate methodological novelty. \n- **Question 3**: What is the rationale for using the reflected sigmoid function with multiple hyperparameters in the Eq. (4)? This design choice seems critical, as it raises concerns that the observed performance improvements might depend heavily on hyperparameter tuning rather than the proposed MWAM mechanism itself. I could not find a clear explanation or reference supporting this design choice, and those 4 hyperparameters.\n- **Question 4**: Have the authors considered evaluating MWAM on datasets that include additional modalities such as audio or text? This would help demonstrate the generalizability of the proposed approach beyond purely visual modalities.\n\n[1] Guo, Shi, et al. \"Spatial-frequency attention for image denoising.\" (preprint)\n\n[2] He, Xilin, et al. \"Towards combating frequency simplicity-biased learning for domain generalization.\" (NeurIPS 2024)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yD8RWXNhnu", "forum": "7KluEfmiXG", "replyto": "7KluEfmiXG", "signatures": ["ICLR.cc/2026/Conference/Submission2549/Reviewer_K6MD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2549/Reviewer_K6MD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761111479447, "cdate": 1761111479447, "tmdate": 1762916276578, "mdate": 1762916276578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles robustness in multimodal image understanding, focusing on cases where some modalities are missing at inference. The authors show that models often become biased toward dominant modalities, causing severe performance drops when those are absent. To address this, they propose Frequency Ratio Metric (FRM) and Multimodal Weight Allocation Module (MWAM). Experiments across multiple tasks and architectures demonstrate that MWAM improves robustness and outperforms several state-of-the-art methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Using frequency domain analysis (FRM) to diagnose and mitigate modality bias sounds interesting, which goes beyond spatial domain balancing.\n2) The proposed MWAM is a lightweight and plug-and-play module with negligible computational overhead and no additional parameters during inference. This makes it attractive for real-world deployment.\n3) The method is validated on multiple tasks (classification, segmentation, detection) and datasets, showing improvements over baseline  methods.  Extensive ablation studies on FRM design, window size, parameter sensitivity, and training setting are ablated."}, "weaknesses": {"value": "1. The approach is tailored to image-based modalities and frequency domain analysis. It is unclear how well MWAM would generalize to other multimodal settings (e.g., audio-text, video-language) where frequency analysis may not be directly applicable\n2. MWAM introduces several hyperparameters that require careful tuning for different tasks and batch sizes. While sensitivity analysis is provided and the authors claim the performance is not that sensistive to such hyper-parameters, considering the performance gain over baseline method is not that significant, such performance perturbations is not neglibile. However, there is no clear guidance about how to set such hyper-parameters."}, "questions": {"value": "1) Can MWAM and FRM be adapted for multimodal tasks involving non-image data (e.g., audio, text, tabular)? What are the challenges and potential solutions?\n2) How does MWAM handle cases where modalities are highly correlated or redundant? Is there a risk of over-balancing and degrading performance?\n3) Are there specific scenarios or datasets where MWAM fails to improve robustness or even degrades performance? If so, what are the underlying causes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kv1rWrOemb", "forum": "7KluEfmiXG", "replyto": "7KluEfmiXG", "signatures": ["ICLR.cc/2026/Conference/Submission2549/Reviewer_bfT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2549/Reviewer_bfT5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967802643, "cdate": 1761967802643, "tmdate": 1762916276027, "mdate": 1762916276027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a common problem in multimodal learning: performance drops significantly when one of the data modalities is missing.\nThe authors argue this happens because the model learns to rely too much on one \"dominant\" modality during training, while neglecting the others. Their key idea is that this dominance can be measured in the frequency domain of the images.\nTo solve this, they propose:\nA Frequency Ratio Metric (FRM): A simple metric to quantify how much the model prefers each modality by comparing its low-frequency and high-frequency content.\nA Multimodal Weight Allocation Module (MWAM): A lightweight, plug-and-play module that uses the FRM to rebalance the training process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper analyzes the modality imbalance problem from the frequency domain and introduces an FRM to quantify how much the model prefers each modality. This seems to be promising. \n\nBased on FRM, this paper introduces MWAM to modulate the training process of multimodal models so that the performance can be balanced and enhanced.\n\nThe writting is easy to understand."}, "weaknesses": {"value": "\"The overall training process becomes more stable, as evidenced by the reduced variance in the total loss curve\". But in Figure. 4 SF-MD (w/o Intervention)'s total loss is more stable than that of SF-MD (w / MD (w / Loss Intervention ntervention)."}, "questions": {"value": "Consider this paper does not utilize LLMs to perform understanding tasks, so it more like image perception work.\n\n\"SF-MD, we additionally introduce auxiliary heads, whereas we do not in MMANet.\" Why introduce extra heads for SFMD only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JoFik73g9v", "forum": "7KluEfmiXG", "replyto": "7KluEfmiXG", "signatures": ["ICLR.cc/2026/Conference/Submission2549/Reviewer_gN31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2549/Reviewer_gN31"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994190077, "cdate": 1761994190077, "tmdate": 1762916275816, "mdate": 1762916275816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of modality bias and missing modality robustness in multimodal image understanding models. The authors observe that current multimodal systems tend to overfit or over-rely on certain “dominant” modalities (e.g., depth or RGB), leading to severe performance drops when one modality is missing at inference time. To address this, the paper introduces Frequency Ratio Metric and Multimodal Weight Allocation Module. Extensive experiments on segmentation (BRATS2020, NYU-Depth V2) and classification (CASIA-SURF) benchmarks demonstrate that MWAM consistently improves robustness and performance across CNN- and ViT-based architectures with minimal computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- he idea of diagnosing and correcting multimodal imbalance in the frequency domain is both intuitive and underexplored. The FRM formulation provides a new angle that complements existing spatial-domain balancing techniques.\n- MWAM is architecture-agnostic, parameter-light, and easy to integrate into existing backbones. This makes it particularly attractive for practitioners working on robustness in multimodal models.\n- The authors validate their method on diverse datasets and tasks (segmentation and classification) and show consistent improvements in both accuracy (Dice, MIoU, Acc) and robustness (PCR).\n- The ablation studies (gradient vs. loss vs. hybrid intervention) clearly show why hybrid balancing works best. The training loss visualizations (Fig. 4) effectively illustrate MWAM’s stabilizing effect."}, "weaknesses": {"value": "- While the frequency-domain motivation is intuitive, the paper lacks a rigorous theoretical connection between FRM and gradient dynamics. A more formal justification for why FRM effectively measures modality dominance would strengthen the contribution.\n- All experiments are on moderate-sized datasets. It remains unclear whether FRM and MWAM scale effectively to modern multimodal foundation models.\n- Some figures and equations are dense and could be better formatted for readability."}, "questions": {"value": "- How sensitive is the FRM metric to the choice of frequency cutoff (p × p patch size and q × q block size)? Could adaptive frequency selection further improve the metric?\n- How does MWAM compare to feature-level reweighting techniques like modality dropout, dynamic fusion gates, or attention-based balancing in terms of convergence stability?\n- Would FRM or MWAM also help in text–image or video–audio multimodal models, or is the current formulation specific to visual modalities only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RAdqjvX9no", "forum": "7KluEfmiXG", "replyto": "7KluEfmiXG", "signatures": ["ICLR.cc/2026/Conference/Submission2549/Reviewer_Lu1c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2549/Reviewer_Lu1c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141149077, "cdate": 1762141149077, "tmdate": 1762916275530, "mdate": 1762916275530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}