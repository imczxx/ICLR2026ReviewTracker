{"id": "DUXG9E8dEO", "number": 19334, "cdate": 1758295429693, "mdate": 1763754073727, "content": {"title": "Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution", "abstract": "Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.", "tldr": "", "keywords": ["Contrastive learning", "Feature learning", "Training dynamics", "Theoretical analysis"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44756adff740ad81480aad2f03757783a2090a9e.pdf", "supplementary_material": "/attachment/1c08cd9fb244c3cc4a2d8d41b0bce67e7f6afa4d.zip"}, "replies": [{"content": {"summary": {"value": "This paper develops a theoretical framework to analyze how contrastive learning behaves under imbalanced data distributions, with a specific focus on Transformer-based encoders. It identifies how feature frequency imbalance (i.e. majority vs. minority features) affects neuron-level learning dynamics and the quality of representation learning.\n\nThe paper shows that the training progresses through three distinct stages—initial feature growth, specialization of “lucky” neurons, and final convergence. It also shows minority features are learned more weakly, leading to fewer neurons specializing in them and resulting in degraded representations. It further shows introducing magnitude-based pruning during training mitigates these effects by amplifying updates for neurons aligned with minority features, thus improving representation balance.\n\nTheoretical results are corroborated by empirical experiments on various dataset (CIFAR10-LT, CIFAR100-LT, and ImageNet-LT), demonstrating consistent performance gains and reduced accuracy gaps between majority class and minority classes.\nIn addition, the paper provides theoretical proofs for convergence, establishes feature alignment properties, and rigorously derives the effects of pruning on the learning dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality** The paper seems quite novel. Given my knowledge I am not aware of many existing works exploring the statistical generalization theory of contrastive learning and data imbalance. While prior works have addressed imbalance heuristically or empirically, this paper seems the first formal theoretical treatment of the phenomenon. \n\n**Quality**\nThe math of the paper seems rigorous, supported by clearly stated lemmas and theorems. Empirical experimental results seem to align with statistical theory. The synthetic data experiments (Appendix A.2) further validate theoretical predictions in controlled settings.\n\n**Clarity**\nThe clarity of the paper is good. Although this paper is theoretically heavy and thus not easy to read, its clear presentation of results has made the task easier. The three training stages are clearly delineated, and key insights are summarized upfront (Section 3.1). The figures and tables are easy to interpret, and a table of notation is provided. The proof sketch is helpful too.\n\n**Significance** \nThis work has strong implications for self-supervised learning on real-world, imbalanced datasets. It offers theoretical understanding in the study of contrastive learning, showing how imbalance alters neuron specialization and model complexity requirements."}, "weaknesses": {"value": "There is no major technical flaw detected. However, I do have the following concern.\n\nAssumptions might be a bit strong.\n\n(1) The entire analysis seems to focus on the sparse coding model with orthogonal features and independent Gaussian noise. I understand the technical challenge in analyzing a more general setting. But the current setting feels a bit simple, because in reality features are usually correlated. This orthogonality assumption prevents the theory from describing how contrastive learning handles overlapping or dependent semantic features. \n\n(2) The paper’s assumption that self-attention remains identity-fixed fundamentally limits the scope of its theory. In other words, $W_K, W_Q, W_V$​ are not learnable."}, "questions": {"value": "(1) In Transformers, neurons often encode superposed features that are only linearly separable after training. The analysis assumes near-orthogonal features and pure specialization at convergence. Can the authors comment on whether their theory predicts or forbids feature superposition (neurons simultaneously encoding multiple correlated features)?\n\n(2) Could the authors maybe clarify how the temperature parameter ττ influences imbalance sensitivity in their theory?\nSpecifically, does a smaller ττ (sharper similarity weighting) amplify majority-feature dominance by concentrating gradients, while a larger ττ mitigates it by smoothing updates?\n\n(3) If the attention mechanism were trainable, pruning neurons downstream of attention would alter gradient flow through $W_K, W_Q, W_V$​. Could the authors speculate on whether such coupling might reinforce or dampen the minority-feature amplification effect?\n\n(4) Could the authors comment on how neuron specialization (Theorem 3.2) is connected to the generalization performance?\nIn particular, does specialization provably enhance linear separability or any other preferred properties of the learned representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hqhUrVX72", "forum": "DUXG9E8dEO", "replyto": "DUXG9E8dEO", "signatures": ["ICLR.cc/2026/Conference/Submission19334/Reviewer_Zaui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19334/Reviewer_Zaui"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760895782673, "cdate": 1760895782673, "tmdate": 1762931276476, "mdate": 1762931276476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a robust theoretical analysis of the training dynamics of Transformer-MLP models in learning feature representations through contrastive learning in imbalanced data scenarios. Specifically, the analysis of neuron weight evolution reveals how a minority of features undermines overall model performance. Building upon this, they revisit amplitude-based pruning methods, theoretically demonstrating that pruning yields more robust and balanced representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper explores how imbalanced data degrades representation quality in contrastive learning from a novel perspective of neural weight evolution. Through extensive theoretical analysis, the authors demonstrate that a minority of features weakens representational power while increasing the demand for complex architectures. Building upon this, they further prove that pruning techniques enhance gradient updates along these dominant features, thereby mitigating performance degradation caused by imbalance. This forward-looking approach holds significant promise for advancing the field."}, "weaknesses": {"value": "1：This paper uses numerous notations, some of which lack clear definitions upon their first appearance. Additionally, maintaining consistent notation throughout the text would improve readability.\n\n2：Could the authors please clarify the meaning of \"feature frequency\"? Specifically, how are the majority and minority features identified within the unsupervised learning framework?\n\n3: The study is confined to the Transformer-MLP model. Could the authors discuss the generalizability of their approach to other architectures or tasks, such as long-tailed visual recognition?\n\n4: While the authors provide substantial theoretical proofs, the effectiveness of the pruning strategy for imbalanced scenarios remains unverified by strong experimental evidence. Additionally, could its potential as a plug-and-play module for existing methods be discussed?\n\n5:The authors are advised to provide precise citations to the appendix for their key conclusions, and to thoroughly review the manuscript to correct various notational errors and inconsistencies."}, "questions": {"value": "As described in “Weaknesses”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m2nErhKTrj", "forum": "DUXG9E8dEO", "replyto": "DUXG9E8dEO", "signatures": ["ICLR.cc/2026/Conference/Submission19334/Reviewer_yCvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19334/Reviewer_yCvM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485534055, "cdate": 1761485534055, "tmdate": 1762931275977, "mdate": 1762931275977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response on Notation Issues for @Reviewer 8inz and @Reviewer yCvM"}, "comment": {"value": "**General Response on Notation Issues for @Reviewer 8inz and @Reviewer yCvM**\n\n>*Q1: Can the authors make the main text more self-contained? (8inz)*\n\n>*W\\&Q1: This paper introduces many notations, some of which are not clearly defined at first use, and several inconsistencies also affect readability. (yCvM)*\n\n**Response 1:** Thank you for the helpful suggestions regarding self-containment and readability. Yes, the theoretical analysis introduces many notations. Following your advice, we have added clear definitions in the main text when each notation first appears. We have added the order of the batch size $K = \\operatorname{poly}(d_1)$ after equation (3). For $C_m$ and $C_z$, your assumption is correct: they are positive constants. We have added the definition of $C_m$ in Theorem 3.1 and the definition of $C_z$ in Lemma 3.1. The expression $T_1 = \\Theta\\ \\left( \\frac{d_1 \\log d}{\\eta \\log \\log d} \\right)$ has been added in Lemma 3.1, and $T_2 = T_1 + \\Theta\\ \\left( \\frac{d \\tau \\log d}{\\epsilon_{\\max} \\eta \\log \\log d} \\right)$ has been added in Lemma 3.2. We have also added the definition of $\\Xi_2 = d^{C_m - \\left( \\tfrac{\\epsilon_{\\min}}{\\epsilon_{\\max}} \\right)^2}$ in Theorem 3.1. The pruning rate $\\alpha$ has also been introduced in Key Insights 3 when it first appears."}}, "id": "sOSckNZKNK", "forum": "DUXG9E8dEO", "replyto": "DUXG9E8dEO", "signatures": ["ICLR.cc/2026/Conference/Submission19334/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19334/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19334/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763746268302, "cdate": 1763746268302, "tmdate": 1763746268302, "mdate": 1763746268302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes why contrastive learning deteriorates under imbalanced data. Using a sparse latent-feature model and a Transformer-MLP encoder with InfoNCE, it shows that rare (minority) features grow more slowly, fewer neurons specialize in them, and neurons are forced to represent multiple features, effectively increasing the capacity needed to cover all features. To mitigate this, the authors propose a magnitude-based, forward-masked but backward-unmasked pruning scheme: small-magnitude parameters are masked out only in the forward pass, but all parameters are still updated. This selectively amplifies gradients along minority-feature directions and restores more balanced representations. Experiments on CIFAR-LT and ImageNet-LT confirm better linear-probe accuracy and smaller head–tail gaps."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Provides a rare, neuron-level theoretical analysis of contrastive learning under data imbalance, clearly explaining how minority features are under-learned.\n- Connects the analysis to a simple, practical fix (magnitude-based forward-masked, backward-unmasked pruning), making the work actionable.\n- Writing and structure are generally clear, making a dense theoretical contribution reasonably accessible."}, "weaknesses": {"value": "- Experiments mainly compare “with vs. without pruning” and lack baselines from other long-tailed methods.\n- Sensitivity to pruning ratio/schedule is not deeply analyzed.\n- Paper could more explicitly discuss limitations and when the proposed analysis may not apply."}, "questions": {"value": "- The paper sometimes reasons at the neuron level but prunes at the parameter level; please clarify the exact relationship between “parameter-level masking” and the claimed neuron-level effects.\n- Which parts of the analysis rely essentially on (i) identity/self attention, (ii) sparse independent features, or (iii) single-layer MLP, and which parts you believe could be relaxed or extended? A short discussion of “what breaks / what survives” under more realistic assumptions would help.\n- Can you provide empirical evidence of “lucky” vs. mixed-feature neurons (e.g., alignment plots)?\n- In the experiments, how does the method behave when the imbalance ratio = 1 (i.e., no explicit long tail)? Since the proposed pruning is fairly general, do you observe gains in “implicitly imbalanced” settings (e.g., feature-frequency skew induced by augmentations or views) even without a constructed LT distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PVWBOpF5JF", "forum": "DUXG9E8dEO", "replyto": "DUXG9E8dEO", "signatures": ["ICLR.cc/2026/Conference/Submission19334/Reviewer_xbUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19334/Reviewer_xbUv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979016444, "cdate": 1761979016444, "tmdate": 1762931275454, "mdate": 1762931275454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with contrastive learning, a self-supervised framework in which input data are paired into positive or negative pairs based on their similarity in semantic meaning. In particular, data are generated according to the Sparse Coding Model: they are linear combinations of feature vectors plus noise, feature $j$ has an activation probability controlled by a parameter $\\epsilon_j$, and features are in general imbalanced (majority features have $\\epsilon_j = \\epsilon_{\\rm max}$, minority ones $\\epsilon_j = \\epsilon_{\\rm min}$). Positive pairs are formed with inputs sharing the same token-aggregate features. The model is a Transformer-MLP built with a single head attention layer followed by an MLP. Training is performed by minimizing the InfoNCE loss, suitable for contrastive learning. A pruning mask filtering temporarily small magnitude neurons is applied when computing the gradients at each training step, and released before updating the neurons. \n\nThe paper provides formal results on the dynamics of training of this model. In particular, Lemmas 3.1, 3.2 and Theorem 3.1 show the existence of 3 temporal regimes during training with no pruning: in the first, neuron weights grow in feature directions but are suppressed in non-feature directions, with the growth rate in a feature direction depending on its frequency; in the second, few (lucky) neurons align significantly with single features, while ordinary neurons align in composite directions; in the last (at convergence), the training error is small and neurons are strongly aligned with a subset of features, weakly aligned with the remaining features, and small in the non-feature directions (still, only a limited number of neurons specialize in learning a single feature). Theorem 3.2 shows that pruning amplifies the learning of minority features."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Both the architecture and the training protocol are relevant. Theoretical predictions for the training dynamics of Transformer-based encoders are of utmost interest. Narratives on the assumptions behind the data model and on the formal results are provided. Numerical results on real data support the theoretical claims on the advantage of pruning."}, "weaknesses": {"value": "The formal results are hard to read, as the main text is not really self-contained (see Questions below), despite the commendable effort of Table 1. Numerical illustrations in the vanilla setting, even with synthetic data, could help explaining the practical relevance of the bounds provided (for example, by tracking the inner products of lucky/non-lucky neurons with features during training in the 3 regimes, and comparing with theoretical bounds). Considering that reviewing proofs in Appendix is out of question due to time limitations, numerical checks should help strengthening the claims provided by the main text."}, "questions": {"value": "- Can the authors make the main text more self-contained? For example:\n    - I could find the hypothesis on the batch size $K$ (such that empirical gradients approximate population ones) only in Lemma B.1;\n    - I am assuming $C_m$, $C_z$ to be positive constants, is that the case?\n    - The scaling of $T_1$, $T_2$ is not given in the main text;\n    - $\\Xi_2$ is not defined in the main text, etc.\n- In appendix A.2, the authors provide numerical experiments with synthetic data. Can they report in the main text numerical evidence for the 3 training regimes they are able to identify theoretically, for better illustration and check?\n- How are negative pairs chosen for the case of real data of section 4?\n\nMinor:\n\nSometimes * is used instead of $\\star$ to denote lucky neurons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HNJQHy5Kbs", "forum": "DUXG9E8dEO", "replyto": "DUXG9E8dEO", "signatures": ["ICLR.cc/2026/Conference/Submission19334/Reviewer_8inz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19334/Reviewer_8inz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984328832, "cdate": 1761984328832, "tmdate": 1762931274922, "mdate": 1762931274922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}