{"id": "06upBSlAUy", "number": 23238, "cdate": 1758341139376, "mdate": 1759896824854, "content": {"title": "SIPO: Stabilized and Improved Preference Optimization for Aligning Diffusion Models", "abstract": "Preference learning has garnered extensive attention as an effective technique for aligning diffusion models with human preferences in visual generation tasks. However, existing alignment approaches such as Diffusion-DPO suffer from two fundamental challenges: training instability caused by high gradient variances at various timesteps and high parameter sensitivities, and off-policy bias arising from the discrepancy between the optimization data and the policy model's distribution. Our first contribution is a systematical analysis of the diffusion trajectories across different timesteps and identify that the instability primarily originates from early timesteps with low importance weights. To address these issues, we propose SIPO, a Stabilized and Improved preference Optimization framework for aligning diffusion models with human preferences. Concretely, a key gradient, \\emph{i.e.,} DPO-C\\&M is introduced to facilitate stabilize training by clipping and masking uninformative timesteps. Followed by a timestep aware importance re-weighting paradigm to fully correct off-policy bias and emphasize informative updates throughout the alignment process. Extensive experiments on various baseline models, including image generation models on SD1.5, SDXL, and video generation models CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B, demonstrate that our SIPO consistently promotes stabilized training and outperforms existing alignment methods, with meticulous adjustments on parameters.\nOverall, these results highlight the importance of timestep-aware alignment and and provide valuable guidelines for improved preference optimization in diffusion models.", "tldr": "We propose a stabilized and improved preference optimization framework for aligning diffusion generative models with human perferences.", "keywords": ["Diffusion", "DPO", "image generate", "video generate"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38259828575fb003f42adcd3a998c716c9e1533f.pdf", "supplementary_material": "/attachment/0e37b8ed7b6e3832740d3b2b86fda314d0c201d4.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Stabilized and Improved Preference Optimization (SIPO), a framework designed to address two fundamental challenges in applying Direct Preference Optimization (DPO) to diffusion models: training instability and off-policy bias. The authors first conduct a systematic analysis of the diffusion process, identifying that training instability is primarily caused by high gradient variances originating from early time steps that have low importance weights."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Quality**\n\n1. Stability and Robustness.\nThe method demonstrates high training quality by providing a smoother, more stable training loss curve (Figure 1b) and consistently improving test accuracy without the late-stage degradation (reward hacking) seen in Diffusion-DPO (Figure 1c, 8a, 8b). Crucially, SIPO shows remarkable robustness to the $\\beta$ hyperparameter compared to the high sensitivity of Diffusion-DPO (Figure 1a)\n\n2. Extensive Benchmarking.\nThe experiments are high-quality and comprehensive, validating SIPO across a diverse set of large-scale models and tasks, including both popular image generators (SDXL) and challenging video generation models (CogVideoX, Wan2.1-1.3B, FLUX-dev).\n\n\n**Clarity**\n\n1. Clear Problem Framing. The paper is clear in framing the work, articulating the two core challenges (instability and off-policy bias) and immediately tying them to the need for timestep-aware alignment.\n\n2. Logical Flow. The introduction of the method is logically motivated by the preceding analysis on the reward dynamics and importance weights across different time steps (Figure 2 and 4), providing a foundation for the technical solution."}, "weaknesses": {"value": "1. Missing Analysis on Late-Stage Instability.\n\nThe paper states that \"early and late stages introduce instability\" and that \"preference signals are more informative in middle-to-late timesteps\". However, the core analysis focuses heavily on early timesteps being problematic due to low importance weights (Figure 4, low weight up to $t \\approx 63$). The paper is missing an explicit, corresponding analysis of why the very late timesteps, e.g., $t>900$ might also be unstable or uninformative, and how SIPO's mechanisms specifically address this tail-end instability.\n\n\n2. Baselines. The baselines are not sufficient. More SOTA baselines are encouraged to compare with, such as SPPO [1] and RainbowPA [2].\n\n[1] Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization. arXiv:2410.05255, 2025.\n\n[2] Diffusion-RainbowPA: Improvements Integrated Preference Alignment for Diffusion-based Text-to-Image Generation. Transactions on Machine Learning Research, 2025."}, "questions": {"value": "1. Baselines. See W2.\n\n2. Detailed Video Metrics.\n\nThe work explicitly addresses video generation challenges like maintaining temporal coherence. How about a breakdown of VBench results using metrics that specifically quantify temporal quality (e.g., motion smoothness, temporal consistency) in the main results section. This would provide stronger evidence for the successful alignment of video models compared to the general video quality metrics currently presented."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PrDKaAiwDb", "forum": "06upBSlAUy", "replyto": "06upBSlAUy", "signatures": ["ICLR.cc/2026/Conference/Submission23238/Reviewer_X295"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23238/Reviewer_X295"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595147688, "cdate": 1760595147688, "tmdate": 1762942569628, "mdate": 1762942569628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the instability observed in applying Direct Preference Optimization (DPO) to diffusion models. The authors propose two complementary improvements:\n(1) DPO-C&M, which introduces timestep-dependent importance masking and gradient clipping to mitigate gradient explosion and overemphasis on uninformative steps; and\n(2) SIPO, which modifies the DPO objective by applying clipped importance reweighting to the log-likelihood ratio term and reformulates the loss as KL minimization toward a reward-shaped target distribution. By skipping early diffusion steps and leveraging importance sampling, SIPO aims to improve training stability and convergence behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "oth modifications (C&M and SIPO) are lightweight yet principled, and can be easily integrated into existing preference-optimized diffusion frameworks.\nThe experiments demonstrate noticeable stability improvements across image and video generation tasks, suggesting the methods’ robustness in practice."}, "weaknesses": {"value": "1. Inconsistent β values across methods. Table A1 shows that β differs between baselines and proposed methods (e.g., for video: DPO=2 vs. SIPO/DPO-C&M=0.02; for image: DPO=5 vs. SIPO/DPO-C&M=1). Since β directly controls the conservative/aggressive trade-off in DPO, this discrepancy could systematically favor the proposed variants. The authors should present comparisons under matched β settings or include grid-based sensitivity analyses with statistical significance.\n\n2. Lack of human evaluation details. More information about the human annotation setup is needed—number of raters, cleaning or filtering criteria, and aggregation methods. Releasing minimal anonymized examples would further enhance reproducibility and transparency."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5FpsCgkElj", "forum": "06upBSlAUy", "replyto": "06upBSlAUy", "signatures": ["ICLR.cc/2026/Conference/Submission23238/Reviewer_tiFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23238/Reviewer_tiFh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906179914, "cdate": 1761906179914, "tmdate": 1762942569325, "mdate": 1762942569325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SIPO (Stabilized and Improved Preference Optimization) for aligning diffusion models with human (or AI) preferences. The key ideas are: (1) identify that early timesteps in diffusion contribute high-variance, low-importance gradients; (2) introduce DPO-C&M (clipping & masking) using timestep-wise importance weights to stabilize training; and (3) further correct off-policy bias via importance-weighted DPO with clipped, timestep-aware weights. Experiments on SD1.5/SDXL for T2I and CogVideoX/Wan for T2V claim improved stability and accuracy over Diffusion-DPO and other baselines, with lower sensitivity to β and better human evals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear empirical diagnosis of instability: The paper argues and shows that early timesteps have low importance weights and introduce noisy gradients; masking/clipping these improves stability.\n2. Principled off-policy correction: Casting diffusion DPO with importance sampling and clipping is well-motivated by RL literature; the step-wise treatment is natural for diffusion.\n3. Breadth of evaluation: Benchmarks across SD1.5/SDXL (T2I) and CogVideoX/Wan (T2V) with automatic metrics and human ranking; reports reduced β-sensitivity and smoother learning curves."}, "weaknesses": {"value": "1. In line 361, you state “at 1000 steps, Diffusion-DPO collapses (67.28)” whereas Table 1 shows pretrained = 67.28 and Diffusion-DPO = 81.46. This contradiction undermines the claimed failure of Diffusion-DPO at longer training. Please reconcile.\n\n2. Missing ablations: Choices like threshold 0.9 for pruning, the clipping range [1−ε,1+ε], and their per-dataset sensitivity lack rigorous ablations; Importance estimation per-timestep may add overhead; wall-clock comparisons to Diffusion-DPO/SPIN etc. are not reported."}, "questions": {"value": "1. Please clarify the results in Table 1.\n\n2. Please add more ablations such as sensitivity and justification for the 0.9 threshold and ε range across datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oFK61ZRYdB", "forum": "06upBSlAUy", "replyto": "06upBSlAUy", "signatures": ["ICLR.cc/2026/Conference/Submission23238/Reviewer_gHDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23238/Reviewer_gHDb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964848373, "cdate": 1761964848373, "tmdate": 1762942569103, "mdate": 1762942569103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}