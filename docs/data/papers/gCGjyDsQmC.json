{"id": "gCGjyDsQmC", "number": 14564, "cdate": 1758239024306, "mdate": 1759897362143, "content": {"title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models", "abstract": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple candidate responses and then operating over this set to find the best output. A tacit premise behind TTS is that sufficiently diverse candidate pools enhance reliability. In this work, we show that this assumption in TTS introduces a previously unrecognized failure mode. When candidate diversity is curtailed, even by a modest amount, TTS becomes much more likely to produce unsafe outputs. We present a reference-guided diversity reduction protocol (RefDiv) that serves as a diagnostic attack to stress test TTS pipelines. Through extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1, Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and Best-of-N), constraining diversity consistently signifies the rate at which TTS produces unsafe results. The effect is often stronger than that produced by prompts directly with high adversarial intent scores. This observed phenomenon also transfers across TTS strategies and to closed-source models (e.g. OpenAI o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant property of TTS rather than a model-specific artifact. Additionally, we find that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and OpenAI Moderation API), are unable to flag the adversarial input prompts generated by RefDiv, demonstrating that existing defenses offer limited protection against this diversity-driven failure mode. Through this work, we hope to motivate future research on designing robust TTS strategies that are both effective and secure against diversity-targeted stress tests as illustrated by RefDiv.", "tldr": "", "keywords": ["test-time scaling", "diversity", "entropy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19ed690f52a9cf005c1cf103d8973191ae4b9422.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a novel failure mode in Test-Time Scaling (TTS) methods for LLMs: when candidate diversity is reduced, TTS becomes significantly more vulnerable to producing unsafe outputs. The authors propose REFDIV (Reference-Guided Diversity Stress Test Protocol), a genetic algorithm-based attack that constrains candidate diversity using Shannon entropy minimization. Through extensive experiments across multiple open-source models (Qwen3, Mistral, Llama3.1, Gemma3), TTS strategies (Best-of-N, MCTS), and closed-source systems (o3, Gemini-2.5), they demonstrate that diversity reduction consistently increases attack success rates, often surpassing existing jailbreak methods. The attacks also successfully bypass popular safety guardrails like Llama-Guard and OpenAI Moderation API."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a new vulnerability specific to TTS methods. The connection between candidate diversity and safety is intuitive yet previously unexplored, making this a valuable contribution as TTS becomes more prevalent in production systems.\n2. REFDIV consistently outperforms baselines, particularly on harder models (Llama3.1-8B: 0.465 vs 0.368; Gemma3-27B: 0.926 vs 0.749 for Best-of-N), and shows reasonable transferability to closed-source systems."}, "weaknesses": {"value": "1. REFDIV is essentially AutoDAN with a diversity-focused fitness function. The genetic algorithm framework is standard; the main contribution is recognizing diversity as a target rather than methodological innovation. The dynamic weighting scheme α(t) appears somewhat ad-hoc without justification for the specific exponential schedule.\n2. The paper identifies the vulnerability but offers no mitigation strategies. Suggestions for diversity-preserving TTS variants, entropy-based monitoring, or robustness enhancements would strengthen the contribution.\n3. Recent reasoning-model attacks mentioned in related work (Mousetrap, AutoRAN, H-CoT) are not evaluated. Without these comparisons, it's unclear whether REFDIV represents state-of-the-art or merely improves on AutoDAN."}, "questions": {"value": "1. Have you explored diversity-aware defenses? For example, could TTS explicitly maintain entropy thresholds, or could reward models penalize low-diversity candidate sets?\n2. Why does REFDIV show much larger improvements over AutoDAN for Llama3.1-8B and Gemma3-27B? Are certain alignment techniques more susceptible to diversity attacks?\n3. Are REFDIV-generated prompts more or less natural/detectable than AutoDAN prompts? Could simple perplexity filters help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9R6tC5ckes", "forum": "gCGjyDsQmC", "replyto": "gCGjyDsQmC", "signatures": ["ICLR.cc/2026/Conference/Submission14564/Reviewer_zwHe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14564/Reviewer_zwHe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487513614, "cdate": 1761487513614, "tmdate": 1762924953106, "mdate": 1762924953106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a novel failure mode in Test-Time Scaling (TTS) methods for Large Language Models, focusing on how reduced candidate diversity can compromise safety. The authors propose REFDIV (Reference-Guided Diversity Stress Test Protocol), a reference-guided diversity stress-testing attack that constrains the diversity of candidate responses generated during test-time, thereby steering TTS-enabled LLMs toward producing unsafe outputs. The method uses a genetic algorithm with a diversity-guided fitness function that combines generation diversity (Shannon entropy) and reference-guided diversity (alignment with affirmative tokens). Experiments across open-source models on two TTS strategies (Best-of-N and MCTS) demonstrate that REFDIV achieves higher Attack Success Rates compared to baseline jailbreak methods like GCG and AutoDAN. The adversarial prompts also transfer across TTS strategies, to closed-source models and bypass popular safety guardrails (e.g., LlamaGuard-3/4)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Novel Failure Mode Identification: The paper identifies a previously unrecognized vulnerability in TTS methods—their implicit reliance on candidate diversity.\n\n+ Comprehensive Experimental Validation: The authors conduct experiments across multiple dimensions: different TTS strategies (Best-of-N + reward models, MCTS), both open-source and closed-source models, and multiple safety guardrails. This thorough evaluation demonstrates the generality and pervasiveness of the identified failure mode.\n\n+ Strong Transferability Results: REFDIV demonstrates strong transferability across TTS strategies, to black-box closed-source models, and across safety classifiers. This transferability suggests the diversity-driven vulnerability is a fundamental property of TTS frameworks rather than model-specific artifacts."}, "weaknesses": {"value": "+ Unclear Threat Model for TTS: The paper does not clearly articulate the threat model specific to TTS systems. While the white-box assumption is stated (access to M and T but not explicit access to reward model r), it's unclear: (a) what adversarial capabilities are realistic in deployed TTS systems, (b) whether attackers would have access to the TTS strategy details in practice, and (c) how this threat model compares to standard LLM threat models. The motivation for why an adversary would specifically target diversity rather than directly optimizing for harmful outputs needs clearer justification.\n\n+ Missing Technical Details and Result Analysis:\n \n  1. Genetic Algorithm Details: The offspring generation process through \"crossover and mutation\" (Algorithm 1, line 10) is not specified. How are prompts crossed over? What mutation operators are used? These details are critical for reproducibility and should be in the main body of the paper.\n\n  2. Reward Model Training and Safety: The paper uses PairRM and deberta-v3-large-v2 as reward models but provides no information about: (i) how these models were trained, (ii) whether they were specifically trained for safety, (iii) their baseline safety performance, and (iv) whether they might inadvertently contribute to the vulnerability.\n\n  3. Entropy Analysis: While Figure 4 shows Shannon entropy decreases for REFDIV, the paper lacks deeper analysis of: (i) what specific entropy thresholds lead to safety failures, (ii) how entropy correlates with ASR quantitatively, and (iii) why some models (e.g., Llama3.1-8B) are more sensitive to diversity reduction than others.\n\n  4. Transferability Mechanisms: The paper observes that prompts from Llama3.1-8B transfer better to closed-source models but only provides a high-level explanation. What specific properties of these prompts enable better transfer?\n\n\n+ Baseline Comparisons Not State-of-the-Art: The paper compares against GCG and AutoDAN, but these are not the most recent state-of-the-art jailbreaking methods. Notable omissions include:\n  1. PAIR (Chao et al., 2023) - which is cited but not used as a baseline despite being more effective than GCG\n  2. AutoDAN-Turbo (Liu et al., 2024) - which is cited and explicitly described as extending AutoDAN with \"lifelong learning,\" yet only the original AutoDAN is used for comparison"}, "questions": {"value": "+ MCTS Clarification: Can you provide a more detailed explanation of how MCTS is used as a TTS strategy in Section 4.1? Specifically: How does MCTS explore the solution space of LLM generations? The paper states \"each instantiation is run with default parameters for the number of children (=3), for a total of 3 MCTS iterations\" (Section 4.1). This requires justification: Why were these specific values chosen (3 children, 3 iterations)? How sensitive are the results to these hyperparameters?\n\n+ Fitness Function Design: The dynamic weighting α(t) transitions from reference-guided to intrinsic diversity. What is the sensitivity to the specific exponential schedule? Have you tried other schedules (linear, sigmoid, etc.)?\n\n+ Reward Model Vulnerability: To what extent does the vulnerability stem from the reward model itself versus the TTS strategy? Could adversarially robust reward models mitigate this attack?\n\n+ Defense Mechanisms: Beyond identifying the vulnerability, what potential defenses could mitigate diversity-targeted attacks? For example, could diversity-aware safety filtering or ensemble-based verification help?\n\n+ Real-world Impact: In practical deployments, how likely is it that adversaries would have the white-box access required for REFDIV? Could the method be adapted to black-box settings?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AUnoIBshwW", "forum": "gCGjyDsQmC", "replyto": "gCGjyDsQmC", "signatures": ["ICLR.cc/2026/Conference/Submission14564/Reviewer_vxQJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14564/Reviewer_vxQJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878425728, "cdate": 1761878425728, "tmdate": 1762924952606, "mdate": 1762924952606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the ways to break the diversity (entropy) in the output of the LLM and discusses how this can potentially expose the vulnerabilities of the target model when all the answers are unsafe. Then in their ablation study they show that their method successfully decreases the entropy towards one unsafe direction as opposed to other methods that do not impact the entropy. Finally, they study the transferability of their method across multiple black-box targets and reward models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- The paper studies a new problem that I view it as an important failure mode in the era of reasoning, and can transcend the jailbreaking literature. Rather than looking at the input-to-output process in the model as a black-box and attempting to jailbreak it, they find a vulnerability to the TTS methods that is entirely novel and important to current literature. \n\n2- Their method is novel as far as I am aware, where they use a combination of the output's entropy and the some fixed sequence's likelihood as the objective. (the second term is added as sort of a regularization to prevent the target's answers from collapsing before finding a jailbreak)\n\n3- The paper indeed shows that their method is effective at decreasing the entropy."}, "weaknesses": {"value": "1- The main table of the paper is not convincing to me. They have provided a set of very limited target models where Mistral does not even  have a guardrail. As for llama3, there are other results that are not mentioned here and achieve 100%:\n\nAndriushchenko et al., \"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks\".\n\nSabbaghi et al. \"Adversarial Reasoning at Jailbreaking Time\".\n\n2- When it comes to the baselines, they compare their method with GCG and AutoDan that, even though they are well-known, they are not considered state-of-the-art anymore. While I understand that this paper is based on AutoDan in the core and aims to show that considering the entropy is effective, there are many other jailbreaking methods that target reasoning models (some of them included in the related work). The paper does not do comparisons with any of them. \n\n3- As for black-box models, I would like to see some results on Claude 3.5 or 4.0 as well for a better comparison with previous work."}, "questions": {"value": "Pls see above for my questions. Also:\n\n1- In figure 4, are the target models deployed with BoN or MCTS? \n\n2- Can you explain how your method relates to other methods where there is no TTS and you are working with an API?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QXZ3knYThc", "forum": "gCGjyDsQmC", "replyto": "gCGjyDsQmC", "signatures": ["ICLR.cc/2026/Conference/Submission14564/Reviewer_6k26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14564/Reviewer_6k26"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119680205, "cdate": 1762119680205, "tmdate": 1762924952100, "mdate": 1762924952100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}