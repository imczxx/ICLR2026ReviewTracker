{"id": "6W9KT77Wux", "number": 11619, "cdate": 1758202532982, "mdate": 1759897564392, "content": {"title": "Graph-Augmented Tabular Transformers: The Simplicity Advantage", "abstract": "Transformer models have recently advanced tabular prediction, but they usually treat rows as independent, ignoring that similar instances often share outcomes. Graph augmentation introduces an explicit inductive bias by connecting instances or features and refining embeddings with Graph Neural Networks (GNNs).\nWe present TANGO (Transformers Augmented with Graphs for Tabular Predictions), a large-scale systematic study (to our knowledge, the largest to date) of graph-augmented tabular transformers across 193 datasets (117 classification, 76 regression). Across this benchmark, TANGO not only improves a strong transformer backbone but also surpasses state-of-the-art tabular foundation models (TabPFNv2, TabICL, MITRA) and consistently outperforms classical tree ensembles (CatBoost, XGBoost) in both classification and regression, achieving the most rank-1 wins, lowest average rank, and smallest relative error gaps.\nOur analysis yields three insights. (1) Graph augmentation consistently improves a strong transformer backbone across diverse tasks. (2) Static graphs outperform dynamic ones, offering better stability and generalization. (3) Within static graphs, frozen embeddings are overall more reliable, consistently outperforming other variants in regression and classification.\nThese results overturn the assumption that dynamic graphs or joint training are always superior, showing instead that schema-anchored graph priors drive generalization: static graphs enforce a stable relational structure, while dynamic ones often introduce instability and risk overfitting.", "tldr": "Augmenting tabular transformers with graph structures improves representation learning and predictive performance.", "keywords": ["Tabular data", "Graph Neural Networks", "Transformer-Graph Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa148678feb7bde22c7d1c3065faed9292d624f6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TANGO (Transformers Augmented with Graphs for Tabular Predictions) , a method that augments a standard transformer backbone (FT-Transformer)  with a Graph Neural Network (GNN) to model inter-instance relationships in tabular data. The authors conduct a large-scale systematic study across 193 datasets (117 classification, 76 regression)  to evaluate different graph-augmentation strategies. The core of the study compares static  graphs versus dynamic graphs , and two-stage embeddings versus end-to-end training . The authors claim that TANGO, particularly the static-frozen variant (TANGO-SF), outperforms both classical tree ensembles (CatBoost, XGBoost) and state-of-the-art tabular foundation models (TabPFNv2, TabICL, MITRA)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The large-scale empirical evaluation across 193 datasets provides broad coverage of diverse tabular learning scenarios.\n- The ablation study clearly isolates the effects of different graph construction strategies and training paradigms.\n- TANGO demonstrates competitive performance, outperforming several recent tabular foundation models."}, "weaknesses": {"value": "- The comparison set is limited. The paper focuses primarily on foundation models but omits GNN-based tabular baselines (e.g., TabGNN, T2G-Former, CARTE) as well as other SOTA models for tabular data (e.g., ExcelFormer SAINT), which are directly relevant to the proposed approach. This omission weakens the empirical claims.\n- Novelty is limited. The use of graphs and GNNs for tabular data has been explored extensively in prior work. Both static and dynamic graph construction strategies are well-studied. The authors should more clearly articulate what distinguishes TANGO from these existing approaches.\n- The evaluation metrics are not fully convincing. Reporting only rank-based metrics can obscure the magnitude of performance differences. Mean and median performance across all datasets should be provided for a fairer comparison.\n- The main contribution appears to be the scale and thoroughness of the experimental study, rather than methodological innovation. Given this, the work might be better framed as a benchmark or empirical study, rather than a novel model contribution for the main ICLR track.\n- No statistical significance tests are reported, leaving it unclear whether observed improvements are robust.\n- The choice of the FT-Transformer as the backbone seems weakly justified. The authors should discuss whether the findings generalize to stronger transformer architectures."}, "questions": {"value": "- How were the “representative datasets” in Table 1 and Table 3 selected?\n- Can the authors include more baseline methods including SOTA tabular models (e.g., ExcelFormer [1], SAINT[2], RealMLP[3])and GNN-based methods (e.g., TabGNN, T2g-Former[4], CARTE[5])\n- Could the authors report the mean and median absolute performance of TANGO across all datasets, rather than relying solely on ranking metrics?\n- Have the authors conducted statistical significance tests to confirm that TANGO’s improvements are not due to random variation?\n\n[1] Chen, Jintai, et al. \"Can a deep learning model be a sure bet for tabular prediction?.\" *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 2024.\n\n[2] Somepalli, Gowthami, et al. \"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training.\" *NeurIPS 2022 First Table Representation Workshop*.\n\n[3] Holzmüller, David, Léo Grinsztajn, and Ingo Steinwart. \"Better by default: Strong pre-tuned mlps and boosted trees on tabular data.\" *Advances in Neural Information Processing Systems* 37 (2024): 26577-26658.\n\n[4] Yan, Jiahuan, et al. \"T2g-former: organizing tabular features into relation graphs promotes heterogeneous feature interaction.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 9. 2023.\n\n[5] Kim, Myung Jun, Leo Grinsztajn, and Gael Varoquaux. \"CARTE: Pretraining and Transfer for Tabular Learning.\" *International Conference on Machine Learning*. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nlddRuqklO", "forum": "6W9KT77Wux", "replyto": "6W9KT77Wux", "signatures": ["ICLR.cc/2026/Conference/Submission11619/Reviewer_u5fb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11619/Reviewer_u5fb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484978902, "cdate": 1761484978902, "tmdate": 1762922692449, "mdate": 1762922692449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents two different approaches for solving tabular prediction tasks where the input table is augmented with additional graph structure. The first one creates a homogeneous kNN graph based on the the distance between samples in latent space. The second one creates a heterogeneous graph with nodes corresponding to distinct features and samples being connected to these nodes depending on which values of categorical and numerical features they have. Together with the node embeddings produced by tabular Transformer, the obtained graph structure is processed by GNN. Extensive experiments in end-to-end and two-stage regimes on numerous tabular datasets show that the proposed method achieves the highest average rank and the highest number of 1-rank wins when compared with tuned GBDT models and recent tabular foundation models in ICL mode."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Extensive experiments on a large number of tabular datasets, including classification and regression.\n2. An interesting idea of using the similarity between samples and their feature description to construct a graph that can be used by GNN model."}, "weaknesses": {"value": "1. The two approaches to construct a graph are fundamentally very different — the first makes a homogeneous graph using the similarity between representations in latent space, while the second makes a heterogeneous graph with nodes and edges of very different types, involving both samples or features. Thus, it is quite strange to see them together in the same work and have no detailed discussion about how to use each of them properly and get the most out of it.\n2. In the construction of heterogeneous graph, there are so many design choices that have not been investigated in any way. Thus, there is no clear explanation as how does it turn out that such an approach with connections between samples and their feature values represented as distinct nodes even works.\n3. There is also no comparison with other numerous ways to construct a graph using latent representations or original features of data samples.\n4. In fact, the comparison *dynamic* vs *static* is very misleading — it is rather *homogeneous kNN graph on latent representations* vs *heterogeneous bipartite graph on samples and extra nodes representing distinct features*.\n5. No source code is provided, and some important details lack in the paper, which raises a number of questions. Please, refer to them below."}, "questions": {"value": "1. What features are placed in the nodes of heterogeneous graph?\n2. Which part of heterogeneous graph is the most important and impactful — edges to categorical features, edges to numerical features, edges between categorical features, etc.?\n3. How are the PPMI weights used by GNN model?\n4. Why do the authors use TFMs only in ICL mode and do not consider finetuning mode?\n5. In two-stage training mode, is tabular backbone pretrained to solve the same original prediction task using standard supervised loss?\n6. Why do the authors omit RealMLP [1] or TabM [2] baselines, which have been proven to be more effective than FT-Transformer?\n7. Why do the authors omit ModernNCA [3] baseline, which does basically the same as what the authors propose as one of the options — dynamic kNN in trained latent space? It should provide very decent results given such a large train part in data split.\n8. Why do the authors use custom 80/10/10 data splits instead of the original ones introduced in TabPertNet and TabArena? It can be problematic to compare the obtained results with the existing ones.\n9. To the best of my knowledge, there are no so many good tabular datasets in open source, and some datasets used in this study can have serious drawbacks. Can the authors provide the average ranks for TabArena and TabPertNet independently? It would be very useful to see both the main comparison with baselines and the ablation study of different approaches to graph construction and training regime. I also would highly recommend to run experiments on TabReD benchmark [4] that provides industry-grade datasets with more realistic temporal data splits.\n\n[1] Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data, NeurIPS 2024\n\n[2] TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling, ICLR 2025\n\n[3] Revisiting Nearest Neighbor for Tabular Data: A Deep Tabular Baseline Two Decades Later, ICLR 2025\n\n[4] TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IF6uhWTDKk", "forum": "6W9KT77Wux", "replyto": "6W9KT77Wux", "signatures": ["ICLR.cc/2026/Conference/Submission11619/Reviewer_R3rX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11619/Reviewer_R3rX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510713008, "cdate": 1761510713008, "tmdate": 1762922691976, "mdate": 1762922691976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether explicitly modeling inter-instance similarity via graphs improves tabular transformers. The authors introduce TANGO, a family of models that attach a GNN head to a tabular transformer and compare static bipartite graphs to dynamic kNN similarity graphs built over evolving instance embeddings. Training is either joint or two-stage. Across 193 datasets (117 classification, 76 regression) drawn from TP-BERTa and TabArena, TANGO reports the most rank-1 wins, lowest average rank, and smallest average relative error gaps versus baselines (CatBoost, XGBoost, FT-Transformer) and recent tabular foundation models. Static graphs with frozen embeddings perform best overall, yielding a simplicity advantage and overturning the assumption that dynamic or fully joint training must dominate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The methodology is clearly specified: algorithms for graph construction (dynamic k-NN; static instance–feature bipartite with PPMI), GNN choices, and two training paradigms (joint vs. two-stage) are described, with Optuna-tuned hyperparameters and unified preprocessing. Results report rank-1 wins, average rank, and “relative gap,” a scale-free improvement measure; classification uses macro-F1, regression uses RMSE. Ablations convincingly support the main claims: static+frozen dominates; dynamic variants underperform, especially on regression.\n2. The paper is easy to follow: the problem is motivated crisply, the method is modular, and implementation details and search spaces are provided in the appendix, aiding reproducibility."}, "weaknesses": {"value": "1. Limited statistical testing and variance analysis for large-scale claims. Results report rank-1 wins, average rank, and relative gaps on a single fixed split per dataset; I did not see paired tests (e.g., Wilcoxon signed-rank on per-dataset scores), confidence intervals, or multi-seed variability for neural models. The setup uses one random seed to create splits (§4.1) and Optuna with 100 trials, but variability across seeds/trials is not summarized.\n2. The method is described as backbone-agnostic, yet all graph-augmented models are built on FT-Transformer; the authors do not show results when the transformer is stronger/different (e.g., SAINT-style row/column attention) or when the backbone itself is a foundation model."}, "questions": {"value": "1. How sensitive are dynamic graphs to the similarity metric and to k?\n2. Can you provide a compact study showing backbone-agnostic behavior, e.g., TANGO on a SAINT-style attention encoder or a column-then-row stage—as a sanity check that the simplicity advantage is not an FT-Transformer artifact?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R9mOBizPja", "forum": "6W9KT77Wux", "replyto": "6W9KT77Wux", "signatures": ["ICLR.cc/2026/Conference/Submission11619/Reviewer_5qJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11619/Reviewer_5qJ1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995128446, "cdate": 1761995128446, "tmdate": 1762922691604, "mdate": 1762922691604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TANGO, a graph-augmented framework for tabular Transformers, so that samples can use information from related samples instead of being treated as independent. It studies two graph constructions: a dynamic kNN graph built from current embeddings, and a simple static, schema-driven bipartite graph linking samples to feature/value nodes. It adds a GNN on top of the Transformer embeddings to propagate information over the constructed graph before prediction. On 193 tabular datasets, it shows that the simplest variant — static graph + frozen Transformer (TANGO-SF) — is the most stable and often outperforms strong tabular baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper frames tabular learning as “row representations + graph relations\", and systematically compares static (schema-driven) vs. dynamic (embedding-driven) graphs, leading to the useful and somewhat surprising “simplicity advantage” insight.\n\n2. Under a unified split and hyperparameter budget, the paper reports results on both classification and regression, comparing against tree models and recent tabular foundation models; through the four variants (TANGO-SF/SE/DE/DF), the authors conduct an informative ablation study.\n\n3. The paper gives clear implementation details: simple construction of static graphs, the dynamic kNN pipeline, and how to plug the graph module into FT-Transformer."}, "weaknesses": {"value": "1. The novelty is somewhat modest: using graphs to connect tabular instances or feature co-occurrences has been explored before\n\n2. Static-graph design lacks fine-grained ablations, especially PPMI. PPMI feature–feature edges are optional (turned on only when memory fits), but there’s no “with vs. without PPMI” ablation to quantify their marginal benefit\n\n3. Scalability is acknowledged but not quantified. Although the Limitations section notes that static bipartite graphs can be memory-intensive, the paper would be stronger with concrete complexity/scalability evidence, like gpu memory consumption w.r.t. the number of rows.\n\n4. Statistical support is missing. The paper states that the median improvement over 117 classification datasets is 0.0% (Line 400), yet does not provide paired statistical tests and confidence intervals for average rank/relative gap.\n\n5. Insufficient isolation of the graph’s effect. Although TANGO beats FT-Transformer, the paper does not systematically compare FT-Transformer alone vs. TANGO (i.e., FT-Transformer + graph) to analyze how much of the gain actually comes from the graph."}, "questions": {"value": "1. The paper compares two graph constructions—a dynamic kNN graph and a simple static bipartite graph—and finds that the static one performs better. What would happen if the two were combined? That is, on top of the static graph, also add edges between each node and its kNN “top-k neighbors\"\n\n2. Complexity / scalability analysis \n\n3. Ablation on PPMI edges. Since PPMI-based feature–feature edges are optional and may be dropped for large/high-cardinality tables, please report “with vs. without PPMI\"\n\n4. TabICL citation. The current citation of TabICL appears incorrect / hallucinated. The correct reference should be:  \nJingang, Q. U., Holzmüller, D., Varoquaux, G., & Le Morvan, M. TabICL: A Tabular Foundation Model for In-Context Learning on Large Data. In Forty-second International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xdmJAjVptH", "forum": "6W9KT77Wux", "replyto": "6W9KT77Wux", "signatures": ["ICLR.cc/2026/Conference/Submission11619/Reviewer_gjaz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11619/Reviewer_gjaz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007372796, "cdate": 1762007372796, "tmdate": 1762922690851, "mdate": 1762922690851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}