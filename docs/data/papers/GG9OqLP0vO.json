{"id": "GG9OqLP0vO", "number": 4935, "cdate": 1757814046459, "mdate": 1759898004279, "content": {"title": "Mamba-IVP: A Denoising State-Space Initial Value Problem Framework for SOTA Clinical Time Series, Healthcare Alternative", "abstract": "Missing clinical time series is a critical bottleneck in intensive care units (ICUs). In large-scale ICU electronic health record datasets such as MIMIC-IV, missing rates exceed 90% due to sensor failures, monitor degradation, and systemic outages, while aging devices inject unstable noise that makes reliable modeling nearly impossible. Existing methods remain unsafe for deployment: statistical heuristics distort missingness, deep models collapse under block-wise gaps and noise, and ODE- or diffusion-based approaches demand prohibitive computation. To overcome these limitations, we propose Mamba-IVP, a state-space generative model with a Mask-Aware Dual-Mamba Encoder (MADME) to handle block-wise missingness and a Mamba-Hybrid Decoder (MHD) to denoise continuous-time reconstructions. We validate our method through 61 experiments across two tasks: time series forcatsing and node classification. Our experiments involve 7 classic and state-of-the-art target models and 3 publicly available datasets: (1) it achieves state-of-the-art accuracy, reducing MSE by 3.0%, improving AUROC by 3.0%, and enhancing AUPRC by 3.9%; and (2) it remains robust under noise and block-wise missingness up to 12h, where other models degrade sharply.", "tldr": "Mamba-IVP is a lightweight state-space generative model that robustly imputes clinical time series under noise and missingness, achieving SOTA accuracy with superior efficiency.", "keywords": ["Clinical Time Series", "Mask-Aware Dual-Mamba Encoder", "Mamba-Hybrid Decoder", "Initial Value Problem", "Denoising Proof"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f21c5b72004f2a3cdce46a9a7a978783d7ce0d79.pdf", "supplementary_material": "/attachment/6409f6ce08476099e0c6d5a23ed37d73c571f39b.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Mamba-IVP, a state-space generative framework designed for clinical time-series modeling under extreme missingness and noise conditions. The model integrates a Mask-Aware Dual-Mamba Encoder (MADME) to jointly process observed values and missingness masks, ensuring robustness under block-wise gaps, and a Mamba-Hybrid Decoder (MHD) to reconstruct and denoise continuous trajectories through parallelizable latent evolution. Mamba-IVP is evaluated on three large-scale ICU datasets  (MIMIC-IV, PhysioNet 2012, and eICU) for forecasting and mortality classification tasks. The model achieves state-of-the-art performance, outperforming GRU-D, mTAN, and IVP-VAE in both accuracy and efficiency. It also provides a theoretical analysis of Mamba’s variance-contraction property, demonstrating stability under noisy and missing conditions.."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clinically important problem**: The paper tackles the early detection and modeling of sepsis, a highly impactful clinical task where accurate forecasting can have direct consequences for patient outcomes. The focus on handling severe missingness reflects a realistic and pressing challenge in ICU time-series data.\n\n- **Mathematically sound formulation**: The proposed Mamba-IVP framework is well formulated and theoretically motivated. The dual design of the encoder–decoder structure (MADME and MHD) is clearly presented, and the mathematical derivations appear consistent and rigorous.\n\n- **Strong empirical results**: The model achieves competitive or superior performance across multiple benchmark datasets (MIMIC-IV, PhysioNet 2012, and eICU), demonstrating both its robustness to missing data and its practical relevance for clinical forecasting and classification tasks"}, "weaknesses": {"value": "- **Limited diversity of clinical data**: If I understand correctly, the three datasets used in the study (MIMIC-IV, PhysioNet 2012, and eICU) contain only ICU patients diagnosed with sepsis. Is there a specific reason for restricting the analysis to this condition? Expanding the evaluation to other diseases or broader ICU cohorts could better demonstrate the model’s generalization capacity and support its use.\n\n- **Clarity and structure of the method section**: While the proposed framework is mathematically sound, the explanation in Section 3 is somewhat difficult to follow. The presentation could benefit from a clearer structure, helping readers connect the MADME and MHD components more intuitively.\n\n- **Comparison with diffusion-based baselines**: The paper briefly mentions diffusion models such as CSDI and argues that they are less suited for this domain due to computational cost. However, including quantitative results for CSDI or PriSTI [1], even in a limited setting, would provide a clearer picture of where Mamba-IVP stands in terms of accuracy–efficiency trade-offs.\n\n- **Missing discussion on consistency models**: Beyond diffusion approaches, it would also be interesting to include a brief discussion on consistency models [2], which can be viewed as distilled, discrete formulations of diffusion models that emphasize faster inference. In the context of time-series imputation, the recently proposed CoSTI model [2] follows this idea and has also been applied to PhysioNet data. Understanding how Mamba-IVP compares to these diffusion-like but more efficient paradigms would clarify its position in the broader generative modeling landscape.\n\n[1] Liu, M., Huang, H., Feng, H., Sun, L., Du, B., & Fu, Y. (2023, April). Pristi: A conditional diffusion framework for spatiotemporal imputation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE) (pp. 1927-1939). IEEE. https://arxiv.org/abs/2302.09746\n\n[2] Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, and Isabel A. Nepomuceno-Chamorro. Costi: Consistency models for (a faster) spatio-temporal imputation. Knowledge-Based Systems, 327:114117, 2025. https://arxiv.org/abs/2501.19364"}, "questions": {"value": "- **Model capacity**: Could the authors provide details on the number of parameters in Mamba-IVP (and optionally compare it to other baselines)? This would help assess the model’s complexity.\n\n- **Addressing concerns**: I would be glad to revise my evaluation if the authors can improve upon some of the points mentioned above, particularly by clarifying the scope of datasets, improving the presentation of Section 3, and expanding the discussion or comparisons with diffusion-based and consistency-based models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KBbIwqXGok", "forum": "GG9OqLP0vO", "replyto": "GG9OqLP0vO", "signatures": ["ICLR.cc/2026/Conference/Submission4935/Reviewer_E9kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4935/Reviewer_E9kx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882497414, "cdate": 1760882497414, "tmdate": 1762917778863, "mdate": 1762917778863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Mamba-IVP, a novel model that integrates a Mamba-based autoencoder to effectively handle time-series with block-wise missingness and improve robustness to noisy measurements. Mamba-IVP is evaluated on time-series classification and forecasting tasks, demonstrating a better performance/efficiency trade-off compared to IVP-VAE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Mamba-IVP shows better performance than IVP-VAE across different tasks and demonstrates improved robustness under varying noise and missingness levels.\n\n* The paper provides theoretical analysis that supports the use of Mamba blocks."}, "weaknesses": {"value": "**Contributions**\n\n* Mamba-IVP is basically an IVF-VAE with a Mamba autoencoder which seems an incremental technical contribution.\n\n**Evaluation**\n\n* Mamba-IVP achieves better performance than IVP-VAE but uses ~3× more parameters. Although Mamba-IVP remains ~40% faster, a comparison under the equal number of parameters is important to determine whether the gains are due to the Mamba architecture or simply more parameters.\n\n* Mamba-IVP is compared against GRU-D, while its counterparts (BRITS[1] and SAITS[2]) show stronger performance in prior work. Also, there are no comparisons to diffusion-based imputation methods [3,4], where [4] also uses a state-space architecture. More recent generative-based methods [5,6] are also not discussed or compared.\n\n&nbsp; &nbsp; &nbsp; The authors claim that these methods exhibit prohibitive computation cost, struggle to handle structured missingness and have limited robustness, but I could not find any evidence for these claims either in Mamba-IVP or IVP-VAE.\n\n* I can see that Mamba-IVP, as well as IVP-VAE and GRU-D, perform classification on top of the latent representations while many imputation methods, including [1-6], explicitly impute missing values and then train a downstream classifier on the filled data. Moreover, if Mamba-IVP follows the IVP-VAE training setup, the classifier is trained jointly with the main model. I believe it is an important detail to ensure fair comparisons.\n\n&nbsp; &nbsp; &nbsp; Are all other baselines also train classifiers jointly in the feature space? Could the authors compare unsupervised Mamba-IVP with s.o.t.a. baselines under the explicit imputation setup?\n\n**Writing**\n\n* The paper claims a 7.3% MSE reduction and 40× speedup, but the results show up to 4% MSE reduction and 40% speedup compared to IVP-VAE. Framing the gains relative to the weakest baseline sounds like an overclaim.\n \n* Figure 1 is somewhat confusing; I suggest refining the visualization and extending the caption to better explain the scheme.\n\n* The training procedure is not described. I assume it uses a reconstruction or VAE loss, but this should be explicitly stated. Also, it is unclear if the classification loss is used jointly with the main loss. \n\n---\n\n[1] BRITS: Bidirectional Recurrent Imputation for Time Series. NeuriPS2018\n\n[2] Saits: Self-attention-based imputation for time series. ESWA2023\n\n[3] CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. NeuriPS2021\n\n[4] Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models. TMLR2022\n\n[5] Frequency-aware Generative Models for Multivariate Time Series Imputation. NeurIPS2025\n\n[6] SADI: Similarity-Aware Diffusion Model-Based Imputation for Incomplete Temporal EHR Data. AISTATS2024"}, "questions": {"value": "Please address the concerns in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4tNtScYylS", "forum": "GG9OqLP0vO", "replyto": "GG9OqLP0vO", "signatures": ["ICLR.cc/2026/Conference/Submission4935/Reviewer_YMkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4935/Reviewer_YMkW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864484406, "cdate": 1761864484406, "tmdate": 1762917778484, "mdate": 1762917778484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing imputation methods to handle high missingness (distort, collapsing, heavy computing) in clinical time series may face unsafeness in deployment. In this work, the authors propose a generative method to tackle those challenges. Specifically, the work employed generative model with a dual-Mamba encoder and a Mamba-Hybrid decoder. The experiment is conducted using 3 medical time series datasets, compared to multiple forecasting and classification baselines. And the proposed method shows state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates an important problem, i.e. high missingness in medical time series in forecasting and classification tasks.\n- The experiment is conducted on 3 broadly employed datasets and used multiple evaluation metrics to justify the proposed method\n- The proposed method is clearly described"}, "weaknesses": {"value": "- The claimed gaps in existing work and proposed method seems not well connected and the comparison seems unfair. In Introduction, the authors only discuss the shorthands of existing imputation approaches, while the experiments investigated the forecasting and classification performance across approaches. The key motivation and the entire structure are unclear to me: there are types of existing works aimed at imputation, classification/forecasting, and classification/forecasting with missingness but may not impute the data explicitly. If the work only discussed the weaknesses imputation aimed approaches and claimed they can tackle them, I don’t think it’s a fair comparison or the claimed novelty in the technical part would not well supported.   It’s also not very clear to me how the proposed work tackle each claimed challenge (distort, collapsing, heavy computing) and the advances compared to prior work. If the focus is imputation, the evaluation metrics should include error metrics (e.g. RMSE) of measuring imputation effects, and they may need to compare their method to imputation method, and/or combine existing imputation method with a classifier/forecaster to check their potential in forecasting/classification tasks. Moreover, if one weakness of existing ODE- or diffusion-based approaches is heavy computing, execution time should be reported of baselines IVP-VAE.\n- If we say imputation approaches, statistical imputation methods include more modern ones, such as MICE, 3D-MICE [1], TA-DualCV [2], etc. Deep learning methods have many state-of-the-art ones [3-5], I would recommend them to be carefully discussed. And I don’t think existing imputation works all assumed random missingness, since many of them didn’t explicitly assume so, and they can impute real clinical data (naturally with mixed types of missingness) well.\n- A unified framework, some work already did missingness (with/without imputation)+forcasting/classification [3-7]. If the paper claims their framework contributes from this way (e.g., lines 117-120), I would recommend the related work has to be included as baselines for a fair comparison.  \n- I feel IVP-VAE is a strong baseline especially in terms of AUPRC, its results should be in bold as the standard deviation (if I understand correctly since the table 1 doesn’t have a notion after ±) from multiple runs indicates the un-distinguishable performance between IVP-VAE and Mamba-IVP. \nMinor issues:\n- Missing proper citations for introduction part that refers to important existing work. E.g. MIMIC-IV, and the entire part of discussing prior imputation works such as GRU-D et al. \n- I understand the page limitations, but related work at least should be placed in main content for self-containance \n- Seems a typo: line 256: “a multi-layer perceptron (ODE)” -> “a multi-layer perceptron (MLP)”\n\n[1] 3D-MICE: integration of cross-sectional and longitudinal imputation for multi-analyte longitudinal clinical data. Journal of the American Medical Informatics Association, 25(6), 645-653.\n\n[2] Reconstructing missing ehrs using time-aware within-and cross-visit information for septic shock early prediction. In 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI) (pp. 151-162). IEEE.\n\n[3] Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models, TMLR 2023\n\n[4] Missing value imputation methods for electronic health records.\" IEEE Access 11 (2023)\n\n[5] Kowsar, Ibna, Shourav B. Rabbani, and Manar D. Samad. \"Attention-Based Imputation of Missing Values in Electronic Health Records Tabular Data.\" 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI). IEEE, 2024.\n\n[6] Gradient Importance Learning for Incomplete Observations. 10th International Conference on Learning Representations (ICLR). 2022.\n\n[7] Temporal Belief Memory: Imputing Missing Data during RNN Training. In In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI-2018)."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYuAJGiTvJ", "forum": "GG9OqLP0vO", "replyto": "GG9OqLP0vO", "signatures": ["ICLR.cc/2026/Conference/Submission4935/Reviewer_h4Gc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4935/Reviewer_h4Gc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974959227, "cdate": 1761974959227, "tmdate": 1762917778141, "mdate": 1762917778141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}