{"id": "jHfXoYId0S", "number": 7331, "cdate": 1758016299290, "mdate": 1763346270702, "content": {"title": "Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space", "abstract": "Unified multimodal generative models aim to integrate image understanding and generation abilities, offering significant advantages in harnessing multimodal corpora, particularly interleaved text-image data. However, existing unified models exhibit limitations in image synthesis quality, autoregressive error accumulation, and image editing capability. In this work, we propose Nexus-Gen, a novel architecture that unifies image understanding, generation, and editing tasks in a shared image embedding space. This shared space serves as a bridge for the autoregressive and diffusion models, which seamlessly integrates their complementary strengths in cross-modal modeling. To mitigate the severe error accumulation during autoregressive embedding prediction, we propose a novel prefilled autoregression strategy that aligns training-inference dynamics by prefilling input sequences with learnable embeddings. After multi-stage and multi-task training on our constructed large-scale dataset with 26.3 million samples, Nexus-Gen achieves state-of-the-art performance on the evaluation benchmarks spanning image understanding, generation and editing tasks. All models, datasets, and codes will be released to facilitate further advancements across the field.", "tldr": "We propose Nexus-Gen, a novel architecture that unifies image understanding, generation, and editing tasks in a shared image embedding space.", "keywords": ["Unified Multimodal Generative Models", "Image Understanding", "Image Generation", "Image Editing", "Prefilled Autoregression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/761f09e66fcc00ad83764887267d9a7100f33f20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Nexus-Gen, a unified multimodal framework that combines an autoregressive model (based on Qwen2.5-VL) with a diffusion-based vision decoder (FLUX). Nexus-Gen uses a shared continuous image embedding space as the interface between language and vision, enabling image understanding, text-to-image generation, and image editing within one model. To address the error accumulation problem in autoregressive prediction of continuous embeddings, this work proposes a prefilled autoregression mechanism, where learnable embeddings are used to keep training and inference consistent. The model is trained in three stages on a 26M multi-task dataset and achieves strong results across several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces Nexus-Gen, a model designed to unify image understanding, generation, and editing within a single architecture. To address error accumulation during autoregressive prediction, the paper proposes a prefilled autoregression strategy, which is both simple and effective. The experimental results are comprehensive, covering understanding, generation, and editing benchmarks, with consistent improvements over comparable unified models. The work holds significant engineering and practical value, demonstrating a realistic pipeline for large-scale multimodal training."}, "weaknesses": {"value": "- The technical novelty is somewhat limited. The unified-embedding approach and the combination of AR and diffusion decoders have been widely explored in recent works. The prefilled autoregression strategy can be viewed as a practical adaptation of teacher-forcing or prefix-tuning strategies rather than a fundamentally new modeling concept.\n\n- While the prefilled strategy empirically reduces autoregressive MSE, the paper presents few variants (e.g., initialization schemes, shared vs. task-specific embeddings, or scheduled alternatives) or theoretical justification for when this heuristic may fail. More ablations would help establish robustness and applicability.\n\n- The three-stage training pipeline requires a systematic study of stage ordering, joint vs. staged optimization, and data-mixing strategies to justify the optimality of this schedule. Without such analysis, the superiority of the specific design remains unclear.\n\n- The model learns text-to-embedding during multitask pretraining and embedding-to-pixel during decoder adaptation. However, the paper does not clearly analyze how much of the generation quality comes from the autoregressive embedding predictor versus the diffusion decoder adaptation. Explicit ablations of the T2I objective would help clarify the contribution of each component.\n\n- The paper employs large, mixed datasets and a multi-stage training strategy, yet it omits detailed resource accounting (GPU type/count and total GPU hours) and a precise breakdown of data licensing and composition. This makes it difficult for others to reproduce the work or make fair cost-benefit comparisons."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qhbfBkDi7I", "forum": "jHfXoYId0S", "replyto": "jHfXoYId0S", "signatures": ["ICLR.cc/2026/Conference/Submission7331/Reviewer_gxb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7331/Reviewer_gxb3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207573007, "cdate": 1761207573007, "tmdate": 1762919448557, "mdate": 1762919448557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Nexus-Gen, a unified multimodal generative framework that integrates image understanding, generation, and editing within a shared image embedding space. The model bridges autoregressive and diffusion paradigms to leverage their complementary strengths. To address error accumulation in autoregressive embedding prediction, the authors propose a prefilled autoregression strategy that aligns training and inference by initializing image tokens with learnable embeddings. Trained on a large-scale dataset of 26.3M samples, Nexus-Gen achieves state-of-the-art performance across multimodal benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper proposes a comprehensive unified framework that integrates image understanding, generation, and editing, effectively bridging autoregressive and diffusion modeling paradigms within a shared embedding space.\n\n2.The prefilled autoregression strategy is a well-motivated solution to mitigate error accumulation and exposure bias in autoregressive prediction, improving training–inference consistency.\n\n3.The paper demonstrates strong empirical performance on multiple multimodal benchmarks, supported by large-scale experiments (26.3M samples) and clear ablation analyses."}, "weaknesses": {"value": "1.The architectural novelty appears limited — combining autoregressive and diffusion components within a shared embedding space has been explored in several prior unified or hybrid generative models. The framework design is well-engineered but does not represent a fundamentally new modeling paradigm.\n\n2.The proposed prefilled autoregression technique seems only loosely related to the unified multimodal objective. It primarily addresses the exposure bias issue in autoregressive prediction, which could, in principle, also apply to standard language models. Its contribution to multimodal unification therefore appears indirect.\n\n3.The paper lacks clarity on training cost and computational efficiency. Given the scale of 26.3M training samples, it is important to report details such as GPU hours, batch size, or hardware configuration to assess the practical feasibility of reproducing or extending the work."}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "63reTCM8Lf", "forum": "jHfXoYId0S", "replyto": "jHfXoYId0S", "signatures": ["ICLR.cc/2026/Conference/Submission7331/Reviewer_fhHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7331/Reviewer_fhHJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636731696, "cdate": 1761636731696, "tmdate": 1762919448068, "mdate": 1762919448068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Nexus-Gen, a novel unified model for image understanding, generation, and editing. The authors identify two key limitations in prior work (e.g., AR+VAE models, AR+Diffusion models): 1) poor image synthesis quality and 2) error accumulation during the autoregressive (AR) prediction of continuous image embeddings used to condition diffusion decoders. Nexus-Gen addresses this by proposing a unified architecture that bridges a powerful AR model (Qwen2.5-VL) and a diffusion decoder (FLUX.1-Dev) through a shared continuous image embedding space."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper's core technical contribution is a novel and highly effective method for mitigating error accumulation in AR models predicting continuous embeddings. This is a key problem in AR+Diffusion hybrids, and the solution is well-supported by a strong ablation (Fig 5).\n2.  The architecture is clean and effective. The use of a shared embedding space is standard, but the decision to employ two *separate*, specialized decoders (one for generation, one for editing) is a smart design choice that is well-justified by ablations (Fig 4) and leads to high-fidelity editing.\n3. The curation of the 26.3M dataset, and particularly the new `ImagePulse` editing dataset, is a significant resource contribution that addresses a known lack of high-quality editing data."}, "weaknesses": {"value": "1. The image editing performance (Table 3) is evaluated on a test set from their own `ImagePulse` dataset. While this dataset is a contribution, the pipeline used to create it (FLUX.1, Qwen-VL, ControlNet) could introduce biases that their model is well-suited to, but which may not generalize. It would be more convincing to also show evaluations on a hold-out from an existing, standard benchmark (e.g., Magic Brush, or a set from UltraEdit not used in training).\n2. The authors honestly state this as a limitation. While the model excels on \"understanding\" benchmarks (VQA, MMMU), these are largely recognition- and knowledge-based. The paper does not explore more complex, emergent capabilities like in-context learning with interleaved images/text or step-by-step visual reasoning, which are key motivators for unified models.\n3. The model's strong performance is built on top of very powerful, state-of-the-art base models (Qwen2.5-VL-7B and FLUX.1-Dev). This makes it slightly difficult to disentangle how much of the final performance comes from the \"glue\" (the novel contributions) versus the power of the components. This is a common issue in SOTA research but worth noting."}, "questions": {"value": "1.  Could you clarify the nature of the \"learnable embeddings\" used in the prefilled autoregression strategy? Are they static (like positional encodings), or are they dynamically modulated by the text prompt? How are they initialized and trained?\n2.  To address the potential for \"overfitting\" to your own data-generation pipeline, could you provide editing evaluation results on an existing, external benchmark (e.g., Magic Brush) that was not part of your training mix?\n3.  The architecture seems to naturally support in-context learning and interleaved image-text inputs. Have you performed any preliminary experiments on this? How does the model perform on such tasks compared to models like Emu3?\n4.  What is the computational overhead (e.g., in FLOPS or inference time) of the prefilled autoregression strategy compared to the naive token-by-token AR baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f2lMvuRm2p", "forum": "jHfXoYId0S", "replyto": "jHfXoYId0S", "signatures": ["ICLR.cc/2026/Conference/Submission7331/Reviewer_muUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7331/Reviewer_muUH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659611651, "cdate": 1761659611651, "tmdate": 1762919447352, "mdate": 1762919447352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Nexus-Gen, a unified multi-modal large language model (MLLM). The core innovations lies in the Prefilled Autoregression Strategy. By initializing image tokens with learnable embeddings (instead of relying on real previous tokens during training), this strategy aligns training and inference processes. Experiments are conducted on MMMU, GenEval, and ImagePulse."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: The prefilled autoregression strategy is a non-trivial improvement over naive autoregressive generation.​\n\n2. Clarity: Technical components (e.g., dataset construction, training stages) are described with specific numbers (e.g., 430K aesthetic fine-tuning samples) and clear definitions (e.g., ImagePulse’s three subsets), making the work reproducible.​"}, "weaknesses": {"value": "1. Lack of experiments to validate the effectiveness of the core innovation, i.e. prefilled autoregression strategy. For example, please report the result w/o prefilled autoregression strategy in Table1-3.\n\n2. Omitting comparisons with some recent work, e.g. OpenUni [a], BLIP3-o-8B [b], and Bagel [c]. BLIP3-o achieves 0.84, OpenUni 0.86 and Bagel 0.88 on GenEval. These scores are higher than that of the proposed method (0.81). \n\n3. Prompt Robustness Limitation: The paper acknowledges that Nexus-Gen requires specific instruction templates (e.g., structured prompts for editing), but it does not quantify this limitation (e.g., error rates with unstructured prompts) or propose mitigation strategies. This reduces usability for real-world scenarios where users may input arbitrary prompts.​\n\n[a] Wu S, Wu Z, Gong Z, et al. OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation[J]. arXiv preprint arXiv:2505.23661, 2025.\n[b] Chen J, Xu Z, Pan X, et al. Blip3-o: A family of fully open unified multimodal models-architecture, training and dataset[J]. arXiv preprint arXiv:2505.09568, 2025.\n[c] Deng C, Zhu D, Li K, et al. Emerging properties in unified multimodal pretraining[J]. arXiv preprint arXiv:2505.14683, 2025."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yAdOcY0EBr", "forum": "jHfXoYId0S", "replyto": "jHfXoYId0S", "signatures": ["ICLR.cc/2026/Conference/Submission7331/Reviewer_5r2C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7331/Reviewer_5r2C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841582404, "cdate": 1761841582404, "tmdate": 1762919446808, "mdate": 1762919446808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}