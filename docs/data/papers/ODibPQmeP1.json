{"id": "ODibPQmeP1", "number": 5208, "cdate": 1757866381064, "mdate": 1759897988422, "content": {"title": "CHARM: Calibrating Reward Models With Chatbot Arena Scores", "abstract": "Reward models (RMs) play a crucial role in Reinforcement Learning from Human Feedback by serving as proxies for human preferences in aligning large language models. However, they suffer from various biases which could lead to reward hacking. In this paper, we identify a model preference bias in RMs, where they systematically assign disproportionately high scores to responses from certain policy models, leading to unfair judgments. To mitigate this bias, we propose a calibration method named **CH**atbot **A**rena calibrated **R**eward **M**odeling (**CHARM**) that leverages Elo scores from the Chatbot Arena to construct debiased preference datasets and adjust reward model scoring. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench and exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings.  Beyond this, **CHARM** enhances robustness to stylistic variations, mitigates implicit pattern bias, and generalizes to unseen models. These results demonstrate that **CHARM** provides a simple, effective, and broadly applicable approach to building more reliable and fair reward models.", "tldr": "We propose CHARM, a calibration method that mitigates model preference bias in reward models by leveraging Elo scores from Chatbot Arena, improving fairness and alignment with human preferences.", "keywords": ["large language model", "reward model", "bias mitigation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/70a485629a8eac83487a5bb2b3ff18abe10171a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission compares the scores generated by popular reward models with the elo scores provided by chatbot arena and finds an inconsistency for certain models, which the submission interprets as misscalibration.\nTo prevent this issue, the authors to propose to first generate a dataset using the over-valued policies and reference policies and then use this dataset to construct a calibrated dataset of winning/losing reply pairs.\nIn experiment they show that finetuning an RM using this dataset can improve agreement with the reward bench dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The authors identify a new bias of popular reward models, particularly the preference for certain models beyond the probability implied by the chatbot arena elo. This is an interesting finding and is useful to be aware of when training RMs\n * The proposed method yields an improvement in RM accuracy on reward bench\n * The analysis of stylistic variations is interesting"}, "weaknesses": {"value": "* the entire paper is based on the premise that chatbot arena accurately reflects human preferences and that we want reward models to accurately correspond to chatbot arena scores, which is not sufficiently justified, particularly when actually using an RM for post-training\n * experiments also only consider correcting for the over-valuation of a single model (gemma-2-9b-it-SimPO). It is thus not clear whether  the results hold for other over-valued models, or in cases where we want to correct for multiple over-valued models at once \n * experiments show that calibration on chatbot arena leads to an improved reward accuracy on RM-bench. Experiments do not investigate whether using the calibrated RM is beneficial when preference-tuning a model. Recent research [2,3] has shown that reward accuracy on its own does not necessarily result in a better alignment of a post-trained model. Using the calibrated RM for post-training would significantly strengthen the experimental validation\n * terminology in the paper is inconsistent with the common usage of \"calibration\" in the context of supervised learning. Calibration usually refers to the error in probabilities $P_\\theta(y|x)$ given by a classifier vs the true $P(y|x)$, in particular on a sample-wise level. The submission instead considers a class/model level bias of the reward model. Further the entire literature on classifier calibration is entirely ignored by the submission, see for example [1] for a survey\n\n\n[1] Filho et al. \"Classifier Calibration: A survey on how to assess and improve predicted class probabilities\", Machine Learning 2023\n[2] Razin et al. \"What Makes a Reward Model a Good Teacher? An Optimization Perspective\", NeurIPS 2025\n[3] Chen et al. \"The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\", EMNLP 2024"}, "questions": {"value": "* Why was the target winrate P(O) determined from elo scores? Chatbot arena provides head-to-head winrates of model pairings, which directly yield P(O) per matchup instead of averaging over models. It seems like using this would be more direct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9fykZeKGb", "forum": "ODibPQmeP1", "replyto": "ODibPQmeP1", "signatures": ["ICLR.cc/2026/Conference/Submission5208/Reviewer_Wq5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5208/Reviewer_Wq5j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761470422400, "cdate": 1761470422400, "tmdate": 1762917947049, "mdate": 1762917947049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies model preference bias—a systematic tendency of scalar reward models (RMs) to overrate responses from certain policy models (e.g., Gemma-2-9b-it-SimPO), even when those models perform modestly on human-aligned benchmarks like Chatbot Arena. To address this, the authors propose CHARM, a calibration method that leverages publicly available Elo scores from Chatbot Arena to adjust RM scoring via a learned offset Δ, thereby constructing debiased preference pairs for fine-tuning. Experiments show that CHARM improves RM performance on RM-Bench and RewardBench (especially in the Chat domain), better aligns RM judgments with human preferences, enhances robustness to stylistic variations, and generalizes to unseen models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Studies an important and underexplored problem: calibrating RMs using real-world human preference signals (Chatbot Arena Elo).\n2. Demonstrates clear empirical gains for small-to-medium scalar RMs after calibration, with thorough analysis of bias reduction and generalization."}, "weaknesses": {"value": "1. The method fits RM scores to Elo-derived win rates via MSE. But why not train directly on Chatbot Arena’s raw battle data or use Elo-predicted probabilities as soft labels? That would seem more direct and principled.\n2. Results are limited to scalar RMs. Given the growing shift toward Generative Reward Models (GRMs), it’s crucial to show whether CHARM can also improve GRMs (e.g., by relabeling preference data used to train LLM-as-a-judge systems).\n3. The Elo scores used—are they the standard raw Arena Elo or style-controlled (e.g., length-normalized)? If the latter, the observed debiasing might stem from style control rather than the calibration mechanism itself. Clarification is needed."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nRGMLy4B9B", "forum": "ODibPQmeP1", "replyto": "ODibPQmeP1", "signatures": ["ICLR.cc/2026/Conference/Submission5208/Reviewer_8Hwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5208/Reviewer_8Hwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996185735, "cdate": 1761996185735, "tmdate": 1762917946587, "mdate": 1762917946587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CHARM, a method to alleviate reward hacking via calibrating the Reward Models (RM) with the Elo rating (Human Preference). Specifically, they utilize the Elo Rating from the public Chatbot Arena and calibrate the reward model's training dataset to align the win-rate estimated from the reward scores with the expected win-rate from the Elo rating. They then fine-tune the reward model to calibrate the reward score to mitigate the Model Preference Bias. Experiments show that the calibrated RM is more aligned with human preference/Elo rating and obtains performance gains against existing RM benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear and interesting. Using the Elo rating to calibrate the reward models to make the reward model more aligned with human preferences is sound.\n2. The formulation of the method part is clear and easy to follow. The authors first analyze the correlation between Elo rating and reward model scores, identifying the potential Model Preference Bias, and then explain how they address it (via calibration).\n3. Experiments show that using the calibrated dataset to fine-tune the reward model can improve its performance."}, "weaknesses": {"value": "1. My main concern is that CHARM is not scalable. Each reward models need their own calibrated dataset for further fine-tuning. Besides, as the elo rating is updated frequently, do you need to recalibrate when the Chatbot Arena is updated? (I may misunderstand the method; point me out if I'm wrong.)\n2. While over-valued and reference models play an important role in calibration, how to select them is not adequately elaborated in the paper. This hinders the critical understanding of CHARM, i.e., how to select or identify the potentially over-valued models in a more systematic manner."}, "questions": {"value": "1. Instead of using the calibrated dataset, what if we directly use the preference data collected from the chatbot arena (e.g., LMSYS-Chat-1M) to fine-tune the reward model?\n2. How to select the over-valued and reference models?\n3. Can you explain the difference between the calibrated data curated by CHARM and human-annotated preference data?\n4. Have you considered using training-free calibration methods, such as temperature scaling, for calibrating the scalar RM?\n5. Does the update of the chatbot arena's Elo rating affect the calibration performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JIjOn4ww6t", "forum": "ODibPQmeP1", "replyto": "ODibPQmeP1", "signatures": ["ICLR.cc/2026/Conference/Submission5208/Reviewer_B6Db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5208/Reviewer_B6Db"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025355619, "cdate": 1762025355619, "tmdate": 1762917946229, "mdate": 1762917946229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies \"Model Preference Bias\" in reward models, which is a systematic tendency to assign disproportionately high scores to responses from certain policy models. To mitigate this, the authors propose CHARM, a calibration method that uses Elo ratings from Chatbot Arena as a proxy for ground-truth human preferences. CHARM computes a global score offset Δ for an \"over-valued\" model to align the RM's empirical win rate with the Elo-derived expected win rate. The calibrated RMs show improved performance on RM-Bench and RewardBench, better alignment with Chatbot Arena rankings, and enhanced robustness to stylistic variations. The authors also introduce a \"Mismatch Degree\" metric to quantify bias and demonstrate generalization to unseen models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper articulates a subtle but important bias (i.e., \"Model Preference Bias\") that has been overlooked in reward modeling literature. \n\n- CHARM is straightforward to implement, requiring only a single additive offset per model. This simplicity makes it attractive for practitioners who cannot afford complex retraining pipelines. The method leverages readily available Chatbot Arena data, which is a clever use of existing resources.\n\n- The consistent improvements across multiple RMs (Table 1) and the reduction in Mismatch Degree (Figure 2) are compelling. The analysis of stylistic patterns (Table 3) provides plausible evidence that CHARM mitigates implicit biases beyond just model-specific ones."}, "weaknesses": {"value": "- Oversimplified Calibration Mechanism: The core assumption that a *single global additive offset* Δ can correct complex, instruction-dependent biases is theoretically questionable. Model preference bias likely manifests differently across prompt categories (e.g., coding vs. creative writing), yet CHARM applies a uniform correction. The paper provides no analysis of whether Δ varies by domain or instruction type, nor ablations showing why a more nuanced correction (e.g., instruction-specific offsets) is unnecessary.\n\n- Weak Ground Truth Assumption: The method treats Chatbot Arena Elo scores as absolute ground truth for human preferences, but these scores have well-documented limitations: they reflect a specific user population, are influenced by positional bias, and conflate multiple criteria (helpfulness, safety, style). The paper doesn't address how Chatbot Arena's biases might propagate into CHARM. For example, if Arena users prefer verbose responses, CHARM might inadvertently bake this length bias *into* the RM rather than remove it.\n\n- Limited Calibration Scope: The main experiments calibrate using only *one* over-valued model (Gemma-2-9b-it-SimPO) and *one* reference model (GPT-4o-mini). This is a major limitation: (1) The paper claims bias is systematic across \"preference-optimized\" models, but only demonstrates calibration on a single instance. Testing with multiple over-valued models (e.g., DPO-tuned, PPO-tuned) is essential to validate generalizability. (2) The offset Δ is fundamentally tied to the choice of π_R. The paper fails to explore how Δ changes with different references (e.g., a weaker model like Llama-3-3.1-8B vs. GPT-4o). This raises questions about the stability and interpretability of the calibration."}, "questions": {"value": "What is the reward model in line 240 to produce the uncalibrated preference dataset? Will the choice of this reward model influence the final results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vIvTja9dac", "forum": "ODibPQmeP1", "replyto": "ODibPQmeP1", "signatures": ["ICLR.cc/2026/Conference/Submission5208/Reviewer_q9aZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5208/Reviewer_q9aZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5208/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762569816511, "cdate": 1762569816511, "tmdate": 1762917945456, "mdate": 1762917945456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}