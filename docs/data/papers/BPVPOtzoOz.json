{"id": "BPVPOtzoOz", "number": 13584, "cdate": 1758219444926, "mdate": 1763769338046, "content": {"title": "NetArena: Dynamically Generated LLM Benchmarks for Network Applications", "abstract": "As large language models (LLMs) expand into high-stakes domains like network\nsystem operations, evaluating their real-world reliability becomes increasingly\ncritical. However, existing benchmarks risk contamination due to static design,\nshow high statistical variance from limited dataset size, and fail to reflect the\ncomplexity of production environments. We introduce NetArena, a dynamic\nbenchmark generation framework for network applications. NetArena features a\nnovel abstraction and unified interface that generalizes across applications, effec-\ntively addressing the challenges of dynamic benchmarking posed by the diversity\nof network tasks. At runtime, users can generate unlimited queries on demand.\nNetArena integrates with network emulators to provide execution-time feedback\non correctness, safety, and latency. We demonstrate NetArena on three repre-\nsentative applications and find that (1) it significantly improve statistical reliability\namong LLM agents (confidence interval overlap reduced from 85% to 0), (2) agents\nachieve only 13–38% average performance (as low as 3%) for large-scale, realistic\nqueries, (3) it reveals finer-grained behaviors missed by static, correctness-only\nbenchmarks. NetArena also enables use cases such as SFT and RL fine-tuning on\nnetwork system tasks. Code is available anonymously at https://anonymous.4open.science/r/netarena_iclr2026-BE94/README.md", "tldr": "", "keywords": ["LLM for Network Systems", "Dynamic Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dfed2e82ff42e9f8f3fa442a77019b5c5deb4eb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a dynamic benchmarking framework named NETARENA, designed to evaluate the performance of LLMs in network system applications. Unlike traditional static benchmarks, NETARENA can dynamically generate unlimited queries and integrates with high-fidelity network emulators to assess correctness, safety, and latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a dynamic LLM benchmark generation framework specifically for the networking domain, demonstrating clear innovation.\n2. Beyond traditional correctness metrics, the benchmark incorporates safety and latency as key evaluation dimensions, which better align with the needs of high-stakes systems.\n3. The paper is well written and clearly presented."}, "weaknesses": {"value": "1. Although the paper defines safety and latency evaluation standards, it lacks explicit quantitative formulas or threshold specifications.\n2. The evaluation focuses on three types of network tasks, but broader validation across more diverse scenarios is missing. The authors could further discuss potential directions for future evaluation (additional experiments are not necessary).\n3. While correctness, safety, and latency often involve trade-offs, the paper does not provide corresponding quantitative analyses to characterize these relationships."}, "questions": {"value": "1. Can NETARENA support cross-task generalization testing? For example, can a model trained on routing tasks generalize to microservice policy troubleshooting tasks?\n2. In practice, how much time and computational resources are required to complete a full-scale evaluation?\n3. Were the dynamically generated natural language task templates reviewed or validated by human experts? How do you ensure consistency between task descriptions and the simulated system states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "YLmk4uW7GU", "forum": "BPVPOtzoOz", "replyto": "BPVPOtzoOz", "signatures": ["ICLR.cc/2026/Conference/Submission13584/Reviewer_d7Bd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13584/Reviewer_d7Bd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570206898, "cdate": 1761570206898, "tmdate": 1762924176111, "mdate": 1762924176111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework, NETARENA, for evaluating LLM-based agents on realistic, execution-time network/system tasks. Instead of relying on small, static, possibly contaminated benchmarks, it dynamically generates tasks over a unified state–action interface, runs them in emulators (Mininet/K8s/DC simulator), and scores agents on correctness, safety, and latency. Experiments across several network-style applications show current LLM agents perform much worse in these realistic, dynamic settings than static benchmarks suggest."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear unified state–action abstraction that works across three concrete network apps (DC capacity planning, Mininet routing, K8s policy troubleshooting), not just a toy demo.\n- Dynamic, on-demand query generation with stochastic sampling and emulator-backed ground truth, explicitly to cut contamination and widen coverage\n- Execution-time evaluation on correctness, safety, and latency inside real emulators (Mininet, K8s, DC simulator), which exposes failure modes that static, correctness-only benchmarks miss"}, "weaknesses": {"value": "- RL/SFT “use cases” are proof-of-concept and on small models (Qwen2.5-0.5B, limited SFT splits), so the “can be used for rl training” claim is ahead of the evidence.\n- All results are still in three networking-style environments; claims of generality beyond these domains are argued but not empirically shown.\n- The dynamic generation relies on hand-designed templates and app-specific state equivalence/safety checks; portability to other operators’ emulators may be non-trivial."}, "questions": {"value": "1. In 5.1 you show a GRPO run with Qwen2.5-0.5B in Mininet and note it “does not fully solve routing issues.” Can you clarify whether NETARENA currently supports stable, long-horizon RL runs (multiple episodes, curriculum, failure replay), or whether this is mainly a demonstration of feasibility? If it’s the latter, please make the scope explicit and report at least learning curves / success-per-episode to show the environment is not too sparse.\n\n2. You claim the unified state–action abstraction “generalizes across applications,” but all experiments are DC capacity planning, Mininet routing, and K8s policy. Can you point to a non-network/system domain where you tried to plug in the same pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OHywUu3goR", "forum": "BPVPOtzoOz", "replyto": "BPVPOtzoOz", "signatures": ["ICLR.cc/2026/Conference/Submission13584/Reviewer_kJeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13584/Reviewer_kJeZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960983825, "cdate": 1761960983825, "tmdate": 1762924175822, "mdate": 1762924175822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents NETARENA, a dynamic benchmarking framework for evaluating LLMs in realistic network and system environments. It addresses critical limitations of existing static benchmarks, including data contamination risks, high statistical variance from limited dataset sizes, and inadequate representation of production environment complexity.. The framework defines a unified state–action abstraction that enables automatic query and ground truth generation across applications such as datacenter capacity planning, routing misconfiguration, and microservice policy troubleshooting. By integrating high-fidelity network emulators like Mininet and Kubernetes, NETARENA provides runtime feedback on correctness, safety, and latency. Experiments with models such as GPT-4o and Qwen-72B demonstrate low average correctness (13–38%), underscoring the complexity of real-world network tasks. NETARENA also supports supervised fine-tuning and reinforcement learning, enabling scalable, dynamic, and contamination-resistant evaluation of LLM agents in safety-critical network operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper effectively solves data contamination risk through dynamic generation, eliminates statistical unreliability of small datasets , and captures real-world complexity missing in existing benchmarks\n1. It integrates with production-grade emulators (Mininet, Kubernetes), and provides execution-grounded assessment beyond simple correctness, including safety and latency metrics.\n1. It supports 9,250+ queries with unlimited generation, while maintaining diversity across complexity levels and task types that static benchmarks cannot achieve.\n1. The framework provides an environment that supports RL training and evaluation of LLMs in realistic network applications."}, "weaknesses": {"value": "1. Limited agent diversity: The evaluation only includes baseline prompting strategies (CoT, Few-shot, ReAct), which may not fully represent the capabilities of advanced LLM-based agents in network reasoning tasks.\n1. The integration with high-fidelity emulators may introduce significant setup challenges, potentially reducing the reproducibility and accessibility of the framework.\n1. While correctness, safety, and latency are meaningful metrics, the evaluation could be enriched with additional dimensions.\n1. Although RL post-training is mentioned, the paper does not include experimental results or analysis for RL-based fine-tuning.\n\nMinors:\n1. QWen -> Qwen"}, "questions": {"value": "1. What is the complexity of running the emulators and evaluation process at scale?\n1. How is the ground truth constructed for the SFT dataset described in Section 4.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SJsefvQtgx", "forum": "BPVPOtzoOz", "replyto": "BPVPOtzoOz", "signatures": ["ICLR.cc/2026/Conference/Submission13584/Reviewer_7kRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13584/Reviewer_7kRM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13584/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762346002678, "cdate": 1762346002678, "tmdate": 1762924175560, "mdate": 1762924175560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}