{"id": "Fw6PBELcFs", "number": 17779, "cdate": 1758280443017, "mdate": 1759897154436, "content": {"title": "HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness", "abstract": "Reinforcement Learning (RL) has become a key driver for enhancing the long chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs). \nHowever, prevalent methods like GRPO often fail when task difficulty exceeds the model's capacity, leading to reward sparsity and inefficient training. \nWhile prior work attempts to mitigate this using off-policy data, such as mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide policy updates \nIn this work, we identify a core issue underlying these failures, which we term low training affinity. \nThis condition arises from a large distributional mismatch between external guidance and the model's policy. \nTo diagnose this, we introduce Affinity, the first quantitative metric for monitoring exploration efficiency and training stability. \nTo improve Affinity, we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness, an adaptive hinting framework. \nInstead of providing direct answers, HINT supplies heuristic hints that guide the model to discover solutions on its own, preserving its autonomous reasoning capabilities.\nExtensive experiments on mathematical reasoning tasks show that HINT consistently outperforms existing methods, achieving state-of-the-art results with models of various scales, while also demonstrating significantly more stable learning and greater data efficiency.\nCode is available on Github.", "tldr": "", "keywords": ["Large Language Models", "Group Relative Policy Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ce47c501ded8713f7e18ab481d124f9bbe273a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduced first a new definition of low training affinity, where it is a key failure of RL that incorporate off-policy data. And they introduce a metric that can monitor it during RL training. Furthermore, they introduce a new method that utilizes hints that mimics Socratic method in teaching, where it provides thoughtful prompts rather than partial answers. Results demonstrated its robustness and better generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- writing mostly clear, topic is relevant"}, "weaknesses": {"value": "- I dont see too much of a difference of this than other methods that also provide hints, such as what if you just give the answer to the model and let it reason, which is what STaR did with rationalization and also [1].\n- the method can be summarized to use a teacher model to rewrite the prompt, which the novelty can be an issue overall since many papers did similar things such as STaR with rationalization\n- the results seem very weird, for 7b models, for the average scores, almost all methods that leverage off policy data is worse than GRPO, which is not really what the other papers reported in their papers. The 3b model similar trend was found.\n\n[1] GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning"}, "questions": {"value": "- is the hints/question pair fixed thoughout training?\n- which teacher model was used for the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0IkHBNLq7B", "forum": "Fw6PBELcFs", "replyto": "Fw6PBELcFs", "signatures": ["ICLR.cc/2026/Conference/Submission17779/Reviewer_cu2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17779/Reviewer_cu2X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760683280396, "cdate": 1760683280396, "tmdate": 1762927621227, "mdate": 1762927621227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a RL framework that leverages hints to handle harder problems where GRPO fails to generate any successful responses, resulting in zero rewards and undefined advantages. The proposed method first generates hints for training problems using a more capable model (e.g. Qwen2.5-72B-Instruct) and fine-tunes a less capable model (e.g. Qwen2.5-7B) with these generate hints as guidance. The proposed method is evaluated on math reasoning benchmarks and outperforms other hint-based RL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive experiments with diverse baseline methods substantially strengthens the paper’s claims.\n- Overall, the paper is well-written and easy to understand."}, "weaknesses": {"value": "- The reliability of the evaluation for Qwen2.5-7B is questionable, since the majority of hint-based baselines underperform GRPO, despite GRPO not leveraging any hints.\n- The idea of using hints is not particularly novel. STaR first introduced answer-level hints [1], and at least two papers on solution-level hints have been accepted to this year’s NeurIPS [2,3]. Actually, most hint-based methods are conceptually similar to jump-start RL [4], in which a guide policy first reduces the search space and then the base policy solves the problem.\n- Since the hints are generated by a more capable model, a fair comparison should include a knowledge distillation experiment. However, the paper does not report such results.\n\n[1] Zelikman et al., STaR: Bootstrapping Reasoning With Reasoning, NeurIPS 2022 \\\n[2] Moon et al., Learning to Better Search with Language Models via Guided Reinforced Self-Training, NeurIPS 2025 \\\n[3] Zhang et al., BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning, NeurIPS 2025 \\\n[4] Uchendu et al., Jump-Start Reinforcement Learning, ICML 2023"}, "questions": {"value": "Since most hint-based baselines were released only in mid-2025, I appreciate that the authors have reproduced and reported them. Given that RL performance can vary substantially depending on factors such as the PPO clipping coefficient, the handling of truncated samples, and the specific RL library used, it is challenging to reproduce the baselines exactly. Therefore, I do not place much weight on the precise numerical comparison with these baselines. However, I believe that comparisons with GRPO or knowledge distillation should be conducted more carefully. I will raise the score to 6 or 8 if you answer my questions well.\n\n- The naive knowledge-distillation baseline should (1) generate candidate solutions with Qwen2.5-72B-Instruct, (2) discard unsuccessful responses, and (3) fine-tune Qwen2.5-7B on the remaining successful responses. You may also (4) apply GRPO on top of this baseline. Could you run these experiments?\n- You claim that GRPO suffers from entropy collapse. Have you tested recent techniques to address this? For example, a higher clipping ratio [1], Dr-GRPO–style loss aggregation [2], or excluding truncated samples from training [3].\n- I believe that the form of the hint plays a critical role. A hint may take various forms, for instance, a partial solution, a concise text extracting essential information from the solution, or a human-written complete solution. It appears that you chose the second form. Could you discuss the advantages and disadvantages of each design choice? You don't have to do additional experiments for this question.\n\n[1] Yu et al., DAPO: An Open-Source LLM Reinforcement Learning System at Scale, arXiv 2025 \\\n[2] Liu et al., Understanding R1-Zero-Like Training: A Critical Perspective, COLM 2025 \\\n[3] Luo et al., DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I have no ethical concerns"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "atLLCty4OG", "forum": "Fw6PBELcFs", "replyto": "Fw6PBELcFs", "signatures": ["ICLR.cc/2026/Conference/Submission17779/Reviewer_n2ue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17779/Reviewer_n2ue"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761296525116, "cdate": 1761296525116, "tmdate": 1762927620740, "mdate": 1762927620740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel hinting framework called HINT, designed to alleviate the commonly observed issue of reward sparsity in reinforcement learning for large language model reasoning tasks. Instead of providing answer-level hints, HINT utilizes heuristic hints to guide the model towards autonomous reasoning, thereby avoiding policy drift and training instability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduce the novel concept of Training Affinity, along with quantitative metrics—EUR, UC, and Affinity—to evaluate training stability.\n- The proposed heuristic hinting mechanism demonstrates advantages over prior methods such as GRPO in various experimental settings."}, "weaknesses": {"value": "- Although the paper dedicates considerable space to the discussion of Affinity and related metrics, these metrics are only used for monitoring during training and are not integrated into the optimization objective. The core contribution still boils down to providing heuristic hints. While these hints appear to yield better scores under the newly proposed metrics, the practical significance of these metrics remains unclear. For example: Why is staying within the trust region inherently good? Why is moderate variability in updates desirable? These assumptions are not justified either theoretically or empirically.\n- The writing quality is suboptimal. Many terms are introduced without explanation—for instance, “rollout prompt” and “policy prompt.” While one might infer that these refer to the input during sampling and policy optimization respectively, such terminology is not standard and should be clearly defined. In addition, basic concepts from reinforcement learning and GRPO could be better introduced **before** the methodology section to improve clarity and logical flow."}, "questions": {"value": "- Since HINT may involve two rounds of sampling in the case of sparse rewards, the method could incur higher training time. As shown in Figure 5, under the same time budget, HINT actually produces fewer rollouts. Were the baselines compared under the same number of training steps or the same wall-clock time? If the goal is to demonstrate the efficiency of HINT, comparisons based on fixed time budgets would be more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "am0kBTkxRs", "forum": "Fw6PBELcFs", "replyto": "Fw6PBELcFs", "signatures": ["ICLR.cc/2026/Conference/Submission17779/Reviewer_MRbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17779/Reviewer_MRbK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742286076, "cdate": 1761742286076, "tmdate": 1762927619763, "mdate": 1762927619763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a fundamental bottleneck in reinforcement learning (RL) for large language model (LLM) reasoning: reward sparsity when task difficulty exceeds model capacity. The authors identify a novel failure mode—low training affinity—arising from distributional mismatch between off-policy guidance (e.g., hints or supervised fine-tuning data) and the model’s evolving policy. To quantify and mitigate this, they introduce:\n\nAffinity metric, a unified quantitative measure combining Effective Update Ratio (EUR) and Update Consistency (UC) to track exploration efficiency and training stability.\n\nHINT framework (Helping Ineffective rollouts Navigate Towards effectiveness), an adaptive two-stage hinting scheme where heuristic, non-answer-revealing hints are provided only when all sampled trajectories are incorrect. These hints are generated by a stronger teacher model and serve to “rescue” failed rollouts without leaking answers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Methodological Soundness\n\nThe two-stage adaptive rollout is clean and well-motivated. By activating hints only under reward sparsity, HINT balances guidance and autonomy—avoiding shortcut learning associated with direct answer hints.\n\n2. Empirical Backup\n\n- Comprehensive benchmarks and strong baselines (GRPO, GHPO, LUFFY, etc.) provide convincing evidence.\n- The inclusion of Affinity curves (EUR/UC plots) gives transparency to training dynamics and stability analysis.\n- Additional analyses (entropy dynamics, sampling efficiency, case studies) strengthen interpretability."}, "weaknesses": {"value": "1. Evaluation\n\nAll experiments rely on Qwen2.5 models; testing on a diverse set of latest base models (e.g., Qwen3, Llama) would better demonstrate robustness.\n\nIn Figure 4, it seems that GRPO behaves similar to HINT at the middle-late stage of RL training, but GRPO training shows inferior performance boost than HINT. So how is such metrics relevant to the overall performance after training?\n\n2. Theoretical Grounding\n\nThe Affinity formulation (Eq. 3) is intuitive but somewhat heuristic—no theoretical derivation linking it to expected return or policy divergence bounds is provided. A more formal justification or ablation could clarify sensitivity."}, "questions": {"value": "What the symbol s_i and a_i mean in Eq. (1)? Though people can guess, but it's better to avoid such guess."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "imHUe6hd1O", "forum": "Fw6PBELcFs", "replyto": "Fw6PBELcFs", "signatures": ["ICLR.cc/2026/Conference/Submission17779/Reviewer_dYuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17779/Reviewer_dYuh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865956639, "cdate": 1761865956639, "tmdate": 1762927618822, "mdate": 1762927618822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}