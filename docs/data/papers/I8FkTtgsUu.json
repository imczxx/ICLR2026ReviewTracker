{"id": "I8FkTtgsUu", "number": 6044, "cdate": 1757951437717, "mdate": 1759897938378, "content": {"title": "Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations", "abstract": "Learning representations that transfer well to diverse downstream tasks remains a central challenge in representation learning. Existing paradigms---contrastive learning, self-supervised masking, and denoising auto-encoders---balance this challenge with different trade-offs. We introduce the {contrastive Mutual Information Machine} (cMIM), a probabilistic framework that extends the Mutual Information Machine (MIM) with a contrastive objective. While MIM maximizes mutual information between inputs and latents and promotes clustering of codes, it falls short on discriminative tasks. cMIM addresses this gap by imposing global discriminative structure while retaining MIM’s generative fidelity.\n\nOur contributions are threefold. First, we propose cMIM, a contrastive extension of MIM that removes the need for positive data augmentation and is substantially less sensitive to batch size than InfoNCE. Second, we introduce {informative embeddings}, a general technique for extracting enriched features from encoder--decoder models that boosts discriminative performance without additional training and applies broadly beyond MIM. Third, we provide empirical evidence across vision and molecular benchmarks showing that cMIM consistently outperforms MIM and InfoNCE on classification and regression tasks while preserving competitive reconstruction quality.\n\nThese results position cMIM as a unified framework for representation learning, advancing the goal of models that serve both discriminative and generative applications effectively.", "tldr": "We introduce cMIM, a contrastive extension of the probabilistic auto-encoder MIM, that unifies generative and discriminative representation learning without contrastive data augmentation and is robust to small batch sizes.", "keywords": ["representation learning", "contrastive learning", "latent variable models", "probabilistic models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/395935796c9dc4a65147d92150725f879f4ecd68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Contrastive Mutual Information Machine (CMIM), a novel probabilistic framework for self-supervised representation learning. CMIM extends the Mutual Information Machine (MIM) by incorporating a contrastive objective.\nThe central problem addressed is the practical gap between existing representation learning paradigms:\n1. Generative Auto-Encoders (like MIM): Learn structured latent spaces and preserve generative fidelity, but their representations often underperform on discriminative downstream tasks. 2. Contrastive Methods (like InfoNCE): Achieve strong discriminative performance, but their success is often contingent on carefully chosen positive data augmentations and can be highly sensitive to batch size"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CMIM successfully eliminates two major pain points of modern contrastive learning: the need for positive data augmentation and the sensitivity to batch size.\n\n2. CMIM effectively combines the strengths of generative modeling (MIM) with the power of contrastive learning, creating a model that excels at both discriminative and generative tasks.\n\n3. The Informative Embeddings concept is an original, powerful, and generalizable technique to enhance discriminative performance post-training."}, "weaknesses": {"value": "The paper notes that CMIM retains the MIM mutual-information bound but \"does not enjoy the classical InfoNCE MI bound\". While the empirical results are strong, a deeper theoretical investigation into the exact mutual information dynamics imposed by the CMIM objective would strengthen the paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eDeplxBi9g", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Reviewer_Uqjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Reviewer_Uqjf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595327937, "cdate": 1760595327937, "tmdate": 1762918426929, "mdate": 1762918426929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Contrastive Mutual Information Machine (CMIM), a novel probabilistic framework for self-supervised representation learning. CMIM extends the Mutual Information Machine (MIM) by incorporating a contrastive objective.\nThe central problem addressed is the practical gap between existing representation learning paradigms:\n1. Generative Auto-Encoders (like MIM): Learn structured latent spaces and preserve generative fidelity, but their representations often underperform on discriminative downstream tasks. 2. Contrastive Methods (like InfoNCE): Achieve strong discriminative performance, but their success is often contingent on carefully chosen positive data augmentations and can be highly sensitive to batch size"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CMIM successfully eliminates two major pain points of modern contrastive learning: the need for positive data augmentation and the sensitivity to batch size.\n\n2. CMIM effectively combines the strengths of generative modeling (MIM) with the power of contrastive learning, creating a model that excels at both discriminative and generative tasks.\n\n3. The Informative Embeddings concept is an original, powerful, and generalizable technique to enhance discriminative performance post-training."}, "weaknesses": {"value": "The paper notes that CMIM retains the MIM mutual-information bound but \"does not enjoy the classical InfoNCE MI bound\". While the empirical results are strong, a deeper theoretical investigation into the exact mutual information dynamics imposed by the CMIM objective would strengthen the paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eDeplxBi9g", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Reviewer_Uqjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Reviewer_Uqjf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595327937, "cdate": 1760595327937, "tmdate": 1763689607601, "mdate": 1763689607601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a self-supervised framework that unifies contrastive learning and mutual information maximization without requiring positive-pair augmentations. By adding a probabilistic contrastive term to the Mutual Information Machine (MIM), cMIM encourages angular separation among dissimilar samples while maintaining MIM’s local clustering and reconstruction quality. It also proposes informative embeddings, extracted from decoder hidden states, to improve downstream discrimination. Across vision and molecular benchmarks, cMIM outperforms MIM and InfoNCE on classification and regression tasks. The authors also explore sensitivity to batch size and shows that cMIM does not have as stong a dependence on batch size as other methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- An interestesting new loss function that has contrastive-like properties without explicitly contrasting augmented input signals\n- Nice theoretical connection to the InfoNCE loss\n- Nice analysis of batch dependence and a promising reason to consider this kind of loss as batch size scaling is a large challenge in modern contrastive learning"}, "weaknesses": {"value": "- nit: in lime 52 you introduce x and z without definiing/stating what they are first\n- nit:  In figure 1 theres strange characters horizontally flanking the figure\n- In section 2.2 its a bit unclear what g_ii is given that there is no positives. Is s_ii in this case just 1? or something else? Clarifying the exposition here, going a bit slower, and being explicit might make the paper a bit easier to follow. \n- Experiments on real datasets are limited to molecular studies, theres been a considerable ammount of literature in image based contrastive learning and it might make the paper more convincing to show its relation to other methods from this more competitive benchmark\n- Experiments compare InforNCE to cMIM and MIM, but you could imagine adding InfoNCE and MIM together to make a loss that cared about both discrimination and generation, as your additional term serves mainly ads adding a contrastive-like signal to the loss. \n- Data augmentation was applied in the experiments, which makes me question the authors claims of the \"brittleness of augmentation design\"\n- Baselines in molecular property prediction seem old and might not represent the strongest models in the community\n- How much of the batch size sensitivity claim is because cMIM and MIM have reconstruction terms which dont require large batch sizes for clear gradient signal? Its hard to tell whether the improvement comes from combining contrastive and reconstruction losses, or comes from the specific formulation of cMIM"}, "questions": {"value": "- Its unclear whether the lack of negatives here really improves things. For example, if you actually included negatives in your framework would it help or hurt? \n- In alot of these loss functions that dont involve negatives theres usually something that has a negative \"flavor\" like spatial centering and moving average comparison, what do you think the inductive bias of this loss function is more intuitively? COuld you think of this as this as contrasting a positive pair with other samples closeby in some appropriately defined neighborhood?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NFc1D38jjt", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Reviewer_LAuA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Reviewer_LAuA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875090573, "cdate": 1761875090573, "tmdate": 1762918426618, "mdate": 1762918426618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a contrastive extension of the Mutual Information Machine (MIM) with the goal of unifying discriminative and generative self-supervised learning. It introduces a probabilistic contrastive mechanism that removes the need for positive augmentations and reduces sensitivity to batch size. The authors also propose *informative embeddings*, extracted from decoder hidden states, as a general method for richer representations. The experiments are on MNIST-like images and molecular ZINC15 domains that matches MIM on reconstruction but actives higher downstream tasks accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's effort to bring unify generative and contrastive learning is genuinely worthwhile, especially since methods like InfoNCE still depend heavily on careful data augmentations and large batch sizes to work well."}, "weaknesses": {"value": "1- the biggest weakness is that the method has been only evaluated on toy-level data such as MNIST-like datasets and small molecular regression. I would suggest the authors to test their method on larger standard datasets.\n\n2- there's no ablations on hyper-parameters such as $\\tau$ (temperature) and dimensionality size. \n\n3- there's no analysis or comparisons against other augmentation-free contrastive methods (e.g., VICReg and Barlow Twins)."}, "questions": {"value": "1- how does the proposed method perform on CIFAR or ImageNet without augmentations?\n\n2- how does it compare against VICReg, SimSiam, or BYOL?\n\n3- what's the effect of  hyper-parameters such as temperature and dimensionality size?\n\n4- how does your framework compare to other unifying approaches for representation learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DmnNc3njsU", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Reviewer_JWvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Reviewer_JWvT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991286378, "cdate": 1761991286378, "tmdate": 1762918426341, "mdate": 1762918426341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a contrastive mutual information machine (cMIM). The approach does not require positive-pair augmentations and is less sensitive to batch size than InfoNCE. cMIM improves performance on diverse MNIST-like datasets and Molecular Property Prediction."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tests cMIM on two types of data: images (MNIST variants) and molecules (ZINC15).\n- cMIM shows consistent improvements across many batch sizes"}, "weaknesses": {"value": "- Performance is reported mainly as z-scores and ranks. Without showing raw metrics or baseline values, it’s hard to judge how meaningful the reported improvements are.\n- The typical self-supervised approaches are not included, such as BYOL, SimSiam, VICReg, or other generative/reconstruction baselines.\n- The experiments are limited to small and relatively simple datasets, mainly MNIST variants.\n- There is no clear ablation isolating the effect of the informative embeddings. Hence, it is not clear whether the performance improvements are from the new contrastive objective or the use of the decoder-based embeddings."}, "questions": {"value": "- Beyond reconstruction error, could the authors report any quantitative or qualitative generative metrics (e.g., FID for images, validity for molecules) to support the \"generative fidelity\" claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rgSh4cINCv", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Reviewer_M3RK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Reviewer_M3RK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119926006, "cdate": 1762119926006, "tmdate": 1762918425857, "mdate": 1762918425857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "## Authors’ summary to all reviewers\n\nWe thank you for the thoughtful and constructive feedback. We will make the following concrete revisions:\n\n* Clarity & exposition. We will fix typos/formatting (e.g., Fig. 1) and explicitly define s_{ij}=\\cos(z_i,z_j)/\\tau, g_{ij}=e^{s_{ij}}, and g_{ii} in Sec. 2.2, adding a brief walk‑through of the loss and gradients for readability. \n* Theory. We will promote the main algebra from Appendix C to the paper body: -\\log p_{k=1} equals InfoNCE with a fixed positive‑logit offset \\log(B-1) and cMIM inherits MIM’s MI bound (with k\\equiv1). We will add a calibration figure and a short proof sketch. \n* Reporting. Alongside the cross‑dataset z‑scores/ranks, we will include full per‑dataset raw accuracies (images) and the already‑reported absolute RMSE tables (molecules) for every batch size and model, plus FID (images) and validity/uniqueness (molecules) to quantify “generative fidelity.”\n* Datasets & baselines. To strengthen external validity, we will add CIFAR‑10/100 results. We will clarify that our image augmentations were used only as regularization (not to form positives).\n* Ablations. Unfortunately, we will not be able to expand the τ‑sweep and latent‑size ablations and add image results with/without informative embeddings. Current results already show cMIM achieves the best average accuracy/rank across batch sizes (Fig. 3), is least batch‑sensitive (Fig. 4, Table 2), preserves reconstruction (Fig. 5), and that cMIM‑Σ and InfoNCE‑X degrade performance (Fig. 6).\n\nOverall, cMIM (i) requires no positive‑pair augmentations, (ii) is less sensitive to batch size, and (iii) improves downstream discrimination while preserving reconstruction across vision and molecular domains. We believe the above additions address the main concerns and further clarify the paper’s contribution."}}, "id": "KjvAHb50wD", "forum": "I8FkTtgsUu", "replyto": "I8FkTtgsUu", "signatures": ["ICLR.cc/2026/Conference/Submission6044/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6044/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6044/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763584028958, "cdate": 1763584028958, "tmdate": 1763584028958, "mdate": 1763584028958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}