{"id": "WkGnZsnCDR", "number": 25493, "cdate": 1758368627237, "mdate": 1759896718903, "content": {"title": "Hierarchical Graph-coding Diffusion Model with Adaptive Information Bottleneck for Multichannel Speech Enhancement", "abstract": "Diffusion models have achieved strong performance in multichannel speech enhancement, especially in unseen noisy scenarios. However, most existing diffusion method rely on globally consistent guidance applied either to the output or uniformly across denoiser layers, which fails to provide layer-specific adaptation and introduces redundancy, thereby constraining denoising performance.To address these challenges,we propose a novel hierarchical graph-coding diffusion model with adaptive information bottleneck (HG-Diff-IB) for multichannel speech enhancement. Specifically, we introduce a hierarchical alignment method to align graph-coding with the denoiser at different depths, together with a layer-wise graph-coding modulation mechanism that injects graph information into intermediate features, enabling layer-specific guidance of diffusion feature distributions. Furthermore, we introduce an adaptive information bottleneck that dynamically adjusts the feature compression according to the estimated SNR, effectively balancing noise suppression and target feature preservation. Experimental results demonstrate that our proposed method outperforms baselines in various evaluation metrics.", "tldr": "", "keywords": ["hierarchical graph-coding", "diffusion model", "layer modulation", "adaptive information bottleneck", "multichannel speech enhancement"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bdb5e477105dda0016c2dfaed9ac41c94632786d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes HG-Diff-IB, a hierarchical graph-coding diffusion model with an adaptive information bottleneck for multichannel speech enhancement. The method introduces hierarchical alignment and layer-wise graph-coding modulation to provide layer-specific guidance for diffusion features, and employs an adaptive information bottleneck that adjusts feature compression based on estimated SNR. Experiments show that HG-Diff-IB achieves superior performance over baseline methods across multiple evaluation metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Interesting Topic.** The paper focuses on a timely and increasingly popular research topic. In recent years, many researchers have explored integrating generative models (such as diffusion models) with traditional regression-based tasks like speech enhancement, making this work relevant to ongoing trends in the field.\n- **Performance.** The proposed model consistently outperforms many advanced baselines across multiple evaluation metrics."}, "weaknesses": {"value": "- **Unclear Motivation for Graph-Coding.** The motivation for introducing graph-coding is not clearly explained. It remains unclear why a GNN is needed for the SE task. The authors do not provide sufficient justification for this design choice, nor do they include implementation details that would help readers understand how the graph structure is constructed or how it benefits the model. As a result, the role and necessity of the graph-coding module remain ambiguous.\n\n- **Missing Definitions and Clarity.**  Several symbols in the paper are undefined or insufficiently explained. For example, in Equation (6), it is unclear what $m_t$ and $y$ represent, and why a part of $y$ is subtracted. Moreover, the formulations of the forward and reverse diffusion processes are not clearly described. These missing definitions and unclear explanations make it difficult for readers to fully understand the methodology and reproduce the results. I suggest the authors carefully review all mathematical expressions and ensure that every symbol is explicitly defined in the main text.\n\n- **Lack of Efficiency Analysis.** The paper focuses primarily on performance improvements over baselines but lacks any analysis or discussion of efficiency. Important aspects such as model complexity (e.g., parameter count, FLOPs) and inference time are not reported. Without these comparisons, it is difficult to evaluate whether the observed performance gains are achieved through better modeling design or simply by increasing computational cost. An explicit efficiency analysis would make the contribution more convincing and practically relevant."}, "questions": {"value": "- **Eq. 5. (Lines 159-161)** The authors state that $\\beta_{\\text{adapt}}$ allows the model to regulate feature compression based on real-time SNR. However, if the goal is to model the relationship with SNR, why do the authors choose to use $x_t$ rather than $t$ (or an explicit SNR estimate)? Since t already has a monotonic relationship with the SNR in diffusion-based formulations, conditioning on $t$ would seem more straightforward and interpretable. Could the authors clarify this design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtDODG8PZ9", "forum": "WkGnZsnCDR", "replyto": "WkGnZsnCDR", "signatures": ["ICLR.cc/2026/Conference/Submission25493/Reviewer_TwDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25493/Reviewer_TwDz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575070375, "cdate": 1761575070375, "tmdate": 1762943451986, "mdate": 1762943451986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical graph-coding diffusion model with adaptive information bottleneck for multichannel speech enhancement, and shows the effectiveness with experiments on synthesized datasets. The proposed method demonstrates superior performance on two metrics to measure speech quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Reasonable proposal of the new method for speech enhancement based on diffusion model.\n\n- Experiments support the proposed method with relative superiority to the comparing methods."}, "weaknesses": {"value": "- The proposed method requires a thorough (maybe theoretical) justification.\n\n- The experiments are too limited to draw any interesting conclusions. Larger and more datasets are required to validate the proposed method."}, "questions": {"value": "- What is the main contribution of the proposed method? Simple adoption of hierarchy would not be enough to justify the novelty of the proposed method.\n\n- How can you guarantee the superiority with the larger benchmark datasets? Simple synthesis of a new dataset would limit the verification of the experiments. More metrics should be introduced to provide the in-depth analysis of the results for deeper discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5SvwAiTRTH", "forum": "WkGnZsnCDR", "replyto": "WkGnZsnCDR", "signatures": ["ICLR.cc/2026/Conference/Submission25493/Reviewer_eHQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25493/Reviewer_eHQd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618313382, "cdate": 1761618313382, "tmdate": 1762943451815, "mdate": 1762943451815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary: this paper proposes a hierarchical graph-coding diffusion model with adaptive information bottleneck for multi-channel speech enhancement. Contributions are two-fold. First, they propose a hierarchical alignment method to align gradh-coding with the denoiser at different depths. Besides, they introduce an adaptive infomration bottleneck that can adaptively adjust the feature compression at different timesteps."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. They propose a hierarchical graph-coding method to align shollow and deep graph-coding features with the denoiser encoder and decoder.\n2. They propose a layer-wise graph coding modulation to inject graph information into intermediate layers.\n3. They propose an adaptive information bottleneck to regulate feature compression at different timesteps."}, "weaknesses": {"value": "Weakness:\n1. The authors show rather negative attitude on the presentation of the paper. For example, in Fig. 1, no clear illustration is given toward the training/inference pipeline, and no illustration is provided toward the abbreviations listed in the figure, making the readers quite confusing. Besides, no related works, and no preliminary demonstrations toward the problem formulation and existing challenges are provided. I think such a sumit is a pure waste of time and should be desk-rejected. \n2. I have trained all the baselines the authors listed in Table 1, and the reported PESQ scores seem anomalous, especially considering the spatial feature can be utilized. \n3. Table 2, the improvements brought by IB and adaptiveIB seem marginal and inadequate to validate the claim."}, "questions": {"value": "Questions:\n\n1. Please rewrite the whole paper, and check your code to ensure all the results you reported are valid.\n2. Practice your writing skill in research paper and do not throw a meaningless manuscript in ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ddAnY1yMLF", "forum": "WkGnZsnCDR", "replyto": "WkGnZsnCDR", "signatures": ["ICLR.cc/2026/Conference/Submission25493/Reviewer_H7Q7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25493/Reviewer_H7Q7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900764968, "cdate": 1761900764968, "tmdate": 1762943451617, "mdate": 1762943451617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HG-Diff-IB, a multichannel speech enhancement (SE) framework that combines (i) hierarchical alignment between a graph-coding network (STGCN) and a diffusion UNet, (ii) layer-wise feature modulation using AdaIN with scale/bias predicted from the graph features, and (iii) an adaptive information bottleneck (IB) whose trade-off parameter varies with an SNR proxy derived from self-similarity of STFT features. Experiments on synthetic 6-mic arrays show modest gains in PESQ and small changes in STOI over diffusion and graph/diffusion baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is simple yet interesting, combining well-established mechanisms (hierarchical conditioning, AdaIN modulation, and information bottleneck)\n2. The paper includes a comprehensive evaluation with ablation study that assess the contribution of each module."}, "weaknesses": {"value": "1. The contribution appears incremental relative to existing work. Similar ideas have been explored in diffusion models guided by external features across layers.\n2. The paper is poorly written with multiple grammatical errors and typos (e.g. “methed”, “setted”). More importantly, several architectural and training details are missing (UNet and STGCN configuration, STFT parameters, hyperparameters, optimizer, batch size etc).\n3. The dataset used is fully synthetic, which limits the conclusions. Also, the metrics used (PESQ/STOI) are not sufficient for perceptual evaluation - a mean opinion score would be better.\n4. The performance gains are small, and at high SNR the improvements seem to vanish."}, "questions": {"value": "1. How is I(Z;X) computed in Eq. (6)? e.g. MINE, InfoNCE.\n2. Were the baseline models fine-tuned on the same synthetic dataset before comparison? If not, how was fairness ensured?\n3. Can you show results on real-world data to show the generalization abilities of your model?\n4. How does the model’s performance scale with different microphone counts (e.g. 2, 4, or 8 mics)? Also, how sensitive is the model to the \\beta_{adapt} value?\n5. Have you seen which noise types the model performs best/worst? Can you show these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SQnwXGapMS", "forum": "WkGnZsnCDR", "replyto": "WkGnZsnCDR", "signatures": ["ICLR.cc/2026/Conference/Submission25493/Reviewer_PFfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25493/Reviewer_PFfg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986642654, "cdate": 1761986642654, "tmdate": 1762943451358, "mdate": 1762943451358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}