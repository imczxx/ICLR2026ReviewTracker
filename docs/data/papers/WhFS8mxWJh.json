{"id": "WhFS8mxWJh", "number": 17976, "cdate": 1758282605464, "mdate": 1759897141523, "content": {"title": "Robust Adversarial Attacks Against Unknown Disturbance via Inverse Gradient Sample", "abstract": "Adversarial attacks have achieved widespread success in various domains, yet existing methods suffer from significant performance degradation when adversarial examples are subjected to even minor disturbances. In this paper, we propose a novel and robust attack called IGSA (**I**nverse **G**radient **S**ample-based **A**ttack), capable of generating adversarial examples that remain effective under diverse unknown disturbances. IGSA employs an iterative two-step framework: (i) inverse gradient sampling, which searches for the most disruptive direction within the neighborhood of adversarial examples, and (ii) disturbance-guided refinement, which updates adversarial examples via gradient descent along the identified disruptive disturbance. Theoretical analysis reveals that IGSA enhances robustness by increasing the likelihood of adversarial examples within the data distribution. Extensive experiments in both white-box and black-box attack scenarios demonstrate that IGSA significantly outperforms state-of-the-art attacks in terms of robustness against various unknown disturbances. Moreover, IGSA exhibits superior performance when attacking adversarially trained defense models. Code is available at https://github.com/nimingck/IGSA.", "tldr": "This paper proposes IGSA, a robust adversarial attack framework that significantly improves the resilience of adversarial examples against unknown disturbance through inverse gradient-based sampling and iterative refinement.", "keywords": ["Adversarial sample", "Transferable attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9325dd7709ad4d8fce06c05f5bc062d0d7c9b8d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Inverse Gradient Sample-based Attack (IGSA), which can generates adversarial examples that remain effective under diverse unknown disturbances. The authors theoretically analyze how inverse gradient sampling improves sampling coverage and transferability of adversarial examples. Through experiments, the authors demonstrate that IGSA outperforms other adversarial attack baselines under disturbances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Propose a novel gradient-based method to enhance the robustness of adversarial examples\n2. Provide theoretical analysis on the efficiency of the proposed method compared to EOT\n3. Experiments show that the adversarial examples generated by proposed method are robust to unknown disturbances and are transferable across different networks"}, "weaknesses": {"value": "**1. Trivial theoretical solution:** The authors theoretically analyze the efficiency of IGS and EOT. However, IGS utilizes the gradient information, while EOT is a black-box method. Therefore, the theoretical solution that IGS is more efficient than EOT is not surprising. It would be better that you could compare IGS with a white-box baseline.\n\n**2. Adversarial training:** It would be interesting to adopt IGSA in adversarial training, and see whether the trained model is robust to different attacks, including IGSA."}, "questions": {"value": "1. How many sampling points does EOT needs to reach the similar attack success rate of IGSA?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "I have reviewed this paper at NeurIPS 2025. Some results on ViT are completely the same as those on DenseNet. However, the results on DenseNet are removed in this version."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5QwsUCwoRD", "forum": "WhFS8mxWJh", "replyto": "WhFS8mxWJh", "signatures": ["ICLR.cc/2026/Conference/Submission17976/Reviewer_mtxc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17976/Reviewer_mtxc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761301379305, "cdate": 1761301379305, "tmdate": 1762927770143, "mdate": 1762927770143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Inverse Gradient Sample-based Attack (IGSA), a novel adversarial attack framework designed to enhance the robustness of adversarial examples against unknown disturbances while improving their transferability in black-box settings. The method iteratively samples disturbances and optimizes adversarial examples under these disturbances, demonstrating strong performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1、 Experiments reveal that adversarial examples generated by IGSA exhibit a significantly larger robustness boundary compared to existing methods, indicating superior resistance to a variety of disturbances.\n\n2、The paper provides extensive experiments on multiple datasets (e.g., ImageNet, CIFAR-10, CelebA) and model architectures (e.g., VGG, ResNet, ViT), thoroughly validating the effectiveness of IGSA under both additive and non-additive disturbances.\n\n3、Theoretical analysis is provided to support the claims, including efficiency gains over EOT and the relationship between data likelihood and robustness.\n\n4、IGSA not only improves robustness but also enhances the transferability of adversarial examples, making it highly applicable in practical black-box attack scenarios."}, "weaknesses": {"value": "1、The theoretical results, while valuable, are presented in a highly mathematical form without intuitive explanations. For broader accessibility, the authors should provide more intuitive descriptions or visual illustrations of the key theorems (e.g., Theorems 1–4).\n\n2、Algorithm 1 is presented without sufficient explanation in the main body. The authors should describe its key steps, hyperparameters, and how it integrates into the overall IGSA framework."}, "questions": {"value": "The authors summarize the challenges as limited sampling coverage, distribution mismatch, and transferability considerations. While the method and experiments show clear improvements, it would be helpful to clarify why these three challenges are the most critical in the context of robust adversarial attacks, and whether other potential challenges (e.g., computational cost, semantic preservation) were considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CJN70SBqtG", "forum": "WhFS8mxWJh", "replyto": "WhFS8mxWJh", "signatures": ["ICLR.cc/2026/Conference/Submission17976/Reviewer_6pSC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17976/Reviewer_6pSC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761487504239, "cdate": 1761487504239, "tmdate": 1762927768976, "mdate": 1762927768976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Inverse Gradient Sample-based Attack (IGSA), a robust adversarial attack method that maintains effectiveness under various unknown disturbances such as blur, rotation, and compression.\nThe main claim is that existing attack methods are less robust against unknown disturbance distributions; therefore, IGSA aims to design a transferable, imperceptible, and disturbance-robust attack framework. \nIGSA iteratively samples disturbance directions via inverse gradient steps and refines adversarial examples to resist these perturbations.\nThe approach extends the EOT framework with a theoretically motivated sampling rule, claiming improved efficiency and higher data likelihood of adversarial examples.\nExtensive experiments on CIFAR-10, ImageNet, and CelebA show superior robustness and transferability compared to existing attacks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly motivates the problem by highlighting that existing transfer-based attacks easily fail under unseen disturbances, and explicitly defines robustness as a key missing property.\n2. The presentation is very well-structured and easy to follow, with a logical flow from framework to theoretical analysis to experiments.\n3. Theoretical formulations are sound, and the analysis of inverse gradient sampling provides a quantitative comparison to EOT in terms of sampling efficiency.\n4. Experiments are extensive, covering multiple datasets, diverse disturbance types, and both white-box and black-box settings, along with detailed ablations and hyperparameter studies.\n5. Empirical results consistently show strong improvements, with IGSA maintaining high success rates and transferability even under adversarial defenses."}, "weaknesses": {"value": "1. Limited conceptual novelty. The proposed Inverse Gradient Sampling (IGS) is presented as a new sampling mechanism, but in practice it closely resembles a single-step gradient ascent in disturbance space. The method can be seen as a localized refinement of existing gradient-based attacks (e.g., EOT or PGD) rather than a fundamentally new paradigm. While the paper’s formulation is mathematically elegant, the conceptual contribution feels incremental unless the authors can demonstrate that IGS leads to qualitatively different optimization behavior or transfer dynamics. \n2. Restrictive theoretical assumptions. The theoretical analysis assumes convexity and smoothness of the loss surface, conditions that rarely hold for deep neural networks. As a result, the convergence guarantees and efficiency bounds provided may not translate to realistic, highly non-convex settings. Moreover, the paper does not discuss convergence stability or potential divergence behaviors of the inverse gradient iteration—an important aspect since the attack involves repeated optimization in disturbance space. Without such analysis or empirical evidence of stability, the theoretical claims remain limited in scope. \n3. Weak empirical support for the data-likelihood claim. The paper claims that IGS improves the data likelihood of adversarial examples, suggesting that they align more closely with the natural image distribution. However, this claim is supported only by a single energy-based OOD metric, which provides an indirect and limited view of sample realism. More robust probabilistic analyses, such as likelihood ratio estimation, diffusion-based density approximations, or human perceptual validation, would be necessary to substantiate the connection between IGS and likelihood improvement. \n4. Insufficient implementation transparency. The paper provides limited discussion on the comparability of experimental settings across baselines. Key factors such as the number of iterations, query budgets, and perturbation strengths directly affect attack success rates, yet their parity across methods is not explicitly guaranteed. Without clear evidence that all methods were evaluated under identical resource and constraint settings, it becomes difficult to attribute improvements solely to IGSA’s algorithmic design. This omission undermines the reliability and fairness of the reported results. \n5. Incomplete baseline coverage and unclear relation to domain generalization. While IGSA aims to improve robustness under disturbances, this goal conceptually overlaps with domain generalization (DG) problems, where models are trained or evaluated under distribution shifts. However, the paper does not clarify whether disturbances are treated as domain shifts or as stochastic corruptions, leaving the problem setting somewhat ambiguous. Additionally, several recent robustness-oriented attacks and defenses addressing distributional shift are not included in the experimental comparison. The lack of both conceptual positioning and comprehensive baselines weakens the empirical credibility of the paper’s main claims."}, "questions": {"value": "1. Could the authors clearly define what is meant by “disturbance” and specify whether it refers to data corruption, augmentation, or domain shift in the introduction section? Providing this definition early on would enhance clarity and readability.\n2. The paper reports both targeted and non-targeted results, but the discussion primarily focuses on targeted attacks, where IGSA performs better. Could the authors clarify why targeted attacks are emphasized, and how the relatively weaker non-targeted results should be interpreted in terms of transferability?\n3. What is the underlying reason that inverse gradient sampling improves transferability? Is it related to smoother loss landscapes, gradient alignment, or increased data likelihood?\n4. The paper claims imperceptibility as one of the main goals, but no quantitative evaluation or perceptual metric is provided in the main text. Are there plans to include such analysis or measurements?\n5. Could the method be reformulated with KL divergence to extend IGSA beyond classification tasks such as dense prediction? This reformulation could significantly enhance the impact and generality of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q3QgLifwye", "forum": "WhFS8mxWJh", "replyto": "WhFS8mxWJh", "signatures": ["ICLR.cc/2026/Conference/Submission17976/Reviewer_5uoS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17976/Reviewer_5uoS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533025220, "cdate": 1761533025220, "tmdate": 1762927768490, "mdate": 1762927768490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adversarial attack framework called Inverse Gradient Sample-based Attack (IGSA) to generate robust adversarial examples that remain effective under unknown disturbances such as noise, transformations, and compression.\nThe method iteratively samples disturbances using inverse gradient sampling to find the most destructive perturbation direction, and then refines adversarial examples through disturbance-guided gradient updates.\nThe authors provide theoretical analyses showing that IGSA improves sampling efficiency exponentially compared to the conventional Expectation over Transformation (EOT) approach, and that it implicitly aligns adversarial examples with the natural data distribution, enhancing robustness and transferability.\nExtensive experiments on CIFAR-10, ImageNet, and CelebA demonstrate that IGSA significantly outperforms state-of-the-art attacks in both robustness and transferability, even against adversarially trained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem setting of generating robust adversarial examples is interesting and meaningful, especially for understanding the vulnerability of models under real-world perturbations.\n- The paper includes theoretical analyses that provide a credible explanation of why the proposed method improves robustness and efficiency.\n- The experimental evaluation is comprehensive, covering multiple datasets, model architectures, and defense settings, which makes the results reliable."}, "weaknesses": {"value": "- It is unclear what robust adversarial examples are useful for. The paper emphasizes generating stronger attacks, but it does not clearly discuss potential downstream applications, such as improving model robustness or evaluating defense strategies.\n- The proposed IGSA method appears to be computationally expensive (e.g., 0.423s (ISGA) vs 0.186s (VDA) in Table 1). Adversarial examples are mainly used to improve model robustness through adversarial training, where generating a large number of examples quickly is important. In this sense, faster generation could be more desirable, even at some cost to robustness."}, "questions": {"value": "1. What can robust adversarial examples be used for? Beyond demonstrating attack robustness, how could they contribute to improving or evaluating model robustness in practice?\n2. Do the authors have any ideas for accelerating IGSA? For instance, could there be a trade-off analysis between robustness and generation speed, or a possible approximation that maintains robustness with reduced computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YJXcwKGcq1", "forum": "WhFS8mxWJh", "replyto": "WhFS8mxWJh", "signatures": ["ICLR.cc/2026/Conference/Submission17976/Reviewer_KBuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17976/Reviewer_KBuh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17976/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905904115, "cdate": 1761905904115, "tmdate": 1762927767879, "mdate": 1762927767879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}