{"id": "Eo2OSOQL1P", "number": 13438, "cdate": 1758217857589, "mdate": 1759897437510, "content": {"title": "MMMG: A Comprehensive and Reliable Benchmark for Multitask Multimodal Generation", "abstract": "Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 55 tasks (including 31 newly developed ones), each with a carefully designed evaluation pipeline, and 1248 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.4%. Benchmarking results on 29 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 70.7% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable improvement space in audio generation, highlighting an important direction for future research.", "tldr": "", "keywords": ["Evaluation", "Benchmark", "Multi-modal Generation Benchmark", "Multimodal Large Language Models", "Multimodal Generation", "Image Generation", "Audio Generation", "Interleaved Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f5e8dc27fd0e3ac6f17f06a13ae068ace7f301c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMMG, a comprehensive benchmark for evaluating multitask multimodal generation, designed to address the misalignment between automated metrics and human evaluation, as well as the limited coverage of interleaved modalities in existing benchmarks. MMMG spans 4 modality combinations (image, audio, interleaved text-image, interleaved text-audio) and includes 55 tasks (31 newly developed) with 1248 structured instructions, targeting key capabilities like reasoning, controllability, and cross-modal coherence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark exhibits high quality through rigorous data curation (only 10% of generated instructions pass strict filtering), comprehensive evaluation validation (high agreement among multiple annotators, with an average inter-annotator agreement of 97.1%), and strong reproducibility (detailed documentation of data sources, model checkpoints, and evaluation protocols).\n\n2. The paper is well-structured, with intuitive presentations of task definitions (e.g., detailed metadata in Table 1), experimental design (e.g., evaluation pipelines in Section 3.2), and failure analyses (e.g., model weaknesses in Figure 3). This clarity makes it easy for readers to understand the scope of tasks and interpret benchmark results.\n\n3. MMMG establishes a reliable evaluation baseline for multimodal generation, guides future research by identifying critical gaps (e.g., limited performance in audio generation), and enables fine-grained analysis of model weaknesses (e.g., poor multimodal reasoning in top models). Collectively, these contributions advance the field of multimodal generation."}, "weaknesses": {"value": "1. Although MMMG includes 55 tasks (31 newly developed), Appendix B.1 (Data Source) reveals that most task data relies on simple sampling or instruction modification from existing datasets (e.g., HotpotQA for object reasoning, EmuEdit for image editing, LibriSpeech for speech replication). This heavy dependence on pre-existing data undermines the novelty of the \"newly developed\" tasks, as they do not involve fully independent or original data curation.\n\n2. The paper provides insufficient details about the data sources and construction processes for the 31 newly developed tasks. Unlike the detailed descriptions of data derived from existing datasets (e.g., COCO for object images, CLEVR for color modification), there is no clear documentation of how the newly designed tasks were sourced, how their instructions were synthesized, or how reference outputs (if any) were created—hindering reproducibility and assessment of task validity.\n\n3. The analysis of multimodal reasoning failures is inadequate. Benchmark results show top models perform poorly on such tasks (e.g., 10.1% accuracy for math, 31.8% for 3D code transformations), but the paper fails to: (1) Diagnose root causes—whether failures originate from model architecture (unified autoregressive vs. agent-based), limited training data for math/code multimodal scenarios, or overly strict VLM evaluation prompts; (2) Propose mitigations—without identifying the source of weaknesses, MMMG offers limited guidance for researchers to prioritize improvements.\n\n4. The claim that top models \"fall short on multimodal reasoning\" lacks sufficient rigor. For math interleaved reasoning tasks, the paper adapts data from MM-IQ (Appendix B.2) by converting original multiple-choice questions into free-form generation tasks. This modification not only increases task difficulty but also introduces ambiguity: it is impossible to disentangle whether poor model performance stems from genuine weaknesses in multimodal reasoning or limitations in image generation (e.g., failing to render clear visual elements for math problems)."}, "questions": {"value": "1. Regarding the rigor of the \"models fall short on multimodal reasoning\" claim: Given that math interleaved reasoning tasks were adapted from MM-IQ by converting multiple-choice questions to free-form (Appendix B.2), which may confound reasoning weaknesses with image generation limitations, do you have additional data or controlled experiments (e.g., testing models on the original multiple-choice format vs. free-form, or evaluating image generation quality independently for math tasks) to disentangle whether poor performance stems from multimodal reasoning deficits or inadequate visual rendering?\n\n2. For the 31 newly developed tasks: Since Appendix B.1 details data sourcing for tasks derived from existing datasets (e.g., HotpotQA, EmuEdit) but lacks clarity on the 31 new tasks, could you supplement details on their specific data sources, instruction synthesis processes (e.g., how prompts were designed beyond GPT-4O generation), and creation of reference outputs (if applicable)? This would help assess the novelty and validity of these \"new\" tasks compared to modified existing ones.\n\n3. On the insufficient analysis of multimodal reasoning failures: Given top models’ low accuracy on math (10.1%) and 3D code transformation (31.8%) tasks, have you conducted follow-up analyses to identify root causes—such as comparing performance across model architectures (unified autoregressive vs. agent-based), testing with augmented math/code multimodal training data, or adjusting VLM evaluation prompts to rule out strictness bias? Additionally, do you have preliminary insights or suggestions for mitigating these failures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TKtP7CW2lm", "forum": "Eo2OSOQL1P", "replyto": "Eo2OSOQL1P", "signatures": ["ICLR.cc/2026/Conference/Submission13438/Reviewer_R7P8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13438/Reviewer_R7P8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834891465, "cdate": 1761834891465, "tmdate": 1762924063316, "mdate": 1762924063316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MMMG, a benchmark designed to evaluate multitask multimodal generation across text, image, audio, and mixed modalities. It contains 1,248 instructions covering 55 tasks and uses automatic human-aligned scoring to assess model outputs. The benchmark is validated against human preference data and achieves high agreement and consistency across modalities. It provides a unified framework for measuring the performance of multimodal generative models and comparing their strengths and weaknesses across different modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposes a unified benchmark that evaluates multimodal generation across text, image, audio, and interleaved modalities within a single framework.\n2. Uses automatic human-aligned scoring verified against human preference data, reducing the reliance on manual evaluation.\n3. Provides detailed cross-modal comparison results that reveal modality-specific weaknesses, especially in audio and multi-image generation."}, "weaknesses": {"value": "1. Some task have limited coverage, for example, the code tasks involve only a single programming language, which restricts representativeness.\n2. The analysis of modality-specific challenges is not sufficiently in-depth, lacking systematic error-type distributions, cross-model comparisons, and causal interpretation that could provide more actionable insights."}, "questions": {"value": "1. Could you consider expanding the diversity of certain tasks, such as including multiple programming languages in the code-related tasks, to avoid narrow coverage?\n2. Could you provide a more in-depth analysis of modality-specific challenges, including systematic error-type statistics, cross-model comparisons, and possible causes behind these differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3ayQB9nSdN", "forum": "Eo2OSOQL1P", "replyto": "Eo2OSOQL1P", "signatures": ["ICLR.cc/2026/Conference/Submission13438/Reviewer_cF2M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13438/Reviewer_cF2M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987321073, "cdate": 1761987321073, "tmdate": 1762924062948, "mdate": 1762924062948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MMMG, a large-scale benchmark for multitask multimodal generation, including text, image, and audio modalities. Specifically, it involves 55 tasks, with 31 newly designed, and the evaluation pipeline is fully automatic. In this paper, 29 generative models are evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the paper extand the scope of the previous benchmarks, with more comprehensive tasks and generative models\n- the evaulation process is fully automatic, without human evaulation"}, "weaknesses": {"value": "- reproducibility: while the paper claims that “all tasks and evaluation scripts will be released,” no code, data, or even minimal examples are available to reviewers\n- reliability: the reported high human–judge agreement lacks sufficient methodological details: the scale of human annotations, sampling strategy, agreement metric, and per-task breakdown are all missing\n- task design: although 31 new tasks are claimed, many appear to be prompt variants or loosely defined categories. the authors provide limited evidence that these tasks **systematically** capture **distinct** generation abilities or correlate with real-world applications"}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQbAegu488", "forum": "Eo2OSOQL1P", "replyto": "Eo2OSOQL1P", "signatures": ["ICLR.cc/2026/Conference/Submission13438/Reviewer_4AuQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13438/Reviewer_4AuQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996686224, "cdate": 1761996686224, "tmdate": 1762924062197, "mdate": 1762924062197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MMMG, a benchmark framework aiming to evaluate multitask multimodal generation models across diverse modality combinations, including image, audio, text-image, and audio-text tasks. MMMG features 55 tasks (with 31 newly developed), 1248 systematically curated instructions, and task-specific automated evaluation pipelines calibrated to human judgment. The work validates its evaluation methodology with extensive human studies, demonstrates high human alignment (average 94.4%), and provides detailed benchmarking results over 29 multimodal models, highlighting current strengths and limitations, particularly in audio and interleaved modalities"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MMMG spans a much broader set of modalities and tasks than previous benchmarks, which contrasts task coverage and evaluation methods with prior works. This breadth not only increases utility for the community but also allows for detailed capability analysis of models across modalities.\n\n2.  The evaluation section describes the tailored combination of automatic and programmatic metrics (VLM, CLAPScore, SSIM, DreamSim, etc.), extensively calibrated to maximize human agreement per modality. Figure 1 provides transparent examples showing task definitions, pseudo-code, and sample outputs, illustrating both the richness and the reliability of the evaluation protocols.\n\n3.  Benchmarking is conducted over 29 diverse, open, and proprietary models. The results, broken down by subtask as seen in Figure 2 and surrounding text, illuminate where leading models excel and fail, and highlight meaningful gaps in current generation capabilities."}, "weaknesses": {"value": "1.  While Table 2 provides a broad comparative overview, the paper omits direct citation and discussion of several recent, highly related benchmarks (see \"Potentially Missing Related Work\" below), such as MMIG-Bench[1]  and GEM[2].\n\n2. While the Spearman correlation with Chatbot Arena is reported, the issue of overfitting to benchmark-specific instruction distributions versus true generalization remains open. The impact of this gap is only partially discussed and not probed empirically, leaving open the possibility that MMO's human alignment may not extend to long-tail, natural instructions.\n\n3. While the paper boasts coverage of subjective reasoning (e.g., commonsense, multi-hop, creative attribute manipulation), it does not always specify rigorous, repeatable scoring formulas for these tasks, especially where \"human alignment\" cannot be boiled down to a single automatic test. \n\n4. Although the paper highlights the use of programmatic and model-based evaluation (CLAPScore, Whisper, WavLM, DreamSim), there is limited concrete discussion or evidence regarding the limitations of these metrics in more complex settings (e.g., music genre, instrument separation, conversation consistency for speech-audio tasks). For example, the sensitivity of CLAPScore to reference selection (Section 4.1) is acknowledged, but quantitative ablation results or detailed error analysis are missing. This leaves ambiguity about how robust the metrics truly are, especially for subjective or creative generation tasks that resist narrow programmatic evaluation.\n\n\n[1] Hua, H., Zeng, Z., Song, Y. (2025): MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models.\n\n[2] Su, Y., et al. (2024): GEM: A General Evaluation Benchmark for Multimodal Tasks."}, "questions": {"value": "As shown in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hhwpjfcJdH", "forum": "Eo2OSOQL1P", "replyto": "Eo2OSOQL1P", "signatures": ["ICLR.cc/2026/Conference/Submission13438/Reviewer_RE41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13438/Reviewer_RE41"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164888759, "cdate": 1762164888759, "tmdate": 1762924061698, "mdate": 1762924061698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}