{"id": "EQsJwLbGo5", "number": 7390, "cdate": 1758019422140, "mdate": 1759897855704, "content": {"title": "PROMPTGNN-SIM: DEEP FUSION AND ALIGNMENT OF GNN AND LLMS FOR TEXT-ATTRIBUTED GRAPH LEARNING", "abstract": "Text-Attributed Graphs (TAGs), which integrate rich textual semantics with graph structural information, play a critical role in graph learning tasks. However, current fusion approaches suffer from a fundamental limitation: they treat textual and structural modalities as separate inputs in a shallow, unidirectional pipeline. This one-way information flow prevents a deep, interactive exchange between modalities, leading to suboptimal performance, particularly in challenging scenarios with sparse connectivity and when generalizing across different graphs. To overcome these limitations, we introduce PromptGNN-sim, a novel bi-directional structure-semantic fusion framework that enables deep, symbiotic collaboration between GNNs and LLMs. At its core, PromptGNN-sim leverages a Graph Attention Network (GAT) to perform semantically-aware neighborhood selection, combining structural attention with textual similarity. This GNN-derived structural context is then used to dynamically generate rich, structure-aware prompts for an LLM, which explicitly include the target node’s textual summary, predicted label, and representative keywords from semantically similar neighbors. Unlike traditional methods, our framework incorporates bi-directional cross-modal contrastive learning and cross-attention mechanisms during training to jointly optimize both GNN and LLM components for enhanced performance and robustness. We conduct comprehensive experiments on six public datasets, including Cora, Pubmed, and WikiCS, evaluating both task performance and robustness under cross-task transfer, cross-dataset generalisation, and sparse perturbations. Results show that PromptGNN-sim significantly outperforms classical GNNs, LLMs, and recent state-of-the-art GNN–LLM fusion methods in terms of accuracy, generalisation, and robustness. This work not only introduces an effective framework for deep GNN–LLM collaboration but also lays a solid foundation for future research on truly interactive multi-modal graph learning.", "tldr": "We propose a novel framework that overcomes the limitations of shallow GNN-LLM fusion by dynamically guiding LLMs with structural information from GNNs, significantly improving performance on text-attributed graphs.", "keywords": ["Text-Attributed Graphs", "Graph Neural Networks", "Large Language Models", "Multi-modal Learning", "GNN-LLM"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c94f320c31ea75d59c7f85642b6c69e43f5e138.pdf", "supplementary_material": "/attachment/07b63d97ca2c7341ab8c01aeeb0221a6b7e7c51a.zip"}, "replies": [{"content": {"summary": {"value": "The paper focuses on Text-Attributed Graphs (TAGs), where each graph node has both textual content and structural connections (e.g., papers connected by citations, users by social links). Existing models usually fuse text and structure shallowly and unidirectionally (text → structure), which limits deep interaction between modalities. PromptGNN-sim introduces a bi-directional fusion framework that allows GNNs (for structure) and LLMs (for text) to interact deeply and symbiotically. It dynamically generates structure-aware prompts for the LLM and integrates text-based semantic cues into GNN neighborhood aggregation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a structure-aware, adaptive prompting framework that integrates node texts with filtered neighborhood information based on both structural and semantic similarity.\n\n2. Designs dual attention allowing GNN and LLM to interact mutually, enhancing representation by capturing complementary structural and textual cues.\n\n3. Introduces a multi-view contrastive objective aligning raw text with generated prompts, promoting consistent semantic representations.\n\n4. Experiments on six public datasets (e.g., Cora, PubMed, WikiCS) show superior accuracy, generalization, and robustness under sparse or perturbed graph conditions."}, "weaknesses": {"value": "1. Lack of comparison with strong LLM-based baselines\n\nAlthough PromptGNN-sim demonstrates improvements over traditional GNNs and shallow fusion methods, it does not compare against recent LLM-augmented GNN frameworks such as LLM-as-GNN (PromptGFM), InstructGLM, or other instruction-tuned multimodal graph models. These baselines are crucial for validating the claimed “deep, bi-directional integration between GNNs and LLMs”, since they also use LLM prompting or reasoning-based fusion. Without such comparisons, it remains unclear whether the observed gains stem from the proposed cross-modal alignment or simply from the strong backbone (e.g., LLaMA-3B).\n\n2. Incomplete ablation and missing Stage-1 verification\n\nThe ablation study (Table 5) only removes modules such as warmup, cross attention, contrastive learning, and tf-idf weighting.\nHowever, it does not isolate or analyze the Stage-1 process—the part that integrates node texts with filtered neighborhood information based on both structural and semantic similarity. This stage is central to the paper’s claimed “structure-aware dynamic prompting”, yet there is no quantitative evidence showing its individual contribution or how structural and semantic filtering interact.\n\n3. Limited cross-domain transfer and generalization analysis\n\nWhile the authors highlight robustness under perturbation and sparse connectivity, domain transferability (e.g., cross-dataset or cross-domain adaptation) remains untested. After the GNN–LLM alignment, the model still seems domain-anchored to the training graph distribution, showing no explicit mechanism for transfer to unseen graph domains or unseen text distributions.\nThis questions whether the proposed “alignment” truly captures domain-invariant semantics."}, "questions": {"value": "as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kzv1t3UTvB", "forum": "EQsJwLbGo5", "replyto": "EQsJwLbGo5", "signatures": ["ICLR.cc/2026/Conference/Submission7390/Reviewer_wrZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7390/Reviewer_wrZ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761077440077, "cdate": 1761077440077, "tmdate": 1762919513653, "mdate": 1762919513653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify that current models fail to create an interactive exchange between graph structure and text semantics, leading to poor performance on sparse or new graphs. PromptGNN-sim addresses this through a bi-directional fusion framework with three main components. It uses a Graph Attention Network (GAT) to find the most relevant neighbors by combining structural attention with textual similarity. It uses the GNN-derived context to dynamically create structure-aware prompts for an LLM. These prompts adapt based on a node's text length and its degree (number of connections). The framework uses cross-attention mechanisms and a cross-modal contrastive learning objective to jointly optimize and align the GNN and LLM components. Experiments on six datasets (e.g., Cora, Pubmed, WikiCS) show that PromptGNN-sim significantly outperforms existing SOTA methods in node classification, link prediction, cross-task transfer, and robustness."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The bi-directional fusion and joint optimization via cross-attention and contrastive learning are a significant step beyond simple GNN-LLM pipelines.\n\n2. The experimental setup is thorough. The authors validate the model's robustness and generalization by testing against perturbations (edge/node/text dropping) and in transfer-learning scenarios (cross-task and cross-domain).\n\n3. The model achieves state-of-the-art results across all six datasets for both node classification (Table 1) and link prediction (Table 2)."}, "weaknesses": {"value": "1. Despite its strong results, the framework's design introduces several significant challenges and potential limitations. The model's success is heavily dependent on its \"dynamic prompt construction\". This is a \"hard-coded\" heuristic. The authors had to design specific, complex prompt templates for different datasets (see Table 8). This approach is brittle and raises overfitting concerns: The model may be \"overfitting\" to the specific keywords and prompt structures (e.g., \"Key References,\" \"Common Research Themes\")  that the authors engineered. It cannot be deployed on a new dataset out-of-the-box. A human expert would need to manually design, test, and validate new prompt templates, which is a significant practical barrier. The performance difference in Table 9 for different prompts validates this sensitivity. \n\n2. The method relies on feeding neighborhood information into an LLM prompt. This creates a fundamental bottleneck. LLMs have fixed context windows. While a GNN can aggregate information from thousands of neighbors, this model can only \"select the top-k neighbors\" to fit into the prompt. In graphs with large, dense \"hub\" nodes (common in social or e-commerce networks), this \"top-k\" approach will discard a massive amount of structural information, leading to a poor understanding of the node's true context. \n \n3. The evaluation, while using six datasets, is confined to a very specific type of graph: academic and e-commerce networks. These graphs are ideal for this method because their nodes have rich, long-form, and well-structured text (e.g., abstracts, reviews). The method would likely fail in many other common scenarios:  No Text: Biological (e.g., protein-protein interaction) or molecular graphs, where nodes have numerical/categorical features, not text. The model's premise starts with text attributes. The authors used high-end NVIDIA A100 80GB GPUs. This, combined with a large number of sensitive hyperparameters (e.g., $\\lambda$ for fusion , $k$ for neighbors , temperature $\\tau$ ), makes the model extremely expensive to train and difficult to tune."}, "questions": {"value": "1. The Abstract claims the dynamic prompt includes the \"predicted label\". This is logically inconsistent, as the label is what the model is trying to predict. The prompt templates in Appendix F (Table 8) correctly show the label as part of the question being asked, not as an input feature.\n\n2. In Section 4.1, the authors discuss a new, higher accuracy score on Cora. They then state, \"we report the earlier result in Table 2\". This is a typo. Table 2 is for link prediction; the correct reference for node classification accuracy is Table 1.\n\n3. There is a numerical inconsistency between the main results and the ablation studies for the Citeseer dataset.\n\nIn Table 11 (Macro-F1 results), PromptGNN-sim achieves 77.92%.\nIn Table 16 (Ablation Macro-F1), the full \"PromptGNN-sim(Llama3B)\" model is listed with a score of 76.49%. These two numbers should be identical, as they represent the same model on the same task\n\n4. The paper emphasizes \"dynamic prompt construction\" as a core component. How exactly does the framework adapt its prompts differently for a node with a high degree (i.e., \"highly cited\") versus a node with a low degree?\n\n5. How does the adaptive fusion mechanism in Section 3.1 decide whether to prioritize structural attention (from the GAT) or semantic similarity when selecting neighbors for a node?\n\n6. Section 3.3 describes a contrastive learning objective. What are the two specific \"views\" of a node that the model tries to align, and how are they generated?\n\n7. What is the functional difference between the \"Text-guided Graph Attention\" module and the \"Graph-guided Text Attention\" module in the cross-modal attention fusion step?\n\n8. In the node classification task (Table 1), PromptGNN-sim shows significant gains on Cora, CiteSeer, and WikiCS. Why might the performance gain be less pronounced on the Pubmed dataset compared to the other SOTA methods\n\n9. According to the perturbation experiments (Figures 2 & 3), does the model appear to be more resilient to structural (Edge/Node Drop) or semantic (Text Drop) perturbations?\n\n10.Table 3 shows the results for \"cross-task\" transfer from node classification (NC) to link prediction (LP)9999. How significant is the performance drop when transferring tasks (e.g., NC $\\rightarrow$ LP) compared to training and testing on the same task (e.g., NC $\\rightarrow$ NC), and what does this imply about the learned representations?\n\n11. Based on the ablation study in Table 5, which component's removal causes the most significant drop in performance: \"cross attention\" or \"contrastive learning\"10101010?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xjD1IuDJeM", "forum": "EQsJwLbGo5", "replyto": "EQsJwLbGo5", "signatures": ["ICLR.cc/2026/Conference/Submission7390/Reviewer_2WbF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7390/Reviewer_2WbF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531056280, "cdate": 1761531056280, "tmdate": 1762919513122, "mdate": 1762919513122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PromptGNN-sim, a novel bi-directional fusion framework that integrates GNN and LLMs for text-attributed graphs. It employs a GAT encoder to fuses structural and semantic information through cross-attention, facilitating mutual adaptation between the graph and text modalities. It further adopts contrastive learning to jointly optimize both components. \n\nExperimental results on six public datasets (Cora, CiteSeer, PubMed, WikiCS, History, Photo) show that it can outperform both traditional GNNs and previous GNN–LLM fusion baselines in terms of accuracy, generalization, and robustness, laying a solid foundation for interactive multi-modal graph learning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the intersection between prompt learning and graph neural networks, a direction that has recently drawn increasing attention in both the GNN and LLM communities.\n2. The overall framework is logically coherent and easy to follow, and the authors also provided specific case study examples.\n3. The paper provided theoretical analysis, adding a degree of rigor beyond empirical study.\n4. The hyperparameter setting is stated clear, which provide convince to reproduce the results."}, "weaknesses": {"value": "1. Missing related works. The reported accuracy gains appear overstated. The paper does not compare with strong and directly relevant baselines such as GraphPrompter or PromptGFM, which are likely competitive or superior under similar settings.\n2. Incomplete ablation study. Since GAT is a core module, a control experiment for GAT should be conducted when analyzing robustness.\n3. Limited backbone diversity. PromptGNN-sim is only demonstrated with LLaMA-3B as the language model backbone. Additional models like Flan-T5 and LLaMA-7B should be tested.\n4. Limited data scale. The datasets used in this paper are quite small. Although the future work section acknowledges that scalability issues will be addressed later, I believe this problem needs to be resolved now."}, "questions": {"value": "1. How does PromptGNN-sim fundamentally different from GraphPrompter and PromptGFM? Please also include the experiment results of GraphPrompter [1] and PromptGFM [2] in Table 1 and Table 2. I believe these are strong baselines.\n2. Please also include additional backbones (e.g., Flan-T5/LLaMA-7B). The conclusion of increasing accuracy is constrainted by the backbone LLaMA-3B model or is it a generalized finding?\n3. Please include the GAT degradation analysis in Section 4.4. If this addition is not needed, please justify why only GCN is used for robustness evaluation.\n4. Since the proposed fusion module relies on attention mechanisms between prompt and graph representations, how does the method scale to large graphs like ogbn-arxiv and even ogbn-products? Could authors provide some discussions? I believe such discussion should be included in the main body.\n\n[1] Liu et al. \"Can we soft prompt llms for graph learning tasks?.\" WWW 2024.\n\n[2] Zhu et al. \"LLM as GNN: Graph Vocabulary Learning for Graph Foundation Model.\" (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iNR06wHPYH", "forum": "EQsJwLbGo5", "replyto": "EQsJwLbGo5", "signatures": ["ICLR.cc/2026/Conference/Submission7390/Reviewer_Tj7H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7390/Reviewer_Tj7H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758768707, "cdate": 1761758768707, "tmdate": 1762919512716, "mdate": 1762919512716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PromptGNN-SIM, a novel framework for deep fusion and alignment between Graph Neural Networks (GNNs) and Large Language Models (LLMs) in the context of Text-Attributed Graphs (TAGs). The core idea is to enable bidirectional collaboration between the structural and semantic modalities that existing shallow fusion approaches fail to capture. The model operates through three main components: (1) a dynamic prompting mechanism that adaptively constructs structure-aware textual prompts by integrating both semantic similarity and graph attention; (2) a cross-modal attention fusion module that allows mutual information exchange between GNN and LLM representations; and (3) a contrastive learning objective that aligns original and LLM-generated text embeddings to enhance robustness and semantic consistency. Through extensive experiments on six benchmark datasets (Cora, PubMed, Citeseer, WikiCS, History, and Photo), PromptGNN-SIM consistently outperforms both classical GNNs and recent hybrid GNN–LLM baselines in node classification and link prediction tasks. The paper also provides thorough analyses on transferability, ablation, and perturbation robustness, supported by theoretical justification for causal robustness under confounder independence assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s strengths are threefold: 1) it presents a conceptually clear and technically consistent framework that genuinely unifies GNNs and LLMs through bidirectional attention rather than the one-way fusion seen in prior work, making the integration more expressive and interactive; 2) the dynamic prompting mechanism is particularly innovative, using adaptive neighbor selection and textual conditioning to tailor each prompt, which improves both interpretability and adaptability across diverse graph structures; and 3) the experimentation is comprehensive and credible, spanning multiple datasets, ablation and robustness tests, and transfer scenarios, all demonstrating consistent superiority over strong baselines while maintaining interpretability and generalization across domains."}, "weaknesses": {"value": "Despite its strong design, the paper has notable weaknesses: 1) the novelty is incremental, as most components (e.g., contrastive alignment, cross-modal attention) are extensions of established multimodal fusion techniques adapted to TAGs rather than wholly new innovations; 2) the scalability and efficiency aspects are underexplored—given the reliance on large LLMs for dynamic prompt construction, the framework could become computationally expensive and impractical for large-scale graphs; and 3) the analysis of internal mechanisms is somewhat superficial—the ablation studies confirm usefulness but do not explain why certain modules contribute more, and the theoretical causal section feels more aspirational than validated, lacking empirical grounding or measurable causal tests."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yGfMpSmnnG", "forum": "EQsJwLbGo5", "replyto": "EQsJwLbGo5", "signatures": ["ICLR.cc/2026/Conference/Submission7390/Reviewer_8Xne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7390/Reviewer_8Xne"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919965872, "cdate": 1761919965872, "tmdate": 1762919512344, "mdate": 1762919512344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}