{"id": "MaHNO7vmsj", "number": 5603, "cdate": 1757922260888, "mdate": 1759897965492, "content": {"title": "Concept-Guided Dictionary Learning for Interpretable Concept Extraction and Attribution in Large Vision–Language Models", "abstract": "Autoregressive Large Vision-Language Models (LVLMs) generate text sequentially, conditioning each token on evolving multimodal states. This makes it difficult to assess whether predictions are grounded in \\textbf{visual concepts} or instead reflect hallucination or bias. Existing concept-discovery approaches such as \\textbf{TCAV}, \\textbf{CRAFT}, and \\textbf{CLIP-Dissect} are designed for encoder-only or contrastive models. At the same time, recent LVLM methods (CoX-LMM) depend on labeled concepts and simplified settings, limiting scalability.  \n\nWe propose \\textbf{Concept-Guided Dictionary Learning (CGDL)}, an sem-supervised and scalable framework for multimodal concept discovery in autoregressive LVLMs. CGDL first probes the model to surface textual concepts from a dataset. For each concept, it constructs positive and negative patch sets using concept-grounded crops and randomized backgrounds. A contrastive dictionary-learning stage then disentangles concept-aligned activations from residual noise, yielding sparse, monosemantic vectors that reveal \\textbf{semantically aligned visual–textual interactions} and enable faithful attribution of predictions to visual evidence.  \n\nOn \\textbf{ImageNet-1k, MSCOCO}, CGDL outperforms recent interpretability methods with up to \\textbf{4\\% higher sparsity}, \\textbf{11\\% greater stability},  \\textbf{17\\% lower overlap}, and strong attribution faithfulness, while scaling efficiently to large concept vocabularies. These results advance concept-based interpretability for LVLMs and provide a practical step toward transparent multimodal reasoning.", "tldr": "We propose a method to extract and quantify monosemantic concept in large vision-language models (LVLMs), enabling better interpretability of their internal representations.", "keywords": ["Vision–Language Models", "Large Multimodal Models", "Concept-Guided Feature Extraction", "Feature Attribution", "Interpretability", "Explainable AI (XAI)", "Concept Attribution", "Representation Learning", "Cross-Modal Understanding", "Model Transparency"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6706215e77dac64fedee8e8bd2609f3c6688ae7c.pdf", "supplementary_material": "/attachment/2b6c3de1a8e26bc89e2a8050c5656235fcfa8dfb.pdf"}, "replies": [{"content": {"summary": {"value": "This work proposes CGDL, a novel, weakly supervised method for discovering and attributing visual–textual concepts in autoregressive LVLMs. Unlike prior concept­-extraction techniques (e.g. TCAV, CRAFT, CLIP-Dissect), which assume static encoders or rely on labeled concepts in simplified settings, CGDL can scale to multi-object, open‐vocabulary scenarios. The key innovation is to prompt the LVLM with a binary “concept vs. no-concept” question (e.g. “Does this image contain concept cₖ?”), collect activation residuals on positive and negative patches, then solve a contrastive dictionary learning problem per concept. By decomposing each patch’s activation into a “concept” basis plus a residual basis, CGDL enforces monosemantic, sparse, and non-overlapping vectors that align visual regions with textual concepts. Experimentally on ImageNet-1k and MSCOCO, CGDL shows improvements over prior baselines in sparsity, stability, low overlap, and attribution faithfulness (e.g. +9 % in CLIPScore, +4 % BERTScore)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. CGDL introduces a weakly supervised, scalable approach to concept extraction—a setting where most prior methods (e.g., TCAV, CRAFT, CLIP-Dissect, CoX-LMM) fail to generalize. By formulating concept discovery as a contrastive one-vs-all dictionary learning problem, CGDL avoids heavy supervision and scales effectively to thousands of visual–textual concepts.\n2. The proposed two-basis factorization (concept vs. residual) and contrastive residual extraction lead to sparse, disentangled, and monosemantic concept vectors. This improves interpretability by ensuring that each discovered component corresponds to a single, semantically coherent concept rather than a polysemantic mixture.\n3. Empirical results on ImageNet-1k and MSCOCO demonstrate stronger attribution faithfulness and higher quantitative interpretability metrics.\n4. CGDL treats LVLMs as black-box systems, requiring only access to intermediate activations, and thus applies broadly to different architectures. Its per-concept two-basis decomposition reduces computational cost and avoids the atom collapse problem common in large-vocabulary dictionary learning, ensuring scalability without sacrificing quality.\n5. Through contrastive concept prompts and SAM-based localization, CGDL effectively links visual patches with linguistic concepts, offering a coherent view of how LVLMs ground textual outputs in visual evidence. This multimodal grounding represents a key step toward transparent multimodal reasoning."}, "weaknesses": {"value": "1. CGDL relies on SAM to construct positive and negative patch sets. While this provides weak supervision at scale, the quality of discovered concepts is sensitive to segmentation accuracy and crop relevance. In cluttered or fine-grained scenes, imperfect segmentation can lead to noisy or overlapping concept bags, weakening monosemanticity.\n2. The method assumes that multimodal activations can be linearly decomposed into a “concept” and a “residual” subspace. This simplification may not capture the nonlinear dependencies or compositional reasoning inherent in LVLM representations, potentially limiting the interpretability of more abstract or relational concepts.\n3. Although CGDL addresses autoregressive LVLMs, it focuses on single-layer residual activations and per-concept snapshots. It does not explicitly model how concepts evolve across generation steps or interact in temporal or contextual chains, which are key to understanding how LVLMs reason during long-form captioning or dialogue.\n4. The experiments primarily report quantitative metrics (e.g., sparsity, overlap, CLIPScore, BERTScore) on ImageNet-1k and MSCOCO, which may not fully capture the qualitative interpretability or human alignment of discovered concepts. More extensive human studies or causal verification could strengthen claims about faithfulness and semantic clarity.\n5. While the two-basis formulation improves efficiency, it may oversimplify concept structure when scaling to very large or nuanced concept vocabularies. Complex concepts that require multi-dimensional representations (e.g., “a person holding a red umbrella”) might not be well captured by a single basis vector per concept."}, "questions": {"value": "1. How sensitive is CGDL to the assumption that activations can be linearly decomposed into concept and residual subspaces? Have you explored non-linear factorization methods (e.g., kernelized or autoencoder-based versions) to capture more complex multimodal interactions?\n2. You analyze activations from the penultimate residual stream of the language model. Did you experiment with other layers or modalities (e.g., cross-attention maps) to assess whether concept alignment is layer-dependent?\n3. The binary “concept vs. no-concept” prompting strategy is elegant, but could the binary framing bias activations toward token-level differences rather than broader semantic structures? How do you ensure it generalizes to abstract or relational concepts?\n4. Since the framework depends on SAM for concept localization, how robust is CGDL to noisy or ambiguous segmentation masks? Would substituting SAM with another localization model (e.g., grounding DINO or CLIP-based proposals) materially change performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GxGNXFjoQy", "forum": "MaHNO7vmsj", "replyto": "MaHNO7vmsj", "signatures": ["ICLR.cc/2026/Conference/Submission5603/Reviewer_Bj5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5603/Reviewer_Bj5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886906170, "cdate": 1761886906170, "tmdate": 1762918154412, "mdate": 1762918154412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a weakly supervised method for discovering and attributing visual–textual concepts in autoregressive LVLMs. Unlike prior concept­-extraction techniques (e.g. TCAV, CRAFT, CLIP-Dissect), which assume static encoders or rely on labeled concepts in simplified settings, CGDL can scale to multi-object, open‐vocabulary scenarios. The key innovation is to prompt the LVLM with a binary “concept vs. no-concept” question (e.g. “Does this image contain concept cₖ?”), collect activation residuals on positive and negative patches, then solve a contrastive dictionary learning problem per concept. By decomposing each patch’s activation into a “concept” basis plus a residual basis, CGDL enforces monosemantic, sparse, and non-overlapping vectors that align visual regions with textual concepts. Experimentally on ImageNet-1k and MSCOCO, CGDL shows improvements over prior baselines in sparsity, stability, low overlap, and attribution faithfulness (e.g. +9 % in CLIPScore, +4 % BERTScore)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. CGDL introduces a weakly supervised, scalable approach to concept extraction—a setting where most prior methods (e.g., TCAV, CRAFT, CLIP-Dissect, CoX-LMM) fail to generalize. By formulating concept discovery as a contrastive one-vs-all dictionary learning problem, CGDL avoids heavy supervision and scales effectively to thousands of visual–textual concepts.\n2. The proposed two-basis factorization (concept vs. residual) and contrastive residual extraction lead to sparse, disentangled, and monosemantic concept vectors. This improves interpretability by ensuring that each discovered component corresponds to a single, semantically coherent concept rather than a polysemantic mixture.\n3. Empirical results on ImageNet-1k and MSCOCO demonstrate stronger attribution faithfulness and higher quantitative interpretability metrics.\n4. CGDL treats LVLMs as black-box systems, requiring only access to intermediate activations, and thus applies broadly to different architectures. Its per-concept two-basis decomposition reduces computational cost and avoids the atom collapse problem common in large-vocabulary dictionary learning, ensuring scalability without sacrificing quality.\n5. Through contrastive concept prompts and SAM-based localization, CGDL effectively links visual patches with linguistic concepts, offering a coherent view of how LVLMs ground textual outputs in visual evidence. This multimodal grounding represents a key step toward transparent multimodal reasoning."}, "weaknesses": {"value": "1. CGDL relies on SAM to construct positive and negative patch sets. While this provides weak supervision at scale, the quality of discovered concepts is sensitive to segmentation accuracy and crop relevance. In cluttered or fine-grained scenes, imperfect segmentation can lead to noisy or overlapping concept bags, weakening monosemanticity.\n2. The method assumes that multimodal activations can be linearly decomposed into a “concept” and a “residual” subspace. This simplification may not capture the nonlinear dependencies or compositional reasoning inherent in LVLM representations, potentially limiting the interpretability of more abstract or relational concepts.\n3. Although CGDL addresses autoregressive LVLMs, it focuses on single-layer residual activations and per-concept snapshots. It does not explicitly model how concepts evolve across generation steps or interact in temporal or contextual chains, which are key to understanding how LVLMs reason during long-form captioning or dialogue.\n4. The experiments primarily report quantitative metrics (e.g., sparsity, overlap, CLIPScore, BERTScore) on ImageNet-1k and MSCOCO, which may not fully capture the qualitative interpretability or human alignment of discovered concepts. More extensive human studies or causal verification could strengthen claims about faithfulness and semantic clarity.\n5. While the two-basis formulation improves efficiency, it may oversimplify concept structure when scaling to very large or nuanced concept vocabularies. Complex concepts that require multi-dimensional representations (e.g., “a person holding a red umbrella”) might not be well captured by a single basis vector per concept."}, "questions": {"value": "1. How sensitive is CGDL to the assumption that activations can be linearly decomposed into concept and residual subspaces? Have you explored non-linear factorization methods (e.g., kernelized or autoencoder-based versions) to capture complex multimodal interactions?\n2. You analyze activations from the penultimate residual stream of the language model. Did you experiment with other layers or modalities (e.g., cross-attention maps) to assess whether concept alignment is layer-dependent?\n3. The binary “concept vs. no-concept” prompting strategy is elegant, but could the binary framing bias activations toward token-level differences rather than broader semantic structures? How do you ensure it generalizes to abstract or relational concepts?\n4. Since the framework depends on SAM for concept localization, how robust is CGDL to noisy or ambiguous segmentation masks? Would substituting SAM with another localization model (e.g., grounding DINO or CLIP-based proposals) materially change performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GxGNXFjoQy", "forum": "MaHNO7vmsj", "replyto": "MaHNO7vmsj", "signatures": ["ICLR.cc/2026/Conference/Submission5603/Reviewer_Bj5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5603/Reviewer_Bj5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886906170, "cdate": 1761886906170, "tmdate": 1763071051271, "mdate": 1763071051271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article addresses the pain point of verifying whether autoregressive Large Vision-Language Model (LVLM) predictions are grounded in visual concepts or result from bias or hallucination. The proposed method, Concept-Guided Dictionary Learning (CGDL), is designed for interpretable concept extraction and demonstrates consistent performance gains over the prior art (COX-LMM). A core experimental result highlights CGDL's effective scalability from 10 to 1k concepts, achieving a strong Combined Score of $0.64 \\pm 0.06$ on the ImageNet 1k concept set when using combined text and image conditioning"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research exhibits several notable strengths. First, the work tackles the critical and challenging problem of concept grounding and attribution within autoregressive Large Vision-Language Models (LVLMs), successfully extending interpretability methods beyond simpler encoder-only architectures. Second, the proposed Concept-Guided Dictionary Learning (CGDL) framework demonstrates robust scalability, showing consistent performance gains when moving from small-scale (10 concepts) to large-scale (1k concepts) benchmarks, which is essential for real-world concept dictionaries. Third, the empirical results are clearly quantified and rigorously benchmarked, with CGDL achieving a high Combined Score (CS) of $0.64 \\pm 0.06$ on the ImageNet 1k concept set, providing strong evidence of its attribution effectiveness."}, "weaknesses": {"value": "Despite its strengths, the work presents a few limitations that warrant further discussion. The generalizability of the CGDL method may be limited as the current results primarily emphasize performance relative to COX-LMM; hence, verification on a broader spectrum of distinct LVLM architectures is needed. Additionally, the rationality of core design choices in the dictionary learning process, such as the selected dictionary size or the specific sparsity constraints, requires a more detailed sensitivity analysis to demonstrate that the framework is robust across hyperparameter variations. Furthermore, the unexplored direction of concept types could be addressed by extending the evaluation beyond the current visual concepts (ImageNet 1k) to include more abstract or relational concepts."}, "questions": {"value": "Could the authors provide a more extensive ablation study detailing the influence and stability of the core hyperparameters governing the dictionary learning process within CGDL, offering empirical evidence to justify the final selection of key parameters? I also recommend including a discussion of the method’s performance and stability when applied to different foundational LVLM architectures or when tasked with attributing more abstract, non-object concepts critical for complex reasoning to confirm broader generalizability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9lWoqqHxRZ", "forum": "MaHNO7vmsj", "replyto": "MaHNO7vmsj", "signatures": ["ICLR.cc/2026/Conference/Submission5603/Reviewer_AqDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5603/Reviewer_AqDc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895855986, "cdate": 1761895855986, "tmdate": 1762918154103, "mdate": 1762918154103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of discovering interpretable and faithful concepts in autoregressive LMMs, where predictions may stem from hallucinations or spurious correlations rather than grounded visual evidence. To this end, the authors propose CGDL, a weakly supervised framework that automatically constructs positive and negative concept examples using segmentation-based localization and performs contrastive dictionary learning to disentangle concept-aligned activations from residual noise. By introducing a two-basis decomposition and monosemantic regularization, CGDL yields sparse, disentangled, and multimodally aligned concept representations. Extensive experiments demonstrate that CGDL achieves superior sparsity, stability, and faithfulness compared to existing interpretability methods for LVLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* This paper takes an important step by directly modeling concepts as generative factors within LMMs. I am happy that compared with existing post-hoc explanation methods the proposed approach provides stronger guarantees for model interpretability.\n* The authors discuss the related literature in considerable detail.\n* The paper is easy to follow.\n* The authors provide code to facilitate reproducibility checks."}, "weaknesses": {"value": "1. Please discuss the training cost of the proposed method and inference efficiency (e.g., FLOPS and memory usage compared to the LMM backbone), and compare the computation efficiency with existing approaches, if possible.\n2. It is recommended that the authors include more visualizations of the learned concepts and analyze cases of both correct and incorrect model reasoning to better illustrate the advantages and limitations of the proposed method compared with prior works.\n3. This paper is poorly written, with incorrect citation formatting and numerous punctuation issues, including inconsistent and incorrect use of dashes and hyphens."}, "questions": {"value": "My questions are in Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uPQ81W2vHi", "forum": "MaHNO7vmsj", "replyto": "MaHNO7vmsj", "signatures": ["ICLR.cc/2026/Conference/Submission5603/Reviewer_wx7H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5603/Reviewer_wx7H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911026823, "cdate": 1761911026823, "tmdate": 1762918153732, "mdate": 1762918153732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the CoX-LMM (NeurIPS 2024) method for concept-based explainability of vision-language models, such as LLaVA, Gemma, and Qwen-VL. It improves basic metrics for concept extraction by leveraging the Segment Anything Model (SAM) and Concept Activation Vectors (CAV). The experimental evaluation features qualitative examples that demonstrate the method's ability to discover fine-grained, monosemantic concepts, as well as its utility in attributing concepts to the model's responses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The key strengths of the paper are:\n1. Convincing motivation, i.e. we need good explanation methods for large vision-language models. \n2. Comprehensive experiments, spanning multiple models, metrics, and applied use-cases.\n3. Good writing and presentation (Figures, Tables)."}, "weaknesses": {"value": "The main result of this work is improving a few metric values by a bit with a complex methodology stitched from known approaches; not gaining new insights or discoveries:\n1. The proposed method is overcomplex; it includes four steps (Sec. 4.1-4.4), which have already been proposed (countless references to related work and reusing known ideas), making it challenging to evaluate the paper's contribution. For example, how is labeling concepts with an LVLM novel? See e.g. [a,b]. Or using the Segment Anything model in the process; see e.g. [c]. Sections 4.3 and 4.4 already begin with \"Following Parekh et al. (2024)/Kim et al. (2018)\" and so on.\n2. This paper heavily builds on the previous work \"CoX-LMM (Parekh et al., 2024)\", mentioning it 36 times, and compares only to it in experiments. The experimental setting (dataset, metrics, etc.) is specific to these two methods, making the contribution less significant.\n\n[a] Label-free concept bottleneck models. ICLR 2023\n\n[b] Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. CVPR 2023\n\n[c] DCBM: Data-efficient visual concept bottleneck models. ICML 2025"}, "questions": {"value": "1. The word \"scalable\" is used 6 times in the paper; how is it measured? Similarly, \"flexible\" and \"efficient\" appear without any supporting evidence.\n2. Why are the three highlighted metrics: \"4% higher sparsity, 11% greater stability, 17% lower overlap\" important for interpretability?\n3. Why are the experiments conducted using the Gemma and Qwen models, when CoX-LMM was evaluated with the DePALM and LLaVA models? This mismatch is confusing.\n\n- There is just too much bold and italicized text in the paper, making it hard to read.\n- In Table 1: \"Unlimited-object\" listed as a \"Key Limitation\" sounds odd."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YTVhBYuI3s", "forum": "MaHNO7vmsj", "replyto": "MaHNO7vmsj", "signatures": ["ICLR.cc/2026/Conference/Submission5603/Reviewer_PnqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5603/Reviewer_PnqH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931178675, "cdate": 1761931178675, "tmdate": 1762918153459, "mdate": 1762918153459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}