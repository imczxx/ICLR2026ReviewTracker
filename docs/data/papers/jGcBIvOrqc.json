{"id": "jGcBIvOrqc", "number": 16006, "cdate": 1758258537606, "mdate": 1759897267813, "content": {"title": "To Think or Not To Think, That is The Question for LLM Reasoning in Theory of Mind Tasks", "abstract": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of 11 advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform base models and sometimes perform worse. A fine-grained analysis reveals two main failure reasons. First, slow thinking collapse: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. These results highlight the advancement of LRMs in formal reasoning (e.g., math, code) cannot be transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods and we provide a preliminary exploration of such an approach with a combination of Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention.", "tldr": "", "keywords": ["Theory of Mind", "Large Language Model"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/017340623fa6a736a53464e418e5598b0487a1a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "n/a - please see ethics review. I believe this paper should be desk rejected because of its field."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n/a."}, "weaknesses": {"value": "n/a."}, "questions": {"value": "n/a."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I no longer think it is ethical to work on Theory of Mind in academia, and I think we should be actively discouraging development of these methods. I know that's a strong statement; please bear with me for a minute. I wrote one of the papers referenced heavily in this work and others like it, and I have since found out that my work has since been used for unethical applications including persuasion, marketing, and political advertising, in academia and in industry. Yes there are upsides to Theory of Mind – that's why I and many others started working on it in the first place. Better affective technologies, agents that can anticipate your needs without you asking, therapeutic agents, education agents...etc. But I think while these are indeed upsides, we can achieve these goals in other ways that leave the user's agency and mind uninvaded by AI's – e.g., by creating tools that users can have an easier time adapting to and learning, rather than tools that anticipate and adapt to the user. Essentially, I think the upsides are limited and clever solutions could get to the same ends in less invasive ways. And the downsides are basically infinite. If we're successful in building agents that really can understand what we're thinking and feeling without saying it, those agents will be used by big companies and people in power to successfully simulate our thinking and manipulate us into doing what they want. This isn't conjecture or conspiracy theory; each major tech company is actively investigating AI persuasion and marketing for exactly this reason, as are political campaigns and governments throughout the world. I would know, because many of them have reached out to me to help them implement their methods. As people, we should be scared by this.\n\nThere's a counter argument of course, which is that this is a cat and mouse game like security, and that academia shouldn't attempt to restrict research into these methods: it's best to have this all out in the open so that people can research defenses. While it's possible that defenses could be created against this type of technology, I think that is a dangerous and unproven hypothesis to rely on. It may be the case that this is a setting in which offense is much easier than defense, especially because unlike in computer security, the defender is ultimately a human, not an algorithm. The best and most secure defense would be to train humans to think critically about these types of attacks, and that is a very hard thing to do, especially as the attacks get more and more sophisticated and human education systems become increasingly politicized and unequal. I think it is much better to keep this pandora's box closed for as long as possible, and to work on functional uses of AI that have fewer ethical downsides. I would encourage the ethics committee to think carefully about this, discuss among themselves (and maybe reach out to me if you want to chat more), and do as much as possible to prevent works like this from being published in top conferences (which increases the visibility and incentive to work on this subject)."}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "NpTADqY13M", "forum": "jGcBIvOrqc", "replyto": "jGcBIvOrqc", "signatures": ["ICLR.cc/2026/Conference/Submission16006/Reviewer_3FPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16006/Reviewer_3FPz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761324264127, "cdate": 1761324264127, "tmdate": 1762926213660, "mdate": 1762926213660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper performed systematic analysis contrasting the theory-of-mind performance of reasnoning and non-reasoning models. The results show the “resoning” abilities that advance the performance on analytical tasks does not consistently bring improvement on ToM tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a quantitative study on the behaviour of reasoning models, covering several models and datasets, revealing several behavioral patterns of the reasoning models on this type of task."}, "weaknesses": {"value": "1. In general, the studies in the paper provide \"correlation\" between different behaviours, e.g. long response, i.e. extended thinking, is correlated with lower performance. The reviewer thinks this does not suffice for the causality conclusions made in the paper, e.g. slow thinking leads to reasoning collapse.\n2. The methods proposed by the paper (stop the thinking process when \"wait\" appears too many times; remove options in the prompt to force \"thinking\") are straightforward; the technical contribution is limited."}, "questions": {"value": "1. line 204\n> \"reasoning-tuned models can be more prone to error when confronted with deeply nested inference\"\n\nThe performance of gpt series int Table 2 does not support this claim, as the reasoning variants (o3 and o4mini) are consistently better than their non-reasoning variant (4o).\n\n2. In Table 1 and Table 2, the results are mixed, with no clear pattern across model families or datasets. E.g. the conclusion in Section 4.1.1, \"reasoning collapses in higher-order inference\" is not supported by gpt series and Qwen3-32B series. The discrepancy of behaviours in different models is not discussed.\n\n3. Pattern in Figure 1 also exhibits in analytical tasks. The reason behind this pattern in those tasks can be that when a problem is too difficult, the model tends to repeatedly generate useless reasoning steps. Simply from Figure 1 it's hard to draw the conclusion that the excessive reasoning is the **cause** of the failure. It could be that when a problem is out-of-distribution, and the model simply cannot solve it, its failure will be associated with a long generation. The correlation between performance and response length cannot contribute to conclusions about the causal effect of extended reasoning on performance, i.e. the first conclusion in Section 4.3.1 is not supported.\n\n4. In Section 4.3.2, for the comparison between ToM and \"formal reasoning\", there is no experiments on formal reasoning in the paper. The comparison is unclear and less supported. It would be helpful to report concrete numbers on different domains and then compare them.\n\n5. How does the proposed S2F method differ from [1]?\n\n[1] Zhang, Junyu, et al. \"AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time.\" arXiv preprint arXiv:2505.24863 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QLqdRoqo99", "forum": "jGcBIvOrqc", "replyto": "jGcBIvOrqc", "signatures": ["ICLR.cc/2026/Conference/Submission16006/Reviewer_VWm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16006/Reviewer_VWm9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874643330, "cdate": 1761874643330, "tmdate": 1762926213203, "mdate": 1762926213203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “To Think or Not To Think” investigates whether reasoning-oriented language models genuinely improve Theory of Mind (ToM) capabilities. Evaluating 11 models across three ToM benchmarks, the authors find that reasoning-tuned models often perform no better—and sometimes worse—than their non-reasoning counterparts. Through detailed behavioral analysis, they identify two key issues: slow-thinking collapse, where longer reasoning chains degrade accuracy, and option-matching shortcuts, where models rely on surface cues instead of true understanding. To address these, they propose two strategies—S2F, which trims unproductive reasoning, and T2M, which separates reasoning from answer selection—both improving ToM performance and highlighting fundamental differences between formal reasoning and social cognition in LMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper evaluated a fair number (11) of reasoning models from different model families, which makes the analysis more compelling.\n- The paper conducted a fairly comprehensive analysis of when and why the reasoning model fails to outperform, which provides valuable insights to the LLM ToM community.\n- Their proposed interventions, S2F (“stop slow-thinking failure”) and T2M (“think-then-match”), offer measurable gains."}, "weaknesses": {"value": "- The \"causality vs. correlation\" problem: the paper's claim “reasoning tuning hurts ToM” is supported empirically, but confounds (training data, safety filters, decoding defaults) aren’t fully isolated. The paper compares across different model families, not controlled ablations.\n- Claims about “formal vs social reasoning” rest only on the evaluation of three datasets. This small number of datasets makes the conclusion less convincing.\n- Most of the analysis is done only on a single benchmark (Hi-ToM).\n- The proposed methods constitute a main contribution of the paper. Yet they are pretty preliminary, and the paper lacks enough analysis of the methods. Also, the S2F method has minimal impact on ToMBench (and mostly on ToMATO)."}, "questions": {"value": "- Does \"vanilla\" and \"CoT\" prompting for reasoning models mean disabling and enabling thinking mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hSAFGWFCyc", "forum": "jGcBIvOrqc", "replyto": "jGcBIvOrqc", "signatures": ["ICLR.cc/2026/Conference/Submission16006/Reviewer_iDRt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16006/Reviewer_iDRt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950851949, "cdate": 1761950851949, "tmdate": 1762926212615, "mdate": 1762926212615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on the discussion around the reasoning advances of large reasoning models and their benefits in various applications compared to other types of non-reasoning large language models, this paper discusses the nuances of comparing reasoning and non-reasoning models in Theory of Mind (ToM) as one of the interesting applications of large language models. ToM involves unique characteristics not found in other types of reasoning benchmarks, such as math or logic. The authors find that reasoning in large reasoning models can be detrimental to ToM tasks, where specifically long chains of reasoning collapse the reasoning abilities of the models. They further find that there are certain shortcuts in benchmarks that models tend to utilize, and these shortcuts, in fact, make their reasoning abilities fragile and prevent them from true, unbiased reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is interesting in terms of the problem it tackles, examining the advantages of reasoning in large reasoning models for ToM-type tasks that are inherently different from other types of reasoning, such as mathematical reasoning. Their findings also expand upon other results in the literature on overthinking and efficient reasoning, where similar problems with lengthy reasoning have been discussed.\n\nThe paper is also comprehensive in terms of the experimental setup, covering a sufficient number of benchmarks and models from different families to make the findings more robust. It is relatively well-written and easy to follow in terms of the flow of arguments and reasoning provided by the authors.\n\nThere are also interesting experiments with insightful results, for instance:\n– The moderate thinking analysis, where they compare reasoning models with non-reasoning models that are prompted to perform chain-of-thought reasoning;\n– The evaluation of reasoning effort in GPT models; and\n– The finding that harder questions are sometimes complementarily solved by non-reasoning and reasoning models."}, "weaknesses": {"value": "Some of the comparisons in the paper are questionable, which in turn makes some statements and arguments less clear and less defensible:\n\n1.\tYou argue that reasoning models think longer, and this is evident in the cases where they fail in reasoning (Figure 1). But why have you compared the R1 model with the Claude model here? Wouldn't a better comparison be between R1 and V3, which belong to the same family?\n\n2.\tRelated to the same topic, apart from the figure that has the mentioned issue, you have not discussed anywhere else what the length distribution looks like for non-reasoning models. When questions are more difficult, you show that reasoning models produce much longer responses with incorrect reasoning. This, to some extent, is attributed to their characteristic verbosity, but it could also be because of the increased difficulty of the problem, which causes them to reason more. This might be evident in non-reasoning models as well. I actually think the authors' argument is intuitive and probably correct, but the evidence in the paper does not sufficiently support it.\n\n3.\tThe authors have included discussions of the performance of the Grok and Claude models, listed as reasoning models, and have also compared their performance with R1, for instance. I’m not sure if I follow why these two models are included, especially when they are compared with other models not in their family and thus not directly comparable. This makes the arguments about them less informative and, to some extent, flawed.\n\n4.\tWhy have you conducted the S2F experiments only with the Qwen models and not with others? I don’t want to be pedantic and ask for every experiment to be run on all models, but the current setup feels somewhat arbitrary. The same argument holds for the T2M experiments.\n\nSome details in the paper are inconsistent or missing, which makes replicability difficult. For instance, the paper hasn't mentioned any evaluation metric or evaluation criteria used, or if it has, the explanation is unclear. Other issues include inconsistent use of model names, for example, sometimes you mention Qwen-32B-Reasoning, and in other places R1-Distill-Qwen-32B.\n\nSince the authors mention System 1 and System 2 as a potential approach to finding a good dynamic system that combines the abilities of reasoning and non-reasoning models, it would be good to cross-check and reference the following paper as well:\n\nZiabari, A. S., Ghazizadeh, N., Sourati, Z., Karimi-Malekabadi, F., Piray, P., & Dehghani, M. (2025). Reasoning on a spectrum: Aligning LLMs to System 1 and System 2 thinking. arXiv preprint arXiv:2502.12470.\n\nSimilarly, the authors haven’t connected their arguments with findings in the overthinking literature, such as the following work. Adding this would enhance both clarity and the connection between their work and the broader line of research on this subject:\n\nSui, Y., Chuang, Y. N., Wang, G., Zhang, J., Zhang, T., Yuan, J., … & Hu, X. (2025). Stop overthinking: A survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419."}, "questions": {"value": "Please respond to the issues that I have raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TFQDC60BMN", "forum": "jGcBIvOrqc", "replyto": "jGcBIvOrqc", "signatures": ["ICLR.cc/2026/Conference/Submission16006/Reviewer_Qmcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16006/Reviewer_Qmcm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967705345, "cdate": 1761967705345, "tmdate": 1762926212221, "mdate": 1762926212221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the reasoning abilities of Large Reasoning Models, which have achieved major success in structured domains like mathematics and code, also have improvements in Theory of Mind tasks. The author is asking: Does explicit reasoning actually improve ToM performance, or it actually worsen the performance, and why.\n\nThe author used 11 LLMs (7 reasoning, 4 non-reasoning) and test their performance on 3 ToM benchmarks. The main finding is: reasoning models fail to outperform non-reasoning counterparts across most benchmarks. In specific, in high-order inference, reasoning model performs badly.\n\nThe author analyzed the reason: slow thinking on complex ToM tasks leads to failure (although moderate thinking with CoT prompt on non-reasoning model increases performance); reasoning models rely on matching multiple-choice options rather than genuine deduction and that hurts the performance. These are the difference between formal reasoning and ToM reasoning.\n\nThe author gives 2 ways to increase reasoning model's performance on ToM tasks (directly related to the 2 reason why reasoning model performs badly): i) slow-to-fast reasoning: Essentially force the model to end the thinking process when the reasoning is too long (signaled by the multiple usage of \"wait\"); and ii) Think-to-Match: Forces the model to reason without seeing options first, then introduces options only after generating internal reasoning"}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s central strength lies in its conceptual reframing of reasoning in large language models from formal to social cognition, challenging the implicit assumption that methods improving logical or mathematical reasoning automatically enhance Theory of Mind.\n\nThe paper also discovered that thinking long may not be better in the ToM tasks.\n\nThe above summary part has already covered the details."}, "weaknesses": {"value": "### Quality of the Control Study\n\nThe control design (section 3.1) relies on comparing different “reasoning” and “non-reasoning” model variants (e.g., Qwen3-Reasoning vs. Qwen3), but these pairs are still different models, differ in multiple confounding factors such as training dataset and reinforcement objectives (you are unlikely to know these training details, to begin with), making it difficult to attribute performance differences purely to the presense/absense of reasoning behavior. \n\nIn short, **this is not a good control study** and in my opinion there is a clearly better way to do that: use a within-model manipulation, prompting the same reasoning-tuned model to skip its reasoning process (inject a neutral “answer directly” context template, or fill in some dummy reasoning like \"I must answer the question now\"), to isolate the causal effect of explicit reasoning itself. \n\n### Justification of Section 4.2.1\n\nOne of the paper's main claim is: \"slow thinking on complex ToM tasks **leads** to failure\". This suggests a causal relationship. However in this section I only see correlation. One could easily argue that if a question itself is difficult, it causes both long reasoning and low accuracy, not that long reasoning caused low accuracy. (more suggestions are in the \"Question\" section below)\n\n### Other Minor Issues\n\n- Page 2, line 073: \"our **ablation study** on HiToM demnstrates that ...\": \"ablation study\" means removing a component of an ML system (for example, an attention head) and then analyzing the resulting performance of the system. So please use another term to avoid confusion;\n- Page 3, sec 3.1, first line: There are 11 models in total (7 reasoning+4 non-reasoning). The paper said 10;\n- Page 3, sec 3.2, fourth line: \"counterintuitive\" is a word, please use this instead of \"conter-intuitive\""}, "questions": {"value": "Please address/justify the weaknesses mentioned in the \"weakness\" section. Below are my specific questions/suggestions:\n\n### Quality of the Control Study\n\nI would suggest that the author either\n- Do the control study mentioned in the \"weakness\" section, or\n- Justify that the original paper's design is enough to attribute performance differences purely to the presense/absense of reasoning behavior, and why my proposal is not better\n\nAlso, Claude and Grok models are included in these 11 models but I do not see a reason of doing so, as no meaningful comparison group is set up.\n\n### Justification of Section 4.2.1\n\nThere are some causality-related experiments mentioned in the following chapters. In specific, sec 4.4.1 (slow to fast reasoning) can be reframed as a causal evidence. I would suggest that author \n- Expand sec 4.4.1 to all models (currently only Qwen is covered), and\n- Put them as direct evidence to support your argument that \"slow thinking on complex ToM tasks leads to failure\", rather than a discussion/exploration\n\n### Other Minor Issues\n\nPlease address the minor issues mentioned in the \"weakness\" section as well\n\n### Citation Cleaning\n\nWhen a paper appears on both arxiv and a published proceeding/conference, it would be better to mention the conference instead of leaving only an arxiv link. Below are some examples and please self-check the rest\n\n- Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023. **This paper appears in EMNLP findings, 2023. \"T\" and \"M\" should be capitalized**\n\n- Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. **This paper appears in EMNLP 2024**\n\n### Additional Questions on experiment details\n\nThe link in the \"reproducibility statement\" shows that \"the requested files are not found\". I would like the author to briefly explain, that for each of the 11 models you used, whether you call the model's api or do inference locally. A table showing the computation you used is also welcomed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PqbSBNFpDM", "forum": "jGcBIvOrqc", "replyto": "jGcBIvOrqc", "signatures": ["ICLR.cc/2026/Conference/Submission16006/Reviewer_auky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16006/Reviewer_auky"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16006/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976459463, "cdate": 1761976459463, "tmdate": 1762926211671, "mdate": 1762926211671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}