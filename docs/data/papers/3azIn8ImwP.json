{"id": "3azIn8ImwP", "number": 1630, "cdate": 1756899084362, "mdate": 1759898197624, "content": {"title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "abstract": "Vision–Language–Action (VLA) models leverage pretrained Vision–Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To improve performance, many methods have been proposed to incorporate additional observation cues (e.g., depth maps, point clouds) and auxiliary modules (e.g., object detectors, encoders), enabling more precise and reliable task execution. Although effective, these approaches often require extensive data collection and additional training or fine-tuning, limiting their flexibility and scalability. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory'', we propose **U**ncertainty-**a**ware **O**bservation **R**einjection (**UAOR**), an effective training-free and plug-and-play module for VLA models. Specially, when the current language model layer exhibits high uncertainty, measured by **Action Entropy**, it reinjects the observation information into the next layer's Feed-Forward Network (FFN) in a blending manner. This mechanism helps VLA models look more clearly on the observation during inference, enabling more confident and faithful action generation. Comprehensive simulation and real-world experiments show that our method consistently improves the performance of heterogeneous VLA models across various tasks and embodiments while incurring minimal computational overhead. Notably,  **UAOR** eliminates the need for extra observation cuse or modules, making it a versatile and practical plug-in for existing VLA pipelines.", "tldr": "An effcient training-free module to augment Vision-Language-Action Models by Uncertainty-aware Observation Reinjection", "keywords": ["Vision-Language-Action", "Uncertainty-Aware", "Observation Reinjection"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5f3fcb574850bb9b286cfbfdc9096cb6c97f38b.pdf", "supplementary_material": "/attachment/a5675fd4f9b1a93bd03efbc45f274c205d532dc1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Uncertainty-aware Observation Reinjection (UAOR), a novel, training-free, and plug-and-play module designed to enhance the performance of Vision-Language-Action (VLA) models. The core idea is to counteract the \"forgetting\" of initial observation information as data propagates through the model's layers. The authors propose a metric, \"Action Entropy,\" to measure layer-wise uncertainty during inference. When this uncertainty surpasses a predefined threshold at a given layer, UAOR \"reinjects\" the original observation features into the Feed-Forward Network (FFN) of the subsequent layer. This mechanism is inspired by the concept of FFNs acting as key-value memory, allowing the model to dynamically \"re-focus\" on crucial sensory inputs when its confidence wanes. The authors provide a theoretical analysis based on information theory to justify their approach and validate its effectiveness through extensive experiments on multiple simulation benchmarks (LIBERO, SIMPLER, CALVIN) and in the real world, demonstrating consistent performance improvements across various VLA architectures with negligible computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The work is motivated by a clear and compelling intuition—that observation information decays through deeper network layers, leading to increased uncertainty.\n- The authors demonstrate the effectiveness of UAOR across three different VLA baselines with varying architectures (single-system and dual-system) and scales (0.5B to 7B), on three distinct simulation benchmarks.\n- The inclusion of a theoretical analysis (Section 3.4) adds significant depth and credibility to the paper.\n- The paper is exceptionally well-written, with a logical flow and clear explanations of complex concepts."}, "weaknesses": {"value": "- The proposed \"Action Entropy\" metric relies on projecting the hidden states of every intermediate layer through the MLP of the final layer to get a probability distribution. This approach feels somewhat ad-hoc and potentially inefficient. It raises questions about whether this is the most direct or optimal way to measure uncertainty, as it depends on a component (the final layer's MLP) far downstream from where the uncertainty is being measured.\n\n- The rationale for this one-layer delay is not discussed. Ablation studies exploring reinjection into the current layer's FFN, or even into the self-attention block, are missing and would provide valuable insight into this specific design choice.\n\n- While the selection of baselines is commendably diverse, the paper acknowledges that it does not apply UAOR to some of the most recent and powerful state-of-the-art models like $\\pi_0$. Demonstrating gains on these near-SoTA models would make the claims of general applicability even more powerful.\n\n- The analysis in Section 4.4 explores a heuristic for weighting visual tokens based on language similarity but finds it does not improve performance over uniform weighting. While the authors provide plausible hypotheses, this result is somewhat counter-intuitive and warrants a deeper investigation. It might suggest that the simple language-similarity weighting is flawed, rather than the concept of weighting itself."}, "questions": {"value": "- Could you provide a more detailed rationale for the \"Action Entropy\" metric design? Specifically, why project intermediate hidden states through the final layer's action head, rather than using a more direct uncertainty measure from the hidden states themselves (e.g., entropy of the feature distribution, or a lightweight learned uncertainty head)? What is the computational overhead of these repeated projections during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YxwnpJFske", "forum": "3azIn8ImwP", "replyto": "3azIn8ImwP", "signatures": ["ICLR.cc/2026/Conference/Submission1630/Reviewer_rnbV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1630/Reviewer_rnbV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551740553, "cdate": 1761551740553, "tmdate": 1762915836894, "mdate": 1762915836894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UAOR, a lightweight, training-free module designed to boost VLA models. It aims to enable VLA models\nlook more clearly on the observation during inference, enabling more confident and faithful action generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Demonstrates a strong understanding of the current limitations of VLA models.\n\n2. Attempts to address the identified problems in a general and systematic way.\n\n3. Proposes the concept of action entropy and applies it effectively in the forward process. The accompanying theoretical analysis is solid and convincing.\n\n4. Provides a well-designed ablation study to examine the effects of observation injection."}, "weaknesses": {"value": "1. The explanation of why forgetting leads to uncertainty lacks a clear reasoning process.\n\n2. Theorems 3.1–3.4 appear largely independent and not well integrated. It would be better to unify them into a holistic framework or justify that they represent complementary perspectives on the same problem.\n\n3. Real-world experiments are only conducted using Open-VLA, neglecting other baseline models such as CogACT.\n\n4. The relationship between α and γ should be jointly analyzed, as variations in one may influence the behavior or trend of the other."}, "questions": {"value": "1. Is the action entropy gate essential for injecting observation features?\n\n2. Can hidden states or proprioceptive states reliably represent action entropy?\n\n3. In Equation (9) of Algorithm 1, why is h_t^{(l+1)} used instead of h_t^{(l)}? The paper does not discuss uncertainty propagation between adjacent layers.\n\n4. In Section 4.4, the discussion on token-level weighting explores applying weights to visual tokens based on language instructions. However, the ablation study in Section 4.3 shows that injecting language instructions offers no performance gain. This suggests that the similarity between visual and language tokens provides limited benefit. Why not explore similarities between visual and proprioceptive tokens instead? Besides, It is plausible that redundant (similar) information adds little value, whereas orthogonal (complementary) information may be more beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UUXSTGlFIh", "forum": "3azIn8ImwP", "replyto": "3azIn8ImwP", "signatures": ["ICLR.cc/2026/Conference/Submission1630/Reviewer_A2fr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1630/Reviewer_A2fr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637847272, "cdate": 1761637847272, "tmdate": 1762915836773, "mdate": 1762915836773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UAOR (Uncertainty-Aware Observation Reinjection), a training-free and plug-and-play module designed to enhance VLA models. UAOR measures layer-wise action entropy to detect high-uncertainty regions during inference. When uncertainty exceeds a threshold, UAOR reinjects observation features into the next layer’s FFN, treating the module as a key-value memory to restore visual representation. Experiments across multiple benchmarks and real-world robot tasks show a certain degree of performance gains with negligible computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. I think the problem addressed by this paper is very important, as overly deep LLM layers do tend to ignore certain visual information.\n\nS2. The paper proposes a simple yet efficient method to alleviate this issue."}, "weaknesses": {"value": "W1. My main concern lies in whether using Action Token Entropy to measure visual uncertainty is reasonable.\n\n\nW2. Pretrained VLA models typically generate actions only from the final layer, without utilizing or supervising intermediate features for action prediction. Therefore, the observed layer-wise “action” token entropy may result from the training paradigm itself rather than reflecting the actual dynamics of feature changes within the model. I suggest that the authors finetune the VLA model so that each LLM layer outputs actions for verification.\n\nW3. Regarding the analysis of Action Token Entropy in Figure 1, I also find it unconvincing. Prior machine learning studies have shown that deeper layers’ feature representations are more task-specific, which could naturally cause the entropy variation (rather than the reason proposed by the authors)."}, "questions": {"value": "Q1. My minor concern is that the performance improvements brought by UAOR are relatively small. For example, on LIBERO and CALVIN, the gains are only about 0.9\\%, which seems quite incremental. I therefore recommend that the authors consider finetuning the model so that it can better adapt to the newly introduced token sequence.\n\n\nQ2. Since the authors’ motivation is very good, I would be willing to raise my score if they can address my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fLwp1V3DQ9", "forum": "3azIn8ImwP", "replyto": "3azIn8ImwP", "signatures": ["ICLR.cc/2026/Conference/Submission1630/Reviewer_7dgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1630/Reviewer_7dgg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894584058, "cdate": 1761894584058, "tmdate": 1762915836654, "mdate": 1762915836654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free method for reinjecting observations back to transformer layers of VLAs. The paper claims that VLAs tend to progressively forget observations, and they proposes to blend the observation features with the FFN output via the key-value memory mechanism. The paper compares against other VLAs on the LIBERO, SIMPLER, and real-world benchmarks, showing some gains while not introducing too much latency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is fairly well-written and easy to understand.\n- The problem studied is important."}, "weaknesses": {"value": "- The core premise of the paper is not well-supported. The author claims that: \"Our key intuition is that after ingesting the observation, the model tends to progressively “forget” during forward inference\" and back this up by Figure 1, where the **early** layers of the VLA experiences a mild increase in action uncertainty. This is neither convincing nor well-explained. To me, Fig. 1 is actually quite reasonable: early in the computation, there's more uncertainty in the action distribution but as the model has the chance to process the representations more in later layers, the uncertainty decreases, almost to 0 at the last layer. \n\t- Even if the claim that increasing in uncertainty equals to forgetting observation, this doesn't explain how the uncertainty eventually decreases at the mid layer and almost to 0 at the final layer.\n\t- It is not clear that action uncertainty can be tied to \"forgetting observation\" as neural networks dynamics can be complicated. We need stronger evidence, for example drawing tools from interpretability research, perhaps looking at some sort of cross attention between the outputs and the observations, to validate the core premise.\n- The quantitative results are not clear.\n\t- All the gains are very modest.\n\t- No measure of statistical confidence. No details on how many trials were run. \n\t- Table 4 is not clear. What task / benchmark is this?\n\n- Some technical details are not well-justified. \n\t- It is not clear that if you just take the middle hidden representations and decode that, computing the \"action\" entropy is still meaningful. For intuition, for classifer guidance in image diffusion literature, you can't just take the intermediate results during denoising, and ask a classifer that was ever only trained on clean images to infer on noisy inputs. You either need to train the classifier on noisy images too or use techniques like diffusion posterior sampling, tangent projection et cetera. So again, it's not clear to me that decoding actions from an intermediate layer is a meaningful operation.\n\t- If I understand this correctly, the paper applies their re-injection during test-time without ever training the base policy. During training, the base model is only exposed to hidden representations while during inference it must deal with \"out-of-distribution\" embeddings from reinjecting observation. It's not clear why this OOD issue wouldn't destroy performance."}, "questions": {"value": "- Do you see any connection between the problem and residual networks? The intuition of skip connection is fairly similar: as the depth increases, it's harder to train, and the network \"forgets\" its observation. Thus, skip connection adds the observation back after each layer.\n- It would be nice to see a conceptual and also quantitative discussion / comparison with skip connections (which is actually already present in the transformer architecture)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWWb8DeQW3", "forum": "3azIn8ImwP", "replyto": "3azIn8ImwP", "signatures": ["ICLR.cc/2026/Conference/Submission1630/Reviewer_surj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1630/Reviewer_surj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928180847, "cdate": 1761928180847, "tmdate": 1762915836515, "mdate": 1762915836515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}