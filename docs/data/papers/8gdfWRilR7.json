{"id": "8gdfWRilR7", "number": 13403, "cdate": 1758217438334, "mdate": 1762973597143, "content": {"title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "abstract": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating Artificial Intelligence (AI) agents on time series machine learning engineering challenges. Existing benchmarks lack scalability, focus narrowly on model building in well-defined settings, and evaluate only a limited set of research artifacts (e.g., CSV submission files). To make AI agent benchmarking more relevant to the practice of machine learning engineering, our framework scales along two critical dimensions. First, recognizing that effective ML engineering requires a range of diverse skills, TimeSeriesGym incorporates challenges from diverse sources spanning multiple domains and tasks. We design challenges to evaluate both isolated capabilities (including data handling, understanding research repositories, and code translation) and their combinations, and rather than addressing each challenge independently, we develop tools that support designing multiple challenges at scale. Second, we implement evaluation mechanisms for multiple research artifacts, including submission files, code, and models, using precise numeric measures and _optionally_ LLM-based qualitative assessments. This strategy complements objective evaluation with subjective assessment when appropriate. Although our initial focus is on time series applications, our framework can be readily extended to other data modalities, broadly enhancing the comprehensiveness and practical utility of agentic AI evaluation. We [open-source](https://anonymous.4open.science/r/TimeSeriesGym-9CF6/) our benchmarking framework to facilitate future research on the ML engineering capabilities of AI agents.", "tldr": "We introduce TimeSeriesGym, a general environment to evaluate AI Agents on time series machine learning challenges.", "keywords": ["AI Agents", "Time Series", "Scalable Benchmarking", "Fine-Grained Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6cd8669fa02248564c688c635051e70a37a8c33.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an open-source benchmark TimeSeriesGym for MLE agents evaluation, mostly focused on time series data. It contains 33 challenges and evaluated 3 scaffolds of agents with each scaffold evaluated on up to 3 base models. The framework supports multimodal output evaluation, specific skill evaluation and holistic evaluation(quantitative & qualitative)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This benchmark fills a gap on time series ML pipeline among existing ML benchmarks. \n\nThe proposed benchmark offers specific skills evaluation and hybrid scoring of qualitative + quantitive evaluation."}, "weaknesses": {"value": "This paper emphasizes benchmark at scale which would require limited human efforts but there is no discussion on how exactly new challenges should be adapted to the time series gym. It's mentioned that new challenge can be added within two hours, but how is the quality of the generated new challenges? In addition, two hours time is probably also enough to set up a specialized evaluation for a challenge without the need to integrate it to the current evaluation framework. The evidence for claimed scalability is limited, at least not well-presented. \n\nIt feels like this paper is mostly doing the same thing as MLE-bench but with specifically curated challenges focusing on time series and additionally took inspiration from other benchmarks and combined their advantages such as multimodal, skill-based, and holistic evaluation. \n\nThe experiment findings are also very predictable. Existing agents don't have the full capability to automate complete ML pipeline and highly likely the reasoning model produces more reasonable solutions. It's hard to find new takeaways or interesting insights from this paper. \n\nSome experiment settings are not meaningful. For example, providing remaining time reminders to agents vs. without time reminders. I find it hard to see the value in this experiment. \n\nThe evaluation is mostly limited to gpt 4.1 model, and there is no open source models being evaluated at all. \n\nSome challenges feel very synthetic. For example, implement MOMENT for anomaly detection task. But why do we need to integrate such task into this benchmark at all given that the code to solve this task is already available in MOMENT official github repo."}, "questions": {"value": "The paper lacks clarity. It's not clear how exactly the challenges are defined, are they all manually crafted based on Kaggle and Github Repos? Line 155-163 shows what each challenge is made or but is it manual effort to create these information based on the dataset or repository? \n\nIn line 315-316, it's mentioned that \"simply loading and re-saving the provided sample submission file without any model inference or data processing is deemed unreasonable\". How is this checked? Is it through llm-as-a-judge evaluation or manual inspection? \n\nIn line 228-231, the paper mentions that it can grade diverse artifacts generated throughout the MLE life cycle but how? I don't think this is explained. Rather than spending almost a whole page on limitation/future work/open questions, I would suggest spending more text to improve the clarity of the paper and explaining details of this benchmark framework in a thorough manner."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rJGPQoyWat", "forum": "8gdfWRilR7", "replyto": "8gdfWRilR7", "signatures": ["ICLR.cc/2026/Conference/Submission13403/Reviewer_E1hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13403/Reviewer_E1hF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547531492, "cdate": 1761547531492, "tmdate": 1762924036578, "mdate": 1762924036578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TimeSeriesGym, a benchmark and environment for evaluating agents that perform end-to-end machine learning work on time series problems. The benchmark aggregates a collection of challenges that span forecasting, classification, anomaly detection, data cleaning, hyperparameter tuning, and code migration. It evaluates not only final predictions but also multiple artifacts such as code, trained models, and submission files. It defines aggregate metrics for validity and reasonableness and reports results across different toolchains and foundation models. The paper argues that time series is an underrepresented setting for agent evaluation and that real engineering workflows require capabilities beyond model inference. The experimental section compares several agent frameworks under controlled resources and reports ablations on time budgets and guidance strategies. The authors claim that TimeSeriesGym is scalable, extensible, and better aligned with the realities of time series engineering than prior leaderboards that focus on a single metric or a single output file."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation that time series engineering requires more than single step prediction and that agents should be evaluated on multi stage workflows.\n\n- The authors collected a large number of datasets, with broad task coverage and inclusion of multiple domains which improves the ecological validity of the benchmark.\n\n- In this paper, multi artifact evaluation that considers predictions, code quality checks, and trained models which aligns the benchmark with real practice.\n\n- Agent agnostic design that supports different frameworks and enables trajectory collection for future training.\n\n- A documented process for adding new tasks which supports scalability and long term maintenance."}, "weaknesses": {"value": "1. The paper mixes the benchmark, the task generation mechanism, the multi artifact scoring, the trajectory data loop, and the time series focus without a clear primary to secondary hierarchy and without a concise contribution figure. It does not decompose difficulty across different time series task families and it does not design difficulty along agent reasoning dimensions.\n\n2. Due to compute constraints most experiments are conducted on the Lite set of six tasks. Although the authors state that these tasks cover key skills there is no statistical validation of domain diversity or difficulty distribution which can bias conclusions toward low cost scenarios.\n\n3. The contribution is primarily engineering. The paper lacks a unifying methodological or theoretical insight. Although this is a benchmark paper it would benefit from a central methodological principle that guides design choices across sections.\n\n4. The table presents results for 4/50→12/150 and for Step wise reminder vs No reminder, but it does not include significance testing.\n\n5. The definitions of Valid submission and Reasonable submission appear in prose in the methods section without a unified symbolization or an explicit decision boundary. This reduces clarity and hurts reproducibility."}, "questions": {"value": "1. Can you provide stratified results by task family with confidence intervals and tests for significance. This would help to understand which capabilities drive aggregate gains.\n\n2. How do you ensure that the Lite subset is representative of the full benchmark. Please include quantitative evidence of coverage and difficulty distribution.\n\n3. Can you formalize the validity and reasonableness criteria using symbols and thresholds and show calibration plots or decision curves.\n\n4. What steps can be taken to reduce reliance on leaderboard medians or external competitions that may change over time."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pT9RYAcR8n", "forum": "8gdfWRilR7", "replyto": "8gdfWRilR7", "signatures": ["ICLR.cc/2026/Conference/Submission13403/Reviewer_9ifC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13403/Reviewer_9ifC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890184043, "cdate": 1761890184043, "tmdate": 1762924036101, "mdate": 1762924036101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark framework named TimeSeriesGym, designed to evaluate the capabilities of AI agents in time series machine learning engineering tasks. The core idea is to provide a scalable and agent-agnostic evaluation environment that encompasses time series challenges across multiple domains, while assessing various outputs generated by agents—including prediction files, code, and models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1、 The benchmark collects and designs tasks based on real-world data science scenarios, including Kaggle competition problems and practical research tasks such as code migration and model evaluation. These challenges span a wide range of skills—including data processing, model construction, and code understanding and adaptation—reflecting the multifaceted challenges faced by real-world machine learning engineers.\n\n2、 TimeSeriesGym evaluates multiple forms of agent outputs, not only focusing on prediction accuracy and error metrics, but also assessing code generation, model artifacts, and more. Additionally, it introduces optional LLM-based review mechanisms (e.g., code auditing) to supplement quality evaluation.\n\n3、 The authors provide tooling mechanisms that enable large-scale automatic generation of new tasks, making the benchmark highly extensible and sustainable, with the ability to continuously incorporate new challenges."}, "weaknesses": {"value": "1、 The primary contribution of the paper lies in the construction of the benchmark. Many of its ideas—such as using LLMs for code review, incorporating multi-source tasks, and adopting multi-metric evaluations—are extensions and integrations of existing work rather than entirely novel innovations.\n\n2、 Although the paper lists several existing benchmarks, the distinctions and connections between TimeSeriesGym and those benchmarks are not sufficiently elaborated. For example, beyond the domain difference, how does TimeSeriesGym improve upon the evaluation philosophy of MLE-Bench? A deeper comparative analysis would strengthen the positioning of the proposed framework.\n\n3、 Among the 33 current challenges, tasks sourced from Kaggle tend to be well-structured and may have standard solutions, while the original TimeSeriesGym tasks are often highly complex. GPT-4 achieves only a 12.1% “reasonable solution” rate on TimeSeriesGym, with many tasks yielding no valid outputs. It is recommended that future task sets include medium-difficulty challenges to ensure a smoother difficulty gradient, enabling better tracking of agent performance from beginner to advanced levels. Additionally, establishing baselines for each task—such as simple algorithms or human-level performance—would help characterize task difficulty and the potential improvement space for agents."}, "questions": {"value": "1、 As the authors noted, due to funding and time constraints, most experiments were conducted on a Lite subset of 6 tasks. This raises two concerns: (A) Can the Lite subset sufficiently represent the full benchmark? Although the authors selected diverse tasks, the sample size of six remains relatively small. (B)Some results—such as the extension of agent steps to 12 hours—were only tested on the Lite subset. It is unclear whether similar trends would hold across the full benchmark.\n\n2、 Additionally, at line 76 below Figure 1, there appears to be white-colored text revealing the authors' institutional affiliation (“Ⓒ 2025 Auton Lab, Carnegie Mellon University”). This may violate the double-blind review policy and should be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vApgYrdfP4", "forum": "8gdfWRilR7", "replyto": "8gdfWRilR7", "signatures": ["ICLR.cc/2026/Conference/Submission13403/Reviewer_CCeW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13403/Reviewer_CCeW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981693521, "cdate": 1761981693521, "tmdate": 1762924035655, "mdate": 1762924035655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce TimeSeriesGym, a new benchmarking framework designed to evaluate AI agents on machine learning (ML) engineering tasks specifically for the time series domain. The paper argues that existing benchmarks are flawed, focusing too narrowly on well-defined Kaggle-style problems, lacking scalability, and only evaluating final submission files. TimeSeriesGym aims to solve this by sourcing diverse challenges (including 20 \"Original\" tasks based on real-world engineering like code migration and hyperparameter tuning), designing a framework to grade multiple artifacts (code, models, and submissions), and employing a holistic evaluation that includes both quantitative metrics and qualitative \"LLM-as-a-judge\" assessments. The framework also claims to be scalable with tools for new challenge creation. The paper's main experimental finding is that current state-of-the-art agents perform very poorly on these tasks, often failing to produce even a \"reasonable\" submission."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Identifies a Clear and Important Gap: The paper is correct that time series is an underserved domain in agentic benchmarking. Creating a dedicated benchmark for this is a valuable contribution.\n\nFocuses on \"ML Engineering,\" Not Just Modeling: The strongest part of the paper is its inclusion of \"TimeSeriesGym Originals\" (Table 4). Challenges like \"Convert ResNet TensorFlow implementation to PyTorch\" or \"Improve PTB-XL ECG Classification Code\" are excellent, real-world tasks that go beyond the standard \"train-a-model-on-a-CSV\" format.\n\nHolistic Evaluation Concept: The ambition to evaluate multiple artifacts (code, models) and use a \"two-faceted grading approach\" (quantitative and qualitative, see Appendix E) is the right direction for a comprehensive benchmark.\n\nScalability as a Design Goal: Designing the benchmark to be scalable from the start (even if not fully proven) is a smart, forward-thinking approach that addresses a major flaw in static benchmarks."}, "weaknesses": {"value": "**Critically Flawed Evaluation of Kaggle Challenges:**\n\nThe paper's primary evaluation metrics, \"Valid Submission (%)\" and \"Reasonable Submission (%),\" are insufficient.\n\n- The benchmark fails to report the actual leaderboard scores or ranks for the 13 included Kaggle competitions. This is a significant omission, as these are the most standardized, competitive tasks in the dataset.\n\n- The bar for a \"Reasonable Submission\" is set at scoring \"above median on the competition's public leaderboard\" (Section 4). This threshold is exceptionally low and provides no real insight into an agent's capabilities. An agent that barely surpasses the median is treated identically to one that achieves a state-of-the-art, gold-medal-winning score.\n\n- This lack of granular, comparable results (as seen in Table 10) makes it impossible for the research community to evaluate an agent's true performance or benchmark progress against established human or SOTA baselines on these problems.\n\n**Conceded Risk of Data Leakage:**\nThe paper's reliance on popular, public Kaggle competitions fundamentally compromises its ability to evaluate modern LLM-based agents. The authors' defense—that agents \"performed poorly anyway\" or that the benchmark is \"scalable\" to add new tasks—is unconvincing. It effectively concedes that the current Kaggle-based portion of the benchmark is unsuitable for reliably evaluating frontier models, as performance may be confounded by memorization.\n\nSubjective and Non-Scalable Evaluation Protocol:\n\n- The paper's claim of being a \"scalable benchmark\" is directly undermined by its own evaluation methodology for the \"Originals\" challenges.\n\n- For non-Kaggle tasks, \"reasonableness\" is determined by the authors \"manually inspecting\" if a \"genuine modeling attempt\" was made (Section 4, Metrics). A benchmark that requires subjective, manual intervention from its creators for a primary metric is, by definition, not scalable or objective.\n\n- The proposal to use \"LLM-as-a-judge\" (Appendix E) as a secondary evaluation method introduces a notoriously unreliable, biased, and difficult-to-reproduce component, which is not a substitute for rigorous, objective, quantitative metrics.\n\n- The authors themselves admit the protocol's flaws in Section 5 (\"Defining and measuring success\"), stating that \"our current evaluation approaches have inherent limitations.\"\n\n**Miscalibrated Difficulty (Floor Effects):**\n\n- The reported results are so poor that the benchmark, in its current form, largely fails as a diagnostic tool.\n\n- The best agent on the full benchmark (AIDE + GPT-4.1) only achieved a \"reasonable\" submission 12.1% of the time (Section 4.1).\n\n- Even on the hand-picked, \"easy\" Lite benchmark, the best agent (AIDE + Claude 3.7) only succeeded 38.9% of the time (Table 2).\n\nThese results indicate a significant floor effect. The tasks are currently too difficult for SOTA agents, preventing any meaningful differentiation between models or scaffolds. The benchmark primarily demonstrates that all agents fail, offering little insight into why they fail or which approaches are more promising."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UF2tyjb7Kw", "forum": "8gdfWRilR7", "replyto": "8gdfWRilR7", "signatures": ["ICLR.cc/2026/Conference/Submission13403/Reviewer_c71P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13403/Reviewer_c71P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981823497, "cdate": 1761981823497, "tmdate": 1762924035195, "mdate": 1762924035195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}