{"id": "3m3E1TEjtb", "number": 12768, "cdate": 1758210174218, "mdate": 1759897488267, "content": {"title": "MODE: Multi-Objective Dynamic Coreset Selection", "abstract": "We present \\mode (Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \\mode adapts selection criteria to training phases: \nemphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves $(1-1/e)$-approximation with $O(n \\log n)$ complexity \nand demonstrate competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \\mode reduces memory requirements \n%by 10× on ImageNet \nwhile providing actionable insights about which data types matter most during different training phases.", "tldr": "MODE adaptively selects the most useful data throughout training, achieving theoretical guarantees and scalable efficiency while preserving model performance with far less data.", "keywords": ["Coreset selection", "Submodularity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/036608f528dd91ed5d767d03b7b9af68efc7a16c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for coreset selection. It progressively selects training data based on model training results. Several strategies are introduced to boost efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Although the performance is sub-optimal, extensive empirical experiments are conducted to compare MODE and baselines."}, "weaknesses": {"value": "1. The title and method name are misleading. The problem formulation is not multi-objective in any sense. The problem statement minimizes only the coreset size as a single objective.\n2. The paper is confusing and difficult to read. The description of the method—how the coreset is progressively selected—is missing from the main text. Section 2 merely lists several metrics and defers the full algorithmic details to the appendix, which severely disrupts readability. This issue recurs elsewhere (e.g., Line 265 refers readers to Table 12, which is located at the very end of the paper). Much of the main text focuses instead on implementation details related to efficiency optimization, which are too niche for the core paper. Section G.1, which contains important results, should be moved to the main body. The authors should reconsider the paper’s structure and prioritize what truly matters in the main text. Reviewers are not obliged to read the appendix, and the current layout makes the paper unnecessarily hard to follow.\n3. Performance is sub-optimal. The method consistently underperforms compared to CREST. Furthermore, there is no efficiency comparison between MODE and CREST. If MODE takes longer to train while achieving worse results, its practical value is questionable.\n4. Insufficient discussion of the method’s nature. The idea of progressively selecting training data during training closely resembles active learning, yet the paper barely discusses this connection. The authors attempt to overcome the one-shot limitation of traditional coreset selection by adopting a dynamic selection process inspired by active learning, where data are chosen iteratively throughout training. However, the proposed approach **retrains the model from scratch** at each selection stage, which contradicts the claim of being “adaptive” or “real-time.” According to Algorithm 1 (Line 40), the model is completely reinitialized after every validation step, so it does not retain or update knowledge adaptively across rounds.\n5. Figures require major improvement. Figure 1 appears in the paper without any reference in the text, making its purpose unclear. Figure 2 has excessive white space, making it impossible to discern details without zooming in. Moreover, each subfigure is insufficiently discussed in the main text. I could continue listing such issues, but it would not be a productive use of time.\n6. The proofs appear to omit important steps. For instance, the transition from Eq. (30) to Eq. (32) vaguely attributes the result to “careful manipulation and concentration inequalities” without explanation. Similarly, Eq. (35) presents only a proof sketch without a complete derivation. What's more, the theory does not seem justified by experiment results. In figure 2(d), I don't see how approximation scales as $O(\\frac{1}{\\sqrt{B}})$\n7. The authors claim that their method provides an “interpretable” selection strategy, but no convincing evidence or analysis supports this statement."}, "questions": {"value": "1. Table 5 requires further details. What is the experiment setting that arrives at this table? I couldn't find a description. You only compare with active learning baseline GLISTER. How does the total time to arrive at the coreset compare with other baselines, especially CREST?\n2. In Figure 2, how does randomly select coresets perform in the $\\epsilon$-approximation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dJqC18GGah", "forum": "3m3E1TEjtb", "replyto": "3m3E1TEjtb", "signatures": ["ICLR.cc/2026/Conference/Submission12768/Reviewer_hqLA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12768/Reviewer_hqLA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775281665, "cdate": 1761775281665, "tmdate": 1762923579878, "mdate": 1762923579878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to make data selection an intelligent process that adapts to a model’s learning state, transforming data selection from a static, one-size-fits-all decision into a dynamic, adaptive procedure. The core motivation stems from deep learning’s reliance on large-scale datasets and the substantial costs they incur. To address this challenge, a dynamic, multi-objective adaptive core-set selection framework, MODE, is proposed. MODE coordinates three key components: a set of complementary scoring strategies for evaluating sample value along multiple criteria (uncertainty, diversity, class balance, and boundaryness); an adaptive meta-controller that learns the optimal mixture of these strategies based on real-time training feedback; and a temperature-controlled weighting mechanism that balances exploration for the evolving model with exploitation of validated selection patterns. By allowing the meta-controller to dynamically adjust the emphasis of data selection according to the model’s learning state, MODE constructs compact, high-quality data subsets that substantially reduce training cost while preserving strong model performance, demonstrating superior data efficiency across multiple benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is innovative, notably in the design of the adaptive meta-controller and the emergent curriculum learning. The meta-controller dynamically monitors training state and adjusts the weights of multiple data-selection strategies so that data selection aligns with the model’s evolving learning needs.\n\n2. The experimental design is rigorous and well-justified. The authors compare MODE extensively to classic baselines such as random sampling and to recent benchmarks including CREST across datasets of different characteristics and scales, convincingly demonstrating MODE’s competitiveness. Detailed ablation studies and dynamic analyses further illuminate the internal mechanisms of the framework."}, "weaknesses": {"value": "Major concerns:\n1. In the baseline comparisons MODE’s accuracy is slightly lower than CREST, although MODE offers interpretable selection strategies. However, the interpretability argument currently stops at “what is selected” rather than “why that matters.” The paper shows (Figures 2 and 8) that early-stage selection emphasizes class balance and diversity while later stages emphasize uncertainty and boundary samples. While this is a useful observation, the manuscript does not validate whether this interpretability yields practical benefits—for example, can these observations help practitioners diagnose training problems, or guide future dataset collection? Without such evidence, using interpretability to compensate for the performance gap versus CREST is unconvincing.\n\n2. There is a mismatch between the theoretical guarantees and the practical algorithm. Theorem 1 argues that MODE attains a near-optimal approximation ratio (1 − 1/e) under the assumption that scoring rules used for each selection step are fixed. However, Algorithm 1 shows that the scoring rules are temporarily modified during each selection based on exploratory results—i.e., the scoring rule is dynamic rather than fixed. Because the theoretical guarantee depends on a fixed scoring rule, it does not actually apply to the implemented, dynamically adapting algorithm. The authors should reconcile this gap and clarify whether (and under what conditions) the theoretical bound holds for the practical algorithm.\n\nMinor revisions:\n1. In Section F.3 (MODEL ARCHITECTURES), the description of MobileNetV3-Small appears twice.\n2. In Section F.3 (MODEL ARCHITECTURES), the citation for ResNet-18 is currently a placeholder and should be replaced with the correct reference."}, "questions": {"value": "1. How do the authors justify the near-optimal approximation bound given that the actual algorithm dynamically adjusts its scoring rules? Under what specific conditions does the theoretical guarantee remain valid for the implemented version of MODE?\n\n2. If the theoretical bound does not directly apply to the adaptive setting, could the authors outline a potential extension or empirical validation to bridge this theoretical–practical gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RgmMTyOzbo", "forum": "3m3E1TEjtb", "replyto": "3m3E1TEjtb", "signatures": ["ICLR.cc/2026/Conference/Submission12768/Reviewer_A2Wh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12768/Reviewer_A2Wh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803458950, "cdate": 1761803458950, "tmdate": 1762923579616, "mdate": 1762923579616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MODE is an adaptive core set selection framework that optimizes sample selection at different training stages by dynamically adjusting the weights of four sample selection strategies. This method has verified its effectiveness on multiple image classification datasets, especially outperforming traditional static methods in low-budget (10%-30%) scenarios. The paper also provides theoretical guarantees (such as approximation ratio and weight convergence) and detailed experimental analysis."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper conducts detailed theoretical derivations to elaborate on the proposed method.\n\n2. The paper provides code for verifying the experimental results, and this practice is commendable."}, "weaknesses": {"value": "1. The color in Figure 1 is so dull and unattractive.\n\n2. Figure 2 is very small and unattractive.\n\n3. The organizational structure of the thesis is very poor.\n\n4. The latest baseline is CREST and it is from 2023. Surprisingly, its performance is even worse than that of the baseline from two years ago."}, "questions": {"value": "1. What proportion of the paper is generated by large language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDjXbN2iPF", "forum": "3m3E1TEjtb", "replyto": "3m3E1TEjtb", "signatures": ["ICLR.cc/2026/Conference/Submission12768/Reviewer_Abdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12768/Reviewer_Abdq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897600599, "cdate": 1761897600599, "tmdate": 1762923579352, "mdate": 1762923579352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}