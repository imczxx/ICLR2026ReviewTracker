{"id": "032sg6mGp9", "number": 18276, "cdate": 1758285923748, "mdate": 1759897114753, "content": {"title": "Identifiability in Noisy Label Learning: A Multinomial Mixture Modelling Approach", "abstract": "Learning from noisy labels (LNL) is crucial in deep learning, in which one of the approaches is to identify clean-label samples from poorly-annotated datasets. Such an identification is challenging because the conventional LNL problem, which assumes only one noisy label per instance, is non-identifiable, i.e., clean labels cannot be estimated theoretically without additional heuristics. This paper presents a novel data-driven approach that addresses this issue without requiring any heuristics about clean samples. We discover that the LNL problem becomes identifiable if there are at least $2C - 1$ i.i.d. noisy labels per instance, where $C$ is the number of classes. Our finding relies on the assumption of i.i.d. noisy labels and multinomial mixture modelling, making it easier to interpret than previous studies that require full-rank noisy-label transition matrices. To fulfil this condition without additional manual annotations, we propose a method that automatically generates additional i.i.d. noisy labels through nearest neighbours. These noisy labels are then used in the Expectation-Maximisation algorithm to infer clean labels. Our method demonstrably estimates clean labels accurately across various label noise benchmarks, including synthetic, web-controlled, and real-world datasets. Furthermore, the model trained with our method performs competitively with many state-of-the-art methods.", "tldr": "", "keywords": ["label noise learning", "expectation-maximisation", "mixture models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39e718f6250a4d1ffcf2cdc9270d45e29131db80.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a multinomial mixture modelling approach to address the identifiability problem in learning from noisy labels (LNL). The authors theoretically prove that LNL becomes identifiable when each sample has at least 2C−1 independent noisy labels, enabling the unique recovery of clean label distributions without relying on heuristic assumptions. To make this feasible in practice, they propose generating additional pseudo noisy labels from nearest neighbours and applying an Expectation–Maximization algorithm to infer clean labels. Extensive experiments on synthetic, web-controlled, and real-world noisy datasets demonstrate that the proposed method accurately estimates clean labels and achieves performance competitive with state-of-the-art LNL techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed theorem on the identifiability condition is interesting and theoretically sound.\n2. The experimental results demonstrate the effectiveness of the proposed approach in improving model robustness under noisy settings."}, "weaknesses": {"value": "1. To be honest, I do not catch up the meaning of the task setting. The paper assumes that each sample is associated with multiple noisy labels, but in practical training datasets, it is rare for a single sample to have multiple annotations. This seems inconsistent with the practicality the authors aim to address.\n2. Requiring 2C−1 additional noisy labels per sample would result in a prohibitively large workload.\n3. It is not clear whether there are additional constraints for the identifiability condition—such as the i.i.d. assumption for the noisy samples. For instance, what happens if the 2C−1 noisy labels all belong to the same category?\n4. The experimental setup lacks sufficient detail. I am a bit confused about what the accuracy in Tables 3 and 4 represents—does it refer to the accuracy of clean label identification, or to the image classification accuracy of the model trained on the selected clean samples? If it is the former, what about the recall metric? If it is the latter, what is the clean identification accuracy?\n5. In Figure 1, only classification accuracy is reported. Since classification accuracy can be influenced by various factors (e.g., data scaling effects), directly showing the curve of clean-label identification accuracy would more clearly demonstrate the proposed identifiability condition."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ur3yGYd6qM", "forum": "032sg6mGp9", "replyto": "032sg6mGp9", "signatures": ["ICLR.cc/2026/Conference/Submission18276/Reviewer_apZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18276/Reviewer_apZK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890716926, "cdate": 1761890716926, "tmdate": 1762927998758, "mdate": 1762927998758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the long-standing issue of identifiability in learning from noisy labels (LNL). The authors show that, under a multinomial mixture modeling approach, the LNL problem becomes identifiable if at least $2C-1$ independent and identically distributed (i.i.d.) noisy labels are available per instance (where $C$ is the number of classes). As manually acquiring such redundancy is impractical, the paper proposes estimating additional noisy labels via nearest-neighbour augmentation in feature space. Then the paper use an Expectation-Maximisation (EM) algorithm to estimate the clean label distributions. This algorithm works on the mixture model. The experiments show strong results on both synthetic and real-world datasets. This paper also ran many ablation studies. These studies back up our theoretical ideas and design decisions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Theoretical Contribution:** The paper provides a precise, transparent identifiability condition for the LNL problem via multinomial mixture models, generalizing and clarifying prior results by relating the required number of noisy labels per sample to the number of classes in the task (Section 3.1, Table 1).\n- **Methodological Novelty:** This paper proposes a practical, systematic way to generate the required number of noisy label replicates using feature-space nearest neighbours—trading off annotation quality for quantity, which is substantiated both theoretically and empirically .\n- **Empirical Validation:** The paper shows strong results on several datasets, including CIFAR-10N, CNWL, mini-WebVision, and Animal-10N. It also tests the method under different kinds of label noise. These experiments prove the approach is reliable. Even when compared to current top methods, it holds up well.\n- **Clarity and Reproducibility:** The paper gives full math details, especially in Appendix B. It clearly explains why each modeling choice was made. It also talks openly about the tradeoffs in the design. All the practical details are included.\n- **Figure Engagement:** The ablations in **Figure 1** sharply illustrate the phase transition as the number of noisy labels or neighbours increases, validating both theoretical claims (left subfigure: test accuracy saturates at $N = 2C-1$) and practical effect of relabeling (middle: % clean after EM)."}, "weaknesses": {"value": "1. **i.i.d. Sampling Validity and Empirical Support:** Although the paper claims that additional noisy labels from the approximation $\\tilde{P}(\\hat{Y}|X)$ are i.i.d. by construction (Section 3.2.1), this rests on the functional form of the mixture, and the empirical validation is limited. While Table 5 indicates a high degree of label consistency among neighbours on CIFAR-10N, it does not quantify the residual dependencies or their downstream effect on identifiability for more structured or heavily corrupted labels.\n2. **Computational Complexity for Large C:** The method’s sampling and EM steps incur $\\mathcal{O}(C^2(K+1))$ complexity per sample (Table 2, Appx E), which becomes prohibitive as $C$ grows (e.g., ImageNet-scale tasks). The practical efficacy of the C0-sparsification trick (Appendix F) is not deeply analyzed or benchmarked. The bottlenecks, highlighted in the right subfigure of **Figure 1**, may limit scalability and have not been fully benchmarked against distributed or alternative data-driven LNL schemes.\n4. **Potential Overfitting to Benchmark Settings:** The method depends heavily on pretrained feature extractors and heavy preprocessing—such as SimCLR or DINO—in many experiments. This might unintentionally leak information or lead to biased results. The paper tries to reduce this risk by using co-teaching (Section 3.2.2). However, it’s not clear how much the good performance comes from strong self-supervised features. It’s also uncertain whether the approach would work as well in areas where those kinds of features aren’t as helpful.\n5. **Algorithmic Details and Practical Usability:** The nearest-neighbour scheme, use of LLC, and the stochasticity of the sampling routine, while justified, would benefit from more explicit sensitivity analyses and implementation guidance. Algorithm 1, as presented, leaves choices of $K$, $L$, and ensemble size to heuristics; there is limited discussion on tuning strategies for practitioners."}, "questions": {"value": "1. **On i.i.d. Label Assumption:** Can the authors provide more rigorous empirical or theoretical support for the i.i.d. nature of generated noisy labels via nearest neighbours—especially in high-dimensional, structured data? Specifically, have they measured mutual information or entropy between neighbour-based samples, and how does this affect identifiability guarantees?\n2. **Scalability:** How does the proposed method scale as $C$ increases? Could the authors provide more benchmarks (or at least detailed running time/memory analysis) for substantially larger $C$ (e.g., full ImageNet), and is the C0-sparsification sufficient for realistic streaming or web-scale settings?\n3. **Non-Uniformity/Imbalance:** How does the method handle class imbalance or structured non-uniformity in annotator error rates? Is the mixture modeling robust under such misspecification? Are there modifications/extensions that could strengthen performance in such cases?\n4. **Sensitivity to Hyperparameters:** Could the authors detail or provide ablations on the influence of $K$ (number of neighbours) and $L$ (number of pseudo-label samples) in more challenging benchmarks? Are there principled ways to set or automatically tune these hyperparameters (see Section 4.3 and Algorithm 1)?\n5. **Algorithmic Reproducibility:** Is there publicly available implementation for the entire EM and sampling routine (including feature extraction and pre-processing), and are there resources to support scaling to multi-GPU/distributed environments?\n6. **Extensions to Partial Labeling and Non-i.i.d. Annotators:** The Missing Related Works section includes several approaches to partial/noisy labelers. Could the proposed approach adapt to scenarios with partial, subjective, or correlated annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AVKWRBfTig", "forum": "032sg6mGp9", "replyto": "032sg6mGp9", "signatures": ["ICLR.cc/2026/Conference/Submission18276/Reviewer_VsSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18276/Reviewer_VsSa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896387034, "cdate": 1761896387034, "tmdate": 1762927998332, "mdate": 1762927998332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the fundamental issue of identifiability in learning with noisy labels (LNL).\nThe authors demonstrate that when each sample is annotated with at least 2C−1 i.i.d. noisy labels (where C is the number of classes), the true clean-label distribution becomes identifiable under a multinomial mixture model.\nSince collecting that many labels per sample is infeasible in practice, the authors propose a practical algorithm that approximates i.i.d. noisy labels using KNN and LLC, followed by an EM procedure to recover clean posterior estimates.\nExtensive experiments on multiple benchmarks show that this surrogate approach is both theoretically justified and empirically effective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and intuitive idea.\n\nThe identifiability-based perspective is conceptually novel in the LNL literature. The proposed KNN-based generation of “pseudo i.i.d. noisy labels” is both intuitive and innovative, providing a clear interpretation and offering potential for many future extensions.\n\n2. Solid empirical results.\n\nThe method achieves competitive and stable performance across real-world noisy datasets. Despite its theoretical nature, the approach remains practical and delivers comparable results to state-of-the-art robust learning methods."}, "weaknesses": {"value": "1. Practical labeling feasibility.\n\nIn practice, collecting 2C−1 i.i.d. labels per instance is unrealistic, so the framework relies entirely on pseudo-label generation. My main concern is the algorithm’s dependency on the quality of synthetic labels: when the neighborhood structure is unreliable or features are biased, the identifiability assumption may not hold.\n\n(Optional, minor) The approach might also be computationally demanding for large C, as both the KNN and EM steps scale quadratically with the number of classes."}, "questions": {"value": "1. In extremely noisy scenarios, generating high-quality pseudo i.i.d. labels via KNN may be difficult.\nDid the authors analyze the effect of noise level on KNN reliability?\nFor example, is there an empirical study (similar to Table 5) showing how varying noise levels impact the accuracy or consistency of KNN-based pseudo-label construction (I mean various noise levels and various ks)?\n\n2. Have the authors considered using feature denoising or representation refinement (e.g., pre-trained or self-supervised embeddings) to mitigate the dependence on feature quality during pseudo-label generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q8Lv3taOc5", "forum": "032sg6mGp9", "replyto": "032sg6mGp9", "signatures": ["ICLR.cc/2026/Conference/Submission18276/Reviewer_vNTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18276/Reviewer_vNTX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905886496, "cdate": 1761905886496, "tmdate": 1762927997896, "mdate": 1762927997896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the foundational identifiability problem in learning from noisy labels (LNL). It establishes that the standard single-label LNL setting is non-identifiable in theory, meaning the clean label distribution cannot be recovered without additional assumptions. The key contribution is proving that if each instance has at least 2C−1 i.i.d. noisy labels (where C is the number of classes), then clean labels are identifiable when modeling noisy labels as a multinomial mixture. Extensive experiments on synthetic and real-world noisy-label benchmarks support the theoretical claims, showing competitive or improved performance relative to state-of-the-art baselines such as DivideMix, HOC, and others."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper provides a clear identifiability condition for LNL: at least 2C−1 i.i.d. noisy labels needed.\n- This paper offers a clean alternative to full-rank transition matrix assumptions used in prior work.\n- Nearest-neighbor-based generation of pseudo noisy labels is elegant and interpretable.\n-This paper includes complexity analysis and ablation experiments clarifying where the cost arises (sampling and EM)."}, "weaknesses": {"value": "- This paper relies on good features for nearest-neighbor graph; co-teaching partially mitigates but tight coupling remains.\n- This paper argues generated labels are i.i.d. when sampled from estimated distributions, but correlation in neighbor graph may violate strict independence.\n- Behavior in class-dependent noise not explicitly reported."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T4t5H3p3uv", "forum": "032sg6mGp9", "replyto": "032sg6mGp9", "signatures": ["ICLR.cc/2026/Conference/Submission18276/Reviewer_84f6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18276/Reviewer_84f6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18276/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010661291, "cdate": 1762010661291, "tmdate": 1762927997562, "mdate": 1762927997562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}