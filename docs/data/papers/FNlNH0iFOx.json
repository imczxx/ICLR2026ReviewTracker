{"id": "FNlNH0iFOx", "number": 14477, "cdate": 1758236826831, "mdate": 1759897367910, "content": {"title": "R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning", "abstract": "Practical guidance on training Large Language Models (LLMs) to leverage Code Interpreter across diverse tasks remains lacking. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. Unlike prior RL + tool-use efforts focused on narrow domains such as math or retrieval, we curate 144 diverse reasoning and planning tasks and show that training a general-purpose Code Interpreter across them presents significant challenges due to task heterogeneity and scarcity of effective samples. To address this, we introduce a multi-stage curriculum learning approach that partitions training samples by measured improvement potential. The RL training prioritizes samples with higher potential and gradually shifts to lower-potential ones, increasing the average RL gains from merely +3.4\\% to +9.3\\% across Qwen-2.5 models (3/7/14B). Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.1\\% to 72.4\\%, outperforming text-only GPT-4o (58.6\\%) and GPT-4o with Code Interpreter (70.9\\%). Notably, R1-CI-14B also exhibits emergent self-checking behavior through code generation.", "tldr": "We present R1-Code-Interpreter, an LLM trained with supervised and multi-stage RL to integrate code execution with reasoning, achieving strong performance across 144 diverse tasks and outperforming GPT-4o with Code Interpreter.", "keywords": ["Large Language Models", "Code Interpreter", "Reinforcement Learning", "Curriculum Learning", "Symbolic Reasoning", "Tool Use"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd4c5ebbe88c2f5ea3b9b150bee7296016b48605.pdf", "supplementary_material": "/attachment/c1ddb8861441327230a6b84039d78dcdf1bcb044.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces R1-Code-Interpreter, a novel framework for training Large Language Models (LLMs) to autonomously leverage a code interpreter across a diverse set of reasoning and planning tasks. The authors first identify a critical challenge: standard reinforcement learning (RL) methods like GRPO fail to yield significant improvements when applied to a heterogeneous set of tasks due to task heterogeneity and the scarcity of effective samples.\nTo address this, the core contribution of the paper is a multi-stage curriculum learning (CL) approach guided by \"Improvement Potential.\" This method first uses an \"Agent group\" to estimate the empirical correctness rate $p_i$ for each training sample, then calculates its \"Improvement Potential\" $\\Pi_i = 4 p_i(1-p_i)$. This metric is maximized when $p_i \\approx 0.5$, i.e., when the model is \"on the fence\" about the sample. The authors theoretically justify this by linking it to the variance of the GRPO gradient, $p(1-p)$, where samples with $p_i \\approx 0.5$ provide the strongest learning signal. The RL training curriculum proceeds in stages, starting with high-potential samples and gradually incorporating lower-potential ones.\nAdditionally, the paper presents a \"Code Execution Sandbox\" that decouples code execution from GPU gradient computation, significantly improving training efficiency (39% reduction in training time). Experimental results show that their R1-CI-14B model (based on Qwen-2.5-14B) achieves a 72.4% average accuracy on 37 test tasks, outperforming GPT-4o (58.6%) and GPT-4o with Code Interpreter (70.9%). The paper also reports an \"emergent self-checking behavior,\" where the model learns to generate code to verify its previous reasoning steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical, replicable CI protocol: simple python code blocks, a clear final-answer marker, and tight caps on tool use (e.g., max code calls and per-call timeout) that help reproducibility.\n2. Tangible engineering win: decoupling code execution into a CPU sandbox reduces RL training wall-clock and avoids GPU stalls.\n3. Within-scope performance & diagnostics: decent gains on their own benchmark plus ablations and behavior analysis (e.g., code-based self-checking, typical call counts) that clarify where improvements come from."}, "weaknesses": {"value": "1. Improvement Potential and curriculum learning: This looks novel at first glance, but in RL it is increasingly standard to focus on data where the model neither gets everything right nor everything wrong as the main RL signal. The paper largely describes this under a Bernoulli correctness assumption (Pi = 4p(1 - p)), which—at least to me—does not amount to a significant new idea or contribution beyond formalizing that intuition.\n2. Limited evaluation breadth; same-source testing; augmented SFT without ablations: The paper evaluates on targeted splits from the same source suites and uses data augmentation for SFT. For example: “To generate SFT supervision, we prompt GPT-4o to produce multiple reasoning/execution trajectories per task and retain only those yielding correct answers. To enhance diversity and adaptability, we use varied prompt formats: some allow free-form reasoning such as the prompt in Table 1, while others enforce transitions between text and code.” However, there is no experiment isolating how much this augmentation/design choice contributes. As far as I know, data quality is a very important factor in LLM training, so the lack of targeted ablations weakens the evidence.\n3. Missing comparisons to other open-source SOTA models at similar scale: The paper does not include head-to-head comparisons with strong contemporary open-source peers of comparable size (e.g., Qwen3-8B, etc.)."}, "questions": {"value": "1. Training uses 107 tasks and testing 37 tasks from the same three suites. Can you provide leave-one-benchmark results (train on two suites, test on the held-out suite) and/or results on entirely new, unseen tasks to demonstrate out-of-distribution generalization beyond the same-source split?\n2. Your SFT is generated by GPT-4o with multiple trajectories per task, retaining only correct ones and varying prompt formats (including enforced text↔code transitions). How much does each design choice contribute? Please include ablations toggling (i) keep-only-correct vs. keep-all, (ii) prompt-format diversity on/off, and (iii) any multi-turn emphasis, and report the impact on accuracy, variance, and training stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WwwR8jCGfl", "forum": "FNlNH0iFOx", "replyto": "FNlNH0iFOx", "signatures": ["ICLR.cc/2026/Conference/Submission14477/Reviewer_HYQj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14477/Reviewer_HYQj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795719339, "cdate": 1761795719339, "tmdate": 1762924876762, "mdate": 1762924876762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce R1-Code-Interpreter, training a single LLM to orchestrate natural-language reasoning with Python execution across 144 heterogeneous task families. The training pipeline couples 6.5k multi-turn SFT traces with GRPO-based RL, and its key ingredient is an improvement-potential curriculum that prioritizes items the model currently solves ~50% of the time, maximizing learning signal. Code is executed in a CPU sandbox (with an 8-call cap) to keep GPUs saturated while enabling multi-turn tool use. On 37 held-out tasks, the 14B model improves from 44.1% to 72.4%, narrowly surpassing GPT-4o + Code Interpreter. The authors conduct ablations, showing that vanilla RL plateaus on batches that are either too easy or too hard, while potential-guided scheduling sustains informative variance. The authors also characterize code-call counts, response lengths and report emergent self-checking where the model writes code to verify its own outputs. The authors provide this as evidence for a multi-stage, code interpreter-centric training paradigm in LLM reasoning research."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Comprehensiveness:**\nThe authors provide a broad, carefully controlled experimental program, spanning diverse task families, staged training with curriculum learning, warm-start ablation, strong baselines, and behavioural system measurements (emergent self-checking, code-usage, verbosity), that provides compelling evidence of the paper’s general-purpose claims.\n\n**Unlocking Code Interpreter Potential:**\nThis work offers a concrete, scalable recipe that elevates code execution from a math-only crutch to a general-purpose reasoning tool, yielding sizable gains across heterogeneous tasks and setting a practical foundation for the research community.\n\n**Technical & Artifact Contribution:**\nThe authors commit to open-sourcing the code, model checkpoints, and datasets, and they document clearly the 144-task suite and the SFT/GRPO dataset-synthesis pipeline, materially strengthening reproducibility and the research community further studies."}, "weaknesses": {"value": "**Single Scope Language:**\nThe paper aims for a general code interpreter for code generation and reasoning across tasks and domains, but trains and evaluates only with a Python executor; transfer to other languages/runtimes is untested. Identifying details on different languages would materially strengthen the general code-generation claim."}, "questions": {"value": "Your reward seems largely binary (correct/incorrect) with small format/turn terms. Did you fine-graded or curriculum-aware reward shaping (unit-test pass fraction, etc)? \n\nHow sensitive are learning curves and final accuracy to reward design, and did you observe reward-hacking (optimizing for format/turns over task progress)?\n\nSmall Comments:\n- Paper mentioned (15 agents), but I'm unsure? Did the authors mean N=20?\n- Typo: polynimial equation -> polynomial"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XGPowdRB6l", "forum": "FNlNH0iFOx", "replyto": "FNlNH0iFOx", "signatures": ["ICLR.cc/2026/Conference/Submission14477/Reviewer_Eyn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14477/Reviewer_Eyn2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881029978, "cdate": 1761881029978, "tmdate": 1762924875881, "mdate": 1762924875881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces R1-Code-Interpreter, a framework to train text-only Large Language Models (LLMs) to effectively utilize a Code Interpreter by using multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL). Addressing the challenges of task heterogeneity and a scarcity of effective samples when training across 144 diverse reasoning tasks, the authors propose a novel multi-stage curriculum learning approach. This curriculum prioritizes training samples based on their measured \"improvement potential\" (IP)—focusing on samples where the model's success is mixed rather than trivially easy or excessively difficult—which boosts RL gains from +3.4% to +9.3%. The final model, R1-CI-14B, significantly improves average accuracy on test tasks to 72.4%, outperforming both text-only GPT-4o (58.6%) and GPT-4o with its Code Interpreter (70.9%), while also exhibiting an emergent self-checking behavior through code generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper extends the task of combining symbolic code generation with text with reasoning to broader benchmarks, providing a thorough investigation of the generalizability of this paradigm.\n\n2. This paper demonstrates innovation by proposing a curriculum learning method based on Improvement Potential, successfully extending TIR training from single-task settings to multi-task scenarios.\n\n3. Compared to traditional curriculum learning approaches, the proposed method is designed based on improvement potential, enabling effective adaptation to TIR tasks.\n\n4. Comprehensive evaluation across multiple benchmarks demonstrates the effectiveness of the proposed training method."}, "weaknesses": {"value": "1. The improvement potential score defined in this paper is modeled as a function symmetric about p=½. In this case, even if two samples have the same improvement potential score, their empirical correctness rates may differ significantly. Therefore, in curriculum learning, simply incorporating samples with low improvement potential scores may overlook the training contribution differences brought by samples with different empirical correctness rates. For example, training samples with low empirical correctness rates focuses on enhancing the model's ability to handle complex problems, while training samples with high empirical correctness rates focuses on improving the model's ability to handle simple problems.\n\n2. Curriculum learning is the core contribution of this paper, yet the paper lacks detailed ablation studies on curriculum learning rounds, only showing a single RL validation curve.\n\n3. The paper lacks exploration of the method's adaptability to other algorithms beyond GRPO, such as Reinforce++, ARPO, CISPO, etc.\nIn Figure 3(b), the distinguishability between different curves is poor. I suggest trying more discriminative visualization methods (e.g., using logarithmic scale).\n\n4. In curriculum learning, the model used to compute the improvement potential score of samples is the initial SFT model, while the model being trained is not the SFT model. Therefore, this curriculum learning method is an offline reinforcement learning approach, and I believe its training effectiveness may differ from online reinforcement learning methods."}, "questions": {"value": "See weakness and follow questions：\n\n1. Why is it assumed that sample rewards follow a Bernoulli distribution? Given the diversity of tasks and the complexity of model parameters, the reward distribution of samples may be complex and difficult to represent explicitly. I am uncertain whether modeling it as a Bernoulli distribution is justified.\n\n2. What is the criterion for incorporating new lower-potential groups at each stage of curriculum learning? Specifically, at each stage, what improvement potential score threshold determines which samples are included? What is the data volume at each stage? This aspect is not clearly explained in the paper.\n\nWhile the method demonstrates good generalizability and methodological soundness, and I believe it would be effective, I have concerns regarding the experimental rigor and the support for some claims. If these concerns are adequately addressed, I would consider raising my score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bHeVeigu3q", "forum": "FNlNH0iFOx", "replyto": "FNlNH0iFOx", "signatures": ["ICLR.cc/2026/Conference/Submission14477/Reviewer_Rp2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14477/Reviewer_Rp2m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762659014759, "cdate": 1762659014759, "tmdate": 1762924875307, "mdate": 1762924875307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}