{"id": "xF0Dcmvsl0", "number": 8834, "cdate": 1758099577208, "mdate": 1759897761011, "content": {"title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs", "abstract": "Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models.", "tldr": "We introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly predict Visual Reference Token for multi-modal understanding and reasoning.", "keywords": ["Multimodal Large Language Model", "Referring Expression Comprehension", "Visual Reference Token"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7752697e003fe8dcafb2ffa27b23747a6392a699.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Patch-as-Decodable Token (PaDT), a unified framework that enables MLLMs to generate both textual and visual outputs. It proposes Visual Reference Tokens, which are learnable tokens derived from image patch embeddings, and they are treated as decodable elements within the LLM’s output sequence. The proposed architecture and training strategy consistently achieve state-of-the-art performance across benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed framework effectively unifies diverse visual tasks such as detection, segmentation, grounding, and captioning under a single paradigm. The introduced VRT module aligns more naturally with both textual and visual semantics than coordinate-based outputs, reducing inconsistencies and hallucinations. Overall, the approach demonstrates consistent performance improvements over existing models."}, "weaknesses": {"value": "1. The multimodal codebook needs to be adjusted at each forward pass, which may complicate deployment and increase inference latency.\n2. The decoder still relies on task-specific components (e.g., bounding box, mask, and score tokens), limiting the framework’s full generality.\n3. The evaluation primarily focuses on single-image tasks; it would be valuable to assess the model’s generalization to video or multi-image scenarios."}, "questions": {"value": "1. Table 4 shows that PaDT Pro underperforms PaDT on the referring image captioning task. Could the authors elaborate on the possible reasons for this performance gap?\n2. How well does the dynamic vocabulary expansion scale when processing high-resolution images with thousands of patches?\n3. What is the memory and computational overhead associated with dynamically updating the embedding table at each forward pass?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J7RYc0BoRM", "forum": "xF0Dcmvsl0", "replyto": "xF0Dcmvsl0", "signatures": ["ICLR.cc/2026/Conference/Submission8834/Reviewer_XCqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8834/Reviewer_XCqy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629475960, "cdate": 1761629475960, "tmdate": 1762920604599, "mdate": 1762920604599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Patch-as-Decodable Token (PaDT), a unified paradigm for multimodal large language models (MLLMs) that enables direct generation of both textual and diverse visual outputs. The core innovation is the Visual Reference Token (VRT), derived from image patch embeddings and interleaved with LLM textual tokens. A lightweight decoder transforms these outputs into detection, segmentation, and grounding predictions. The method claims improved localization, semantic alignment, and flexibility over prior approaches that serialize visual outputs as text (e.g., bounding box coordinates). Extensive experiments on detection, segmentation, grounding, and captioning tasks demonstrate state-of-the-art performance, even compared to much larger generalist models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed way to bridge textual and visual outputs, allowing MLLMs to generate structured predictions (e.g., bounding boxes, masks) in a unified format. Similar idea was used in Ma et al. 2025, but there is some difference in codebook design. However, I am not sure whether the proposed method is limited to fixed input visual token length, if yes, it will be a limitation compared to Ma et al. 2025.\n2) PaDT achieves strong results across multiple tasks and datasets, outperforming larger models (e.g., InternVL3 78B) in detection and segmentation benchmarks. Extensive ablation analysis is also provided."}, "weaknesses": {"value": "1) The paper does not sufficiently address how PaDT handles flexible visual token lengths or variable image resolutions. While adaptive tiling and dynamic codebook expansion are mentioned, it remains unclear whether the method can robustly generalize to images of arbitrary sizes or aspect ratios, especially in real-world scenarios.\n2) The comparison with generalist models like QWen and InternVL3 may not be entirely fair. These baselines are designed for broad multimodal tasks, whereas PaDT is specifically trained for detection/segmentation. The paper should clarify whether baselines were fine-tuned for the same tasks or if the comparison is strictly zero-shot. This distinction is crucial for interpreting the reported gains.\n3) What is the computational overhead of dynamic codebook expansion and VRT decoding compared to standard coordinate-based approaches? Is inference time affected for high-resolution images?"}, "questions": {"value": "Refer to the above part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5Y0X7mdaTi", "forum": "xF0Dcmvsl0", "replyto": "xF0Dcmvsl0", "signatures": ["ICLR.cc/2026/Conference/Submission8834/Reviewer_Cmsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8834/Reviewer_Cmsv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018013083, "cdate": 1762018013083, "tmdate": 1762920604026, "mdate": 1762920604026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a core limitation in current Multimodal Large Language Models (MLLMs): their inability to produce fine-grained visual outputs like bounding boxes or segmentation masks directly. Existing methods typically force the model to generate coordinates as a string of text (e.g., [x1, y1, x2, y2]), which is inefficient, prone to formatting errors, and creates a semantic gap between the visual features and the textual output.\nTo solve this, the authors introduce Patch-as-Decodable Token (PaDT), a new paradigm that allows an MLLM to directly output visual tokens that act as pointers to image patches. The key contributions are:\n1. Visual Reference Tokens (VRTs): The core idea is to create special tokens derived from the visual patch embeddings of an input image. These VRTs are dynamically added to the LLM's vocabulary for each specific image.\n2. Unified Input-Output: This dynamic vocabulary allows the LLM to seamlessly interleave text and VRTs in its output. For example, to describe a cat, instead of generating coordinates, the model might generate: \"Here is the cat <VRT_123><VRT_124><VRT_125>\".\n3. Lightweight Decoder: A simple, lightweight decoder then takes these few predicted VRTs and translates them into structured visual outputs like bounding boxes or segmentation masks.\n4. SoTA Performance: Through a robust training strategy, the authors show that PaDT achieves outstanding results across a range of tasks, including referring expression comprehension, segmentation, and open-vocabulary detection. Notably, their 3B parameter model significantly outperforms much larger models (e.g., 78B InternVL3) that rely on text-based coordinate generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Elegant Paradigm: The core idea of dynamically creating and predicting patch-level visual tokens (VRTs) is a significant conceptual advance over generating text-based coordinates. It creates a more natural and semantically coherent link between the model's reasoning and the visual content.\n2. Empirical Performance: The results are a standout feature of this paper. The PaDT models not only set a new state-of-the-art on multiple challenging benchmarks but do so with remarkable efficiency. The fact that their 3B model outperforms an 78B competitor (Table 1) is a powerful testament to the superiority of their approach. The near-doubling of performance on the COCO open-vocabulary detection task (Table 3) is particularly good.\n3. Unified and Flexible Framework: The PaDT method is not a one-trick pony. It provides a single, unified mechanism for handling diverse visual outputs, including bounding boxes and segmentation masks, across different tasks like referring comprehension and open-vocabulary detection. The lightweight decoder design adds to this flexibility.\n4. Clarity and High-Quality Presentation: The authors do a great job of explaining a complex idea in a simple and intuitive way. The paper is well-organized, and the figures are highly effective at conveying the core concepts and results."}, "weaknesses": {"value": "Overall, I think the idea is good. I still have some questions regarding the method. \n\n1. Scalability to High-Resolution Images: The number of VRTs in the dynamic vocabulary is directly tied to the number of input image patches. The paper evaluates on standard resolutions, but it's unclear how the method's computational cost (especially memory for the dynamic vocabulary and classifier weights) would scale to very high-resolution images that are divided into thousands of patches. A discussion on this limitation would be welcome. \n2. The random sampling of 5 VRTs per object for training is shown to be very effective. What is the intuition behind this random strategy? Have you experimented with more deterministic or \"intelligent\" sampling methods, for example, using attention maps to select the most salient patches or explicitly selecting patches near object boundaries? \n3. The performance on open-vocabulary detection is particularly strong. Do you have a hypothesis for why the VRT approach is so much more effective than text-based coordinates for this specific task? \n4. To what extent is the significant performance improvement attributable to simply providing the lightweight decoder with a richer, multi-token representation of the visual target? In other words, is the primary benefit that VRTs provide more detailed visual information to the final prediction head, which prior methods lack, rather than a fundamental improvement in the LLM's spatial reasoning itself?\n5. It is better not to only focus on the fine-grained benchmarks, but also try the method on some global semantic tasks, e.g., MMBench, MMVet, to make sure the introduced modules will not hurt on the other important tasks we mostly care about, rather than the specific domains. This makes the whole method more general."}, "questions": {"value": "The proposed method is well-designed and directly targets the clearly articulated limitations of prior work. Please also refer to the weaknesses and hopefully the authors could address them. The paper is technically good. I would like to raise the scores if the authors can address my questions properly. \n\nBesides, some other citations are important, but the authors might be missing: \n\n1. Zhang, Haotian. \"Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models.\" arXiv preprint arXiv:2404.07973 3 (2024): 21. \n2. Lian, Long, et al. \"Describe anything: Detailed localized image and video captioning.\" arXiv preprint arXiv:2504.16072 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yNOX4FYWPK", "forum": "xF0Dcmvsl0", "replyto": "xF0Dcmvsl0", "signatures": ["ICLR.cc/2026/Conference/Submission8834/Reviewer_npQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8834/Reviewer_npQb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8834/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144295093, "cdate": 1762144295093, "tmdate": 1762920603663, "mdate": 1762920603663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}