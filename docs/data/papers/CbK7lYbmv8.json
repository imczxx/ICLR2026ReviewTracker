{"id": "CbK7lYbmv8", "number": 11059, "cdate": 1758188359473, "mdate": 1759897611759, "content": {"title": "Rethinking LLM Reasoning: From Explicit Trajectories to Latent Representations", "abstract": "Large Language Models (LLMs) have achieved impressive performance on complex tasks by generating human-like, step-by-step rationales, referred to as \\textit{reasoning trajectory}, before arriving at final answers. However, the length of these reasoning trajectories often far exceeds that of the final answers, which incurs substantial inference costs even for relatively simple tasks. Advanced methods typically attempt to compress reasoning trajectory length through post-training, but they remain decoding-intensive and fail to inherently mitigate the efficiency challenge. In this work, we challenge the necessity of generating full reasoning trajectories and empirically demonstrate that LLMs can generate accurate answers using only fragmental reasoning paths, without relying on complete token-by-token sequences. To this end, we propose a novel \\textbf{Latent Reasoning Tuning (LRT)} framework, which empowers LLMs to perform reasoning using implicit, compact, learnable representations instead of explicit textual trajectories. Technically, LRT replaces the costly autoregressive generation of reasoning steps with a single forward pass through a lightweight reasoning network, which generates latent vectors that encapsulate the necessary reasoning logic and condition the LLM to produce the final answer. Experiments on mathematical and out-of-domain benchmarks demonstrate that our LRT consistently outperforms relevant efficient reasoning methods. Moreover, by transforming explicit reasoning into latent reasoning, our approach surpasses the state-of-the-art Qwen3 hybrid reasoning framework.", "tldr": "", "keywords": ["large language model", "efficient reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/975471c16f507d4f5c5f35583a65b6f045032801.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Latent Reasoning Tuning (LRT), a framework that replaces explicit token-by-token reasoning with compact latent representations generated by an auxiliary network. It aims to improve reasoning efficiency by performing implicit reasoning without generating lengthy step-by-step rationales."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "+ The problem of overthinking and reasoning inefficiency is indeed practical and highly relevant for today’s LLMs, so it’s valuable to see it studied from this new latent-reasoning perspective.\n+ The writing is clear and easy to follow—the method and its intuition are well-explained, making the technical parts digestible even on a first read."}, "weaknesses": {"value": "+ Even though reasoning efficiency is an important problem, modern inference engines (like vLLM or SGLang) already accelerate long-token generation with KV-caching and routing optimizations. The proposed module doesn’t directly integrate with these frameworks, so it might actually reduce efficiency when generating the same number of tokens—this needs to be considered for a fair comparison with baselines.\n+ Using only 512 tokens for distilled-R1 feels too artificial. For genuinely hard reasoning tasks (like AIME-style problems), original responses often exceed 10k tokens, so of course the performance drops drastically. It would make more sense to compare against short-response RL methods (like LC-RL, L1, or even RLVR with the same token cap) for a fairer setup.\n+ The 512-token setup feels too synthetic and not representative of real-world reasoning scenarios. Trying longer contexts—even if not as long as full 32k setups—would strengthen the empirical validity."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pTraAuNRtW", "forum": "CbK7lYbmv8", "replyto": "CbK7lYbmv8", "signatures": ["ICLR.cc/2026/Conference/Submission11059/Reviewer_gaFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11059/Reviewer_gaFX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760654138194, "cdate": 1760654138194, "tmdate": 1762922233522, "mdate": 1762922233522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Latent Reasoning Tuning (LRT), which replaces the explicit, token-by-token generation of reasoning trajectories in LLMs with compact latent representations. The authors show that reasoning LLMs can maintain high accuracy even when conditioned on fragmented reasoning paths, suggesting significant redundancy in explicit reasoning chains. LRT introduces a lightweight \"reasoning network\" G_phi that maps input questions to fixed-length latent trajectories, which then condition the base LLM to generate final answers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The trajectory analysis provides compelling evidence that models are robust to token omission with minimal performance degradation.\nIt replaces O(k) autoregressive steps with a lightweight model with fixed reasoning size, which should provide efficiency gains.\nThe approach is modular, allowing switching between latent and explicit reasoning modes.\nThe results seem to show improvements over \"baseline efficient reasoning\" methods across several benchmarks."}, "weaknesses": {"value": "Architecture of G is underspecified: I think there is no clear description of the reasoning network architecture. The paper mentions it uses \"Qwen3-Embedding-0.6B\" but it's unclear what the actual architecture is. The discussion says the latent reasoner is not trained from scratch, then it means it reuses some LLM parts? This was not clear to me when reading the paper.\n\nMissing efficiency analysis: One of the main motivations for compressing reasoning chains is computational efficiency, but the computational overhead of the latent reasoning is not discussed. No metrics are provided for inference time, FLOPs, or memory usage. Since the approach uses fixed reasoning length and lightweight modules, there should be efficiency gains, but these are not reported or analyzed. It could be a useful addition to the paper.\n\nNo statistical significance testing: All tables lack measures of variability (confidence intervals or standard deviations). This makes it unclear whether and when the improvements are statistically significant. This is important for rigorous science.\n\nConcurrent work: The paper mentions other latent reasoning work (Hao et al., 2024; Saunshi et al., 2025; Wu et al., 2025) in only one sentence of the related work section. It is important to better distinguish this work from other reasoning in latent space approaches. Even though most of them are fairly recent, the authors are aware of them and the discussion dismisses these related work too easily."}, "questions": {"value": "What is the exact architecture of the reasoning network G?\nWhat are the actual efficiency gains at inference time? \nAre the performance improvements statistically significant? Can you provide confidence intervals or significance tests?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5VtnSguGyf", "forum": "CbK7lYbmv8", "replyto": "CbK7lYbmv8", "signatures": ["ICLR.cc/2026/Conference/Submission11059/Reviewer_RA4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11059/Reviewer_RA4D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867776196, "cdate": 1761867776196, "tmdate": 1762922233092, "mdate": 1762922233092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel reasoning framework, latent reasoning tuning (LRT), which uses an auxiliary network to generate a sequence of latent representations in a single forward pass, and then concatenates the prompt and the latent representations to generate the final answer. Experimental results on various benchmarks show its consistent performance gain over baseline methods, showing its effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of using latent representations for efficient reasoning makes sense.\n2. The method is clean and efficient for training.\n3. The performance on different benchmarks compared to baseline methods is good.\n4. It reduces the computational cost of generation compared to explicit CoT."}, "weaknesses": {"value": "1. (minor) In Table 2, it would be beneficial to include the results for the thinking mode, which would make the table more comprehensive and the comparison more straightforward, allowing for a clearer view of the trade-off between computation and accuracy.\n\n2. It would make the paper stronger if the authors could try larger models (such as 7B models) for LRT to show the method is scalable for larger base models. This should be doable (at least for SFT only) since the LRT method only needs to train the reasoning network, which can be smaller than the base model."}, "questions": {"value": "What is $P\\_{ref}$ in line 239 (since $P\\_{\\theta}$ is fixed during training) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XlJMRDGpdT", "forum": "CbK7lYbmv8", "replyto": "CbK7lYbmv8", "signatures": ["ICLR.cc/2026/Conference/Submission11059/Reviewer_dCpV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11059/Reviewer_dCpV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891890150, "cdate": 1761891890150, "tmdate": 1762922232709, "mdate": 1762922232709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Latent Reasoning Tuning framework, a novel approach to enhance the reasoning capabilities of Large Language Models (LLMs) without modifying their parameters. The method involves training a lightweight, external reasoning network (specifically, Qwen3-Embedding-0.6B) to generate a set of latent vectors (256 learned embeddings) that condition the frozen base LLM's output. The framework is trained in two stages: Supervised Fine-Tuning (SFT) on the OpenR1-Math-220k dataset, followed by Reinforcement Learning (RL) fine-tuning on the DeepScaleR-Preview-Dataset.\nThe authors evaluate their method on several math and general reasoning benchmarks, including MATH and GPQA. The results demonstrate that this approach consistently outperforms the base models (tested on 1.7B and 4B parameter models) as well as other efficient reasoning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the critical and practical problem of improving the inference efficiency of LLM reasoning.\n2. A key strength is the framework's efficiency. By only training a small reasoning network and keeping the base LLM's weights frozen, the method is computationally lightweight. This modular design also suggests high flexibility, as the reasoning network could potentially be paired with various pre-trained base models.\n3. The authors provide ablation studies to justify key design choices, such as the necessity of the two-stage training pipeline (SFT followed by RL).\n4. The method achieves consistent improvements over the selected baselines across multiple benchmarks, validating the effectiveness of the proposed latent reasoning approach."}, "weaknesses": {"value": "1. One of the major drawback of the proposed method is the loss of interpretability of the reasoning traces. It would be great to provide some analysis on the learned reasoning vectors.\n2. Although results show consistent improvements on 1.7B and 4B models, it remains unclear how the reasoning network scale with base model sizes. e.g. would a 0.6B reasoning backbone still sufficient for much bigger LLMs? Table 3 shows that the performance degrade when the number of reasoning token increases from 256 to 512, which seems to indicate that there's a sweet spot for the number of tokens depending on the model capacities, but this needs further experiments with various model sizes to verify. \n3. The paper lacks an appendix and fails to provide crucial details about the hyperparameters used for the experiments. This omission significantly hinders the reproducibility of the work.\n4. Although the method avoids the cost of fine-tuning the base LLM, it introduces the training and inference overhead of an additional reasoning model. The paper would benefit from a more direct comparison of the trade-offs (e.g., total training flops, inference latency) against other latent-variable baselines that do involve fine-tuning the base LLM, which would provide a more complete picture of the method's efficiency."}, "questions": {"value": "1. How interpretable are the reasoning vectors? \n2. Could you provide some insights on the scalability of LRT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "21NJh9jVzO", "forum": "CbK7lYbmv8", "replyto": "CbK7lYbmv8", "signatures": ["ICLR.cc/2026/Conference/Submission11059/Reviewer_nNzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11059/Reviewer_nNzk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11059/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962533415, "cdate": 1761962533415, "tmdate": 1762922232401, "mdate": 1762922232401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}