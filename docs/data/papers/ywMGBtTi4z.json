{"id": "ywMGBtTi4z", "number": 23412, "cdate": 1758343434959, "mdate": 1759896816022, "content": {"title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "abstract": "The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics that are not firmly grounded in established theories of creativity. To address this gap, we introduce $\\text{C}^2$-Eval, a holistic benchmark for the unified assessment of creativity in FMs. Specifically, $\\text{C}^2$-Eval distinguishes between two complementary forms of Creativity ($\\text{C}^2$): convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., story telling). It evaluates both dimensions using fine-grained assessments derived from social science theories, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we provide a comprehensive analysis of trade-offs in their creative capabilities. \nOur results highlight the capabilities and challenges of current FMs in pursuing a creative machine mind, showing that $\\text{C}^2$-Eval provides an effective lens for examining the evolving landscape of creative AI.", "tldr": "", "keywords": ["Creativity Evaluation", "Foundation Models", "Convergent and Divergent thinking", "Generative AI"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9738f7df9ec499511b3c6f8165033e52fd2e0133.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission introduces the C2-Eval framework to assess the creative potential of various LLMs. The C2 comes from evaluating models on both \"convergent\" and \"divergent\" tasks. Convergent tasks are ones that can be evaluated for accuracy, whereas divergent are more open-ended (e.g., creative writing).\nFor either convergent and divergent tasks, the work proposes three metrics of Usefulness, Originality and Surprise. For convergent tasks, they're based on similarity between a set of responses, and/or model self-reported confidence. For divergent tasks, the work relies on LLM as a judge to evaluate U, O and S.\n\nThe findings indicate that different models shine at convergent or divergent creativity, with thinking models outperforming non-thinking models. Prompt engineering to encourage creativity helps almost all models on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well written and the motivation of wanting to study creativity potential in LLMs is strong.\n- The experiments are thorough in that they involve a lot of different LLMs, both open- and closed-source, and involve varied tasks."}, "weaknesses": {"value": "- The way the metrics are defined feels a bit ad-hoc, and there's no validation with expert annotation. For example focusing on the convergent tasks: usefulness is not really relating to creativity (just accuracy), originality is based on essentially set-wise diversity (which is somewhat unrelated) and surprise is essentially self-assessed by the LLM itself (though prior work has shown that asking an LLM for confidence often does not work). On the other hand for the divergent tasks, it is simply LLM-as-a-judge. But we would need some validation to trust that this protocol works. Given two samples that are judged to be more/less creative by this protocol, how likely is a human expert to agree?\n- It feels to me that there are simple ways to modify the generation procedure that would lead in terms of these metrics to increased numbers. For instance increasing temperature, or over-generating and filtering similar responses. It is unclear however that this would in fact represent more creativity on behalf of the model, which shows the potential limitations of the metric.\n- Regarding the use of LLM-as-a-judge: since we are evaluating whether LLMs can be creative, it feels like asking an LLM to do the judgement of whether something is creative assumes that the LLM understands creativity in the first place, which feels circular.\n- Originality typically relates to something that has not been done before (by anyone else), yet here it seems to be defined more in relation to what an LLM has not done or generated before. Isn't that intrinsically limited?\n- In general, can we work on creativity and creativity evaluation without involving creative professionals or participants that can judge the creativity of an artifact?"}, "questions": {"value": "Please see the listed questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RC8tDJ5005", "forum": "ywMGBtTi4z", "replyto": "ywMGBtTi4z", "signatures": ["ICLR.cc/2026/Conference/Submission23412/Reviewer_H7Fy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23412/Reviewer_H7Fy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588245763, "cdate": 1761588245763, "tmdate": 1762942650960, "mdate": 1762942650960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Attempts to operationalize a framework from the social sciences (e.g., Simonton 2012) to measure the creativity of foundation models undertaking various tasks. The authors split these tasks into those that require \"convergent creativity\" (e.g., question answering and code generation) and those requiring \"divergent creativity\" (e.g, storytelling) with entirely different metrics for each type. The framework measures creativity in terms of \"usefulness\", \"originality\", and \"surprise\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is enjoyable and fun to read. When evaluating the creativity of foundational models, it's important to look at it from a traditional social science perspective. The experiments are comprehensive and clearly reported."}, "weaknesses": {"value": "I am not 100% happy about how the standard components of computational creativity are mapped into the benchmark. \"Usefulness\" is sometimes called \"familiarity\" or \"appropriateness\". Random nonsense is not creative, even if it is novel and surprising, because it seems to have no value and does not connect with the observer. I guess I can accept that we can reasonably measure usefulness in terms of correctness, coherence, etc.\n\nMy concern comes with the measures of originality and surprise. In the creative sense, they are related measures, differing by context. Originality is set in the broader social context. Is an artefact novel in the global sense that nobody or nothing has created something like this before? Surprise requires an observer. Is the artefact novel in the mind of the viewer? Or at least this is how I understand the distinction, having read the relevant literature, but long ago. To make a distinction, wouldn't an LLM-judge need to be grounded in the perspective of a particular observer (or class of observer) in order to make a distinction.\n\nThe distinction between originality and surprise is critical, and this is not really called out in the paper, which is very vague about the distinction. Moreover, there are just some vague sentences about how the distinction is operationalized.\n\nIt's also unclear which model is used as a judge. I see only that \"we employ an automatic rater, generally the most advanced judge model\". I've read the paper several times, but I don't see anything beyond that statement. I see lots of models being evaluated. Wouldn't it make sense to have each model in turn used as a judge."}, "questions": {"value": "Convince me that your operationalizations of originality and surprise really align with standard models of creativity.\n\nWhy not have the models judge each other, rather than picking a single model? It's also unclear which model you are using for this purpose.\n\nIs this UOS model really Simonton? I suppose I should go back and look, but that doesn't align with my memory."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KTJ6sVM2IG", "forum": "ywMGBtTi4z", "replyto": "ywMGBtTi4z", "signatures": ["ICLR.cc/2026/Conference/Submission23412/Reviewer_8wrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23412/Reviewer_8wrJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610185054, "cdate": 1761610185054, "tmdate": 1762942650216, "mdate": 1762942650216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops C2-Eval, a benchmark to quantify creativity in foundation models (FMs) across both ‘convergent’ tasks (with objective correctness, e.g., QA, code) and ‘divergent’ tasks (open-ended generation, e.g., storytelling). The framework operationalizes creativity using a Usefulness–Originality–Surprise (U-O-S) rubric originated in social science and applies a scoring pipeline across task types, using LLM judges. The authors benchmark FMs and show that, interestingly, creativity does not scale monotonically with model size. Instead, increasing size of output tokens, using reasoning-based model and creative prompting yield disproportionately higher creative gains. Results reveal nuanced trade-offs between convergent and divergent creativity, indicating these capabilities are correlated but not interchangeable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Timely contribution to the recent area of LLM creativity, with a new framework to measure both convergent and divergent creativity.\n\nThe proposed evaluation framework could be a good resource for the community to improve LLMs on creativity.\n\nExperiments are rather extensive, evaluating several current LLMs on the proposed creativity measurement."}, "weaknesses": {"value": "Framing tasks as mutually exclusive divergent and convergent might not be appropriate, for example, code generation is considered a convergent task here but there may be multiple possible code solutions for a given coding task.\n\nFormulation of creativity measurement is questionable. For example, the final composite creativity score is an average across the usefulness, originality and surprise score values but not evidence to support this design choice. For example, a solution that is not useful may still score high on originality and surprise but will not have any practical value."}, "questions": {"value": "In Figure 1, isn’t the autograder an LLM judge? If so, why is it depict to be different entities?\n\nIt would be interesting to see exactly why approaches such using reasoning-based model and creative prompting increases creativity, i.e. which of U-O-S is it improving. \n\n\nOther related work on measuring divergent and convergent creativity that would be helpful to also discuss:\nhttps://arxiv.org/abs/2407.09007\nhttps://arxiv.org/pdf/2410.04197\nhttps://openreview.net/forum?id=hocpzqMqB5\nhttps://arxiv.org/abs/2507.18368"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BW0IkgkHH7", "forum": "ywMGBtTi4z", "replyto": "ywMGBtTi4z", "signatures": ["ICLR.cc/2026/Conference/Submission23412/Reviewer_YEZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23412/Reviewer_YEZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912866395, "cdate": 1761912866395, "tmdate": 1762942649943, "mdate": 1762942649943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces C²-Eval, a benchmark for evaluating creativity in foundation models. The framework separates \"convergent creativity\" (structured tasks like code generation and QA) and \"divergent creativity\" (open-ended tasks like story generation), evaluating both through the lens of Usefulness, Originality, and Surprise (U-O-S)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "No Strengths IMHO"}, "weaknesses": {"value": "The abstract threw me off. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics that are not firmly grounded in established theories of creativity. Just because authors knowledge of Creativity Research is limited doesnt mean work doesnt exist\n\n- Art or Artifice? Large Language Models and the False Promise of Creativity (https://dl.acm.org/doi/10.1145/3613904.3642731)\n- Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking (https://dl.acm.org/doi/abs/10.1145/3706598.3714198)\n- LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play (https://arxiv.org/abs/2405.06373)\n\nThis is a below average paper. The core components of this work are largely derivative. \n\n- Standard benchmarks such as (TriviaQA, LiveCodeBench.) relabeled as \"creativity\" tasks\n- There are 50 papers in HCI literature on Convergent and Divergent Thinking. Authors are reinventing the wheel\n- The categorization of Question Answering as a creativity task is fundamentally problematic.  Factual QA requires correctness, not creativity\n- Why U-O-S framework ? Why not other Creativity frameworks like Torrance Test or Consensual Assessment Technique ?\n- For divergent tasks, using GPT-based judges is meaningless. GPT-4 judging GPT-4o's creativity is deeply flawed given its been shown LLMs favor their own generation. \n- Inter-annotator agreement statistics are absent\n- Lower confidence doesn't imply creative surprise. Its quite a leap of faith\n- Temperature and sampling parameters unreported that is critical for generation diversity"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FucgLgPOh3", "forum": "ywMGBtTi4z", "replyto": "ywMGBtTi4z", "signatures": ["ICLR.cc/2026/Conference/Submission23412/Reviewer_cK1g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23412/Reviewer_cK1g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977141420, "cdate": 1761977141420, "tmdate": 1762942649517, "mdate": 1762942649517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}