{"id": "Z0jDtLL7aM", "number": 25510, "cdate": 1758368762854, "mdate": 1759896718076, "content": {"title": "Efficient Spectral Graph Diffusion based on Symmetric Normalized Laplacian", "abstract": "Graph distribution learning and generation are fundamental challenges with applications in drug discovery, materials science, and network analysis. While diffusion-based approaches have shown promise, existing spectral methods suffer from eigenvalue imbalance and limited scalability. We introduce Efficient Spectral Graph Diffusion (ESGD), which advances spectral graph generation in three key ways: (1) compressing eigenvalues of the Symmetric Normalized Laplacian (SNL) into a bounded domain to eliminate spectrum imbalance with theoretical convergence guarantees; (2) designing a degree-matrix recovery algorithm to reconstruct adjacency matrices from SNL representations; (3) scaling to graphs with thousands of nodes where other models fail. The SNL transformation reduces condition numbers and learning difficulty for complex distribution patterns. Empirically, ESGD achieves state-of-the-art performance on generic graphs and competitive results on molecular generation, while successfully extending to large graphs. ESGD converges in 20 epochs (vs. >2000 for baselines) with 6-10× fewer sampling steps, establishing an efficient foundation for spectral graph diffusion.", "tldr": "", "keywords": ["Efficient Graph Generation", "Spectral Diffusion", "Eigenvalue Normalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4de5cd1d3054f2186c6cacd42331f145608047f3.pdf", "supplementary_material": "/attachment/ffca4ac02c763ba52eef83867a7deca8bca31fb1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Efficient Spectral Graph Diffusion (ESGD), a graph generative modeling framework built on the symmetric normalized Laplacian (SNL). Its key idea is to perform diffusion in the bounded SNL spectral space, which improves stability and convergence. To reconstruct graphs from spectral representations, the authors design a degree-matrix recovery algorithm. The framework also introduces an ego-subgraph decomposition strategy to make large-graph training computationally feasible."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors took a great care of making their approach theoretically grounded and evaluate on a variety of settings.\n\n- Presentation is good and Figure 1.a) illustrates greatly what kind of trade-off the authors aim to achieve with their method."}, "weaknesses": {"value": "- The method description requires clarification : you mention l132 that you keep $U$ fixed,  and then mention $\\hat{U}$ as the recovered eigenvectors l 148 : what is $\\hat{U}$ ? Such a crucial element of the reconstruction process should be clearly explained.\n\n- Section 3.2 lists properties and theorems without giving any intuition and explanations on them. For example, it's not clear for me at all why Remark 3.6 makes sense.\n\n- Table 1 lists a lot of outdated methods but more recent, major ones are missing, such as DiGress, DisCo, Cometh, DeFoG etc.\n\n- In Table 2, the Valid, Unique and Novel (VUN) metric is missing. Therefore, your evaluation do not assess the ability of the model to respect the structural constraints of the datasets.\n\n- QM9 and Zinc have reach saturation for years. For a method that specifically targets efficiency I would have expected evaluation on large scale datasets such as Moses or GuacaMol.\n\n- No errors bars are provided, even though multiple works have demonstrated how MMD metrics can exhibits high variance."}, "questions": {"value": "- See first weakness, how do we get U to reconstruct samples ?\n\n- It is not clear to me if learning on large networks like Cora is meaningful or not. You claim that your ego-based approach allows to enhance generalization, but you train and test on the same graph. In the end, it seems that overfitting the training graph will yield the best results. By the way, how do you compute the MMD metrics for those large networks. Do you extract k-hop ego subgraphs from the training graph ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "b3ZfcQfDKm", "forum": "Z0jDtLL7aM", "replyto": "Z0jDtLL7aM", "signatures": ["ICLR.cc/2026/Conference/Submission25510/Reviewer_tBih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25510/Reviewer_tBih"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649290640, "cdate": 1761649290640, "tmdate": 1762943457142, "mdate": 1762943457142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper presents 1) an incremental improvement over the (cited) GSDM model, replacing the use of adjacency matrix with the symmetric normalized laplacian, and  2) a study of  the theoretical implications of the change, to explain the observed empirical improvements, which consist of\n\n- improved conditions numbers and eigengaps, yielding faster convergence of the sampling process\n    \n- improved performance on a number of metrics evaluated on community-small,enzymes,grid,ego-small,QM9,Zinc250k,Citeseer,Cora,Pubmed,panar,sbm and tree graphs…\n    \n- ,,,while allowing for parameter reduction, which coupled with the\n    \n    improved training convergence allowing much  more fficient training (as well as sampling)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- well motivated, sensible\n    \n- theoretical analysis which seems correct\n    \n- clear efficiency gains compared to baselines\n\nhitting the dimensions explicitly\n\n1. originality: incremental improvement over GDSM\n2. quality: some nits about the evaluation and comparison, else no flaws\n3.  clarity: clearly written and proofs legible\n4. significance: clear improvement in convergence speed, decent incremental advance for this"}, "weaknesses": {"value": "- inconsistent/varying comparison set of baselines  => while its good to do many evals, that makes cherry picking possible, needs justification (or pick one and stay consistent with it)\n- unfair comparison without isomorpism/VUN check on larger datasets: digress edge etc. generate from scratch, GSDM and present store eigenvectors of training data set => should run an ablation generating the eigenvectors as well, as in GGSD\n- needs multiple seeds of the method/multiple sampling rounds and CI intervals, same values are quite close\n- would be good to report wallclock time/flop estimate (since e.g. the decoding might add wall clock at low flops/steps)\n- compute isomorphisms with dataset on generated graph vs baselines => are we just memorizing due to keeping the Eigenvectors? (this is a flaw inherited from GDSM )\n- try guacamol/moses (larger graphs) to see if things hold up there or the differences are washed out"}, "questions": {"value": "See weaknesses.\n\nThe most important elements to address are using multiple models/evaluations for reporting CIs on the metrics, trying larger datasets and performing an experiment applying the method to GGSD,then checking the graph isomoprhism rate to the training set and reworking the presentations etc. are extras"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w3bHaV1AmH", "forum": "Z0jDtLL7aM", "replyto": "Z0jDtLL7aM", "signatures": ["ICLR.cc/2026/Conference/Submission25510/Reviewer_tHHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25510/Reviewer_tHHN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983038929, "cdate": 1761983038929, "tmdate": 1762943456868, "mdate": 1762943456868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a generative diffusion model that operates in the spectral domain of the symmetric normalized Laplacian.\nSpecifically, instead of performing diffusion on the adjacency matrix, the method works only on the eigenvalues of the operator $S = -D^{-1/2} A D^{-1/2}$, while keeping the eigenvectors U fixed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "As stated by the authors, performing diffusion only on the eigenvalues is advantageous in terms of computational efficiency."}, "weaknesses": {"value": "- The results reported in Table 4 for Q9 should be discussed more thoroughly in relation to Table 12, in particular regarding the very low novelty value.\n- I’m not fully convinced in terms of novelty, as the proposed approach appears similar to prior spectral diffusion methods such as SPECTRE. The paper would benefit from a clearer discussion of how ESGD differs from this existing model.\n\nMinor:\n- In Figure 1, unless I missed something, acronyms are not defined from the beginning."}, "questions": {"value": "A major limitation concerns the assumption of a fixed spectral basis. The authors state that graph reconstruction is achieved by combining the generated eigenvalues with a fixed eigenbasis U, but it is not clearly explained where U is obtained (from the training set?) or how the model could generate graphs with different topologies if the eigenbasis cannot change. This point should be clarified and discussed in greater depth, as it is a key aspect of the proposed method; if convincingly addressed, it could positively affect my score.\n\nThe results in Tables 1 and 2 are promising, yet it is unclear why Table 3 includes fewer competitor methods. Why is GGSD not reported in the first two tables?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QNEn76sxOk", "forum": "Z0jDtLL7aM", "replyto": "Z0jDtLL7aM", "signatures": ["ICLR.cc/2026/Conference/Submission25510/Reviewer_L2tt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25510/Reviewer_L2tt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762449950316, "cdate": 1762449950316, "tmdate": 1762943456617, "mdate": 1762943456617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}