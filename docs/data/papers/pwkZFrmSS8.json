{"id": "pwkZFrmSS8", "number": 13610, "cdate": 1758219787156, "mdate": 1759897424884, "content": {"title": "Silent Neighbors, Loud Secrets: Privacy Leakage from Nearby Classes in Unlearned Models", "abstract": "In this paper, we reveal a significant shortcoming in class unlearning evaluations: overlooking the underlying class geometry can cause privacy leakage. We further propose a simple yet effective solution to mitigate this issue.\nWe introduce a membership-inference attack via nearest neighbors (MIA-NN) that uses the probabilities the model assigns to neighboring classes to detect unlearned samples. Our experiments show that existing unlearning methods are vulnerable to MIA-NN across multiple datasets. We then propose a new fine-tuning objective that mitigates this privacy leakage by approximating, for forget-class inputs, the distribution over the remaining classes that a retrained-from-scratch model would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model’s distribution accordingly. The resulting Tilted ReWeighting (TRW) distribution serves as the desired distribution during fine-tuning. We also show that across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior unlearning metrics. More specifically, on CIFAR-10, it reduces the gap with retrained models by $19\\%$ and $46\\%$ for U-LiRA and MIA-NN scores, accordingly, compared to the SOTA method for each category.", "tldr": "We introduce a nearest-neighbor membership inference attack to expose privacy leakage in class unlearning, and propose an output reweighting method that defends against it while matching retraining performance.", "keywords": ["machine unlearning", "Selective Forgetting", "class unlearning", "membership inference attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b079817abd1250ca68fd294f5ec42d349662189.pdf", "supplementary_material": "/attachment/9102302ef3b684a5eed5fef075bc58a3a7fec7de.zip"}, "replies": [{"content": {"summary": {"value": "This paper argues that standard class-unlearning evaluations overlook class geometry, leaving leakage that a new attack (MIA-NN) can expose. The authors proposed TRW to mitigate the privacy leakage caused by MIA-NN and did extensive experiments to support their claims."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The nearest-neighbor leakage perspective is simple and intuitive.\n\n2. TRW is an output-space objective, easy to implement, and adds little overhead relative to fine-tuning."}, "weaknesses": {"value": "1. The MIA via nearest neighbors attack is not clearly substantiated in the main text. For an attack, at least the threat model, capability of each role and the attacking goal should be defined clearly. In this paper, it seems that all things are conducted by the ML server.\n\n\n2. What MIA-NN really measures. As defined, MIA-NN trains per-class discriminators on retrain (and then on the unlearned model) and computes their accuracy on forget-class test samples to quantify a gap to retrain. This is not a per-example membership decision about whether a specific sample was in the original training set; it is a distributional test of whether forget-class behavior matches retrain. The paper sometimes presents MIA-NN as a “membership inference attack” without clearly distinguishing this from standard per-sample MIAs.\n\n\n3. The proposed Tilted ReWeighting (TRW) objective adjusts the output distribution proportionally to class similarity, but this only ensures proportional alignment. It does not guarantee the decision boundary geometry or higher-order distributional structure. As a result, while the marginal behavior may resemble retraining, the actual local decision regions may diverge.\n\n4. The paper's writing and methodological details are not clear, making it hard to follow:\n - In Section 3.4, the constant c (from the definition of set A) is not explained clearly (between Eq. 1 and Eq. 2, no marker). How to determine the value of expected similarity c, and why the paper set β as 10?\n - The theoretical motivation for using an exponential term (exp) in the tilting factor in Eq. 2, rather than a simpler linear weighting, is not discussed in the main text. The proof in the appendix shows this is a result of KL-divergence minimization, this should be mentioned after Eq.2 at least 1 sentence.\n - A method named TRW-2R appears in multiple tables (e.g., Table 2, 3) and often performs differently than TRW. However, this method is never defined or explained in the paper. This is a significant omission that needs to be addressed.\n\n5. For paper citing, authors could use \\citep{} to replace \\cite{}."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "abSP4g1fhA", "forum": "pwkZFrmSS8", "replyto": "pwkZFrmSS8", "signatures": ["ICLR.cc/2026/Conference/Submission13610/Reviewer_HDYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13610/Reviewer_HDYu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372042027, "cdate": 1761372042027, "tmdate": 1762924192453, "mdate": 1762924192453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new membership inference attack on the class unlearning task and then proposes a novel unlearning method with design robustness against membership inference attacks. The author made an observation that the retrained model demonstrates consistent misclassification of some retained classes when tested on the unlearning class. Based on the observation, the author designed MIA-NN, which exploits the probability assigned to the closest forget set to achieve a stronger MIA. The paper proposes Tilted Re-Weighting(TRW) to achieve secure unlearning under MIA. TRW redistributes the probability among the remaining classes to achieve a consistent prediction with a retrained model."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed unlearning method is clear and insightful. \n2. Strong theory foundation with clear mathematical explanation. \n3. Comprehensive benchmark method comparison."}, "weaknesses": {"value": "1. The table description is inaccurate and causes confusion due to the lack of notation in the table. Specifically, Table 1 is described as follows: \" Higher values indicate better unlearning; however, the paper also describes that the gap between the Acc_i and Acc^Mu_rn is used to measure the unlearning effectiveness. In the experiment section, the paper proposes that the MIA score of the unlearned model is expected to match that of the retraining model. However, the table does not provide the MIA score of any retraining model or the difference between the unlearned model and the retrained model. These problems pose difficulties in understanding the results of the paper. \n2. In Table 2 and Table 4, the method TRW-2R outperformed TRW unlearning  introduced in the paper. In Appendix B.5, the author also mentioned the proposed TRW-2R as the fastest among other baseline methods. However, there's a lack of the implementation and theoretical details of the methodTRW-2R.  The authors do not explicitly specify what additional components or improvements has been made to distinguish TRW-2R from TRW. Without the details, it’s hard to understand and verify the reason for the experiment improvement. \n3. The discussion of the method is limited to class unlearning on classical vision models. This presents a dual limitation for extending this method to either other types of models(for example, GNN and LLM) or type of unlearning (for example, per-example unlearning). I would appreciate some discussion of the possibility of extending this method to other models, such as ViT."}, "questions": {"value": "One of the important insights from this paper is the vulnerability of the unlearned model after class unlearning, and a corresponding attack method, MIA-NN, has been proposed to exploit it. I am curious why MIA-NN is not part of the evaluation matrix(except for B.7 with an ablation study focus on the effect of beta)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LRvpgXREPS", "forum": "pwkZFrmSS8", "replyto": "pwkZFrmSS8", "signatures": ["ICLR.cc/2026/Conference/Submission13610/Reviewer_YnLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13610/Reviewer_YnLx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385829576, "cdate": 1761385829576, "tmdate": 1762924192112, "mdate": 1762924192112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper offers a crisp diagnostic (MIA-NN) and a pragmatic fix (TRW) that together materially advance evaluation and practice of class unlearning. While the core ideas are simple, they are impactful and well-validated. Clarifications on the attack’s practicality and broader stress-testing would further solidify the case for acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper pinpoints a blind spot in current class-unlearning evaluation—evaluations ignore how retrained models systematically misclassify forgotten-class samples toward semantically similar retained classes. This motivates a new attack (MIA-NN) that probes leakage via the nearest neighbor of the forgotten class and exposes failures of many SOTA methods.\n2. The proposed Tilted ReWeighting (TRW) modifies the fine-tuning objective by zeroing the forget label and tilting the remaining class distribution using inter-class similarities derived from logit weights; the resulting target distribution can be seen as an information projection with a linear moment constraint. It is lightweight (drop-in during fine-tuning) and conceptually clean."}, "weaknesses": {"value": "1. TRW hinges on a particular class-similarity score (cosine in a PCA-projected logit space with a sharp softmax temperature). While intuitive, this is heuristic and sample-independent; performance sensitivity to the choice of similarity, PCA dimension, temperature, and $\\beta$ needs deeper analysis beyond brief ablations.\n2. The attack identifies a “nearest neighbor” via statistics from multiple retrained models and trains an SVM on logits. Although the paper claims the attack does not assume access to training data, the practicality of assembling enough scratch-retrained references (and the knowledge required) deserves clearer discussion and a black-box-only variant analysis."}, "questions": {"value": "1. How many scratch-retrained models are required for MIA-NN to be reliable, and under what access (black-box probabilities only vs. logits vs. labels)? Can you report MIA-NN performance under strictly black-box probability queries and with one or zero reference retrains (e.g., using public checkpoints as surrogates)?\n2. How sensitive is TRW to (i) PCA dimension, (ii) softmax temperature over similarities, (iii) cosine vs. centroid-distance vs. feature-space CKA similarities, and (iv) the tilt parameter $\\beta$  (beyond the brief ablation)? Please include a grid showing ACCr/ACCf/MIA/U-LiRA vs. these hyperparameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SDdkd5Rxcd", "forum": "pwkZFrmSS8", "replyto": "pwkZFrmSS8", "signatures": ["ICLR.cc/2026/Conference/Submission13610/Reviewer_QFzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13610/Reviewer_QFzx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13610/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834623003, "cdate": 1761834623003, "tmdate": 1762924191842, "mdate": 1762924191842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}