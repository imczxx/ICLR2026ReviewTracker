{"id": "ytCDRzvf5f", "number": 12083, "cdate": 1758205597276, "mdate": 1759897535095, "content": {"title": "ERPV: Enhancing Visual Reinforcement Learning with Partially Reliable Knowledge from VLMs", "abstract": "Visual Reinforcement Learning (VRL) aims to learn optimal control policies from scratch,  a process that often suffers from low exploration efficiency. Integrating large-scale vision-language models (VLMs) offers a promising solution, as they provide rich prior knowledge about the environment. However, VLMs are only partially reliable when directly applied to VRL: the inferred actions may be wrong in certain states, and the inability to identify reliable action alignment can result in excessive exploration by the agent. We propose ERPV, a novel method that effectively enhances VRL with partially reliable knowledge from VLMs. ERPV introduces two key modules: (1) Value-aware Policy Guidance, which estimates the reliability of VLMs across different states and adaptively selects trustworthy VLM-inferred actions to guide policy learning; (2) VLMs-guided Entropy Regularization, which reduces over-exploration by comparing the confidence between VRL policy and VLMs-inferred actions. Extensive experiments show that, compared to the state of the art, ERPV achieves competitive performance in both policy effectiveness and sample efficiency under diverse, complex visual control tasks. The code has been placed in the supplementary materials.", "tldr": "", "keywords": ["Large-scale vision-language models", "Reinforcement learning", "Decision making", "Knowledge distillation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/834312afbc1e5199aa6a13b19d0d4edcf247c07e.pdf", "supplementary_material": "/attachment/0f1aad63eae5dcffe13b912be1e8421f3898c887.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ERPV, a novel visual reinforcement learning method.\nSpecifically, ERPV leverages the vision language model to (1) select actions to guide policy learning and (2) provide reference actions for better exploration.\nExperiment results on various benchmarks show that ERPV achieve superior performance and learning efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper writing and structure are clear.\n2. Good motivation and novelty regarding VLM as the prior knowledge for policy learning and exploration (VPG and VER components).\n3. Empirical results demonstrate the superior improvements on various benchmarks."}, "weaknesses": {"value": "1. As mentioned in the limitation, it would be great to analyse the theoretical properties of ERPV, including convergence and optimality. It feels like be easy to extend from SAC.\n2. The training speed deeply depends on the inference speed of VLM. It would be better to have the statistics of the training speed comparison in section 4.4."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iMB8rmDP91", "forum": "ytCDRzvf5f", "replyto": "ytCDRzvf5f", "signatures": ["ICLR.cc/2026/Conference/Submission12083/Reviewer_ZmUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12083/Reviewer_ZmUU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577410036, "cdate": 1761577410036, "tmdate": 1762923054326, "mdate": 1762923054326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ERPV as an approach to benefit from common sense in VLM in visual reinforcement learning (RL). ERPV leverages predictions frmo a VLM  in a soft-actor-critic RL framework  by regerresing the VLM action. The regression loss is weighted by a coefficient representing the advantage of using VLM-based actions over using action from the RL policy. The authors also propose using the prediction error of a dynamics model conditioned on the VLM action as an exploration signal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written\n- Leveraging VLM common sense to guide policy search is a novel and interesting idea\n- The results are strong and quite promising\n- Experiments include ablations of various design choices and clearly disentangle the role of different components"}, "weaknesses": {"value": "- The paper lacks an extensive discussion of the main assumption made here: VLMs are trained with action-free data, the best they could actually do in action selection is either some sort of nearest neighbor if the domain data was seen during VLM training, or provide actions that are at a semantic level at best. Most action representations in complex systems (e.g. robotic manipulation) are far more challenging on the lower (non-semantic) level. In such complex environments, it is unclear how such a supervision could even help beyond just simple early-state exploration ( a problem that is potentially non-existent if expert data is available, which is becoming the case)\n- Most environments used in the experiments are low-dimensional, and involve simple dynamics, it would be interesting to demonstrate the applicability of the method to more complex domains, or at least understand its limits in such domain"}, "questions": {"value": "Can you provide a more extensive discussion on how you expect your assumption of \"VLM actions being a good source of supervision\" to scale or fail to scale to high-dimensional complex tasks that involve low-level action?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g00QPIMjrx", "forum": "ytCDRzvf5f", "replyto": "ytCDRzvf5f", "signatures": ["ICLR.cc/2026/Conference/Submission12083/Reviewer_cNdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12083/Reviewer_cNdj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702835302, "cdate": 1761702835302, "tmdate": 1762923053875, "mdate": 1762923053875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ERPV, a method that integrates partially reliable knowledge from Vision-Language Models (VLMs) into visual reinforcement learning (VRL). The key motivation is that while VLMs contain useful commonsense priors, their action reasoning is often unreliable or inconsistent across states. To address this, the authors propose two mechanisms: Value-aware Policy Guidance (VPG): which dynamically estimates the reliability of VLM-inferred actions by comparing their Q-values with those of the RL policy, selectively applying guidance when the VLM’s suggestion appears better. VLM-guided Entropy Regularization (VER): adjusts exploration via an entropy coefficient that depends on how consistent the RL policy’s actions are with the VLM’s inferred actions, encouraging exploration when they diverge and exploitation when they align. Experiments across Carla, DMControl, and CarRacing benchmarks show that ERPV improves both sample efficiency and final performance, even when the VLM guidance is noisy or unreliable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely and relevant problem: The paper addresses an emerging and underexplored question — how to integrate large pretrained vision-language models into reinforcement learning while handling their imperfect reasoning.\n\n- Thoughtful formulation: The introduction of reliability estimation (VPG) and entropy adjustment (VER) feels intuitive and conceptually clean, combining the strengths of teacher–student distillation and adaptive exploration control.\n\n- Robust empirical results: Across several benchmarks, ERPV consistently outperforms prior VLM-assisted RL baselines (e.g., DSF, ASF, DGC), and even performs comparably to base RL when VLMs are unreliable.\n\n- Comprehensive experiments: The inclusion of diverse settings (e.g., CARLA, DMControl), multiple VLM backbones (Qwen2-VL, LLava), and ablations on both modules (VPG/VER) provides solid empirical evidence.\n\n- Clear presentation: Figures and tables (especially Fig. 5 showing dynamics and entropy coefficient) make it easy to follow how the method behaves during training."}, "weaknesses": {"value": "- Incremental conceptual novelty: The main novelty lies in combining selective guidance and entropy modulation, but both components resemble ideas from adaptive distillation and uncertainty-aware exploration. The conceptual leap is moderate.\n\n- Lack of theoretical insight: The paper would benefit from some formal justification (e.g., why the proposed difference or transition-based confidence metric leads to stable convergence). \n\n- Dependence on pretrained critic: Since ​the critic is trained using VLM actions, its generalization and possible bias are underexplored — what happens if the pretraining domain diverges from the RL environment?\n\n- Limited real-world validation: All experiments are simulation-based. The authors’ claim of “real-time deployability” is interesting, but there’s no demonstration on an actual robotic platform.\n\n- Scalability & compute details: The training cost of ERPV compared to vanilla SAC or other VLM-distilled methods isn’t discussed, making it hard to assess practicality."}, "questions": {"value": "-How sensitive is ERPV to the hyperparameters lambda? Does over-weighting VPG risk reinforce VLM errors?\n\n- Could the proposed Action Confidence Function (ACF) be replaced by simpler distance metrics (e.g., cosine similarity between action logits) without major loss?\n\n- How much does the performance depend on the choice or pretraining quality of the VLM critic?\n\n- Have you tested ERPV with textual prompts that are deliberately ambiguous or wrong to examine robustness?\n\n- Could the approach be generalized to multi-modal feedback beyond actions, such as state representations or reward shaping?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FqxY8yIiyx", "forum": "ytCDRzvf5f", "replyto": "ytCDRzvf5f", "signatures": ["ICLR.cc/2026/Conference/Submission12083/Reviewer_K2Jp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12083/Reviewer_K2Jp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939277153, "cdate": 1761939277153, "tmdate": 1762923053426, "mdate": 1762923053426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ERPV, to address the issue of low exploration efficiency in VRL by introducing prior knowledge provided by pre trained VLMs. Experiments are conducted on Carla, DMC and CarRacing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. To the best of my knowledge, the paper is the first to systematically identify and formalize the challenge of “partially reliable knowledge.” \n\n2. The work targets the central tension in integrating VLMs with VRL. \n\n3. The experimental evaluation is extensive."}, "weaknesses": {"value": "1. The core idea does not move beyond the teacher–student paradigm and remains within the traditional setting where the teacher provides knowledge and the student learns it. \n\n2. Discarding the VLM at test time forfeits substantial information encoded in the VLM and risks overfitting to the test environment due to limited training. \n\n3. There is no comparison of computational cost or runtime for the training phase."}, "questions": {"value": "1. Given that VPG can already approximate the ground-truth Q reasonably well, why not directly use this Q estimate to optimize the actor network? \n\n2. In VER, high consistency between the VLM and RL policies indicates reliability, whereas low consistency indicates unreliability and triggers increased exploration. Suppose the VLM’s action is unreliable, but after exploration the RL policy becomes reliable and efficient; then the consistency between the VLM and RL policies would remain low. In that case, would ERPV continue to explore such states indefinitely? Furthermore, if both the VLM’s action and the RL’s action are reliable but diverse, the consistency may still remain low. In this scenario, does ERPV favor exploration or exploitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FSHTGA187Y", "forum": "ytCDRzvf5f", "replyto": "ytCDRzvf5f", "signatures": ["ICLR.cc/2026/Conference/Submission12083/Reviewer_oTMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12083/Reviewer_oTMT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997539623, "cdate": 1761997539623, "tmdate": 1762923052891, "mdate": 1762923052891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}