{"id": "lndDn7i8W6", "number": 24054, "cdate": 1758352195898, "mdate": 1759896784039, "content": {"title": "Leveraging Discrete Function Decomposability for Scientific Design", "abstract": "In the era of AI-driven science and engineering, we often want to design discrete\nobjects (e.g., circuits, proteins, materials) in silico according to user-specified\nproperties (e.g., that a protein binds its target). Given a property predictive model,\nin silico design typically involves training a generative model over the design\nspace (e.g., over the set of all length-L proteins) to concentrate on designs with the\ndesired properties. Distributional optimization, formalized as an estimation of distribution algorithm or as reinforcement learning policy optimization, maximizes\nan objective function in expectation over samples. Optimizing a distribution over\ndiscrete-valued designs is in general challenging due to the combinatorial nature\nof the design space. However, many property predictors in scientific applications\nare decomposable in the sense that they can be factorized over design variables in a\nway that will prove useful. For example, the active site amino acids in a catalytic\nprotein may need to only loosely interact with the rest of the protein for maximal catalytic activity. Current distributional optimization algorithms are unable to\nmake use of such structure, which could dramatically improve the optimization.\nHerein, we propose and demonstrate use of a new distributional optimization algorithm, Decomposition-Aware Distributional Optimization (DADO),\nthat can leverage any decomposability defined by a junction tree on the design\nvariables. At its core, DADO employs a factorized “search distribution”—a\nlearned generative model—for efficient navigation of the search space, and invokes graph message passing to coordinate optimization across all variables.", "tldr": "", "keywords": ["scientific", "protein", "design", "generative model", "decomposability"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5f78480cb3dfc55d6d6ef7320a7a3790cfa59db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Decomposition-Aware Distributional Optimization (DADO) as a new algorithm for optimizing discrete functions that exhibit decomposable structures. The method learns a decomposed functional form of an objective and then incorporates a factorized search and message-passing schema to coordinate updates. The method is evaluated against Estimation of Distribution Algorithm (EDA) on both synthetic and real protein design tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of my knowledge, the core idea of applying message passing for distributional optimization is novel and an interesting approach to the problem of scientific design.\n- I appreciate the statistical tests reported in the manuscript."}, "weaknesses": {"value": "At a high level, my main concerns/questions stem from the learned decomposition of the objective function. While there is evidence that DADO works if a good function decomposition exists, it is not clear that (1) such a good function decomposition always exists; and (2) that the method outperforms black box optimization methods that do not even take advantage of a function decomposition.\n\n1. In the paragraph starting from line 100, the manuscript describes a method to \"guess\" the functional form of the predictive model and then fitting a model that enforces the guessed decomposition to the available data. However, this might result in a potential distribution shift between the fitted model and the true objective, which may or may not adhere to the \"guessed\" functional form. This is of particular concern in situations where the \"high scoring\" candidate designs are rare in the training data and might obey a different underlying functional form compared to other designs. It would be nice to better understand how much this possible distribution shift is actually a concern.\n2. Somewhat related to the above comment, but it would be good to ablate the size of the training dataset used to learn the MLP-based function decomposition, instead of using all the data available (line 451). In particular, I feel it would be important to investigate the performance of DADO if only the bottom $x$th percentile of designs were used to learn the surrogate function.\n3. The manuscript seems to specifically focus on applications of DADO for protein design (in addition to the synthetic function testing). However, it seems from the title, abstract, introduction, and other parts of the text that DADO is proposed as a general method for many different possible discrete design problems in AI4Science. In this light, it would be good to include additional evaluations across different domains (e.g., circuit, molecule, and material design, for instance) to better illustrate how DADO actually generalizes across different scientific domains. \n4. It seems like a number of baselines are missing from experimental evaluation - in particular, how does DADO compare with methods that do not involve learning a junction tree decomposition at all? This would include any black-box optimization method - PPO, BO, FDA, MCTS (some of which are non-distributional but can still be evaluated using the distributional optimization framework). This would help better clarify the added benefit of even learning a functional decomposition in the first place.\n5. It would be good to include results on ablating $K$ in step 1 (line 154)."}, "questions": {"value": "6. Is it ever possible for the functional form to enforce the wrong prior over the input space (for example, if the training data is messy, noisy, affected by a confounding variable, or otherwise unreliable)? If so, what happens to the performance of DADO in these situations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hhw2JRGcwY", "forum": "lndDn7i8W6", "replyto": "lndDn7i8W6", "signatures": ["ICLR.cc/2026/Conference/Submission24054/Reviewer_dP4b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24054/Reviewer_dP4b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761080878915, "cdate": 1761080878915, "tmdate": 1762942915423, "mdate": 1762942915423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a critical bottleneck in AI-driven scientific design—scaling distributional optimization to high-dimensional discrete spaces (e.g., proteins, circuits)—by exploiting function decomposability. Its contributions are well-aligned with the needs of both machine learning (ML) for optimization and domain sciences (e.g., protein engineering), making it a valuable addition to the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unlike standard Estimation of Distribution Algorithms (EDAs) or reinforcement learning (RL) policy optimization, DADO explicitly leverages the decomposability of objective functions (via junction trees) to avoid optimizing over intractable full combinatorial spaces. This is a departure from \"black-box\" optimization approaches that treat the objective as monolithic.\n 2. It generalizes classical max-plus message passing (used for exact global optimization) to distributional optimization, replacing hard maximization with soft, sample-based expectations. This enables DADO to balance exploration (via a factorized generative model) and exploitation (via coordinated message passing)—a key innovation not seen in prior work like Factorized Distribution Algorithms (FDAs) or chain-structured RL policies.\n3. The paper also explores a practical tradeoff: tuning decomposability (via residue distance thresholds in proteins) to balance predictive model accuracy and optimization efficiency. This empirical insight bridges theoretical algorithm design and real-world scientific constraints."}, "weaknesses": {"value": "1. The paper relies heavily on structure-based decomposability (AlphaFold3 contact graphs) for proteins but does not explore other practical sources of decomposability: For example, in protein design, decomposability could also be derived from sequence homology (conserved vs. variable regions) or functional annotations (binding sites vs. structural loops). Similarly, in circuit design, decomposability might come from modular components.\n2. The paper empirically shows that \"loose\" decomposability (e.g., t=2.75 Å for GB1) retains predictive accuracy, but lacks theoretical bounds on how much decomposition error DADO can tolerate. \n3. The paper uses D=20 (amino acids) for all experiments but does not address scalability to larger discrete alphabets (e.g., D=100 for small molecules or circuit components)"}, "questions": {"value": "1. For domains without 3D structural data (e.g., novel peptides or synthetic materials), what alternative methods would you recommend to derive decomposability for DADO? For example, could unsupervised learning (e.g., clustering design variables by co-occurrence) be used to \n2. If you intentionally introduce errors into the junction tree (e.g., remove 10% of true residue contacts for TDP43), how much does DADO’s performance degrade relative to using the correct tree? Are there any heuristics (e.g., adding \"redundant\" edges to the junction tree) to mitigate this error?\n3. For D=50 (e.g., small molecules with 50 building blocks), would DADO’s current MLP-based search distribution require prohibitive compute? If so, what modifications would you propose to scale DADO to larger alphabets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wuFjOghCaY", "forum": "lndDn7i8W6", "replyto": "lndDn7i8W6", "signatures": ["ICLR.cc/2026/Conference/Submission24054/Reviewer_6Cz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24054/Reviewer_6Cz8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636012101, "cdate": 1761636012101, "tmdate": 1762942915163, "mdate": 1762942915163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an algorithm (DADO) for learning a distribution over a discrete design space to maximize an expected reward, where that reward function can be expressed as a kind of generalized additive model. The authors show favorable performance relative to classical EDA in both synthetic experiments as well as on a task involving the optimization of the score assigned by a protein property predictor developed with the required structure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors are focused on an important and challenging problem in scientific design, as the design space in such settings tends to be discrete or combinatorial and therefore is challenging to explore or optimize over. Their focus on reducing the combinatorial complexity of this search problem to one that is significantly more manageable is a worthwhile pursuit and has real applications. The authors motivated the problem well and provided strong justification for an approach such as theirs for more efficiently optimizing the design space. Further, the generalized additive model structure induced by a junction tree that defines the relationship between the design variables and the property being optimized is a unique choice and lends itself to the paper's originality. Improved efficiency for exploring large combinatorial spaces is of great significance to the field, and the authors have proposed a method that performs capably on the considered problems."}, "weaknesses": {"value": "The requirement that the decompositional form be known is a very strong one in practice. Indeed, in the protein property prediction experiments, the authors resorted to a very specifically designed predictor to make use of this decomposition--a design which limits the accuracy of the proposed property predictor. Hence, it is unclear to me whether this method has much utility to the community. It would be helpful if the authors could better profile the impact of the decomposition on the accuracy of the underlying property predictor.\n\nAdditionally, I found the the experiments to be rather limited. There is one set of experiments on small synthetically curated systems with shallow junction trees, and another on a protein property prediction task using the same model architecture for each of the four considered proteins. It is difficult from these limited experiments to build good intuition on the method's ability to scale to problem size and complexity. It would be helpful if the authors could better profile or characterize the behavior of their method on a more representative set of problems involving the optimization over a discrete design space. It would also be helpful to establish performance against other baselines for such problems, not just a vanilla EDA."}, "questions": {"value": "How restrictive is the assumed compositionality? From the paper's definition \"consider the form f(x) = C1(˜x1) + C2(˜x2), . . . , Cκ(˜xκ), where Ci denotes an arbitrary function on a set of design variables, x˜i,\" it seems rather general, but the experiments focus on a fairly narrow set of circumstances (i.e., shallow junction trees), so it would be helpful to understand what this definition covers. For example, an arbitrary neural network that takes x as input, transforms it through layers into some d-dimensional embedding, which are then transformed through a linear layer to a scalar output would seem to apply to this definition (in this case, the Ci functions are the individual embedding dimensions which are functions of the x's times the associated weight from the linear layer). Which would indicate to me that this can be applied arbitrarily to neural network predictors, although the experiments suggest this is not in fact the case. Can you clarify?\n\nHow well does the method scale to larger sequence lengths compared to those considered in the paper (L < 100)? What about to larger junction trees? Are there important qualitative differences in predictive performance for the underlying property predictors as sequence length increases?\n\nHow robust is DADO in settings where the assumed decomposition is incorrect or incomplete?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8EkTwQeLBh", "forum": "lndDn7i8W6", "replyto": "lndDn7i8W6", "signatures": ["ICLR.cc/2026/Conference/Submission24054/Reviewer_59py"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24054/Reviewer_59py"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931712686, "cdate": 1761931712686, "tmdate": 1762942914794, "mdate": 1762942914794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Decomposition-Aware Distributional Optimization (DADO) that leverages decomposability and employs a generative model to guide the search for the target distribution.\n\nDADO is compared to standard Estimation of Distribution Algorithm (EDA), a form of Expectation-Maximization that is not decomposition-aware.\n\nThe core of DADO is its use of an objective function decomposed into a junction tree, which enables node-level estimation of value (Q- and V-) functions that represent the choice of variables at each edge and node respectively. These value functions are used to update the search distribution via dynamic programming in the form of message-passing.\n\nDADO outperforms standard EDA on synthetic functions as well as a multi-layer perceptron (MLP)-based predictive model of protein property functions."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- DADO is well-motivated and extensively derived\n- DADO clearly converges faster than standard EDA\n- DADO is a novel, more efficient algorithm than fills a gap in the literature"}, "weaknesses": {"value": "- This paper is hard to read due to large amounts of text, in-line math, and few subsections.\n- The evaluation is limited to three synthetic functions and four learned protein property functions\n- Standard EDA that is unaware of function decomposability can outperform DADO in the absence of ad hoc hyperparameter tuning.\n- The GB1 evaluation appears prematurely ended, as the EDA does not appear to converge by the final training iteration\n\nTypo:263 shaing (shaping)"}, "questions": {"value": "- How long does it take to run DADO and standard EDA?\n- Can DADO still be used for larger alphabet sizes or sequence lengths?\n- How would DADO change if positions did not share the same alphabet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XPOITVk9c5", "forum": "lndDn7i8W6", "replyto": "lndDn7i8W6", "signatures": ["ICLR.cc/2026/Conference/Submission24054/Reviewer_75LP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24054/Reviewer_75LP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24054/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971225910, "cdate": 1761971225910, "tmdate": 1762942914418, "mdate": 1762942914418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}