{"id": "SwHnok3JtD", "number": 6850, "cdate": 1757998220149, "mdate": 1759897888423, "content": {"title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning", "abstract": "Device-cloud collaboration has emerged as a promising paradigm for deploying large language models (LLMs), combining the efficiency of lightweight on-device inference with the superior performance of powerful cloud LLMs. An essential problem in this scenario lies in deciding whether a given query is best handled locally or delegated to the cloud. Existing approaches typically rely on external routers, implemented as binary classifiers, which often struggle to determine task difficulty from the prompt's surface pattern. To address these limitations, we propose a framework where the on-device LLM makes routing decisions at the end of its solving process, with this capability instilled through post-training. In particular, we formulate a reward maximization problem with carefully designed rewards that encourage effective problem solving and judicious offloading to the cloud. To solve this problem, we develop a group-adaptive policy gradient algorithm, featuring a group-level policy gradient, designed to yield an unbiased gradient estimator of the reward, and adaptive prompt filtering, developed to enforce the constraint on cloud LLM usage. Extensive experiments across models and benchmarks show that the proposed methodology consistently outperforms existing baselines and significantly narrows the gap to full cloud LLM performance.", "tldr": "We propose a unified post-training framework that integrates routing optimization, enabling the on-device LLM to improve its problem-solving ability while learning routing strategies.", "keywords": ["Large Language Models", "Device-Cloud Collaboration", "Post-Training", "Routing Optimization", "Reinforcement Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8679960b72492aa494eb76bf2247f21c1bacee23.pdf", "supplementary_material": "/attachment/0abec1d957d5dfe99e2938019c58c05fb162a7f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a novel framework for collaborative device-cloud LLM inference that eliminates the need for an external router. The core idea is to empower the on-device LLM to autonomously decide when to offload a query. This is achieved through a unified reinforcement learning (RL) framework where the model is post-trained to either solve a task or \"call for help\" after attempting a solution. A hierarchical reward function guides this learning process, and a new Group-Adaptive Policy Gradient (GAPG) algorithm is proposed for stable optimization under a cloud usage budget. Experiments demonstrate that this integrated approach significantly outperforms baseline methods and substantially narrows the performance gap to a full cloud-LLM solution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a highly novel and elegant paradigm by embedding the routing logic directly into the on-device LLM, which eliminates the need for a separate, external router and simplifies the overall system architecture.\n- The proposed Group-Adaptive Policy Gradient (GAPG) algorithm is well-motivated, addressing a specific failure mode of existing RL methods in this hierarchical reward setting , and the adaptive prompt filtering is a practical mechanism for handling budget constraints.\n- The empirical evaluation is thorough, demonstrating consistent outperformance over strong baselines across multiple models and benchmarks. The results effectively validate the practical benefits of the proposed unified training framework."}, "weaknesses": {"value": "- The claimed novelty of the core gradient estimator in the GAPG algorithm is questionable. The key modification, removing standard deviation normalization from the GRPO advantage calculation, appears to be identical to the one proposed in the concurrent work \"Dr. GRPO\" (ICML Workshop, https://arxiv.org/abs/2503.20783), which is not cited or discussed in this paper.\n-The experiments are confined to mathematical reasoning tasks. The framework's applicability to more complex tasks (e.g., code generation, etc.) is not demonstrated, and designing an effective reward signal for such tasks could be a significant challenge.\n- The routing decision is made only after the on-device model has completed its full generation process. This \"post-hoc\" decision-making process means that for any query ultimately offloaded to the cloud, the initial on-device computation is entirely wasted, leading to inefficient use of device resources and increased end-to-end latency for the user.\n- The collaboration mechanism is a simple binary hand-off. The on-device model cannot pass its partial reasoning to the cloud model, which misses an opportunity for a more nuanced and potentially more efficient collaboration."}, "questions": {"value": "- The core modification in your GAPG algorithm (using r - mean(r) for advantage) appears to be identical to the one proposed in the concurrent work \"Dr. GRPO.\" Could you please clarify the relationship between your work and this prior art and better position your contribution?\n- The routing decision is made at the end of the generation process. Does this mean the on-device model always completes its full reasoning, even if it determines early on that a solution is unlikely? Have you explored mechanisms for an \"early exit\" to save on-device computation?\n- How sensitive is the model's final performance and routing behavior to the specific weights chosen for the hierarchical reward function (i.e., $\\alpha_a, \\alpha_c, \\alpha_f$)? A sensitivity analysis would help understand the robustness of the reward design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ENHhaDbye5", "forum": "SwHnok3JtD", "replyto": "SwHnok3JtD", "signatures": ["ICLR.cc/2026/Conference/Submission6850/Reviewer_yewN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6850/Reviewer_yewN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770843309, "cdate": 1761770843309, "tmdate": 1762919107744, "mdate": 1762919107744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework for collaborative device-cloud LLM inference where lightweight on-device models learn to autonomously decide when to offload queries to powerful cloud LLMs. Unlike existing approaches using external routers, the method integrates routing capability directly into post-training through a reward maximization problem with hierarchical rewards. The authors develop a Group-Adaptive Policy Gradient (GAPG) algorithm featuring unbiased group-level gradient estimation and adaptive prompt filtering to balance local problem-solving with cloud assistance. Experiments across mathematical benchmarks demonstrate the approach consistently outperforms baselines, maintains stable training, and significantly narrows the performance gap to full cloud LLM usage while respecting offloading constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ This work elegantly integrates routing optimization directly into the post-training process, eliminating the need for separate external routers. This unified approach allows the on-device LLM to simultaneously improve its problem-solving capabilities while learning judicious offloading strategies.\n+ The group-level policy gradient addresses GRPO's limitations in collaborative settings, while adaptive prompt filtering effectively enforces cloud usage constraints, preventing the model from over-relying on external assistance during training.\n+ The evaluation demonstrates performance across multiple model sizes (1B to 3B parameters) and diverse mathematical benchmarks."}, "weaknesses": {"value": "- The work lacks clear motivation and validation for why routing capability should be embedded in the on-device LLM with a first-device-then-cloud cascade way rather than using separate routers for directly choosing which LLM. The optimization goal appears vague—it achieves significantly lower accuracy than cloud LLM alone (cf. Table 1), yet doesn't explicitly quantify or optimize the cost trade-off between on-device computation and cloud invocation, making it difficult to assess whether the approach provides genuine practical benefits. In particular, accuracy should be the first objective in real-world deployment.\n\n- The core technical contribution is incremental. The group-level policy gradient (Proposition 3.1) closely resembles existing GRPO with minor modifications. The only novel component—adaptive prompt filtering—is a simple heuristic that selects training prompts based on response correctness ratios. This lacks theoretical justification and appears to be an ad-hoc empirical design without principled foundations for why this specific filtering strategy is optimal.\n\n- Evaluation is restricted exclusively to mathematical reasoning tasks (Countdown and MATH benchmarks). The paper completely ignores other important domains like general question answering, commonsense reasoning, coding, summarization, or multi-turn dialogue. This severely limits claims about generalizability, as mathematical problems may have unique characteristics that don't transfer to other collaborative inference scenarios where routing decisions differ fundamentally.\n\n- The paper compares against only one routing method (Task-Tuning&Router using DeBERTa) and naive random offloading, missing numerous recent works in LLM routing and cascade systems. Notable omissions include FrugalGPT, RouteLLM variants, context-aware cascading policies, and ensemble-based routing methods cited in related work but never experimentally compared, weakening claims of superiority.\n\n- The paper provides no ablation analysis to isolate the contribution of individual components. Critical questions remain unanswered: How much does hierarchical reward design contribute versus the gradient estimator? What's the impact of different reward weights (αₐ, αc, αf)? How sensitive is performance to the filtering ratio ρ? Without ablations, it's impossible to understand which design choices actually matter or whether simpler alternatives might work equally well."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ReKF9CHhr", "forum": "SwHnok3JtD", "replyto": "SwHnok3JtD", "signatures": ["ICLR.cc/2026/Conference/Submission6850/Reviewer_Xpht"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6850/Reviewer_Xpht"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797215310, "cdate": 1761797215310, "tmdate": 1762919107366, "mdate": 1762919107366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses the efficiency-accuracy trade-off in device-cloud LLM collaboration by proposing a reinforcement learning-based unified framework that eliminates external routers. \nThe core idea is to train the on-device LLM to simultaneously enhance its problem-solving ability and learn routing strategies during post-training. Key innovations include: (1) a collaboration-aware hierarchical reward that guides the on-device LLM to call the cloud LLM (e.g., DeepSeek-R1) only when it cannot solve tasks independently; (2) a Group-Adaptive Policy Gradient (GAPG) algorithm with an unbiased group-level gradient estimator and adaptive prompt filtering (controlling cloud usage). \nExperiments on mathematical benchmarks show the method outperforms baselines (e.g., task-tuning only, router-based offloading) in accuracy, narrows the performance gap to full cloud LLM, and maintains stable cloud usage under constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Eliminates External Router Limitations: Unlike prior two-stage pipelines (task-tuning + separate router), the framework embeds routing into the on-device LLM’s post-training. This avoids redundant computation/storage of routers, resolves the router’s inability to judge task difficulty from prompt surface patterns, and simplifies system complexity— a critical practical improvement.\n- Theoretically Grounded RL Design: The GAPG algorithm addresses GRPO’s key flaws: the group-level gradient estimator  ensures unbiased optimization of the reward objective\n- The method is validated across diverse on-device models (1B–3B parameters) and benchmarks. It consistently outperforms baselines, maintains accuracy gains when cloud call ratios vary, and transfers well to unseen tasks— demonstrating robustness beyond training datasets."}, "weaknesses": {"value": "- Limited Task Scope to Mathematics: All experiments focus on mathematical reasoning tasks (arithmetic puzzles, competition math). The framework’s effectiveness for other domains (e.g., natural language understanding, code generation) is untested— especially critical since prompt design  and reward logic  may not generalize to subjective tasks.\n- The method relies on historical user interaction data to train routing strategies, but it does not address cold-start for new users/on-device models (no prior task knowledge) or dynamic cloud constraints (e.g., real-time bandwidth limits, cloud cost fluctuations). These are major gaps for real-world deployment.\n- Overhead of Group Sampling in GAPG: The algorithm requires sampling G responses (G=8 in experiments) per prompt for group-level gradient estimation. This increases training computation, with no analysis of how G impacts training efficiency/performance."}, "questions": {"value": "- How would the framework adapt to non-mathematical tasks (e.g., text summarization, code debugging)? For subjective tasks where \"accuracy\" is hard to quantify (e.g., summary quality), would the hierarchical reward need re-design (e.g., human preference-based rewards) require domain-specific adjustments?\n- Can the method be extended to cold-start scenarios (new on-device models or users with no task history)? For example, could pre-trained routing priors (e.g., from similar tasks) or auxiliary signals (e.g., item category for recommendation) initialize the on-device LLM’s routing ability before task-specific post-training?\n- What is the computational-efficiency trade-off of group size G in GAPG? Does reducing G (e.g., G=4) lower training overhead without sacrificing performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Byw9C4JKrx", "forum": "SwHnok3JtD", "replyto": "SwHnok3JtD", "signatures": ["ICLR.cc/2026/Conference/Submission6850/Reviewer_faeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6850/Reviewer_faeX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844400381, "cdate": 1761844400381, "tmdate": 1762919106950, "mdate": 1762919106950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework for device-cloud LLM collaboration, where an on-device model is post-trained to jointly solve problems and decide when to delegate difficult queries to a cloud LLM under a budget . The authors identify that standard RL algorithms like GRPO fail on this task, as their reward normalization flattens the crucial reward hierarchy (e.g., correct answer > call for help), causing the model to learn a suboptimal, \"always-delegate\" policy . To fix this, they introduce the Group-Adaptive Policy Gradient (GAPG), a novel RL algorithm featuring an un-normalized policy gradient to respect the reward hierarchy and adaptive prompt filtering to enforce the cloud budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The core idea of embedding the routing decision within the on-device LLM is a practical and intuitive contribution. It correctly leverages the internal state of the model to inform the routing decision, which is a clear advantage over \"prompt-only\" routers.\n\nThe paper also provides a clear analysis of why a state-of-the-art RL algorithm like GRPO fails in this setting (Section 3.3). The identification of the \"misalignment of normalized advantage\" (where high-priority sparse rewards and low-priority dense rewards are treated as equal post-normalization) is a sharp insight that provides a solid motivation for their proposed algorithm."}, "weaknesses": {"value": "Limited Task Diversity: The experiments are exclusively focused on mathematical and arithmetic reasoning tasks. These tasks are well-suited for this problem as they have clear, binary correctness signals. However, the paper's central claim is about a general collaborative framework. It is unclear how this method would perform on more open-ended, creative, or subjective tasks (e.g., summarization, dialogue, writing) where defining a simple, non-binary reward signal r(x,y) is a significant challenge in itself.\n\nLimited Comparison to Suitable RL Alternatives: While the analysis of GRPO's failure is sound, the paper does not justify why a new algorithm (GAPG) is necessary over other, more established RL frameworks. The problem is formulated as a classic constrained, multi-objective problem. Furthermore, the paper omits comparisons to entire classes of algorithms designed for this setup, such as Multi-Objective RL (MORL) (e.g., Hayes et al., 2022), which are built to handle competing objectives. Moreover, comparison with DPO and CPO methods is missing.\n\nComparison with the router-based approach: The paper's experimental comparison to the \"Task-Tuning&Router\" baseline is weak, as it appears to compare against a \"prompt-only\" router. The authors' main critique—that it is difficult to assess task difficulty from the prompt alone—is valid, but this is a known limitation. A much stronger and more practical baseline would be a \"post-inference\" router that takes the on-device LLM's full response and/or confidence as input. This type of \"quality-gap\" predictor, which is also mentioned in the work they cite (Ding et al., 2024) would serve as a much stronger and more relevant baseline for comparison.\n\n\n[CF Hayes, R Rădulescu, et al. A Practical Guide to Multi-Objective Reinforcement Learning and Planning]\n\n[D Ding et al. Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing]"}, "questions": {"value": "1. In Figure 5, the \"call-for-cloud ratio\" is treated as an independent variable. Could you clarify how this graph was generated for the \"Collaboration-Aware Tuning\" (GRPO) baseline? Given the claim in Section 3.3 that this method collapses to an \"always-call-for-cloud\" policy, how was it forced to operate under lower, fixed budgets (e.g., 20%) at test time?\n\n2. The proposed GAPG algorithm still requires calls to the cloud LLM during training (Algorithm 1, Step 6) to compute the coordination reward. This introduces a significant training cost. The x-axis in Figures 3 & 4 (training iterations) does not capture this. Could you provide an analysis of the total training cost (e.g., number of cloud calls) required by GAPG versus the baselines (including the two-stage cost of training the router) to achieve the final reported accuracy?\n\n3. The 'Task-Tuning&Router' baseline is presented as a single final performance point (⋆) in Figures 3 and 4. To help verify that this baseline was trained sufficiently and to understand its learning dynamics, could you provide a plot showing this baseline's accuracy as a function of its router-training iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s8olmTS6xA", "forum": "SwHnok3JtD", "replyto": "SwHnok3JtD", "signatures": ["ICLR.cc/2026/Conference/Submission6850/Reviewer_6Grk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6850/Reviewer_6Grk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852805247, "cdate": 1761852805247, "tmdate": 1762919106387, "mdate": 1762919106387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}