{"id": "PYlbrgaJKp", "number": 2896, "cdate": 1757298521012, "mdate": 1759898120501, "content": {"title": "Beyond Fixed Budgets: Dynamic Reasoning Efficiency Reward for Large Language Model", "abstract": "The \"slow thinking\" paradigm has been widely validated to enhance the reasoning capabilities of large language models, but it also introduces reasoning inefficiency: models may overthink simple problems while prematurely shifting their reasoning paths when tackling complex problems. To address this, we propose AdapThink, a simple yet efficient post-training framework designed to control preferences for ``slow thinking'' pattern adaptively. Unlike directly imposing length budgets or setting overlong filters, AdapThink leverages group-level length distributions and reflective word distributions to construct reasoning process rewards and introduce a two-stage sampling strategy aimed at maximizing group diversity.\nExperimental results demonstrate that when post-training two DeepSeek-distilled Qwen models under a context length limit of only 2K tokens, AdapThink achieves a 27\\% improvement in convergence rewards compared to the GRPO baseline. Notably, when testing models with a 32K token limit, AdapThink also achieves 12.6\\% improvement over base model in several mathematical benchmarks.", "tldr": "", "keywords": ["Overthinking; LLM post-training; Efficient CoT"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7c1d0ac7fb30b2d0371e4a75d3d9ea5c7ee2d06.pdf", "supplementary_material": "/attachment/6f40a6069b2894baa154438cb53c47168e55c0ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AdapThink, a post-training framework designed to enhance reasoning efficiency in large language models (LLMs) by adaptively balancing “slow thinking” and token efficiency. Unlike prior works that apply static token budgets or uniform length penalties, AdapThink introduces: \n1. Group-relative reasoning process rewards, which quantify reasoning efficiency using the statistical distribution of reasoning length and reflection-related words.\n2. Diversity-aware sampling, which promotes diverse reasoning trajectories while preserving accuracy.\nExperiments on multiple mathematical reasoning datasets (e.g., MATH-500, AIME 2024/2025, AMC 2023) show that AdapThink outperforms GRPO and other length-control methods, achieving 6–7% higher accuracy with fewer tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear motivation: The paper addresses a timely challenge in reasoning efficiency for LLMs, especially as “slow thinking” paradigms (e.g., O1, DeepSeek R1) become mainstream.\n\nThe proposed method is technically sound: The group-relative reasoning reward is reasonable, which shifts from token-level control to process-level adaptation.\n\nThe experiments show that the proposed AdapThink consistently improves performance on both reasoning accuracy and token efficiency across datasets."}, "weaknesses": {"value": "1. The limited novelty. While AdapThink introduces new reward formulations, the overall framework (RL-based post-training with process rewards) resembles prior GRPO-like or MERA approaches, with only moderate innovation in principle. It is some tricks more than a solid method.\n\n2. The adopted models are small-scale (1.5B and 7B). However, the overthinking problems mainly happened with large-scale models with deep reasoning abilities.\n\n3. Experiments are limited to mathematical reasoning. Although an OOD test is included, these evaluations are shallow and do not convincingly demonstrate generalization to non-mathematical reasoning."}, "questions": {"value": "1. Does AdapThink maintain stability when scaling to much larger context lengths?\n2. How sensitive is AdapThink to the choice of the reflection-word list or the threshold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SMINU4DP2Z", "forum": "PYlbrgaJKp", "replyto": "PYlbrgaJKp", "signatures": ["ICLR.cc/2026/Conference/Submission2896/Reviewer_9FJf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2896/Reviewer_9FJf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726093756, "cdate": 1761726093756, "tmdate": 1762916433905, "mdate": 1762916433905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles overthinking/underthinking in LLM reasoning and proposes AdapThink, a post-training framework that adaptively regulates “slow thinking” without fixed length budgets. It combines a group-relative process reward (using response length and reflection-word frequency) with a diversity-aware sampling scheme."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conducted systematic and comprehensive ablation experiments for the proposed method.\n2. The paper is weel-written and orignized."}, "weaknesses": {"value": "1.\tThe “group-relative process reasoning\nreward” method proposed in this paper, judging from its formulation (Eq. 4), appears to primarily guide the model to optimize its reasoning process on easy problems (i.e., those the model can already answer correctly), while providing little effective guidance for the reasoning on difficult problems.  I did not see results demonstrating that training with this method effectively alters the distribution pattern presented in Figure 2.\n2.\tComparing the official DeepSeek-R1 results (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) with the data in Table 1 of this paper, the “Original” model after GRPO training actually underperforms the pre-training base model on certain benchmarks. For example, DeepSeek-R1-Distill-Qwen-7B attains 92.8 on MATH-500 (pass@1) and 55.5 on AIME 2024 (pass@1), whereas the “Original” model in this paper achieves only 86.9 and 53.1, respectively. This result is somewhat anomalous and may affect the validity of the comparative experiments reported.\n3.\tFrom Fig. 4, the length of the model’s responses shows a steadily decreasing trend. However, according to the DeepSeek-R1 report and related replication studies, performance improvements during training are typically accompanied by increasing response length. Therefore, if the experiment in Fig. 4 were continued, would the response length exhibit an inflection point?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2wmxMGQufw", "forum": "PYlbrgaJKp", "replyto": "PYlbrgaJKp", "signatures": ["ICLR.cc/2026/Conference/Submission2896/Reviewer_6DnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2896/Reviewer_6DnZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834772714, "cdate": 1761834772714, "tmdate": 1762916433736, "mdate": 1762916433736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose AdapThink framework to fix overthinking/underthinking in LLMs. It use group-level reasoning process reward that compare each response with group average length and reflection words. Also use diversity sampling to get varied reasoning patterns. Test on DeepSeek-Qwen models, train with 2K tokens and get 27% improvement over GRPO baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-written and proposes a simple and intuitive method that is easy to understand and follow.\n- This paper conducts comprehensive experiments and the ablation studies have shown that AdapThink can teach the model general principles, not just compress output."}, "weaknesses": {"value": "- This method is too simplistic - just comparing with group average and counting reflection words. No fundamental improvement over existing approaches.\n- This method was only tested on two models from the same family (DeepSeek-Qwen 1.5B/7B). Meanwhile, the improvements are marginal, which is not sufficient to justify the effectiveness of AdapThink."}, "questions": {"value": "Have you considered testing AdapThink on non-math domains like code generation, commonsense reasoning？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L0Lr0IBaJt", "forum": "PYlbrgaJKp", "replyto": "PYlbrgaJKp", "signatures": ["ICLR.cc/2026/Conference/Submission2896/Reviewer_udEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2896/Reviewer_udEf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923019599, "cdate": 1761923019599, "tmdate": 1762916433586, "mdate": 1762916433586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a post‑training framework for reasoning LLMs that augments GRPO with a group‑relative process reward that favors responses whose length and count of reflection words are below their group mean, with a cosine weight that increases when group accuracy is high. They also propose a two‑stage, diversity‑aware sampling procedure that upsamples responses, then down‑samples to maximize entropy over binned pairs while keeping group correctness non‑degenerate."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Instead of imposing static budgets, the method uses the group’s length and reflection word distributions and the model’s confidence to determine how much slow thinking is appropriate. The proposed method achieves a 27 % improvement in convergence reward over the GRPO baseline for 2k-token context and 12.6 % accuracy improvement over the base model for 32k-token context. On AIME‑2025 and MATH‑500, AdapThink outperforms baselines such as GRPO and LCPO both in pass@1 and token consumption. These results indicate that the method does not merely cut tokens, but translates the efficiency into measurable accuracy gains."}, "weaknesses": {"value": "Using counts of hand‑picked words as the target for reflection control is brittle, language‑ and style‑dependent, and can easily be gamed. The ablation itself shows penalizing check/verify hurts, which contradicts the claim that reflection words are a good proxy to control.\n\nThe observation that incorrect answers have more tokens and reflection markers does not justify a reward that penalizes those markers causal‑mechanistically. Competing works already handle difficulty and underthinking more directly."}, "questions": {"value": "Please provide a theoretical argument why group‑relative proxies of style (length, word‑counts) should generalize across tasks/model scales, or replace them with learned process rewards.\n\nPlease elaborate more on the novelty with respect to related work such as LCPO, DAST, DEER, MERA, TIP/NoWait, CoD/C3oT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S81PTQ0cF8", "forum": "PYlbrgaJKp", "replyto": "PYlbrgaJKp", "signatures": ["ICLR.cc/2026/Conference/Submission2896/Reviewer_xE1y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2896/Reviewer_xE1y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762178797406, "cdate": 1762178797406, "tmdate": 1762916433420, "mdate": 1762916433420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}