{"id": "KVWaCzJrrq", "number": 3009, "cdate": 1757316005649, "mdate": 1759898113732, "content": {"title": "AutoDrive-R²: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving", "abstract": "Vision–Language–Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R², a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR²-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.", "tldr": "", "keywords": ["Applications", "Robots", "Vision–Language–Action Models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52f7f1b08939a27b8a39202b9a75a022bfe95316.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method based on VLAs for trajectory prediction in autonomous driving. The proposed method consists of a two-stage process: 1) a chain-of-thought dataset based on NuScenes used for fine-tuning of an existing VLA (the paper uses Qwen 2.5 VL) so that it learns to \"think\" through the trajectory generation; 2) a reinforcement learning stage based on GRPO to align the model's predictions with physical trajectory properties (e.g. position) and model output format. The method is evaluated on the NuScenes and Waymo datasets and compared to other methods and ablated baselines, showing significant gains in positional accurate of predicted trajectories."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper builds on recent progress in using VLAs for trajectory prediction, improving model accuracy.\n- The proposed dataset is novel and could be broadly useful beyond this particular work.\n- The empirical evaluation shows significant gains.\n- The writing is clear (for me)."}, "weaknesses": {"value": "Overall I enjoyed reading this paper, though I noticed a couple of issues:\n\n1) Limited metrics used in evaluation: the evaluation only uses a L2-based metric on predicted vs true positions. But it does not measure other important aspects, such as feasibility (based on vehicle model) or rule/constraint violations of generated trajectories. Prior literature on trajectory prediction has developed a number of metrics to provide a more complete picture (see next point).\n\n2) Limited discussion of work prior to VLA: related to above comment, I find that the paper lacks discussion of works prior to the recent VLA-based research. There is a large body of work on trajectory prediction in autonomous driving, and it would help readers to see the relationship with prior works. For example, prior works have developed evaluation metrics for trajectory prediction e.g. [1,2]; and there is also prior work on explainable AI in autonomous driving e.g. [3,4], which is one of the stated motivations for the proposed CoT fine-tuning method in this paper.\n\n[1] https://arxiv.org/abs/2210.06106 (RAL 2023)\n\n[2] https://arxiv.org/abs/2203.08251 (IROS 2022)\n\n[3] https://arxiv.org/abs/2402.10086 (T-ITS 2024)\n\n[4] https://arxiv.org/abs/2002.02277 (ICRA 2021)\n\nI think the paper could be strengthened by addressing these two aspects."}, "questions": {"value": "Q1: The \"self-correction\" examples presented in appendix A.3 are not clear to me. In what sense does the red text in Fig4 show an \"Aha moment\"? I can't tell what error was detected and how it was corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R8xkuVOvX3", "forum": "KVWaCzJrrq", "replyto": "KVWaCzJrrq", "signatures": ["ICLR.cc/2026/Conference/Submission3009/Reviewer_vs3C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3009/Reviewer_vs3C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404138516, "cdate": 1761404138516, "tmdate": 1762916500351, "mdate": 1762916500351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AutoDrive-R^2, a novel VLA framework designed to enhance the reasoning and self-reflection capabilities of autonomous driving systems, addressing the limitations of existing methods such as physically infeasible trajectory generation and inadequate reasoning for complex scenarios. The framework adopts a two-stage training approach: in the first stage, a CoT dataset named nuScenesR^2-6K (with 6,000 image-trajectory pairs) is constructed for SFT, which guides the model through a four-step logical chain to build cognitive connections between input information and output trajectories. In the second stage, a physics-grounded reward framework integrated with GRPO is employed for RL, incorporating spatial alignment, vehicle dynamics, and temporal smoothness constraints to ensure trajectory feasibility. Experimental results on nuScenes and Waymo datasets demonstrate that AutoDrive-R^2 achieves state-of-the-art  performance and robust zero-shot generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "nuScenesR^2-6K is the first dataset in autonomous driving that integrates self-reflection for validation, providing detailed reasoning chains to bridge input information and output trajectories, which enhances model interpretability.\n\nThe combination of SFT with structured CoT reasoning and RL with physics-grounded rewards effectively addresses both reasoning inadequacy and physical infeasibility of trajectories\n\nAutoDrive-R² outperforms state-of-the-art methods on nuScenes and Waymo datasets, with significant error reductions and robust zero-shot capabilities, validating its effectiveness and generality."}, "weaknesses": {"value": "The RL stage with GRPO requires generating multiple candidate responses, which may introduce high computational overhead, but the manuscript does not discuss inference speed or real-time deployment feasibility.\n\nAlthough the paper emphasizes the role of self-reflection in the four-step reasoning chain, it does not clearly explain how the model corrects inconsistent trajectories during self-reflection. For example, there is no detailed description of the decision rules (e.g., threshold for determining trajectory inconsistency) or the specific adjustment strategies (e.g., how to modify velocity or steering angle) adopted in the self-reflection stage.\n\nThis paper does not analyze the computational overhead (e.g., inference time per trajectory, GPU memory usage) or compare it with lightweight baseline methods. This may restrict the practical deployment on edge devices with limited computing resources.\n\nTable 2 appears to be outside the page content"}, "questions": {"value": "For the dataset: How were the manual annotations of the CoT reasoning process validated? How to ensure the consistency of reasoning steps? Additionally, since the dataset is derived from nuScenes, does it inherit the scene bias of the original dataset (e.g., urban road dominance), and if so, how does this affect the model's generalization to non-urban scenarios (e.g., highways or rural roads)?\n\nThe paper sets all weight coefficients (λ_pos, λ_ste, λ_vel, λ_tem) to 1 in experiments. Have you tested the impact of different weight combinations on performance? Besides, have you analyzed the impact of GRPO hyperparameters (e.g., number of candidate responses, beta in KL-divergence) on trajectory prediction accuracy and training stability?\n\nFor the zero-shot performance on Waymo, this paper attributes the excellent zero-shot generalization to the model's structured reasoning capabilities, but it does not compare with other methods that also claim zero-shot adaptation. Could you explain why your method has a more significant zero-shot advantage, and whether the dataset contains scene features that are common to both nuScenes and Waymo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Z8edGwrcE", "forum": "KVWaCzJrrq", "replyto": "KVWaCzJrrq", "signatures": ["ICLR.cc/2026/Conference/Submission3009/Reviewer_TqXQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3009/Reviewer_TqXQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442702038, "cdate": 1761442702038, "tmdate": 1762916499962, "mdate": 1762916499962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoDrive-R², a novel Vision–Language–Action (VLA) framework for autonomous driving that aims to enhance both reasoning interpretability and trajectory feasibility. Supervised Fine-Tuning (SFT) using a new Chain-of-Thought dataset (nuScenesR²-6K) that instills structured reasoning and self-reflection. Reinforcement Learning (RL) using Group Relative Policy Optimization (GRPO) with a physics-grounded reward framework that integrates spatial alignment, steering dynamics, and temporal smoothness, ensuring physically feasible trajectories. The method is evaluated on nuScenes and Waymo benchmarks, achieving state-of-the-art (SOTA) performance with substantial reductions in L2 trajectory errors compared to EMMA+ and DriveVLM, and demonstrating strong zero-shot generalization. The work positions itself at the intersection of multimodal reasoning and embodied intelligence, extending recent reasoning-incentivized LLM/RL techniques (e.g., DeepSeek-R1) to the autonomous driving domain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces reasoning-incentivized CoT learning and self-reflection for VLA models — a clear conceptual advance over purely data-driven planners.\n2. Sound integration of SFT and GRPO with well-formulated physics-aware rewards; rigorous ablations confirm design choices.\n3. Large quantitative gains on nuScenes (Avg L2 error 0.19 m vs 0.29 m for EMMA+) and Waymo (0.20 m vs 0.30 m), plus zero-shot transferability.\n4. The four-stage reasoning framework (Observation–Calculation–Logic–Reflection) is both intuitive and interpretable."}, "weaknesses": {"value": "1. Although the nuScenesR²-6K dataset is novel, 6k samples may not capture full traffic complexity. The manual synthesis with Qwen2.5-VL could introduce distributional bias.\n2. Experiments are open-loop trajectory predictions; it remains unclear how the system performs in end-to-end control or real-time deployment.\n3. The GRPO ablations demonstrate quantitative gains, but lack qualitative discussion on how each reward term alters trajectory smoothness or safety metrics."}, "questions": {"value": "1. How does AutoDrive-R² perform under closed-loop simulation or in real-world deployment (e.g., CARLA)? Would the physics-grounded reward suffice without explicit simulator feedback?\n2. How were the CoT reasoning steps validated for correctness or consistency during dataset construction? Were human checks performed post-Qwen2.5-VL synthesis?\n3. Could you show qualitative examples where AutoDrive-R² fails (e.g., multi-agent interactions or occlusions) and explain whether the reflection step mitigates or worsens these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pKjxp7LxJA", "forum": "KVWaCzJrrq", "replyto": "KVWaCzJrrq", "signatures": ["ICLR.cc/2026/Conference/Submission3009/Reviewer_okCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3009/Reviewer_okCa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668459470, "cdate": 1761668459470, "tmdate": 1762916498676, "mdate": 1762916498676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The application of VLA models in the field of autonomous driving has been attracting increasing attention. Among their capabilities, reasoning and self-reflection play a pivotal role. This paper trained a VLA model using a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to equip it with enhanced reasoning and reflective abilities. Specifically, This paper introduces a reasoning dataset that incorporates a Chain-of-Thought (CoT) process, along with a physics-compliant reward framework to guide model training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors construct a dataset and a reward framework grounded in practical problems; overall, the work is solid and technically sound. The paper makes clear contributions in dataset design and in the formulation of the reward function."}, "weaknesses": {"value": "While the authors identify salient issues with VLA models in autonomous driving and present technically solid efforts to address them, the overall solution exhibits limited novelty: it defines conventional tasks, applies standard approaches, and consequently yields expected results. The paper would benefit from a clearer articulation of the challenges encountered during the design of the solution and the insights gleaned from addressing them."}, "questions": {"value": "- The paper primarily trains a VLA model to predict future vehicle trajectories. What advantages does this approach offer over rule‑based prediction and purely data‑driven prediction methods? VLA models, trained with textual data, indeed have strengths in language understanding and reasoning, but they tend to be limited in precise numerical computation. Would employing a VLA model for fuzzy decision‑making be more appropriate?\n- The paper uses L2 error to indirectly assess the performance improvement of the trained model. This provides some evidence of data and reward effectiveness; however, more direct methods demonstrating gains in reasoning, computation, and reflection would make the contribution clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9v6ZfnoLJp", "forum": "KVWaCzJrrq", "replyto": "KVWaCzJrrq", "signatures": ["ICLR.cc/2026/Conference/Submission3009/Reviewer_3jDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3009/Reviewer_3jDT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725454451, "cdate": 1761725454451, "tmdate": 1762916497896, "mdate": 1762916497896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}