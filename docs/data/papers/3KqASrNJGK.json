{"id": "3KqASrNJGK", "number": 25566, "cdate": 1758369136554, "mdate": 1759896715353, "content": {"title": "But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors", "abstract": "Detecting subtle forms of dishonesty like sycophancy and manipulation in Large Language Models (LLMs) remains challenging for both humans and automated evaluators, as these behaviors often appear through small biases rather than clear false statements. We introduce Judge Using Safety-Steered Alternatives (JUSSA), a novel framework that employs steering vectors not to improve model behavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA applies steering vectors during inference to generate more honest alternatives, providing judges with contrastive examples that make subtle dishonest patterns easier to detect. While existing evaluation methods rely on black-box evaluation, JUSSA uses model internals to create targeted comparisons from single examples. We evaluate our method on sycophancy detection and introduce a new manipulation dataset covering multiple types of manipulation. Our results demonstrate that JUSSA effectively improves detection accuracy over single-response evaluation in various cases. Analysis across judge models reveals that JUSSA helps weaker judges on easier dishonesty detection tasks, and stronger judges on harder tasks. Layer-wise experiments show how dishonest prompts cause representations to diverge from honest ones in middle layers, revealing where steering interventions are most effective for generating contrastive examples. By demonstrating that steering vectors can enhance safety evaluation rather than just modify behavior, our work opens new directions for scalable model auditing as systems become increasingly sophisticated.", "tldr": "We use steering vectors to obtain alternative, honest responses, helping external LLM-judges detect subtle instances of dishonest or manipulative behavior.", "keywords": ["LLM-as-a-judge", "steering vectors", "safety", "manipulation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a65cb9fcf2da8afeaac51846dbfb003661d61b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces JUSSA, a framework that uses steering vectors to help large language model judges better detect subtle dishonesty such as sycophancy and manipulation by generating more honest contrastive alternatives. Experiments on new manipulation and sycophancy datasets show that JUSSA improves dishonesty detection for weaker judges on simple tasks and stronger judges on complex ones by increasing the score difference between honest and dishonest responses.\n\nOverall, this work presents a creative and promising idea: using steering not to align but to evaluate models. The approach is simple and conceptually distinct from existing methods. However, the experimental scope and comparative analysis are limited, making it difficult to judge the method’s relative effectiveness or scalability. A stronger positioning within the literature and additional baselines would considerably improve the paper’s impact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and under-explored problem: detecting dishonesty in LLM behavior and improving judge reliability. This direction is timely and impactful for the trustworthy evaluation of AI systems.\n- The core idea (repurposing activation steering as a means to provide contrastive auditing) is novel and conceptually strong. It reframes steering from alignment enforcement to judgment assistance, allowing evaluators to expose hidden model biases through counterfactual honesty generation.\n- The method is lightweight and efficient, requiring minimal data and computational resources compared to prior large-scale steering approaches. This makes it accessible and practical across different model families.\n- The paper introduces a carefully curated dataset for nuanced dishonesty detection. The manual curation and validation steps demonstrate attention to quality and ensure realistic coverage of manipulative behaviors that are difficult for LLMs to detect."}, "weaknesses": {"value": "- **Limited Related Work Coverage.** While the paper provides a focused and relevant related work section discussing dishonesty benchmarks and the foundational steering vector methods underpinning the proposed approach, it falls short of adequately situating its contributions within the broader landscape of techniques aimed explicitly at improving judge LLM capabilities. Instead, it primarily focuses on methods closely aligned with its own framework. I would encourage to expand their related work discussion to a broader and more systematic review of recent research on LLM-based evaluation and dishonesty detection would better contextualize the contribution and demonstrate deeper engagement with parallel efforts in this space.\n\n- **Missing Baseline Comparisons.** The paper does not provide direct quantitative comparisons to recent judge-improvement frameworks (e.g., [1][2]. Without these, it is difficult to assess whether JUSSA offers state-of-the-art performance or mainly a conceptual novelty.\n\n- **Limited Model Scale.** Experiments are restricted to Gemma-2-2B. While this choice demonstrates feasibility, it does not fully capture challenges that arise when scaling steering techniques to larger, more complex models. Validation on higher-capacity models (e.g., Gemma-4B or Llama-8B) would strengthen claims of generality and robustness.\n\n- **Benchmark Generalization.** The evaluation relies primarily on the authors’ own datasets. Applying JUSSA to existing honesty benchmarks (e.g., MASK [3], BeHonest [4]) would allow fairer and more interpretable comparisons against prior work.\n\n  - [1] Zhang, Qiyuan, et al. \"Crowd comparative reasoning: Unlocking comprehensive evaluations for llm-as-a-judge.\" arXiv preprint arXiv:2502.12501 (2025).‏\n  - [2] Fujinuma, Yoshinari. \"Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge.\" arXiv preprint arXiv:2510.18196 (2025).‏\n  - [3] Ren, Richard, et al. \"The mask benchmark: Disentangling honesty from accuracy in ai systems.\" arXiv preprint arXiv:2503.03750 (2025).‏\n  - [4] Chern, Steffi, et al. \"Behonest: Benchmarking honesty in large language models.\" arXiv preprint arXiv:2406.13261 (2024).‏"}, "questions": {"value": "1. Why weren't other judge-improvement baselines included in the evaluation?\n2. How does JUSSA’s performance and efficiency (practicality) compare to other judge improvement methods?  \n3. Do you anticipate challenges when applying steering-based auditing to larger architectures (e.g., Llama-8B)? If so, what modifications might be required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rCbnrqHxsm", "forum": "3KqASrNJGK", "replyto": "3KqASrNJGK", "signatures": ["ICLR.cc/2026/Conference/Submission25566/Reviewer_To6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25566/Reviewer_To6N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856713398, "cdate": 1761856713398, "tmdate": 1762943477382, "mdate": 1762943477382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Jussa (Judge Using Safety-Steered Alternatives), a framework that uses steering vectors to generate honest alternative responses, helping LLM judges detect subtle forms of dishonesty. The paper also introduces a new manipulation dataset covering different categories across three manipulation types."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- using steering vectors to help judges instead of fixing models is novel to the best of my knowledge and also interesting\n- the method is clear and the empirical results are promising"}, "weaknesses": {"value": "- limited models tested: for the LLM-agent it was tested a single model (gemma-2b) making this study limited because you cannot generalize the finding without looking if other models may exhibit different patterns\n- the steering vector is trained on only one example from a single category. This poses a limitation as it's not understood how the sample was chosen, if the results are sensitive on choosing this sample etc\n- the dataset being synthetically generated raises concerns on its quality\n- missing comparison with several related works such as: [1], [2]\n- (minor) figure 1 is blurred and this should be fixed\n\n[1] Gu, Jiawei, et al. \"A survey on llm-as-a-judge.\" arXiv preprint arXiv:2411.15594 (2024).\n[2] Zheng, Lianmin, et al. \"Judging llm-as-a-judge with mt-bench and chatbot arena.\" Advances in neural information processing systems 36 (2023): 46595-46623."}, "questions": {"value": "- does the findings are similar if another LLM-agent is employed? (maybe larger ones)\n- ablation studies on how to choose the samples to optimize the steering vector."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W0H1kTY3h8", "forum": "3KqASrNJGK", "replyto": "3KqASrNJGK", "signatures": ["ICLR.cc/2026/Conference/Submission25566/Reviewer_8vA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25566/Reviewer_8vA5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942986499, "cdate": 1761942986499, "tmdate": 1762943477102, "mdate": 1762943477102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JUSSA, a framework that doesn’t steer the task model to be safer for its own sake, but uses a single sample steering vector to generate \"more honest\" alternative responses that are then shown to an LLM judge for contrastive evaluation. The key hypothesis is that judges struggle to spot subtle dishonesty in isolation, but become more accurate when they can compare the original response against a steered \"honest\" alternative produced from the same agent. The steering vector is trained by promotion steering to increase the likelihood of the base (honest) response when the model is prompted with the provocation; at inference time it is applied to layer 10 (residual stream) of Gemma‑2b‑it to produce steered variants for both base and provoked prompts. Detection is scored via AUROC over judge assigned \"manipulation/sycophancy\" scores for base vs. provoked responses. \n\n\nTo test the idea, the authors adapt a sycophancy benchmark and introduce a model written manipulation dataset spanning 13 categories (later narrowed to 9 after QC). A paired judge sanity check and a 130‑example human study assert that provoked answers are more manipulative and that steering reduces manipulation while improving correctness. Main results: for manipulation detection, JUSSA improves AUROC for GPT‑4.1‑base judges from 0.893 -> 0.946 and Claude‑3.5‑Haiku from 0.859 -> 0.929. For GPT‑4.1 variants, JUSSA hurts nano on manipulation (0.756 -> 0.677) but helps base (0.777 -> 0.870), for sycophancy it helps small (nano 0.709 -> 0.848) yet hurts large (base 0.957 -> 0.907) judges. A layer wise analysis suggests steering is most effective in mid layers (~8-13) where representations of honest vs. provoked trajectories diverge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel framing: Repurposes steering vectors for judging rather than controlling model behavior. This reframing of steering as an evaluation aid is conceptually creative and aligns with recent interest in interpretability-aided evaluation.\n\n- Layer analysis: Provides a careful exploration of where steering is most effective, showing that layers 8–13 are most sensitive to honesty steering, consistent with known interpretability findings about mid-layer semantic representations.\n\n- Empirical signal: Demonstrates consistent improvements in AUROC on manipulation detection across most categories for major models like GPT-4.1-base and Claude Haiku, suggesting practical potential in enhancing judge discrimination.\n\n- Lightweight setup: Uses a single sample \"promotion steering\" approach to train an honesty vector efficiently, avoiding large labeled datasets and making replication feasible.\n\n- Human validation: Includes human evaluation confirming that provoked responses are more manipulative and that steering mitigates manipulation, lending some empirical support to the central hypothesis."}, "weaknesses": {"value": "Prompt leakage confound: In the judge prompt, the “steered” output is explicitly labeled as the “more honest alternative,” which likely biases the judge toward assigning it higher honesty scores. This framing undermines the central claim that JUSSA’s improvements arise from genuine model-level contrast rather than prompt wording.\n\n- Lack of proper baselines: The paper does not compare JUSSA against simple black box contrastive setups, such as generating a second \"objective\" or \"selfcritical\" response without steering. Without these, it is unclear whether steering contributes anything beyond giving the judge a comparison.\n\n- Limited generalization: The approach is tested only on one agent model (Gemma-2b-it). There are no results demonstrating cross model transfer or replication on other architectures like Llama or Mistral, limiting external validity.\n\n- Dataset construction issues: The manipulation dataset is model written and partially filtered after evaluation, with four categories dropped due to poor quality. This post-hoc pruning weakens claims about robustness and can inflate reported performance.\n\n- Missing statistical rigor: The results report raw AUROC values but omit confidence intervals, significance testing, or multi seed variation. Moreover, the sycophancy results show performance drops for stronger judges, yet this regression is not deeply analyzed."}, "questions": {"value": "Q1. Judge prompt confound: In the paper the judge prompt explicitly refers to “Response B – more honest alternative response.” Can you confirm whether this phrasing was included verbatim in your main experiments? If so, have you re-run the evaluation with neutral wording (e.g., “Response A” and “Response B”) to quantify how much of the AUROC gain remains once this label bias is removed?\n\nQ2. Contrastive baseline comparison: Since JUSSA provides a second, steered response for contrast, how much of the improvement comes simply from having any contrastive pair versus specifically from the steering vector? A strong rebuttal could include results for simple non steered alternatives (e.g., temperature sampling, \"be objective\" prompting, self critique generation) to show that the steering signal itself adds value.\n\nQ3. Cross model and reproducibility concerns: All experiments use Gemma-2b-it as the agent. Could you show at least one replication on a different base model (e.g., Llama 2/3 or Mistral or Qwen) to demonstrate that the single sample honesty vector generalizes? If not, can you discuss how architecture specific factors (layer size, activation norms, residual design) might affect steering transferability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QqckZKxLXI", "forum": "3KqASrNJGK", "replyto": "3KqASrNJGK", "signatures": ["ICLR.cc/2026/Conference/Submission25566/Reviewer_85f2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25566/Reviewer_85f2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959300712, "cdate": 1761959300712, "tmdate": 1762943476775, "mdate": 1762943476775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JUSSA (Judge Using Safety-Steered Alternatives): instead of steering a task model to behave honestly at deployment, it uses a lightweight steering vector during inference to produce a more-honest counterfactual answer from the same model, and gives the judge a pair (original vs. steered) to evaluate. The hypothesis is that contrast along an honesty axis helps LLM-judges detect subtle dishonesty (sycophancy/manipulation) better than single-response judging.\n\nThe intuition is clear - it is helpful to have a frame of reference for what \"good\" looks like. I imagine this is especially true for LLM judges.\n\nThe steering vector is learned from a single example via promotion steering on a middle layer’s residual stream, optimizing the model (under a provocation prompt) to prefer tokens of the honest/base response. Experiments use Gemma-2B-it as the agent and multiple judges (GPT-4.1 nano/mini/base; Claude-3.5 Haiku) on (i) an adapted sycophancy set and (ii) a new manipulation dataset (13 categories → 9 after quality filtering). AUROC improves with JUSSA for most manipulation categories with capable judges; small judges benefit notably on sycophancy, but JUSSA can mildly hurt when judges already perform near-optimally or when the task exceeds capacity. A layer study shows middle layers (~8–13) are most effective, aligning with measured representation divergence between honest and provoked trajectories.\n\nContributions: (1) a practical contrastive auditing recipe using steering for evaluation; (2) a manipulation dataset with human validation; (3) analysis of where honesty steering works best."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple, intuitive, and original idea: Reframes steering as an evaluation aid. Contrastive, honesty-steered alternatives give judges a cleaner signal than single-response scoring. As far as I'm aware, I have not seen steering used for this purpose.\n\n- Lightweight steering: Single-sample promotion steering suffices to generate informative counterfactuals.\n\n- Layer insight: Middle-layer effectiveness matches representation divergence analysis; actionable for activation-space methods.\n\n- Careful validation: Human annotators corroborate that provoked > base in manipulation, and steered-provoked reduces manipulation; four weak categories are transparently filtered out.\n\n- Candid limitations: Authors note where JUSSA can hurt (easy tasks with strong judges; hard tasks with weak judges)."}, "weaknesses": {"value": "- Missing baselines: No comparison to (a) k-sample self-contrast (two unsteered samples), (b) prompt-only “honesty” contrast (no steering), or (c) SAE-targeted/CAA steering as alternative ways to generate contrasts.\n- Limited models: Agent is only Gemma-2B-it; unclear robustness across agent sizes/architectures or different provocation styles.\n- Residual stylistic cues: Although disclaimers are partially analyzed, gains may still be partly driven by surface cues (length, boilerplate)? Stronger cue-controls would help.\n- Dataset external validity: Manipulation set is largely model-authored and curated; broader inclusion of human-written prompts would make it more realistic."}, "questions": {"value": "If the authors could address any of the weaknesses above, particularly around the baselines, I'd be open to change my reviews."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QOgrjarv8X", "forum": "3KqASrNJGK", "replyto": "3KqASrNJGK", "signatures": ["ICLR.cc/2026/Conference/Submission25566/Reviewer_nxs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25566/Reviewer_nxs6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762415404868, "cdate": 1762415404868, "tmdate": 1762943476559, "mdate": 1762943476559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}