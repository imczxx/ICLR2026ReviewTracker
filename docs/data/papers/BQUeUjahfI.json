{"id": "BQUeUjahfI", "number": 24105, "cdate": 1758352753107, "mdate": 1759896781471, "content": {"title": "Learning Tractable Distributions of Language Model Continuations", "abstract": "Controlled language generation is the task of generating text conditional on some constraint: for example, valid syntax, or alignment to human values. A fundamental challenge is that these constraints may depend on future tokens, which standard autoregressive language models (LM) cannot readily account for. Prior work has tackled this problem by modeling future completions using a *tractable probabilistic circuit* as a surrogate model that aims to approximate the distribution over future completions, which is used to adjust the LM logits at decoding time. However, we surprisingly find that their distribution over future tokens is often insensitive to the past context, limiting their effectiveness in controlling LMs. By viewing tractable probabilistic circuits as latent variable models, we show via theoretical and empirical analysis that this can be largely attributed to the suboptimal context encoder used in these models. In this paper, we overcome this limitation by utilizing a separate, more powerful neural language model encoder. Building on this, we propose an efficient decoding-time controller that reuses the same language model used for generation to encode a context-rich prior over the latent states, while the tractable surrogate performs exact future lookahead from the conditioned state. Importantly, we show how this can be done without incurring significant decoding-time overhead. Experiments across diverse controlled generation tasks demonstrate significant improvements over prior approaches in terms of constraint satisfaction and perplexity of generations.", "tldr": "", "keywords": ["probabilistic circuit", "tractable model", "exact inference", "controlled language generation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26d2da60cbe5f2a40d995c86c04cac0cace80a57.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles constraint-following in large-scale language modeling. Prior work employed a Hidden Markov Model (HMM) to preemptively detect future constraint violations, but its generic design often discarded previously generated context. The proposed LTLA model integrates the HMM with a neural encoder, preserving context while enforcing constraints. Empirically, LTLA lowers perplexity relative to standard HMM baselines. The approach also transfers to vision–language models, where it reduces caption toxicity compared with QWEN generation and prompt-engineering strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and presents a clear, coherent narrative. \n- It tackles the important challenge of achieving controllable text generation while preserving fluency."}, "weaknesses": {"value": "* Framing–implementation gap: the paper motivates “learning tractable distributions” broadly (e.g., various TPMs) but evaluates only an HMM instantiation.\n* Figure 1 is under-analyzed: the example is not explained clearly, and no takeaway is drawn from its components.\n* Related work and conclusion are blended and feel disorganized; the related-work discussion does not clearly position this paper relative to prior art or trace specific influences.\n* Missing comparison to Sequential Monte Carlo (SMC), which is a natural baseline for constrained sampling and tractable approximations.\n* Limited empirical scope: more datasets are needed to assess robustness and generality.\n* No large-scale evaluation on long contexts. Real-world deployments often require maintaining constraints over 4k–128k tokens.\n* Lacks benchmarks against established conditional/controllable generation methods.\n* Appendix C lacks an analysis of computational trade-offs; it does not clarify which algorithm is superior in time and/or space complexity."}, "questions": {"value": "1. **Baseline comparisons (SMC).**\n   Could you add results against a Sequential Monte Carlo (SMC) baseline, or point me to where these appear in the paper if I missed them?\n\n2. **Terminology: TPC vs. TPM.**\n   The abstract mentions *tractable probabilistic circuits*. Are you building specifically on TPCs, or more broadly on tractable probabilistic models (TPMs)? Please clarify the intended scope and what is actually instantiated in experiments.\n\n3. **Long-sequence evaluation.**\n   Can you provide tests at higher sequence lengths (e.g., ≥4k tokens, and, if feasible, 32k–128k) to assess constraint retention and stability?\n\n4. **Positioning vs. related work (Sec. 5).**\n   Could you expand on how your approach differs from the works in Section 5 and why it constitutes an improvement? A brief mapping from prior methods to your components would help.\n\n5. **Controllable-generation baselines.**\n   Can you report comparisons to established conditional/controllable generation methods (e.g., constrained decoding, plug-and-play/classifier guidance, recent conditional LM approaches), along with metrics such as constraint satisfaction, fluency/perplexity, and toxicity where applicable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nEuocL8aTS", "forum": "BQUeUjahfI", "replyto": "BQUeUjahfI", "signatures": ["ICLR.cc/2026/Conference/Submission24105/Reviewer_hB4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24105/Reviewer_hB4y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888098542, "cdate": 1761888098542, "tmdate": 1762942939809, "mdate": 1762942939809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, Learning Tractable Distributions of Language Model Continuations, introduces Learning To Look Ahead (LTLA), a novel hybrid framework that combines neural language model encoders with tractable probabilistic models to improve controlled text generation. The authors identify that prior tractable probabilistic circuits, such as HMM-based surrogates, poorly capture contextual dependencies, limiting their effectiveness in conditioning autoregressive language models. LTLA addresses this by reusing the same LM backbone to produce context-rich latent priors for a tractable surrogate (e.g., an HMM), maintaining exact inference capabilities while adding minimal computational overhead. Empirical results across language and vision-language generation tasks show that LTLA substantially enhances conditional log-likelihood and controllability compared to prior HMM and sampling-based baselines, particularly in tasks like logical constraint satisfaction and toxicity control. Overall, this work presents a theoretically grounded and computationally efficient advancement in tractable modeling for controllable language generation, though its reliance on hybrid architectures may raise questions about scalability and generalization to more complex constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper makes an original contribution by introducing Learning To Look Ahead (LTLA), a hybrid framework that bridges neural language models and tractable probabilistic models for controlled generation. Its originality lies in reconceptualizing tractable circuits—such as HMMs—as context-aware lookahead models that can be neurally conditioned, overcoming a key limitation in prior work where tractable models were effectively context-insensitive. The proposed method is technically sound and thoughtfully designed: it leverages the expressivity of pretrained LMs while preserving the exact inference and efficiency benefits of tractable models, representing a novel and elegant integration of two traditionally separate modeling paradigms. The paper’s quality is high, with solid theoretical grounding (e.g., mutual information analysis, complexity discussion) and comprehensive empirical validation across both text and multimodal generation tasks. The writing is clear and well-organized, explaining a fairly technical idea in an accessible way with helpful figures and ablations. Finally, the significance is notable: LTLA advances controllable generation by enabling more accurate and efficient computation of conditional queries, a capability that is crucial for value alignment, safety, and constraint satisfaction in large generative models. Overall, the paper is both conceptually innovative and practically impactful."}, "weaknesses": {"value": "While the paper presents a compelling and elegant approach, several aspects could be strengthened to fully realize its potential. \n\nFirst, the empirical evaluation, though broad, remains somewhat narrow in scope and scale — the experiments are conducted primarily on medium-sized models (e.g., GPT-2 large, Qwen2-VL-2B) and synthetic or benchmark-style constraints. It would be valuable to test LTLA on stronger, more diverse LLMs and real-world control settings (e.g., factuality, safety, or long-horizon reasoning) to better demonstrate scalability and generalization. \n\nSecond, the dependence on pretrained LM encoders raises questions about efficiency and modularity: while the paper claims minimal overhead, it is not clear how well LTLA performs in streaming or large-batch inference scenarios, or how its memory footprint compares to sampling-based baselines under constrained compute budgets. \n\nThird, the theoretical contribution, though sound, could be deepened — for example, the mutual information bound and tractability discussion are insightful but not tightly linked to empirical metrics or model design choices (e.g., how hidden size or encoder capacity affects conditional query fidelity). \n\nFourth, the ablation and analysis of architectural choices could be expanded, especially regarding why the Monarch parameterization yields limited gains and how the trade-off between neural encoder complexity and HMM capacity behaves in practice. Finally, the clarity around limitations and failure cases is limited; for instance, when LTLA fails to improve over baselines (e.g., longer continuations or less structured constraints), the paper does not analyze why. A more explicit discussion of such cases and potential mitigations would make the work more robust and informative."}, "questions": {"value": "Please address weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LCryslZFPo", "forum": "BQUeUjahfI", "replyto": "BQUeUjahfI", "signatures": ["ICLR.cc/2026/Conference/Submission24105/Reviewer_k6yu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24105/Reviewer_k6yu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888988377, "cdate": 1761888988377, "tmdate": 1762942939600, "mdate": 1762942939600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They identify that a core problem within controllable generation is modeling future completions for a current position. To solve this, they propose using a neural HMM to 1) encode the context, and 2) output a tractible distribution over the next positions that can easily be sampled from. They demonstrate that this neural HMM approach outperforms other variants of the HMM, and do a fairly detailed ablation over the possible approaches for parameterizing / learning this HMM. They demonstrate effectiveness on CommonGen and producing non-toxic captions from toxic images."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "**Motivation**: I do agree with their assessment on why continuation prediction is an important problem with controllable generation. The distribution for each position should be biased towards tokens that lead to better total sequences, instead of being just position specific. \n\n**Solution Novelty**: The proposed solution — a hybrid HMM conditioned on the hidden representation that is able to “look ahead” by cheaply generating a continuation from a fixed position — is quite elegant."}, "weaknesses": {"value": "**Experimental Details**: The discussion on experimental details could be more thorough. The appendix does not contain enough information regarding the experimental setup. What was the learning setup for the neural HMM? The learning rate? Batch size, etc? And what about generated samples — why are there no examples in the appendix? As it is, the appendix needs to be significantly strengthened. \n\nThis is a major weakness, but I also feel that it should be relatively easy to fix with revision. I would highly recommend including examples for the detoxification task. Maybe show an example of a hateful meme + description with the prompt, and then show how the LTLA results in a non-toxic description.\n\n**Breadth of Experiments**: The paper only investigates two downstream tasks (which are relevant and well chosen), but there should be more uses of this method. What about inference time alignment? Or just normal language detoxification (no images)? Or topic-constrained generation? The latter three are fairly common benchmarks for controlled generation. \n\n**Baselines**: The idea has high potential, as it solves a problem that could enable superior control algorithms for constrained autoregressive generation. It is important to empirically verify that solving this problem leads to better constraint satisfaction by comparing against prior controlled generation algorithms that do not use this approach. The baselines currently seem too limited. \n\n**Presentation**: I would recommend making the core idea a bit easier to understand: perhaps by adding a visual diagram of how the method actually works. [2, 3, 4] have excellent diagrams that make it easier to understand the core idea — something like visualizing how the hmm avoids the need to compute the next tokens through the entire model and instead use a lightweight neural HMM would greatly strengthen the presentation.  \n\n[1] Controlled LLM Decoding via Discrete Auto-regressive Biasing. Pynadath, Zhang. ICLR 2025.  \n\n[2] BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases. Liu et al. May 2023. \n\n[3] Gradient-Based Constrained Sampling from Language Models. Kumar et al. Nov 2022.\n\n[4] COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. Qin et al. NeurIPS 2022."}, "questions": {"value": "1. How does this method compare to [1]? It would be a good idea to compare LTLA against LM-Steer, as they also propose using a light weight linear layer to assist in controlled generation. \n\n2. Many EBM approaches for controlled generation rely on generating multiple samples autoregressively to compute the gradient of the constraint, which makes such methods impractical. But LTLA could enable these algorithms to be more practical by serving as a cheap oracle for continuations of each position. How LTLA fit in with an algorithm like [2]? It would be interesting to see how different controlled generation techniques behave when working in conjunction with LTLA. It seems that LTLA could improve a wide range of controllable generation techniques beyond what is explicitly studied in the submission. \n\n[1] Word Embeddings Are Steers for Language Models. Han et al. 2024. \n[2] Controlled LLM Decoding via Discrete Auto-regressive Biasing. Pynadath, Zhang. ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5BG4kG21cj", "forum": "BQUeUjahfI", "replyto": "BQUeUjahfI", "signatures": ["ICLR.cc/2026/Conference/Submission24105/Reviewer_Efqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24105/Reviewer_Efqv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018748160, "cdate": 1762018748160, "tmdate": 1762942939376, "mdate": 1762942939376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Prior work using standalone Tractable Probabilistic Models (TPMs), such as HMMs, as lookahead surrogates were limited because their learned continuation distribution was often insensitive to the context.\n\nThe paper proposes Learning to look ahead (LTLA).\n- LTLA addresses the inability of standard LLMs to enforce future constraints (like specific syntax or topics) during decoding.\n- Prior work used simple Tractable Probabilistic Models (TPMs) for lookahead, but these models failed because they were context-insensitive.\n- LTLA solves this by reusing the LLM's Transformer backbone to create a powerful neural encoder.T his encoder efficiently predicts a context-rich prior over the latent states z_t.The TPM then performs exact and efficient lookahead from the conditioned state to ensure constraint satisfaction."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The work presents an efficient hybrid architecture that combines the deep context-encoding power of a Transformer with the tractable lookahead capability of an HMM.\n\n- The method demonstrates significant empirical gains in modeling perplexity and generation quality over prior TPM-based controllers.\n\n- The design reuses the LLM's hidden states, ensuring the lookahead process is highly efficient and incurs minimal decoding-time overhead.\n\n- The approach shows versatility across diverse tasks, improving both hard logical constraints and soft semantic attributes (like VLM detoxification)."}, "weaknesses": {"value": "- Lookahead's effectiveness diminishes over longer continuation. $z_t$ as described in the paper is limited in capability on how much information it can store.\n- Expressiveness is limited when using simple HMM structure which ultimately creates a bottleneck. So even though it can be pretty useful for analysis purposes, it cannot be used for practical use cases around generation.\n- With a large C, and |z_t|, the lookup table can get very large, further reducing practicality."}, "questions": {"value": "see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5lYs1Gemx", "forum": "BQUeUjahfI", "replyto": "BQUeUjahfI", "signatures": ["ICLR.cc/2026/Conference/Submission24105/Reviewer_v6Tr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24105/Reviewer_v6Tr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019166342, "cdate": 1762019166342, "tmdate": 1762942939088, "mdate": 1762942939088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LTLA, a hybrid model that improves controllable text generation by combining a neural encoder with a tractable probabilistic decoder such as an HMM. This design lets the system efficiently “look ahead” while keeping inference exact and computationally cheap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Clear motivation: The paper identifies a genuine limitation of prior tractable control methods, poor context sensitivity, and provides a clean, theoretically supported solution by replacing only the encoder with a neural module while keeping tractable inference intact.\n+ Conceptual elegance: The “neural encoder + tractable decoder” design is simple yet effective, offering a principled way to blend neural expressiveness with exact probabilistic reasoning.\n+ Compatibility: LTLA can be plugged into existing controllable-generation frameworks without retraining or high computational cost. Demonstrated effectiveness across both text-only and vision-language models (VLMs), showing it generalises beyond pure language generation."}, "weaknesses": {"value": "+ Narrow evaluation scope:  All benchmarks involve short sequences (≤32 tokens) from GPT-2 or Qwen2-VL models. The approach has not been tested on longer contexts or more recent/larger LLMs. Do you think it is necessary to evaluate on newer models? Are there specific reasons that prevent applying this approach to more recent architectures?\n+ Limited empirical improvement: The proposed method shows only marginal perplexity gains over standard HMMs (e.g., Fig. 3, left subfigure). The perplexity results suggest the advantage is relatively small. However, I think the constrained-generation setting more critical for this paper. Please clarify this.\n+ Inconsistent results: In the controllable generation tasks, gains in fluency (lower perplexity) don’t always align with text-quality metrics like BLEU or ROUGE. Could the authors provide an explanation or discussion for this discrepancy?\n+ Possibly incomplete baseline comparison:  In the controllable-generation task, the experiments only compare against standard HMMs and their variants, but omit widely used controllable-generation baselines such as FUDGE, GeDi, etc. Would it be more reasonable to include comparisons with these established methods?\n\n\nI would be willing to raise my scores if the authors’ rebuttal satisfactorily addresses these concerns."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VdZ18VdlOi", "forum": "BQUeUjahfI", "replyto": "BQUeUjahfI", "signatures": ["ICLR.cc/2026/Conference/Submission24105/Reviewer_qZkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24105/Reviewer_qZkk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060244212, "cdate": 1762060244212, "tmdate": 1762942938884, "mdate": 1762942938884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}