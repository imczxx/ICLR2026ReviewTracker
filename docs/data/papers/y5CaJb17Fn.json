{"id": "y5CaJb17Fn", "number": 10970, "cdate": 1758185869275, "mdate": 1759897617411, "content": {"title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "abstract": "Vision-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent works have begun to explore the incorporation of latent actions, abstract representations of motion between two frames, into VLA pre-training. In this paper, we introduce villa-X a novel Vision-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies.\nOur approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. We demonstrate that villa-X can generate latent action plans in a zero-shot fashion, even for unseen embodiments and open-vocabulary symbolic understanding. This capability enables villa-X to achieve superior performance across diverse simulation tasks in SIMPLER and on two real-world robotic setups involving both gripper and dexterous hand manipulation. These results establish villa-X as a principled and scalable paradigm for learning generalizable robot manipulation policies. We believe it provides a strong foundation for future research.", "tldr": "", "keywords": ["embodied AI"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91fdd8e52fa98a300957b25195fed8d3e623781e.pdf", "supplementary_material": "/attachment/edd257188f9cfb1f142e73921798518d7ff94987.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents villa-X (vision-language-latent-action), a framework that integrates latent actions into vision-language-action (VLA) models. The core idea of villa-X consists of two components:\n- incorporating existing latent action models (LAMs) into a proprioceptive module to obtain more physically grounded latent actions\n- jointly modeling latent actions and real robotic actions.\n\nExperimental results demonstrate that (1) the proprioceptive module enables LAMs to extract higher-quality latent actions, and (2) the resulting VLA model significantly outperforms existing VLA baselines in both simulated and real-world environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] villa-X achieves strong performance in both real-world and simulated settings. The authors conduct extensive experiments that convincingly demonstrate the effectiveness of the proposed framework.\n- [S2] The paper is clearly written and well-organized. Figures and equations are concise yet informative, effectively illustrating how villa-X operates."}, "weaknesses": {"value": "- [W1] From a high-level perspective, this work largely follows the structure of the existing framework [1]. The process—first training latent action models (LAMs) with a VQ-VAE-style objective to generate pseudo labels (latent actions) for robot data, and then training the VLA model to predict those latent actions—remains similar. The proposed addition of a proprioceptive module and the joint prediction of latent and robotic actions, while useful, appear somewhat incremental.\n- [W2] The paper provides limited discussion on why villa-X performs well across diverse environments and tasks. The experiments primarily report performance improvements without deeper analysis of contributing factors or underlying mechanisms.\n- [W3] No statistical significance analysis is presented. The reported performance metrics lack measures of variance or confidence, making it difficult to assess the robustness of the claimed improvements.\n\n**References**\n\n[1] Ye et al., Latent Action Pretraining from Videos. In ICLR, 2025."}, "questions": {"value": "- [Q1] In Table 1, the performance of LAPA and Go-1 on the WidowX robot appears noticeably poor. Could the authors elaborate on the underlying cause of this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iwxHv11EYd", "forum": "y5CaJb17Fn", "replyto": "y5CaJb17Fn", "signatures": ["ICLR.cc/2026/Conference/Submission10970/Reviewer_7xUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10970/Reviewer_7xUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539282305, "cdate": 1761539282305, "tmdate": 1762922163065, "mdate": 1762922163065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework called villa-X, whose core idea is to incorporate latent actions into the pretraining and policy learning of Vision-Language-Action (VLA) models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The system design is simple, effective, and scalable.\n\n2. The experiments are sufficient and comprehensive.\n\n3. The writing is clear and provides a good reading experience."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "1. I recently came across latent action learning in a survey paper [1]. How does the latent action learning mentioned in your work differ from that?\n\n2. I also believe that latent learning is a promising approach for solving cross-embodiment transfer. What other potential solutions do you foresee in the future?\n\n3. Equation (3) makes me a bit confused — what exactly is the dataset ID, and why is the context vector ce composed of these two parts?\n\n4. How large is the VILLA-X model in terms of scale?\n\n[1] Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey. arXiv 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1xASDEm9S1", "forum": "y5CaJb17Fn", "replyto": "y5CaJb17Fn", "signatures": ["ICLR.cc/2026/Conference/Submission10970/Reviewer_oHXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10970/Reviewer_oHXE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840086173, "cdate": 1761840086173, "tmdate": 1762922162631, "mdate": 1762922162631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VILLA-X, a Vision-Language-Latent-Action framework that advances robot policy learning by improving both how latent actions are learned and integrated into Vision-Language-Action models. It enhances latent action learning through a proprioceptive forward dynamics module that grounds latent representations in physical robot dynamics, and it introduces a joint diffusion-based policy that conditions robot action generation on latent action planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured and clearly written. The problem motivation is sound, the technical approach is explained logically.\n\n2. The evaluation is thorough, encompassing systematic ablations, major simulation benchmarks (SIMPLER, LIBERO), and real-world deployment on two distinct platforms. \n\n3. The demonstrated capability for zero-shot generalization to novel embodiments addresses a core challenge in the field."}, "weaknesses": {"value": "1. The technical contributions, while valuable, exhibit limited novelty relative to existing literature. The proposed proprioceptive Forward Dynamics Model (proprio-FDM), which grounds latent actions by predicting low-level states, is conceptually similar to the approach of Nikulin et al. [1], who employ a linear decoder on latent tokens to predict actions. The efficacy of this general principle for grounding has also been previously analyzed by Zhang et al. [2]. Furthermore, the architectural design of separate experts for latent and robot actions (ACT-latent and ACT-robot) bears a strong resemblance to the module separation employed in GO-1.\n\n2. The characterization of the GO-1 baseline may be inaccurate. Based on its open-source implementation, GO-1 does not appear to autoregressively predict latent actions using a next-token-prediction (NTP) loss, but rather uses an L1 loss for latent action learning. Consequently, the description in Section 4.2 and the subsequent analysis in Table 1 could be misleading regarding the true nature of this baseline.\n\n3. The experimental comparisons lack benchmarks against several highly relevant contemporary works, notably IGOR [3] and UniVLA [4]. These methods also focus on cross-embodiment generalization and leverage latent actions learned from web videos, making their inclusion critical for properly contextualizing the claimed advancements of this work.\n\n4. The zero-shot generalization analysis in Section 4.3 would be strengthened by clarifying the training procedure for the world model used for visualization. \n\n\n\n______\n\n[1] Nikulin, Alexander, et al. \"Latent action learning requires supervision in the presence of distractors.\" arXiv preprint arXiv:2502.00379 (2025).\n\n[2] Zhang, Chuheng, et al. \"What Do Latent Action Models Actually Learn?.\" arXiv preprint arXiv:2506.15691 (2025).\n\n[3] Chen, Xiaoyu, et al. \"IGOR: Image-goal representations are the atomic control units for foundation models in embodied ai.\" arXiv preprint arXiv:2411.00785 (2024).\n\n[4] Bu, Qingwen, et al. \"UniVLA: Learning to act anywhere with task-centric latent actions.\" arXiv preprint arXiv:2505.06111 (2025)."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IarNAU1Dtb", "forum": "y5CaJb17Fn", "replyto": "y5CaJb17Fn", "signatures": ["ICLR.cc/2026/Conference/Submission10970/Reviewer_XiVL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10970/Reviewer_XiVL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901542430, "cdate": 1761901542430, "tmdate": 1762922162269, "mdate": 1762922162269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach to learn latent actions, and leverage latent actions when predicting robot actions through joint diffusion. In addition to learning forward and inverse dynamics models, as is done in existing work, the paper proposes to learn an embodiment-conditioned model which predicts future robot states and actions. During inference-time, the action head predicts both latent actions and robot actions given image observations, language instructions, proprioceptive states and embodiment embedding, where an attention mask is used to enforce the factorization of the conditional probability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed approach has strong empirical results. In particular, in Table 2, the proposed approach outperforms state-of-the-art VLA models, such as OpenVLA, OpenVLA-OFT, $\\pi_0$, and Gr00T. It also outperforms other algorithms such as MoTo and LAPA.\n\n- Using an embodiment-specific embedding is an interesting approach to leveraging diverse robot datasets, where different datasets have slightly different dynamics and action spaces. Ablation study shows that the embodiment embedding has a positive impact on the success rate.\n\n- The ablation study is in general quite thorough, showing the effectiveness of each component of the proposed approach, e.g., latent action model, proprioceptive state prediction, etc."}, "weaknesses": {"value": "- Although the main design components are validated through ablation studies. Some finer-grained design choices lack discussion and study. See more details in the **Questions** section.\n\n- Figure 3, probing experiment results, is not very easy to understand. Perhaps a plot showing the distribution of error in different intervals, for both w/ pp and w/o pp,  would be more informative. It is also not very convincing why $L_\\infty$ (max across all dimensions) is preferred over $L_1$ (summing/averaging over all dimensions). \n\n- The results in 4.3, although helpful and intuitive, are not very informative scientifically. Since the images are obtained from a “separately trained world model”, it is hard to tell whether the latent action actually encodes the robot behavior or these outcome images just come from hallucination from the world model. It may be good to swap it with another ablation study in the appendix.\n\n- It is unclear why only Gr00t is used as the baseline for real-world experiments, where other state-of-the-art VLA models are left out.\n\n- The tasks evaluated in the experiment are mainly simple pick and place tasks, without much dexterity. \n\n- Minor: Incomplete sentence in Appendix D.3: \"Both models were trained on 10...\""}, "questions": {"value": "- How is $K$, the number of future steps when training FDM and IDM, determined?\n\n- Why is the prediction only for $o_t$ and $o_{t+K}$ when training observation F/IDM in Equation (1), but for the entire sequence when prediction robot states $q_{t+1:t+K}$ and actions $a_{t+1:t+K}$ in Equation (2)?\n\n- In Equation (3), why is the context vector conditioned on dataset ID instead of robot ID. Wouldn’t it make sense for two datasets with the same robot to share an ID?\n\n- Why does the action head predict $(n-1)K$ latent actions and $m$ robot actions? How to choose $n$ and $m$?\n\n- Why is the robot action branch “overly relying on latent actions” harmful? Isn’t the latent action supposed to provide sufficient information for action prediction (Equation (2))?\n\n- What is the action space for the model? How does the gripper command translate to Xhand command in the realworld experiments?\n\n- In Appendix A.1, how is the codebook size of $32$ decided?\n\n- In Appendix B, can you explain how different $\\tau$ distribution can be used for latent actions and robot actions? This seems contradictory to (5) where all actions are bundled together during denoising. \n\n- In Appendix F, is the attention mask ablation referring to the block-wise causal attention mask or random masking of attention during training?\n\n- In Appendix H, why is OpenVLA-OFT not included in Table 9? Especially considering that it is included in Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hy3gpjamvr", "forum": "y5CaJb17Fn", "replyto": "y5CaJb17Fn", "signatures": ["ICLR.cc/2026/Conference/Submission10970/Reviewer_kiPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10970/Reviewer_kiPs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951170904, "cdate": 1761951170904, "tmdate": 1762922161862, "mdate": 1762922161862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}