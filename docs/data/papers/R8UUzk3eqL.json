{"id": "R8UUzk3eqL", "number": 10764, "cdate": 1758181334338, "mdate": 1759897631224, "content": {"title": "VideoMB: Steering Representations towards Motion Balanced Caption Generation in Vision-Language Models", "abstract": "Large Vision-Language Models (VLMs) need to balance between spatio-temporal understanding and computational efficiency. For standard architectures, this trade-off is largely determined by balancing visual feature compression and spatial fidelity. We show that this limitation results in heavy bias towards appearance, often failing to discern or caption moving objects in a video. To address this, we introduce VideoMB, a novel framework that orchestrates feature embedding manipulation, fundamentally reshaping the model's perceptual priorities. VideoMB incorporates cross-attention layers which establish temporal understanding by modeling information flow between consecutive frames. We propose a fine-tuning paradigm which jointly optimizes caption generation with a global matching objective, constraining learned visual representations to exhibit maximal similarity with the embeddings corresponding to the positions of moving objects. Our approach is computationally efficient and can be seamlessly integrated into existing models. Extensive experiments demonstrate that VideoMB significantly improves motion-based captioning accuracy, particularly for challenging scenarios involving small or low-resolution moving objects, while maintaining competitive performance on appearance-focused tasks. These findings offer a generalizable solution for steering attention towards desired visual elements, providing fine-grained control over perceptual focus in video understanding tasks.", "tldr": "We introduce VideoMB, a framework for transforming video language models from being appearance-biased to motion-balanced, by enhancing moving object representation through cross-attention mechanisms and dual-objective fine-tuning.", "keywords": ["Vision Language Models", "Vision Understanding", "Motion Perception"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c0df7fd7e9a2548120e6c894720e968e73cc846.pdf", "supplementary_material": "/attachment/6de3afb405d318ba84f7d7815e73eeb5b741dbee.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VideoMB, a framework designed to correct the appearance bias in Vision-Language Models (VLMs) and improve their understanding of motion. The method incorporates cross-attention layers between consecutive video frames to enhance features of moving objects. It is fine-tuned using a dual objective: the standard captioning loss and a novel auxiliary loss, which uses an off-the-shelf point tracker to supervise the learning of spatiotemporal correspondences. Experiments demonstrate that this lightweight approach significantly improves motion-based captioning on two different VLMs, particularly in challenging scenarios with small or blurry objects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality: The approach of using an external point tracker to generate supervision for a patch-matching loss is a highly novel and effective way to explicitly inject motion priors into a VLM.\n\nQuality: The technical execution is strong, with a well-designed method validated by comprehensive experiments. The consistent improvements on two different VLM architectures and standard benchmarks, supported by insightful qualitative visualizations, are convincing.\n\nClarity: The paper is exceptionally well-written and easy to follow. The problem, method, and results are presented logically and clearly, with effective diagrams and figures.\n\nSignificance: The work provides a practical and computationally efficient solution to a fundamental limitation in video understanding. Its plug-and-play nature makes it a valuable tool for improving a wide range of VLMs."}, "weaknesses": {"value": "1. Dependency on External Tracker: The method's performance is inherently tied to the quality of the external point tracker (CoTracker). The paper does not sufficiently discuss the impact of tracker failures or noise, which could lead to the model learning flawed motion patterns.\n2. Unquantified Inference Overhead: While the fine-tuning is efficient, the paper claims the overall approach is computationally efficient without quantifying the added latency or computational cost (e.g., FLOPs) of the new cross-attention layers during inference.\n3. Limited Temporal Scope: The model only processes relationships between consecutive frames. This may be insufficient for understanding long-range temporal dependencies or events where key actions occur across non-adjacent frames."}, "questions": {"value": "1. How robust is VideoMB to noise or errors from the CoTracker? Have you analyzed how performance degrades when the tracker fails on challenging videos (e.g., with severe occlusions or non-rigid objects)?\n2. Could you please quantify the inference overhead introduced by VideoMB? For instance, what is the percentage increase in latency or FLOPs compared to the baseline models?\n3. The current design focuses on consecutive frames. How does this limit the model's ability to understand long-range temporal relationships, and have you considered strategies to extend it to non-adjacent frames?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O6CChh2BRC", "forum": "R8UUzk3eqL", "replyto": "R8UUzk3eqL", "signatures": ["ICLR.cc/2026/Conference/Submission10764/Reviewer_6hXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10764/Reviewer_6hXR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378298126, "cdate": 1761378298126, "tmdate": 1762921980595, "mdate": 1762921980595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces VideoMB, a novel framework designed to address the inherent appearance bias in Large Vision-Language Models (VLMs) when applied to video captioning. The authors identify a core problem: the tokenization process in VLMs compresses visual information, leading to a loss of temporal fidelity and a consequent failure to adequately capture and describe moving objects. VideoMB tackles this by integrating two key components into pre-trained VLMs: 1) Cross-attention layers between consecutive frames to enhance temporal alignment and feature matching, and 2) An auxiliary loss function that enforces similarity between corresponding patch embeddings in adjacent frames, using point tracks from an off-the-shelf model (CoTracker) as supervision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper clearly identifies a significant and under-explored problem in VLMs: the trade-off between spatial fidelity and temporal reasoning leading to motion neglect. \n2. A key strength of VideoMB is its practical design, particularly for analyzing the motion of small objects in VLMs."}, "weaknesses": {"value": "1. The architecture relies on established techniques: cross-attention for feature alignment and self-supervised losses for temporal consistency. Since these are well-known in video understanding, the overall novelty of the model is constrained.\n2. The method's auxiliary loss relies on pseudo-labels from CoTracker. This introduces a dependency on the performance and biases of this external model. \n3. The cross-attention mechanism is exclusively applied between consecutive frames. This design may struggle to capture long-range temporal dependencies or motions. The model might miss patterns that require a broader temporal context for accurate reasoning.\n4. The model encourages feature similarity for corresponding patches but does not explicitly represent or output motion features (like optical flow or displacement vectors) that could be directly utilized by the LLM backbone. The motion understanding remains an implicit property of the embeddings, which may limit the LLM's ability to perform explicit spatio-temporal reasoning.\n5. The ablation study, while present, is somewhat limited. It primarily ablates the loss function components but does not ablate the core architectural change, which is the cross-attention layers themselves. Questions remain: How much performance gain comes from the cross-attention alone versus the auxiliary loss? What is the effect of the number and placement of these layers?\n6. The experimental evaluation primarily demonstrates the improvement achieved by applying VideoMB to two specific base models (InternVL3-1B and smolVLM2). However, it lacks a comprehensive comparison against other recent and state-of-the-art (SOTA) video understanding VLMs that are specifically designed for temporal reasoning, such as MotionSight, CogVLM2, or VideoLLaMA. Without this broader comparison, it is difficult for the reader to assess whether VideoMB establishes a new SOTA on these benchmarks or simply narrows the performance gap between efficient base models and more sophisticated, temporally-aware architectures."}, "questions": {"value": "1. Could you please specify the exact number (\"K\") and the insertion points of the cross-attention layers within the vision encoder (e.g., after which transformer blocks)? What was the rationale for this specific configuration?\n2. What is the individual contribution of the cross-attention mechanism versus the auxiliary similarity loss? Have you experimented with using the cross-attention layers but with only the standard captioning loss \n3. Was there a specific rationale for choosing a point tracker like CoTracker over more traditional, dense optical flow methods? Given that your auxiliary loss requires point-level correspondences, did you consider deriving pseudo-tracks by sampling points and then tracking them using a dense flow estimator?\n4. Was there a specific reason you chose to use cross-attention layers rather than, for instance, applying a Parameter-Efficient Fine-Tuning (PEFT) technique like LoRA to the existing self-attention or feed-forward layers within the pre-trained vision encoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yMWpb3jSpo", "forum": "R8UUzk3eqL", "replyto": "R8UUzk3eqL", "signatures": ["ICLR.cc/2026/Conference/Submission10764/Reviewer_MTgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10764/Reviewer_MTgH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829230477, "cdate": 1761829230477, "tmdate": 1762921979479, "mdate": 1762921979479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper showed VLMs have bias towards appearance, where models often fail to capture motion dynamics in videos, and instead describing a summary of static objects. The authors introduce VideoMB, a fine-tuning framework designed to steer a VLM's representations towards temporal reasoning. The core of the method consists of two main contributions: (1) integrating new cross-attention layers between consecutive frame embeddings to model temporal information flow, and (2) introducing a dual-objective loss function. This loss combines the standard captioning objective with a novel motion-similarity lossâ€‹. This auxiliary loss is supervised by displacement vectors extracted from an off-the-shelf point tracker (CoTracker), encouraging the model's internal representations to align with real-world motion patterns. The framework is applied to two existing VLMs (SmolVLM2 and InternVL3-1B), showing improved performance on motion-centric video understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of using an external, high-quality point tracker to generate a supervisory signal for a VLM's internal representations is solid.\n- By only fine-tuning the newly introduced cross-attention layers and keeping the base VLM's weights frozen , the approach is computationally efficient and can be seamlessly integrated into existing pre-trained models."}, "weaknesses": {"value": "- The paper fails to evaluate the trade-offs of improving motion understanding. The entire objective is to steer the model's representations towards motion, which could plausibly harm its ability to perform appearance-focused tasks.\n- The authors themselves note that the evaluation benchmarks (TempCompass and MotionBench) primarily test \"fine-grained human movements\" with \"limited motion magnitude\". This seems misaligned with the method's potential.\n- Insufficient architectural details: the provided \"details\" are very limited and I have concern on reproducibility."}, "questions": {"value": "The weaknesses are significant. The complete dependency on an external tool without a thorough analysis of its failure modes, the lack of evaluation on static tasks to check for negative transfer, and the somewhat limited scope of the quantitative evaluation prevent me from recommending acceptance at this time.\n\n- Please add a discussion on the limitations of CoTracker and how its failure modes could impact VideoMB. (A qualitative example showing a case of tracker failure and its effect on the model's learning would be highly informative.)\n- Evaluating the fine-tuned models on a standard static image captioning benchmark (like COCO) and an appearance-centric video task to show that you are not harming the model's general abilities.\n- Please provide a deeper analysis of the poor performance on the \"Order\" sub-task in TempComp."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dmthlktz1b", "forum": "R8UUzk3eqL", "replyto": "R8UUzk3eqL", "signatures": ["ICLR.cc/2026/Conference/Submission10764/Reviewer_Uj6n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10764/Reviewer_Uj6n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859938560, "cdate": 1761859938560, "tmdate": 1762921979131, "mdate": 1762921979131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new vision language model which provides a better captioning taking into account not just the frame features but also the fluent information across the frames. Using only the frame features often leads to loss of temporal information. This is addressed by incorporating a consecutive frame cross attention layer which captures correlation of information across the evolving frames. The loss function is accordingly modified to to minimise both feature representation error as well as temporal misalignments. The approach is likely to work for loss of temporal information in the form of occlusion and noise. Extensive experimental results are presented for various kinds of video scenes and noise conditions. The proposed method shows promising results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The cross-attention layer across frames is a novel approach.\n2) The loss function is suitable for the proposed task.\n3) Handles occlusion and noise conditions\n4) extensive experimental results are presented"}, "weaknesses": {"value": "1) Using only consecutive frame alignment via cross attention might be simplistic for complex video. Many temporal events requiring semantic segmentation might be undetected.\n2) There might be overfitting in the case of pretraining and fine tuning in case of OOD videos.\n3) Comparison with many state of art video captioning LLM is missing\n4) Ablation study should include hyeprparameter tuning effects like loss function compositions under varying weighting schmes etc"}, "questions": {"value": "1) What is the effect of transition across semantic video segments on the cross attention layer modeling consecutive frame alignment? Are these effects smoothened out. Are the caption generation features not so much dependent on segment transition?\n\n2) Comparison with other transition aware captioning schemes should be made. See for example:\nProgress-Aware Video Frame Captioning, Zihui Xue, Joungbin An, Xitong Yang, Kristen Grauman; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025\n\n3) The ablation study may be extended by considering various hyper-parameter combinations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "q06V1AKWJv", "forum": "R8UUzk3eqL", "replyto": "R8UUzk3eqL", "signatures": ["ICLR.cc/2026/Conference/Submission10764/Reviewer_U4Gk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10764/Reviewer_U4Gk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910823399, "cdate": 1761910823399, "tmdate": 1762921978691, "mdate": 1762921978691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}