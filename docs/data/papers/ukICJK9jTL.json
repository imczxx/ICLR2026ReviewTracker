{"id": "ukICJK9jTL", "number": 7841, "cdate": 1758038542591, "mdate": 1759897828045, "content": {"title": "CAVINR: Coordinate-Aware Attention for Video Implicit Neural Representations", "abstract": "Implicit Neural Representations (INRs) have emerged as a compelling paradigm, with Neural Representations for Videos (NeRV) achieving remarkable compression ratios by encoding videos as neural network parameters. However, existing NeRV-based approaches face fundamental scalability limitations: computationally expensive per-video optimization through iterative gradient descent and convolutional architectures with shared kernel parameters that provide weak pixel-level control and limit global dependency modeling essential for high-fidelity reconstruction. We introduce CAVINR, a pure transformer framework that fundamentally departs from convolutional approaches by leveraging persistent cross-attention mechanisms. CAVINR introduces three contributions: a transformer encoder that compresses videos into compact video tokens encoding spatial textures and temporal dynamics; a coordinate-attentive decoder utilizing persistent weights and cross-attention between coordinate queries and video tokens; and temperature-modulated attention with block query processing that enhances reconstruction fidelity while reducing memory complexity. Comprehensive experiments demonstrate CAVINR's superior performance: 6-9 dB PSNR improvements over state-of-the-art methods, $10^5\\times$ encoding acceleration compared to gradient-based optimization, $85-95\\%$ memory reduction, and 7.5$\\times$ faster convergence with robust generalization across diverse video content, enabling practical deployment for large-scale video processing applications.", "tldr": "", "keywords": ["Implicit Neural Representations", "Neural Representations for Videos."], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9fcd010480582defdffeb40b8bb074914ce5749.pdf", "supplementary_material": "/attachment/219f99aaf2dded2b2a65017768107abf3b681bd9.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposed a method for encoding videos using implicit neural representations which uses a transformer hypernetwork to improve visual fidelity and encoding time. The method uses a shared video encoder to produce video tokens as a latent representation for the video. The video tokens are then decoded by a lightweight decoder where they are cross-attended to the requested x,y,t coordinate. The paper provides extensive benchmarks which show that the method produces higher PSNR than prior works and has efficient encode/decode FPS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The encoding speed looks very good\n- The image quality results also look good"}, "weaknesses": {"value": "- Unclear architecture\n- Unclear comparisons\n- Unclear compression performance\n- Unclear decoding speed"}, "questions": {"value": "This is overall a very good idea but I was a little confused about some of the claims being made and how they fit into the larger INR landscape. Also despite the extensive benchmarks I think there are some missing pieces. \n\nFirst, on the good side, I think the idea itself is very interesting and it's not something I've seen before. Training a trainsformer encoder to produce video tokens should yield a powerful representation and coupling it with a lightweight decoder shifts compute burden to the encode side. Using more compute for encode is generally how INRs function but since this method resembles more of a traditional learned-compression algorithm the design makes a lot of sense. The efficacy of the design is shown empirically in Tables 1 and 2 where CAVINR consistently outperforms prior work. \n\nNow we get into some of the things I was a little confused about. Firstly: is this really an INR? I get that it takes coordinates as input to the decoder and produces outputs but I was unclear on what parts of the network, if any, were being specialized to the video. Are all the network weights shared? Is the entire payload the video tokens?\n\nNext, I'm a little unclear about the actual compression performance of this method. I think this is presented in Table 4 but it's a bit confusing. Usually, in a compression paper, we would look at something like a rate-distortion curve that shows how distortion varies over different bitrates. And typically that would be the primary result. So it's hard to evaluate this is a practical video codec if I can't see how bitrate is varying with quality. Looking over Table 4 it seemed like size was generally larger and PSNR comparable to H.264 but it really wasn't clear.\n\nI also was unclear on the different comparison methods. For example on encoding comparisons in Fig 6 it looks like the primary comparison is to NeRV although I see some other hypernetwork methods on there. There might be some better comparisons like NIRVANA for example which is somewhere in between the traditional INR and the hypernetwork and put a lot of effort into improving encoding time. These comparisons should really also be consistent like in Tables 1 and 2 if possible (or let me know why that doesn't make sense). In addition I think it makes sense to compare against traditional \"autoencoder\" based methods as this architecture seems more closely related to those than INRs.\n\nLastly, I didn't see any discussion of decoding speed, only encoding speed, or if there was discussion it wasn't clear enough. There is a section all the way at the end titled \"Processing Efficiency and Applications\" which mentions \"CAVINR processes 125 videos per second\" but I don't know what that means. Processes how? End-to-end? Decoding? How long are the videos? On what hardware?  \n\nSpecific Questions\n1. Clarify the INR portion: which parts are shared and which are specialized?\n2. Clarify the payload, is it only video tokens?\n3. What is the actual compression performance? Show rate distortion\n4. Clarify decoding speed and \"Processing Efficiency\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4x18VS7YrY", "forum": "ukICJK9jTL", "replyto": "ukICJK9jTL", "signatures": ["ICLR.cc/2026/Conference/Submission7841/Reviewer_c2uG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7841/Reviewer_c2uG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318072442, "cdate": 1761318072442, "tmdate": 1762919884527, "mdate": 1762919884527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Implicit networks rely on slow per video optimization with the promise of fast decode speeds. But the encode bottleneck and the cost of compute to do so have prevented widespread use and adoption. \nThe paper introduces an architecture that aims to solve this problem by having a pre-trained transformer based encoder which can give useful priors on the video followed by a small coordinate based decoder which can maintain the speed. \nWith architectural tweaks combined with smart use of compute - time trade-offs, the authors show a path to scale the paradigm of Implicit models, while outperforming prior works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is generally well written, easy to follow with most claims addressed with extensive ablations and experiments. \n- The paper is filled with a series of seemingly  small tweaks like using a learnt conv tokenizer instead of Unfold op, better frequency selection, use of cross attention as decoder,  choosing YUV as a target over RGB and many more result in a system that delivers fast encoding with the best video quality. \n- The justification to use cross attention over directly predicting the weight space (Appendix A3) is an important result for the community. If this method scales well, it can potentially save us from going down the wrong path. I feel this should be in the main paper."}, "weaknesses": {"value": "- I am unsure about the Block Query processing and in general the use of (X,Y,T) as input to the coordinate decoder. It simply cannot scale ! For example a 30fps-1080p will not fit into any modern hardware's memory and processing it block by block will kill the decoding speed. Roughly estimating current method for a 1080p video would yield a very low fps. \nThe current experiments are validated for short sequences at a small spatial resolution of 256x256. \nI would like to know the authors take on this and in general how they believe this system (or a variation of this family) can be scaled. \n\n-"}, "questions": {"value": "- Would like to know the authors' thoughts on how to scale this system to work on say, 1080p videos or even 4K."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9REwOtQOJJ", "forum": "ukICJK9jTL", "replyto": "ukICJK9jTL", "signatures": ["ICLR.cc/2026/Conference/Submission7841/Reviewer_9bRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7841/Reviewer_9bRF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723842087, "cdate": 1761723842087, "tmdate": 1762919883900, "mdate": 1762919883900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAVINR, a framework based on a transformer that exploits persistent cross-attention mechanisms. Contributions of CAVINR are: a transformer encoder that compresses videos into compact video tokens by encoding spatial textures and temporal dynamics; a coordinate-attentive decoder utilizing persistent weights and cross-attention between coordinate queries and video tokens; and temperature-modulated attention with block query processing that enhances reconstruction fidelity while reducing memory complexity. Experiments reveal better performance: over SoA methods, better acceleration compared to gradient-based optimization, memory reduction, and faster convergence with robust generalization across diverse video content."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Good analytics presented, lot of experiments and good illustrations make the paper quite complete.\n\nSignificant contributions are:\nBetter PSNR, Less memory and faster convergence - all are appreciable.\n\nUse of TRANSFORMER HYPERNETWORK ENCODER and COORDINATE-AWARE ATTENTION VIDEO DECODER provide more power to representation.\n\nThe use of adaptive split in addressing the spatial-temporal frequency disparity in video data, appears to be quote a good idea in Axis-Adaptive Positional Encoding scheme."}, "weaknesses": {"value": "Perhaps a lack of code release (even anonymous link would have been better), is a matter of concern.\n\nA few high-RES frames of output videos generated vs that of SoA would have been nice, \n\nHow about the temporal continuity in output videos ? - need to observe your outputs, not just the frames - humans are good at spotting sudden transients  - hence sequence of few frames are not enough for acceptable (superior) visual quality.\n\nReveal cases of failure of your model. \n\nINR representation may gave certain drawbacks - which are not highlighted. \nSay, in dense texture regions, PiP frames, cluttered scenes etc.- will it work well?\n\nTraining needs 8 NVIDIA A800 GPUs, as mentioned.\nWill your code adapt to Quad or dual-GPU systems ?\n\nThe difference in quality of the frames in 3rd and 4th rows in Fig. 3 is  uncomprehending, even after zooming into figure.\nBetter to have zoomed in parts of that figure in supple-doc to highlight your work."}, "questions": {"value": "Can this representation be extended for future frame prediction?\n\nWhat is the limit of range of resolution of the frames (high & low) your system can deal with?\n\nIn certain cases when dataset sample is small, can your model be modified with concepts of self-supervision,\ncontrastive paradigm and meta-learning to deal with such issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBZwuenaOc", "forum": "ukICJK9jTL", "replyto": "ukICJK9jTL", "signatures": ["ICLR.cc/2026/Conference/Submission7841/Reviewer_KCsd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7841/Reviewer_KCsd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836006246, "cdate": 1761836006246, "tmdate": 1762919883387, "mdate": 1762919883387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CAVINR, which goes a step further than recent hypernetworks for INRs. Instead of modulating a single layer (like GINR), the modulate no layers in the INR such that the network is implicit in the sense that it maps coordinates to video, but not implicit in the sense that it doesn't store any of the signal in its weights. Instead, this instance-agnostic INR gets video information from the token outputs of a transformer encoder. In this sense, the model is very similar to a traditional autoencoder, where an encoder produces some latent embedding, and a decoder converts this embedding to video. Compared to traditional autoencoders, the main difference is that CAVINR is structured like an INR- it uses coordinate inputs, and the video tokens essentially modulate the axis-adaptive positional encoding of these coordinates. This approach has much slower training/decoding than FastNeRV, but even at equal/less training time it delivers higher quality outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The method outperforms the previous hypernetworks in terms of quality and storage size for video compression.\n\nS2. Some components of the method, like switching to YUV, are lightweight and seem like they could be generally useful to other INR methods.\n\nS3. The paper clearly communicates the contribution, and the ablations help to convey the importance and effect of the different components."}, "weaknesses": {"value": "W1. The paper is challenging to evaluate. The similarities with autoencoders leave me inclined to think it might be appropriate to request comparisons with non-INR approaches like DCVC-RT. At the same time, I could be persuaded that the coordinate mapping is the more essential part of what makes something an \"INR.\"\n\nW2. I do not agree with the provided rational about the poor VPS. This is VPS at 256x256. For realistic resolutions, it would be far worse. FastNeRV needs its \"extra\" VPS in order to have any hopes of scaling to higher resolutions. And this is part of the problem. It is known that autoencoders beat INR for size/quality and especially encoding time. The whole problem is that, until DCVC-RT, they struggled with decoding speed. If one converts the INR into a static decoder, and the decoding speed becomes much worse, then the method is not just conceptually an autoencoder, but has the same practical drawbacks as well.\n\nW3. The related work completely omits the NVC/DCVC family of methods. It is one thing to not compare to these, but it seems incorrect to omit them from discussion altogether."}, "questions": {"value": "1. Why switch from a t-based approach like FastNeRV to x,y,t, considering the efficiency issues that presents?\n\n2. Why should this approach not be compared to something like DCVC-RT, which is a neural network autoencoder for video compression that supports high quality, low storage, fast decoding, etc. even for higher resolutions?\n\n3. Prior works like TransINR also evaluate on image, 3D, and other signals. Can this method be applied to these as well? If not, which component prevents this from being possible? If so, how is the performance compared to these prior works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uERMegNQJB", "forum": "ukICJK9jTL", "replyto": "ukICJK9jTL", "signatures": ["ICLR.cc/2026/Conference/Submission7841/Reviewer_dW3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7841/Reviewer_dW3g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7841/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858273091, "cdate": 1761858273091, "tmdate": 1762919882950, "mdate": 1762919882950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}