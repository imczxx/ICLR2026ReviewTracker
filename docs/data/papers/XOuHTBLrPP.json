{"id": "XOuHTBLrPP", "number": 10093, "cdate": 1758160513421, "mdate": 1759897674483, "content": {"title": "TAR-TVG: Enhancing LVLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding", "abstract": "Temporal video grounding aims to localize relevant video segments based on a given query. Large Vision-Language Models (LVLMs) can address this by taking a video and query as input and outputting the time duration. Recently, some methods fine-tune LVLMs with reinforcement learning (RL), encouraging them to generate reasoning traces for better interpretability. They also prompt the model to include `<timestamp></timestamp>` tags into the reasoning process to strengthen the connection between the reasoning and the final output. However, these prompts only implicitly guide the model to output timestamp tags, often leading to missing, incorrect-formatted, or irrelevant tags. To address this issue, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG). By designing reinforcement learning reward functions, we explicitly enforce the inclusion of timestamp tags as anchors within the reasoning traces, providing explicit format control and accuracy validation based on soft IoU. Furthermore, when multiple timestamp anchors appear, the reward function is designed to ensure that the accuracy of these anchors progressively improves, thereby mimicking the human-like thought process of refining from coarse to fine. These additional constraints on timestamp anchors encourage the model to better understand the task of temporal video grounding, thereby improving its grounding performance. Besides, we adopt a three-stage training process: initial RL training, followed by supervised fine-tuning (SFT) using collected SFT data, and finally another round of RL training. Experiments show that our model achieves state-of-the-art performance while producing verifiable reasoning chains with progressively refined temporal estimations.", "tldr": "TAR-TVG is a reinforcement learning framework that improves Temporal Video Grounding by introducing timestamp anchors to guide reasoning, boosting both accuracy and interpretability.", "keywords": ["Reinforcement Learning", "Temporal Video Grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/480fb3e20bf09099fa2de76190ece9d9b789fec1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose TAR-TVG, a novel timestamp anchor-constrained reasoning framework for temporal video grounding, which includes a efficient reinforcement learning strategy for extracting high-quality reasoning traces. The experiments reveal the improved performance for temporal video grounding with verifiable reasoning chains for progressively refined temporal estimations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method adopts a three-stage training process with reinforcement learning, improving the interpretability and accuracy of temporal video grounding.\n\n2. The experiment result is solid with high performance on several evaluation benchmarks for temporal video grounding.\n\n3. Convincing visualization examples are provided to prove the effectiveness of the proposed method."}, "weaknesses": {"value": "1. Only testing on temporal video grounding tasks would be limited for the proposed method adopted with LVLMs. Many temporal-aware LVLMs also demonstrate effective generalization ability for related video understanding tasks, not limited to temporal video grounding only. I encourage the authors to evaluate the proposed method on more temporally related video understanding benchmarks.\n\n2. The challenges claimed by this paper, that ‘ the prompts of the used method only implicitly guide the model to output timestamp tags, often leading to missing, incorrect-formatted, or irrelevant tags’, are relatively weak. Since quite a few LVLM-free methods, such as FlashVTG, achieve good temporal video grounding performance and do not have such challenges. The authors should reorganize the statement of addressed challenges."}, "questions": {"value": "1. See weakness.\n\n2. Why adopt a three-stage training process in the order of RL-SFT-RL instead of processes(such as SFT-RL only) in other orders? This may need to be better clarified by more ablation experiments with both evaluation performance and training cost.\n\n3. The paper may require a small adjustment of compilation format, such as citation font color in the main text and the underline in the reference, which is different from papers from previous years and other reviewed papers, and may be caused by the compilation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1HVyq8frUz", "forum": "XOuHTBLrPP", "replyto": "XOuHTBLrPP", "signatures": ["ICLR.cc/2026/Conference/Submission10093/Reviewer_EVwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10093/Reviewer_EVwv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294690899, "cdate": 1761294690899, "tmdate": 1762921477593, "mdate": 1762921477593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Timestamp Anchor-constrained Reasoning (TAR-TVG) for temporal video grounding with Large Vision-Language Models (LVLMs). Instead of merely prompting a model to add <timestamp> tags inside chain-of-thought, TAR-TVG explicitly constrains the reasoning with (1) a format reward that enforces valid <think>…</think>, <timestamp>…</timestamp>, and <answer>…</answer> structures, (2) a soft IoU reward that can be negative to provide graded feedback even with non-overlapping segments, and (3) a timestamp anchor reward that weights later anchors more and rewards progressive refinement of timestamps. Training follows a three-stage RL→SFT→RL routine: initial GRPO to mine ~30k high-quality CoT traces, SFT on those traces, and final GRPO with anchor constraints. On Charades-STA and QVHighlights, the method reports state-of-the-art or competitive results (e.g., mIoU 61.1 and R1@0.7 50.2 on Charades-STA with a 7B LVLM), and shows zero-shot gains on ActivityNet-Captions and TVGBench."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear:\nIdentifies a concrete failure mode of prior “prompt-only” reasoning and answers it with explicit, verifiable anchors coupled to the final answer. The progressive-refinement constraint is especially compelling and differentiates TAR-TVG from previous works which supervise format or outcomes but not intermediate time anchors directly.\n\n2. Three-stage RL→SFT→RL training is a effective way of collecting SFT data.\nMining 30k CoT traces with explicit anchor quality thresholds, then SFT, then RL again is an effective pipeline that improves the rate of valid-format reasoning and final accuracy. The paper quantifies each stage’s contribution. \n\n3. The ablation study is comprehensive."}, "weaknesses": {"value": "1. presentation can be improved:\nThe text in figure 1 and 3 is too small and hard to see. Please make them bigger.\n\nThe well-known background such as GRPO and some trivial implementation such as format reward can be moved to appendix. Make more room for your ablation study which is more interesting.\n\n2. the contribution is limited:\nThe main innovations are (1) explicitly include <timestamp></timestamp> in the reasoning process. This is mainly about output formating.\n(2) reward to encourage predicting progressively improved time periods. An ad-hoc design of reward functions which is mainly based on empirical observation. \n(3) leveraging some criteria and a pretrained model to produce SFT data. This method has been widely adopted in many previous papers [1]. \n\n3. Lack comparison to previous RL based method such as Video-R1 [1]\n[1] Video-R1: Reinforcing Video Reasoning in MLLMs"}, "questions": {"value": "1. In the first stage of the RL-SFT-RL TRAINING STRATEGY, do you optimize the model or not? If you only collect data in this stage without training the model, please remove the term GRPO since it means you optimize the model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eypmU3kul6", "forum": "XOuHTBLrPP", "replyto": "XOuHTBLrPP", "signatures": ["ICLR.cc/2026/Conference/Submission10093/Reviewer_jaMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10093/Reviewer_jaMU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969633998, "cdate": 1761969633998, "tmdate": 1762921477288, "mdate": 1762921477288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TAR-TVG, a reinforcement learning method for Temporal Video Grounding (TVG) that introduces explicit constraints on timestamp anchors within the model's reasoning process. The core innovation involves designing reward functions that enforce: (1) correct formatting of timestamp tags, (2) progressive refinement of temporal predictions (later timestamps must be more accurate than earlier ones), and (3) control over the number of generated timestamps. To address training instability, the authors employ a three-stage RL→SFT→RL strategy that automatically generates high-quality Chain-of-Thought data. The method achieves state-of-the-art results on Charades-STA and shows strong performance on QVHighlights, ActivityNet-Captions, and TVGBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of timestamp anchors and the progressive refinement reward (TARrefine) is a conceptually novel and well-motivated approach. It explicitly encourages the model to mimic a human-like, coarse-to-fine reasoning process, which is a clear advancement over prior RL-based methods that only implicitly prompted for timestamps.\n\n2. The paper demonstrates compelling state-of-the-art performance on multiple established benchmarks (Charades-STA, QVHighlights). The improvements over strong baselines like Time-R1 are significant and well-documented across various metrics (mIoU, R1@0.5, R1@0.7).\n\n3. The proposed RL→SFT→RL pipeline is a pragmatic solution to the cold-start problem where base models fail to generate initial timestamp tags. The method of automatically curating a high-quality CoT dataset from initial RL rollouts is efficient and eliminates the need for manual annotation."}, "weaknesses": {"value": "1. The proposed method is a complex, multi-stage pipeline (RL→SFT→RL) built upon another complex framework (GRPO). This \"pipeline-ception\" raises concerns about reproducibility, computational cost, and practicality in general. The need for such a heavy-handed approach suggests that the core idea might be fragile or complex, making it challenging to optimize directly.\n\n2. The three-stage training process is exceptionally resource-intensive. The first RL stage is acknowledged to have a low success rate for generating sound samples, making it highly inefficient. Training requires up to 16 A100 GPUs for 60+ hours. This level of resource consumption poses a significant barrier for most researchers, limiting the practical adoption and verifiability of the work.\n\n3. The method is highly specialized for the Temporal Video Grounding task. The heavy reliance on specific output formatting (<think>, <timestamp>) and custom rewards makes it non-trivial to adapt to other video reasoning tasks (e.g., captioning, VQA). The paper does not demonstrate the generality of the \"progressive anchor\" concept beyond TVG.\n\n4. While the method produces \"reasoning chains,\" the evaluation is solely based on the final grounding accuracy (IoU). There is no qualitative or quantitative analysis of the faithfulness or correctness of the generated reasoning itself. The examples in Appendix A highlight that previous models produce flawed logic; however, it remains unproven whether TAR-TVG's reasoning is truly more logical or faithful, or if it has simply learned to exploit the reward structure by placing correct timestamps within a templated text.\n\n5. The method's success is closely tied to a carefully engineered prompt (detailed in Appendix B.4) that explicitly guides the two-step reasoning process. The performance gains might be partially attributable to this superior prompt design rather than the RL reward mechanism alone. An ablation where the same prompt is given to a strong baseline is missing."}, "questions": {"value": "While TAR-TVG presents a novel idea and achieves strong results, the combination of extreme complexity, high computational cost, lack of demonstrated generalization, and unresolved questions about the true nature of the learned reasoning leads me to lean towards a weak rejection. The core idea of progressive anchor refinement is promising, but the current execution feels overly engineered and inefficient for the gains achieved. I would be willing to reconsider my decision if the authors can convincingly address the concerns above, particularly those related to generality and cost-effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xi5kJSMYj0", "forum": "XOuHTBLrPP", "replyto": "XOuHTBLrPP", "signatures": ["ICLR.cc/2026/Conference/Submission10093/Reviewer_PYVz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10093/Reviewer_PYVz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016467945, "cdate": 1762016467945, "tmdate": 1762921476880, "mdate": 1762921476880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding (TAR-TVG). The proposed method introduces intermediate timestamp anchors during a reasoning chain (in a large vision-language model) and enforces that each reasoning step progressively improves the timestamp prediction. The training is a three-stage process: 1) initial RL (GRPO) to generate high-quality reasoning traces with anchors; 2) supervised fine-tuning (SFT) on that distilled data; and 3) final RL fine-tuning with the anchor constraints. Experiments on standard TVS benchmarks (Charades-STA, QVHighlights) show that the proposed method achieves state-of-the-art performance and improves interpretability via the anchor-based reasoning chains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**[S1]** The paper is well-written and easy to follow.\n\n**[S2]** The proposed method is interesting, and the mechanism is well-designed to enable the model to produce accurate predictions. Especially, the proposed soft IoU reward is simple yet effectively complements the timestamp anchor-constrained reward.\n\n**[S3]** The interpretability angle (reasoning chains with anchors) is a positive addition."}, "weaknesses": {"value": "**[W1]** Efficiency analysis\n- The requirement for heavy RL training (30K reasoning traces, etc) may limit reproducibility and practical adoption. Providing full budget, hardware, and training time details would be helpful to highlight the contribution of the proposed method.\n\n**[W2]** Ablation study\n- It is not clear how much of the performance gain is from the anchor mechanism vs simply using more RL/training data/backbone size.\n- The interpretability angle is a bit weak. The interpretability claim is only meaningful if users inspect the reasoning traces. The paper should include many qualitative examples and maybe user studies.\n\n**[W3]** Minor issue\n- If the method uses multiple anchors, but the best ablation turns out to be just two anchors, then the general claim “introduce anchors (e.g. more than three) and progressive refining” may over-claim the breadth of benefit. The paper should avoid implying many‐anchor advantages if two is sufficient."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0a4fBXwMG8", "forum": "XOuHTBLrPP", "replyto": "XOuHTBLrPP", "signatures": ["ICLR.cc/2026/Conference/Submission10093/Reviewer_EJ9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10093/Reviewer_EJ9d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762855723977, "cdate": 1762855723977, "tmdate": 1762921476534, "mdate": 1762921476534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}