{"id": "ofgxkMLqic", "number": 13447, "cdate": 1758218033623, "mdate": 1759897437020, "content": {"title": "Human-AI Curation Synergy: Scaling Preference Data Curation via Human-Guided AI Feedback", "abstract": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches incorporating advanced training techniques have failed to yield meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models~(LLMs) perform automatic curation based on human guidance. Based on this preference mixture, we train simple Bradley-Terry reward models ranging from 0.6B to 8B parameters on a carefully curated subset of 26 million preference pairs from the 40M pool. We demonstrate that the resulting reward models are versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling. These reward models achieve state-of-the-art performance across seven major reward model benchmarks, outperform the latest paradigm of generative reward models, and demonstrate strong downstream performance. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. Our approach represents substantial progress in open reward models, revealing the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.", "tldr": "Through a human-AI synergistic curation pipeline, we curate a high-quality, large-scale preference data mixture of 40 million preference pairs, enabling state-of-the-art reward models on seven major reward model benchmarks.", "keywords": ["preference data", "reward modeling", "data curation", "data annotation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01bdeb9e05ce592bf238918f1a39d1d12aeb58ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SynergyPref-40M, a preference dataset of 40M pairs (with 26M curated) and a two-stage human–AI curation pipeline for training Bradley–Terry reward models (0.6B–8B params). Stage 1 uses human-verified “gold” labels plus LLM-assisted “silver” labels with error-driven adaptive retrieval; Stage 2 scales automatic curation by enforcing consistency with a gold reward model and the current best RM. Trained RMs achieve state-of-the-art results on seven benchmarks (RewardBench v1/v2, PPE Preference/Correctness, RMB, RM-Bench, JudgeBench), with strong best-of-N scaling, resistance to style bias, and competitive correctness; ablations attribute gains to curation quality in addition to scale."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Two-stage, human-guided curation with adaptive retrieval is thoughtfully engineered and demonstrates measurable impact over raw LLM curation; Stage-2 consistency filtering is a pragmatic way to scale beyond the human budget.\n2. Seven strong benchmarks, including RewardBench v2 and JudgeBench, with detailed breakdowns (style bias on RM-Bench, BoN scaling on PPE Correctness). Results are consistently SOTA for open RMs.\n3. Training details are explicit; the authors plan to release data/models and provide scripts for reproduction."}, "weaknesses": {"value": "1. While authors recognize raw LLM curation is weak and mitigate with human guidance, final labels in large parts of the mixture still depend on LLM aggregation; a more thorough error analysis comparing human vs LLM-judged segments would strengthen soundness claims.\n2. The “in-the-wild” pool composition, de-duplication against evaluation sets, and licensing/PII filters are not described in depth.\n3. Stage-2 selection relies on agreement with a gold RM trained on human data; without stringent de-duplication and contamination checks, this risks overfitting to the gold RM’s inductive biases and to benchmarks that overlap with the mined pool. Stronger leakage analysis (e.g., prompt/response near-duplicates) might be needed."}, "questions": {"value": "1. How did you ensure that mined preference pairs (and their paraphrases) do not overlap with RewardBench v2, JudgeBench, etc.?\n2. Can you report how often flips contradict original human labels vs synthetic labels, and whether flipped pairs increase spurious correlations (e.g., penalizing certain styles)?\n3. Any results on downstream RLHF or human studies confirming that the SOTA RM ranking translates into policy improvements and user-perceived quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sldkkvzmax", "forum": "ofgxkMLqic", "replyto": "ofgxkMLqic", "signatures": ["ICLR.cc/2026/Conference/Submission13447/Reviewer_2akh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13447/Reviewer_2akh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761171400940, "cdate": 1761171400940, "tmdate": 1762924071666, "mdate": 1762924071666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a large-scale preference dataset (SynergyPref-40M) and a series of reward models trained on the dataset. It describes an iterative data curation pipeline merging human and LLM annotations. They show that trained reward models outperform existing models even at smaller model sizes across a wide range of benchmarks. \n\nOverall, this is a solid paper overall, with a clear contribution. The results are very impressive, and given that the dataset and reward models are released as promised, it is a very nice resource for other researchers. The reward model results and ablations are nice. However, the paper lacks more details on the dataset composition, choice of prompts, and generations. The methodology itself is described at a relatively abstract level. The description of evaluation protocols, instructions given to annotators, etc., is limited. Also, no actual examples from the dataset are provided. I expect some improved transparency for a revised version, to ensure reproducibility of results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ I assume that the dataset itself will be made available as promised. Obviously, a dataset at this scale and level of curation is a very strong contribution to the field\n+ The reported benchmark scores are very impressive, clearly outperforming existing reward models\n+ The evaluation of the trained reward models is thorough and very comprehensive\n+ I really appreciate the provided ablation studies. Evaluating the trade-offs of data curation and dataset scaling is insightful. In particular, sections 4.3 and 4.4 make a strong case for continued human data curation\n+ The paper presents a nice discussion and comparison of recent reward modeling benchmarks"}, "weaknesses": {"value": "- The actual dataset generation/curation process is missing details, in particular: What is the composition of the dataset (i.e., which categories does it contain? How are the prompts+responses formulated? What is the origin of prompts + responses? Does it contain multi-lingual samples?) Also, there are just no samples or insights given about the contents of the collected dataset. I think maybe it would also be possible to provide some more details in the main paper, important information like annotator information is only given in the appendices, a summary in the main text would be appreciated\n- For many of the lessons learned, I would appreciate more detailed insights. It was noted that access to tools was crucial for high-quality annotations. How did the annotators actually use them? What kind of prompts are most relevant for?"}, "questions": {"value": "- I do not follow the conclusion from Figure 1 (or at least I feel it’s too strongly worded). RewardBench still shows positive (and for most benchmarks) decently strong correlation. For example, in Figure 1-Left, I only see a single derivative model that outperforms RewardBench but is noticeably worse on the other benchmarks, so it seems more like an outlier. The general point that there is over-optimization on the reward bench is a totally valid concern, and I believe this to be the case, but i do not draw this conclusion from Figure 1. Could you give your oppinion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6B3LyhfwSm", "forum": "ofgxkMLqic", "replyto": "ofgxkMLqic", "signatures": ["ICLR.cc/2026/Conference/Submission13447/Reviewer_J7rE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13447/Reviewer_J7rE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828263121, "cdate": 1761828263121, "tmdate": 1762924070933, "mdate": 1762924070933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the perceived brittleness and poor performance of existing open-source reward models (RMs), hypothesizing that the root cause is low-quality preference data rather than flawed modeling techniques. To solve this, the authors propose a two-stage, human-AI synergistic pipeline for large-scale data curation. This pipeline leverages a small set of human-verified \"gold\" data to guide iterative, error-driven LLM annotation and then uses a \"gold RM\" trained on this data and LLM annotations to automatically filter and curate millions of \"in-the-wild\" preference pairs. Training on this data, they produce the Skywork-Reward-V2 series of models (0.6B to 8B parameters), which are shown to achieve new state-of-the-art performance across seven major RM benchmarks, outperforming all existing open RMs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates a clear and significant performance improvement\n2. The release of a new series of top-performing RMs and the massive underlying dataset is a valuable contribution to the open-source ecosystem"}, "weaknesses": {"value": "The paper's primary weaknesses are twofold: (1) a pervasive lack of clarity and the omission of essential methodological details, and (2) as a result, it is difficult to determine the true source of the claimed performance improvements.\n\n**Lack of Clarity and Omission of Essential Details**\n\nThe paper is extremely difficult to follow. The authors have clearly performed a massive amount of work, but the execution is not explained clearly, hindering reproducibility and full comprehension. Many terms are used without definition, and no single subsection seems to provide all the necessary details.\n\na. Ambiguity in Stage 1 Data Curation:\n\nInitial Data: The method for annotating the initial $y_w > y_l$ pairs is not described in the main paper. (The reviewer guesses this might be in Appendix E.1, but it must be stated).\n\nHuman vs. LLM Verification: Line 191 states human annotators follow protocols, but Line 198 mentions an \"LLM-verified\" portion. It is unclear how this LLM verification is performed. Is it the same as the “Preference-aware labeling”?\n\nMissing Examples: The paper provides no example of the \"LLM-generated preference attributes $a$\" (Line 184) or the full 5-tuple. An example and an analysis of the quality of these attributes are needed.\n\nRetrieval Step: In Step 2 (Line 208), it's unclear what data is used for for the retrieval to calculate the similarity. Is it from $D_{gold}$?. This makes Line 215 confusing: how do \"retrieved examples\" from $D_{un}$ come with \"human labels\"?\n\nLine 191, it is said that human annotators perform verification following the protocols. How is the LLM-verified portion at line 198 annotated? Is it annotated by LLM using the designed protocols as described in Appendix E.3?\n\nLLM Annotation Process: The \"preference-aware\" labeling (Lines 239-243) is opaque. The paper must specify which LLMs were used for annotation and provide the prompts. Without the prompt, this core step is irreproducible.\n\nb. Ambiguity in Stage 2 Data Curation:\n\nData Sources: The origins of $D_{un}$ and $D_{wild}$ are never explained.\n\n\"Best Reward Model\": The paper repeatedly refers to \"the best reward model\" used for filtering in Stage 2, but it is not specified which model this is.\n\nFinal Data Composition: It appears that in Stage 2, all pairs with $p>0.5$ are discarded, and all pairs with $p≤0.5$ are re-annotated by an LLM. This critical detail needs confirmation, as it would imply all data in the final dataset is LLM-annotated.\n\nc. Omissions for Ablations:\n\nFigure 6: The ablation on filtering/correction is confusing. It's unclear how these steps are performed during \"iter1-8\" or what the exact training set is. Stage 1 does not involve filtering $D_{wild}$ and correction.\n\nFigure 7: This figure shows five bars, but the text (Line 416) provides only four descriptions. The descriptions are not clear. What is “LLM curation only” and what is “both human and LLM curation”? Does (4) use more data? The exact training set for each of the five bars must be specified.\n\nd. Omissions in Figures and Tables:\n\nFigure 1 (Right): The method for calculating \"correlation\" is not described.\n\nTable 3: The caption should clarify that this metric is \"accuracy on the Best-of-N split,\" not the more common \"Best-of-N accuracy.\"\n\n**Ambiguity Regarding the True Source of Improvement**\n\nBecause the methodology is so poorly described, it is impossible to determine why the model works so well. The paper's core claim is that its human-AI synergistic pipeline is key, but the details suggest that \"a group of strong LLMs\" with self-consistency may be doing most of the heavy lifting. D_{silver} annotations are generated by LLMs. At Stage 2, all the data included after “Preference consistency with the best reward model” are again annotated by LLMs.\n\nMissing Baselines: The paper fails to provide the most critical baseline: what happens if you only use the LLM annotation: LLM-as-a-Judge with self-consistency? A crucial missing experiment is: (1) Use the \"best RM\" to filter out $p>0.5$ pairs, then (2) apply only LLM self-consistency annotations to the remaining data. Comparing this simple baseline to the full Skywork-Reward-V2 would clarify how much the complex, human-guided pipeline actually contributes. As an additional experiment: If you ensemble all the strong LLMs used in your annotation system to act as a single judge (similar to the GPT-4o judge in Table 1) with self-consistency, how does its performance compare to your final trained RMs?\n\nIn summary, the paper presents a complex pipeline. While the results are strong, the lack of clear explanation and proper baselines makes it impossible to validate the authors' claims about why it is strong. With all the missing details, it is contradictory to the reproducibility statement that “We have made extensive efforts to ensure the reproducibility of our work across all components of our research pipeline”."}, "questions": {"value": "Please address all the points in weakness.\n\nFigure 1 (Left): This chart is not fully convincing because model sizes are omitted. It is impossible to tell if a modification \"fails to yield consistent gain\" or if it is simply being compared to a larger model.\n\nPlease provide the prompts used for the \"preference-aware\" LLM annotation (L239-243) and specify which LLM(s) were used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GdHsx38WQC", "forum": "ofgxkMLqic", "replyto": "ofgxkMLqic", "signatures": ["ICLR.cc/2026/Conference/Submission13447/Reviewer_oVFu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13447/Reviewer_oVFu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976437700, "cdate": 1761976437700, "tmdate": 1762924070359, "mdate": 1762924070359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SynergyPref-40M introduces a 40M-pair preference dataset and eight reward models (0.6B–8B params) trained via a human-AI curation pipeline. Models achieve SOTA across seven benchmarks, demonstrating strong alignment with human preferences, safety, and bias resistance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The open contributions and deliverables include a new, large-scale, high-quality preference dataset, a valuable asset for the research community. \n\nIt verified that the brittleness is a root cause of RM underperformance and proposed a solution to it, which is human-AI curation synergy, that contains an elegant hybrid of human and LLM curation, balancing quality and scalability.\n\nEmpirically, Models (1.7B, 8B) achieve SOTA across seven benchmarks, outperforming much larger closed-source RMs."}, "weaknesses": {"value": "The conclusion of this paper is favorable. However, for pairwise preference, it follows transitive rules. The quality of pairwise preferences can be compromised by intransitivity observed in human annotations. The paper below highlights the existence of such 'intransitivity':\n\n- https://arxiv.org/abs/2409.19325 (Duan et al, 2017) \n\nIn a realistic world where an 'intransitive' relationship accumulates, quality control of the curated dataset is critical, but was not clarified in the proposed pipeline."}, "questions": {"value": "Given the budgeting for targeted performance, the paper still provides limited guidance on how to plan, evaluate, and achieve it.\nCan you provide some thoughts on how to achieve a target performance under a given cost budget in dollar terms? \n\nFor your reference, to overcome the cost constraints and to avoid high reliance on the availability of pairwise annotation, mechanism design has been explored as follows:\nhttps://arxiv.org/abs/2409.18417 (Zhang, 2024), leveraging a mechanism design mindset to construct a dataset for RLHF in a cost-efficient manner.\n\nHowever, the existing approaches are in different lines of techniques, more algorithmic than data-driven."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UTt50NcgS3", "forum": "ofgxkMLqic", "replyto": "ofgxkMLqic", "signatures": ["ICLR.cc/2026/Conference/Submission13447/Reviewer_tDR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13447/Reviewer_tDR3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981035721, "cdate": 1761981035721, "tmdate": 1762924069595, "mdate": 1762924069595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary Response to All ACs and Reviewers"}, "comment": {"value": "# Summary Response to All ACs and Reviewers\n\nThank you to all ACs and reviewers for the thorough evaluation of our submission. We appreciate the time and effort you invested in providing detailed feedback, especially given the reviewing workload. Your critiques and questions have helped strengthen our work.\n\n## Summary of Reviews\n\nWe received 4 reviews with initial ratings of 8 (accept), 6 (marginally above threshold), 6 (marginally above threshold), and 2 (reject).\n\n### Strengths Acknowledged by Reviewers\n\nReviewers recognized several aspects of our work:\n\n- Thoughtfully engineered human-AI curation pipeline with measurable impact (Reviewer 2akh)\n- State-of-the-art performance across seven major benchmarks, outperforming existing open reward models (Reviewers 2akh, j7rE, tDR3)\n- Comprehensive evaluation with detailed ablations on data quality and curation methods (Reviewers 2akh, j7rE, tDR3)\n- Contribution to the research community through the large-scale curated dataset and open model release (Reviewers j7rE, tDR3)\n- Clear demonstration that data quality, not just scale, drives reward model performance (Reviewer 2akh)\n- Hybrid approach balancing human annotation quality with AI scalability (Reviewer tDR3)\n\n### Main Concerns Raised\n\nReviewers identified several areas needing clarification:\n\n1. Dataset transparency and composition (Reviewers j7rE, oVFu): lack of details about data sources, prompt origins, language distribution, and concrete examples\n2. Methodology clarity (Reviewer oVFu): ambiguities in Stage 1 and Stage 2 processes, missing annotation details, unclear baseline comparisons\n3. Quality control mechanisms (Reviewers 2akh, tDR3): questions about contamination checks, intransitivity handling, and systematic bias mitigation\n4. Practical guidance (Reviewer tDR3): limited direction on cost-performance budgeting and achieving target performance levels\n5. Attribution of improvements (Reviewer oVFu): difficulty determining whether gains stem from the pipeline design versus strong LLM judges\n\n## Our Response Efforts\n\nWe've addressed each reviewer's concerns in detail:\n\n**For Reviewer 2akh,** we provided new agreement rate analysis showing human-guided LLM curation significantly outperforms pure LLM or pure human annotation. We detailed our multi-step decontamination process and PII analysis (0.07% flagged, mostly indirect identifiers). Human agreement tests demonstrate Stage 2 filtering reduces systematic biases—our dual-RM approach achieves 86% and 92% agreement on kept and flipped pairs, compared to baselines at 60-72%.\n\n**For Reviewer j7rE,** we provided complete dataset composition (40+ HuggingFace sources, task distribution, objectivity levels, language distribution). We expanded on annotation insights: LLMs with tools excel at objective tasks, but we struggle with diverse, conflicting subjective preferences lacking proper specification. We revised Figure 1 interpretation to avoid overclaiming—RewardBench improvements from ~80 to 90+ don't consistently translate to other benchmarks.\n\n**For Reviewer tDR3,** we explained intransitivity handling through Stage 1 metadata stratification, error-driven adaptive retrieval targeting unstable regions, and Stage 2 dual-RM consistency filtering. We provided a concrete budgeting guide linking scaling curves to cost models, showing just 1.8% of our 16M mixture (~290K pairs) surpasses previous SOTA. We connected our approach to mechanism design literature, clarifying complementary rather than competing approaches.\n\n**For Reviewer oVFu,** we clarified the complete pipeline including two distinct retrieval steps in Stage 1 and automatic Stage 2 scaling. Critical baseline experiments show LLM + Best RM filtering plateaus at 74-75% while our full recipe reaches 83%; LLM-as-a-Judge aggregation achieves 86.0% vs. our 88.6%. We shared annotation prompts, model lists, dataset composition, quality metrics (96.2% agreement on category labels), and licensing information (CC BY-NC 4.0).\n\n## Final Summary\n\nThrough detailed rebuttals, we added substantial new experimental evidence including agreement analyses, human studies, and additional baselines. We provided comprehensive methodological transparency through prompts, protocols, and data composition. We clarified that human guidance and LLM automation are complementary—neither alone achieves our hybrid approach's performance. We demonstrated rigorous quality control throughout the pipeline.\n\nThese clarifications address the core concerns while preserving the fundamental contribution: a principled, human-AI synergistic framework that unlocks high-quality preference data at scale, enabling state-of-the-art open reward models.\n\nThank you for the constructive feedback that has helped us improve both the technical rigor and presentation clarity of our work. We'll incorporate all improvements into the camera-ready version if accepted."}}, "id": "aCmNOr0YtL", "forum": "ofgxkMLqic", "replyto": "ofgxkMLqic", "signatures": ["ICLR.cc/2026/Conference/Submission13447/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13447/Authors"], "number": 23, "invitations": ["ICLR.cc/2026/Conference/Submission13447/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763691519358, "cdate": 1763691519358, "tmdate": 1763691519358, "mdate": 1763691519358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}