{"id": "aECIKvAYcd", "number": 16589, "cdate": 1758266441209, "mdate": 1763462024481, "content": {"title": "HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming", "abstract": "Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video.\n(2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm.\n(3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\\% for VOD and 26\\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\\%.", "tldr": "Practical video streaming with LLM-based highlight detection", "keywords": ["Video Streaming", "Highlight Detection", "Large Language Model", "Time Series Forecasting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b209261b2debf4a3302f48cb469b0c89bbe6ed47.pdf", "supplementary_material": "/attachment/d04a39cd16f32bf5ecad16a261ba937f7d463442.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes HiVid, a three-stage framework that leverages Large Language Models (LLMs) as human proxies to estimate content-aware saliency weights for adaptive video streaming (both VOD and live).\nThe framework comprises:\n- a Perception module, which assesses sampled frames through a sliding-window prompting strategy to generate local saliency scores and iterative video summaries;\n- a Ranking module, which applies a LLM-guided merge sort to globally re-rank frames and eliminate local inconsistencies; \n- a Prediction module, which performs multi-modal time-series forecasting to estimate future saliency weights in live-streaming settings.\n\nExperiments on TVSum, SumMe, and YouTube-8M show improved correlation metrics (up to +11.5% PLCC for VOD and +26% for live streaming) compared to video summarization and highlight detection baselines. A small user study reports better correlation between predicted QoE and human MOS."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting and timely idea: leveraging LLMs as scalable surrogates for subjective human judgments is an emerging and relevant research direction. Applying this concept to video streaming optimization is novel and has clear practical implications.\n\n- Modular design: the separation into perception, ranking, and prediction modules is conceptually clean and covers both offline and online (live) cases.\n\n- Novel use of LLM reasoning: using an LLM as a semantic comparator in a merge-sort procedure is unconventional and potentially generalizable to other ranking tasks."}, "weaknesses": {"value": "- Conceptual confusion or inconsistent terminology: The paper repeatedly refers to “video saliency prediction” or \"saliency score\", but most baselines (e.g., DETR, VASNet, PGL-SUM) are video summarization or highlight detection methods, not visual saliency models. While the \"saliency\" term is commonly associated in literature with spatial or spatio-temporal saliency maps that highlight visually regions within frames or videos, here it is used to denote subjective importance or priority score assigned to temporal chunks for bitrate allocation in streaming. This broader use of the \"saliency\" term may be misleading, especially readers familiar with classical video saliency prediction tasks. The paper would benefit from clearly defining \"saliency score\" early on and explicitly distinguishing its intended meaning from the traditional notion of saliency map. Providing alternative terminology (such as \"importance score\" or \"temporal relevance weights\") for the chunk-level scores could improve clarity and avoid confusion. \n\n- A potential limitation of the proposed ranking module lies in its reliance on LLM as the comparator function within the merge sort algorithm. While innovative, this design assumes that LLM can consistently and reliably perform pairwise comparisons that satisfy the properties required for sorting (e.g. transitivity and anti-symmetry). However, LLM outputs may be inherently variable, subjective, sometimes inconsistent, especially in tasks involving nuanced semantic judgments. There is no formal guarantee that the LLM will always induce a valid total order, which may lead to instability or errors in the final global ranking. The paper lacks a detailed analysis or empirical evidence on the robustness and consistency of the LLM-guided comparison. Addressing this aspect with more thorough evaluation or fallback mechanism would strengthen the approach. \n\n- Limited scientific contribution for ICLR: the paper primarily presents an application-driven pipeline that leverages existing LLMs as a zero-shot reasoning tool for relevance score assessment, combined with a learning-based forecasting module for live streaming weight prediction. While the system is creative, it is mostly engineering-driven, and the technical novelty in terms of learning methodology remains limited. This raises concerns about whether the contribution advances fundamental learning representation or model innovation, which constitutes the core criteria for ICLR acceptance.\n \n- The reported user study involves only 10 participants, which is a small sample size for drawing statistically reliable and generalizable conclusions regarding subjective Quality of Experience (QoE) in video streaming. Such a limited number of users increases the risk that individual preferences and variability disproportionately influence the results, reducing statistical power and limiting meaningful subgroup analyses. Therefore, this small sample size represents a methodological limitation of the study, and expanding the participant pool with a more diverse and larger user base would strengthen the validity and impact of the empirical evaluation.\n\n- Some implementation aspects of the Live Prediction Module are explained in more detail in the appendices (e.g., the use of both CLIP’s image and text encoders, the cross-attention fusion of modalities, and the training with randomized latency Δt to enable variable-length prediction). These details are only briefly mentioned or omitted in the main text (§3.4), making it difficult for readers to fully understand the model’s structure and training procedure without consulting the supplementary material. Providing a more self-contained description in the main paper—especially of how the adaptive decoding works—would substantially improve clarity and reproducibility.\n\n- The current evaluation mainly focuses on hyperparameter variations (e.g., window size, prediction horizon) but does not report experiments that isolate the contribution of the main components—Perception, Ranking, and Live Prediction. A more explicit analysis of how each module affects overall QoE correlation or latency would help clarify the role of individual stages and strengthen the empirical validation. \n\n- Evaluation of forecasting autonomy:  Since the forecasting module is intended to approximate the LLM’s saliency outputs, it would be informative to assess how well the system performs when operating autonomously—using the predictor without periodic LLM updates.\n Including such an experiment could clarify whether the proposed approach meaningfully reduces dependence on LLM inference and would provide stronger evidence for its real-time applicability.\n\n- Scalability and efficiency not demonstrated at realistic scale: The overhead analysis is performed on a single 201-second video, which does not convincingly demonstrate the scalability of the pipeline for longer or continuous live streams. Evaluating cost, latency, and performance over larger datasets or multi-hour content would strengthen the claims of efficiency and applicability to real-world streaming scenarios."}, "questions": {"value": "-\tHow do you ensure that LLM-generated scores are consistent across runs and models?\n-\tCould the authors clarify the exact nature of the “ground truth” used for correlation computation? Are the reference saliency weights derived from human annotations, subjective MOS labels, or pseudo-labels generated by the LLM?\n-\tCould the authors report results for a “forecast-only” setting, where the predictor operates without periodic LLM refresh? This would help understand how much the system relies on LLM inference in practice.\n-\tGiven the small sample (10 participants × 10 videos), could the authors provide information on inter-rater consistency or statistical significance of the reported PLCC improvements?\n-\tThe overhead analysis focuses on a 201-second clip. Have the authors explored or estimated how the cost and latency would scale for longer or continuous live streams?\n-\tCould the authors elaborate on how the LLM-based ranking is implemented in practice? For instance, are the pairwise comparisons deterministic (e.g., with fixed temperature and prompt order), and was any measure of ranking consistency across runs or window pairs evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4PM15rfWCW", "forum": "aECIKvAYcd", "replyto": "aECIKvAYcd", "signatures": ["ICLR.cc/2026/Conference/Submission16589/Reviewer_44Re"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16589/Reviewer_44Re"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918296838, "cdate": 1761918296838, "tmdate": 1762926664172, "mdate": 1762926664172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using a multimodal LLM as a substitute for human judgments to predict temporal visual saliency in videos, with adaptive bitrate (ABR) as the primary application.\n\nBecause most current multimodal LLMs do not support video inputs and cannot take all video frames as context, the paper evaluates representative frames within local windows. The local-window constraint harms global consistency of saliency scores, which the paper attempts to fix by re-ranking with the LLM used as a comparison function.\n\nThe paper also considers a live streaming setting. Since saliency-based ABR there requires predictions for future chunks, they propose a CLIP-based forecasting module."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive pipeline design spanning VOD and live streaming settings\n- The proposed method is evaluated against other methods using metrics derived from volunteer human ratings, and it shows improvements."}, "weaknesses": {"value": "- The proposed method is based on proprietary LLMs. This makes the approach not robust to change in the proprietary LLM's service specifications and operating conditions."}, "questions": {"value": "- Table 3 reports latency for forecasting. What compute environment is used for this inference, and how intense the runtime compute is? This matters because it is likely to run alongside high-load tasks such as video playback.\n- The table on page 15 appears disproportionately large. It might be better to consider resizing the table size.\n- The detail of implementations of the forecasting model is hard to understand. This would cause reproducibility problem. Especially, In Figure 9 the legend shows MLP/Attention. Are MLPs also used for the QKV projections? Are CLIP weights updated? Would clarity improve if the CLIP vision encoder and text encoder were depicted separately?\n- It seems the positions of Table 3 and Table 4 swapped."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0ZUmjZ573Q", "forum": "aECIKvAYcd", "replyto": "aECIKvAYcd", "signatures": ["ICLR.cc/2026/Conference/Submission16589/Reviewer_y7h1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16589/Reviewer_y7h1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993578053, "cdate": 1761993578053, "tmdate": 1762926663652, "mdate": 1762926663652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript makes valuable contributions to content-aware streaming, a critical area for optimizing subjective QoE. The proposed HiVid framework addresses two long-standing pain points in streaming weight generation—prohibitive human annotation costs and poor generalization of vision-saliency models—by innovatively leveraging LLMs as a scalable human proxy. The work is theoretically motivated, methodologically rigorous, and experimentally comprehensive, with clear validation of performance gains for both VOD and live streaming scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "HiVid’s core idea—using LLMs to replace costly human annotation for streaming chunk importance weighting—is both creative and problem-driven. Unlike prior work that relies solely on vision-based saliency or limited human labels, the framework bridges LLMs’ semantic understanding capabilities with streaming’s practical needs, addressing a critical scalability gap. The three modules (perception, ranking, prediction) are tightly aligned to solve non-trivial, scenario-specific challenges:"}, "weaknesses": {"value": "Clarify LLM implementation details: The manuscript does not specify which LLM(s) were used (e.g., GPT-4, open-source models like LLaMA) or how LLM inference latency was managed for live streaming. Adding these details will improve reproducibility and help readers assess HiVid’s computational feasibility for edge deployment.\n\nElaborate on module ablation studies: While the overall performance gains are reported, a brief summary of ablation experiments (e.g., how removing the LLM-guided merge-sort affects VOD accuracy, or the impact of content-aware attention on live prediction) would reinforce the contribution of each module.\n\nExpand on video diversity: The manuscript does not specify the types of videos evaluated (e.g., sports, movies, animations). Adding a note on whether HiVid generalizes across diverse content genres would further highlight its robustness."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YriZWg8oIr", "forum": "aECIKvAYcd", "replyto": "aECIKvAYcd", "signatures": ["ICLR.cc/2026/Conference/Submission16589/Reviewer_zB8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16589/Reviewer_zB8q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097111999, "cdate": 1762097111999, "tmdate": 1762926663081, "mdate": 1762926663081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We are very grateful to all reviewers for their critical and very constructive evaluation of our work. We summarize the main concerns and our updates in the main paper.\n\n---\n\nCommon questions:\n\n>**1. The underlying LLMs of HiVid like GPT-4o are proprietary, these may raise concerns about model consistency and robustness across runs.**\n\nWe address this issue by combining HiVid with open-source multimodal LLMs in Table 4 in main paper. The results prove that local inference can guarantee absolute consistency while achieving competitive accuracy, e.g. InterVL incurs PLCC of 0.64 compared with 0.66 from GPT-4o.\n\n---\n\n>**2. The detailed forecasting procedure in live streaming is hard to understand without consulting the appendix, especially about how the adaptive prediction coordinates with the LLM inference.**\n\nWe address this issue by adding a more detailed live streaming pipeline workflow in lines 345-356. We clarify that the key to real-time is the asynchronous LLM inference and forecasting, and adaptive decoding in Equ. 7 ensures that the predicted future weights have covered the inference delay. We further provide an example to depict the detailed timeline of each module.\n\n---\n\n>**3. The detailed ablation of each module of HiVid.**\n\nWe address this issue by clarifying the experiments in main paper Table 5. The increased accuracy compared with HiVid without either perception or ranking module proves the effectiveness. We further add ablation on our content attention design in forecasting model in Table 15. The accuracy gains compared with plain modality combination or uni-modal demonstrate our design novelty.\n\n---\n\nSpecific concerns from Reviewer 44Re:\n\n- **Potential terminology ambiguity**: We address by clearly denoting saliency score as subjective chunk-level importance score.\n\n- **Inherent instability or errors from LLMs**: We address by clarifying our cache-based fallback mechanism and thorough robustness evaluation across runs and models.\n\n- **Limited learning contribution**: We clarify our general application across subjective domains like video quality assessment to construct high-quality datasets, which facilitate fundamental model learning.\n\n- **Limited user study scale**: We address by extending our participant pool and providing inter-rater consistency and statistical significance.\n\n- **Forecasting performance with or without LLM ratings**: We address by clarifying the clean prediction in main Table 3 and providing clear accuracy comparison.\n\n- **Overhead analysis over longer live streaming**: We address by adding experiments on a 2-hour example video. We show the linearly increased costs from sliding window, stable accuracy and near-zero latency, which proves our effective prediction module.\n\n- **Question about the ground truth**: We clarify the datasets construction process, especially for Youtube-8M which collects the \"Most Replayed\" statistics for chunk-level importance score.\n\n---\n\nSpecific concerns from Reviewer y7h1:\n\n- **Computation intensity for forecasting**: We clarify that forecasting is executed along with each LLM response, therefore the computation is sparse, e.g. 10 seconds intervals.\n\n- **Table size presentation**: We address by adjusting each table and figure size.\n\n- **Forecasting model structure**: We clarify the QKV projection and CLIP usage. We also update Fig. 6 to highlight the trainable parameters.\n\n---\n\nSpecific concerns from Reviewer zB8q:\n\n- **Which underlying LLMs are used in HiVid**: We clarify the proprietary LLMs like GPT-4o as default model. However, we also include new experiments with open-source LLMs and demonstrate the generalization of our design.\n\n- **The detailed video types for evaluation**: We clarify the dataset scale that includes >6000 videos spanning different categories. We also clarify the robustness on specifically ambiguous videos in Table 7 in main paper.\n\nIn general, we have carefully addressed each issue by conducting new experiments or providing more detailed explanation of methodology design.\n\n---\n\n**Summary of changes in main paper:**\n\n- **New experiments**: We have added new Table 4, 8, 12, 13, 14, 15, accompanied by explanation and analysis marked in $\\\\text{\\\\color{blue}blue}$.\n\n- **Adjusted content**: We have moved the forecasting model structure and algorithm details to methodology section to improve clarity, as suggested by Reviewer 44Re.\n\n- **More detailed explanation**: We add some clarification about saliency terminology, dataset scale, LLMs usage and a new live streaming pipeline in the main sections.\n\nThe final main text is 10 pages with most additional experiments presented in the appendices."}}, "id": "pF9mFSAVey", "forum": "aECIKvAYcd", "replyto": "aECIKvAYcd", "signatures": ["ICLR.cc/2026/Conference/Submission16589/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16589/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission16589/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763461947999, "cdate": 1763461947999, "tmdate": 1763461947999, "mdate": 1763461947999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}