{"id": "EJ8HnNTEAv", "number": 19031, "cdate": 1758292856211, "mdate": 1759897064808, "content": {"title": "DGNet: Learning Spatiotemporal PDEs with Discrete Green Networks", "abstract": "Spatiotemporal partial differential equations (PDEs) underpin a wide range of scientific and engineering applications, yet classical solvers are computationally costly on large or irregular domains. Neural PDE solvers have emerged as an efficient alternative, but they struggle to generalize to unseen source terms, which represent external forcing such as heat generation, body forces, or reaction rates. Since existing models typically mix the source with the system state, they lack a principled mechanism to capture source responses. We propose DGNet, a discrete Green network that explicitly decouples system evolution from source response. The key idea is to transform the classical \nGreen’s function---a cornerstone of PDE theory---into a graph-based discrete formulation, preserving the superposition principle in a computable update rule. To ensure fidelity on irregular meshes, we construct a hybrid operator by combining physics-based discretizations with GNN-based corrections, while a lightweight residual GNN captures dynamics beyond the operator. Across three categories of spatiotemporal PDE scenarios, DGNet consistently achieves state-of-the-art accuracy. In particular, on the most challenging setting \nwith entirely novel source terms, DGNet maintains stable performance while existing approaches collapse.", "tldr": "", "keywords": ["Partial Differential Equations", "Graph Neural Networks", "Physics-Informed Machine Learning", "Generalization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00168a0dbe89b13f4dc6e9818168b0d7334e8aad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **DGNet (Discrete Green Network)** for learning **spatiotemporal PDEs** on irregular meshes, with a core design that **explicitly decouples system evolution from source response** via a **discrete Green’s function** formulation. Concretely, it discretizes ( $\\partial_t u = L_x[u] + f$ ) into a **Crank–Nicolson** update and realizes a per-step Green operator ($G(\\Delta t) = (I-\\tfrac{\\Delta t}{2}L)^{-1}$), enabling superposition of (i) state propagation and (ii) source-term response. The spatial operator is built as a **physics–neural hybrid** ($L=L_{\\text{physics}}+L_{\\text{neural}}$): ($L_{\\text{physics}}$) uses mesh-aware gradient/Laplacian discretizations, while ($L_{\\text{neural}}$) is a GNN correction for discretization errors; a lightweight residual GNN further captures leftover dynamics. An efficient **factorize-once, solve-many** sparse LU scheme makes long rollouts practical. Experiments across **classical PDEs**, **complex geometries**, and **unseen source terms** show consistent SOTA accuracy, with marked robustness when test-time sources differ from training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addresses challenging spatiotemporal PDEs on irregular meshes.**\n   The setting combines *both* complex geometry (unstructured meshes) and temporal evolution, which is closer to real CFD/physical simulations than static or grid-regular cases. Tackling this regime is practically meaningful. \n\n2. **Green’s-function–motivated decomposition.**\n   By formulating updates through a discrete Green operator, the method **explicitly separates** (i) the evolution of the initial state and (ii) the accumulated response to time-varying sources. This superposition-friendly design improves interpretability, allows cleaner handling of unseen source patterns, and provides a principled bridge between physics and learning.\n\n3. **Physics-informed design on irregular meshes.**\n   The hybrid operator and loss terms embed **physically consistent priors** (e.g., mesh-aware differential operators, stability-friendly time-stepping), encouraging fidelity to the governing equations while letting the learned components correct discretization errors. This tends to enhance robustness, boundary handling, and generalization across meshing/sampling changes."}, "weaknesses": {"value": "1. **Heavy reliance on pre-factorization; fairness not quantified.**\n   The method hinges on a **“factorize once, solve many”** sparse LU of ($(I-\\tfrac{\\Delta t}{2}L)$) before rollout, then reuses the factors each step. While efficient per step, this adds a non-negligible **precomputation and memory** burden uncommon in NN-only baselines, yet there’s **no cost table** (params/FLOPs/GPU memory/wall-clock) to establish fairness. The paper mentions hardware (4×RTX4090) and vague training durations (“hours to one day”) but lacks matched-budget reporting across methods. A standardized efficiency comparison is needed.  \n\n2. **Small problem sizes vs. stated motivation.**\n   Core 2D meshes are relatively modest (e.g., **Cylinder ~2.3k nodes**, Sediments ~5.8k, Complex Obstacles ~8.8k; classical PDEs 1.3k–10k). These scales undercut the claim of handling challenging irregular domains; results on **larger meshes/3D** or higher Reynolds/complex boundaries would better substantiate practical robustness. \n\n3. **Baseline scope/positioning leave doubts.**\n   The baseline set (DeepONet, MGN, MP-PDE, PhyMPGN, **BENO**) mixes operator, graph, and hybrid methods, but (i) it’s unclear **how comparability is enforced** beyond a sentence (“tuned to have comparable parameters/training costs”) and (ii) some choices may be **mismatched to spatiotemporal settings** (e.g., BENO targets elliptic PDEs), while **strong recent multiscale/graph/transformer/point-operator** [1-5] baselines on irregular domains are missing. A head-to-head with closer, stronger contemporaries and a **capacity-controlled** comparison would sharpen novelty and fairness. \n\n[1] Feng, Mingquan, et al. \"SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Zhang, Xuan, et al. \"SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations.\" The Twelfth International Conference on Learning Representations.\n\n[3] Wang, Qi, et al. \"P $^ 2$ C $^ 2$ Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics.\" Advances in Neural Information Processing Systems 37 (2024): 68897-68925.\n\n[4] Li, Zhihao, et al. \"Harnessing scale and physics: A multi-graph neural operator framework for pdes on arbitrary geometries.\" Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1. 2025.\n\n[5] Wu, Haixu, et al. \"Transolver: A Fast Transformer Solver for PDEs on General Geometries.\" International Conference on Machine Learning. PMLR, 2024."}, "questions": {"value": "1. **Precomputation & fairness.**\n   Please report a cost table (params, FLOPs/step, GPU memory, LU pre-factorization time for Eq. (9), wall-clock/epoch, hardware) and confirm matched training budgets across baselines.\n\n2. **Scalability & factorization reuse.**\n   Results on substantially larger meshes (and ideally 3D) would align with the motivation. Also clarify when (L) changes across samples (coefficients/BCs/mesh): must you re-factor each time, and what is the end-to-end impact?\n\n3. **Benchmarks & baseline selection.**\n   Add stronger, recent multiscale/transformer/graph operator baselines for irregular spatiotemporal PDEs, and justify inclusion of elliptic-focused baselines. For **cylinder flow**, reconcile why baseline errors are far worse than commonly reported (detail resolution/Re/splits/targets/configs).\n\n4. **Robustness & hybrid ablations.**\n   Provide stress tests (larger $(\\Delta t)$, stiff sources, mesh noise/density shifts) and ablations separating $(L_{\\text{physics}})$ vs. $(L_{\\text{neural}})$ to show stability and the benefit of the physics–neural hybrid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DS8AxKnwMq", "forum": "EJ8HnNTEAv", "replyto": "EJ8HnNTEAv", "signatures": ["ICLR.cc/2026/Conference/Submission19031/Reviewer_pbbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19031/Reviewer_pbbn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550793557, "cdate": 1761550793557, "tmdate": 1762931073615, "mdate": 1762931073615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes DGNet, a GNN-based model, to solve a class of PDEs. The method provides an explicit way to decouple the system evolution term from the effect of the source term based on the superposition principle of the PDE theory. The authors, in addition, investigate the relation between the Green’s function and a discrete time integration method, yielding the discrete counterpart of the superposition using the Green’s function. Further, the spatial derivative operator is expressed as a mixture of the classical numerical method and GNN. The experimental results show that the proposed method demonstrates the best accuracy among the considered baselines and generalization to unseen source terms. The authors conduct an ablation study to confirm that the proposed ingredients work properly."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The focus on the source term is interesting. Since the source/forcing term is used across a wide range of research and engineering applications, the work is of great importance to the community.\n2. The method is simple and reasonable. The authors successfully leverage classical theory to develop an effective approach for elegantly handling the source term."}, "weaknesses": {"value": "1. The applicable domain and limitations of the work are unclear. Since the method relies on the superposition and linear approximation of the spatial operator $\\mathcal{L}_\\boldsymbol{x}$, the reviewer considers it to work only for semilinear PDEs, although the paper says “PDE” with no specifications. The authors should clarify the assumptions and limitations of the work.\n2. Section 3.4.1 is unclearly written. The paper uses $A_i$ and $d_i$ for volume and area, respectively, but this is outside the standard notation. In addition, there is no explanation about $N(i)$, $\\alpha_{ij}$, and $\\beta_{ij}$. The reviewer recommends adding a figure showing the variable settings for improved readability.\n3. The experimental evaluation is weak. The work uses machine learning methods as baselines, but the comparison of computational speed and accuracy with a classical numerical solver should also be provided. In particular, since the method includes the scheme from the classical solver, it is not obvious that the proposed approach is more efficient than its classical counterpart. In addition, since machine learning methods have some error, the evaluation of a classical solver should be performed across multiple settings, e.g., varying the mesh resolution and convergence threshold, to capture the speed-accuracy tradeoff.\n\nMinor point:\n\n* Labels in Figure 5 and description in the main text are not aligned (e.g., w/o fit in the table vs w/o Residual GNN in the text). The reviewer recommends aligning them in the same word."}, "questions": {"value": "1. The method assumes $u$ represents a scalar field (line 125), but can we extend it to vector and tensor fields?\n2. How is the source term varied in the datasets? Can the method generalize to a source with a norm larger than that of the training dataset (e.g., a two-times larger source)? If not, the authors should explicitly present the limitations and the cases where the proposed model fails.\n3. The work assumes the mesh is generated using the Delaunay triangulation. Why did the authors choose that configuration? Can we consider using other types of meshes, e.g., quadrilateral and polygonal meshes? How about 3D meshes? What would be the difficulty in extending the method?\n4. The paper argues for the necessity of the neural correction operator for coarse or skewed meshes (line 265), but since the mesh is generated using the Delaunay triangulation, the reviewer considers there to be no skewed mesh in the present work. In addition, in the experiments, in particular, the FitzHugh–Nagumo dataset, the mesh resolution seems high enough. In that case, how does the author explain the necessity of the neural correction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R3zXDkzL5a", "forum": "EJ8HnNTEAv", "replyto": "EJ8HnNTEAv", "signatures": ["ICLR.cc/2026/Conference/Submission19031/Reviewer_dKtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19031/Reviewer_dKtN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921949542, "cdate": 1761921949542, "tmdate": 1762931073164, "mdate": 1762931073164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DGNet, a discrete Green network framework for solving spatiotemporal PDEs with a particular focus on explicit modeling of source terms.\nUnlike conventional neural PDE solvers that implicitly mix the source and system state, DGNet explicitly decouples the system evolution from the source response, inspired by the Green’s function formalism.\nThe model constructs a graph-based discrete operator that preserves the superposition principle. The operator $\\mathcal{L}$ is composed of a physics-based component $\\mathcal{L}{\\text{physics}}$ and a neural correction component $\\mathcal{L}{\\text{NN}}$, where the former computes spatial derivatives such as gradients and Laplacians numerically, and the latter leverages a message passing neural network (MPNN) to correct approximation errors.\nDGNet further incorporates Crank–Nicolson time integration to ensure stability in temporal evolution. Across multiple PDE benchmarks, including irregular mesh scenarios and novel source terms, DGNet achieves state-of-the-art accuracy and demonstrates strong robustness compared to existing operator-learning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a longstanding limitation in neural PDE solvers—their inability to generalize to unseen source terms—by explicitly incorporating the Green’s function concept into a learnable, graph-based framework.\nThis formulation is conceptually elegant: by separating the effect of the source term from the system dynamics, DGNet captures the response structure in a principled and interpretable manner.\nThe combination of physics-based discretization and neural correction strikes a strong balance between numerical fidelity and data-driven flexibility. In particular, the hybrid operator $\\mathcal{L} = \\mathcal{L}{\\text{physics}} + \\mathcal{L}{\\text{NN}}$ effectively merges computational stability with adaptability, while maintaining computational efficiency.\nEmpirical results show exceptionally high performance—often outperforming existing baselines by large margins—and stability on unseen forcing terms.\nOverall, DGNet’s design provides a conceptually grounded and practically effective approach for learning PDE dynamics on irregular meshes and under novel source conditions."}, "weaknesses": {"value": "Despite its strong results, the paper suffers from several clarity and interpretability issues that limit its accessibility.\nFirst, the presentation of the operator $\\mathcal{L}$ and its components lacks precision. Although equations define $\\mathcal{L}{\\text{physics}}$ and $\\mathcal{L}{\\text{NN}}$, it remains unclear how $\\mathcal{L}{\\text{physics}}$ is numerically computed and integrated into the overall update rule—whether gradients and Laplacians are used merely as input features or as direct numerical operators.\nSimilarly, the interaction between $\\mathcal{L}{\\text{NN}}$ (the MPNN correction) and the source term $f$ is only superficially discussed, leaving readers uncertain about how the model actually combines physical and learned dynamics to advance the PDE state $u$.\nAlthough the abstract emphasizes source-term generalization, the main text provides limited explanation of how the method ensures this capability beyond architectural intuition.\nSecond, the neural component is relatively minimal. Apart from the residual MPNN correction, the rest of the solver heavily relies on standard numerical discretizations. While this hybrid structure is defensible, the role of the NN component could be elaborated to justify the “learning” aspect of the framework.\nThird, the evaluation protocol raises concerns about robustness. Reported test sets are extremely small (3–20 samples), making it difficult to confidently assess generalization claims. The performance gains may partially stem from limited sampling rather than true model generalization.\nFinally, after the numerical solution step, the exposition becomes sparse, with several mathematical derivations (Eqs. 5–7) presented without intuitive interpretation or discussion."}, "questions": {"value": "- In the ablation study, does “w/o $\\mathcal{L}_{\\text{NN}}$” correspond to solving the PDE purely via numerical methods (i.e., without any learned correction)?\n- How exactly is $\\mathcal{L}_{\\text{physics}}$ computed in practice—are the gradients and Laplacians incorporated as feature inputs to the MPNN, or directly as numerical differential operators?\n- The paper emphasizes strong generalization to unseen source terms, but given the small test sizes (e.g., 3–20), can the authors provide additional experiments or statistical evidence to support this claim?\n- How does DGNet compare to PHYMPGN (ICLR 2025) under identical experimental settings? The differences in reported performance seem unexpectedly large.\n- Could larger-scale experiments (more trajectories or longer temporal horizons) be conducted to further validate generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YYtHA4EKYG", "forum": "EJ8HnNTEAv", "replyto": "EJ8HnNTEAv", "signatures": ["ICLR.cc/2026/Conference/Submission19031/Reviewer_HxRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19031/Reviewer_HxRT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974877442, "cdate": 1761974877442, "tmdate": 1762931072746, "mdate": 1762931072746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DGNet, a neural solver for spatiotemporal PDEs that explicitly decouples the system evolution from the source response by turning Green’s function theory into a discrete, graph-based update rule. Starting from a Crank–Nicolson semi-discretization, the authors derive a one-step update that can be interpreted as applying a discrete Green operator to propagate the current state and accumulate the effect of the source over the time slab. Experiments span classical PDEs, transport on irregular domains, and generalization to unseen source terms. DGNet reports SOTA accuracy across tasks and especially large gains when tested on entirely novel sources; an ablation suggests the discrete Green solver is the key driver of performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Solid derivation of a discrete Green update tied to an implicit Crank–Nicolson step, with practical “factorize once, solve many” sparse solves; the operator depends only on mesh and \\Delta t, enabling reuse during rollout.\n\n- Well-motivated hybrid operator that anchors learning with physics priors (Green–Gauss gradient, cotangent Laplacian) plus GNN corrections; clear architectural overview.\n\n- Broad empirical coverage with consistent SOTA on both log-MSE and relative $\\ell_2$ across diverse regimes; the table highlights DGNet edges on every scenario."}, "weaknesses": {"value": "- Some implementation details (e.g., GPU sparse LU and custom adjoint) are in the appendix; a brief complexity/throughput table in the main text would help readers assess practicality across datasets.\n\n- Uses standard message-passing blocks; novelty is concentrated in the Green discretization + operator split, not in GNN mechanics.  \n\n- Although there is an ablation, the residual GNN’s incremental benefit and the sensitivity to factorization accuracy (e.g., fill-in thresholds, preconditioners) could be quantified more rigorously beyond one scenario.\n\n- While the CN-derived update is principled, the scope of PDE classes is primarily parabolic/weakly nonlinear; truly strongly nonlinear or hyperbolic systems are deferred to future work.\n\n- Stability/consistency guarantees for the learned correction are not theoretically analyzed.\n\n- Benchmarks are 2D; without a 3D case, claims about broad scientific impact are somewhat aspirational."}, "questions": {"value": "1. Stability & constraints on $L_{\\text{neural}}$.\nCan you provide spectral/energy bounds or constraints on $L_{\\text{neural}}$ (e.g., skew-symmetry, diagonally dominant correction, Lipschitz bounds) that preserve the A-stability properties of the CN-like update when combined with $L_{\\text{physics}}$? A short lemma or empirical spectrum plots across datasets would strengthen confidence.\n\t\n2. Ablations beyond one geometry.\nYour ablation indicates “w/o Green” is worst on complex obstacles. Could you add two more scenarios (one classical PDE, one laser heat) to separate the contributions of $L_{\\text{physics}}$, $L_{\\text{neural}}$, and the residual GNN more generally? This would clarify the portability of each component. \n\t\n3. Runtime, memory, and scaling.\nPlease include a main-text table: per-step wall-clock, memory footprint, and speedup vs. baselines at equal accuracy, and a mesh-scaling plot up to your largest meshes. What happens if the sparse LU factorization is replaced with ILU/AMG preconditioning for larger systems?\n\t\n4. Robustness to source characteristics.\nThe laser task varies paths; could you report sensitivity to source bandwidth/amplitude and number of concurrent sources (e.g., 1 vs. 10 lasers), including failure modes where superposition might break due to strong nonlinearity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L5h5a3beOm", "forum": "EJ8HnNTEAv", "replyto": "EJ8HnNTEAv", "signatures": ["ICLR.cc/2026/Conference/Submission19031/Reviewer_xgKd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19031/Reviewer_xgKd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236949722, "cdate": 1762236949722, "tmdate": 1762931072349, "mdate": 1762931072349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}