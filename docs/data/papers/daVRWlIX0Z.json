{"id": "daVRWlIX0Z", "number": 6929, "cdate": 1758002425070, "mdate": 1759897883475, "content": {"title": "FedDOR: Orthogonal Initialization and Dual Regularization for Prototype Integrity in Heterogeneous Federated Learning", "abstract": "Heterogeneous Federated Learning (HFL) has garnered significant attention for its potential to leverage decentralized data while preserving privacy. One fundamental challenge in HFL is how to drastically reduce the high communication cost of transmitting model parameters. Prototype-based HFL methods have recently emerged, which exchange only class-wise representations (prototypes) among heterogeneous clients to achieve model training. However, existing methods fail to maintain the semantic integrity of prototypes during the aggregation process, compromising the global model performance in HFL. To overcome the challenge of semantic degradation in prototype aggregation, we propose a novel HFL approach termed FedDOR, which leverages Dual Orthogonal Regularization (DOR) to learn consistent and discriminative prototypes. On the client-side, our key insight is orthogonally initializing prototype embeddings to impose a maximally separated and uniformly distributed prior geometry on the feature space, providing a consistent and optimal learning target. On the server-side, DOR enforces geometric constraints to explicitly minimize intra-class variance while enlarging inter-class separation. Extensive experiments demonstrate that FedDOR achieves superior accuracy over state-of-the-art methods by significant margins, while fully preserving the communication efficiency and privacy advantages of prototype-based federated learning.", "tldr": "", "keywords": ["Federated Learning", "Heterogeneous", "Prototype", "Orthogonal Regularization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7b085b9f8bc6b3808f6aaf4038fb8c8eaa1f7dd.pdf", "supplementary_material": "/attachment/76840a182c930cb6f54f81c3d12a12711d645c12.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces FedDOR (Federated learning with Dual Orthogonal Regularization), a novel heterogeneous federated learning method that addresses the challenge of semantic degradation in prototype aggregation. The key innovation is the use of orthogonal initialization for prototype embeddings on the client side combined with dual orthogonal regularization (intra-class and inter-class) on the server side. The method aims to maintain prototype integrity during aggregation while preserving communication efficiency inherent to prototype-based federated learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper effectively identifies and visualizes (Figure 1) the limitations of existing prototype-based HFL methods, particularly the semantic degradation and feature aliasing issues that occur during naive aggregation.\n2.  The experiments cover multiple datasets (CIFAR-10, CIFAR-100, Flowers102, Tiny-ImageNet), various heterogeneity settings (both model and data), and include ablation studies that validate each component's contribution.\n3. The paper demonstrates robustness with large numbers of clients (50-100), which is important for real-world deployment."}, "weaknesses": {"value": "1. While the orthogonal initialization is well-motivated, the paper lacks formal convergence guarantees or theoretical analysis of how the dual regularization affects the optimization landscape in federated settings.\n2. The paper doesn't analyze the additional computational cost of the orthogonal initialization and the server-side prototype generation module G(E; ΩG). \n3. While the paper claims \"strong privacy protection,\" there's no formal privacy analysis or comparison with differential privacy-based methods. \n4. While Table 6 shows some hyperparameter analysis, the method introduces several hyperparameters (λCE, λalign, scaling factor) whose selection process and sensitivity across different datasets isn't thoroughly investigated.\n5. Some related FL works in prototype learning are missing in related works:\n[1] Rethinking Federated Learning with Domain Shift: A Prototype View\n[2] Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains"}, "questions": {"value": "Please find above in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1jHykwOPvr", "forum": "daVRWlIX0Z", "replyto": "daVRWlIX0Z", "signatures": ["ICLR.cc/2026/Conference/Submission6929/Reviewer_oiEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6929/Reviewer_oiEP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589017636, "cdate": 1761589017636, "tmdate": 1762919163451, "mdate": 1762919163451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a well-motivated approach to HFL by introducing orthogonal geometric priors and dual regularization, which effectively mitigate prototype collapse and class overlap under heterogeneity. The experimental evaluation is comprehensive, covering diverse datasets, model architectures, and non-IID settings. However, the method’s novelty relative to closely related works like FedORGP could be more clearly delineated. Additionally, while ablation studies validate key components (Table 4), the paper does not discuss computational overhead or failure cases under extreme heterogeneity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The dual orthogonal loss (intra-class and inter-class) enhances feature discriminability and reduces class overlap.\n\n Extensive evaluations under model heterogeneity (MHMG groups), statistical heterogeneity (Dirichlet and pathological splits), and scalability (large client counts) demonstrate consistent improvements.\n\nAblation results clearly justify the contribution of each component (orthogonal initialization and dual regularization).\n\nThe framework is well-described with algorithmic pseudocode and architectural diagrams."}, "weaknesses": {"value": "While FedORGP also uses orthogonality, the specific advantages of FedDOR’s initialization and dual regularization are not rigorously compared or analytically justified.\n\nNo analysis of the additional overhead from orthogonal initialization or prototype generation on the server.\n\nThe method’s limitations or scenarios where it underperforms are not discussed (e.g., under extreme class imbalance or very low feature dimensions).\n\nAlthough orthogonality is motivated geometrically, no theoretical guarantees (e.g., convergence or separation bounds) are provided.\n\nThe method relies on careful balancing of $λ_{CE}$ and $λ_{align}$, but no systematic sensitivity analysis beyond a grid search is included.\n\nThe method assumes all clients share the same class set, which may not hold in personalized FL; no evidence of testing under partial label spaces."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zea6U6SKVm", "forum": "daVRWlIX0Z", "replyto": "daVRWlIX0Z", "signatures": ["ICLR.cc/2026/Conference/Submission6929/Reviewer_SS1N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6929/Reviewer_SS1N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618996608, "cdate": 1761618996608, "tmdate": 1762919162970, "mdate": 1762919162970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method FedDOR for heterogeneous federated learning that focuses on learning in heterogeneous settings via prototype sharing and aggregation. The local clients generate class prototypes and share it with the server instead of sharing model parameters, the server appropriately learns from the shared prototypes and transfers revised prototypes to all clients in the next round to achieve collaboration across all clients. The key innovations of the paper include : i) orthogonal initialization of the prototype embeddings on clients, ii) Dual orthogonal regularization on the server to enhance the prototypes in each round."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well motivated and the angle of learning aggregated prototypes at the server instead of performing aggregation is novel. The idea of orthogonal initialization of embeddings locally on each client is also neat.\n2. Evaluations span multiple datasets, model heterogeneity levels and statistical heterogeneity."}, "weaknesses": {"value": "1. While paper emphasizes geometric motivation, there is no discussion supporting the theoretical insights and/or impact of that for the algorithm.\n2. The framework, including the local and global loss functions are very similar to the ones explored in the FedORGP paper. \n3. The literature review misses some of the papers in FL that use similar techniques."}, "questions": {"value": "1. How are the missing classes on clients handled, do the clients still send the initialized embedding for the prototypes for which it does not contain any data or skips it?\n2. Is geometric consistency preserved after few training rounds, and what is the isolated impact of having the orthogonal initializations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Si5aM5nKyC", "forum": "daVRWlIX0Z", "replyto": "daVRWlIX0Z", "signatures": ["ICLR.cc/2026/Conference/Submission6929/Reviewer_FapM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6929/Reviewer_FapM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712742185, "cdate": 1761712742185, "tmdate": 1762919162489, "mdate": 1762919162489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of prototype aggregation in prototype-based Heterogeneous Federated Learning (HFL) by introducing Dual Orthogonal Regularization (DOR). The proposed approach operates on both the client side and the server side. Orthogonality of prototypes is accomplished by initializing local prototype embeddings on the client side and applying geometric constraints to minimize intra-class variance while enlarging inter-class separation on the server side. Extensive experiments under various heterogeneous settings and datasets demonstrate the superiority of the proposed method over existing prototype-based HFL approaches. However, the score of this paper tends to be below the acceptance threshold because: (1) the method is largely incremental over the existing FedORGP, primarily adding orthogonal initialization on the client side, and (2) the contribution and mechanism of this newly added component require clearer explanation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses one of the fundamental problems in prototype-based HFL — the lack of sufficient separation among prototypes — and attempts to solve it through a simple yet effective methodological extension. Extensive experiments under various heterogeneous settings and datasets demonstrate the superiority of the proposed method over existing prototype-based HFL approaches."}, "weaknesses": {"value": "(1) The method is largely incremental over the existing FedORGP, primarily adding orthogonal initialization on the client side. To address this concern, please clarify and provide detailed explanations for the following:\n\n(1-a) The global prototype generation method in Section 3.4 appears similar to that proposed in FedORGP. What are the specific differences between your approach and FedORGP’s? Please explicitly state what is novel in your server-side mechanism beyond the dual orthogonal regularization losses.\n\n(1-b) The authors claim orthogonal initialization of W provides a “geometrically optimal prior” (line 144) and “consistent and discriminative learning objectives” (line 145). After initialization, classifier weights are updated via gradient descent using $L_{CE}$ and $L_{align}$ (Eq. 2). How does the initial orthogonality persist to provide consistent objectives throughout 100 rounds of training? Please provide a theoretical background or rationale behind your claims.\n\n(2) The contribution and mechanism of this newly added component require clearer explanation. To address this concern, please clarify and provide detailed explanations for the following:\n\n(2-a) In Section 3.2, what are the “prototype embeddings” $W ∈ R^{C×d}$ mentioned in line 146? Are they distinct from the local prototypes $p_k^c$ defined in Eq. (5)? Based on the dimensionality $C×d$ and context, they appear to represent classifier weights(parameters)—please confirm or clarify.\n\n(2-b) If W represents classifier weights(parameters), how exactly is orthogonal initialization performed in your implementation? Are the clients’ models pre-trained before federated learning begins? If so, provide the training details regarding the initialization of W.\n\n(2-c) Regarding the results in Table 4, the results on Flowers102 and Tiny-ImageNet raise questions about the contribution of each component. Since OI is applied only at the initialization stage, how can it contribute more significantly to performance improvement than DOR, which is continuously applied during training? To accurately understand the proposed method, could you provide additional interpretation or experimental results explaining this phenomenon?\n\nAlso, there are several minor things to improve the paper:\n\n(3) Figure 2: Improve the resolution to make it easier to read.\n\n(4) The ablation study comparison baseline should be FedProto. If my understanding is correct, the proposed method is essentially FedProto + OI + DOR. If the purpose of the ablation study is to show the contribution of each component to performance gain, then the first column of Table 4 should use FedProto as the baseline for a fair and accurate comparison."}, "questions": {"value": "Please refer to the Weakness section for detailed comments. In particular, I would appreciate clarification on the questions raised for each weakness. I will reconsider my evaluation after reviewing the authors’ rebuttal to these points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OhWMBx97Xt", "forum": "daVRWlIX0Z", "replyto": "daVRWlIX0Z", "signatures": ["ICLR.cc/2026/Conference/Submission6929/Reviewer_WHYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6929/Reviewer_WHYS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814545751, "cdate": 1761814545751, "tmdate": 1762919161945, "mdate": 1762919161945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}