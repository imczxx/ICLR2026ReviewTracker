{"id": "yh7MV2V0ba", "number": 11927, "cdate": 1758204681246, "mdate": 1759897545573, "content": {"title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling", "abstract": "Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines.", "tldr": "", "keywords": ["Masked diffusion models", "Variational autoencoders", "Latent variable models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c5399eda0e2ba2b02711639ee6d6e8d1b634d26.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes VADD, a generative framework combining discrete diffusion models  with a variational continuous latent variable. The goal is to address the limitations of purely discrete diffusion by introducing a latent variable z that captures global structure, while the discrete diffusion handles token-level generation. The joint training objective is derived as a variational ELBO decomposition into generative model and recognition model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper presents a novel idea that introduces latent space modelling to discrete diffusion models, with a VAE-type learning objective formulation. \n2.\tExperiment results on image and text generation demonstrate the effectiveness of the proposed model."}, "weaknesses": {"value": "1. The motivation for introducing a latent variable into the discrete diffusion model is not sufficiently justified. As discussed in Paragraph 2 of Section 1, the issue with MDMs becomes more pronounced when the number of denoising steps is small. However, if a sufficient number of denoising steps are used, would incorporating a latent variable still offer any advantage in terms of accuracy or efficiency? The paper provides no analysis or evidence to support this claim.\n\n2. For image generation, how is the performance comparison with SOTA continuous diffusion models? The comparison is conducted with only non-latent space modeling, which seems not sufficient.\n\n3. In Section 4, a couple of related works are mentioned. Although there is difference between them and the proposed VADD model,  they need to be incorporated as baselines.\n\n4. No code is released for reproducibility checking.\n\n5. For text generation task, how is the latent space defined?\n\n6. Several symbols like “Cat” are not defined in the paper."}, "questions": {"value": "Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bs1RIrb9Ak", "forum": "yh7MV2V0ba", "replyto": "yh7MV2V0ba", "signatures": ["ICLR.cc/2026/Conference/Submission11927/Reviewer_Dd6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11927/Reviewer_Dd6V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807347491, "cdate": 1761807347491, "tmdate": 1762922934029, "mdate": 1762922934029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Variational Autoencoding Discrete Diffusion (VADD), which is a framework that augments masked discrete diffusion models (MDMs) with a latent variable to better capture the correlations between different dimensions. Specifically, the transition probability $p_{\\theta}(x_s | x_t)$ in the backward step is parametrized as the marginal of the product distribution between prior distribution $p(z)$ of the latent variable and the conditional denoising distribution $p_{\\theta}(x_s | x_t, z)$ given the latent variable $z$. As the marginal is generally intractable, the authors introduced a “double ELBO” (DELBO) framework to jointly train an auxiliary recognition model $r_{\\phi}(z | x_s, x_t)$ that approximates the distribution $p_{\\theta}(z | x_0, x_t)$ and the conditional denoiser $p_{\\theta}(x_0 | x_t, z)$. Experiments on 2D multimodal toy distributions, pixel-level image generation on MNIST and CIFAR, as well as text generation via MDMs are provided to justify the effectiveness of proposed method, which implies that adding an amortized latent to discrete diffusion does improve the quality of samples generated by MDMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The reviewer finds both the presentation and the motivation of the paper to be crystal clear. To the best of the reviewer's knowledge, this is probably the first work that tries to address the problem of capturing the correlation between different dimensions via the latent variable structure in MDMs, which is motivated from existing work [1] that uses the latent variable structure for next token prediction in autoregressive models. Details about the choice of hyperparameters in the experiments are also included."}, "weaknesses": {"value": "(1) For the image generation task, this paper only tested the proposed methodology on relatively small-scale datasets like MNIST and CIFAR10. In contrast, existing work [2,3] applying MDMs and related variants to image generation have tested on larger-scale datasets like ImageNet 256. The authors are encouraged to include an extra set of experiments on large-scale image datasets like ImageNet to better illustrate the effectiveness of the proposed methodology. (For instance, the authors may refer to the experimental setting of [3].)\n\n(2) Given that the method proposed in this work can be categorized as a training-based type of method for improving the quality of the samples generated by MDMs, the authors might need to perform a more detailed literature review on related methods. For instance, when the authors discussed distillation-type of methods as related work at the beginning of section 4, they only mentioned [7] and missed a few other concurrent work like [4,5,6]. Also, the quality of the paper can be further improved by adding extra training-based baselines (like those based on distillation) to justify the effectiveness of VADD."}, "questions": {"value": "For the backward transition probability parametrized by the latent variable defined in equation (6), would it possible to make the prior distribution of the latent variable $z$ dependent on the time variable $t$? The reviewer thinks that it might be more reasonable to do so, as the correlation between different dimensions also varies as time changes. (Otherwise, would it be possible for the authors to comment on why it makes sense to use the same prior distribution for all time $t$ here?)\n\nMoreover, it seems that the authors only tested the effect of VADD for MDMs. Would it be possible for the authors to comment on/briefly discuss whether the same idea of using latent variable models can also be applied to the case of uniform discrete diffusion models or not?\n\nOverall, the reviewer thinks that the paper might be considered for top ML venues like ICLR, but the authors should probably address all questions above, add papers listed below as extra references and discuss them appropriately.\n\nReferences:\n\n[1] Kong, Deqian, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si et al. \"Scalable language models with posterior inference of latent thought vectors.\" arXiv e-prints (2025): arXiv-2502.\n\n[2] Hayakawa, Satoshi, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, and Yuki Mitsufuji. \"Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion.\" arXiv preprint arXiv:2510.04525 (2025).\n\n[3] Li, Tianhong, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. \"Mage: Masked generative encoder to unify representation learning and image synthesis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2142-2152. 2023.\n\n[4] Fu, Feiyang, Tongxian Guo, and Zhaoqiang Liu. \"Learnable Sampler Distillation for Discrete Diffusion Models.\" arXiv preprint arXiv:2509.19962 (2025).\n\n[5] Zhu, Yuanzhi, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. \"Di [M] o: Distilling masked diffusion models into one-step generator.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 18606-18618. 2025.\n\n[6] Zhu, Yuanzhi, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. \"Soft-di [m] o: Improving one-step discrete image generation with soft embeddings.\" arXiv preprint arXiv:2509.22925 (2025).\n\n[7] Hayakawa, Satoshi, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, and Yuki Mitsufuji. \"Distillation of discrete diffusion through dimensional correlations.\" arXiv preprint arXiv:2410.08709 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7XyxA3maio", "forum": "yh7MV2V0ba", "replyto": "yh7MV2V0ba", "signatures": ["ICLR.cc/2026/Conference/Submission11927/Reviewer_qhUA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11927/Reviewer_qhUA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080682743, "cdate": 1762080682743, "tmdate": 1762922932640, "mdate": 1762922932640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Variational Autoencoding Discrete Diffusion (VADD) framework which augments masked diffusion models (MDM) with latent variable modeling (using an auxiliary recognition model) for implicitly capturing the dependencies between different tokens when unmasking during the denoising process. This results in a more expressive unmasking framework in the low timestep regime. Empirical results show that VADD outperforms the MDM framework on different benchmarks in the low timestep regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I specifically like the problem formulation in the paper. The idea of using latent variables to model the dependencies between different dimensions is quite straightforward yet elegant.\n\n2. The paper is well written in the sense that the method is easy to follow and the empirical results establish the advantages of the method over the baseline MDLM model."}, "weaknesses": {"value": "**Looseness of the Double ELBO (DELBO) bound**: Can the authors comment on how loose is the bound in Eq. 9 compared to the original log likelihood i.e. log p(x_0) and add more intuition in the main text. This could be quite important for applications where metrics like bits per dim (bpd) are quite important.\n\n**Missing Related Work**: The related work section in the paper is extremely limited. Can the authors discuss more about related work in discrete diffusion models, VAEs and posterior collapse. There is a large body of work in discrete diffusion which might be complimentary to the framework introduced by the authors. However an extensive discussion is currently missing from the paper.\n\n**Re. Empirical Results**:\n\n1. Can the authors add continuous diffusion based baselines for image generation for CIFAR-10? Based on my understanding the image quality in Table 3 is rather bad for CIFAR-10 and continuous time methods do much better on this benchmark. In light of this maybe these results can be moved to the Appendix altogether and the remaining space could be utilized for adding more intuitions on the theoretical results in the paper. Moreover, I would request the authors to include qualitative results for CIFAR-10 in the Appendix for a better sense on the quality of the samples.\n\n2. Do the authors have insights about the scaling properties of VADD with model size in terms of perplexities for the text benchmarks?\n\n**Discussion of limitations**: There is limited discussion on the limitations of the methods. In my understanding enforcing the recognition model to lie close to the prior would most likely suffer from prior hole problems since the underlying recognition model can be quite complex. In general, the proposed framework would inherit the same problems that classical VAE based approaches suffer from. Therefore, I think its quite important to discuss these limitations in the main text."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T6ETtWWVqe", "forum": "yh7MV2V0ba", "replyto": "yh7MV2V0ba", "signatures": ["ICLR.cc/2026/Conference/Submission11927/Reviewer_UPfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11927/Reviewer_UPfa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762211088947, "cdate": 1762211088947, "tmdate": 1762922931514, "mdate": 1762922931514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes variational autoencoding discrete diffusion to enhance the dimensional correlation modeling. Along with introducing additional recognition model, they also propose some adaptions to architecture design. The experiment shows that the model achieve better FID than MDLM in text and image modeling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The performance of model outperforms other discrete diffusion model in pixel and language tasks."}, "weaknesses": {"value": "1. The paper does not technically sound to me. It is not clear why adding latent variable $z$ contribute to model performance since in the sampling, $z$ is simply sampled from prior $p(z)$ distribution without using recognition model. $z$ seems like a style vector like StyleGAN or DDGAN which just help to increase the stochasticity of the architecture.\n\n2. Why not using the train recognition model for other iteration $i < T$ like hierarchical latent VAE technique. It seems a wasteful for training   recognition model.\n\n3. The performance on pixel space seems marginal. \n\n4. The proposed architecture is simple for denoising model, and does not show much novelty and lacks ablation study on different way to integrate $z$ into the model architecture in both denoising and recognition model."}, "questions": {"value": "See that weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YbBoeBIynz", "forum": "yh7MV2V0ba", "replyto": "yh7MV2V0ba", "signatures": ["ICLR.cc/2026/Conference/Submission11927/Reviewer_ZCGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11927/Reviewer_ZCGk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762246732254, "cdate": 1762246732254, "tmdate": 1762922931078, "mdate": 1762922931078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}