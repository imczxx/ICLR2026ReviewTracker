{"id": "khEYpErtMl", "number": 21255, "cdate": 1758315469011, "mdate": 1763673959644, "content": {"title": "Calibrating Generative Models", "abstract": "Generative models frequently suffer miscalibration, wherein class probabilities and other statistics of the sampling distribution deviate from desired values. We frame calibration as a constrained optimization problem and seek the closest model in Kullback-Leibler divergence satisfying calibration constraints.  To address the intractability of imposing these constraints exactly we introduce two surrogate objectives for fine-tuning: (1) the relax loss, which replaces the constraint with a miscalibration penalty, and (2) the reward loss, which converts calibration into a reward fine-tuning problem. We demonstrate that these approaches substantially reduce calibration error across hundreds of simultaneous constraints and models with up to one billion parameters, spanning applications in protein design, image generation, and language modeling.", "tldr": "We propose a general method to calibrate sample statistics of generative models, with applications to diffusion, autoregressive, and normalizing flow models.", "keywords": ["fine-tuning", "reward optimization", "protein design models", "maximum entropy", "diffusion models"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6684d937ef857f246d2ea7772c82c8a5d0700a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose two methods for performing generative modeling under moment constraints:\n\n(1) *CGM-relax* encodes the constraint via a penalty on an unbiased estimate of the squared constraint deviation. (2) *CGM-reward* leverages an equivalence to the maximum-entropy principle, for which a solution of the considered problem can be computed in closed form, in the non-parametric setting and under complete knowledge of the underlying distribution. Thus, the idea behing CGM-reward is to minimize the Kullback-Leibler divergence of a generative model to an estimate of said closed-form solution.\n\nFor both (1) and (2), the authors propose gradient estimates that they show to be unbiased.\n\nBoth methods are evaluated on a range of models with tractable likelihood estimates (Gaussian mixture models, diffusion models, normalizing flows, language models) and tasks (protein design, image generation, 1D toy problem, natural language). Overall, CGM-relax performs favorably to CGM-reward at constraint satisfaction, especially if many constraints are present.\n\nA central limitation of both methods lies in the fact that they rely on tractable estimates of the likelihood."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript is well-written (by and large).\n\n2. The proposed methods are practically relevant.\n\n3. The CGM-reward fine-tuning approach and its connection to the principle of maximum entropy is elegant and insightful.\n\n4. Diverse experiments and rigorous evaluation are performed.\n\n5. The authors highlight limitations openly and clearly (I value this highly, because it lies in contrast to most ICLR submissions).\n\nI need to admit that I am not too familiar with the related work. Hence, it is difficult for me to judge novelty."}, "weaknesses": {"value": "1. I am confused about the framing: The manuscript claims to be about calibration, but it really seems to be about generative modeling under moment constraints. Without going into proper mathematical definitions (like, e.g., [1]), calibration (typically defined for classifiers) means: \"Model uncertainty is in line with true uncertainty\". An extension of this concept to generative modeling would then be that the distribution is modeled well in the sense that there is no mode collapse. It seems the authors define \"calibration\" quite differently. My criticism is just regarding the wording, but I am afraid that many readers will be confused and that the \"actual target audience\" (those which are interested in generative modeling under constraints) will not discover this work. I therefore suggest modifying the title and the main body of the text accordingly.\n\n2. The relax loss is somewhat naive and I have some concerns about it: In constrained optimization, such penalty formulations typically lead to either poor conditioning of the loss surface (for large $\\lambda$) or the constraints are not satisfied (for small $\\lambda$). I believe that this penalty method is a reasonable ablation, but I think it would be more interesting to replace it by the augmented Lagrangian method [2], for instance. One could just plug in the derived estimate for the constraint violation (equation 3) and end up with a more mature method that would very likely work better.\n\nIf the authors (i) implement my suggestion in weakness 1 or provide me a convincing argument that the term \"calibration\" is adequate and (ii) run additional experiments with a mature approach for constrained optimization (ideal) or at least add a critical discussion section about the CGM-relax method, I will raise my score.\n\n[2] Magnus R. Hestenes. Multiplier and Gradient Methods. Journal of Optimization Theory and Applications, 4:303â€“320, 1969."}, "questions": {"value": "* l. 10: *\"wherein class probabilities and other statistics of the sampling distribution deviate from desired values\"*. I suggest replacing *\"desired\"* with *\"true\"*.\n\n* l.22: *\"language models represent gender, race, religion, and age in ways that reinforce societal biases\"*. While this is an important point, I believe this is not related to poor calibration, at least not from the definition I am familar with (e.g., [1]). If the generative model reflects societal biases as they are in the data, then it is well-calibrated. Please see my weakness 1 for more information.\n\n* l.120: *\"Theorem 2.1. Under assumptions, there exists a unique solution to (4) that has the form\"* Would it be possible to re-write this by either (i) listing the assumptions explicitly; or (ii) write something like *\"Theorem 2.1. Under the assumptions listed in Appx. ... , there exists a unique solution to (4) that has the form\"* Otherwise, this is not a proper theorem.\n\n* l.122: I highly recommend writing out what $\\alpha^*$ is. I am afraid that without this information, the theorem statement is incomplete.\n\n* l.136: *\"which states that, under conditions,\"* Similarly as in my previous comment, it would be helpful for the reader if the authors could write out the conditions or refer to the appendix to read up on them.\n\n* l.213-215: Why is this unintuitive? And what do the authors mean by the score function being *\"non-trivial in general\"*?\n\n* l.322: *\"Consistent with our results in Section 3 we find that optimally-tuned CGM-relax outperforms CGM-reward, which falls short of meeting the calibration constraints.\"* Should CGM-relax not fall short of meeting the moment constraints, too? (see weakness 2)\n\n[1] Wang, Cheng. \"Calibration in deep learning: A survey of the state-of-the-art.\" arXiv preprint arXiv:2308.01222 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7noyQfIMi0", "forum": "khEYpErtMl", "replyto": "khEYpErtMl", "signatures": ["ICLR.cc/2026/Conference/Submission21255/Reviewer_HdMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21255/Reviewer_HdMe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579583043, "cdate": 1761579583043, "tmdate": 1762941658178, "mdate": 1762941658178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article introduces a calibration procedure for generative models. Calibration is understood in the article as matching the expected value of a nonlinear function to a desired target value. The article thus views calibration as a constrained optimization problem and introduces two heuristic approaches to solve these, one based on the penalty method, one called 'max entropy approach', which can, however, be interpreted as minimizing a corresponding Lagrange function.\n\nThe article presents numerical results spanning both toy examples and more advanced case studies, including protein folding and vision transformers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method has a low complexity and seems to work in practice. Although the degree to which it works successfully is not direclty visible from the experiments. The presentation of the ideas is adequate and the writing is clear."}, "weaknesses": {"value": "There is very little originality and scientific contribution in the article. Essentially, the article suggests to view calibration through the lens of constrained optimization and applies a penalty method (approach 1) or minimizes a Lagrange dual (approach 2). I found the title and framing misleading, as I would expect statistical guarantees, which, however, cannot be delivered by the ad-hoc nature of the approach (or this would need substantial refinement).\n\nMoreover, the approaches to constrained optimization are largely adhoc (penalty method cannot guarantee constraint satisfaction; approach 2 operates on a fixed multiplier) and cannot guarantee constraint satisfaction. Constraint satisfaction would be important for extracting meaningful statistical guarantees. In addition the problem formulation (minimization subject to expectation constraint) is widely studied in the stochastic optimization community, e.g., under the name of multistage stochastic program."}, "questions": {"value": "Have the authors looked into knowledge distillation? I could imagine that approaches similar to the ones proposed in the article are frequently introduced as baselines.\n\nIn many practical situations one would like to use indicator functions for h(x). In these situations h(x) is no longer differentiable. Did the authors look carefully into this situation? The proposed, gradient-based optimization approaches do not seem to work well for this situation (essentially the gradient of the indicator is zero almost everywhere)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HxZfhg2bVD", "forum": "khEYpErtMl", "replyto": "khEYpErtMl", "signatures": ["ICLR.cc/2026/Conference/Submission21255/Reviewer_VapA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21255/Reviewer_VapA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603194383, "cdate": 1761603194383, "tmdate": 1762941657852, "mdate": 1762941657852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework to correct distribution-level miscalibration in generative models. Authors formalize calibration as a constrained optimization problem. Because the exact constraint is intractable, they propose two surrogate fine-tuning methods, one uses soft quadratic plus KL regularization (CGM-relax) and one approximates the maximum-entropy projection of the base model toward a target exponential-family distribution (CGM-Reward). Then they tested these to proteins, images, and language domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing is cohesive and clear. Problem definition on calibration is sound and clear, and their methods, especially connection with maximum entropy problem is interesting. \n- They also provide a nice practical way to realize theory."}, "weaknesses": {"value": "- I think the main concern of this paper is about end-point only calibration. CGM adjust only the terminal marginal distribution (e.g. final diffusion sample) rather than the entire probability flow. In other words, we lose access on how the probability path changes by this finetuning process. Someone can say this is not a problem if we can sample from the target distribution (with desired constraints) anyways, but in terms of theory I am not sure if this is the direction we really want. For example FK steering tries to control the pathwise dynamics."}, "questions": {"value": "- Regarding weakness 1, what is the benefit of this method compared to recent reward-guided finetuning methods that controls pathwise dynamics? Is there any specific problem setup where CGM is the only remedy? I am happy to discuss this more, and raise score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HCunDYC39l", "forum": "khEYpErtMl", "replyto": "khEYpErtMl", "signatures": ["ICLR.cc/2026/Conference/Submission21255/Reviewer_YGqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21255/Reviewer_YGqy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773178855, "cdate": 1761773178855, "tmdate": 1762941657636, "mdate": 1762941657636, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work formulates calibration of generative models as finding the closest distribution to a base model that matches specified expectation constraints. This paper introduced two fine-tuning losses, CGM-relax (penalized constraint violation + KL to base) and CGM-reward (match exponential-tilt max-entropy target) with unbiased loss/gradient estimators and leave-one-out baselines. The authors tested with protein/image/language generative models and show large reductions in miscalibration under constraints, with a slight degradation in quality compared with the base model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper includes experiments that showed robust empirical coverage across different modalities (proteins/images/text), with different model architectures, making the approach very usable\n- Both algorithms only need sampling, log-density, and score, which most diffusion/flow/LM codebases already support, lowering adoption costs across modalities. The paper also shows how to realize these for continuous-time diffusion and masked LMs."}, "weaknesses": {"value": "- The CGM-relax method relies on the $\\lambda$ parameter, which balances the constraint violation and the KL penalty. The paper currently uses grid search to find optimal values, and an analysis of the sensitivity to $\\lambda$ and heuristics for setting it would make the work more practical.\n- Scale-up to larger LMs is untested. TinyStories-33M and ESM3 1.4B are helpful, but applying CGM to popular 7-30B LMs would stress the need to compute long-sequence log-probs and scores efficiently\n- The method requires tractable likelihoods and scores, making it challenging to extend to VAEs/GANs"}, "questions": {"value": "- For images, did you compute class-conditional FID/FJD to decouple class-mix shifts from visual quality?\n- How does $\\lambda$ interact with temperature, classifier-free guidance, or noise schedules? \n- How sensitive is CGM-relax to the choice of $\\lambda$? For a new problem, do you have any heuristics or intuitions for setting a good initial raneg for $\\lambda$ to avoid a costly search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "az1WQhzFsL", "forum": "khEYpErtMl", "replyto": "khEYpErtMl", "signatures": ["ICLR.cc/2026/Conference/Submission21255/Reviewer_C8DD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21255/Reviewer_C8DD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21255/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172998373, "cdate": 1762172998373, "tmdate": 1762941657234, "mdate": 1762941657234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}