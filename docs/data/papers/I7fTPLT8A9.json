{"id": "I7fTPLT8A9", "number": 13188, "cdate": 1758214875476, "mdate": 1759897457926, "content": {"title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "abstract": "Multimodal large language models (MLLMs) have demonstrated impressive capabilities across various tasks but still struggle with complex mathematical reasoning. Prior work has mainly focused on dataset construction and method optimization, while often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. We introduce WE-MATH 2.0, a unified system that integrates a structured mathematical knowledge hierarchy, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to enhance the mathematical reasoning abilities of MLLMs. Our contributions are fourfold: (1) MathBook Knowledge System: a five-level hierarchy covering 491 knowledge points and 1,819 fundamental principles; (2) MathBook-Standard and MathBook-Pro: datasets that ensure broad conceptual coverage and robust training through dual expansion, a three-dimensional difficulty space, and seven progressive variants per problem; (3) MathBook-RL: a two-stage RL framework including Cold-Start Fine-Tuning to align models with knowledge-oriented chain-of-thought reasoning, and Progressive Alignment RL leveraging average-reward learning with dynamic data scheduling for progressive difficulty alignment; (4) MathBookEval: a benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL achieves competitive performance on four widely used benchmarks and demonstrates strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.", "tldr": "", "keywords": ["Mathematical Reasoning", "Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ba694d891452e3e0cb26c09c79da89bda8bbedd.pdf", "supplementary_material": "/attachment/c60eedc7594b0b48adda5bbaece635858333fdfd.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a unified system to enhance the mathematical reasoning abilities of MLLMs. WE-MATH 2.0 combines a structured math knowledge base, a model-driven data design framework, and an RL-based learning curriculum. Results on various benchmarks suggest the effectiveness of the proposed framework for MLLM reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- WE-MATH2.0 Integrates data generation, training, and evaluation within a closed-loop system, ensuring consistent metrics between learning objectives and assessment benchmarks.\n\n- The paper provides a large-scale, well-balanced benchmark covering both knowledge breadth and reasoning depth, enabling systematic evaluation of multimodal reasoning models."}, "weaknesses": {"value": "- The performance improvements of MathBook-7B are mainly observed on its self-constructed benchmarks like We-Math, while gains on external benchmarks such as MathVista and MathVerse are relatively minor.\n- Although MathBookEval is designed to test reasoning depth and knowledge coverage independently from the training data, MathBook-7B does not show significant advantages on this benchmark. \n- The system focuses only on multimodal reasoning. It has not been evaluated on standard textual datasets such as GSM8K or MATH, leaving its language-only mathematical reasoning ability unverified. It would be great to also discuss the capability on language-only but still challenging math reasoning tasks."}, "questions": {"value": "- How well does MathBook-7B generalize to language-only mathematical reasoning tasks, such as GSM8K or MATH, given that the system is trained and evaluated only on multimodal data?\n- Have the authors conducted human validation or cross-checking to verify the correctness of automatically assigned knowledge labels and reasoning steps?\n- Although MathBook-7B achieved strong results on We-Math and MathBookEval, how do the authors ensure that evaluation items were not seen or paraphrased during training, given the shared data construction pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kkqv53iipJ", "forum": "I7fTPLT8A9", "replyto": "I7fTPLT8A9", "signatures": ["ICLR.cc/2026/Conference/Submission13188/Reviewer_K8f9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13188/Reviewer_K8f9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798922222, "cdate": 1761798922222, "tmdate": 1762923885037, "mdate": 1762923885037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces We-Math 2.0, a system involving a knowledge system for mathematical knowledge, a pair of dataset for training, an RL framework, and a comprehensive benchmark for multimodal mathematical reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work represents substantial engineering effort in an important area and should be applauded for this.\n2. MathBook Knowledge System (MKS) is comprehensive with 491 knowledge points + 1819 fundamental principles\n3. MathBook-Standard/Pro is Built on MKS with annotated problems which are shown later in experiments to be strong together with the proposed MathBook-RL.\n4. MathBook-RL is presented well with ablation studies.\n5. This paper offers a few insights including the observation that MLLMs performance in geometry is subpar compared to that in algebra, and that performance in general correlates negatively with knowledge points."}, "weaknesses": {"value": "Maybe some more experiments on a few more model scale are warranted, but considering the amount of work put into the entire system, I do not see this as much of defect."}, "questions": {"value": "Do we have a mechanism to use the knowledge system at inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rjgRwszIUL", "forum": "I7fTPLT8A9", "replyto": "I7fTPLT8A9", "signatures": ["ICLR.cc/2026/Conference/Submission13188/Reviewer_TdME"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13188/Reviewer_TdME"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804615650, "cdate": 1761804615650, "tmdate": 1762923884234, "mdate": 1762923884234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes We-Math 2.0, a unified framework centered around a structured MathBook knowledge system with 491 knowledge points. It introduces associated datasets and the MathBookEval benchmark, and designs a reinforcement learning training strategy based on curriculum learning. The authors claim their trained model achieves a marginal performance advantage on several existing mathematical multimodal benchmarks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a unified framework (We-Math 2.0) that integrates a structured mathematical knowledge system, a model-centric data space, and an RL-based training paradigm.\n\n- The trained model reportedly demonstrates a marginal advantage on some established mathematical multimodal benchmarks."}, "weaknesses": {"value": "- The categorization and comparison in Table 1 appear inconsistent. For instance, comparing the granularity of the proposed 491-point system with datasets like MathV360k (which contains diverse content like charts and general QA) is not an apples-to-apples comparison and may be misleading."}, "questions": {"value": "- How can the evaluation in Table 1 be made fair and consistent, especially regarding the granularity of category definitions?\n- Could the evaluation on MathBookEval include more open-source reasoning models of comparable scale? This would help disentangle whether the observed performance gains primarily stem from the richness and structure of the training data or from the effectiveness of the MathBook-RL training pipeline itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vcHAqoISWc", "forum": "I7fTPLT8A9", "replyto": "I7fTPLT8A9", "signatures": ["ICLR.cc/2026/Conference/Submission13188/Reviewer_ZPo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13188/Reviewer_ZPo5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806987710, "cdate": 1761806987710, "tmdate": 1762923883835, "mdate": 1762923883835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed We-Math 2.0, a comprehensive framework for visual mathematical reasoning benchmarks. The major contribution of this paper is providing a detailed way to curate the dataset alongside with itself, paving the path for future progress. The authors also include a two-stage RL framework for MLLMs, and a extensive evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well structured and easy to follow. The teaser figure is particularly well structured.\n2. The intuition of the dataset is detailed and inspiring, which could be even more helpful than the dataset iteself.\n3. The evaluation results show the improvement brought by We-Math 2.0."}, "weaknesses": {"value": "No significant weakness within the dataset scope."}, "questions": {"value": "1. I see in the teaser figure the definition has been emphasized a lot. I wonder if the authors want to state that the solution of the question is related to the definition, or the question is about the definition itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "73Tz6bEwTt", "forum": "I7fTPLT8A9", "replyto": "I7fTPLT8A9", "signatures": ["ICLR.cc/2026/Conference/Submission13188/Reviewer_FXnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13188/Reviewer_FXnB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997688535, "cdate": 1761997688535, "tmdate": 1762923883467, "mdate": 1762923883467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}