{"id": "uI9yO6DPIc", "number": 23025, "cdate": 1758338368592, "mdate": 1759896835441, "content": {"title": "Continual Pre-Training for Hallucination Reduction", "abstract": "Hallucinations, where model outputs contradict or cannot be verified by the provided evidence, remain a central obstacle to the reliable use of large language models (LLMs). Thus, it remains to be seen how hallucination can be decreased via sophisticated training methods. Prior works find that the mismatch between pre-training dataset and fine-tuning dataset is main cause for hallucinations. To reduce such effect, we introduce Continual Pre-Training for Hallucinations (CPTHalu), a method that performs fine-tuning of a sample in parallel with continued pre-training of its corresponding factual knowledge. We adapt GRPO to reading comprehension via our training scheme, a first effort for RL knowledge fine-tuning in reading comprehension to our understanding. Our experiments on HaluEval and SQuAD obtain large and consistent performance increases of up to 17 points. To further assess factual grounding, we also perform ablation study with our new Augmented QA benchmarks, being novel question-answer pairs over the same source documents. We obtain improvements for both closed-book and open-book performance. We also validate scalability on smaller models, showing that CPTHalu’s benefits persist under limited capacity. Our results establish CPTHalu as a simple yet effective strategy for mitigating hallucinations in LLMs. Our code and dataset will be released upon publication.", "tldr": "", "keywords": ["GRPO", "MRC", "reading comprehension"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/245ef972606cc860e859abb51520b34dea818868.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes CPTHalu, a joint training recipe that runs GRPO-based RL for answer quality alongside continual pre‑training (next‑token prediction) on the same evidence paragraph. \n\nEvaluated on HaluEval and SQuAD, GRPO already yields large gains over a supervised baseline, and adding CPTHalu provides consistent but modest additional improvements. Augmented Q&A evaluations (closed/open book) and a 0.5B model study show similar trends."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Simple, general recipe: Clear joint loss (GRPO + continual pre‑training) that is architecture‑agnostic and easy to integrate. \n\nStable training: Plots show smooth learning dynamics; no catastrophic forgetting reported.\n\nBreadth of evaluation: Standard RC datasets (HaluEval, SQuAD), augmented Q&A to probe knowledge retention, and small‑model (0.5B) scaling analysis with reward ablations."}, "weaknesses": {"value": "Incremental gains over GRPO: While the paper highlights “up to 17 EM” on SQuAD, that jump is largely GRPO vs. supervised; CPTHalu typically adds ~0–1 EM over GRPO (e.g., HaluEval 82.17→83.10 EM; SQuAD 70.01→70.48/70.14). The marginal benefit may be statistically small.\n\nHallucination proxy: Uses EM/F1 on reading comprehension as the main signal; lacks direct hallucination/faithfulness metrics or human judgments, so the link to “hallucination reduction” is indirect.\n\nRehearsal risk: Continual pre‑training on the very paragraphs used for QA may blur the line between grounding and memorization; augmented Q&A gains in closed‑book are small (e.g., HaluEval EM 16.89→17.55).\n\nAugmented data quality: The augmented Q&A is generated by an external LLM (GPT‑4o); the paper provides limited validation/quality‑control details (e.g., deduping, paraphrase checking, manual auditing).\n\nMissing baselines/ablations at 3B: A clear pretrain‑only vs GRPO‑only vs CPTHalu‑only comparison is shown for 0.5B, but not for the main 3B model; comparisons to alternative RL/DPO/RLAIF or retrieval‑augmented baselines are absent.\n\nReward design is heuristic: Length‑difference reward uses coarse thresholds; sensitivity analyses (scales, normalization) and α\\alphaα selection are limited."}, "questions": {"value": "Measurement of hallucination: Beyond EM/F1, did you compute any faithfulness or hallucination‑focused metrics (or run human evaluations) to substantiate the claim of hallucination reduction?\n\nAblations at 3B: What are the results for pretraining‑only and CPTHalu‑only (no GRPO) at 3B to isolate each component’s contribution? \n\nAugmented Q&A validation: How were items filtered for novelty vs. paraphrase, deduplicated, and quality‑checked? Will you release the augmented sets with provenance/validation metadata?\n\nSensitivity & scaling: How sensitive are outcomes to α\\alphaα, reward weights, and rollout parameters? Any normalization/adaptive weighting between LGRPOL_{\\text{GRPO}}LGRPO​ and LCPTL_{\\text{CPT}}LCPT​?\n\nGeneralization: Does continual pre‑training on training paragraphs affect performance on unseen domains or retrieval‑based QA? Any results on out‑of‑paragraph grounding tasks?\n\nCost/variance: What is the compute/time overhead of CPTHalu vs. GRPO‑only, and how robust are results across multiple seeds/checkpoints (statistical significance)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qQj8ij4Se8", "forum": "uI9yO6DPIc", "replyto": "uI9yO6DPIc", "signatures": ["ICLR.cc/2026/Conference/Submission23025/Reviewer_j5iK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23025/Reviewer_j5iK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760651478613, "cdate": 1760651478613, "tmdate": 1762942482119, "mdate": 1762942482119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to simultaneously fine-tune and (continually) pre-train Large Language Models (LLMs) to reduce hallucinations caused by mismatch between pre-training and fine-tuning data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper validates to a certain extent the hypothesis that hallucinations of LLMs are caused by mismatch between pre-training and fine-tuning data, even though its implications may be limited (more on this in “Weakness”)."}, "weaknesses": {"value": "Although the paper sheds more light on the possible cause of hallucinations, it may or may not imply a general hallucination mitigation strategy. Intuitively, mismatches between pre-training and fine-tuning data should be crucial for LLMs to generalize. Thus the impact of the proposed technique on the general capabilities of LLMs should also be studied."}, "questions": {"value": "Please see “Weakness”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9l96BxReyL", "forum": "uI9yO6DPIc", "replyto": "uI9yO6DPIc", "signatures": ["ICLR.cc/2026/Conference/Submission23025/Reviewer_B3cV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23025/Reviewer_B3cV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761490872632, "cdate": 1761490872632, "tmdate": 1762942481751, "mdate": 1762942481751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CPTHalu, a concurrent training framework designed to mitigate hallucinations in large language models (LLMs).\nThe approach combines GRPO-based reinforcement learning (RL) with continual pre-training on the same factual paragraphs used during fine-tuning. By jointly optimizing the GRPO policy loss and a continual pre-training loss term\nL = L_GRPO + α L_CPT, the method aims to reduce the pre-trained/fine-tune distribution mismatch that contributes to hallucination. Experiments on HaluEval and SQuAD show consistent improvements (up to +17 EM and +14 F1), and further validation on small models and augmented Q&A datasets demonstrates factual retention and robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Methodological clarity. The formulation is explicit, including reward design (word-level F1 and output-length constraints) and its adaptation of GRPO to factuality rather than reasoning.\n2. Analytical insight. The symmetric KL-divergence analysis provides a probabilistic interpretation of why CPTHalu models stay closer to the pretraining distribution."}, "weaknesses": {"value": "1. Limited significance of gains. Improvements of roughly +1 EM/F1 over GRPO are modest; further statistical validation is needed.\n2. Missing baselines. Comparison with other factuality-enhancing approaches (e.g., KDRL, retrieval-augmented fine-tuning) would clarify relative strengths.\n3. Possible data bias. Augmented Q&A sets are generated using GPT-4o, potentially introducing stylistic or lexical bias."}, "questions": {"value": "1. Does continual pre-training risk catastrophic forgetting of instruction-following ability?\n2. Can CPTHalu generalize to other tasks such as summarization or open-ended generation?\n3. Could this framework integrate with retrieval or knowledge-editing systems to further mitigate hallucination?\n4. In the KL-divergence analysis, which models define p and q? Clarify what “closer to the pre-trained model’’ means quantitatively."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bte6pJzT1Y", "forum": "uI9yO6DPIc", "replyto": "uI9yO6DPIc", "signatures": ["ICLR.cc/2026/Conference/Submission23025/Reviewer_7qMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23025/Reviewer_7qMP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709398666, "cdate": 1761709398666, "tmdate": 1762942481330, "mdate": 1762942481330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CPTHalu, a method designed to reduce hallucinations by combining reinforcement learning fine-tuning with continual pre-training on the same knowledge used for reading comprehension tasks. The authors hypothesize that hallucinations often stem from a \"distribution mismatch\" between a model's pre-training data and its fine-tuning data. Using the GRPO algorithm with rewards based on word-level F1 and output length, CPTHalu simultaneously reinforces factual accuracy while maintaining training stability. Experiments on HaluEval and SQuAD show consistent performance gains. The approach also generalizes across model sizes and improves factual grounding in augmented question–answer datasets, which uses the same documents but new questions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Presents a integration of continual pre-training with RL (combining GRPO with a masked next-token loss on the evidence paragraph), directly addressing the pretrain–finetune distribution mismatch that leads to hallucinations.\n- Demonstrates consistent empirical results on multiple benchmarks with measurable factuality improvements.\n- Includes detailed ablation and augmented Q&A studies, providing thorough validation of the method’s factual grounding capability. augmented Q&A to test whether models internalize document facts rather than just match patterns, with improvements reported in both No-Context and Knowledge-Context views."}, "weaknesses": {"value": "- Incremental benefit of CPTHalu over GRPO is modest, typically around 0.5–1 EM and fractions of a point in F1, which may limit practical impact relative to the RL baseline alone. While CPTHalu consistently outperforms the GRPO-only baseline, the additional performance gain from the CPTHalu component is relatively small. The gain is mostly attributable to the GRPO baseline itself, not the novel CPTHalu addition\n- The reward design, while effective, relies on simplistic metrics (F1 and length) that may not fully capture nuanced factual correctness. Such reward design optimizes surface-level word overlap and output length, which may bias models toward extractive phrasing without directly measuring faithfulness or reasoning.\n- The primary experiments are conducted on a single model family,  the findings are not validated on other common architectures (like Llama or Mistral). Some analysis, such as the KL divergence interpretation, remains qualitative and lacks deeper theoretical justification for observed improvements. \nThe CPTHalu method is inherently tied to tasks where a specific \"knowledge paragraph\" is available during training. This work does not address the broader, more common problem of parametric hallucinations, where a model invents facts in an open-domain setting without any provided source text."}, "questions": {"value": "- Given the marginal gains over the strong GRPO baseline, is the added computational complexity of CPTHalu practically justified?\n- Does optimizing for F1 and length rewards encourage true factual faithfulness, or does it merely bias the model toward better extractive phrasing?\n- Since experiments were limited to the Qwen family, how do we know if the CPTHalu method generalizes to other model architectures like Llama or Mistral?\n- As CPTHalu requires a \"knowledge paragraph\" for training, how could this method be adapted to reduce parametric hallucinations in open-domain settings that lack explicit source text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i7zpxeLBPx", "forum": "uI9yO6DPIc", "replyto": "uI9yO6DPIc", "signatures": ["ICLR.cc/2026/Conference/Submission23025/Reviewer_mp5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23025/Reviewer_mp5E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968767552, "cdate": 1761968767552, "tmdate": 1762942480857, "mdate": 1762942480857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}