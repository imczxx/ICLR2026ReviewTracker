{"id": "vrBZs2UL5n", "number": 4299, "cdate": 1757658591165, "mdate": 1763308953436, "content": {"title": "Dynamic Mask Attention: End-to-End Trainable Content-aware Sparse Attention", "abstract": "Self-attention's computational cost, which scales quadratically with sequence length, creates a fundamental bottleneck for long-context modeling in LLMs, limiting applications such as document understanding, multi-turn reasoning, and code generation. Sparse attention has been proposed to mitigate this issue. Early content-agnostic designs such as sliding-window and block-sparse attention reduce computational complexity based on fixed patterns. However, theirstatic structure often overlook important long-range dependencies and lack adaptivity to diverse query contexts. Recent content-aware methods improve adaptivity by conditioning attention sparsity on token representations, but they typically rely on hard binary masks or heuristic key-value selection, introducing runtime overhead and hindering fully differentiability. We propose Dynamic Mask Attention (DMA), a trainable content-aware sparse attention mechanism with head-wise specialization. DMA dynamically generates content-driven dynamic masks with continuous importance weights based on value representations, enabling both expressiveness and full differentiability. We theoretically prove that masked entries are mathematically equivalent to zero in both the forward and backward passes, thereby ensuring unbiased gradients. Furthermore, we developed efficient CUDA kernels with block-skipping for practical acceleration. Extensive experiments demonstrate that DMA consistently outperforms state-of-the-art sparse attention baselines across pre-training and downstream tasks, reducing perplexity, improving accuracy, and delivering substantial long-sequence speedups of up to 10 times.", "tldr": "", "keywords": ["Trainable Sparse Attention", "Dynamic Mask Attention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea1bd15059b158273c29626ab33935316dbea1fe.pdf", "supplementary_material": "/attachment/aa995a6e9cdeaf4d5873f1eec6cd8835a8395354.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a sparse attention mechanism named Dynamic Mask Attention (DMA) to address the computational bottleneck in long-context modeling for Large Language Models. It generates content-driven dynamic masks based on value vectors and assigns continuous importance weights to tokens. This design avoids the non-differentiable hard binary masks used in previous methods and enables end-to-end training. Furthermore, the authors have developed efficient CUDA kernels to achieve practical acceleration. Experimental results demonstrate that DMA outperforms existing sparse attention baselines such as Native Sparse Attention (NSA) in terms of perplexity, downstream task accuracy, and long-sequence inference speed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. End-to-End Trainability: The primary contribution of this paper is a fully differentiable sparse attention mechanism. By generating masks from value vectors and adding them to the attention map, gradients can flow through the mask generation process. This allows the model to be optimized end-to-end.\n2. Efficient Implementation: The authors not only propose a theoretical model but also provide an efficient implementation. It demonstrates significant training and inference speedups on long sequences."}, "weaknesses": {"value": "1. Insufficient dynamism: Although the method is designed to be \"content-aware\" and \"dynamic,\" the sparse pattern is only generated by values and the mask is broadcast to all queries, which limits its dynamism.\n2. Unclear rationality of end-to-end training：The paper devotes considerable space to explaining why adding the obtained scores to the attention map achieves differentiability. However, differentiability is an obvious outcome, and the paper fails to explain why this approach enables the model to learn reasonable top-k sparse behavior.\n3. Unclear decoding methodology：The paper uses the topk method for token selection, but does not clearly explain the model's behavior during the decoding stage.\n4. Incomplete experiment：There is no comparison of training speeds, lacking inference speed comparisons with Flash Attention 2 and 3. Additionally, the model training was only conducted on 2k pre-training and subsequent 8k training, making it difficult to demonstrate scalability on longer samples. Moreover, ablation experiments on model components are missing."}, "questions": {"value": "1. Why generate scores using the method in formula (1) instead of a simpler approach like using a linear projection followed by an activation function?\n2. Regarding \"Mask generation\", \"Mask Broadcasting\", and formulas (2)(3), why are the top w tokens selected directly from the value scores of the complete sequence and broadcast to queries rather than being selected for each query individually?\n3. What do the symbols in Table 2 of Appendix F mean? Why is this model configuration used for NSA?\n4. What implementation is used for the speed in Figure 5, is it efficient implementations like flash attention and flash mla?\n5. In the Native Trainable Sparsity paragraph of Appendix G.2, it is stated that the method proposed in this paper preserves the complete key-value cache and avoids information loss. However, the score for a particular token, which is calculated solely based on its value, does not change during the decoding process. Would this cause some tokens to never be activated, resulting in information loss?\n6. Regarding the Unified Training-Inference Architecture paragraph in Appendix G.2, how is decoding specifically implemented? When encountering newly added key-value pairs, do all of them participate in the computation, or are the top w tokens reselected?\n7. Have you attempted to conduct similar experiments based on GQA instead of MHA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1CwNmqyNcS", "forum": "vrBZs2UL5n", "replyto": "vrBZs2UL5n", "signatures": ["ICLR.cc/2026/Conference/Submission4299/Reviewer_TP8n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4299/Reviewer_TP8n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484517558, "cdate": 1761484517558, "tmdate": 1762917283715, "mdate": 1762917283715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their constructive feedback. In response to the main concerns on model scale and long-context setting, configurations, GQA setup, value-driven scoring and dynamics, decoding strategy, and speed comparison, we have made the following additions and clarifications in the revised manuscript:\n\n1. Model Scale and Long-Context Setup  \nOur main experiments use a 1.7B-parameter model with 2K–8K context, which we found to be a reasonable trade-off between compute cost and reproducibility for long-context pretraining and method validation. In the revision, we additionally perform 16K long-context continued training on the same 1.7B model, and report detailed results on LongBench and RULER to better demonstrate the behavior of our method under longer inputs (see Section 3.4 lines 507-516 and Table 2).\n\n2. Configurations  \nWe add complete configuration tables and complexity formulas for all attention variants in the appendix. For NSA, MLA and other baselines, we follow the recommended configurations from their original papers; for NSA in particular, we further align with the default configuration in the original work and explicitly explain all symbols used in the tables. Overall, we match total parameter counts, training budgets, and effective sparsity levels across methods as closely as possible to ensure fair comparisons (see Appendix F, Tables 3 and 4).\n\n3. GQA Setup\nAll main experiments are conducted with a GQA architecture, using a unified setting of $n_{h_{kv}} = n_h / 2$, and DMA is implemented on top of the same GQA configuration. We now clearly state and emphasize this in the methodology and experimental setup sections so that readers can accurately reproduce our results and see that our setting is aligned with standard efficient attention configurations (see Appendix F and Table 3).\n\n4. Value-Driven Scoring and Top-k Behavior  \nDuring training, we treat the differentiable scores derived from the Value as an additive bias to the attention scores: the forward pass applies a hard top-k mask, and the backward pass propagates gradients only through the selected tokens. Under the supervision of the task loss, useful tokens receive larger gradients and are more likely to be selected again, which leads to a reasonable learned sparsity pattern.  \nWe also compare value-driven scoring with linear-projection-plus-activation approaches such as Retaining. Under the same 8K dense model, continued training to 16K with w=4K, DMA achieves a higher average RULER score than Retaining while remaining very close to MHA, supporting the effectiveness and stability of our design:\n\n|Method|Avg|\n|-|-|\n|MHA|60.6|\n|Retaining|54.7|\n|DMA|60.5|\n\n5. Dynamics and Decoding Strategy  \nWe now provide a unified formal description of both training and decoding. At decoding time, we adopt a hybrid strategy of value-driven global $top_{w_{kv}}$ + per-query dynamic slots $w_q$:  \n- Each attention head maintains a global KV index set of size at most $w_{kv}$, updated incrementally by top-k over Value scores. KV entries evicted from this set are moved to slower memory, keeping the active KV cache size per head constant for arbitrarily long sequences.  \n- For each query, we additionally select up to $w_q$ query-aware positions from a candidate subset. The final attention index set is the union of the global and per-query parts, with size at most $w_{kv}+w_q$, reducing complexity from $O(n_htd_h)$ to $O(n_h(w_{kv} + w_q)d_h)$.  \n- The global sparsity pattern remains relatively stable over time, while $w_q$ provides local dynamics for the current query.  \nRegarding the concern that some tokens may remain inactive for a long time, this is a deliberate trade-off to enable memory-friendly KV eviction. Through the candidate set design and the per-query dynamic slots $w_q$, important tokens still have opportunities to be recalled. As shown on long-context benchmarks such as RULER (see Table 3), this trade-off does not lead to noticeable performance degradation in practice.\n\n6. Speed and Training Efficiency\nWe conduct systematic speed evaluations on A100 with a representative configuration, covering sequence lengths 256–32K for forward/backward and 256–524K for decode (see Fig. 5).  \nWe compare against MHA, SWA, MLA, NSA and other variants, all implemented with efficient open-source kernels; detailed configurations and implementation sources are provided in Appendix F and Table 5. For training, we report forward and backward times separately.  \nThe results show that DMA is significantly faster than MHA and other sparse baselines on long sequences, and achieves similar efficiency to SWA.\n\n7. Long-Context Benchmarks and Multi-Task Results  \nAfter 16K long-context continued training, we evaluate the model on multiple benchmarks including LongBench, and RULER, and directly compare MHA, NSA, and DMA (see Table 2). The results show that, while providing substantial speedups, DMA matches or slightly outperforms MHA on most tasks."}}, "id": "9rRCu6qF9T", "forum": "vrBZs2UL5n", "replyto": "vrBZs2UL5n", "signatures": ["ICLR.cc/2026/Conference/Submission4299/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4299/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4299/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763309487061, "cdate": 1763309487061, "tmdate": 1763309487061, "mdate": 1763309487061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Masked Attention (DMA), a trainable, head-specialized, content-aware sparse attention mechanism. The key is that masks are generated by Value and importance weights are continuous, thus achieving expressiveness and full differentiability. Compared to the current SOTA, it reduces PPL and improves accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of ​​using Value to generate importance weights is very innovative and can achieve better results than fullAttention.\n2. The paper uses the head-wise specialization method and also analyzes the different focuses of each head. The experimental analysis here is quite sufficient.\n3. The paper provides a detailed theoretical proof that this mask method can be trained."}, "weaknesses": {"value": "1. In the experimental results section, the configurations of the comparison models are not detailed enough, so it is difficult to understand whether it is a completely fair comparison. For example, when comparing various sparse methods, the sparse ratio of each method need to be given.\n2. The paper does not show the speed comparison with other sparse methods, which is also a very important evaluation indicator."}, "questions": {"value": "1. Why do the full attention and NSA 8k needle indicators seem to be worse than other papers, such as the original NSA paper？what is the configuration of the full attention length extrapolation that seems to have a poor score?\n2. In the speed comparison, it’s not fair to only compare PyTorch’s nativeScaled Dot-Product Attention. Can you compare it with flashAttn?\n3. What is the block_size in the paper? Are there any ablation experiments with different block_size configurations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u8a1BRysU3", "forum": "vrBZs2UL5n", "replyto": "vrBZs2UL5n", "signatures": ["ICLR.cc/2026/Conference/Submission4299/Reviewer_WvcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4299/Reviewer_WvcK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557982180, "cdate": 1761557982180, "tmdate": 1762917283443, "mdate": 1762917283443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to train a language model with dynamic sparse attention. Concretely it uses the value vector for each attention head to generate a mask for token, which decides whether the token will be masked out for attention calculation.\n\nExperiments show that training with the proposed method (DMA) achieve better better perplexity and downstream performance compared to dense attention and other sparse attention methods, such as NSA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper aims to improve the efficiency of language model by proposing a sparse attention method, which is well-motivated.\n* Experiment are conducted on models from 80M to 1.7B scale, and demonstrate speed-up compared to dense attention, and performance improvement compared to the other sparse attention methods."}, "weaknesses": {"value": "* Experiments are conducted on relatively small-scaled model (only up to 1.7B) and short context (trained only up to 8K sequence length). Experiments are larger scale model or longer context pre-training would be helpful to demonstrate more practical utility of the proposed method, for instance, LongBench contains context length > 8K.\n* I am curious about the reason behind choosing MHA as a baseline (and implementation for DMA) instead of grouped query attention, which is a standard method to improve efficiency."}, "questions": {"value": "* Regarding the hyper-parameter chosen for Table 2, could the authors explain on what basis are these numbers chosen? For instance, is this for fair comparison to the tokens attended, FLOPs, or latency?\n* As the mask is depended on the value vectors, will the mask pattern change during generation or will the pattern be relatively stable? It will also be helpful to report per-task performance on LongBench and [RULER](https://arxiv.org/abs/2404.06654) to conduct a more fine-grained evaluation on different types of task that process long input. \n* There are several typos in the paper that makes the sentence unreadable: line 16-18 and line 24 in the abstract."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AtWHjJ75yd", "forum": "vrBZs2UL5n", "replyto": "vrBZs2UL5n", "signatures": ["ICLR.cc/2026/Conference/Submission4299/Reviewer_3ohf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4299/Reviewer_3ohf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762548970728, "cdate": 1762548970728, "tmdate": 1762917283106, "mdate": 1762917283106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}