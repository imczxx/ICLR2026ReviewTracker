{"id": "57MOec7XQJ", "number": 16707, "cdate": 1758267876484, "mdate": 1759897223730, "content": {"title": "Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning", "abstract": "Large language models (LLMs) have emerged as powerful tools for addressing a wide range of general inquiries and tasks. Despite this, fine-tuning aligned LLMs on smaller, domain-specific datasets, critical to adapting them to specialized tasks, can inadvertently degrade their safety alignment, even when the datasets are benign. This phenomenon makes models more susceptible to providing inappropriate responses. In this study, we systematically examine the factors contributing to safety alignment degradation in benign fine-tuning scenarios. Our analysis identifies three critical factors affecting aligned LLMs: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of state-of-the-art reward models (RMs), which are often used to guide alignment processes. Our findings reveal that these RMs frequently fail to accurately reflect human preferences regarding safety, underscoring their limitations in practical applications. By uncovering these challenges, our work highlights the complexities of maintaining safety alignment during fine-tuning and offers guidance to help developers balance utility and safety in LLMs. Datasets and fine-tuning code used in our experiments will be released after paper acceptance.", "tldr": "We study the reasons causing safety decrease after fine-tuning LLMs on purely benign datasets.", "keywords": ["Safety alignment", "instruction tuning", "LLM", "RM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68d5ae055b46f3c901cce1b9f71e336f24a0aea2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates safety alignment degradation in LLMs when fine-tuned on benign downstream datasets. The authors identify three factors affecting alignment: answer structure formatting, identity calibration, and role-play instructions. They also evaluate the reliability of RMs in scoring downstream data. They use open-source models and tests on medical, code, and STEM datasets. The paper finds that simple reformatting of answers can significantly impact safety alignment. The paper also reveals that state-of-the-art RMs often fail to accurately predict safety outcomes and show significant disagreement in their scoring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper addresses a critically important problem, safety degradation from benign fine-tuning rather than toxic/adversarial scenarios, which is quite common in real-world applications. \n\n2. The study comprehensively examines multiple factors (answer structure, identity calibration, role-play) across different models and datasets, providing robust evidence for their claims.\n\n3. The finding that simple answer reformatting can preserve or enhance safety alignment is immediately applicable and provides concrete guidance for practitioners.\n\n4. The systematic evaluation revealing RM unreliability addresses an important gap in understanding these widely-used tools, with implications for the entire alignment pipeline."}, "weaknesses": {"value": "1. While the empirical findings are strong, the paper lacks deeper theoretical analysis of why these factors affect alignment. The \"data assimilation\" explanation in Appendix 8.10 is interesting but can be improved.\n\n2. Only LoRA fine-tuning is considered; full fine-tuning might show different patterns.\n\n3. English-only datasets limit generalizability.\n\n4. Small model sizes (7-8B parameters) may not reflect behavior of larger models."}, "questions": {"value": "1. Can you provide more rigorous analysis of the causal pathways through which answer formatting affects alignment? Have you considered ablation studies that systematically vary formatting elements?\n\n2. How do these findings translate to larger models (70B+ parameters) or different architectures like MoE models?\n\n3. Could the RM unreliability be traced to specific characteristics of their training data? \n\n4. You mention that identity calibration may reduce user experience - can you quantify this trade-off more precisely? Did you measure how the proposed formatting changes the models' tendency to refuse benign requests, i.e., over-refusal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V7bpBJyrb8", "forum": "57MOec7XQJ", "replyto": "57MOec7XQJ", "signatures": ["ICLR.cc/2026/Conference/Submission16707/Reviewer_ed3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16707/Reviewer_ed3j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855556860, "cdate": 1761855556860, "tmdate": 1762926758394, "mdate": 1762926758394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an empirical study on the degradation of safety alignment in LLMs when they are instruction-tuned on benign, domain-specific datasets. The authors identify three primary factors within the fine-tuning data that significantly influence safety outcomes: answer structure (LLMs are \"picky\" about answer formatting), identity calibration (identity statements in training data answers tend to reinforce the model's aligned persona, thereby preserving safety alignment), role-play (instructions that prompt the model to adopt a specific persona often leading to safety degradation).\n\nAdditionally, the study examines the reliability of open-source Reward Models (RMs). The findings indicate that while RMs can distinguish between high- and low-quality data within a single dataset, they are fundamentally unreliable across datasets. They often fail to identify data formats that improve safety alignment, sometimes assigning lower scores to these safer, reformatted answers.\n\nThe experiments are conducted using multiple open-source LLMs and RMs on diverse benign datasets. Safety is evaluated using the SALAD-Bench framework. The paper, in conclusion, provides guidance for constructing safer instruction-tuning datasets ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates safety degradation from benign data, moving beyond the more commonly studied area of harmful/adversarial training data. Identifying ‘answer structure’ as a key factor is a non-obvious, simple, and practical insight for data curators. These findings coincide with previous literature [1, 2, 3, 4], which should be further cited in the revision.\n\n[1] Xiao, Yuxin, et al. \"When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment.\" arXiv preprint arXiv:2506.07452 (2025).\n\n[2] Hsiung, Lei, et al. \"Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets.\" arXiv preprint arXiv:2506.05346 (2025).\n\n[3] O'Brien, Kyle, et al. \"Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs.\" arXiv preprint arXiv:2508.06601 (2025).\n\n[4] Liu, Guozhi, et al. \"Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning.\" arXiv preprint arXiv:2510.10085 (2025).\n\n- The experiments of isolating the three key factors (structure, identity, role-play) through different ways (e.g., reformatting datasets, adding/removing role-play instructions) are worth noting. The results in Tables 1 and 2 clearly support the claims.\n\n- This paper also discovers that RMs are \"unreliable,\" which is an interesting finding."}, "weaknesses": {"value": "- As acknowledged by the authors, the experiments are restricted to open-source LLMs and English-language datasets. It is unclear if these findings would hold for top-tier commercial, closed-source models or in multilingual contexts.\n\n- While the paper demonstrates that RMs fail, it could explore why in more detail. The hypothesis that RMs are \"overfitting to the RewardBench\"  is a plausible viewpoint, but a deeper analysis of the RMs' training data or scaling properties would strengthen this (admittedly secondary) part of the contribution. Analysis from the alignment dataset perspective could also be discussed in the paper [1, 2]."}, "questions": {"value": "- It is well-known that fine-tuning on a domain-specific dataset can lead to catastrophic forgetting, and safety alignment can be seen as a ‘task’ that is forgotten. A common technique to mitigate this is to mix the fine-tuning data with a ‘replay buffer’ of general-purpose or alignment-specific data. Did you experiment with this? How many of the ‘safety instructions’  are needed to offset the degradation from a factor like \"role-play\"?\n\n- Table 2 explores some interactions between the three factors, but the analysis is not exhaustive. For instance, what happens in a ‘worst-case’ scenario: a dataset with no affinity structure (plain text), no identity calibration, and strong role-play? Conversely, how much safety is gained in a ‘best-case’ (Markdown + identity calibration + no role-play) scenario?\n\n- Please also address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bLSL7Hj7s0", "forum": "57MOec7XQJ", "replyto": "57MOec7XQJ", "signatures": ["ICLR.cc/2026/Conference/Submission16707/Reviewer_hDje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16707/Reviewer_hDje"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971912371, "cdate": 1761971912371, "tmdate": 1762926757945, "mdate": 1762926757945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines why safety alignment in large language models (LLMs) often deteriorates after seemingly benign instruction-tuning. It identifies three key dataset factors—answer formatting, identity calibration (e.g., “As an AI…” disclaimers), and role-play instructions—that significantly influence post-tuning safety. Through extensive experiments, the authors show that markdown-style or itemized response structures and explicit identity calibration generally preserve safety, while certain role-play patterns degrade it. They also assess the reliability of open-source reward models (RMs) and find that although RMs can rank responses within a dataset, they often fail to predict which data formats lead to safer fine-tuned models. The study concludes that safety degradation is largely dataset-dependent and offers practical recommendations for structure-consistent, safety-aware fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper reframes “post-tuning safety drop” as a dataset-side phenomenon and isolates three concrete levers, such as answer structure, identity calibration, and role-play, rather than attributing degradation to overfitting or model drift.\n\nThe study runs controlled interventions (reformatting answers; toggling identity statements; adding or removing role-play) across multiple datasets and base models, then tracks safety alignment changes with clear before/after comparisons, supporting causal claims rather than mere correlations.\n\nKey concepts are defined with care (e.g., “identity calibration” vs “role-play”), the hypotheses are motivated from observed dataset factors, and results are summarised into concrete guidance that ties findings to practitioner decisions in data construction and RM selection.\n\nThe work has an immediate impact for anyone fine-tuning aligned LLMs: small formatting and identity cues in training data can shift safety outcomes, and widely used RMs may not predict those shifts."}, "weaknesses": {"value": "The paper could add a small human evaluation (e.g., 100–200 samples) on safety preservation vs. degradation to verify whether automated jailbreak metrics align with human judgment.\n\nReformatting effects may confound variables: increased verbosity, hedging, or normative phrasing (e.g., “cannot assist”) might themselves cause improved safety, rather than the formatting alone.\n\n\nIdentity calibration may be entangled with lexical or stylistic cues beyond the explicit “As an AI…” statement, so isolating its effect more carefully would strengthen the conclusions.\n\nThe claim that “role-play harms safety” is under-specified, such as persona type (e.g., doctor, hacker, safety officer), stance (pro-social vs. adversarial), and instruction strength, which likely interact and should be analysed separately.\n\nThe evaluation scope is limited to a few open-source models and datasets; it remains unclear if the findings generalise to frontier closed models, multilingual data, or safety-critical domains such as medical or legal contexts.\n\nThe paper shows that common reward models fail to select safer data, but the root cause is unclear. It could be from domain shift, underrepresentation of safety signals in RM training, or sensitivity to instruction style."}, "questions": {"value": "See the weakness, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ccviL16GlJ", "forum": "57MOec7XQJ", "replyto": "57MOec7XQJ", "signatures": ["ICLR.cc/2026/Conference/Submission16707/Reviewer_ZJWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16707/Reviewer_ZJWQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981964915, "cdate": 1761981964915, "tmdate": 1762926757449, "mdate": 1762926757449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically studies why safety-aligned LLMs can lose safety when they are later instruction-tuned on **purely benign** downstream datasets. It shows that aligned models are “picky”: small changes in **answer structure** (e.g., Markdown, itemized format), **identity calibration** (responses that explicitly say “as an AI/language model”), and **role-play instructions** can either preserve or significantly **degrade** safety against jailbreak attacks after fine-tuning. The authors further evaluate several state-of-the-art open-source reward models and find that, although they can separate obviously good/bad items within a dataset, they are **unreliable for pairwise selection** and often give *lower* scores to reformatted data that actually makes the LLM safer, revealing a mismatch between RM scoring and true safety preferences. The paper concludes with practical guidance on building safety-aware instruction-tuning datasets and on choosing reward models that better match the target LLM’s post-training signals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1: Dual-perspective analysis of alignment drift.** \nThe paper approaches the problem from two complementary angles—(i) what happens inside the alignment / instruction-tuning pipeline itself, and (ii) how reliable current reward models are in preserving that alignment—making the overall argument quite convincing. This dual view gives the community a clearer picture of where alignment can break (data, formatting, RM) and why seemingly benign fine-tuning can still hurt safety.\n\n**S2: Insightful findings on identity calibration and role-play.** \nBy separating “identity calibration” style responses (which tend to preserve safety) from “role-play / persona” style prompting (which can erode it), the paper offers a concrete and actionable observation for dataset curators and practitioners. This is a useful nuance that is often glossed over in existing safety-tuning work.\n\n**S3: Coherent use of reformatting throughout the paper.** \nThe reformatting methodology introduced earlier is actually reused in $\\S$5.2, which makes the empirical story self-consistent and avoids the feeling of disconnected experiments. \n\n**S4: Practical guidance for the community.**\nSection 6 (“Guidance for Safety-Aware Fine-Tuning”) is genuinely helpful: it translates the empirical observations into concrete data-construction and fine-tuning recommendations, which makes the paper more than a diagnostic study and increases its impact for practitioners."}, "weaknesses": {"value": "**W1: Limited novelty.**\nThe main contribution is largely analytical, and the “automatic reformatting pipeline” is essentially a deterministic pre-processing script; compared with existing work on alignment-data curation and safety-preserving instruction tuning, the incremental innovation is modest.\n\n**W2: No assessment of utility–safety tradeoffs.**\nThe paper reports how reformatting, role-play, and identity calibration affect safety metrics, but it does not evaluate whether these paradigms hurt the model’s primary task performance. As a result, it is unclear whether the observed safety gains come at the cost of degraded utility.\n\n**W3: Lack of realistic downstream validation.**\nAll experiments follow the “benign fine-tuning → jailbreak evaluation” pattern; there is no end-to-end, high-stakes scenario (medical, code, enterprise assistant) demonstrating that such formatting shifts would actually make a deployed system unsafe, so the impact remains somewhat hypothetical.\n\n**W4: Inconsistent effect of role-play.**\nAs noted around Lines 343–345, the influence of role-play on alignment is model-dependent and not stable across architectures, which makes it hard for practitioners to know whether they should adopt or avoid this paradigm; the paper’s guidance on this point is therefore weaker."}, "questions": {"value": "Clarification on notation (Lines 205–206): The paper introduces $SA_g$ and $SA_a$ at this point, but it is not clear what these symbols stand for, how they are computed, and how they differ from each other. Please define both terms explicitly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTCjDuxS3c", "forum": "57MOec7XQJ", "replyto": "57MOec7XQJ", "signatures": ["ICLR.cc/2026/Conference/Submission16707/Reviewer_SxDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16707/Reviewer_SxDe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986488886, "cdate": 1761986488886, "tmdate": 1762926756943, "mdate": 1762926756943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}