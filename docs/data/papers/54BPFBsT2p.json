{"id": "54BPFBsT2p", "number": 10681, "cdate": 1758179416557, "mdate": 1763008016533, "content": {"title": "Dynamic Rank Adjustment for Accurate and Efficient Neural Network Training", "abstract": "Low-rank training is a primary strategy for efficient deep learning, but it presents a fundamental challenge. It reduces computational cost, yet it permanently caps a model’s representational capacity and accelerates the rank collapse that diminishes its expressive power during training. We address this with dynamic-rank training, a framework built on the intuition that a model can temporarily escape its low-rank constraints to restore its full learning potential. Our approach strategically interleaves full-rank epochs within a low-rank schedule, with the timing of these restorative phases aligned with the learning rate’s noise regimes to maximize their effect. This enables the model to regain expressive power at critical stages of training by restoring the effective rank of its weights. Our extensive evaluations across various computer vision and natural language processing benchmarks show this method achieves the accuracy of full-rank models while retaining the computational advantages of low-rank training.", "tldr": "Mitigating rank decline effect in low-rank training via dynamic rank adjustment.", "keywords": ["Low-rank training", "Rank adjustment", "model reparameterization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/72c9810a5e70f001f9494f21db848286a2a73154.pdf", "supplementary_material": "/attachment/aad43d880e0f0eb11b655ae00bad6c5d00bcf8a3.pdf"}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a new framework to avoid the reduced capacity of low-rank models while pretraining. In particular, the authors propose to alternate phases of low-rank adaptation and phases of rank increase in which the model can increase its capacity. The proposed framework is evaluated on a variety of different benchmarks, both for vision and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is overall well-written and organized clearly. The investigated problem is very relevant in the contect of stable pretraining and fine-tuning."}, "weaknesses": {"value": "1. I personally fail to see the point of Proposition 2: while it is true that the right-hand side increases as a function of the learning rate, the bound on $d_t$ given by \n$$\nd_t \\leq ||\\nabla f(W_t)||_F(1+ O(\\eta)) + O(\\eta^2) \\to ||\\nabla f(W_t)||_F, \\quad \\eta \\to 0\n$$\nTherefore, the tightest bound is obtained in the limit, but it simply says that the relative error\n$$\n\\frac{d_t}{||\\nabla f(W_t) ||} \\leq 1,\n$$\nin the limit $\\eta \\to 0$. For this reason, the bound is fully vacuous, and therefore I fail to see the point of this result.\n\n2. Proposition 1 is a very classical result, I would include it in the form of a remark or observation with reference.\n\n3. [1] is an important piece of literature that the authors failed to discuss and compare with, which is essentially a similar philosophy with the only difference that the rank of the overall weights can increase at each optimization step (by a small amount typically since updates are low-rank).\n\n\n[1] W. Xia et al., Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning, ICML 2024."}, "questions": {"value": "1. It is not very clear how the rank inflation in Algorithm 1 is performed, by performing a couple of optimization steps on $W$?\n\nI would also appreciate having a discussion with the authors concerning the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ejRDGlFZlU", "forum": "54BPFBsT2p", "replyto": "54BPFBsT2p", "signatures": ["ICLR.cc/2026/Conference/Submission10681/Reviewer_ofNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10681/Reviewer_ofNH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761057696493, "cdate": 1761057696493, "tmdate": 1762921928984, "mdate": 1762921928984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Review quality is very poor, and we believe there will be no constructive discussions available. So withdraw our manuscript."}}, "id": "K6smuFuqWb", "forum": "54BPFBsT2p", "replyto": "54BPFBsT2p", "signatures": ["ICLR.cc/2026/Conference/Submission10681/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10681/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763008015776, "cdate": 1763008015776, "tmdate": 1763008015776, "mdate": 1763008015776, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dynamic-rank training framework to address a key limitation of low-rank training, the permanent loss of representational capacity and rank collapse."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The proposed dynamic-rank training can somewhat increase the model capacity in the fine-tuning process while preserving higher performance.\n+ This paper provides a theoretical analysis of the proposed dynamic-rank training.\n+ The proposed method performs well on multiple datasets compared to existing low-ranking training approaches."}, "weaknesses": {"value": "- Limited novelty. Although the paper adopts a dynamic-rank strategy, the approach essentially remains a variant of standard low-rank training. Dynamically adjusting the rank appears to be more of a training trick than a genuine research innovation. Moreover, the observation that higher ranks yield better performance is a well-known and intuitive fact rather than a novel insight.\n\n- Questionable practicality. The proposed method increases training cost due to periodic rank adjustments, which require significantly more memory and time compared to fixed-rank approaches. Additionally, designing the rank-scheduling policy adds extra hyperparameters and implementation complexity. It is unclear how easily the proposed dynamic schedule generalizes across architectures and optimizers. The method may require task-specific tuning of schedule parameters, limiting its practical adoption. According to Figure 2, the approach requires full-rank phases, which makes it conceptually similar to fine-tuning the original full model, reducing its practical advantage. \n\n- Lack of runtime and memory analysis. Although the method achieves accuracy improvements, the paper does not report actual runtime or memory usage. In practical scenarios, efficiency, both in terms of memory footprint and fine-tuning time, is often more critical than small accuracy gains."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "J47Y49unge", "forum": "54BPFBsT2p", "replyto": "54BPFBsT2p", "signatures": ["ICLR.cc/2026/Conference/Submission10681/Reviewer_dLgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10681/Reviewer_dLgF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896111547, "cdate": 1761896111547, "tmdate": 1762921928551, "mdate": 1762921928551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Rank Training (DRT), a framework that strategically interleaves full-rank \"restorative\" phases within low-rank training to mitigate effective rank decline. The key insight is to schedule these phases in alignment with the learning rate's noise regime by placing them at the transition between high and low noise to maximally recover model capacity. Extensive evaluations across vision and NLP benchmarks demonstrate that DRT achieves accuracy comparable to full-rank training while retaining the computational benefits of low-rank methods, and proves compatible with various decomposition techniques and orthogonal regularization methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and the writing, figures, and algorithmic pseudocode are easy to follow.\n\n2. Evaluations on diverse benchmarks are thorough and show consistent performance improvements with moderate computational overhead."}, "weaknesses": {"value": "1. Limited Theoretical Depth. The theoretical analysis remains heuristic, and does not provide rigorous convergence guarantees or formal explanation on how the scheduling principle optimizes rank recovery. \n\ni) Proposition 1 provides a bound on the rank of the reconstructed matrix that is derived under an idealized assumption. The low-rank component is supposed to perfectly cancel the base weights, which may not reflect the complex, stochastic optimization dynamics in practice. \n\nii) Similarly, Proposition 2 shows that the update gap is constrained by a function of the learning rate. However,  Proposition 2 cannot formally guarantee that the proposed scheduling strategy minimizes the cumulative gap or for maximizing rank recovery.\n\n2. Sensitivity to Scheduling Hyperparameters. DRT introduces critical new hyperparameters of inflation (I) and deflation (D) epochs that could rely on the dataset, and could be sensitive to the inflation/deflation schedule, as shown in the appendix where different I/D are used for CIFAR-10 and CIFAR-100. It is non-trivial to identify the optimal schedule for the dataset-specific tuning and could cause computationally expensive search process. High-level scheduling principles are provided in Section 4.2 but are still vague guidelines rather than an automated or theoretically grounded procedure. Theoretical or empirical justification for determining a near-optimal schedule in a model-agnostic way is lacking, especially for billion-parameter architectures. Sensitivity to scheduling hyperparameters hampers the framework from a robust and general-purpose efficiency solution.\n\n3. Insufficient Empirical Validation on Scalability and Practical Efficiency. The empirical evaluation does not fully support the generalizability of the proposed framework, as it lacks testing on large-scale foundation models where low-rank efficiency is most critical. Moreover, the computational analysis relies on an averaged parameter count, but does not provide peak memory usage and actual time overhead during full-rank phases that are usually leveraged to evaluate the practical utility."}, "questions": {"value": "Please refer to the section of weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMMzYTdsQ6", "forum": "54BPFBsT2p", "replyto": "54BPFBsT2p", "signatures": ["ICLR.cc/2026/Conference/Submission10681/Reviewer_VwGw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10681/Reviewer_VwGw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992136284, "cdate": 1761992136284, "tmdate": 1762921928125, "mdate": 1762921928125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}