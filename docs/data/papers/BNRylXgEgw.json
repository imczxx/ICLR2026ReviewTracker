{"id": "BNRylXgEgw", "number": 15628, "cdate": 1758253338870, "mdate": 1763759248937, "content": {"title": "Provable Policy Optimization for Reinforcement Learning from Trajectory Preferences with an Unknown Link Function", "abstract": "The link function, which characterizes the relationship between the preference for two trajectories and their cumulative rewards, is a crucial component in designing RL algorithms that learn from preference feedback. Most existing methods, both theoretical and empirical, assume that the link function is known (often a logistic function based on the Bradley-Terry model), which is arguably restrictive given the complex nature of preferences, especially those of humans. To avoid mis-specification, this paper studies preference-based RL with an unknown link function and proposes a novel zeroth-order policy optimization algorithm called ZSPO. Unlike typical zeroth-order methods, which rely on the known link function to estimate the value function differences and form an accurate gradient estimator, ZSPO only estimates the sign of the value function difference. It then constructs a parameter update direction that is positively correlated with the true policy gradient, eliminating the need to know the link function exactly. Under mild conditions, ZSPO provably converges to a stationary policy with a polynomial rate in the number of policy iterations and trajectories per iteration. Empirical evaluations further demonstrate the robustness of ZSPO under link function mis-specifications.", "tldr": "ZSPO is a zeroth-order sign-based policy optimization algorithm with provable guarantees for reinforcement learning from trajectory preference feedback with an unknown link function.", "keywords": ["reinforcement learning theory", "preference-based reinforcement learning", "unknown link function", "zeroth-order optimization", "policy optimization", "stochastic MDPs", "provable convergence"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd3ea3b103cb903b9f74af80db996122c7d0845a.pdf", "supplementary_material": "/attachment/cae322b4f1f0a7b3317950d8f80a91a4c53b877b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a zeroth-order optimization algorithm for preference-based RL with an unknown link function. The algorithm updates the policy parameters toward a direction that increases the value function by using random perturbations and majority votes. The authors provide convergence guarantees to a stationary point and experiments demonstrating that the algorithm outperforms baselines when the link function is misspecified."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed algorithm is simple to implement and directly optimizes the policy without relying on a reward model, similar to DPO and NLHF.\n\n- The theoretical analysis is thorough and mathematically sound.\t\n\n- The empirical results are also promising: the algorithm performs better than baselines that assume a BT model under misspecified settings, over all three environments."}, "weaknesses": {"value": "- The motivation could be stronger. The authors could further discuss why it is important to consider broader classes of link functions beyond the BT model, given that the BT model performs well empirically and has theoretical justification through the Borda rule in social choice theory.\n- Moreover, while the linear link function is used to demonstrate misspecification in experiments, it is unclear how practical this model is. It is also unclear how much performance degradation occurs in the proposed algorithm (compared to others) if the true link function were sigmoid.\n- Scalability to high-dimensional settings such as LLMs is unclear. It seems unlikely to me that the proposed approach would scale efficiently.\n- The bound converges to 0 as $D \\rightarrow \\infty$, but practically it is difficult to use large $D$ in online settings."}, "questions": {"value": "- In line 161, the global optimizer $\\theta^*$ is defined but never used. It seems that no theoretical results establish convergence to a global optimizer.\n- In experiments, what value of $D$ was used? I only found $D=1$ in the stochastic gridworld example."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lu8iDKuG5R", "forum": "BNRylXgEgw", "replyto": "BNRylXgEgw", "signatures": ["ICLR.cc/2026/Conference/Submission15628/Reviewer_HMb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15628/Reviewer_HMb6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366618124, "cdate": 1761366618124, "tmdate": 1762925892251, "mdate": 1762925892251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We thank all the reviewers for their time reviewing our paper, and we address the common concerns. We have revised our manuscript accordingly.\n\n# Empirical Experiments and Clarifications\nAs suggested by the reviewers, we conducted additional experiments with 10 randomly chosen seeds and report the training return curve and best policy performance as mean $\\pm$ std in Fig.1 in the revision, plotted against the number of trajectories sampled. Specifically, we only used $N=1$ roll-outs per policy iteration and $D=1$ trajectories per batch to obtain preferences. Hence, each comparison is based on a **single pair of trajectories**, ensuring consistency across algorithms. The link function generating the preference is linear, and baseline algorithms assume a logistic link function internally, to compare the performances when there is a link function mis-specification. The complete results and discussion can be found in Section. 5 of our revised paper. We outline the average return of the learned policy as follows:\n\n|Environments|CartPole|HalfCheetah|Hopper|\n|-|-|-|-|\n|ZSPO(Ours)|493|1484|515|\n|ZPG|413|1068|457|\n|RM+PPO|260|226|488|\n|Online DPO|438|60|264|\n|ES|210|778|403|\n\nWe observe that the relative empirical performance ordering of algorithms is unchanged compared to our initial submission. For all environments with a link function mismatch, **ZSPO converges even with small $N$ and $D$, and its average return performance is significantly better than all baselines**, demonstrating its robustness to the link function mis-specification. \n\n# Ablations\nWe also conducted two additional ablation studies as suggested by the reviewers, shown in Fig.2 of our revised paper. \n1. We tested all algorithms when the **true link function $\\sigma$ is indeed logistic**, matching the BT model assumed by baselines in CartPole. The results are shown in Fig.2(a). We observe that baseline algorithms improved significantly when we removed the link function mismatch. Still, ZSPO demonstrated a competitive performance. This ablation study emphasizes the importance of recognizing the link function mis-specification, which could drastically hinder the performance of preference-based RL algorithms, and showcases the robustness of ZSPO. \n2.  We studied the performance of ZSPO **with a different number of roll-outs $N$ per iteration**, to reduce the variance in value function sign estimation and improve the convergence rate. The results are shown in Fig.2(b). We observe that as $N$ increases, ZSPO enjoys a faster convergence rate and an improved final policy performance, which is consistent with our theoretical analysis.\n\n# Practicality of Large Batch Roll-outs and Comparisons\n\nWe would like to point out that the theoretical guarantees are for the worst-case instance, applicable to all problems, while most instances in practice are not worst-case. In our updated experiments, ZSPO works well with small numbers of rollouts and batches, which shows its practicality.\n\nOur updated experiments show **ZSPO works well with very small $N$ and $D$**. We intentionally used $N=1$ and $D=1$, and ZSPO still converges with the best performance under a link function mismatch, so neither large $N$ nor large $D$ is necessary in practice. We want to emphasize that the empirical finding is still consistent with our theory. The convergence rate of ZSPO in Theorem 1 depends on two additional terms: the distinguishability error and the approximation error. In the worst case, we would need $N$ and $D$ to be large enough for the concentration to occur, to reduce these two terms. But in practice, when environments like Gymnasium typically have deterministic transitions with regular deterministic reward functions, the preference-value distinguishability issue is much milder and the variance of return is smaller. This allows ZSPO to also learn from single-pair preference to infer the value difference well enough. For example, in our deterministic environment studies, a single trajectory is sufficient. \n\nFor stochastic MDPs, a large $D$ is necessary to ensure the distribution of return concentrates to exhibit some “regularity”, so that trajectory preference will be consistent with the policy preference with a small distinguishability $\\varepsilon_D^*$. This is necessary for any algorithm to learn the optimal policy with an unknown link function. We discussed this unique challenge in detail in Appendix D.2. On the other hand, a large $N$ for the number of rollouts helps with variance reduction, which is sometimes required when the environmental return is noisy. Therefore, **large $N$ and $D$ are both necessary for the convergence in RL problems with higher stochasticity and noisy observation, independent of the used algorithm**. In practice, the numbers depend on the hardness of the problem and could be much reduced. For example, $N=D=1$ is enough for our Gym experiments.\n\n---\n\nWe hope our response can address the reviewers' concerns."}}, "id": "1Bas0mfuPI", "forum": "BNRylXgEgw", "replyto": "BNRylXgEgw", "signatures": ["ICLR.cc/2026/Conference/Submission15628/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15628/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15628/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763760998172, "cdate": 1763760998172, "tmdate": 1763760998172, "mdate": 1763760998172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work extends the standard Bradley–Terry model for preference-based RL by proposing a method that does not assume knowledge of the link function.\nThe core algorithm, Zeroth-Order Sign Policy Optimization (ZSPO), achieves independence from the explicit link function by only estimating the sign of the value difference. The authors theoretically establish ZSPO's convergence to a stationary policy under smoothness and distinguishability assumptions. Empirical evaluation on classic control benchmarks shows the method's strong robustness when the link function is misspecified."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•  Addresses a critical, well-motivated problem by successfully generalizing preference-based Reinforcement Learning to remove the dependence on a known link function. \n\n•  The paper features a clear and insightful articulation of the deep connection between dueling bandit theory and preference-based RL, which helps situate the proposed method within the broader theoretical landscape."}, "weaknesses": {"value": "• The core theoretical framework and results appear to be a minor adaptation of the Zhang & Ying (2024)'s ZPG framework, rather than offering significant conceptual originality.\n-\tZhang, Qining, and Lei Ying. \"Zeroth-order policy gradient for reinforcement learning from human feedback without reward inference.\" arXiv preprint arXiv:2409.17401 (2024).\n• The reliance on a majority-vote mechanism requiring an extremely large number of rollouts (e.g., $N=1000$) raises serious concerns about practical inefficiency and high computational cost, making it potentially infeasible for realistic RLHF applications.\n• The comparison across algorithms is questionable by the use of different preference oracles (e.g., BT versus linear) for each method. This weakens the direct comparability of the empirical results.\n• Although the experiments demonstrate empirical performance improvement and strong robustness to link-function misspecification, the paper provides no corresponding theoretical analysis to explain the underlying mechanism. Specifically, it is unclear how the proposed method mitigates optimization bias or guarantees convergence stability under such misspecification."}, "questions": {"value": "•  Can the authors clarify what theoretical elements are genuinely novel beyond those inherited from ZPG?\n•  How does the majority-vote approximation error behave in smaller N regimes (e.g., N=32,64) relevant to real RLHF scenarios?\n•  Why are different preference oracles used for different algorithms? Wouldn’t this lead to unfair comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uBZAbOOtDW", "forum": "BNRylXgEgw", "replyto": "BNRylXgEgw", "signatures": ["ICLR.cc/2026/Conference/Submission15628/Reviewer_2LSp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15628/Reviewer_2LSp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988132305, "cdate": 1761988132305, "tmdate": 1762925891935, "mdate": 1762925891935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of **preference-based reinforcement learning (PbRL)** under the assumption that the **link function (σ)** — which defines the relationship between trajectory reward differences and preference probabilities — is **unknown or potentially misspecified**.\n\nTo tackle this, the authors propose **ZSPO (Zeroth-Order Sign Policy Optimization)**, an algorithm that enables provable policy improvement without explicit knowledge of σ.\n\nZSPO estimates only the **sign of the value difference** between two policies and determines an ascent direction based on this binary (±1) signal.\n\nThe method thus relies solely on **directional information** instead of full reward magnitude or an assumed link model, effectively leveraging **1-bit preference feedback** for policy updates.\n\nNotably, while ZSPO is described as a **zeroth-order, gradient-free** algorithm, it is not entirely gradient-free in implementation.\n\nThe update rule still follows a **gradient-ascent-like structure**, where the estimated sign signal acts as a coarse directional proxy for ∇θV(πθ).\n\nIn this sense, ZSPO avoids explicit gradient computation or backpropagation but retains the overall framework of gradient-based policy optimization.\n\nTheoretically, the paper provides convergence guarantees for ZSPO under general MDP settings, showing that the expected policy gradient norm decreases polynomially with respect to the number of iterations and trajectory comparisons.\n\nEmpirically, across CartPole-v1, HalfCheetah-v5, and Hopper-v5, the method demonstrates strong robustness under **link function misspecification** (e.g., true link linear vs. assumed logistic), outperforming baselines including RM+PPO, Online DPO, ZPG, and Evolution Strategies in both stability and final performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Theoretical Formulation** — Unlike most prior works in preference-based RL that assume a known link function (typically the logistic Bradley–Terry model), this paper explicitly formulates the problem under *unknown link functions* and provides the first formal convergence guarantee in this setting.\n2. **Simple but Powerful Algorithmic Insight** — The paper shows that effective policy improvement is achievable using only the **sign** of value differences, discarding magnitude information. This leads to a minimal yet expressive framework for **1-bit feedback–based policy optimization**.\n3. **Mathematical Rigor** — The analysis is well-structured, offering explicit convergence rates (Theorem 1 and Corollary 1), a clear definition of the distinguishability constant ε\\*_D, and an interpretable sample-complexity bound. The proof framework extends Lyapunov-drift analysis into a zeroth-order optimization context, which is an elegant theoretical contribution.\n4. **Empirical Persuasiveness** — The experiments systematically simulate link-function misspecification across three Gymnasium control environments, demonstrating that ZSPO remains consistently robust compared to all baselines.\n5. **Clear Relation to Prior Work** — The paper effectively situates ZSPO within the landscape of existing preference-based RL methods such as DPO, ZPG, ES, and RM+PPO, providing a fair and transparent comparative evaluation."}, "weaknesses": {"value": "1. **Complexity of Online Preference Collection** — The algorithm requires multiple batch-level trajectory comparisons and majority-vote queries per iteration, which may be impractical or expensive in real human-in-the-loop systems.\n2. **Discontinuity in Policy Updates** — Since the update direction is derived from 1-bit sign feedback, it can be highly sensitive to noise. Although convergence is theoretically ensured, the variance of updates may be substantial in practice.\n3. **Limited Experimental Scope** — All experiments rely on synthetic preference oracles; no evaluation with real human or LLM feedback is included, which limits claims of real-world applicability.\n4. **Baseline Comparability Issues** — Some baselines differ in setup: RM+PPO uses semi-offline reward modeling, while DPO includes KL regularization. Hence, the comparisons are not entirely controlled.\n5. **Lack of Qualitative Analysis** — The paper could benefit from a visualization or sensitivity study showing how different link-function shapes (e.g., steep, flat, linear) influence learning dynamics."}, "questions": {"value": "1. **Practical Scope of Link-Function Uncertainty** — In what real scenarios (e.g., human crowd evaluations, noisy LLM feedback) does the “unknown link function” assumption most accurately apply?\n2. **Stability of Policy Updates** — Since 1-bit sign gradients might cause oscillation or overshooting in policy space, have you explored adaptive step-size or momentum strategies to mitigate instability?\n3. **Human-in-the-Loop Applicability** — When extending to actual human feedback, how could the number of comparisons per iteration (N, D) be reduced? Would active preference sampling or uncertainty-driven querying help?\n4. **Partial-Information Extensions** — If partial knowledge of the link function is available (e.g., monotonicity or σ′(0)), can the method be modified to accelerate convergence beyond the purely sign-based approach?\n5. **Policy-Label Feedback (Relation to PPL Work)** — How might ZSPO adapt if behavior-policy labels are provided, as in online PPL settings? Could incorporating such distributional information improve gradient-direction accuracy?\n6. **Comparison with Pebble’s Online PPO** — The Pebble framework (Wirth et al., 2021; Wang et al., 2023) also conducts online preference-based learning using PPO within a fully interactive loop. Given that RM+PPO in this paper follows a semi-offline setup, a comparison with Pebble’s *online* PPO (which shares the same data collection and update loop structure) could provide a more direct empirical baseline.\n    \n    Could the authors clarify why this comparison was not included, and whether differences in data collection protocol or link-function assumptions made it infeasible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H4Z8TAsNFg", "forum": "BNRylXgEgw", "replyto": "BNRylXgEgw", "signatures": ["ICLR.cc/2026/Conference/Submission15628/Reviewer_C5xV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15628/Reviewer_C5xV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001893450, "cdate": 1762001893450, "tmdate": 1762925891406, "mdate": 1762925891406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a zero order preference based algorithm that does not require the specification of the link function, i.e., the function that connects the preference with the underlying rewards. Common algorithms use the Terry-Bradly model (a sigmoid) which can limit the performance of these algorithms in the case of preference misspecification as argued by the authors. The algorithm is a zero-order algorithm that directly searches in the parameter space of the policy. It chooses a random update direction and then computes via the preference feedback whether this direction improved performance or not. The main contribution of the paper is the theoertical analyis showing convergence bounds in terms of the gradient norm of the algorithm. Moreover, the algorithm is compared against baselines on 3 continuous control tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the theoretical analysis seems sound (but I could not check every detail)\n- The new algorithm has proofable convergence\n- Using preference-based RL without specification of the link-function has not been studied so far in the literature"}, "weaknesses": {"value": "- The assumptions made by the algorithm are very far from beeing practical. The algorithm requires that we generate D trajectories for both policies N times in order to compute a single gradient update. In practice, it will be very hard for humans to compare D trajectories. Most practical RLHF algorithms compare trajectories instead of policies, i.e., they can learn from the comparison of single trajectories instead of a batch of trajectories.\n- The black-box manner of the algorithm also brings severe limitations (which is however also comparable to the recent ZPG algorithm). As the preference compares policy parameters and not trajectories, it is for example very hard for the algorithm to take random initial states into account. This will only work if we massively increase the number of trajectories D in the comparison. \n- The experimental evaluation is not convincing. It looks very noisy. Its unclear how many seeds have been used, but from the plot it seems way too small to make a proper statistics. Authors should use at least 10 (better 20) seeds to get better statistics\n- The experimental setting is also not fully clear to me. Are the baselines evaluated in a similar manner then the algorithm (use a batch of trajectories for the preference comparison) or are they applied to single trajectory pairs? Algorithms such as DPO are not black box, so they can leverage single trajectory comparisons in a much more straightforward way. It is an unfair comparison if these algorithms are evaluated only on preferences on trajectory batches as they do not have the same limitations as the presented algorithm. \n- More ablations should be performed. For example, it would be insightful if the derived bounds hold at least approximately (i.e. by showing how the performance changes with number of comparisons N and batch size D). It would also be good to show experiments with different ground truth link functions, in particular, if the ground truth link function is indeed the terry-bradley model, how would the algorithm perform against the baselines that use the terry-bradley assumptions."}, "questions": {"value": "- Please specifiy what exactly a \"linear link function\" is (used for the experiments)\n- The link function is not formally defined in the beginning. That would help the understanding\n- It would be helpful to see the number of trajectories or number of samples on the x axis in the results instead of the number of iterations\n-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gvWX2pP1Wk", "forum": "BNRylXgEgw", "replyto": "BNRylXgEgw", "signatures": ["ICLR.cc/2026/Conference/Submission15628/Reviewer_evWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15628/Reviewer_evWw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15628/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233741992, "cdate": 1762233741992, "tmdate": 1762925890882, "mdate": 1762925890882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}