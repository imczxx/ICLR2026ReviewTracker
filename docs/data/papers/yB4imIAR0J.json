{"id": "yB4imIAR0J", "number": 5447, "cdate": 1757910873609, "mdate": 1759897974598, "content": {"title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit", "abstract": "Analyzing large-scale text corpora is a fundamental challenge in machine learning, crucial for tasks like auditing training data for biases or identifying undesirable model behaviors. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create *SAE embeddings*: interpretable representations whose dimensions each map to a specific concept. Through four text analysis tasks, we show that SAE embeddings offer the controllability that dense embeddings lack while being far cheaper than LLMs. We show that SAE embeddings find useful insights in large amounts of text data, enabling undirected insight discovery, such as (1) identifying differences between datasets and (2) uncovering unexpected correlations between concepts. For example, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings find more prominent differences at 2-8× lower cost and reveal correlations more reliably. Additionally, SAE embeddings are controllable: compared with dense embeddings, SAEs can (3) cluster texts into novel groupings and (4) query documents based on implicit properties. Lastly, we illustrate SAE embeddings' utility for understanding model behavior through two case studies: investigating how OpenAI model behavior has changed over new generations and uncovering a spurious correlation in Tulu-3's training data. These results position SAEs as a versatile tool for text data analysis and highlight the neglected importance of interpreting models through their *data*.", "tldr": "", "keywords": ["Data Analysis", "Sparse Autoencoder", "Embeddings", "Model Evaluations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edce7dcb8c316bdba3f8661150b57a42053bc806.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Large-scale text corpora analysis is challenging. This paper proposes using Sparse AutoEncoders (SAEs) to create interpretable embeddings for analyzing large corpora. \nExtensive quantitative experiments suggest that SAE embeddings are useful and versatile tools for data analysis, benefiting multiple applications, including dataset diffing, correlation, clustering, and retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Some analyses are interesting, especially the LLM characteristics change over different generations of GPT (discussed in Section 5.1), and the identification of spurious correlation in Tulu-3’s SFT dataset (in Section 5.2). This evidence also demonstrates the usefulness of the proposed method.\n- Compared to LLM-based inspection, SAEs are more economical, particularly for data diffing tasks.\n- Experiments are detailed and (apparently) easy to reproduce, improving credibility and reusability."}, "weaknesses": {"value": "- Organization and readability can be improved; heavy cross-referencing to tables/figures disrupts flow. A streamlined structure would significantly help comprehension.\n- The procedure for producing interpretable activation vectors is not very clear. I am confused about how to define the meanings for each feature. For example, in Fig. 1, why does “feature 1” correspond to nouns and “feature 2” to animals?"}, "questions": {"value": "- How are 61,521 latent descriptions obtained from a dictionary of size 65,536? What is the exact pipeline for deriving concrete descriptions per feature?\n- For lines 302–303 and Fig. 8 (retrieval), how are latent labels produced? Are they LLM-generated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ucOc83P2HK", "forum": "yB4imIAR0J", "replyto": "yB4imIAR0J", "signatures": ["ICLR.cc/2026/Conference/Submission5447/Reviewer_3QR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5447/Reviewer_3QR3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761365465148, "cdate": 1761365465148, "tmdate": 1762918067175, "mdate": 1762918067175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper utilizes pretrained SAEs' latens to build an embedding model that is able to capture property of interest and also allows interpretability whereas dense embedding models lack both, and are trained to capture semantics which does not allow full controllabilitiy. With their methodology, they are capable of doing dataset diffing more efficiently and accurately and it enables exploration of unknown correlation between different concepts that are often missed by LLMs. Moreover they adapt these embeddings to other downstream tasks such as clustering and retrieval. Lastly, they analyze two case studies which are the behavior change of openai models illustrated by their personification ability, and analysis of tulu3 sft data in which they find that math/latex like concepts correlate with the hoping and they are able to trigger this behaviour by using the correlated concepts. Overall, they modify the popular SAE framework-which is primarily used to investigate model behaviour- to generate embeddings that could be used in various NLP tasks while often outperforming or being on par with LLMs or dense embedding models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Paper is well written and easy to follow, figures are creative and helpful.\n* Even though SAEs are well known in mech interp, adaptation of them as embedding models is both interesting and novel.\n* Experimental setups are clearly explained and diverse, and claims are coherent with the findings."}, "weaknesses": {"value": "Major\n* Lack of ablations on SAEs(size,corpora etc), and similarly for reader LLM, and also diversity of datasets.\n\nMinor \n* A lot of the results are in the appendix, so there's a lot of back and forth while reading."}, "questions": {"value": "1)How much does the SAE or reader LLM impact quality of embeddings, have you done any ablations on them?\n2) The paper primarily uses chatstyle prompts, have you guys explored any other datasets or prompts that could be treated as somewhat out-of-distribution in dataset-diffing and correlation experiments?\n3) It is known that malicious texts in the pretraining do poison LLMs, how can we adapt this framework to large scale corpora filtering?\n4) As the paper mentions, SAE embeddings are mostly property-based. Could we improve current dense embeddings by training SAEs on them, or by taking a trained SAE from a model like Llama3-70B or Gemma (which already have SAEs) and converting it to dense embeddings with something like LLM2Vec? Have you seen any cases where this actually helps, or are they just naturally different approaches?\n5) Feature relabelling is critical, what if we dont know the distribution of the data so we cant recorrect them which is plausible in the deployment, how much will the quality of embeddings degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nVp1P6ghuW", "forum": "yB4imIAR0J", "replyto": "yB4imIAR0J", "signatures": ["ICLR.cc/2026/Conference/Submission5447/Reviewer_bVzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5447/Reviewer_bVzV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665039876, "cdate": 1761665039876, "tmdate": 1762918066934, "mdate": 1762918066934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of Sparse Autoencoders (SAEs) trained on LLM hidden states to create interpretable embeddings for text data. Each dimension in the SAE embedding is supposed to represent a human-interpretable feature, which the authors use for four text analysis tasks: comparing datasets, finding correlations, clustering, and property-based retrieval. The paper presents two important case studies: The first tracks behavioral changes across ChatGPT versions, showing increasing nuance and self-critique in responses, more personalized follow-ups, and descriptions that personify objects more frequently. The second analyzes Tulu-3’s fine-tuning data and finds that certain prompt formats, such as numbered lists, LaTeX math, and role-playing instructions, consistently cause it to end with the phrase “I hope it is correct.” The paper traces this to a specific training subset and confirm that these prompt patterns directly trigger the behavior in the Tulu-3 model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel and creative application of SAEs beyond their typical role in LLM interpretability to the domain of textual data analysis. I think SAEs are a great choice as a data analysis toolkit for the following reasons:  the interpretable and sparse embeddings offer greater controllability compared to dense embeddings, like enabling pre-filtering of features for targeted analysis of specific properties. Further, SAEs can capture implicit features of chat dialogues beyond coarse semantic content, e.g., sycophancy, anthropomorphism, or the presence of reasoning chains, which dense embedding models typically miss. This makes them a better candidate for analysing model training data/responses\n\nThe real-world use cases presented in the paper are great illustrations of the method’s practicality. Additionally, this paper presents a promising direction for unstructured data analysis of model-related data and might encourage interesting future research."}, "weaknesses": {"value": "Overall, the experiments lack rigor, and the work feels preliminary (details below). I see this paper as a good proof-of-concept, and in its current state, it is more suitable for a workshop or a blog post.\n\nI have listed some weaknesses along with some suggestions below (loosely in order of priority). Many of them are related to the four data analysis tasks. Personally, I think these tasks could be removed altogether. The paper would be stronger if it focused more on the case studies instead. You could then consider organizing the findings into clearer categories, such as debugging fine-tuning data or understanding model behavior both within and across model families.\n\n* Table 6 provides a qualitative sanity check but lacks a quantitative or statistically grounded evaluation. Adding a measurable metric along with a comparison to the LLM baseline would increase the reliability of the claim.\n* The correlation discovery method in Section 4.2 lacks a quantitative evaluation of its signal-to-noise ratio. Quantifying the proportion of trivial or false-positive correlations would help in assessing the method’s practical utility and how much manual checking is required\n* In Section 4.4, the SAE retrieval setup introduces higher computational overhead due to latent-query dense similarity matching. Exploring alternative strategies that are more efficient would make the method more practical for real-world deployment.\n* In Figure 2, the effect of correlated or redundant latents on difference detection is not analyzed. SAEs tend to produce correlated latents, and one should account for their impact on frequency differences.\n* In Section 4.3, clustering results are only reported for one algorithm. Demonstrating consistent results across multiple algorithms would increase the reliability of the claim.\n* Results are reported for only one SAE model. Testing multiple SAE models would make the findings more reliable and better support the claim that SAEs are a good choice.\n* Prompt variation effects across all experiments are not studied. Testing different prompt formulations for the LLM stages and baselines would clarify how sensitive the results are to prompt design.\n* The analysis workflows depend heavily on another LLM to summarize or relabel latent features, suggesting that SAEs alone are not sufficient for the intended tasks. Designing experiments that evaluate the effectiveness of SAEs in isolation would provide a clearer understanding of their independent capabilities and limitations.\n* Many of the key results (Table 6, Figure 12, Table 10, Table 11, Table 12, Figure 13) are placed in the appendix rather than integrated into the main text. Bringing these results forward would make the paper’s main findings more accessible. For instance, while reading the paper, I had to move back and forth multiple times just to understand the results of each experiment. In similar vein, the figure that explains the flow for each of the analysis tasks (Figure 8) is also in the appendix. Including this figure in the main text would improve readability.\n* In section 3, the paper does not describe how “activating examples” for a latent are selected. Describing the process would help better understand the SAE pipeline.\n* The claimed 2-8x cost improvements appear less compelling since it is demonstrated only for the dataset diffing experiment. In fact, it feels somewhat misleading, as the clustering experiment actually shows an increase in computational cost.\n\nFurther, each task involves a fairly complex workflow and consequently could introduce compounding errors at multiple stages."}, "questions": {"value": "* Are the relabelling costs included in the SAE method in Table 2?\n* Is the precision for all tasks affected when the SAE training distribution differs substantially from the distribution of the target datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pYK0dnqFet", "forum": "yB4imIAR0J", "replyto": "yB4imIAR0J", "signatures": ["ICLR.cc/2026/Conference/Submission5447/Reviewer_N1em"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5447/Reviewer_N1em"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907165489, "cdate": 1761907165489, "tmdate": 1762918066659, "mdate": 1762918066659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors propose using sparse autoencoders (SAEs) to generate interpretable text embeddings. Each SAE feature corresponds to an embedding dimension created by max pooling that feature's activations over a document, yielding embeddings where each dimension has a natural semantic interpretation.\n- The authors demonstrate four applications of SAE embeddings: (1) data diffing to identify differences between text corpora, (2) discovering unexpected correlations between features, (3) clustering, and (4) retrieval.\n- For each application, the authors provide both toy setting with known ground truth and open-ended exploratory analyses. In toy settings, they compare SAE embedding performance to corresponding dense embedding or LLM-based baselines.\n- The authors lastly present two case studies: analyzing changes in OpenAI model behavior over time using data diffing, and discovering unexpected feature correlations in Tulu-3's post-training dataset."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "- The idea of using SAEs to generate interpretable text embeddings feels novel and well-motivated.\n- The authors cover an wide breadth of applications - data diffing, correlation discovery, clustering, and retrieval.\n- The experiments have great coverage, including both toy settings with ground truth targets and real-world exploratory analyses. The authors make a solid effort to incorporate baselines (dense embeddings and LLM-based methods) for comparison.\n- The real-world case studies find some interesting behaviors in deployed models (Grok, OpenAI models over time, Tulu-3)\n- The author's use a LLM judge to verify many of the hypothesis generated by the diffing and correlations based methods - although implementation details are unclear"}, "weaknesses": {"value": "- The paper's breadth makes it challenging to communicate each experiment with sufficient depth. The main text requires constant cross-referencing with the appendix, and key details are often unclear or left for the reader to infer—for example, the latent relabeling procedure, synthetic dataset construction in Section 4.2, what constitutes a \"hypothesis,\" and how hypotheses are verified.\n- Many results follow a pattern of generating hypotheses, verifying some subset, and presenting the verified ones. It would be helpful to understand the selection process better: what is the false positive rate? How many proposed hypotheses were actually verified? Providing this context would help assess whether the positive results shown are representative or cherry-picked.\n- Some applications feel less compelling and distract from the stronger results. For instance, the correlation findings reflect obvious dataset structure (e.g., Stack Exchange containing both QA and code). Similarly, the clustering results aren't falsified or verified, and they don't demonstrate the key advantage of SAE embeddings—the ability to cluster on specific interpretable concepts—instead just showing that SAE and dense embeddings produce different clusters.\n\nOverall: my main concerns are about presentation and prioritization rather than the method or experiments themselves. Substantial revision for clarity would improve my assessment of the paper."}, "questions": {"value": "- Can you provide more detail about the false-positive rate of hypotheses generated by your method? For the OpenAI behavior analysis and Tulu-3 data analysis, were the presented results selected from a larger set of hypotheses that were first verified? Understanding the selection process would help assess how representative these findings are. Similarly, was the choice to present only Grok4 behavior diffing results because it had the most differences, or because the other diffed models lacked different behaviors?\n- You currently use one SAE configuration on one model, which is reasonable for a proof of concept. Do you have any preliminary experiments or intuitions about how method performance might change with different SAE configurations—both in terms of dictionary size and the size/complexity of the underlying model?\n- Regarding the token usage comparisons in Table 2, as I understand it, the SAE tokens appear to be from Llama-70B, while the LLM baseline tokens are from whatever frontier model is used to review/process the data (Gemini 2 Flash). This does not seem to be a fair comparison unless I am missing something."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "duhURHZwT6", "forum": "yB4imIAR0J", "replyto": "yB4imIAR0J", "signatures": ["ICLR.cc/2026/Conference/Submission5447/Reviewer_v8eo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5447/Reviewer_v8eo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943385356, "cdate": 1761943385356, "tmdate": 1762918066322, "mdate": 1762918066322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}