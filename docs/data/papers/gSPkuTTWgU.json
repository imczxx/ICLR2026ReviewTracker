{"id": "gSPkuTTWgU", "number": 24756, "cdate": 1758359983161, "mdate": 1759896750356, "content": {"title": "Is Graph Unlearning Ready for Practice? A Benchmark on Efficiency, Utility, and Forgetting", "abstract": "Graph Neural Networks (\\textsc{Gnn}s) are increasingly being deployed in sensitive, user-centric applications where regulations such as the GDPR mandate the ability to remove data upon request. This has spurred interest in graph unlearning, the task of removing the influence of specific training data from a trained \\textsc{Gnn}  without retraining from scratch. While several unlearning techniques have recently emerged, the field lacks a principled benchmark to assess whether these methods truly provide a practical alternative to retraining and, if so, how to choose among them for different workloads. In this work, we present the first systematic benchmark for \\textsc{Gnn} unlearning, structured around three core desiderata: \\emph{efficiency} (is unlearning faster than retraining?), \\emph{utility} (does the unlearned model preserve predictive performance and align with the retrained gold standard?), and \\emph{forgetting} (does the model genuinely eliminate the influence of removed data?). Through extensive experiments across diverse datasets and deletion scenarios, we deliver a unified assessment of existing approaches, surfacing their trade-offs and limitations. Crucially, our findings show that most unlearning techniques are not yet practical for large-scale graphs. At the same time, our benchmarking yields actionable guidelines on when unlearning can be a viable alternative to retraining and how to select among methods for different workloads, thereby charting a path for future research toward more practical, scalable, and trustworthy graph unlearning.", "tldr": "Our benchmark shows that graph unlearning can rival retraining in select scenarios, but in most cases, it remains less reliable than retraining from scratch", "keywords": ["graph unlearning", "GNN", "graph neural network"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b8981fbc7d8e56388f67091f4bd7a564e2eae52.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a benchmark for Graph Neural Network (GNN) unlearning methods, evaluating them based on Efficiency (vs. retraining), Utility (multi-level alignment with retrained models), and Forgetting (MIA resistance). It assesses various existing unlearning techniques across diverse datasets and deletion scenarios. This concludes that most methods suffer from scalability issues (memory, lack of batching), making retraining often preferable for large graphs, and provides guidelines for selecting methods when unlearning is viable. Codebase is also provided."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive Evaluation:** Establishes a benchmark (efficiency, multi-level utility, forgetting, robustness) that (in my opinion, having worked in this field) advances evaluation standards in GNN unlearning.\n2. **Practical Relevance:** Directly compares against retraining, assesses scalability on large graphs, and considers hidden computational costs, providing a reality check on practical viability.\n3. **Clear Insights & Guidance:** Systematically reveals limitations (especially scalability/batching) of current methods and provides clear, evidence-based recommendations summarized effectively (Table 10).\n4. **Clarity and Reproducibility:** Very well-written and organized, making comparisons accessible. Also provides an open-source codebase implementing multiple methods within its framework."}, "weaknesses": {"value": "1. **Limited Scope of \"Forgetting\":** Relies solely on one type of MIA based on posterior shifts. Forgetting is a complex concept, and exploring other attack types (ex., attribute inference, link re-inference) or information-theoretic measures could provide a more complete picture. The MIA results (Figures 1, 3) show subtle differences, making strong conclusions about relative forgetting quality difficult for methods near the 0.5 AUROC baseline.\n2. **Focus on Node Classification:** Primarily evaluates unlearning for node classification. Performance trade-offs might differ for other tasks like link prediction or graph classification.\n3. **Implementation vs. Algorithm:** The scalability critique focuses heavily on current *implementations* lacking batching. While reflecting the practical state, it doesn't definitively rule out that some *algorithms* could be made scalable with more engineering effort. A deeper discussion of algorithmic amenability to batching would be valuable.\n4. **Novelty:** As a benchmarking paper, its novelty lies in the depth and width of its framework and insights rather than proposing new methods. So while highly valuable, I would prefer to have covered all the setups in the same framework for it to be complete."}, "questions": {"value": "1. I would definitely like to see Cognac [1] being evaluated to retain my rating. From what I understand, it can easily be adapted to work and evaluated in this setting, even if they don't evaluate it in this setting in their paper. It might not be the state-of-the-art in this setting, but it is SOTA in a harder setting, so it still needs to be evaluated for completeness.\n2. The MIA results in Figure 1 show PROJECTOR and GRAPHERASER performing poorly. Given that these methods are designed with stronger (sometimes exact) unlearning notions in mind, this seems counterintuitive. Do you have hypotheses as to why they failed the MIA test used? Does it relate to their specific architectures or the assumptions underlying the MIA?\n3. You highlight the lack of batching support as a major scalability bottleneck. Based on the algorithmic principles of methods like MEGU, GIF, or GNNDelete, what are the core technical challenges in developing batch-compatible versions? Are certain paradigms inherently harder to batch than others, or is it just a matter of implementation?\n4. Table 10 provides a useful summary. For MEGU, efficiency is marked 'X' due to being slower than GOLD on large graphs. However, it performs well on utility/forgetting. Could there be scenarios (frequent small deletions on medium graphs) where MEGU's overhead is acceptable given its strong utility? The guidance seems slightly harsh as it's binary (retrain vs. use X).\n5. How sensitive are the efficiency results (Table 7) to implementation details? Could optimized implementations significantly change the ranking or make more methods competitive with retraining?\n6. Sidenote: I'm also not sure if the paper violates margin constraints. It seems awfully dense and cluttered.\n\n---\n\n*[1] Kolipaka, Varshita, Akshit, Sinha, Debangan, Mishra, Sumit, Kumar, Arvindh, Arun, Shashwat, Goel, Ponnurangam, Kumaraguru. \"A Cognac Shot To Forget Bad Memories: Corrective Unlearning for Graph Neural Networks.\" Proceedings of the 42nd International Conference on Machine Learning (ICML).*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wGaccaC7jv", "forum": "gSPkuTTWgU", "replyto": "gSPkuTTWgU", "signatures": ["ICLR.cc/2026/Conference/Submission24756/Reviewer_MdZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24756/Reviewer_MdZs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566969090, "cdate": 1761566969090, "tmdate": 1762943187684, "mdate": 1762943187684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a systematic review of recent graph unlearning techniques, and benchmarks them in line with the usual three properties used to evaluate graph unlearning: utility, forgetting, and efficiency. The authors evaluate a wide array of unlearning methods on the task of random node unlearning. They include an ablation with more targeted unlearning. In particular, they introduce new metrics to measure the utility of a model, where they also look at the fidelity, L2 distance, and parameter difference between the original and unlearnt model. Overall, I feel the work is timely, but the contributions are limited and narrow. I will discuss this further in weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper does well to create a comprehensive unlearning benchmark\n- The paper is a pleasure to read, and is well-written.\n- The paper goes beyond the metrics, and discusses hidden training costs (which I appreciate a lot), as well as OOM issues which most graph unlearning methods suffer from as scale increases—contrary to their claims.\n- I really appreciate the strong concluding section with limitations and insights for practical deployment"}, "weaknesses": {"value": "1. Key contribution 1 (Unified benchmarking framework), is not exactly a contribution. If you look at any graph unlearning paper released in the past couple years, they all measure efficiency, utility and forgetting. This is certainly not the first comprehensive evaluation, and frankly I think is a bare minimum for any paper introducing a new method [1][2]. Therefore, I fail to see why this should be a core contribution.\n\n2. All main experiments are performed on 1 setting: randomised node unlearning. I feel this is very limiting, and cannot in general inform broad claims about unlearning methods. Edge unlearning or feature unlearning are not even mentioned, even though most of the mentioned algorithms focus on all of them. Since the paper positions itself as a benchmark paper, I feel this is a reasonable ask. [3] is mentioned as a competing benchmark in the paper, and performs these tasks and more.\n\n3. The reason for exclusion of [1] in the study is that \"it operates on the paradigm of corrective unlearning instead of privacy preservation\". I find this unsatisfactory for a number of reasons. Firstly, the experimental setting is randomised node unlearning, which by itself has nothing to do with privacy, and is used as a proxy to evaluate unlearning on a set of nodes that were asked to be deleted. From what I can see in [1], the actual procedure of unlearning is identical to other unlearning methods. Second, many of the other methods included here are included in [1] for a comparison, so why is the converse not possible?\n\n4. Continuing from point 3, I feel [1] has a very reasonable and practical evaluation of unlearning, which is certainly better than random unlearning. Since this is a benchmark paper, would it not make sense to include various settings of attacks? A lot of unlearning papers mention that they can operate on adversarial attacks as well (see [2]), even if their main area is privacy. This is an addition to point 2. I feel the paper can benefit from including more experimental settings from both [1] and [2].\n\n5. One point of technical novelty in the paper is the introduction of multiple levels of measuring *utility*. I agree for the need of the fidelity metric, and find that to be a nice contribution, however, I feel the other introduced metrics, the logit distance and the model-level distance, are redundant and don't offer much. I would ask the authors for a clarification on my understanding of these metrics.\n\n    - (1) Won't a similar fidelity score imply a similar logit distance? Since the output class of the predictions is directly dependent on the logits? \n    - (2) The paper says \"If the parameters are close, then the intermediate activations at each GNN layer are also likely to be similar\". I am not sure I can naively believe this. and would prefer some theoretical or empirical backing of this. \n    - (3) I would like to ask how these metrics make a difference in the decision making guidance (since utility is mentioned as a whole there), and if they don't, what is the contribution exactly, and why are they needed?\n\n6. I feel there should be more of a discussion on the actual \"unlearning\" part (forgetting). The paper discusses at length about the impact on their introduced metrics, but they are for model utility. I feel forgetting is more of an under-explored area and MIA is not the best indicator for forgetting performance. I would like the paper to have a longer discussion on forgetting. Specifically, the paper defines \"The objective of unlearning is that the unlearned model Θ˜ be similar to Θ′, which is the gold standard\", and this is true for both utility and forgetting metrics. However, discussion of forgetting is limited. \n\n7. Can the authors clarify what they mean by deletion for retrain-from-scratch, specifically for RQ4? Are the nodes removed from the training set, or are they removed from the graph altogether? Both these settings can drastically change the outcome of the experiments, and warrants a careful discussion for both cases. \n\nOverall, I feel the paper does not adequately answer any of the introduced gaps in the introduction. Evaluation ambiguity remains, Efficiency vs. retraining was the reason inexact machine unlearning methods were introduced, and I don't believe the experimental breadth is sufficient for warranting an analysis of generalisation. Thus, I am leaning towards a rejection.\n\n[1] [Cognac](https://arxiv.org/pdf/2412.00789)\n\n[2] [Megu](https://arxiv.org/pdf/2401.11760)\n\n[3] [Open-GU](https://arxiv.org/pdf/2501.02728)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wiCmLEvQXT", "forum": "gSPkuTTWgU", "replyto": "gSPkuTTWgU", "signatures": ["ICLR.cc/2026/Conference/Submission24756/Reviewer_JbxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24756/Reviewer_JbxA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832175848, "cdate": 1761832175848, "tmdate": 1762943187440, "mdate": 1762943187440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic benchmark for Graph Neural Network (GNN) unlearning methods, evaluating existing approaches across three core desiderata: efficiency relative to retraining from scratch, utility preservation across parameter/logit/performance spaces, and forgetting quality via membership inference attacks. Through extensive experiments on seven datasets (Cora, Citeseer, Photo, Amazon-Ratings, Roman-Empire, OGBN-Arxiv, and Reddit), the authors evaluate eight unlearning algorithms and find that most techniques fail to scale to large graphs due to memory constraints and lack batching-aware implementations. The benchmark reveals that while methods like MEGU achieve strong utility preservation, they often fail the efficiency criterion by being slower than retraining, and that certified unlearning approaches are restricted to impractical linear GNN architectures. The paper concludes that retraining from scratch remains the most viable option for large-scale graphs, calling for fundamental redesigns of unlearning algorithms to support batching."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Comprehensive multi-level evaluation framework**. The paper introduces a rigorous assessment methodology that evaluates unlearning quality at four hierarchical levels (parameter space via L2 distance, logit space via distributional divergence, per-instance fidelity, and aggregate accuracy metrics), which is substantially more thorough than prior work that relied solely on aggregate performance metrics. This multi-level analysis successfully reveals subtle discrepancies, such as IDEA achieving reasonable fidelity (0.92 on Cora in Table 4) while exhibiting large logit divergence (18.03 on Cora in Table 5), demonstrating that aggregate metrics alone are insufficient for assessing unlearning quality.\n\n2. **Critical practical insights with actionable guidance**. The paper goes beyond empirical results to provide concrete decision-making guidelines (Table 10) by systematically comparing all methods against three essential pillars and revealing critical overlooked issues. Specifically, the work exposes widespread runtime under-reporting in existing literature (Section 5.3 discusses how MEGU excludes adjacency matrix conversion, GIF/IDEA omit neighborhood identification, and GNNDELETE ignores edge mask preprocessing), and demonstrates that most methods fail on large-scale graphs where unlearning is most needed (all methods OOM on Reddit except GOLD as shown in Tables 7-8), highlighting a fundamental gap between theoretical unlearning research and practical deployment requirements.\n\n3. **Insightful analysis of theoretical versus practical guarantees**. The paper provides valuable critical examination of certified unlearning methods (Section 3, lines 160-171), clearly articulating why approaches like PROJECTOR, CGU, and SCALEGUN have limited practical applicability despite theoretical exactness guarantees. Specifically, it explains that these methods are \"restricted to a very narrow setting\" requiring custom linear GNNs, binary classification for CGU/SCALEGUN, and assumptions of tractable low-rank Hessians that \"break the efficient update scheme\" for non-linear architectures."}, "weaknesses": {"value": "1. **Limited forgetting evaluation and missing adversarial analysis**. The forgetting assessment relies solely on a simple membership inference attack measuring L2 distance between pre- and post-unlearning posteriors (Appendix A.2), which may not detect sophisticated privacy leaks\n\n2. **Incomplete coverage of recent methods and missing evaluation against state-of-the-art attacks**. While the paper claims to provide \"the first comprehensive evaluation of GNN unlearning\" (page 2, line 67), it omits several recent and promising approaches that address the scalability and efficiency concerns raised in the paper. Notably, Zhang's \"Graph unlearning with efficient partial retraining\" (WWW 2024) [1] and Yang et al.'s \"Erase then Rectify: A Training-Free Parameter Editing Approach\" (AAAI 2025) [2] may also require evaluation in this benchmark. Furthermore, the forgetting evaluation in Section 5.4 and Appendix A.2 uses only basic membership inference attacks, ignoring recent attack methods such as the unlearning inversion attacks by Zhang et al. [3] and the noisy labeler attacks by Sui et al. [4] that specifically target unlearned models and can leak class membership information. Without evaluating against these recent baselines and attacks, the benchmark's claim to provide comprehensive guidance (as stated in the contributions on page 2, lines 72-78) is weakened, particularly since these newer methods may offer better efficiency-utility tradeoffs and the newer attacks may reveal vulnerabilities missed by the simple L2-based MIA employed in the current evaluation.\n\n\n### References\n[1] Jiahao Zhang. “Graph unlearning with efficient partial retraining”. WWW 2024.\n\n[2] Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu. “Erase then Rectify: A Training-Free Parameter Editing Approach for Cost-Effective Graph Unlearning”. AAAI 2025.\n\n[3] Jiahao Zhang, Yilong Wang, Zhiwei Zhang, Xiaorui Liu, Suhang Wang. “Unlearning Inversion Attacks for Graph Neural Networks”. WSDM 2026. \n\n[4] Zhihao Sui, Liang Hu, Jian Cao, Dora D. Liu, Usman Naseem, Zhongyuan Lai, Qi Zhang. “Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy”. IJCAI, 2025."}, "questions": {"value": "1. Could the authors justify why the current evaluation is sufficient to assess GDPR compliance?\n\n2. How would the conclusions in Table 10 and Section 6 change if these methods failed under stronger attack scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1gLm4i4h2o", "forum": "gSPkuTTWgU", "replyto": "gSPkuTTWgU", "signatures": ["ICLR.cc/2026/Conference/Submission24756/Reviewer_hgen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24756/Reviewer_hgen"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884908130, "cdate": 1761884908130, "tmdate": 1762943187148, "mdate": 1762943187148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a systematic benchmark for graph unlearning, organized around three desiderata: efficiency, utility, and forgetting. It evaluates multiple families of unlearning methods across diverse datasets and deletion scenarios, comparing them against a retrained “gold” model. The study reports that while current techniques can sometimes approximate retraining, most are not yet practical for large graphs, and it offers actionable guidance for when to prefer unlearning vs. retraining, plus an open-source implementation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear, multi-level evaluation framework: The work formalizes efficiency/utility/forgetting pillars and defines metrics including fidelity and logit distance beyond accuracy, improving diagnostic resolution. This enhances experimental rigor and comparability. Forgetting measured via MIAs tailored to unlearning, aligning with privacy goals. This matters for trustworthiness.\n2.\tBroad empirical coverage with actionable insights: This paper applies seven datasets spanning homophily/heterophily and scale, which supports generality claims. \n3.\tEffective supplementary material: The appendix provides details and support the main body findings."}, "weaknesses": {"value": "1.\tLimited inclusion of certified methods in main comparisons: Certified approaches are discussed but excluded from benchmarking due to constraints (Sec. 5.1), reducing completeness of the “landscape” under the proposed metrics.\n2.\tRuntime accounting and preprocessing costs: Although the paper critiques under-reporting, its own measurements still aggregate components differently across methods; explicit timing breakdowns (graph construction, neighbourhood extraction, IF computations, etc.) are not standardized. Some claims (e.g., batching incompatibility specifics) are argued qualitatively without per-method ablations quantifying what batching would save or lose.\n3.\tLack of theoretical evidence: The paper does not propose or establish any new theoretical results. It would be beneficial to include theoretical foundations or analyses to support the discussion of existing unlearning algorithms.\n4.\tPrivacy metrics: It is advisable to incorporate privacy metrics into the framework, as privacy is a primary motivation for unlearning."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MYx4SpC5Yp", "forum": "gSPkuTTWgU", "replyto": "gSPkuTTWgU", "signatures": ["ICLR.cc/2026/Conference/Submission24756/Reviewer_hfMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24756/Reviewer_hfMm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895747680, "cdate": 1761895747680, "tmdate": 1762943186839, "mdate": 1762943186839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}