{"id": "05hNleYOcG", "number": 9695, "cdate": 1758135059535, "mdate": 1759897703848, "content": {"title": "PLAGUE: Plug-and-play Framework for Lifelong Adaptive Generation of Multi-turn Exploits", "abstract": "Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization, and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.", "tldr": "Agentic framework for discovering novel potent multi-turn jailbreak attacks that achieve an attack success rate of 67.3% on Claude Opus 4.1", "keywords": ["LLM Red-Teaming", "Agentic AI"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de8dc0979b8266f26b81ee913344d9abba387bb0.pdf", "supplementary_material": "/attachment/da1b9d173949372d38df20cfd54baf183ccdf1be.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces PLAGUE, a plug-and-play framework for designing multi-turn jailbreak attacks on large language models (LLMs). Inspired by lifelong-learning and agentic architectures, PLAGUE divides the attack process into three stages — Planner, Primer, and Finisher — enabling adaptable and modular multi-turn red-teaming. The framework supports integration with prior attacks like GOAT, Crescendo, and ActorBreaker, and achieves significant improvements in attack success rates (ASR) across top-tier models. It also incorporates reflection, memory-based retrieval, and rubric-based evaluation to enhance contextual adaptation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A novel multi-step or multi-agent style plug-and-play architecture elegantly decomposes the multi-turn attack into interpretable stages\n\n- Rigorous evaluation and analyes on Harmbench using various backbones and metrics.  \n\n- High empirical performance: signifixantly outperforms both single- and multi-turn attack baselines in ASR \n\n- Lifelong learning insight with  retrieval-based memory for strategy reuse and adaptation in red timing context \n- Well defined methodology"}, "weaknesses": {"value": "- The main limitation of this paper is that many important related works are missing: X-Teaming (COLM 2025): https://openreview.net/pdf?id=gKfj7Jb1kj, Pandora: (ICLR workshop 2024) https://openreview.net/pdf?id=9o06ugFxIj, Foot-In-The-Floor: https://arxiv.org/pdf/2502.19820, and so on. These methods are also similar. \n\n- Limited novelty in algorithmic components: The phases (planning, reflection, feedback) heavily rely on established agentic principles (e.g., Reflexion, AutoDAN-Turbo, GOAT), combining rather than innovating core algorithms.\n\n- Setting K=2 appears to me that you are considering up to two turns? Is it so? The scores with just k=1 is very low than they were reported in xteaming.\n\n- Writing, evaluation are very confusing. You mentioned SRE and N-ASR were being used interchangeably, which mean you will be reporting either one \n\n- No defense-side evaluation: The paper lacks a systematic analysis of how PLAGUE insights could improve model safety — essential for a balanced ICLR contribution.\n\n- Evaluation uses HarmBench only"}, "questions": {"value": "Setting K=2 appears to me that you are considering up to two turns? Is it so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNNFEkSguZ", "forum": "05hNleYOcG", "replyto": "05hNleYOcG", "signatures": ["ICLR.cc/2026/Conference/Submission9695/Reviewer_WU4n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9695/Reviewer_WU4n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822385069, "cdate": 1761822385069, "tmdate": 1762921205206, "mdate": 1762921205206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PLAGUE, a multi-stage framework for the automated generation of multi-turn jailbreak attacks against Large Language Models (LLMs). The framework decomposes the attack process into three distinct phases: a Planner, a Primer for context-building, and a Finisher for the final attack. The core design aims to enhance the success rate, diversity, and adaptability of multi-turn attacks through a plug-and-play modular architecture combined with a lifelong learning memory mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Systematic Problem Decomposition: A commendable aspect of this work is its attempt to bring a structured and systematic description to the complex and often ad-hoc process of multi-turn attacks. Decomposing the attack into planning, preparation, and execution phases provides a clear workflow for analyzing and designing such attacks.\n\n- Impressive Empirical Results: The method's effectiveness is well-demonstrated, particularly on models known for their strong safety alignment, such as Claude Opus and OpenAI's o3. The data indicates that the system is highly effective in practice."}, "weaknesses": {"value": "- Limited Novelty: Upon closer inspection, the paper's core claimed innovations appear rather weak. First, the \"Primer\" stage, whose central idea is to \"progressively guide the conversation context with a series of seemingly harmless questions,\" is practically the definition of any sophisticated multi-turn attack, not a novel contribution. The \"lifelong learning\" component is essentially a Retrieval-Augmented Generation (RAG) system using vector embeddings to fetch similar strategies from past successes—a common practice in the Agent research domain. Finally, the reflection mechanism, which uses a separate LLM (the Rubric Scorer) to score and provide feedback on generated content, is conceptually identical to the core idea behind agentic reflection frameworks like Reflexion.\n\n- Lack of Deeper Insight: Although the paper successfully jailbreaks the models, it fails to provide deeper insights into the fundamental nature of these LLM security vulnerabilities. It presents an effective attack method but doesn't answer why this method is effective. The lifelong learning module merely reuses similar attack patterns mechanically, without distilling more generalizable principles or patterns from them. For an academic paper, we expect not just a powerful tool, but also a profound understanding of the problem itself."}, "questions": {"value": "In conclusion, this paper leans heavily towards an engineering-focused integration of techniques, presenting a well-constructed and empirically successful multi-turn attack system. However, its original methodological contributions are quite limited, as it primarily integrates and applies existing ideas. This style feels somewhat misaligned with the research-oriented focus of the ICLR community.\nTherefore, I am initially leaning towards a negative rating. My final recommendation will, however, take into account the perspectives of the other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ymKvgkHJvh", "forum": "05hNleYOcG", "replyto": "05hNleYOcG", "signatures": ["ICLR.cc/2026/Conference/Submission9695/Reviewer_4Xpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9695/Reviewer_4Xpd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832842438, "cdate": 1761832842438, "tmdate": 1762921204802, "mdate": 1762921204802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**NOTE: This paper violates the conference formatting guidelines by substantially reducing the page margins to fit more content. I would recommend a desk rejection due to this severe format violation. Nevertheless, I provide my technical evaluation below and defer the final desk-rejection decision to the AC and PC.**\n\n\nPLAGUE is a plug-and-play, lifelong-learning framework for generating modular multi-turn jailbreaks against black-box LLMs: it builds an n-step plan by retrieving successful past strategies (Planner), escalates context with benign-seeming intermediate prompts (Primer), and then executes the final exploit (Finisher), while using rubriced reflection, backtracking, and a memory of successful strategies to adapt over time. Evaluated on the HarmBench benchmark, PLAGUE outperforms prior multi-turn and single-turn methods, achieving ASRs such as 81.4% on OpenAI o3, 67.3% on Claude Opus 4.1, and up to 97.8% on Deepseek-R1, while remaining computationally efficient within a six-turn budget; the authors note ethical risks but argue the framework aids systematic vulnerability evaluation and defense development."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The modular design of PLAGUE is neat.\n\n- PLAGUE introduces a unique embedding-based memory system, enabling it to learn from past interactions and adapt over time to new goals and contexts."}, "weaknesses": {"value": "- The paper’s scope is limited by its exclusive focus on developing attackers without accompanying defensive methods. While PLAGUE advances the study of multi-turn jailbreaks, it offers no systematic exploration of countermeasures or co-evolving defenses. As a result, the work demonstrates how to break safety mechanisms effectively but provides little insight into how to strengthen or adapt them, narrowing its overall contribution to LLM safety research.\n\n- This works misses crucial recent works that introduced performant advances in multi-turn jailbreaks, e.g., https://arxiv.org/abs/2504.13203, https://arxiv.org/abs/2410.10700, https://arxiv.org/abs/2502.19820 which are shown to be substantially better than Crescendo, the baselines included in this paper. In particular, this work shares strong similarities to https://arxiv.org/abs/2504.13203, which also includes planners, optimizers, and intermediate verifiers. Thus it's really important to discuss and compare to these methods."}, "questions": {"value": "In addition to the weakness:\n\n- How does PLAGUE compare to wider range of multi-turn red-teaming methods?\n\n- To serve realistic red-teaming needs for broadly revealing LLM vulnerability, it's crucial that an automatic jailbreak or red-team method to be able to discover a wide range of successful attacks. Is PLAGUE capable of identifying multiple diverse attacks given the same seed harmful query? Could you quantify such ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "twNOgBALCS", "forum": "05hNleYOcG", "replyto": "05hNleYOcG", "signatures": ["ICLR.cc/2026/Conference/Submission9695/Reviewer_LVmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9695/Reviewer_LVmU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150561999, "cdate": 1762150561999, "tmdate": 1762921204415, "mdate": 1762921204415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PLAGUE, a modular, memory-augmented multi-round jailbreak framework that coordinates a three-stage Planner–Primer–Finisher pipeline, achieving state-of-the-art attack-success rates on several mainstream LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Comprehensive experimental coverage. The authors conduct cross model, cross category (all 10 HarmBench classes) multi round attacks on five mainstream commercial and open source LLMs, including closed source heavyweights such as o3 and Opus 4.1, yielding highly credible results.\n2. Rubric-based feedback outperforms binary signals. A fine-grained 10-point scale scored on four dimensions (Relevance, Practicality, Detail, Compliance) is integrated with backtracking and reflection, giving finer control over the attack trajectory."}, "weaknesses": {"value": "1. The “lifelong learning” mechanism is oversold; it is only a static retrieval pool.\nSo-called lifelong learning merely appends a successful attack strategy to a vector base once; there is no online update, forgetting mechanism, policy evolution, or learning from negative samples. It is conceptually misused and far from genuine lifelong-learning techniques such as continual learning or catastrophic-forgetting mitigation.\n\n2. The claimed modularity lacks universal validation.\nAlthough advertised as plug-and-play, only the replacement of GOAT/Crescendo/ActorBreaker is tested. The authors never demonstrate how an arbitrary new module (e.g., a user-designed Planner) would be integrated, specify the interface contract, or show failure cases. Figure 1 also reveals tight coupling (e.g., Primer relies on Planner’s output format), raising doubts about extensibility.\n\n3. Evaluation metrics are one-sided; the diversity–success trade-off is ignored.\nOnly ASR improvement is reported, yet Figure 4 shows that introducing the ActorBreaker Planner raises diversity by 15 % while ASR drops. Attack cost (e.g., manual screening overhead), cross-model transferability (success-rate drop), and human-perceived stealth (ease of user detection) are never analyzed.\n\n4. Technical contribution is incremental; the work is more engineering tuning than principled innovation.\nThe Planner + Primer + Finisher pipeline is essentially an optimized assembly of Crescendo (gradual lure), ActorBreaker (plan generation), and GOAT (strategy pool). Key tweaks such as the 0.7 threshold and vector retrieval are heuristic and lack theoretical grounding or causal attribution (insufficient ablation).\n\n5. Computational cost and latency are unreported, leaving practicality in question.\nAlthough Table 5 counts LLM calls, the authors provide no end-to-end latency (embedding retrieval, summarization, parallel LLM invocations). In real-time red-team settings, six or more API calls plus repeated scoring may exceed the response window of production safety systems, making the threat model unrealistic."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "utFk1lpGtz", "forum": "05hNleYOcG", "replyto": "05hNleYOcG", "signatures": ["ICLR.cc/2026/Conference/Submission9695/Reviewer_7eQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9695/Reviewer_7eQf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9695/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762926271148, "cdate": 1762926271148, "tmdate": 1762926271148, "mdate": 1762926271148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}