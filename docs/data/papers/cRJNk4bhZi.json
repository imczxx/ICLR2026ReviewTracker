{"id": "cRJNk4bhZi", "number": 3601, "cdate": 1757485494489, "mdate": 1759898079126, "content": {"title": "Chart Deep Research in LVLMs via Parallel Relative Policy Optimization", "abstract": "With the rapid advancement of data science, charts have evolved from simple numerical presentation tools to essential instruments for insight discovery and decision-making support. However, current chart data intelligence exhibits significant limitations in deep research capabilities, with existing methods predominantly addressing shallow tasks such as visual recognition or factual question-answering, rather than the complex reasoning and high-level data analysis that deep research requires. This limitation stems from two primary technical bottlenecks: at the training level, existing post-training techniques exhibit deficiencies in handling multi-dimensional reward signal interference and heterogeneous data gradient conflicts, preventing models from achieving balanced development across multiple capability dimensions; at the evaluation level, current methods remain limited to factual retrieval and basic computation, failing to assess end-to-end analytic reasoning and other deep research capabilities. To address the training challenge, we propose PRPO, which performs parallel optimization across reward dimensions and capability partitioning across data types, effectively disentangling conflicts between heterogeneous data and multi-dimensional reward signals while ensuring optimization stability. For the evaluation challenge, we construct MCDR-Bench based on the \"error uniqueness principle,\" transforming subjective generation assessment into objective error identification through controllable error injection, enabling quantifiable evaluation of deep research capabilities. Experimental validation confirms that the proposed PRPO and MCDR-Bench jointly establish a unified framework that systematically advances chart deep research through enhanced collaborative training and objective evaluation.", "tldr": "", "keywords": ["Large Vision Language Model", "Multimodal Deep Research", "Chart Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7579047ff3e1419c3eef536b312f7b06fcfdd62b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of enabling \"deep research\" capabilities in large vision-language models for chart understanding, moving beyond simple factual question answering to complex analytical reasoning. The authors identify a training bottleneck and an evaluation bottleneck. To tackle these, the paper makes two main contributions. First, it introduces Parallel Relative Policy Optimization (PRPO), an algorithm that extends Group Relative Policy Optimization (GRPO) by parallelizing optimization across different reward dimensions (Reward-PRPO) and partitioning data by capability type (Data-PRPO). Second, it proposes a new benchmark, MCDR-Bench, which evaluates deep research capabilities by transforming subjective report generation tasks into objective error-identification tasks based on a novel \"error uniqueness principle.\" The experiments show that PRPO significantly outperforms the GRPO baseline on MCDR-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The motivation and premise of this paper are well-considered. The design of MCDR-Bench also holds certain significance."}, "weaknesses": {"value": "I have the following concerns, and if the authors can address them, I will raise the rating.\n1. The PRPO algorithm appears to introduce significant computational overhead, both in terms of training time and token consumption, due to the parallel reward calculations and the iterative data partitioning process. However, the paper provides no experimental data or theoretical analysis on this front. While the performance gains are impressive, a discussion of the trade-off between performance and computational cost is essential for understanding the practical utility of the method.\n2. A central motivation for PRPO is to resolve conflicts between optimization objectives. However, the proposed solution—a weighted sum of losses with fixed hyperparameters $\\lambda_k$ and $\\lambda_m$—does not fully address this issue. Firstly, manually tuning the $M_{final} + K$ hyperparameters is impractical. The paper states that a default mean aggregation is used(4), which implies uniform weighting. This is unlikely to be optimal and sidesteps the challenge of balancing different objectives. A static, linear combination of losses, as in Equation 4 ($J(\\theta) = \\sum_{k=1}^{K}\\lambda_{k} \\mathcal{L}_k$)(5), does not fundamentally resolve gradient conflicts. During optimization, the gradients from different objectives $\\mathcal{L}_k$ can still point in conflicting directions, leading to destructive interference. This problem has been extensively studied in the field of multi-objective optimization (MOO). The authors should consider and discuss more principled MOO approaches (e.g., [1]) instead of relying on fixed weights.\n3. The core innovation of MCDR-Bench is transforming subjective evaluation into objective error identification via synthetic error injection. While this is a clever evaluation paradigm, the paper does not sufficiently discuss whether these synthetically injected errors are truly representative of the typical failure modes of state-of-the-art LVLMs in real-world scenarios. The paper asserts that the errors are designed to mirror \"typical cognitive biases and logical fallacies\", but there's a risk that the model is simply being trained to detect a specific, synthetic class of errors, which may not perfectly correlate with its ability to produce accurate and logically coherent analysis on its own.\n\n[1] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kRDGIvMF8W", "forum": "cRJNk4bhZi", "replyto": "cRJNk4bhZi", "signatures": ["ICLR.cc/2026/Conference/Submission3601/Reviewer_ehoV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3601/Reviewer_ehoV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835762965, "cdate": 1761835762965, "tmdate": 1762916860441, "mdate": 1762916860441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the chart understanding task of Large Vision-Language Models (LVLMs), aiming to break through the limitation that existing models can only perform simple fact extraction and endow them with complex analytical reasoning capabilities required for \"deep research\". To address the two core issues—training instability caused by multi-dimensional reward conflicts and heterogeneous data, and the lack of an evaluation benchmark suitable for deep analytical reasoning—the study proposes the novel MCDR-Bench benchmark (which transforms subjective chart analysis into an objective error identification task through controlled error injection across five capability dimensions) and the PRPO (Parallel Relative Policy Optimization) training algorithm (extended from GRPO, which mitigates signal interference and gradient conflicts through parallel optimization across reward dimensions and data partitioning by capability type). Additionally, extensive experiments are conducted to verify the effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem and benchmark proposed in the paper are of great significance for the research on deep chart question answering and analytical reasoning.\n\n2. The paper conducts sufficient experiments to verify the effectiveness of the proposed methods.\n\n3. The paper elaborates on the proposed methods in detail.\n\n4. The paper provides code for verifying the experimental results, and this practice is commendable."}, "weaknesses": {"value": "1. The organizational structure of the paper is suboptimal.\n\n2. Hyperparameter sensitivity: The paper mentions hyperparameters λ_k and λ_m but fails to discuss the sensitivity of the results to their settings, lacking a robustness analysis.\n\n3. There is no detailed comparison of the time and resource consumption of different methods.\n\n4. The comparison with baseline methods of the same type is mainly limited to GRPO. However, there are more recent state-of-the-art (SOTA) methods available, such as DAPO."}, "questions": {"value": "1. Could you explain why, in Table 3, the average response length of GRPO after scalar optimization of rewards is longer than that corresponding to format rewards and accuracy rewards, while PRPO can make the response length fall between the two?\n2. In Table 5, why is there such a large score gap when Level 4 and Level 5 are trained separately? How can this phenomenon be explained?\n3. Transforming the task into error detection will encourage the model to be good at \"discovering injected errors\", but will the model rely on subtle traces during the error injection process rather than genuine deep understanding?\n4. Could you elaborate on the differences between the PRPO method proposed in this paper and the corresponding methods presented in References 1 and 3? \n\n[1]  Nguyen, T., Khan, N., Tran, K., Phan, N., & Khalil, I. (2025). PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection. arXiv preprint arXiv:2509.26272. \n\n[2]  Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., ... & Wang, M. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. \n\n[3]  Zhao, Y., Huang, J., Hu, J., Wang, X., Mao, Y., Zhang, D., ... & Chen, Y. (2025, April). Swift: a scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 39, No. 28, pp. 29733-29735)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mRp0QO2tlW", "forum": "cRJNk4bhZi", "replyto": "cRJNk4bhZi", "signatures": ["ICLR.cc/2026/Conference/Submission3601/Reviewer_GFfR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3601/Reviewer_GFfR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895866904, "cdate": 1761895866904, "tmdate": 1762916860143, "mdate": 1762916860143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a unified framework that improves chart research by solving training and evaluation challenges. They present MCDR-Bench, which removes evaluation bottlenecks using an error uniqueness principle that turns subjective assessments into objective error identification, enabling scalable evaluation and reducing reliance on experts. Secondly, authors introduce PRPO, which addresses training bottlenecks by parallel optimization across reward dimensions and capability partitioning across data types, which reduces conflicts between different data and multi-dimensional rewards that have limited progress. Experiments show the combination of their evaluation method and training approach advances chart research beyond surface-level work toward real analytical reasoning and strategic insights, as shown by better exploration, more stable optimization, and stronger performance across multiple analytical dimensions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well written paper.\n- Comprehensive evaluations"}, "weaknesses": {"value": "- Related works can be improved"}, "questions": {"value": "How does PRPO do against improved GRPO versions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tForLgHyN3", "forum": "cRJNk4bhZi", "replyto": "cRJNk4bhZi", "signatures": ["ICLR.cc/2026/Conference/Submission3601/Reviewer_jssn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3601/Reviewer_jssn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987727238, "cdate": 1761987727238, "tmdate": 1762916859794, "mdate": 1762916859794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Chart deep research, which bridges visual chart understanding and Large Language Model (LLM)-driven deep research capabilities, is a complex task that lies at the intersection of pattern discovery, hypothesis testing, and strategic decision support (lines 41-43). Unfortunately, the investigation of chart deep research has been hindered by 1) a lack of a proper evaluation benchmark and framework and 2) the inability of existing post-training techniques to simultaneously handle multiple reward dimensions and heterogeneous data. This work thus releases MCDR-Bench, a high-quality chart benchmark that consists of complex charts with multi-element, multi-layered information. MCDR-Bench supports both subjective and objective evaluation by assessing the report generation and error identification abilities, respectively. In addition, the authors propose Parallel Relative Policy Optimization (PRPO), a novel Reinforcement Learning framework that is capable of mitigating multi-dimensional reward interference (line 201) and data joint optimization conflicts (line 208). PRPO treats each reward dimension as an \"independent optimization objective\" (Reward) and employs capability-based partitioning during the rollout grouping process (Data). Results on MCDR-Bench, as well as other datasets, show that the proposed method significantly outperforms existing approaches in chart deep research."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- MCDR-Bench is an impactful dataset that can fill an important research gap in academia: a lack of a proper benchmark to evaluate deep research capabilities. Not only does it allow researchers to systematically evaluate the chart understanding ability of multimodal LLMs, but it also contributes to assessing LLMs' deep research report generation capabilities.\n\n- The proposed RL training framework for chart deep research is technically sound. The authors pinpoint two key limitations of GRPO that render it inapplicable to the chart deep research domain, and successfully address them, as demonstrated in impressive empirical results.\n\n- In general, the paper was easy to follow. Multiple visual aids were provided to accompany the technical material. Although the dataset was not provided as part of the submission, the authors included sufficient details in the Appendix to give a sense of what the dataset looks like."}, "weaknesses": {"value": "- If the authors could verify that PRPO can be used in a model-agnostic manner by training models other than Qwen2.5 with PRPO, it would make the paper even stronger.\n\n- While PRPO greatly advances the optimization algorithm of GRPO, it still relies on the combination of accuracy + format reward. Do the authors anticipate that PRPO could benefit further from more refined reward shaping? For instance, what would happen if we add more vision-centric rewards, such as those proposed in [1,2] to PRPO? (Considering that this is also a multimodal task.)\n\n[1] Visual-RFT: Visual Reinforcement Fine-Tuning\n\n[2] VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model\n\n- Slight tweak on the above question, do the authors think that PRPO could be generalized to improve the performance on other multimodal tasks?\n\n- Considering that open-sourcing and reproducibility are highly prioritized in today's research landscape (and even more so in datasets & benchmarks papers), I would have appreciated it if the authors had made the benchmark accessible as part of submission (either as an anonymized link or supplementary materials)."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MvQyqcmqA5", "forum": "cRJNk4bhZi", "replyto": "cRJNk4bhZi", "signatures": ["ICLR.cc/2026/Conference/Submission3601/Reviewer_BU6V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3601/Reviewer_BU6V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3601/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173100099, "cdate": 1762173100099, "tmdate": 1762916859572, "mdate": 1762916859572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}