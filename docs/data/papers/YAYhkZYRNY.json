{"id": "YAYhkZYRNY", "number": 20854, "cdate": 1758310969000, "mdate": 1763648770260, "content": {"title": "My Answer Is NOT 'Fair': Mitigating Gender and Race Bias in Vision-Language Models via Fair and Biased Residuals", "abstract": "Gender and race bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield such bias in generative responses. In this study, we focus on evaluating and mitigating gender and race bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.", "tldr": "", "keywords": ["Large Vision-Language Model", "Gender and Race Bias"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e36fc65d323b11bab467ac4d57fc81c81b540c1f.pdf", "supplementary_material": "/attachment/c3da498a9f5bf33b84048b1e490add46defff318.zip"}, "replies": [{"content": {"summary": {"value": "The authors investigate and reveal gender and race biases in the responses of 3 SOTA VLMs by using the MCS method to analyze their outputs and by inspecting their internal layers, measuring the confidence level of each token in the response while proposing a new post-hoc\nmethod to mitigate this issue that can be applied during the inference phase, where the method computes the mean of the residual bias vectors and fair vectors and then makes an orthogonal projection that is used as the new representation vector.\n\nThe paper appears to not have been proofread."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Post-hoc mitigation method is quite good and makes sense, besides of being simple to understand, apply or even adapt it. Opening the layers and investigate the confidence level to show that sometimes the model may seem fair when in reality the layers show they are not is simple and yet necessary to understand the theme.\nHowever I cant find enough contribution and the paper is poorly proofread"}, "weaknesses": {"value": "Although you explain that PAIRS does not provide race data, which you frame as a limitation, the title highlights this issue, while most of the text focuses much more on gender bias than on racial bias. I also had the impression that the main references in the introduction do not vary much in terms of methodological approaches or datasets. Finally, and no less importantly, I understand that the authors aim to help mitigate these issues, but I found it interesting that there was no mention that the input data the models are trained on is the root cause. This could have been explicitly connected to the proposed mitigation approach at inference time.\n\nThis paper is clearly not ready for submission to ICLR. I recomend that the authors perform an extensive proofreading, review the template and consider another venue for submiting their work."}, "questions": {"value": "Why repeat the word gender on the title?\nWhy not post the code in an anonymization platform such as Anonymous GitHub - 4open.science?\nWhat is the main contribution of the paper?\nWhat is the novelty in your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SyMQoA37UD", "forum": "YAYhkZYRNY", "replyto": "YAYhkZYRNY", "signatures": ["ICLR.cc/2026/Conference/Submission20854/Reviewer_1cDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20854/Reviewer_1cDd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744313484, "cdate": 1761744313484, "tmdate": 1762999991251, "mdate": 1762999991251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of gender and racial biases in the predictions of VLMs. There are several presentation issues that make this paper not ready for peer review."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "None."}, "weaknesses": {"value": "This paper does not appear to be ready for review. \n\n* Each page is only ~46 lines instead of ~55. Something went wrong with the stylesheet.\n* The abstract starts with an uncaptialized letter\n* The title contains a repeated n-gram: \"Mitigating Gender and Gender and Race Bias\"\n* The paper seems to only uses one style of citation \\cite instead of \\citep, which leads to a very unnatural visual style."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GOk2vdrQDr", "forum": "YAYhkZYRNY", "replyto": "YAYhkZYRNY", "signatures": ["ICLR.cc/2026/Conference/Submission20854/Reviewer_7SeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20854/Reviewer_7SeF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907384690, "cdate": 1761907384690, "tmdate": 1762999990526, "mdate": 1762999990526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies gender and race biases in generative VLMs through both models’ responses and confidence levels. The authors use multiple choice selection (MCS) to evaluate four open models and show they present biases in two datasets (PAIRS and SCF). In particular, the authors show that bias fluctuates considerably throughout the hidden layers. They then propose two methods to mitigate gender and race bias, which reduces biases in the base models.\n\nWhile the proposed methods seem promising, I believe the paper needs some writing improvements as well as comparisons with existing debiasing approaches before being published."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A study of race and gender bias in generative VLMs through the lens of confidence levels\n2. Two methods proposed to mitigate bias in these VLMs that reduce bias in the base models\n3.  An interesting analysis of the bias level across different layers."}, "weaknesses": {"value": "1. The Introduction section could be improved through better contextualization of the work and task setup, and how it differs from the existing setups. Topics like the ones in L66 and in L73-74 could be further expanded to help the reader better understand.\n2. A lot of space is used to describe system prompts, which are also claimed to be one of the main contributions of the paper by the authors. In my opinion, while important, the system prompts are not substantially different from existing prompts, and could simply be reported in Appendix, leaving space for a more thorough discussion of the task and setup.\n3. In L139-140, it would be great to have an intuition of how / why humans corrected the labels of PAIRS, and whether this is a process that the community should adopt.\n4. The description of the post-hoc mitigation method (Section 3.1) could be improved. For instance, the second subscript of delta moves from the residual number (1 or 2) to the fairness label (fair or biased), which is confusing when first reading. The assumption behind the approach (defined in Appendix C) is also very useful in my opinion, and it should be part of the main paper. The same applies to Figure 6.\n5. The experiments show how the proposed approaches improve bias issues of the base models, but there is no comparison with other bias mitigation methods (such as those discussed in Section 6) that would help the community understand which approach is more promising."}, "questions": {"value": "1. Several typos in the paper: “Gender and” repeated in title; “gender” not capitalized in abstract and L382; abbreviations like “there’s” in L211; missing links to Appendices in L284-325-373; character in Fairness equation (L253); most citations don’t include a surrounding parenthesis (\\cite instead of \\citep).\n2. The paper format is different from the ICLR template, which could be ground for desk rejection.\n3. The plots in Figure 4 are too small. The authors should at least remove the redundant information in the y-axis of plots b, c and d.\n4. Why do you sample one template at random for each concept (L145) rather than evaluating models on 10 templates and averaging the results for robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Iw1BrbNpS8", "forum": "YAYhkZYRNY", "replyto": "YAYhkZYRNY", "signatures": ["ICLR.cc/2026/Conference/Submission20854/Reviewer_x1U1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20854/Reviewer_x1U1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047751621, "cdate": 1762047751621, "tmdate": 1762999990534, "mdate": 1762999990534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response: Format and typo corrected"}, "comment": {"value": "The mentioned tiny format and typo issues have been fixed in the latest PDF file and now reviewers may not overstate the weakness of the these issues while ignoring the technical contributions which have been greatly appreciated by previous reviewers during previous top machine learning conference."}}, "id": "C66qthHWRg", "forum": "YAYhkZYRNY", "replyto": "YAYhkZYRNY", "signatures": ["ICLR.cc/2026/Conference/Submission20854/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20854/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20854/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763649040218, "cdate": 1763649040218, "tmdate": 1763649040218, "mdate": 1763649040218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}