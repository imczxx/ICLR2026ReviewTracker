{"id": "o4dTaxZ1S9", "number": 17181, "cdate": 1758273157588, "mdate": 1759897192191, "content": {"title": "A Multi-domain Benchmark for Machine Unlearning in Classification Tasks", "abstract": "Machine unlearning (MU), the process of removing specific data influences from trained machine learning models, is critical for regulatory compliance (e.g., GDPR’s right to be forgotten) and for addressing copyright and privacy concerns in large-scale models. While a wide range of methods and metrics have been proposed, systematic evaluations remain fragmented, typically limited in scope by modality, metric coverage, or the number of methods considered. In this work, we present the most comprehensive MU benchmark to date, evaluating 12 unlearning methods on 8 datasets and models across four modalities (images, text, tabular data, and graphs) by assessing the three key aspects of an unlearning outcome: utility -- the overall performance of the model after unlearning -- efficacy -- how well the data is forgotten  -- and efficiency -- the computational cost of unlearning. We also introduce LUMA (Laplacian Unlearning Multidimensional Assessment), a unified metric that consolidates them into a single score. Unlike prior metrics, LUMA can flexibly incorporate multiple measures within each dimension (e.g., F1 over test and forget set for utility, UMIA for efficacy, runtime and GPU memory for efficiency), enabling more accurate and extensible comparisons. Our code is reproducible and extensible to serve as a benchmark for MU research.", "tldr": "Largest Machine Unlearning (MU) benchmark for classification to date; introduction of a new unified MU metric.", "keywords": ["Machine Unlearning", "Machine Learning", "Benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b27ced975bb3b2a48d783b7105f547614669f46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a multi-domain benchmark for machine unlearning, covering four modalities: image, text, tabular, and graph data, with two datasets per domain. The benchmark evaluates 12 existing unlearning methods. The authors introduce LUMA, a unified metric that combines utility, efficacy, and efficiency using a harmonic mean with Laplace kernel weighting, aiming to provide a balanced quantitative evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The inclusion of image, text, tabular, and graph domains provides a broad empirical base for comparing unlearning algorithms beyond the typical image-only or text-only setting. \n- Evaluating 12 well-known unlearning algorithms under a unified framework provides useful practical insights into their relative performance and trade-offs. \n- Although simple, the effort to jointly quantify utility, efficacy, and efficiency highlights an important issue in unlearning evaluation—balancing these often conflicting objectives. \n- The benchmark may lower entry barriers for researchers entering the unlearning field, potentially serving as a baseline for cross-domain comparison."}, "weaknesses": {"value": "- The benchmark mainly aggregates existing datasets, models, and methods without introducing new algorithms or theoretical insights. \n- CIFAR-100 and CelebA are legacy datasets; more recent benchmarks (ImageNet subsets, LAION, COCO, etc.) would be preferable. \n- IMDB and AG News are outdated; newer benchmarks like TOFU or RWKU should be used. \n- Relying on ResNet and BERT is no longer representative, recent unlearning works commonly use ViT or LLaMA-based backbones. \n- The paper’s title implies class-level unlearning, but it is unclear whether the experiments actually target class-unlearning or instance-level unlearning. The distinction is important, as the forgetting objective and evaluation criteria differ substantially between the two.\n- LUMA is a simple harmonic mean of three normalized metrics with Laplace kernels, lacking clear theoretical grounding or interpretability. Moreover, it is unclear why a unified metric like LUMA is even necessary, since utility, efficacy, and efficiency inherently capture different and sometimes orthogonal aspects of unlearning performance."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jDsejyGUFl", "forum": "o4dTaxZ1S9", "replyto": "o4dTaxZ1S9", "signatures": ["ICLR.cc/2026/Conference/Submission17181/Reviewer_hDR4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17181/Reviewer_hDR4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810162672, "cdate": 1761810162672, "tmdate": 1762927160194, "mdate": 1762927160194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comprehensive MU benchmark across 4 modalities (image, text, tabular, graph), evaluating 12 methods on 8 datasets. The authors introduce LUMA, a unified metric combining utility, efficacy, and efficiency. While this represents the most extensive MU benchmark to date and fills important gaps in tabular and graph domains, several methodological concerns limit its practical impact."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first comprehensive multi-domain benchmark that systematically covers previously neglected tabular and graph domains, which is a significant contribution to the field.\n2. The proposed taxonomy organizing methods into 4 categories (Fine-Tuning, Selective Weight Modification, Distillation, and Weight Importance) provides a useful framework for understanding the MU landscape."}, "weaknesses": {"value": "1. The paper directly applies general MU methods to specialized domains without acknowledging that domain-specific unlearning approaches exist. For instance, graph unlearning methods like GraphEraser are designed to handle structural dependencies that generic methods cannot address effectively, which limits the practical relevance of findings in these domains.\n2. The choice of 20% as the forget set size lacks proper justification, and the claimed consistency with \"prior MU literature\" needs specific citations. In privacy-focused scenarios, forget sets are typically much smaller (single-digit percentages or individual samples), which significantly impacts the difficulty and practical relevance of the unlearning task.\n3. Using a 1-layer GCN as one of the graph models is problematic since single-layer GNNs cannot effectively capture multi-hop neighborhood information fundamental to GNN performance. The literature widely recognizes that GNNs require at least 2-3 layers to demonstrate their advantages, which undermines the validity of graph domain conclusions.\n4. The benchmark omits influence function-based unlearning methods, which represent a theoretically principled and practically important approach in the MU literature. This is a notable gap given the paper's claim of comprehensive coverage.\n5. The efficiency measurement using peak GPU memory is misleading because it does not reflect actual computational cost. Methods like retraining may show lower peak memory but consume more resources over time due to longer execution, making this metric an inaccurate representation of true efficiency.\n6. Despite LUMA's technical sophistication, reducing unlearning performance to a single score has fundamental limitations. Different application scenarios prioritize different objectives: privacy-critical applications demand maximum efficacy regardless of cost, while production systems may prioritize efficiency. A single aggregate score cannot capture these trade-offs adequately and may mislead practitioners about which method suits their specific needs."}, "questions": {"value": "s use 20% forget sets to justify this choice? How do results change with more realistic proportions like 1%, 5%, or 10% that are common in privacy scenarios?\n2. Can you justify using a 1-layer GCN given that such architectures cannot leverage GNN advantages? What is the performance gap between 1-layer and 2-layer GCNs on these datasets before unlearning?\n3. Have you considered more meaningful efficiency metrics like memory-time product or energy consumption instead of peak memory? How would these affect method rankings?\n4. Rather than a single LUMA score, have you considered presenting Pareto frontiers showing trade-offs between utility, efficacy, and efficiency? This would better serve practitioners with diverse requirements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hHelUkQb6T", "forum": "o4dTaxZ1S9", "replyto": "o4dTaxZ1S9", "signatures": ["ICLR.cc/2026/Conference/Submission17181/Reviewer_7daK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17181/Reviewer_7daK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961517379, "cdate": 1761961517379, "tmdate": 1762927159401, "mdate": 1762927159401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a comprehensive studies of the unlearning approximation algorithms on different data types based on the comprehensive evaluation of unlearning metrics. The paper introduces a Laplacian Unlearning Multidimensional Assessment,\na unified metric that consolidates them into a single score."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "the comprehensive study of unlearning approximation algorithms on different domains and data types all in one place."}, "weaknesses": {"value": "I appriciate the authors' hard work to investigate MU methods from different perspective and providing a compreheisve study on the different domains and models, but I think there are still so much room to work on the contribution of the paper for this research domain. \n  \n\nLine 36 // \"Machine unlearning .... \": exact Machine unlearning is retraining from scratch -  Approximate machine unlearning is the response to address the retraining from scratch.\n\nline 50 //\"\"training appropriate model ... \"\": ambiguous, bad grammar.\n\n\nline 76 // It is standard and common practice in the domain of MU that these aspects of model would be evaluated with different metrics."}, "questions": {"value": "line 50 // \"Applying unlearning methods ... \" : Could you justify this point? Since,  many of the unlearning methods are domain independent and don't need to make any adjustments.\n\nLine 100 // Understanding and evaluating the unlearning approximation method on different domains may not be investigated simultaneously in a single work, but the algorithms such as Gradient Ascent or NegGrad have been well investigated in many domains. from vision to graph neural networks [1,2]. So the question is what understanding we can achieve from evaluating them together in a single research?\n\n\nline 106 // \"the incorporation of ...\": Why we want to incorporate the evaluation metrics of MU in one single metric? What would be the advantage of that? Don't we want to investigate and gain more knowledge about how a method perform from different aspect rather than providing a single score?\n\n\n\n[1]Cheng, Jiali, et al. \"Gnndelete: A general strategy for unlearning in graph neural networks.\" arXiv preprint arXiv:2302.13406 (2023).\n\n[2]Jia, Jinghan, et al. \"Model sparsity can simplify machine unlearning.\" Advances in Neural Information Processing Systems 36 (2023): 51584-51605."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUQHoJ3TjZ", "forum": "o4dTaxZ1S9", "replyto": "o4dTaxZ1S9", "signatures": ["ICLR.cc/2026/Conference/Submission17181/Reviewer_MarZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17181/Reviewer_MarZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977591455, "cdate": 1761977591455, "tmdate": 1762927158915, "mdate": 1762927158915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an extensive benchmark and survey of machine unlearning methods across multiple data modalities, including images, text, tabular, and graph data. It proposes a unified metric, LUMA, to jointly evaluate unlearning methods along the axes of utility, efficacy, and efficiency, and systematically compares representative unlearning techniques across eight datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark systematically evaluates unlearning algorithms not only on standard image datasets (e.g., CIFAR-100, CelebA) but also on tabular and  graph data\n- The benchmark provides valuable takeaways on learning-rate sensitivity and model-size robustness across methods.\n- The analysis highlights which unlearners are stable or sensitive across hyperparameter scales and datasets."}, "weaknesses": {"value": "- By forcing a single evaluation protocol across all data types, the benchmark focuses only on methods that can operate on all domains, which excludes many strong domain-specific unlearners (e.g., GNN-specific or LLM-specific methods). This constraint, while necessary for consistency, inherently limits the scope of the conclusions.\n- The benchmark omits some  recent unlearning methods, in particular DELETE (CVPR 2025)\n- No  discussion is provided (neither in the related works) for LLM-based unlearning, despite recent literature such as “Rethinking Machine Unlearning for Large Language Models” (S. Liu et al., 2024). For graphs, only classification datasets are tested, and the paper omits graph-specific unlearning frameworks, such as:\nWu et al., Certified Edge Unlearning for Graph Neural Networks \nChen et al., Graph Unlearning \nSaid et al., A Survey of Graph Unlearning \n- While innovative, LUMA may obscure specific aspects of performance. Practitioners often need to see utility (model accuracy) and running time separately rather than as an aggregated score.\n- The figure visualizing learning-rate sensitivity (Figure 2) is dense and visually hard to parse. The takeaways described in the text (e.g., LR sensitivity and size invariance) are not immediately evident without extensive reading. Simplifying this visualization (e.g., separate panels per unlearner or clearer legends) would improve accessibility.\n\nMinor: \n- add requirements in the repo"}, "questions": {"value": "- In practice, how dominant are the efficiency components in the final LUMA score? \n- What motivated setting $\\gamma =3 $ for  $M_u$ and $M_e$ ? \n- Similarly, how were the weighting coefficients $w_i$ chosen in LUMA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UaUpW1Y23O", "forum": "o4dTaxZ1S9", "replyto": "o4dTaxZ1S9", "signatures": ["ICLR.cc/2026/Conference/Submission17181/Reviewer_jZd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17181/Reviewer_jZd3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17181/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998464211, "cdate": 1761998464211, "tmdate": 1762927158428, "mdate": 1762927158428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}