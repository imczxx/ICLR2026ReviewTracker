{"id": "IuscGSmfEf", "number": 24662, "cdate": 1758359089916, "mdate": 1759896756215, "content": {"title": "ContextIF: Enhancing Instruction-Following through Context Reward", "abstract": "While supervised fine-tuning (SFT) and preference learning (PL) are widely used to enhance the instruction-following ability of large language models (LLMs), they often struggle to generalize to novel or complex instructions and may compromise the models' general capabilities. In-context learning (ICL) emerges as a promising alternative due to its strong generalization without modifying the model's parameters, but its effectiveness is constrained by the reliance on high-quality, manually curated demonstration pools. To overcome this limitation, we propose ContextIF, a reinforcement learning (RL) framework for automatic context generation. Guided by comprehensive context reward, ContextIF is optimized by Group Relative Policy Optimization (GRPO). It aims to generate precise constraint summaries and optimal context demonstrations tailored to given instructions, thereby improving the instruction-following performance of target LLMs. We evaluate ContextIF on multiple representative instruction-following benchmarks using popular open-source LLMs. Experimental results demonstrate that ContextIF achieves substantial performance gains over existing SFT and ICL methods, while also generalizing effectively to unseen constraint conditions. Moreover, ContextIF preserves the parameters and general capabilities of the target models, offering strong adaptability and scalability. The code is provided in the Supplementary Materials.", "tldr": "Constructing High-Quality Contexts via Reinforcement Learning with Format and Contex Rewards to Enhance Instruction-Following Ability of Large Language Models", "keywords": ["Large Language Models", "Instruction-Following", "Reinforcement Learning", "In-context Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729a9f42d75e52fa03f6539b3f8d32680f90468b.pdf", "supplementary_material": "/attachment/8d9034689f68b3d6ffb23c24fa0cd0cb0553bde2.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents **ContextIF**, a reinforcement learning framework that enables LLMs to **dynamically generate high-quality, task-specific contexts** for instruction following.  \nUnlike static SFT or ICL methods, ContextIF trains a **context generator** guided by a **multi-dimensional Context Reward** and optimized with **GRPO**.  \nExperiments show that an **8B model** trained with ContextIF can **match or surpass** much larger models (e.g., 70B, GPT-4o) across benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel and effective framework:**  \n  Integrates RL with ICL for dynamic context generation, effectively addressing the static nature of traditional fine-tuning or ICL.  \n- **Comprehensive experiments:**  \n  Strong performance and generalization results with clear analysis on catastrophic forgetting and parameter efficiency."}, "weaknesses": {"value": "1. **Motivation:**  \n   The *Introduction* mentions “existing studies” on automatic context generation but lacks citations and fails to clarify how **ContextIF** differs from prior methods.\n\n2. **Contribution clarity:**  \n   The main novelty lies in the **Constraint Reward**, but the paper lacks theoretical justification for the `<constraint>`, `<question>`, `<answer>` structure and the reward components r_summary, r_demoq, r_demoa.\n\n3. **Experimental issue:**  \n   In **Table 1**, **P(S)** and **I(S)** are highlighted even though they underperform **SPAR-8B**, which seems inconsistent.\n\n4. **Other concerns:**  \n   - The generator’s performance ceiling depends on the **judge model**, possibly transferring its biases.  \n   - Using a **70B model** as a reward provider for training an **8B model** is **computationally expensive** and limits scalability."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jGU6A7aKFX", "forum": "IuscGSmfEf", "replyto": "IuscGSmfEf", "signatures": ["ICLR.cc/2026/Conference/Submission24662/Reviewer_KBRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24662/Reviewer_KBRG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760588687741, "cdate": 1760588687741, "tmdate": 1762943153644, "mdate": 1762943153644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes **ContextIF**, a reinforcement learning framework to automate and optimize context generation for in-context learning (ICL) in instruction-following tasks. The core idea is to train a policy model to act as a context generator that, given a user query, produces a self-contained ICL demonstration consisting of: (1) a precise constraint summary and (2) a corresponding question-answer pair. The policy is optimized using Group Relative Policy Optimization (GRPO) guided by a composite context reward that evaluates both structural correctness (format reward) and semantic quality (constraint reward). \n\nThe experiment framework is based on several instruction-following benchmarks. And the proposed method ranks the top."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method can have strong generalization to unseen constraint types (e.g., Language, Keywords, Length), which is a critical advantage over supervised methods.\n2. ContextIF maintains and even enhances the base model's performance on general benchmarks, because it does not change the parameters of the Actor Model."}, "weaknesses": {"value": "1. **Lack of clarity on model and training details:** Which model serves as the Policy Model? The paper does not explicitly state which model is used as the policy (e.g., is it initialized from Llama-3-8B-Instruct, a separate smaller model, or the same base model?).\n\n2. **Decoupling of Policy and Actor models is not well justified:** The paper trains the Policy Model to generate context but keeps the Actor Model (the target LLM) frozen. While this avoids catastrophic forgetting, it raises questions:\n\t- Why not directly train the Generator Model using the same context reward based on the GRPO?\n\t- What if the policy and actor are the same model? Would end-to-end training be more effective?"}, "questions": {"value": "same as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MzXnPqG34o", "forum": "IuscGSmfEf", "replyto": "IuscGSmfEf", "signatures": ["ICLR.cc/2026/Conference/Submission24662/Reviewer_EDdA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24662/Reviewer_EDdA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760597774540, "cdate": 1760597774540, "tmdate": 1762943153239, "mdate": 1762943153239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ContextIF, an RL framework that generates in-context demonstrations automatically by a RL (GRPO) trained model. The reward combines a format check (structure-only) and a constraint reward scored by a judge model over three facets (summary, question parallelism, answer faithfulness). The authors compare their proposed method ContextIF with various baselines and ICL methods, which show superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear improvement over baselines (ICL, base, and instruction-tuned). Across IFEval, Multi-IF, FollowBench, and LiveBench, ContextIF-8B outperforms LLaMA3-8B-Instruct and some previous SFT/DPO systems.\n\n- Beats alternative ICL strategies, including GPT-4o-generated contexts. In the head-to-head ICL comparison, ContextIF-8B tops zero-shot, random/select-context, LLM-context, tuned-LLM-context, and GPT-4o-context. \n\n- Proposed methodology is simple and straightforward."}, "weaknesses": {"value": "- **Limited novelty.** The main move is to train a *policy* that emits a `<constraint>/<question>/<answer>` **XML** block and optimize it with a composite **format + constraint** reward, trained via **GRPO**. All ingredients are established yet this looks like a careful engineering bundle rather than a conceptual jump. (i.e. RLVR is known to work well, and this seems like just applying that for auto-ICL model). Could you please clarify what is *principally novel* in this work?\n\n- **Task narrowness & scalability.** Evaluation targets **instruction-following** only (IFEval, Multi-IF, FollowBench, LiveBench *instruction-following subset*), with rewards tailored to *definitive, machine-/judge-verifiable* constraints leaving open whether this scales to harder, less judgeable tasks. One example could be *execution- or test-based** reward channels (e.g., unit tests for code, tool outcomes) or that require reasoning or planning efforts. The tasks tested here seem to be too simple compared to benchmarks used for evaluating comparatively recent open-source model performances.\n\n- **ICL vs. training the model directly; cost/benefit unclear.** The paper motivates ICL to avoid parameter updates, yet the approach **does** train a policy with RL (GRPO) and adds **inference overhead** (generating the XML context) before answering. There is no compute/latency accounting (rollouts per query, group size, wall-clock, token overhead), so it’s hard to judge whether one should instead train the *task model* with SFT/RLHF/GRPO to answer directly, skipping auto-ICL, especially considering the **narrow coverage of the nature of the task**. Please report training/inference costs and include a **compute-matched** baseline where a model is RL-trained to *answer* (using the same judge) rather than to *generate demonstrations*."}, "questions": {"value": "1. Please directly address the points raised in the weakness section.\n\n2. What motivated choosing Llama-3-8B and Mistral-7B-Instruct? i.e. Can you report results on more recent open-source models and compare trends?\n\n3. Given the narrow task scope (i.e. definite instruction following), why train an in-context generator instead of training the answerer directly under a matched compute budget? Could you include compute-controlled comparisons and token/latency overheads?\n\n4. Why did you use GPT-4o for the demonstration comparison , instead of stronger contemporaries (e.g., GPT-4.1 or GPT-5)? If feasible, add those results or justify the choice.\n\n5. I might have missed this in the paper, but couldn't find the details - which model did you use for reward model during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "awtMSixMMY", "forum": "IuscGSmfEf", "replyto": "IuscGSmfEf", "signatures": ["ICLR.cc/2026/Conference/Submission24662/Reviewer_vWMh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24662/Reviewer_vWMh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676052552, "cdate": 1761676052552, "tmdate": 1762943152454, "mdate": 1762943152454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ContextIF, a reinforcement learning framework that trains a policy model to generate, per query, an XML-structured in-context “context block” consisting of a constraint summary and a parallel QA demonstration, optimized with a composite “context reward” combining a binary format check and a judge-model-based constraint reward under GRPO, to improve instruction-following without modifying the target LLM’s parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem framing: static retrieval or manual pool curation limits ICL for complex, compositional constraints; the proposed generator produces per-query contextual demonstrations with explicit structure and a verifiable reward, which is straightforward to implement and evaluate.​\n- Method simplicity with practical stabilization: GRPO with groupwise normalized advantages, a binary format reward, and a discrete, modular constraint reward decomposed into summary, demo-question, and demo-answer checks using a strong judge model, makes the pipeline reproducible and extensible.​\n- Empirical breadth: strong improvements on IFEval/Multi-IF/FollowBench/LiveBench with LLaMA3-8B and Mistral-7B, comparisons to SFT/DPO pipelines (Conifer, UltraIF, SPAR, AutoIF), and a comparison against LLM-generated and GPT-4o-generated contexts; ablations suggest specific reward components drive most gains."}, "weaknesses": {"value": "- Reward dependence on judge models: the constraint reward relies on a large judge (e.g., Llama3-70B-Instruct), which risks overfitting to the judge’s preferences, raises circularity concerns, and complicates claims of model-agnostic gains; calibration of judge reliability and cross-judge robustness is not systematically analyzed.​\n- Limited theoretical grounding: there are no guarantees that optimizing the composite reward improves downstream adherence under distribution shifts or alternative scoring; the analysis is largely empirical and does not quantify variance, bias, or brittleness in the reward signal despite discrete, thresholded scoring.​\n- Baseline protocol fairness: several baselines are reproduced via the authors’ implementations; compute budgets, hyperparameter sweeps, and prompt formats may differ and could advantage ContextIF, especially vs. retrieval-based ICL with large pools or better rankers not exhaustively tuned here.​\n- Scope of generalization: while unseen constraint categories are tested within IFEval-style settings, broader robustness under different prompting regimes, safety constraints, and multilingual/multi-turn mixtures beyond the chosen benchmarks is not deeply evaluated, limiting the generality claims.​\n- Format coupling risk: strict XML compliance is rewarded; this may inadvertently steer models toward structural conformity rather than deeper semantic adherence, and could reduce transfer to settings without such formatting, which is not assessed with alternate structures or no-format variants.​\n- Cost and scalability details: end-to-end latency and memory profiles, especially with multiple rollouts per query and judge calls, are not characterized across larger batch sizes or longer inputs; comparisons with GPT-4o-context do not clarify cost parity or throughput under matched budgets.​\nPotential leakage and template bias: the generated demonstration is a “parallel” QA with similar constraints; the paper does not quantify whether distributional shortcuts or template echoing contribute to gains versus genuine constraint reasoning, nor analyze failure modes when demonstrations are adversarial or spurious.​\n\nSafety/ethics evaluation is minimal: while datasets are public and filtering is claimed, there is no targeted analysis for harmful content amplification or reward hacking where the policy learns to game the judge without improving real instruction compliance, especially in free-form domains.​"}, "questions": {"value": "- How robust are results to replacing the judge model with different families/sizes or rule-based verifiers, and does performance persist under cross-judge evaluation to mitigate reward overfitting concerns?​\n- What are the compute/latency costs per query relative to retrieval-based ICL and SFT pipelines, including rollout counts, judge inference, and GRPO updates, especially at scale for production settings?​\n- Can the method drop strict XML constraints without loss, or generalize to alternative schemas (JSON, unconstrained text) while retaining gains, and how sensitive are improvements to formatting choices?​\n- How does ContextIF perform under adversarial or misleading constraints, or when instruction sets intentionally include conflicting requirements; is there a failure analysis of generated context harming adherence?​\n- Could combining retrieval with generation (e.g., retrieved seed plus RL-edited context) outperform pure generation, and have such hybrids been tested under matched budgets and pools?​\n- For multi-turn settings, can the context generator adapt across turns with memory, and how does it interact with conversation state vs. single-turn re-generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ikHuVQxCGO", "forum": "IuscGSmfEf", "replyto": "IuscGSmfEf", "signatures": ["ICLR.cc/2026/Conference/Submission24662/Reviewer_EaXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24662/Reviewer_EaXT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990401818, "cdate": 1761990401818, "tmdate": 1762943151739, "mdate": 1762943151739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}