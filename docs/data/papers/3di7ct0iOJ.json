{"id": "3di7ct0iOJ", "number": 10067, "cdate": 1758159560584, "mdate": 1759897676526, "content": {"title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "abstract": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose \\textbf{IV-Bench}, the first comprehensive benchmark for evaluating \\emph{Image-Grounded Video Perception and Reasoning}. IV-Bench consists of 966 videos paired with 2,560 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. These findings collectively provide valuable insights for future research. Our codes and data are released in \\url{https://anonymous.4open.science/r/IV-Bench-A3F7}.", "tldr": "This paper introduces IV-Bench, the first comprehensive benchmark for evaluating Multimodal Large Language Models on image-grounded video perception and reasoning.", "keywords": ["Image-Grounded Video Perception and Reasoning", "Multimodal llms", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/89952ef228b50615b8ecbba26340b14ed020b1f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents IV-Bench, the first large-scale benchmark specifically designed to assess image-grounded video reasoning and understanding in multimodal large language models (MLLMs). IV-Bench comprises 966 videos paired with 2,560 manually annotated image–text queries, covering 13 tasks across 5 diverse domains. Through an extensive evaluation of state-of-the-art MLLMs, the study finds that current models achieve no more than 28.9% accuracy, revealing substantial performance gaps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an important and previously overlooked problem—image-grounded video understanding—by establishing the first comprehensive benchmark in this area, thereby filling a crucial gap in existing multimodal evaluation benchmarks.\n\n2. The paper highlights the limitations of current MLLMs in handling such tasks, offering clear guidance and future directions for advancing MLLM capabilities."}, "weaknesses": {"value": "The analyses in the paper are not sufficiently deep or insightful. For example, the reported phenomenon of \"moderate performance gains with larger models\" lacks both theoretical grounding and concrete case studies. Assertions such as \"increasing model size primarily enhances memorization and shallow pattern recognition rather than reasoning ability\" are not adequately supported by evidence. Similar shortcomings appear throughout other analytical sections. Moreover, the paper fails to propose practical solutions or methodological advances to address the identified challenges."}, "questions": {"value": "I have no questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qiFFtzu2M5", "forum": "3di7ct0iOJ", "replyto": "3di7ct0iOJ", "signatures": ["ICLR.cc/2026/Conference/Submission10067/Reviewer_Zjuc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10067/Reviewer_Zjuc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232726119, "cdate": 1761232726119, "tmdate": 1762921460977, "mdate": 1762921460977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark named IV-Bench that requires the models to perceive and reason the contents from a reference image which is not involved in the input video. The dataset is manually labored and elaborated 7 main findings from evaluating 28 MLLMs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The curation of the dataset does not rely on automated annotation tools (e.g., GPT-4V) and ensures high quality by manually annotations.\n- Assessing model capability to utilize reference images is a novel motivation. Also, excellent presentation with high overall readability and nice figures.\n- Impressively extensive evaluation (28 MLLMs) is reported, providing actionable diagnostics. Also, the performance gap between humans and models is huge."}, "weaknesses": {"value": "- The main concern is that the findings are not surprising compared to the quality of the dataset. Comparisons across the model capacity, video fps, and resolution are trivial according to the scaling law [1].\n- Although it is not necessary, the analysis lacks experiments on meta-data (e.g., subtitles).\n- All tasks are based on multi-choice questions, not requiring open-ended questions even for the “reasoning” category.\n- Inter-annotator agreement and quality-control rejection rates on both rounds are missing.\n- The paper rigorously describes the model performance by model families (e.g., line 349 to 360). I believe that reporting this with averaged gain by model scaling and adding more verifications and allocating more descriptions of author’s findings can make this paper much more stronger.\n\nI will increase the score once current concerns are addressed."}, "questions": {"value": "- How do RL-trained models perform on this benchmark? Are there any inductive bias observed from visual encoders? Are there any underlying biases or trends from the training set? These points are revealed that these two points matter [2,3].\n- Authors analyzed the order of video and image matters. Did the ordering of text inputs change performance?\n- I believe that analysis on attention score can somehow verify the finding in Section 4.3 (line 400). How are the attention scores distributed towards image tokens?\n- How can the evaluated models have forgetting issues (line 401) in the input sequence? Different from LSTM models that sequentially feed tokens to update hidden state, Transformers architecture processes the input tokens in parallel rather than sequentially. Isn’t the “forgetting” happens only when the input tokens are truncated according to the size of the context window? The size of the context window should be reported to make this claim concrete. In other words, how many previous tokens can the models condition on.\n\nReferences\n\n[1] Fang et al. MMBench-Video: A long-form multi-shot benchmark for holistic video understanding. NeurIPS D&B Track 2024.\n\n[2] Wang et al. VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models. arxiv:2406.16338\n\n[3] Li et al. Vidhalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding. CVPR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QR1vx4wy9G", "forum": "3di7ct0iOJ", "replyto": "3di7ct0iOJ", "signatures": ["ICLR.cc/2026/Conference/Submission10067/Reviewer_2qaq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10067/Reviewer_2qaq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718907878, "cdate": 1761718907878, "tmdate": 1762921460527, "mdate": 1762921460527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IV-Bench, the first comprehensive benchmark designed specifically for evaluating Multimodal Large Language Models (MLLMs) on image-grounded video perception and reasoning tasks. The benchmark comprises 966 videos and 2,560 meticulously annotated image-text queries, spanning 13 distinct tasks (7 perception and 6 reasoning) across 5 video categories. The authors conduct an extensive evaluation of 28 state-of-the-art MLLMs, revealing that even the best-performing model (Qwen2.5-VL-72B) achieves only 28.9% overall accuracy, which is significantly lower than human performance (88.8%). Furthermore, through ablation studies, the authors analyze the impact of factors such as the order of image input, number of video frames, and resolution on model performance, providing valuable insights for future model design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. IV-Bench is the first benchmark specifically dedicated to image-grounded video understanding tasks. It effectively addresses the limitation of existing video benchmarks that rely solely on text queries, thereby advancing the field of multimodal reasoning evaluation.\n2.The benchmark's high quality and validity are ensured through a two-round quality control process, the use of externally sourced images, and diverse task design. The inclusion of \"effective distractors\" is particularly noteworthy, as it forces models to rely on the image information.\n3. The systematic evaluation of 28 models thoroughly exposes the significant shortcomings of current MLLMs on this task. The ablation studies offer practical design recommendations, such as the finding that placing the image after the video frames yields better performance."}, "weaknesses": {"value": "1.Although covering multiple categories, the total of 966 videos is smaller than some existing video understanding benchmarks (e.g., Video-Bench with 5,917 videos), which might affect the benchmark's broad representativeness.\n2.The current work focuses only on the triplet input of image-text-video. It does not explore more complex multimodal grounding signals, such as audio, multiple images, or dynamic image sequences (e.g., GIFs).\n3.While emphasizing \"image-grounding,\" the benchmark may not fully account for whether a single static image is sufficient to represent dynamic changes in a video. Some tasks might still lean towards static matching rather than genuine spatio-temporal reasoning."}, "questions": {"value": "1.\tDuring the construction of IV-Bench, did you consider incorporating dynamic image sequences (e.g., GIFs or short video clips) as the grounding signal to better simulate real-world scenarios where users provide visual context?\n2.\tRegarding the particularly poor performance on tasks like \"Temporal Reasoning,\" have you conducted further analysis into the root causes? Is it related to the models' ability to understand long videos or their inherent temporal modeling mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tOeABMG4fU", "forum": "3di7ct0iOJ", "replyto": "3di7ct0iOJ", "signatures": ["ICLR.cc/2026/Conference/Submission10067/Reviewer_MRHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10067/Reviewer_MRHL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923673758, "cdate": 1761923673758, "tmdate": 1762921459724, "mdate": 1762921459724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IV-Bench is one of the first benchmark for image-grounded video perception and reasoning in MLLMs, featuring 966 videos and 2,560 external image-text queries across 13 diverse tasks. Unlike other benchmarks, all queries use external images as anchors, increasing real-world complexity. Evaluations on 28 leading MLLMs show performance is much lower than humans, especially in reasoning. The paper analyzes model size, reasoning methods, and token allocation, and both dataset and code are open-source."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This is one of the first systematic benchmark proposal for image-grounded video perception and reasoning, filling a notable gap in existing evaluation ecosystems.\n\n- The benchmark features high data quality and a robust evaluation framework. All images are sourced externally, preventing information leakage and accurately reflecting the difficulty of cross-source visual grounding in complex tasks such as search and retrieval, which strongly enhances the validity of the assessment.\n\n- Experimental analyses are thorough and the findings are insightful for follow-up research. The paper details the effects of image token order, scale, resolution, and frame count on model performance, and clearly points out the extremely limited reasoning capabilities of current MLLMs in image-grounded video scenarios. It also shows model scaling has minimal impact on reasoning, which offers clear direction for future advances."}, "weaknesses": {"value": "- Some tasks (e.g., Instruction Understanding, Summary) feature only weak connections to the visual “grounding,” where images act in a mainly auxiliary role and do not fully embody the core definition of image grounding.\n\n- The paper lacks granular error analysis. It reports only accuracy without dissecting which sub-tasks exhibit systematic failure (e.g., universal shortcomings in temporal reasoning or attribute change), and does not offer failure case examples. It is suggested to add error type breakdowns (e.g., missed visual cues, text comprehension errors, temporal confusion) and provide typical failed cases versus human labels in the appendix.\n\n- The discussion around the semantic gap and selection logic for external images should be strengthened. Although the necessity and realism of using external images is emphasized, there is no structured description or case analysis of their diversity/difficulty. A quantitative contrast between tasks with highly similar external images (near frame extraction) versus high-gap scenarios should be considered."}, "questions": {"value": "1. On subjective scoring consistency and ground-truth diversity: For reasoning tasks with subjective or diverse answers, how is human label consistency and answer distribution measured? How are multi-answer questions evaluated?\n\n2. On the irreplaceability of images: Has image replacement testing been conducted across all task categories? If an image is replaced by another with similar semantics but different visuals, does the answer change? If not, how can it be demonstrated that the image truly “grounds” the reasoning process?\n\n3. On prompt strategy: In Table 2, do all models use the optimal prompt order (image after video)? If not, have you considered rerunning the experiments for fairness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V7fU2Gh9RP", "forum": "3di7ct0iOJ", "replyto": "3di7ct0iOJ", "signatures": ["ICLR.cc/2026/Conference/Submission10067/Reviewer_AHVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10067/Reviewer_AHVG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929586750, "cdate": 1761929586750, "tmdate": 1762921459451, "mdate": 1762921459451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}