{"id": "NpXGcwpu48", "number": 20998, "cdate": 1758312552972, "mdate": 1759896947642, "content": {"title": "Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits", "abstract": "Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.", "tldr": "Classical Hopfield networks can generalize from a few graphs to memorize an entire isomorphism class by finding approximately invariant solutions.", "keywords": ["Implicit Bias", "Invariance", "Hopfield Networks"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18a352e76ba4f576544e20af81a27bf6afe21b96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a theoretical analysis of the Hopfield network (HN) on binary data. It begins by linking memorization via GD training to known results on minimum-norm implicit bias, and presents a generalization result valid for memorizable input domains. Then it numerically and theoretically shows the convergence of the model's parameters to invariant patterns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The theory is novel and the analysis seems solid."}, "weaknesses": {"value": "The HN model is somewhat outdated and the binary requirement on the input is quite restrictive.  Thus, it is unclear whether the results have implications for modern or practical settings.  \n\nI am not sure that the results/analysis can be extended to other models or non-binary data."}, "questions": {"value": "Memorization, implicit bias of GD, and data/network invariance/equivariance are well studied. Since HN is a special case of recurrent neural networks (RNNs), I wonder if there exist more general results on RNNs.\n\nSection 4 is somewhat hard to follow, though I must admit that graph theory is not my expertise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "16e2XBUKrn", "forum": "NpXGcwpu48", "replyto": "NpXGcwpu48", "signatures": ["ICLR.cc/2026/Conference/Submission20998/Reviewer_MyEX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20998/Reviewer_MyEX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862683544, "cdate": 1761862683544, "tmdate": 1762940017407, "mdate": 1762940017407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how classical Hopfield networks can learn graph isomorphism classes through implicit bias in learning dynamics. The authors prove that gradient descent on the Energy Flow (MEF) objective exhibits an implicit bias toward norm-efficient solutions that lie in a 3-dimensional invariant subspace, enabling generalization to entire graph orbits from polynomial-sized samples. Key theoretical contributions include characterizing the invariant parameter space (Lemma 4.2), connecting MEF to hard-margin SVMs (Theorem 3.1), and establishing polynomial sample complexity bounds O(n||θ*||²m/ε²) for orbit learning (Theorem 3.2). Experiments on small graphs (≤20 vertices) demonstrate the \"few-shot-to-orbit\" phenomenon across cliques, bipartite, and Paley graphs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Theoretical Framework\n  - First rigorous analysis connecting classical Hopfield networks to modern implicit bias theory\n  - Elegant characterization of invariant parameters as a 3-dimensional subspace regardless of graph size\n  - Creative reformulation of memorization as linear programming, bridging associative memory and SVM theory\n\n2. Strong Mathematical Foundations\n  - Rigorous proofs using group theory, convex optimization, and statistical learning theory\n  - Clear geometric intuition: successful networks converge to invariant subspace (Figure 3)\n  - Non-trivial extension of Hillar & Tran (2018) with concrete sample complexity bounds\n\n3. Fundamental Insights into Learning Dynamics\n  - Explains why networks generalize beyond training data through norm-efficiency bias\n  - Shows that group structure in data naturally induces invariance without explicit architectural constraints\n  - Connects classical neural network capacity with modern generalization theory"}, "weaknesses": {"value": "1. Severe Practical Limitations\n  - Restricted to classical Hopfield networks with linear capacity O(n) vs. exponential capacity of modern architectures\n  - Experiments limited to toy graphs (≤20 vertices) with questionable real-world relevance\n  - No comparison with modern graph neural networks or graph isomorphism algorithms that vastly outperform this approach\n\n2. Significant Theory-Practice Gaps\n  - Theorem 3.2 requires exponential iterations for convergence, but experiments use only 1000 iterations\n  - Sample complexity bounds have concerning dependencies on ||θ*|| that aren't well-controlled in practice\n  - Only simplified AHSVM surrogate has proven convergence to invariant subspace (Lemma 4.6), not the full HSVM/MEF\n\n3. Incomplete Technical Analysis\n  - No explanation for why different isomorphism classes require different sample complexities (Figure 2)\n  - Missing analysis of failure modes or cases where exponential samples might be required\n  - Implicit bias result heavily dependent on existing techniques (Soudry et al. 2018) with limited novelty\n\n4. Weak Experimental Validation\n  - Scale too small to validate practical utility or computational advantages\n  - Missing robustness analysis (noise, approximate isomorphisms, larger graphs)\n  - No systematic study of when the approach breaks down\n  - Cherry-picked graph families without principled selection criteria"}, "questions": {"value": "1. Scalability and Computational Complexity\n  - How does the approach perform on graphs with 100+ vertices where classical algorithms struggle?\n  - What is the actual computational cost compared to modern graph isomorphism algorithms?\n  - Can the invariant subspace characterization extend to modern architectures with better capacity?\n\n2. Theoretical Completeness\n  - Why is there a large gap between theoretical sample complexity bounds and experimental observations?\n  - Can you prove convergence to invariant subspace for the full MEF objective, not just AHSVM?\n  - Under what conditions might the approach require exponential samples?\n\n3. Broader Applicability\n  - Does the implicit bias mechanism extend to other symmetry groups beyond graph isomorphisms?\n  - Can similar principles be applied to modern architectures (transformers, GNNs) for better theoretical understanding?\n  - How does performance compare on real graph datasets (social networks, molecules, etc.)?\n\n4. Fundamental Limitations\n  - What graph properties make certain isomorphism classes harder to learn?\n  - Are there graph families where this approach fundamentally fails?\n  - How sensitive is the approach to hyperparameters and initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tBSqDSlziv", "forum": "NpXGcwpu48", "replyto": "NpXGcwpu48", "signatures": ["ICLR.cc/2026/Conference/Submission20998/Reviewer_PwVg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20998/Reviewer_PwVg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883855228, "cdate": 1761883855228, "tmdate": 1762940013315, "mdate": 1762940013315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why classical Hopfield networks can generalize from only a few examples when the data has symmetries, such as graph isomorphisms. It shows that during training, the network’s weights tend to move toward a small symmetric subspace, even without explicitly enforcing symmetry. The authors mathematically describe this symmetry space and prove that it is expressive enough to store whole families of equivalent graphs. They also connect this behavior to the implicit bias of gradient descent toward low-norm, max-margin solutions. Experiments on several graph families support the theory, showing few-shot generalization to entire orbits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good conceptual contribution to connect implicit bias -> min-norm solutions to emergent invariance under group actions for a classical architecture.\n\n- Theoretical framing looks solid, and consistent experiments that supportively show rapid orbit coverage and convergence of weights toward the invariant subspace.\n\n-  The results are stated for any isomorphism class, and three families of cliques, bipartite, and Paley graphs are shown to illustrate.\n\n- Public code is promised and the empirical setup is clear for reproducibility."}, "weaknesses": {"value": "- Payley graph -> Paley graph? This happens a couple of times in the paper / appendix.\n\n- Some of the limitations are already acknowledged by authors. The paper does not prove that HSVM/MEF solutions converge to the invariant subspace for the actual training problem—only for an averaged surrogate (AHSVM) and via empirical evidence. \n\n- Results focus on three families and relatively small graph sizes (v=8 and 20). It would help to include more and larger v and more orbit types."}, "questions": {"value": "- In Theorem 3.2 it is mentioned that MEF bound implies gradient descent may require exponentially many iterations and the authors hypothesize a tail phenomenon for it. Is that somehow validated by an experiment?\n\n- In Theorem 3.2 assuming strict memorization feasibility and sparsity cap of x. Can the assumptions be sometimes too strong for different types of orbits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fPOPQ49wyl", "forum": "NpXGcwpu48", "replyto": "NpXGcwpu48", "signatures": ["ICLR.cc/2026/Conference/Submission20998/Reviewer_wbq4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20998/Reviewer_wbq4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973126764, "cdate": 1761973126764, "tmdate": 1762940010624, "mdate": 1762940010624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies classic Hopfield networks and demonstrate their ability to infer the isomorphism class of graphs. The authors also find that Hopfield networks exhibit a bias towards small norm solutions, as well as solutions that are at least approximately invariant."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors study multiple classes of graphs, and they report multiple trials in their experiments.\n\nMEF is experimentally shown to require fewer samples to perfectly memorize the datasets."}, "weaknesses": {"value": "The manuscript is not very well written, and it is often hard to follow through. The text is often obfuscated with equations, the introduction reads more like a method section, and a conclusion is missing.\n\nThe significance of the problem studied is also not clearly stated in the manuscript."}, "questions": {"value": "1. Are the conclusions drawn from this work relevant for graph machine learning researchers?\n\n2. Are the results consistent across other classes of graphs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Wfcy6UzHBk", "forum": "NpXGcwpu48", "replyto": "NpXGcwpu48", "signatures": ["ICLR.cc/2026/Conference/Submission20998/Reviewer_ahT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20998/Reviewer_ahT5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977776617, "cdate": 1761977776617, "tmdate": 1762940009518, "mdate": 1762940009518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}