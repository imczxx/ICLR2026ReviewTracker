{"id": "dHNBlXCAZw", "number": 22225, "cdate": 1758327986805, "mdate": 1759896879162, "content": {"title": "LeIn-PINN: Learned Initialization to Alleviate Convergence Failures in Physics Informed Neural Networks", "abstract": "Physics-informed neural networks (PINNs) have had a broad research impact in modeling domains governed by partial differential equations (PDEs). However, PINNs often perform sub-optimally or even converge to trivial solutions in challenging scenarios, such as stiff PDE domains or when generalizing to unseen but related experimental contexts. Previous solutions to alleviate catastrophic PINN failures include curriculum-based training techniques and dynamic re-sampling of hard collocation points. These methods face pitfalls: designing a curriculum is ambiguous in multi-parameter PDEs, and dynamic resampling still fails in complex settings. Recent works also suggest that conflicting gradients during PINN training are a major cause of such failures.\n\nWe argue that weight initialization plays a crucial role in the emergence of catastrophic failures. To this end, we propose a novel training methodology based on Learned Initialization (LeIn) to address PINN failures. We call our variant LeIn-PINN. Through rigorous experiments on 1D and 2D PDEs, including challenging 2D fluid dynamics contexts, we show that LeIn-PINN outperforms state-of-the-art methods specifically designed to mitigate PINN failures. LeIn-PINN achieves an average performance improvement of 87% over baselines. We also provide a detailed analysis explaining the improved training dynamics of LeIn-PINN and the convergence failures of traditional PINNs by studying their loss landscapes. Finally, we demonstrate that LeIn-PINN significantly reduces spectral bias compared to traditional PINNs, even in challenging PDE domains.", "tldr": "", "keywords": ["Physics Informed Machine Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7725ea1b9f96686f76ed417c3247dae4248114cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the use of a learned NN initialisation to improve the PINN training convergence. This is done by meta-learning-like approach, with a gating mechanism to focus the learned initialisation to some parameters at a time. The results show improved generalisation accuracy compared to other methods that work on other aspects of PINN training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper attempts to solve the issue of PINN training convergence via the PINN initialisation. This viewpoint is not too common, which is good, although have been looked at in prior works already as well (see Weaknesses section).\n\nThe experiments section is organised well. There are quite clear research questions, and some of the points were answered well and provided evidence from experiments to back it up as well.\n\nExperiments do show promising results, such as for advection equation for high values of beta (which is a classic sanity check for PINN training)."}, "weaknesses": {"value": "Use of learned initialisation is not too well-motivated. While it may be true that changing initialisation can improve things, why can I not spend this effort into tuning other aspects of PINN training? I suspect there may be some motivation related to amortising the PINN training (there might be some reason that you have to train multiple PINNs on the same initialisation), which can make it a better case that the initialisation is the thing that we should try to fix. I think this part can strengthen the motivation of the problem by quite a bit.\n\nMeta-learning for PINNs and learning of some PINN initialisation are already studied in the context of PINNs [1-4], as cited by authors and as other papers have that were not mentioned in the paper. It would be good to more explicitly discuss the shortcomings of these existing works with respect to improved PINN training.\n\nRelated to above, more explicitly compare the proposed methods against these benchmarks would also be useful (particularly in some of the settings that are also used in those papers). The existing methods that LeIn compares with are quite orthogonal to proposed methods (in that I could use those methods alongside with LeIn as well), so more direct comparisons with more similar methods may be more informative. If the authors compare against with just these methods, then there are also other methods for improving PINN training convergence that the authors could have chosen to compare with (regarding collocation point selection, loss function balancing, PINN architecture, etc.) which are relatively newer than the benchmarks used. \n\nNone of the reported results have error bars.\n\nWhile the results show a sample of different PDE cases that trains from the learned initialisation, I feel it could be trained more extensively as well, in that a much denser set of PDE conditions could have been tried (e.g., training 50 different PDE parameters on the same initialisation for demonstration instead of just 6 different PDE parameters). This would strengthen the use of a learned initialisation.\n\nAround Line 122 -- what is consider relatively easier PDE configurations may not be trivial, and will be PDE-dependent. A more concrete definition for this (if exists) or some concrete criteria to determine this to be used in practice may be useful.\n\nAround Line 139 -- would like to see more motivating why the gating is needed. It seems to be done a bit, but would like to see it described more, especially connected to the results that the authors show later on regarding this part.\n\n---\n\n[1] Meta-learning pinn loss functions. 2022.\n\n[2] Meta-pde: Learning to solve pdes quickly without a mesh. 2022.\n\n[3] DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks. 2024. \n\n[4] PIED: Physics-Informed Experimental Design for Inverse Problems. 2025."}, "questions": {"value": "1. How is the time required to obtain the learned initialisations? Is it sufficiently more than the time required to just train the PINNs normally with the other proposed methods for a longer period of time? A similar question can be asked for other resource usages as well, including GPU memory for training.\n\n2. It might be interesting to see what the learned initialisation looks like after they are learned but before trained on the specific PINN case. This might provide with further understanding on why the convergence overall is better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XLYcvCjOJt", "forum": "dHNBlXCAZw", "replyto": "dHNBlXCAZw", "signatures": ["ICLR.cc/2026/Conference/Submission22225/Reviewer_26a7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22225/Reviewer_26a7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550319615, "cdate": 1761550319615, "tmdate": 1762942123618, "mdate": 1762942123618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the convergence and generalization of PINN in challenging scenarios. A novel PINN training methodology is proposed that 1) the model parameters are initialized by easy PDE settings and 2) a gated layer-wise optimization procedure mitigates spectral bias. Experiments have been conducted on different types of PDEs compared to various baseline models to present the improved performance on extrapolation regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The generalization of PINN on unseen PDE domains is critical, and the idea of better initialization for PINNs is important.\n- The methodology is well-structured.\n- The experiments were extensive on various PDE domains with a detailed explanation."}, "weaknesses": {"value": "- Some details in the methodology need to be justified.\n- Additional experiments are suggested to better present the effectiveness of the proposed method."}, "questions": {"value": "- Section 3: The bi-level optimization in the proposed method is heavily based on model-agnostic meta-learning, with the gated layer-wise optimization implemented in the outer loop. However, why the GLO improves the “hard” settings is not clear, especially since it is not applied separately to the physics-based loss and the data-driven loss. Could the authors justify how the GLO addresses the non-trivial dynamics of different loss terms?\n- Section 4.1 & 4.2: The comparison of the proposed method with baseline models is unfair. The proposed model has easy settings to obtain a better initialization, while the other models are only trained and tested on hard settings. It would be more convincing to compare the baseline models trained on the same easy settings used in the proposed model and fine-tuned on hard settings.\n- Table 1 - Table 3: Could the authors provide the variance of all metric numbers? The authors should provide visualizations to justify the quantitative improvements.\n- Section 4.3: The ablation study is not sufficient. The design of GLO, including the number of layers and its effect on particular loss terms, should be studied."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZUMVFtQLUt", "forum": "dHNBlXCAZw", "replyto": "dHNBlXCAZw", "signatures": ["ICLR.cc/2026/Conference/Submission22225/Reviewer_SMmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22225/Reviewer_SMmg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945842853, "cdate": 1761945842853, "tmdate": 1762942123386, "mdate": 1762942123386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a novel Physics-informed Neural Network (PINN) training methodology called Learned Initialization (LeIn), specifically designed to mitigate frequent catastrophic convergence failures in challenging Partial Differential Equation (PDE) domains. The paper rigorously explores the critical effect of learned weight initialization on PINN training dynamics, and they introduced Invariance Encoding and Gated Layer-wise Optimization to address convergence failures across diverse PINN-based PDE problems. The authors conducted both quantitative and qualitative experiments, to address three core questions: the role of learned initialization in mitigating catastrophic training failures, the performance comparison against state-of-the-art (SoTA) approaches, and the individual effect of invariance encoding and gated layer-wise optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is simple but powerful, effectively overcoming the common convergence failures of PINNs when applied to hard or challenging PDE equations. By distilling invariant physics into the initial weights, LeIn provides a superior starting point that stabilizes the optimization process.\n\nThe rigorous experiments across 1D and 2D PDE domains, demonstrate that LeIn-PINNs significantly outperform state-of-the-art approaches.\n\nThe work includes ablation studies to isolate and confirm the advantage of each proposed component, namely Invariance Encoding and Gated Layer-wise Optimization. Furthermore, the paper provides a detailed Hessian Eigenvector Landscape analysis and evidence of reduced spectral bias to support the strength of the proposed method."}, "weaknesses": {"value": "1. Missing PDE Background in Introduction: The Introduction and abstract introduce the problems (Convection, Helmholtz, Navier-Stokes) without providing explanation. Concepts like transport phenomena (1D convection) and wave propagation (2D Helmholtz) are only briefly mentioned in the Results & Discussion section. To enhance reader comprehension, it is better to include the Introduction a concise, clear explanation of these PDEs.\n\n2. Incomplete Related Work: While the paper references Wang et al., 2022a, the comparative analysis in the Method section relies on the methodology presented in Wang et al., 2021a. Introducing and briefly explaining the 2021a in the Related Work section will allow readers to understand the comparison baseline better.\n\n3. Ambiguous Terminology and Definition: The consistent use of subjective terms like \"stiff,\" \"challenging,\" and \"hard\" to describe PDE configurations throughout the paper lacks rigorous definition. Providing a clear, quantifiable criterion or metric for distinguishing between \"easy\" and \"hard\" PDE problems is necessary.\n\n4. Equation and Abbreviation Clarification:\n- The full name for the abbreviation MAML (Model-Agnostic Meta-Learning) should be provided upon its first mention in the Related Work section.\n- In the Problem Formulation section, the explanation of the lower letter b within Equation (1) is required for complete mathematical clarity.\n\n5. Lack of Specificity: In the Problem Formulation section, the Invariance Encoding component relies on a \"set of relatively easier configurations\". The criteria and mechanism for generating or selecting this set are not elaborated. \n\n6. Insufficiency in Claiming Novelty: If the Gated Layer-wise Optimization is a novel contribution of this paper, the authors must assert its originality more forcefully in the Introduction and Method sections, explicitly stating that it is the first application of its kind used to address PINN convergence failures.\n\n7. Clarity on Initialization Baseline: \n- The paper claims PINN address poor convergence associated with conventional initialization schemes (Xavier/Kaiming). Providing how poor the convergence with Xavier, Kaiming or other initialization schemes will support the strength of LeIn-PINN. \n- The authors should also clarify the paper's contribution regarding weight initialization: Is LeIn-PINN the first work to explore initialization of weight in the PINN training?\n\n8. Deeper Explanation of Landscape Analysis: In Section 4.1 (Hessian Eigenvector Landscape analysis), the claim that LeIn-PINNs achieve a \"smoother landscape that supports convergence to better minima\" is too succinct. Providing more explanation like, the presence of a \"rugged surface with deep valleys\" often leads to undesirable local minima, will help convincing readers understand better."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HJ4SJfuRom", "forum": "dHNBlXCAZw", "replyto": "dHNBlXCAZw", "signatures": ["ICLR.cc/2026/Conference/Submission22225/Reviewer_qwKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22225/Reviewer_qwKR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962551423, "cdate": 1761962551423, "tmdate": 1762942122934, "mdate": 1762942122934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LeIn, a meta-learned initialization for PINNs aimed at reducing convergence failures on challenging PDEs (e.g., stiff regimes or difficult inverse problems). LeIn has two components:\n(1) Invariance Encoding (IE): meta-updates over a distribution of easy PDE instances to encode common physics into the weights, (2) Gated Layer-wise Optimization (GLO) — a layer-gating schedule during meta-updates that gradually unlocks deeper layers to stabilize optimization. After meta-training, the learned weights serve as initialization for standard PINN training on a hard target instance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Once the initialization is learned, downstream training uses the standard PINN objective with minimal code changes.\n\n2. IE targets cross-instance invariants; GLO acknowledges heterogeneous loss dynamics across layers and mitigates instability early in training.\n\n3. The method shows consistent gains on benchmark PDEs where vanilla PINNs are known to fail."}, "weaknesses": {"value": "1. The baseline suite appears limited; several baselines seem to time out. A budget-controlled comparison with stronger baselines (curriculum, transfer initialization, and meta-learning methods such as MAML/Reptile and Hyper-LR-PINN) is needed for fairness.\n\n2. Meta-training cost & data design: The method depends on a well-curated distribution of “easy” tasks and incurs one-time meta-training overhead. The paper would benefit from a sensitivity study to the coverage/quality of this task distribution.\n\n3. Ablations of GLO: The paper motivates hard gating, but does not fully explore whether soft gating, layer-wise learning-rate schedules, or alternative unlock schedules (cosine, exponential) would perform as well or better."}, "questions": {"value": "1. Since the core idea is meta-initialization, please compare against generic meta-learning methods (e.g., MAML, Reptile) and meta-PINN approaches (e.g., Hyper-LR-PINN) under the same task distribution and collocation. Report failure rate across seeds, median/mean errors, and time-to-target-accuracy.\n\n2. Test higher-dimensional PDEs (e.g., 3D Navier–Stokes) and complex boundary/geometry to substantiate generality. Even small-scale 3D or simplified geometries would strengthen the claim.\n\n3. Since each unseen instance still needs fine-tuning, please report (i) adaptation steps/time, (ii) final storage (base LeIn weights + per-instance deltas). Consider an adapter/LoRA variant to quantify memory savings.\n\n4. Operator-learning baselines (e.g., FNO/DeepONet) or conditional PINNs for parameterized families—while not apples-to-apples (data-driven vs physics-informed), they are practical alternatives for multi-instance generalization. A brief discussion with controlled experiments would strengthen positioning.\n\n5. Please standardize notation for parameters/coefficient vectors and clearly define the “easy” task distribution in the main text (not only in appendix)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "68zPZN1iqO", "forum": "dHNBlXCAZw", "replyto": "dHNBlXCAZw", "signatures": ["ICLR.cc/2026/Conference/Submission22225/Reviewer_PMRR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22225/Reviewer_PMRR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993125095, "cdate": 1761993125095, "tmdate": 1762942122544, "mdate": 1762942122544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}