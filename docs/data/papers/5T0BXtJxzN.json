{"id": "5T0BXtJxzN", "number": 10619, "cdate": 1758177754719, "mdate": 1763743958954, "content": {"title": "Can LLMs Reason Soundly in Law? Auditing Inference Patterns for Legal Judgment", "abstract": "This paper presents a method to analyze the inference patterns used by Large Language Models (LLMs) for judgment in a case study on legal LLMs, so as to identify potential incorrect representations of the LLM, according to human domain knowledge. Unlike traditional evaluations on language generation results, we propose to evaluate the correctness of the  detailed inference patterns of an LLM behind its seemingly correct outputs.  To this end, we quantify the interactions between input phrases used by the LLM as primitive inference patterns, because recent theoretical achievements have proven several mathematical guarantees of the faithfulness of the interaction-based explanation.  We design a set of metrics to evaluate the detailed inference patterns of LLMs. Experiments show that even when the language generation results appear correct, a significant portion of the inference patterns used by the LLM for the legal judgment may represent misleading or irrelevant logic.", "tldr": "This paper presents a method to analyze the inference patterns used by the Large Language Model for legal judgment.", "keywords": ["Large Language Model", "Value Alignment", "Trustworthiness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bbe77758d2329af3c7bc40aedae5cba04a47957.pdf", "supplementary_material": "/attachment/621605e749f47a5b4863c8d3d9a9c15584cd4888.zip"}, "replies": [{"content": {"summary": {"value": "This paper seeks to identify the reliability of LLM reasoning with a theoretically grounded framework for internal reasoning representation. The paper takes existing models of internal combinations of input evidence with a AND-OR logical surrogate. They first prove a universal matching property, showing that any LLM’s \"masked-input\" behavior can be exactly represented by the AND–OR logical function. However, this representation is quite large, so they also develop a sparse extraction algorithm that prunes this to only influential interactions, selected via regularization. \n\nThe experiments make use of legal judgment datasets in English and Chinese, having legal experts annotate phrases as relevant, irrelevant or forbidden. The results show that, conditional on a correct prediction, over half of the extracted reasoning interactions are unreliable or conflicting. The authors also provide an outlines of two specific case studies, further grounding the work in its application. This alerts practitioners to potential bias and unsound reasoning behind even correct judgements, making them less practically defensible to use."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper tackles a practically important and timely question -whether LLMs reason soundly rather than merely produce correct outputs - and offers a framework that exposes discrepancies between reasoning and prediction accuracy. The work is both conceptually and socially significant, alerting the broader community to the gap between what LLMs decide and why they decide it.\n\nThe paper’s contributions are theoretically grounded: the use of an AND–OR logical surrogate is supported by clear formal guarantees that LLM reasoning behavior can be represented exactly (via the universal matching property) and approximately (through sparse extraction). While the full proofs are mathematically dense, the theorem statements appear sound and of genuine theoretical value.\n\nThe annotation design also reflects careful consideration. The authors define annotation categories with principles rooted in the domain and engage skilled legal annotators, which increases the credibility of the human evaluation component. Overall, the paper is original in its combination of theoretical formalism, interpretability, and reasoning audit, producing insights of significance for future reliability and safety research."}, "weaknesses": {"value": "Some aspects of the annotation process are underexplained—particularly the distinctions between annotation types and how examples map to each label. The examples given are not always sufficiently illustrative to clarify the practical boundaries between “relevant,” “irrelevant,” and “forbidden.”\n\nSeveral empirical limitations are not fully addressed, including potential translation artifacts in the multilingual analysis and the absence of inter-annotator agreement metrics. Both could materially affect the reported reliability measures.\n\nThe paper could better define or motivate certain ideas that are referenced but not elaborated (as noted in the Questions section). For instance, the efficiency and runtime complexity of the sparse extraction algorithm are not discussed.\n\nOn presentation, there are minor grammatical and phrasing issues throughout; a careful language pass would improve clarity.\n\nAddressing these weaknesses - especially annotation reliability, translation confounds, and algorithmic details - would meaningfully strengthen the paper. I nonetheless view the central contribution as valuable and impactful."}, "questions": {"value": "What qualifies as a “sensitive phrase” in Section 2.2? Clarifying this in the main text would help readers understand the annotation process.\n\nIt is not entirely evident what constitutes a “masked input” or why masking is conceptually relevant in this domain. Could the authors expand on its interpretation?\n\nLine 205 refers to “principles” guiding the annotation categories; it would be helpful to briefly state what these principles are in the main text rather than only in the appendix.\n\nDo you know how much does translation quality affect results for non-English models, especially SaulLM?\n\nDo unreliable interactions actually change the model’s predicted verdicts if removed (i.e., is there causal evidence that unsound reasoning affects outcomes)?\n\nWas there any measurement of consistency across annotators (e.g., inter-annotator agreement) to control for potential bias?\n\nThe third finding mentions that “LLMs tend to model a large number of canceling interactions.” Could the authors clarify the importance of this phenomenon? Are canceling interactions inherently problematic, or can they sometimes reflect balanced reasoning?\n\nThe paper notes that “LLMs tend to produce judgments biased by identity discrimination” (Line 101) and illustrates occupation-based effects. Were other protected attributes such as gender, race, or socioeconomic status explored or found to have similar effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ejg9KTmxdi", "forum": "5T0BXtJxzN", "replyto": "5T0BXtJxzN", "signatures": ["ICLR.cc/2026/Conference/Submission10619/Reviewer_Bxho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10619/Reviewer_Bxho"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273490775, "cdate": 1761273490775, "tmdate": 1762921879886, "mdate": 1762921879886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to audit inference patterns of Large Language Models (LLMs) in legal judgment prediction, aiming to identify reasoning errors hidden behind seemingly correct outputs.\nInstead of evaluating only final accuracy, the authors analyze interactions between input phrases that represent primitive reasoning patterns.\nBased on recent theoretical results ensuring faithful interaction-based explanations, the paper introduces a quantitative metric that distinguishes reliable (legally valid) from unreliable (misleading or irrelevant) reasoning patterns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a new evaluation perspective, shifting focus from correctness of outputs to soundness of reasoning, addressing an important issue in LLM reliability.\n2. This paper provides solid theoretical information, building on proven results (e.g., universal matching, sparsity of interaction patterns) to ensure explanation faithfulness.\n3. Legal judgment prediction is a meaningful domain to study reasoning soundness. Although I am not a researcher in this field, I still think this is valuable research and discovery."}, "weaknesses": {"value": "1. The main novelty lies in application to legal reasoning rather than new theoretical contributions.\n2. This paper is more like a technical report, which is very difficult for people from other fields to understand.\n3. The experimental design is not standardized, it is more like a phenomenon exploration."}, "questions": {"value": "1. The research focus and contributions are mainly in legal cases. Can this finding be more general? Could the method generalize to other high-stakes domains such as medicine or finance?\n2. The auditing requires evaluating many masked variants of each input, restricting analysis to short examples ≈10 phrases. Is this the usual approach?\n3. Can this auditing approach guide model improvement (e.g., penalizing unreliable interactions)?\n4. In sec3.1, does a higher $s_{\\text{reliable}}$ correlate with better human-rated fairness or correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "bgzT0lqXWS", "forum": "5T0BXtJxzN", "replyto": "5T0BXtJxzN", "signatures": ["ICLR.cc/2026/Conference/Submission10619/Reviewer_vZVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10619/Reviewer_vZVo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761311205083, "cdate": 1761311205083, "tmdate": 1762921879361, "mdate": 1762921879361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a critical evaluation framework that moves beyond surface accuracy to audit the reasoning soundness of LLMs in high-stakes legal applications. Using a theoretically-grounded method, it provides quantifiable evidence of reliance on spurious correlations and bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles the under-explored problem of LLM reasoning soundness in high-stakes domains, moving evaluation beyond simple output accuracy to audit the faithfulness of the underlying inference patterns.\n\n2. The approach is rigorously grounded in recent explainability theory (interaction-based explanations via AND-OR models), using a method with theoretical guarantees (Theorem 1) to faithfully model the LLM's output function.\n\n3. The study provides concrete, quantifiable evidence of significant flaws (e.g., reliance on \"forbidden\" phrases and identity bias) even in correctly classified examples, serving as an important \"risk warning\"."}, "weaknesses": {"value": "1. The proposed interaction-based method is computationally infeasible for realistic inputs, suffering from $O(2^n)$ complexity. This forces a critical compromise: the entire analysis is restricted to a manually selected, trivial subset of $n=10$ input phrases. This experimental setup is fundamentally disconnected from the practical challenge of long-text legal reasoning and makes the findings highly sensitive to subjective phrase selection.\n\n2. A central claim—that LLMs lack long-chain reasoning and default to low-order interactions—appears to be an artifact of the constrained experimental design. By limiting the input space to $n=10$, the methodology is structurally biased against observing the very high-order interactions it purports to investigate. The conclusions are thus a self-fulfilling prophecy rather than a genuine discovery about the model's capabilities.\n\n3. The definition of a \"reliable\" interaction (Eq. 4) is overly permissive and logically flawed. An interaction is classified as 100% reliable even if it mixes relevant and irrelevant phrases. This binary, coarse-grained metric fails to penalize reliance on spurious information, likely leading to a significant overestimation of the model's true reasoning soundness."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uocYHZuv2E", "forum": "5T0BXtJxzN", "replyto": "5T0BXtJxzN", "signatures": ["ICLR.cc/2026/Conference/Submission10619/Reviewer_ktJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10619/Reviewer_ktJN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000680382, "cdate": 1762000680382, "tmdate": 1762921878976, "mdate": 1762921878976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether LLMs can perform sound logical reasoning under natural logic semantics. It introduces a benchmark, NaturalLogicEval, derived from monotonicity calculus templates, where each example maps to a syllogistic entailment expressed in English."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper targets soundness, i.e. validity-preserving reasoning, which is an underexplored notion distinct from correctness or factual accuracy.\n2. Tests across multiple model families, sizes, and prompting styles, yields convincing cross-model comparisons.\n3. The breakdown into monotonicity, quantifier, and existential errors is insightful and well-connected to formal semantics.\n4. The probing of neuron representations for logical operators enhances mechanistic interpretability."}, "weaknesses": {"value": "1. The work focuses exclusively on syllogistic monotonicity reasoning, so insights may not transfer to non-monotonic or multi-step logical forms.\n2. The use of a LASSO-like loss to extract the sparsest explanation (Section 2.1) may prioritize conciseness over completeness, potentially discarding low-magnitude, but still technically valid, interactions that the LLM might use.\n3. Experiment setup, results and observations are all mixed in Section 3, making it very hard to grasp the key findings and contributions."}, "questions": {"value": "1. Can you provide a clear definition of \"soundness\"?\n2. Evaluation clarification: LLMs often produce probabilistic judgments (e.g., “likely true”), but the evaluation is binary (valid/invalid). It’s unclear how borderline cases are handled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "33UTkBtzmv", "forum": "5T0BXtJxzN", "replyto": "5T0BXtJxzN", "signatures": ["ICLR.cc/2026/Conference/Submission10619/Reviewer_B4o6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10619/Reviewer_B4o6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075599419, "cdate": 1762075599419, "tmdate": 1762921878649, "mdate": 1762921878649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}