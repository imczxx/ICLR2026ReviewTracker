{"id": "2Dn4yHYLQJ", "number": 633, "cdate": 1756757759356, "mdate": 1759898249470, "content": {"title": "Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs", "abstract": "Zeroth-order optimizers have recently emerged as a practical approach for fine-tuning large language models (LLMs), significantly reducing GPU memory consumption compared to traditional first-order methods. Yet, existing zeroth-order methods rely on hand-crafted, static sampling strategies that are not adaptable to model-specific structures. To address this, we propose ZO Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically learns efficient perturbation strategies through a compact and memory-efficient design. Crucially, our approach is motivated by the observation that only a small number of foundation models and their derivatives are widely adopted in practice. Therefore, learning the optimizer once for a given LLM and reusing it across diverse downstream tasks is both feasible and highly desirable. Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the foundation-model era by supporting one-time training per LLM with minimal overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1\\% of task-model combinations, thereby demonstrating strong performance and scalability for efficient LLM fine-tuning.", "tldr": "", "keywords": ["Zeroth-order Optimization", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91644f5f47cc4976ad1eeb52c84d0690bdfc1666.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a learned zeroth-order (ZO) optimizer for memory-efficient LLM fine-tuning. Instead of fixed perturbation scales (e.g., MeZO), the method learns block-level variance policies through lightweight neural modules and normalizes their overall scale. The optimizer is meta-trained once and reused across tasks and models. Experiments show consistent improvements over existing ZO approaches with minimal overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Block‑wise variance learned by tiny MLPs; total aux parameters <2 MB even for OPT‑30B\n\n2. A finetuner learned on COPA transfers across 7 datasets and to LLaMA‑3.1‑8B‑Instruct, retaining gains。\n\n3. Near‑inference memory and negligible extra compute (e.g., OPT‑30B 62 GB vs 316 GB for FO‑Adam"}, "weaknesses": {"value": "1. According to Eq. 4, with anisotropic Gaussians E[uₜuₜ^⊤]=Σₜ, so the expected update is Σₜ∇L, not ∇L. The method neither whitens by Σₜ^{-1} nor analyzes the induced bias; fixing ‖Σₜ‖_F cannot remove directional preconditioning.\n\n2. The optimizer is trained to minimize one‑step post‑update loss (Eq. 5), which risks greedy behavior and gives no guarantees for long‑horizon stability under accumulated ZO noise.\n\n3. In Algorithm 2, after updating PertNN (line 8), they advance the model with SGD to produce the next training state; at deployment there is no backprop at all. This teacher‑forcing bias is not addressed.\n\n4. Per block, inputs are only Mean, Var, previous σ, and two losses. These statistics are loosely related to local curvature/gradient structure, undermining the “block‑diagonal Hessian” motivation.\n\n5. The method is compared only to ZO baselines. Including parameter-efficient first-order baselines at similar memory budgets (e.g., LoRA/QLoRA with 8-bit optimizer states) would clarify the method’s place in the practical landscape.  \n\n6. The informal theorem hinge on approximate block‑diagonality, yet the paper reports no quantitative measure of off‑diagonal energy/spectral norms on real LLMs."}, "questions": {"value": "The method is motivated by the assumption that curvature is approximately block-structured. Have you examined Hessian or Fisher block interactions in practice?\n\nHow does the approach compare to well-tuned LoRA/QLoRA baselines under similar memory constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TFNSjeutIQ", "forum": "2Dn4yHYLQJ", "replyto": "2Dn4yHYLQJ", "signatures": ["ICLR.cc/2026/Conference/Submission633/Reviewer_rsyj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission633/Reviewer_rsyj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983225825, "cdate": 1761983225825, "tmdate": 1762915572975, "mdate": 1762915572975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new zeroth-order optimizer for LLM fine-tuning that learns a variance term for perturbation at each layer for an LLM. These learned perturbation strategies then generalize to other fine-tuning tasks that use the same foundation model. The ZO Fine-Tuner approach outperforms existing methods such as MEZO on training loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is fairly clearly presented, and the main idea is sensible. The main hypothesis, that perturbation strategies can generalize from task to task with the foundation model fixed, is interesting and does seem to be validated appropriately by the results in Table 3. The memory usage and runtime measurements in Section 4.3 do a good job of giving us a sense of the cost of the method."}, "weaknesses": {"value": "As far as I can tell, there are just no validation/test measurements for anything done in this paper, so we have no idea how ZO Fine Tuner might affect generalization. Even though the paper is otherwise strong, the lack of any (unless I missed something!) validation/test measurements seems fatal."}, "questions": {"value": "Where (I hope I just missed it) are the validation/test measurements in the paper to show generalization or applicability to downstream tasks on data different from that on which the model was trained? What would Table 1 look like if the numbers reported were on the test set rather than the training set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4pE5V7xS9c", "forum": "2Dn4yHYLQJ", "replyto": "2Dn4yHYLQJ", "signatures": ["ICLR.cc/2026/Conference/Submission633/Reviewer_bS2A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission633/Reviewer_bS2A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119874172, "cdate": 1762119874172, "tmdate": 1762915572772, "mdate": 1762915572772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ZO Fine-tuner, a learning-to-learn zeroth-order optimizer for LLM fine-tuning. Instead of drawing perturbations from a fixed distribution, it attaches a tiny auxiliary “PertNN” to each parameter block (e.g., Q/K/V, projections, embeddings) to predict that block’s perturbation variance. A simple norm-budget normalization keeps variance scale decoupled from the step size. The method is motivated by a block-diagonal Hessian view and an informal result suggesting per-block adaptive variance can tighten the expected one-step loss bound versus standard MeZO. Training uses a short first-order meta-learning phase done once per base model; after that, the learned optimizer is reused across tasks and derivative checkpoints of the same model family. Experiments on 4 LLMs across 7 datasets (28 settings) show lower converged loss in most cases and an average accuracy gain of about 2.5% over ZO baselines, with negligible runtime and memory overhead relative to MeZO. A small transfer test indicates the optimizer trained on LLaMA-8B also works on LLaMA-8B-Instruct. Ablations highlight the benefits of the variance normalization trick, periodic resets during meta-training, and block-wise sharing over layer-wise sharing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Framing ZO fine-tuning as learning a per-block perturbation variance via tiny auxiliary networks is a neat twist that removes hand-crafted noise schedules and makes ZO more adaptive. The norm-budget trick to decouple step size from variance scale is simple but fresh and broadly applicable.\n\nQuality: The method is implemented cleanly with low overhead, and the experiments cover multiple LLM sizes and seven benchmarks. Useful ablations (normalization, reset strategy, sharing granularity) help isolate which pieces matter. Runtime/memory tables demonstrate practical deployability."}, "weaknesses": {"value": "1. The “cross-model generalization” evidence only tests transfer from LLaMA-8B to LLaMA-8B-Instruct, which share the same architecture and most parameter shapes. This is effectively within the same model family, not true cross-model generalization. The claim of “train once, reuse widely” is therefore overstated.\n\n2. The experiments mostly use short-sequence or classification-style tasks (SST-2, CB, COPA, BoolQ, etc.). There is no evidence that the proposed optimizer generalizes to long-sequence dataset."}, "questions": {"value": "1.Generalization beyond text classification – Have you evaluated ZO Fine-tuner on longer-sequence.\n\n2.Good analysis on Training loss and Inference loss in experment, does author try to analyze the accuracy curve in experment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0dsC4a3IeY", "forum": "2Dn4yHYLQJ", "replyto": "2Dn4yHYLQJ", "signatures": ["ICLR.cc/2026/Conference/Submission633/Reviewer_TZJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission633/Reviewer_TZJo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762133565051, "cdate": 1762133565051, "tmdate": 1762915572617, "mdate": 1762915572617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ZO Fine-tuner, a learned zeroth-order optimizer for LLM fine-tuning. The authors propose using tiny per-block PertNN networks to predict block-wise perturbation variances, and a normalization step fixes the overall variance budget so a single learning rate remains stable. The fine-tuner is trained once and reused across various downstream tasks. Experiments show lower loss and imporved accuracy across a collection of models/tasks with negligible time and memory overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Meta optimizer is trained once and reused across tasks and model derivatives with consistant gains over MeZO.\n- Algorithm is very lightweight and adds minimum memory/time overhead compared to MeZO.\n- The paper conducts detailed experimentation across different models and datasets to show generalizability."}, "weaknesses": {"value": "The paper's evaluation is a bit narrow, it only compares against optimizer variants of MeZO (HiZOO/LOZO) and ignores other memory-focused fine-tuning approaches. There exists low-memory baselines such as ZO + quantization workflows, first order + quantization methds (e.g. QLoRA) and recent sparse-ZO approaches (e.g. sparse-MeZO and SenseZOQ) \n\nAppendix D.3 shows OPT-30B requires 62G vram with ZO-Fine-tuner, while prior work demonstrates far better memory efficiency, QLoRA fine-tuned a 65B model on a 48G gpu, and SensZOQ fine-tuned a 7B model within 8G of vram. Since the authors emphasize memory efficiency, comparisons against these stronger, memory focused baselines is important but currently missing.\n\n1. QLoRA https://arxiv.org/abs/2305.14314\n2. Sparse MeZO https://arxiv.org/abs/2402.15751\n3. SenseZOQ https://openreview.net/pdf?id=myYzr50xBh"}, "questions": {"value": "1. How does ZO-Fine-tuner perform in terms of memory usage and convergence speed when compared against\n  - Sparse MeZO with its dynamic mask implementation\n  - SenseZOQ which uses a static 0.1% weight mask + 4bit quantization\n  - QLoRA, which is the first order + quantization baseline \n\n2. It is known from the literature that ZO methods suffers from slow convergence speed issues, so how does ZO-Fine-tuner's convergence speed compare with ZeRO-3 offloading, since both trade training speed for memory reduction? What is the accuracy/performance gap between ZO-Fine-tuner and full FO fine-tuning under comparable resource settings?\n\n3. Table 3 reports no FO baseline for direct comparison, could the authors include results for FO-Adam or FO-SGD to quantify how much performance is lost when moving from first-order to zeroth-order optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CZopHN4bDF", "forum": "2Dn4yHYLQJ", "replyto": "2Dn4yHYLQJ", "signatures": ["ICLR.cc/2026/Conference/Submission633/Reviewer_7BNG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission633/Reviewer_7BNG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157331033, "cdate": 1762157331033, "tmdate": 1762915572433, "mdate": 1762915572433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ZO Fine-tuner, a learning-based zeroth-order optimizer for fine-tuning large language models (LLMs).\nInstead of using manually fixed perturbation strategies (e.g., MeZO), the authors design lightweight per-block neural networks (PertNNs) that learn adaptive perturbation variances.\nThe optimizer is trained once through a learning-to-learn (L2L) process on a single base model and can be reused across multiple downstream tasks or derived models.\nExtensive experiments on 4 LLMs (1B–30B) and 7 datasets show consistent gains—82.1% of task–model pairs achieve lower loss than MeZO with <3.5% runtime overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Introduces a learning-based perturbation policy for ZO optimizers—conceptually new compared to all existing handcrafted ZO methods.\n\n2. Demonstrates strong generalization: trained once, the optimizer transfers to other datasets with consistent gains.\n\n3. Theoretical justification (block-diagonal Hessian) is reasonable and empirically supported by ablations.\n\n4. Runtime and memory overheads are negligible, maintaining MeZO-level efficiency.\n\nOverall, I think this is a good paper."}, "weaknesses": {"value": "1. The periodic reset policy, trajectory selection (optimizer/steps/lr), and seed sensitivity could materially affect the learned policy. While Algorithms 1–2 outline the process, the paper should report concrete reset schedules, ablate trajectory choices (e.g., Adam vs. SGD), and release fixed-seed scripts to ensure stable reproduction.\n\n2. Hessian assumption not stress-tested across architectures.\nThe block-diagonal Hessian premise motivates block-wise variance. The paper would be stronger with empirical Hessian structure diagnostics or small-scale visualizations, especially for MoE or architectures with stronger cross-layer coupling."}, "questions": {"value": "1. Can you provide head-to-head comparisons with the latest ZO optimizers targeting speed and variance reduction, using identical hardware, context length, and training budget?\n\n2. How sensitive is the learned policy to the first-order trajectory source (AdamW vs. SGD, different lrs) used during L2L?\n\n3. What is the reset schedule (trigger, frequency) in L2L, and how does it affect convergence and the loss-region coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YpHX1kRYb3", "forum": "2Dn4yHYLQJ", "replyto": "2Dn4yHYLQJ", "signatures": ["ICLR.cc/2026/Conference/Submission633/Reviewer_xRx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission633/Reviewer_xRx2"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission633/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762171884074, "cdate": 1762171884074, "tmdate": 1762915572167, "mdate": 1762915572167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}