{"id": "jYDHVscRO3", "number": 20025, "cdate": 1758301644767, "mdate": 1763759009781, "content": {"title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change", "abstract": "This paper introduces the Behavioral Ambivalence/Hesitancy (BAH) dataset collected for the Ambivalence/Hesitancy (A/H) recognition task in videos. In particular, this task involves recognizing conflicting emotions linked to A/H from question-and-answer videos captured for behavior analysis. The dataset contains videos from 224 subjects with different age, ethnicity collected across 9 Canadian provinces via webcam through our developed web platform. Each user answers to 7 questions that we designed to induce Ambivalence/Hesitancy. Each video captures the response for one question in the subject's environment totaling 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. BAH is a first and unique dataset for Ambivalence/Hesitancy recognition. Our behavioral team annotated timestamp segment where A/H occurs providing frame- and video-level annotation, in addition to used cues for annotation such as face, audio, body, and language. Video transcripts and their timestamps as well as per-frame cropped and aligned faces are also included. This work offers initial baselines for A/H recognition in videos at frame- and video-level with different analysis with single and multimodal setups. The data, code, and pretrained weights are publicly accessible.", "tldr": "We introduce new dataset for Ambivalence/Hesitancy recognition in videos with 224 subjects and 1118 videos. Data and code are publically available.", "keywords": ["Ambivalence", "hesitancy", "affective computing", "emotion recognition in videos", "multimodal", "eHealth", "behavioral change"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cb25428fb64d96895bf82ff380af5f7232bf450.pdf", "supplementary_material": "/attachment/3f7f3d125b31901f8f44daaaff04f1b82560bfaa.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new multimodal dataset for recognizing ambivalence and hesitancy. It includes recordings with expert annotations covering facial, vocal, verbal, and bodily cues, offering a valuable resource for studying complex emotional states relevant to behavior change. The authors provide baseline experiments across different modalities and fusion strategies, showing that temporal context and multimodal learning improve performance but that the task remains challenging, paving the way for future research on nuanced emotion understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The first public dataset focused on recognizing ambivalence and hesitancy, emotional states that play an important role in behavior-change research but have received little attention in machine learning.\n2.The authors conducted comprehensive experiments comparing different setups: single-modality vs. multi-modality, with or without temporal context, and various fusion methods. The results show that adding temporal context improves performance, simple concatenation often works surprisingly well, and combining all three modalities does not always outperform simpler setups, indicating that multimodal fusion for ambivalence recognition remains a challenging problem."}, "weaknesses": {"value": "1. The dataset provides only binary (0/1) labels for Ambivalence/Hesitancy, which may be too simplistic. Incorporating continuous annotations such as valence–arousal or PAD scales could enable a more fine-grained analysis of emotional states and cues.\n2. Although the dataset includes facial, linguistic, audio, and body cues, these are annotated only at the video level. While this adds interpretability, it limits temporal precision. Adding timestamps or ideally frame-level cue annotations would make the dataset far more useful for detailed temporal modeling.\n3. The paper describes the annotation process and co-annotation procedures but does not report quantitative inter-annotator agreement. Without this metric, it is difficult to assess the reliability and consistency of the labels.\n4. The paper does not provide metadata statistics for the training, validation, and test splits, particularly regarding demographic balance. Reporting and ensuring demographic diversity across splits would support fairness analysis and enhance the dataset’s research value."}, "questions": {"value": "1. What are the inter-annotator agreement (IAA) scores at both the video and frame levels for Ambivalence/Hesitancy labeling, as well as for cue tagging? Additionally, how do the models perform on high-certainty versus low-certainty segments?\n2. How did the annotators distinguish ambivalence from mixed emotions or uncertainty without intent conflict? Please provide annotated examples and an error typology to clarify how these cases were handled.\n3. Please include metadata statistics for the training, validation, and test splits, especially regarding demographic distribution, to assess data balance and fairness.\n4. It would strengthen the paper to clearly explain the conceptual and methodological differences between Ambivalence/Hesitancy recognition and temporal emotion recognition, highlighting why A/H detection requires distinct modeling strategies."}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety"]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eOtk0nsOIz", "forum": "jYDHVscRO3", "replyto": "jYDHVscRO3", "signatures": ["ICLR.cc/2026/Conference/Submission20025/Reviewer_pr3G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20025/Reviewer_pr3G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761339621355, "cdate": 1761339621355, "tmdate": 1762932922514, "mdate": 1762932922514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We would like to express our sincere gratitude to all reviewers for their invaluable feedback throughout this review process. Their insightful questions and suggestions have significantly helped us clarify and strengthen our paper. We truly appreciate the positive recognition of our work shared by all four reviewers including the novelty of the task and multimodal dataset (tpao, RBNh, GdzA, pr3G), richness of modalities and annotation (tpao), demographic diversity (GdzA), well-designed data collection platform (tpao), and the valuable and comprehensive benchmarking (RBNh, GdzA, pr3G) that highlights the difficulty and challenges posed by the new task of video-based ambivalence/hesitancy (A/H) recognition.\n\nReviewers mainly requested clarifications on the difference between standard action recognition and A/H recognition in videos. Essentially, A/H recognition is more challenging as A/H are expressed in a subtle way, with high inter-person variability,  and is often associated with conflicting expressions across different modalities. State-of-the-art multimodal expression recognition models are ill equipped to accurately predict under such conflict. Additionally, reviewers requested additional interpretations of our results. Combining all modalities did not yield the best results compared to pairwise combinations. More specialized fusion techniques are needed to efficiently combine modalities for A/H recognition. Reviewers also asked for additional experiments with other visual modalities such body image, and head-pose features. Our results using full frame (with body) provides a wider context and provides competitive results compared to using cropped faces, yet head-pose features achieved lower performance. Moreover, reviewers requested further clarification on the annotation process. The three expert annotators divided the dataset to balance quantity and quality of annotation to construct such a large dataset for machine learning training. Additionally, our analysis of multi-annotator agreement over a random subset of videos showed a moderate to substantial agreement between global- and frame-level annotation.  All reviewer comments were answered below and integrated into the revised manuscript. We have also included new results with full frame and head-pose features.\n\nSince the submission of our manuscript, we have collected and annotated 309 videos from 76 additional participants. The final BAH dataset will have a total of 1427 videos from 300 participants. This larger version of our dataset is available to reviewers. We request from all reviewers permission to update the dataset. The main updates to the manuscript for the dataset include an analysis of annotation cues; demographic statistics of train, validation, and test sets; the distribution of participants according to the country of birth; meta-data of participants with the dataset; an inter-annotator agreement analysis; and a new section for recommendations for designing future specialized methods for video-based AH recognition.\n\nFinally, we would like to clarify that the purpose of our benchmarking work is mainly to provide a starting  baseline to initiate future works. Our results show also the limitations of standard and well-known models for multimodal expression recognition, fusion, and spatio-temporal. Researchers can compare to our results, and develop more specialized methods for video-based AH recognition."}}, "id": "ad03izJ1RP", "forum": "jYDHVscRO3", "replyto": "jYDHVscRO3", "signatures": ["ICLR.cc/2026/Conference/Submission20025/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20025/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20025/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763759065980, "cdate": 1763759065980, "tmdate": 1763759065980, "mdate": 1763759065980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Behavioural Ambivalence/Hesitancy (BAH) dataset, a multimodal video corpus for recognizing ambivalence/hesitancy (A/H) states. It includes 1,118 videos from 224 participants across Canada, annotated at video- and frame-levels with onset–offset segments and multimodal cues (face, audio, text). The dataset aims to model complex, sustained emotional states rather than discrete peaks. Baseline experiments perform binary A/H vs. non-A/H classification, showing the task’s difficulty and establishing a benchmark for future affective computing research."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a novel multimodal dataset that integrates visual, audio, and textual modalities, focusing on the recognition of ambivalence and hesitancy (A/H), which are complex, sustained emotional states rather than discrete basic emotions. It thereby proposes a new affect recognition task that broadens the traditional emotion recognition paradigm.\n2. The dataset is of moderate scale, containing videos from 224 participants across 9 Canadian provinces, ensuring demographic and ethnic diversity and supporting fairer modelling of human affect. This makes it a valuable contribution to the affective computing community.\n3. The baseline experiments are well designed and relatively comprehensive, covering frame-level and video-level recognition in both unimodal and multimodal settings. The paper also reports zero-shot prediction results and unsupervised domain adaptation models for personalization, which further enhance the dataset’s utility and research relevance."}, "weaknesses": {"value": "1. While the dataset itself is valuable, the title and task definition are misleading. The term “Ambivalence/Hesitancy Recognition” suggests separate classification of ambivalence and hesitancy, yet all experiments address a binary detection task (A/H vs. non-A/H) without distinguishing the two.\n2. The inclusion of “for Behavioural Change” in the title appears overstated, as the study does not involve any behavioural intervention, longitudinal tracking, or pre– post change analysis. The link between recognizing A/H emotions and actual behavioural change is discussed only conceptually, not empirically demonstrated.\n3. The scientific relevance of detecting A/H remains insufficiently justified. While ambivalence and hesitancy are theoretically relevant to behaviour regulation, the paper does not clarify how A/H recognition could inform or improve behavioural interventions. Strengthening this conceptual bridge would significantly enhance the contribution."}, "questions": {"value": "1. As mentioned in weaknesses. We suggest distinct recognition of ambivalence and\nhesitancy and a connection to behavioural change, while the paper only conducts a binary A/H vs. non-A/H classification without behavioural intervention. A clearer alignment between the task scope and the title would improve conceptual precision.\n2. The abstract could be more concise and focused on the dataset’s conceptual contribution and key findings rather than listing detailed statistics. Streamlining this section would make the paper’s main message clearer.\n3. In the Introduction, the motivation for detecting A/H remains vague. The paper would benefit from a clearer explanation of why distinguishing A/H from non-A/H emotions is important and in which practical contexts this task has value. At present, the classification objective seems somewhat detached from real-world applications. The connection to behavioural change also remains conceptual. Clarifying how A/H recognition could be used in adaptive feedback or intervention systems would make this link more convincing.\n4. Finally, the explanation of the multimodal fusion results (Table 5 / Table 15) is not entirely satisfactory. The paper attributes the drop in performance with tri-modal fusion (visual + audio + text) to “modality conflicts.” However, given that ambivalence and hesitancy are themselves conflictive emotional states, such cross- modal inconsistency might in fact be an informative signal rather than noise. It is unclear why three-modality fusion causes conflict while two-modality setups do not, especially since the text modality is derived from speech. Further clarification and analysis of this phenomenon would strengthen the interpretation of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YEIR415W4Z", "forum": "jYDHVscRO3", "replyto": "jYDHVscRO3", "signatures": ["ICLR.cc/2026/Conference/Submission20025/Reviewer_GdzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20025/Reviewer_GdzA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679854703, "cdate": 1761679854703, "tmdate": 1762932921827, "mdate": 1762932921827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the first Behavioural Ambivalence//Hesitancy (BAH) dataset collected for subject-based multimodal recognition of A/H in videos. It contains videos from 224 participants captured across nine provinces in Canada, with different age, and ethnicity. BAH contains 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. The paper also provides preliminary benchmarking results using baseline models trained on BAH for frame- and video-level recognition with mono- and multi-modal setups. It also includes results on models for zero-shot prediction, and for personalization using unsupervised domain adaptation. The limited performance of baseline models highlights the challenges of recognizing A/H in real-world videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is interesting to have a dataset for ambivalence and hesitancy (A/H)  recognition which  involve subtle and conflicting emotions that are manifested by a discord between multiple modalities, such as facial and vocal expressions, and body language. \n\n2. The authors conducted extensive experiments to demonstrate the potential of this dataset for frame- and video-based emotion recognition."}, "weaknesses": {"value": "1. In the experimental results, the authors only consider CNN- and ViT-based models which are originally designed for image classification. It would be more interesting to consider video-based models to demonstrate the difficulty of the proposed benchmark.\n\n2. While a dedicated dataset for ambivalence and hesitancy (A/H)  recognition is interesting, it is not clear what is the major difference between this task compared with other video recognition task such as activity recognition. In other words, what aspect makes this task challenging or difficult. \n\n3. The dataset size is relatively small with only a total duration of 8.26 hours which can possibly limit the usefulness of the proposed dataset."}, "questions": {"value": "1. In table 5, it is shown that with Visual + Audio + Text, the results are worse than other cases, can the authors explain the possible reason? \n\n2. What are main challenges of ambivalence and hesitancy (A/H)  recognition compared with other types of emotion recognition such as anger or happy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EnZhn2l9rY", "forum": "jYDHVscRO3", "replyto": "jYDHVscRO3", "signatures": ["ICLR.cc/2026/Conference/Submission20025/Reviewer_RBNh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20025/Reviewer_RBNh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962659615, "cdate": 1761962659615, "tmdate": 1762932921264, "mdate": 1762932921264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BAH, a dataset for recognizing ambivalence and hesitancy (A/H) in short webcam-style videos. The dataset consists of 1118 videos (~8 h total) from 224 participants across 9 Canadian provinces, each responding to 7 prompts. The dataset includes both frame-level and video-level A/H annotations. Baseline models are provided for visual, audio, and text modalities, as well as simple fusion and contextual vs non-contextual comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The first publicly available dataset specifically focused on ambivalence/hesitancy detection.\n2. Clearly described data collection and annotation framework, incorporating both global- and frame-level labels"}, "weaknesses": {"value": "1. All subjects are from a single country (Canada), which may limit cultural and linguistic generalization of ambivalence expressions.\n2. Contextual and multimodal (tri-modal) results are unexpectedly similar to non-contextual or single-modality baselines, suggesting limited exploitation of temporal or cross-modal dependencies."}, "questions": {"value": "1. The contextual and multimodal (especially tri-modal) results are very close to those of single-modality models. How do you explain this? Why did tri-modal fusion underperform pairwise combinations? Are these differences statistically significant across runs? \n2. Given that ambivalence is highly context-dependent, what temporal window or duration do you consider sufficient for a valid A/H judgment? How sensitive are the annotations or models to this choice? \n3. How was ground truth established when annotators disagreed? Was it based on majority vote, adjudication by a lead annotator, or consensus discussion? \n4. The codebook explicitly includes body language cues, yet the modeling pipeline omits body or pose features. Why didn’t you include these, and do you plan to in future work? \n5. Can you provide inter-annotator reliability metrics (e.g. Cohen’s κ) for A/H labels? Also, how were temporal boundaries defined and aligned between annotators?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Dataset includes identifiable human subjects (faces, voices, transcripts) and is shared via public links with passwords. The paper does not specify whether data from participants who did not consent to public release were excluded."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kG5zfUmzDb", "forum": "jYDHVscRO3", "replyto": "jYDHVscRO3", "signatures": ["ICLR.cc/2026/Conference/Submission20025/Reviewer_tpao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20025/Reviewer_tpao"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980456278, "cdate": 1761980456278, "tmdate": 1762932920831, "mdate": 1762932920831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}