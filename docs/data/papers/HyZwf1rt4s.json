{"id": "HyZwf1rt4s", "number": 20658, "cdate": 1758308690640, "mdate": 1759896965675, "content": {"title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review", "abstract": "Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs. Additionally, we explore a context-aware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI. To support future research and reproducibility, we will publicly release our dataset upon publication.", "tldr": "We reveal the difficulty of detecting AI-written peer reviews using a new benchmark dataset and propose Anchor as a first step forward.", "keywords": ["Machine Learning Evaluation", "Benchmark Datasets", "Robustness in NLP", "Large Language Models (LLMs)", "Generative AI", "Human–AI Alignment", "Ethical Considerations in ML"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a24d1ef532ce0930c72c6beb7e7b9af74c9d94c1.pdf", "supplementary_material": "/attachment/80f2bef0bc0b6d1a9616c585555ed4f57ab30b67.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to detect peer reviews generated by LLMs. The authors construct a benchmark dataset, where there are both human reviews and AI-written reviews of papers submitted to computer conferences in recent years. This paper also proposes a detection method named \"Anchor\" to distinguish between such two types of peer reviews. Additionally, a series of evaluations are proposed to analyze the differences of the two types of review and the detection process, where the effectiveness of the proposed method is proved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper targets at an ethical issues, detection of peer reviews from LLMs. Existing benchmarks mainly focus on general AI text detection, while this paper provides a new dataset for specific peer reviews, offering a useful benchmark for this problem.\n2. In the evaluation part, the paper considers various aspects to analyze the problem, among which different prompts and LLM-assisted editing are meaningful. Furthermore, authors also discuss different types and parts of peer reviews, such as text-based reviews and score-based reviews, showing AI reviews' straight influence on paper acceptance."}, "weaknesses": {"value": "1. There might be deviation in distribution of words because different papers have special domain biases. The paper lacks analysis of such diversity, which tends to amplify the differences between LLM-generated texts and human-written reviews. Whether usage of human-written reviews in a specific domain will improve the quality of AI-written reviews for similar papers has not been discussed here.\n2. Despite \"AI peer review detection robust to prompt variations\" is described in section 5.2, four reviewer archetypes which lack diversity do not make sense enough. This paper does not analyze how people will write prompts in real situations via conducting user studies or analyzing over a series of real prompts.\n3. The authors introduce several interesting aspects in the evaluation part, but the analysis merely remains at the level of describing certain phenomena. More detailed cases might be more convincing to show how much the phenomena or factors influence the detection to show your benchmark will be effective in peer review."}, "questions": {"value": "1. Could the authors provide the diversity of the collected papers? Parts of the papers focus on specific application or domain and might be characteristic, and what is the proportion of such papers? Will the benchmark and proposed method be more effective if such diversity is considered?\n2. Could the authors provide some detailed cases when there are changes to the analyzed factors? It is convincing if there are examples to show whether the changes of some factors might influence the detection and whether existing methods might fail to handle such changes. And the authors might analyze why the proposed method is better from concrete examples or whether some factors lead to some misclassification results.\n3. Chapter 4 only has one subsection 4.1 (this subsection and its title might be useless). The authors introduce the proposed method in detail after the 'EXPERIMENTAL RESULTS' chapter and such results prove the method, where the writing logic is confusing. The authors might reorganize these parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oCsX8NvicD", "forum": "HyZwf1rt4s", "replyto": "HyZwf1rt4s", "signatures": ["ICLR.cc/2026/Conference/Submission20658/Reviewer_xwvH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20658/Reviewer_xwvH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746826236, "cdate": 1761746826236, "tmdate": 1762934047847, "mdate": 1762934047847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the growing concern that LLMs are being used to generate peer reviews for academic conferences. The authors created a large-scale dataset for AI-generated peer reviews and benchmarked 18 existing AI text detection methods to assess their effectiveness at identifying LLM-written reviews, showing that most existing detection methods struggle significantly at low false positive rates. The proposed \"Anchor\" detection method uses manuscript context by comparing test reviews to reference AI reviews generated for the same paper, which is simple but effective. The authors also identified several differences between human and AI-written reviews, and conducted robustness analyses to demonstrate that detection performance remains stable across different prompting strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a critical and emerging problem in scientific peer review, the detectability of LLM-generated reviews.\n\n2. The constructed dataset is large-scale and can be useful for the ongoing studies of AI text detection. The authors benchmarks 18 existing AI text detection methods, showing that most existing detection methods struggle significantly at low false positive rates.\n\n3. I like how authors study the differences between human and AI-written reviews. This could be very helpful for understanding the limitations of LLM-generated peer reviews and developing better detection methods.\n\n4. The overall structure is well organized and the contents are sufficient to show the challenges in detecting LLM-generated reviews."}, "weaknesses": {"value": "1. One potential explanation for the poor performance is that peer review text may be out-of-distribution for these detection models, have the authors considered evaluating whether fine-tuning existing detection methods on peer review data could substantially improve detectability?\n\n2. Although the authors tested the robustness of their methods across different prompts and pipelines, they all follow the same paradigm: given a paper, generate reviews on it. However, in reality, this may not be how reviewers actually use LLMs to generate reviews. For example, a reviewer might provide some bullet points to an LLM and ask it to expand on them [1].\n\n3. A potential weakness of the Anchor method is its reliance on a reference dataset of AI-generated reviews, which may quickly lose its effectiveness as AI evolves.\n\n[1] Lee, Mina, et al. \"A design space for intelligent and interactive writing assistants.\" Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 2024."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uXlUNdisBf", "forum": "HyZwf1rt4s", "replyto": "HyZwf1rt4s", "signatures": ["ICLR.cc/2026/Conference/Submission20658/Reviewer_6PDK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20658/Reviewer_6PDK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037583998, "cdate": 1762037583998, "tmdate": 1762934047228, "mdate": 1762934047228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of detecting AI-generated text in the peer-review process.\n\nIts contribution includes:\n\n1. A comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews for ICLR and NeurIPS papers (2016–2024)\n2. Benchmarks 18 existing AI-text detection methods 3 Proposes a simple context-aware “Anchor” approach that leverages manuscript content by comparing the semantic similarity between a target review and an LLM-generated reference review.\n3. Share some interesting insight in stylistic differences between human and AI reviews."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shows a certain level of originality. It creates a large-scale dataset in a new domain - peer review. Existing detection datasets e.g., HC3, RAID-TD, M4, Beemo, GRiD does not cover this domain. Also, the benchmarking framework’s focus on achieving reliable detection under low false-positive constraints is also a meaningful and novel contribution, as few prior works explicitly address this important practical and ethical consideration in review integrity contexts.\n\n2. The paper is clearly written and easy to follow. The experimental setup is rigorous, with careful exclusion of post-2022 reviews to minimize potential LLM contamination. It provides extensive benchmarking across 18 detection methods. It also includes thoughtful analyses on prompt robustness, AI-assisted editing, and behavioral differences between human and AI reviewers."}, "weaknesses": {"value": "1. The proposed Anchor method offers only incremental innovation. Its core idea—measuring embedding similarity between a suspected review and an LLM-generated reference—is conceptually similar to prior work such as DetectLLM and DNA-GPT, which also compare generated and reference texts to assess authorship likelihood.\n\n2. The dataset also relies entirely on synthetic AI reviews, which may not fully represent real-world LLM-assisted peer-review behavior. Also, if the same or related model family (e.g., GPT-4) is used for both, the high similarity might simply reflect shared model characteristics rather than meaningful evidence of AI authorship."}, "questions": {"value": "1. Did you test alternative embedding models or similarity metrics (e.g., sentence-transformers or language-agnostic encoders) to ensure the Anchor method’s robustness across representations?\n\n2. Have you explored the “human revises AI draft” scenario, where an LLM-generated review is partially edited or rewritten by a human? How does the method perform under this mixed authorship condition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns listed above"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z7Z7kB2sRg", "forum": "HyZwf1rt4s", "replyto": "HyZwf1rt4s", "signatures": ["ICLR.cc/2026/Conference/Submission20658/Reviewer_9j4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20658/Reviewer_9j4M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332783427, "cdate": 1762332783427, "tmdate": 1762934046698, "mdate": 1762934046698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}