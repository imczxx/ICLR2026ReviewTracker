{"id": "C2RWqJ8OYL", "number": 2288, "cdate": 1757049671562, "mdate": 1763109434900, "content": {"title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "abstract": "The rapid advancement of educational applications, artistic creation, and AI-generated content (AIGC) technologies has substantially increased practical requirements for comprehensive Image Aesthetics Assessment (IAA), particularly demanding methods capable of delivering both quantitative scoring and professional understanding. Multimodal Large Language Model (MLLM)-based IAA methods demonstrate stronger perceptual and generalization capabilities compared to traditional approaches, yet they suffer from modality bias (score-only or text-only) and lack fine-grained attribute decomposition, thereby failing to support further aesthetic assessment. In this paper, we present: (1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main categories and 15 sub categories, each annotated by professional experts with 8-dimensional attributes analysis and a holistic score. Both the model and dataset will be made public to advance the field.", "tldr": "We introduce ArtiMuse, an MLLM-based Image Aesthetics Assessment (IAA) model that combines quantitative scoring and expert-level understanding, and ArtiMuse-10K, the first expert-curated dataset with 8 attribute annotations and holistic scores.", "keywords": ["multimodal; aesthetic assessment; image understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/17b12af669052391deb8fbc304146d698bc51008.pdf", "supplementary_material": "/attachment/9c10b19481c061c02695941e86f5d494be408b6c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ArtiMuse (a multimodal LLM for image aesthetics assessment) and ArtiMuse-10K (10k expert-annotated images with 8 fine-grained analyses + 0–100 scores). ArtiMuse uses two-stage training (text pretraining via LoRA, score finetuning via \"Token As Score\") and outperforms SOTA models in scoring/text analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly and coherently written.\n2. Its proposed ArtiMuse-10K dataset is meaningful for the IAA field."}, "weaknesses": {"value": "1. In Section 4.2 (lines 273-276), the paper uses a score-guided approach to generate aesthetic captions, which has been used in UniQA [1]. The paper does not properly cite this research.\n\n2. Experiments (a), (b), and (c) should discuss the impact of data size on the results. Images with score annotations may be numerous, so they have a greater impact on the results.\n\n3. I would like to know whether the first stage text training is effective for other methods in the second stage (Text As Score, Level-As-Score). This is related to the generalization performance of the proposed data.\n\n[1] UniQA: Unified vision-language pre-training for image quality and aesthetic assessment"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VxiCjCGXRa", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Reviewer_7pr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2288/Reviewer_7pr4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714747175, "cdate": 1760714747175, "tmdate": 1762916176059, "mdate": 1762916176059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We respectfully request to withdraw this submission. After internal discussion, our team decide to further extend and reposition the work, and therefore plan to submit an updated version to a more suitable venue. All co-authors have agreed to this withdrawal. We appreciate the consideration of the program committee."}}, "id": "f9mGEpzcpN", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763109433906, "cdate": 1763109433906, "tmdate": 1763109433906, "mdate": 1763109433906, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new Multimodal Large Language Model (MLLM) called ArtiMuse for Image Aesthetics Assessment (IAA). The model is capable of simultaneously achieving precise aesthetic scoring and expert-level, fine-grained textual understanding. To realize this, the authors also constructed the ArtiMuse-10K dataset and introduced the Token As Score strategy for continuous score prediction within MLLMs, which are inherently designed for discrete token generation. Experimental results show that ArtiMuse achieved state-of-the-art performance across multiple widely-used benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The expert-annotated dataset (ArtiMuse-10K), featuring professional annotations across many fine-grained attributes (e.g., composition, technical execution, creativity), addresses the issues of coarse granularity, and lack of expert guidance in existing IAA datasets.\n\n2. The ArtiMuse model successfully integrates quantitative scoring with qualitative interpretation.\n\n3. The Token As Score strategy resolves the inherent limitation of MLLMs in performing continuous score prediction by densely mapping existing tokens to a continuous value."}, "weaknesses": {"value": "1.Data Limitations and Distribution: Is a quantity of 10k sufficient, considering the high-dimensional nature of aesthetic assessment? The paper needs to clarify whether the data distribution across different categories is uniform or long-tail, and address the potential risks of data bias.\n\n2.Limited Potential for Foundation Model Enhancement: The fine-tuning process is focused solely on the score prediction task. The paper does not explore or demonstrate the capacity of this task to RL and positively enhance the general capabilities (e.g., reasoning, language understanding) of the foundational MLLM . This limits the perceived value of the fine-tuning beyond the specialized IAA task.\n\n3.Evaluation Reliability: The fairness and reliability of using a Multi-modal Large Language Model (AI) as the sole judge for the quality of aesthetic analysis require stronger theoretical justification. This AI-centric evaluation should be substantiated or replaced by a more comprehensive, large-scale traditional human assessment."}, "questions": {"value": "Answering questions about weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tEJqNgWeEo", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Reviewer_pXL5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2288/Reviewer_pXL5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835920253, "cdate": 1761835920253, "tmdate": 1762916175888, "mdate": 1762916175888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ArtiMuse, a multimodal large language model (MLLM)-based framework for comprehensive Image Aesthetics Assessment (IAA). It integrates joint scoring with expert-level interpretability to provide both holistic and fine-grained aesthetic understanding. Additionally, the authors present ArtiMuse-10K, a professionally curated dataset containing 10,000 images annotated across eight aesthetic dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tIt introduces an expert-annotated, multidimensional aesthetic dataset (ArtiMuse-10K).\n2.\tThe model, ArtiMuse, provides both holistic scores and fine-grained attribute analysis.\n3.\tThe model enhances interpretability and professional-level understanding.\n4.\tThe model addresses modality bias of prior score-only or text-only models."}, "weaknesses": {"value": "1.\tOne of the most puzzling aspects of this work is that it mixes different types of images into a single dataset. It remains unclear whether the learned “aesthetic understanding” truly reflects aesthetic principles or merely fits the dataset distribution. For instance, are the scoring standards for Children’s Paintings and Chinese Paintings the same? Although the model performs well during inference without explicit category labels, an additional experiment involving category classification would strengthen the argument.\n\n2.\tThe paper’s writing could be clearer — particularly regarding the purpose of constructing the 350K dataset and the details of the model training process.\n\n3.\tThe source of the eight aesthetic attributes is unclear. The appendix states they were specified by “domain experts,” but which specific domain are these experts from, and how authoritative are they?\n\n4.\tThe basis for category division is ambiguous. Categories such as Children’s Painting and Chinese Painting appear to follow inconsistent classification criteria.\n\n5.\tThe idea of “Token As Score” is interesting. However, according to the appendix, some selected tokens are numbers (0–9) while others are not. The former inherently preserve ordinal relationships, but the latter do not. Was there any targeted design to handle this inconsistency during training?\n\n6.\tIn Table 2, the model AesExpert, which underwent aesthetic training, performs worse than models without aesthetic training. Could the authors provide further analysis on this phenomenon?\n\n7.\tRegarding existing datasets, how were the rating scales across different datasets standardized or unified?\n\n8.\tHow was the test set in Table 2 constructed? Was ArtiMuse already trained on the 350K dataset prior to evaluation?"}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZHjaqsnQWq", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Reviewer_yo8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2288/Reviewer_yo8s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914723755, "cdate": 1761914723755, "tmdate": 1762916175686, "mdate": 1762916175686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ArtiMuse, a multimodal large model designed for Image Aesthetic Assessment (IAA), along with the creation of an expert-annotated dataset, ArtiMuse-10K, consisting of 10,000 images annotated across 8 aesthetic attributes. Experimental results show that ArtiMuse outperforms existing methods on multiple benchmarks and achieves significantly higher human preference scores in user studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper introduce a large-scale, expert-labeled dataset that provides valuable resources for advancing research in aesthetic image understanding.\n2.\tThe proposed *Token As Score* strategy is simple yet effective, enabling the model to handle continuous aesthetic scoring naturally.\n3.\tExtensive experiments across multiple datasets convincingly demonstrate the effectiveness and robustness of ArtiMuse."}, "weaknesses": {"value": "1.\tIncomplete review of related work. For example, AesExpert [1] provides a dataset of 21,904 images across three categories with multiple annotated aesthetic attributes. \n\n[1] Huang Y, Sheng X, Yang Z, et al. Aesexpert: Towards multi-modality foundation model for image aesthetics perception[C]//Proceedings of the 32nd ACM International Conference on Multimedia. 2024: 5911-5920.\n\n2.\tThe rationality of Token selection strategy needs further explanation. The paper proposes using double-letter combinations (e.g., “aa”, “ab”, …) as scoring tokens mapped to a continuous range of 0–100. While this *Token As Score* strategy is innovative, it remains unclear why double-letter combinations were chosen over other alternatives. Why not use numeric tokens, or explicitly defined score tokens instead? Moreover, do these tokens possess any latent semantic meaning in the pretrained vocabulary that could potentially bias the model’s predictions?\n\n3.\tThe paper claims the model generates expert-level aesthetic commentary, but only presents qualitative examples. Please add quantitative evaluations to systematically assess the generated text quality. \n\n4.\tCould you provide an inter-annotator agreement analysis? Given the inherent subjectivity in IAA, it is crucial for verifying the reliability of the dataset."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5HPvc9RVYs", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Reviewer_FnMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2288/Reviewer_FnMT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963792637, "cdate": 1761963792637, "tmdate": 1762916175504, "mdate": 1762916175504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ArtiMuse, a multimodal large language model for image aesthetics assessment, designed to provide both continuous scores and eight-dimensional textual analysis. It is supported by a new dataset, ArtiMuse-10K, annotated by experts, and a \"Token-As-Score\" strategy for continuous score prediction. While the work addresses a practical problem and the proposed components demonstrate competitive performance on several benchmarks, its contributions are constrained by significant methodological weaknesses in evaluation, questions about technical novelty, and concerns regarding data integrity and practical applicability, which limit its overall impact."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's problem definition is clear and highly relevant, addressing the growing demand for interpretable and quantitative Image Aesthetics Assessment (IAA) in fields like education, content creation, and AIGC quality control. The newly introduced ArtiMuse-10K dataset is a valuable contribution, offering superior coverage and annotation granularity with its five main categories, 15 subcategories, and eight expert-annotated aesthetic attributes, which is a significant improvement over existing resources that offer only scores or general comments. The proposed Token-As-Score strategy is a creative and efficient solution for continuous scoring that cleverly avoids the information loss associated with discretization by leveraging the LLM's existing tokenizer. Furthermore, the pragmatic two-stage training strategy effectively mitigates the common issue where fine-tuning for a scoring task degrades the model's textual generation capabilities."}, "weaknesses": {"value": "Insufficient evaluation rigor\nThe core claim is expert-level textual analysis, yet the assessment leans heavily on another LLM, Gemini-2.0-flash, as the judge. This setup invites evaluation bias, and the paper does not report how closely the LLM’s judgments align with human experts. Without agreement statistics or confidence intervals, the validity of the text-quality claims remains uncertain. The dataset story is also thin. There is no clear annotation protocol, no inter-rater reliability metrics such as Krippendorff’s alpha, and no description of how disagreements were resolved. These gaps weaken the case that the annotations are dependable.\n\nLimited technical novelty in scoring\nToken-As-Score is an appealing idea, but the evidence does not establish clear superiority. Comparisons focus on text-based or discrete-level scoring, while stronger continuous baselines are missing. Readers never see results for a direct numerical regression head or for distribution-based approaches such as Beta regression. Without these head-to-head tests, it is hard to judge the actual innovation and the size of the performance gains.\n\nRisk of data leakage and contamination\nTraining draws on large public datasets such as AVA, and evaluation is also conducted on these sources. The paper does not document a robust deduplication process or quantify overlap between training and test splits. If near duplicates or exact matches slip through, reported metrics may be inflated, which would blur the picture of true generalization.\n\nConstrained practicality and generalizability\nThe scoring scheme depends on the token order in the Qwen2.5-7B vocabulary, which complicates portability to other base models and tokenizers. The authors also acknowledge a practical shortcoming. The system explains but does not yet guide improvement. Without actionable suggestions that close the loop from critique to edit, the utility for real creative workflows remains limited."}, "questions": {"value": "To strengthen the work, detailed annotation protocols and inter-rater reliability (IRR) metrics should be reported for the ArtiMuse-10K dataset. The evaluation methodology should be enhanced by expanding the human study component and explicitly reporting the statistical agreement between human and LLM judges. In terms of the model, it is crucial to add experimental comparisons against strong continuous and distribution-based regression baselines to better situate the contribution of the Token-As-Score method. The authors should also conduct a thorough data leakage analysis and investigate the method's portability by testing its adaptation to other LLM families. Finally, future work could explore bridging the gap from analysis to suggestion, thereby creating a more complete and valuable tool for creative applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UQVC3H8tuG", "forum": "C2RWqJ8OYL", "replyto": "C2RWqJ8OYL", "signatures": ["ICLR.cc/2026/Conference/Submission2288/Reviewer_EX7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2288/Reviewer_EX7i"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977993080, "cdate": 1761977993080, "tmdate": 1762916175375, "mdate": 1762916175375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}