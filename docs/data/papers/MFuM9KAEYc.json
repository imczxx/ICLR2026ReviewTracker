{"id": "MFuM9KAEYc", "number": 17373, "cdate": 1758275177220, "mdate": 1759897179422, "content": {"title": "What Matters for Bioacoustic Encoding", "abstract": "Bioacoustics, the study of sounds produced by living organisms, plays a vital role in conservation, biodiversity monitoring, and behavioral studies. Many tasks in this field, such as species, individual, and behavior classification and detection, are well-suited to machine learning. However, they often suffer from limited annotated data, highlighting the need for a general-purpose bioacoustic encoder capable of extracting useful representations for diverse downstream tasks. Such encoders have been proposed before, but are often limited in scope due to a focus on a narrow range of species (typically birds), and a reliance on a single model architecture or training paradigm. Moreover, they are usually evaluated on a small set of tasks and datasets. In this work, we present a large-scale empirical study that covers aspects of bioacoustics that are relevant to research but have previously been scarcely considered: training data diversity and scale, model architectures and training recipes, and the breadth of evaluation tasks and datasets. We obtain encoders that are state-of-the-art on the existing and newly proposed benchmarks.\nWe also identify *what matters* for training these encoders, such that this work can be extended when more data are available or better architectures are proposed. Specifically, across 26 datasets with tasks including species classification, detection, individual ID, and vocal repertoire discovery, we find that self-supervised pre-training followed by supervised post-training on a mixed bioacoustics + general-audio corpus yields the strongest in- and out-of-distribution performance. We show the importance of data diversity in both stages. To support ongoing research and application, we will release the model checkpoints.", "tldr": "A comprehensive cross-taxa evaluation of general-purpose audio encoders (19 models) on bioacoustics benchmarks (26 datasets)", "keywords": ["bioacoustics", "evaluation", "benchmarks", "audio", "sound", "classification", "detection", "clustering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6df85cf6a1e32831644ee5171111ea31f8b099fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a large-scale empirical study to identify \"what matters\" for training bioacoustic encoders. It investigates the impact of model architectures, training data composition (bioacoustics-only, general audio, and mixed corpora), and training paradigms (SL, SSL /pre-training and post-training recipes). The authors collect a large-scale pre-training dataset and evaluate on an extended evaluation set. Their key finding is that a recipe of self-supervised pre-training on a mixed bioacoustic and general audio dataset, followed by supervised post-training, yields best performing models in- and out-of-distribution tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Comprehensive and valuable empirical study**: The primary strength of this work is its ambition and scope. The paper tackles an important question in the bioacoustics community by systematically comparing different modeling components. This kind of large-scale, well-structured empirical study is valuable for future research.\n- **Expanded evaluation framework:** The authors curate and introduce a broader evaluation benchmark.  The paper moves beyond species classification and includes tasks like individual ID and vocal repertoire discovery. It also augments existing benchmarks with clustering and retrieval metrics, providing a potentially more realistic assessment of an encoder's performance.\n- **Actionable insights:** The paper provides actionable results for training robust bioacoustic encoders. The finding that a combined self-supervised and supervised approach on a diverse data mix yields the best overall performance is an important insight."}, "weaknesses": {"value": "While the findings of the paper are valuable, its contribution is limited by its presentation, methodological choices/novelty, and questions about its reproducibility. A short disclaimer: Given that the paper's contribution is not methodological, the focus necessarily shifts to the quality and execution of its empirical investigation which poses some questions.\n\n**[Presentation and readability]**\n\n- Clarity of figures and tables: The presentation of results could be significantly improved. Figure 2 is difficult to read due to its small size. The tables, particularly Tables 2, 3 and 4, use a complex system of abbreviations that makes them hard to parse and compare models at a glance. A more structured in-table organization with sub-information could help here.\n- Lack of structure in text: Long sections of text, such as the Related Work and Proposed Models or Evaluation Setup sections, would benefit greatly from some kind of sub-headers. This would improve readability and help the reader navigate the information being presented.\n- Formatting issues: A few minor formatting issues exist, such as Figure 1 being over the text height, placeholders like \"ADD REF TO AGILE PAPER\" (L.316), and Figure 2 being very hard to read.\n\n**[Methodological Choices]:**\n\n- Dataset and model selection: The paper lacks a clear motivation for the specific choice of datasets in the training corpus and the selection of models for comparison. For example, the rationale for choosing BEATs, for which the official training implementation is unavailable, over a more recent and reproducible open-source model like SSLAM from last year’s ICLR is not provided.\n- Incomplete baselines: While the study includes many models, it omits direct comparisons with SSL bioacoustic models mentioned in the related work. Including these would provide a more complete picture and better contextualize the performance of the proposed recipes as the respective baselines.\n\n**[Evaluation details and reproducibility]:**\n\n- Lack of statistical and methodological detail:  It is unclear how the \"win-rate\" in Figure 2 is calculated or whether any statistical significance testing  (or repetitions) was performed to validate the results. Furthermore, the process for hyperparameter selection across the many models and training recipes is not described, making it difficult to assess the fairness of the comparisons.\n- Limited probing techniques: The evaluation relies exclusively on linear probing. While standard, recent work in self-supervised learning in computer vision has shown attentive probing can provide a more nuanced view of an embedding's quality, and its omission here is a missed opportunity. If there were more methodological contributions, this would not be a problem but it would greatly strengthen the contribution of an empiricial study.\n- No code or release plan: For a study of this complexity, reproducibility is important, but there is no code available during the review process. The promise to release checkpoints \"upon paper acceptance\" is not enough. There is also no clear plan for releasing (if it happens) the curated datasets, splits, and evaluation protocol to establish this work properly."}, "questions": {"value": "1. Could you provide a clearer rationale for your selection of training datasets and baseline models? Specifically, why was (the older) BEATs chosen over other reproducible open-source SSL models?\n2. Could you clarify some details of the evaluation? (a) How is the \"win-rate\" metric in Figure 2 formally defined and calculated? (b) Were statistical significance tests (or the mean of some repetitions) performed to validate the superiority of your proposed recipe? (c) Why was only linear probing used and not other methods that might better utilize the learned representations (and change results)?\n3. To ensure its impact and allow for verification, can you provide access to the code for the review process? Furthermore, what is your concrete plan for releasing not just the model checkpoints, but (maybe) also the curated data splits or the full evaluation framework to the community?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xwFRQLjvUi", "forum": "MFuM9KAEYc", "replyto": "MFuM9KAEYc", "signatures": ["ICLR.cc/2026/Conference/Submission17373/Reviewer_ykFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17373/Reviewer_ykFi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740422917, "cdate": 1761740422917, "tmdate": 1762927284532, "mdate": 1762927284532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the factors that contribute to effective bioacoustic\nencoding by evaluating design choices like data sources, model architectures and\ntraining strategies. The authors therefore conduct an empirical study comparing\nmultiple pre-training datasets (general audio recordings with AudioSet, birds\nfrom Xeno-Canto, Sapsucker Woods and WABAD, diverse animal sounds from\niNaturalist and Animal Sound Archive, marine mammals from Watkins), model\narchitectures (EfficientNet, ViT) and training strategies (supervised pre- and\npost-training, self supervised pretraining with EAT). Evaluation is carried out\non the BEANS and BirdSet benchmark and two additional tasks of individual\nidentification and vocal repertoire discovery. Generally, models benefit from\ndiverse pretraining data (also including AudioSet and not only bioacoustic\ndata). In addition, supervised posttraining improves performance for\nSSL-pretrained models. The authors wrap these findings in a recipe for post-training SSL-backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Research topic is relevant and motivated well.\n- Contributions are clearly stated.\n- Experimental design is comprehensive, covering multiple datasets,\n  architectures, and training strategies.\n- Results are novel but not entirely surprising."}, "weaknesses": {"value": "- For a systematic comparison of model architectures and training paradigms the study lacks SSL models based on CNN architectures and SL models based on ViTs. This would help to better disentangle the effects of architecture and training paradigm.\n- To simplify from the results of one SSL-pretrained model (EAT) to SSL in general is a bit of a stretch. Including more SSL training techniques would strengthen the claims.\n- The evaluation protocol only considers the \"clip\" embeddings and leaves the patch embeddings for future work. [1,2] shows that the general audio models compare better when using patch embeddings with attentive probing.\n- BirdMAE as a strong bioacoustic SSL baseline is missing in the comparison.\n- The 2 and 3.4 would benefit from paragraph headings to improve readability.\n- Glitch in l. 316\n\n[1] Can Masked Autoencoders Also Listen to Birds? https://arxiv.org/abs/2504.12880\n[2] Foundation Models for Bioacoustics -- a Comparative Review\nhttps://www.arxiv.org/abs/2508.01277"}, "questions": {"value": "- How were the hyperparameters selected? Why are different LR used for BEATs and EAT? What HP are used for linear probing?\n- How many seeds were used for the experiments? What are the standard deviations?\n- Which BEATs checkpoint was used?\n- Do you plan to release the training and evaluation code?\n- Analysis of the \"curation level\" of the datasets would be interesting as initially analyzed in [1]. How many samples per class are needed, how diverse should the data be? Have you considered analyzing this aspect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DgDwojDxXZ", "forum": "MFuM9KAEYc", "replyto": "MFuM9KAEYc", "signatures": ["ICLR.cc/2026/Conference/Submission17373/Reviewer_WxmL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17373/Reviewer_WxmL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848855242, "cdate": 1761848855242, "tmdate": 1762927284125, "mdate": 1762927284125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on building strong bioacoustic embedding models that can be used for a wide range of ecological applications including species ID, individual ID and repertoire discovery. The authors explore supervised and self-supervised methods with a focus on data diversity and scale to suggest a training recipe that works well for bioacoustics tasks, particularly when transferring to novel domains. They benchmark existing methods extensively across 26 datasets, different model architectures, training recipes and data distributions. Part of this benchmarking are the individual-ID and repertoire discovery tasks for which the authors propose newly curated evaluation datasets. Based on their results, the authors recommend training recipes and best practices for learning strong bioacoustic embeddings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper includes extensive experiments with multiple model architectures, different training recipes and a number of evaluation tasks, both well established and newly proposed.   \n2. The paper presents a strong benchmark unifying different existing approaches. Different works often follow different evaluation strategies in bioacoustics; and the baselines and evaluation protocol in this paper could serve as a good standardized benchmark and reference for future work.   \n3. The architecture, pre/post-training recipes and design choices for evaluation tasks are sufficiently detailed and well-motivated.  \n4. The newly curated tasks for individual ID and repertoire discovery are quite interesting and can help understand the ability to capture details at finer granularities than the already fine-grained species identification."}, "weaknesses": {"value": "Overall, I believe this work has the potential to be a strong contribution to the community and commend the authors’ effort. However, I think there is still room for improvement in several key areas before publication. \n\n\n**\\[Novelty\\]**\n\n1. The technical novelty of the paper is relatively low. Separately, self-supervised learning has been explored by BirdMAE, mixing bioacoustic data with general audio data has been explored by Perch 2.0 \\[1\\] (although this is more of a concurrent work), and effects of including non-avian species have also been explored by iNatSounds and SurfPerch. \n\n**\\[Standardized benchmarking\\]**\n\n2. For BirdSet, if I understand correctly, the authors use train splits for each datasets and learn linear probes. Given that the training dataset includes XenoCanto, I suspect that most if not all species in BirdSet tasks should be a part of the training dataset. Can the authors also report performance with the classifier head learnt during post-training? I believe the latter was used by the BirdSet authors, and a consistency in evaluation can be helpful for future work. These extra numbers could just go in the supplementary if the authors would like to preserve the current flow.  \n\n3. For BEANS classification tasks, if I understand correctly, the authors use “time-averaged embeddings”, but BEANS benchmark uses the first window only. I think the authors’ choice is more reasonable, but again argue in favour of consistency in evaluation. I understand that the baseline results presented by the authors are consistent with each other, but it would be helpful for future work to have easily comparable results.   \n4. For BEANS classification tasks, looking at the supplementary table 8, it seems like the authors include ESC-50, but not speech commands. This choice affects the mean-BEANS metrics and should be justified in the main text. ESC-50 is also missing in Table 5-6. \n\n**\\[Results\\]**\n\n5. All rows in Table 3 are repeated as-is in Table 4\\. This seems unnecessary to me. I ask that the authors either keep them in a single table or remove the rows from Table 4\\. I would recommend the former. I understand the benefit of the distinction of these results, but perhaps the authors can use a different background color or some other way to highlight the Table 3 numbers.   \n6. The authors choose to use WABAD as part of the training in an ablation. Since this is a densely labelled soundscape dataset, I believe this could be more useful as an evaluation dataset instead.   \n7. In Table 8, I was looking at “sl-BEATs-bio” and “sl-BEATs-all”. It seems like using AudioSet helps with ESC-50 considerably. Given the smaller aggregate improvement in Table 4, I take this to mean that including Audioset actually worsened the performance for the aggregate of the rest of BEANS classification tasks, which are the actual bioacoustics tasks. This seems counter to the claims of the benefit of training on general audio. The considerable improvement in ESC-50, which contains environmental sounds, may be partly due to similar sounds in AudioSet and may actually be misleading as an evaluation for bioacoustics tasks.    \n8. I am unclear about the motivation for Table 2b. As per my understanding, while the BEANS classification tasks have focal recordings, they still have a domain shift with regards to the species being considered. For example, aquatic species in watkins and insects in HumbugDB. The detection tasks also have this “label shift”, but for different species. To me, this is a confounding factor and I am unclear if the difference in behavior for the two can be attributed to the focal vs soundscape domain shift. Following the previous point, if ESC-50 is included in classification tasks, this is another confounding factor when assessing the effect of adding AudioSet. \n\n**\\[Minor Points\\]**\n\n9. Elements of the paper are going out of the margin in a few places (Table 2, Fig 4 and 5\\)  \n10. Fig 2: font size of text is too small  \n11. Line 316 “ADD REF TO AGILE PAPER” :)   \n12. Line 213: I believe Perch is trained from scratch instead of starting from ImageNet. Not critical to the discussion, but may be worth noting.    \n13. The notation in Table 4 is confusing. What is the prefix “sl-E…” in sl-EAT-all$^{SL-SSL}$?  \n14. Notation (superscripts) is different in Table 8 vs Table 4\n\n**References**\n\n\\[1\\] van Merriënboer, Bart, et al. \"Perch 2.0: The Bittern Lesson for Bioacoustics.\" arXiv preprint arXiv:2508.04665 (2025)."}, "questions": {"value": "1. I am curious about the Individual ID and Vocal repertoire discovery tasks. In the post training stage, different call types and individuals of the same species should have been assigned the same label, so I imagine the model should be learning similar embeddings for different individuals and calls of the same species. How do the authors explain this “emergent” ability? Is self-supervised learning the main contributor? Although for Individual ID, Perch and BirdNET seem to be doing on par and maybe even better without any self-supervision.   \n2. The takeaways about the benefits of self-supervised learning are counter to the observations in Perch 2.0 \\[1\\]. Could the authors suggest possible factors to explain this difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QCNBbBBmPB", "forum": "MFuM9KAEYc", "replyto": "MFuM9KAEYc", "signatures": ["ICLR.cc/2026/Conference/Submission17373/Reviewer_B3CZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17373/Reviewer_B3CZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940885127, "cdate": 1761940885127, "tmdate": 1762927283585, "mdate": 1762927283585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is an empirical study of bioacoustic encoder. The paper explores several benchmarks and training configurations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The overall goal of studying bioacoustics is fascinating and important. I think it is a great application domain for the NeurIPS community."}, "weaknesses": {"value": "The contribution is interested by limited in several ways. The reported results will be interesting for the community working on this area, but might not be interesting for a wider set of researchers. An experimental paper would need to introduce challenges derived from bioacustics that can motivate technical advances, or introduce a new dataset, or a better definition of the task with new evaluation metrics, or solving a new problem. But the current contribution is limited along all those dimensions. \n\nThe technical novelty is limited. \n\nSome of the results are not very surprising such as\n- adding general audio into the training significantly improves model transferability\n- self-supervised models under-perform supervised models\nBoth those results are not unexpected. \n\nThe approach uses standard evaluation metrics (linear probing, retrieval, and clustering) and tasks (classification and detection) used by the community. Although some of those are extensions of existing benchmarks, on its own are a limited contribution."}, "questions": {"value": "Would it be possible to better articulate if there are contributions along these questions and what those contributions are?\n- what new technical advances are needed? \n- Is there a new dataset being introduced?\n- Is there a better definition of the task being proposed? \n- Are there new evaluation metrics being introduced?\n- Is there a new problem/task being introduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5MGQc0c6G", "forum": "MFuM9KAEYc", "replyto": "MFuM9KAEYc", "signatures": ["ICLR.cc/2026/Conference/Submission17373/Reviewer_Jxj8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17373/Reviewer_Jxj8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17373/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095376818, "cdate": 1762095376818, "tmdate": 1762927283127, "mdate": 1762927283127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}