{"id": "dS8yBzHV70", "number": 21328, "cdate": 1758316286875, "mdate": 1759896928291, "content": {"title": "Learning to Interrupt in Language-based Multi-agent Communication", "abstract": "Multi-agent systems using large language models (LLMs) have demonstrated impressive capabilities across various domains. However, current agent communication suffers from verbose output that overloads context and increases computational costs. Although existing approaches focus on compressing the message from the speaker side, they struggle to adapt to different listeners and identify relevant information. An effective way in human communication is to allow the listener to interrupt and express their opinion or ask for clarification. Motivated by this, we propose an interruptible communication framework that allows the agent who is listening to interrupt the current speaker. Through prompting experiments, we find that current LLMs are often overconfident and interrupt before receiving enough information. Therefore, we propose a learning method which predicts the appropriate interruption points based on the estimated future reward and cost. We evaluate our framework across various multi-agent scenarios, including 2-agent text pictionary games, 3-agent meeting scheduling, and 3-agent debate. Experiment results show that our HANDRAISER can reduce communication cost by 32.2% compared with the baseline with a comparable or superior task performance. Such learned interruption behavior can also generalize to different agents and tasks.", "tldr": "", "keywords": ["Multi-agent System", "Large Language Model", "Communication Efficiency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61c5e5df5eeb1839bccb1d05d5b3aa128ce3d6a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HANDRAISER, an interruptible communication framework for large language model (LLM)-based multi-agent systems. Unlike previous speaker-oriented compression approaches, it introduces a listener-oriented mechanism that allows agents to interrupt speakers dynamically during message generation to reduce redundant communication. The method combines a chunk-based streaming protocol with a learning strategy that predicts optimal interruption points based on expected task rewards and communication costs. Experiments across three scenarios—textual Pictionary, meeting scheduling, and multi-agent debate—show that HANDRAISER significantly reduces communication cost while maintaining comparable task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper tackles a valuable and interesting problem. To address the issues of communication redundancy, context overload, and high reasoning cost in current multi-agent systems, this paper introduce a listener-oriented optimization framework. This perspective is distinct from the conventional speaker-oriented compression paradigm and is conceptually insightful.\n\n2. The paper further proposes a simple but efficient method to realize interruptible communication."}, "weaknesses": {"value": "1. My main concern is that the evaluation environments appear task-specific rather than representative of a general, open multi-agent system.\nAcross the three evaluated tasks, the interrupter’s role is more akin to a “user” or “supervisor” rather than an autonomous agent within the system. This one-directional communication topology makes the experiments resemble human–agent interaction more than multi-agent collaboration, where dialogue participants have symmetric communication rights.\nTherefore, the reported results mainly demonstrate the effectiveness of a single-listener interruption mechanism, rather than that of a fully scalable multi-agent communication framework.\n2. The framework lacks validation in general multi-agent settings that allow multi-directional interruption, where agents can act as both speakers and listeners. Its stability and scalability in open environments (e.g., GAIA, AIME) therefore remain uncertain. As a result, the paper reads more like a conceptual proposal introducing an interesting idea, but lacks sufficient empirical evidence to demonstrate its effectiveness in general multi-agent systems."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YAH6y8nfV8", "forum": "dS8yBzHV70", "replyto": "dS8yBzHV70", "signatures": ["ICLR.cc/2026/Conference/Submission21328/Reviewer_w2uu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21328/Reviewer_w2uu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961249018, "cdate": 1760961249018, "tmdate": 1762941698374, "mdate": 1762941698374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an interruptible communication framework for multi‑agent systems using large language models (LLMs). Current agent communication is verbose and offloads the entire message from the speaker, increasing context and computational costs. Existing methods compress messages from the speaker side but do not allow listeners to control information flow. Inspired by human conversations, the authors introduce a framework in which a listening agent can send an interruption signal to stop the current speaking agent when the listener has enough information or needs clarification.  Initial prompting experiments show that vanilla LLMs tend to interrupt prematurely, so the paper proposes a learning‑based controller called HANDRAISER that predicts optimal interruption points based on estimated future reward (e.g., task performance) and cost (communication tokens). The controller is trained from multi‑agent rollouts with sampled interruptions to estimate the payoff of interrupting versus not interrupting.  HANDRAISER is evaluated on diverse multi‑agent tasks, including 2‑agent text pictionary games, 3‑agent meeting scheduling, and 3‑agent debates. Results indicate that the framework reduces communication cost by around 32 % relative to a baseline while maintaining or improving task performance. The learned interruption behavior generalizes across different agents and tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel notion of interruptible communication in multi-agent LLM systems, shifting the control of information flow from the speaker to the listener. This is a fresh angle compared to prior work that focuses on compressing speaker output.  \n- The HANDRAISER controller is derived from a principled objective that trades off future task reward versus communication cost, and is trained on sampled rollouts to estimate the payoff of interruptions. The authors perform ablations to show that naive prompting-based interrupts make LLMs overconfident and the learning approach improves efficiency.  \n- Efficient communication is a practical bottleneck in multi-agent systems using large LLMs. Demonstrating a ~32 % reduction in communication cost on several tasks while maintaining or improving performance suggests the method could meaningfully reduce computational cost for multi-agent coordination.  \n- The paper is generally well written, with a clear problem statement, illustrative figures of the protocol (e.g., pictionary example), and detailed experimental setup. The authors carefully motivate the need for listener-driven interruptions and describe the training procedure, baselines, and metrics."}, "weaknesses": {"value": "- The experiments are limited to small toy scenarios like text Pictionary, simplified meeting scheduling, and controlled debates. It would strengthen the paper to include more realistic multi-agent tasks or to scale up the number of agents to demonstrate robustness.  \n- The work compares mainly to a baseline of no interruptions. It would be valuable to benchmark against existing communication-efficiency techniques such as message compression, summarization, or reinforcement-learning‑based communication protocols.  \n- The paper claims that vanilla prompting leads to premature interruptions. However, more in‑depth analysis of why this happens and under what conditions HANDRAISER might still make suboptimal decisions would be helpful. For example, does the learned controller sometimes interrupt too late, affecting task quality?  \n- The proposed controller is trained using specific tasks and reward structures. It is unclear how sensitive the learned policy is to the choice of reward weights or whether it can transfer to other domains without retraining. Some discussion or experiments on generalization would improve the work."}, "questions": {"value": "- Could the authors clarify how the payoff function is defined and whether it requires task‑specific tuning (e.g., weighting communication cost versus task reward)? How sensitive is the method to these hyperparameters?  \n- Is the HANDRAISER controller trained jointly across all tasks or separately per task? If trained jointly, how does it avoid overfitting to one task's dynamics?  \n- In the debate scenario, how is the quality of arguments evaluated? Is there any human evaluation to ensure that interruptions do not degrade content quality?  \n- Have the authors considered combining their approach with speaker‑side compression or summarization methods? A hybrid approach might yield further improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kF8v9xyrIM", "forum": "dS8yBzHV70", "replyto": "dS8yBzHV70", "signatures": ["ICLR.cc/2026/Conference/Submission21328/Reviewer_j9eZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21328/Reviewer_j9eZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761259228551, "cdate": 1761259228551, "tmdate": 1762941697996, "mdate": 1762941697996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of communication inefficiency in LLM-based multi-agent systems, where verbose messages increase computational costs and context overload. Instead of the common \"speaker-oriented\" approach of compressing messages, the authors propose a novel \"listener-oriented\" framework where listener agents can interrupt speakers. The authors first demonstrate that naive, prompting-based interruption fails because LLMs are \"overconfident\" and interrupt prematurely, often harming task performance. To solve this, they propose HANDRAISER, a model trained to predict optimal interruption points. This model is finetuned on data labeled by estimating the future task reward and communication cost of potential interruptions via tree-based sampling. Experiments across three distinct multi-agent tasks, including Text Pictionary, Meeting Scheduling, and MMLU-Pro Debate, show that HANDRAISER significantly reduces communication costs by 23-49% while maintaining or improving task success rates compared to both non-interruptible and naive interruption baselines."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its novel and intuitive \"listener-oriented\" interruption framework, which is a departure from standard speaker-side compression. A significant contribution is the paper's clear diagnosis of why naive, prompting-based interruption fails, providing strong empirical evidence that LLMs are \"overconfident\" and interrupt prematurely, often to their own detriment. This core finding is validated through a thorough evaluation on three diverse multi-agent tasks, demonstrating that the learned HANDRAISER model significantly outperforms strong baselines."}, "weaknesses": {"value": "1. The data-labeling process relies on tree sampling to estimate the future cost and performance of an interruption. However, these future rollouts are conducted using a random interruption policy. This seems like a potentially high-variance and inaccurate estimator for the true value of an interruption, which should depend on optimal or near-optimal future actions, not random ones. But the paper did not provide further details of the choice or analyze what impact it will have.\n\n2. The paper claims there are reductions in \"communication cost\", usually measured in tokens, and \"latency\". However, the proposed framework introduces a new computational cost. The listener must run an inference step for the HANDRAISER model at every received chunk. This inference overhead (network + compute altogether) is not measured. In a real-world scenario, it's not very clear whether running 10+ small classifications is actually faster or cheaper than simply processing 30 extra tokens from the speaker.\n\n3. The ablation study in Table 2 indicates that the learned behavior can be used on other tasks, however the performance and cost reductions of different tasks are always lower than those of the original training tasks.  This indicates that the acquired \"interruption\" skill is highly task-specific and not a universally applicable behavior, thus constraining the method's \"plug-and-play\" functionality in novel, unfamiliar contexts."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JUetkhntBC", "forum": "dS8yBzHV70", "replyto": "dS8yBzHV70", "signatures": ["ICLR.cc/2026/Conference/Submission21328/Reviewer_kzej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21328/Reviewer_kzej"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842399344, "cdate": 1761842399344, "tmdate": 1762941697607, "mdate": 1762941697607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses the issues of \"poor adaptability in speaker-oriented compression\" and \"LLMs' tendency to be overconfident when interrupting directly\" in LLM-based multi-agent communication. It proposes an interruptible communication framework, HANDRaiser, which allows the listener to receive the speaker’s message in fixed-sized chunks, estimates future cost and performance via tree sampling to identify reasonable interruption points, and learns the optimal interruption timing through supervised fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes an interruptible multi-agent communication framework that allows the listener to actively interrupt the speaker during the conversation. This is an important improvement over the traditional “wait-to-finish” model. Introducing the human-like “interruption” mechanism into LLM-based multi-agent systems is both realistic and inspiring.\n2. The motivation and problem scope are clear: the paper clearly identifies a practical and timely bottleneck in multi-agent LLM systems—verbosity leading to context overload and computational cost. The focus on listener-initiated interruption is well-justified both conceptually and empirically."}, "weaknesses": {"value": "1. The method is computationally complex. The tree sampling and trajectory annotation processes incur high computational costs, especially in multi-agent, multi-turn settings, which may limit its deployment in real-world systems.\n2. The task setup is somewhat idealized: only one agent is allowed to interrupt in the experiments, avoiding the complex scenario of “cascading interruptions.” However, this may not hold in real multi-agent environments. All tasks have clear termination conditions (e.g., guessing the word, scheduling meetings, voting), and it remains unclear whether the method applies to open-domain tasks."}, "questions": {"value": "1. What happens in scenarios where multiple agents are allowed to interrupt? Have you considered, simulated, or conducted rollouts in settings with potential cascading or conflicting interruptions?\n2. Would the authors consider conducting human evaluations of communication naturalness, coherence, or user satisfaction to further demonstrate the “human-like” nature of the proposed framework?\n3. Can the authors provide a specific analysis or empirical results on the computational cost and scalability of the tree sampling process, especially as the number of agents or chunk granularity increases? How does inference time change in longer or more intensive communication protocols?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bATfig9FMH", "forum": "dS8yBzHV70", "replyto": "dS8yBzHV70", "signatures": ["ICLR.cc/2026/Conference/Submission21328/Reviewer_w8hb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21328/Reviewer_w8hb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989675294, "cdate": 1761989675294, "tmdate": 1762941697214, "mdate": 1762941697214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}