{"id": "RhGT2HdYnS", "number": 12624, "cdate": 1758209085469, "mdate": 1763766076040, "content": {"title": "Transformers tend to memorize geometrically; it is unclear why.", "abstract": "In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored.\nWe begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences\nspecified during training. Instead, the model must have somehow synthesized its\nown geometry of atomic facts, encoding global relationships between all entities,\nincluding non-co-occurring ones. This in turn has simplified a hard reasoning\ntask involving an ℓ-fold composition into an easy-to-learn 1-step geometric task.\n\nFrom this phenomenon, we extract fundamental aspects of neural embedding\ngeometries that are hard to explain. We argue that the rise of such a geometry,\ndespite optimizing over mere local associations, cannot be straightforwardly\nattributed to typical architectural or optimizational pressures. Counterintuitively,\nan elegant geometry is learned even when it is not more succinct than a brute-force\nlookup of associations. Then, by analyzing a connection to Node2Vec, we\ndemonstrate how the geometry stems from a spectral bias that—in contrast to\nprevailing theories—indeed arises naturally despite the lack of various pressures.\nThis analysis also points to practitioners a visible headroom to make Transformer\nmemory more strongly geometric. \n\nWe hope the geometric view of parametric\nmemory encourages revisiting the default intuitions that guide researchers in areas\nlike knowledge acquisition, capacity, discovery and unlearning.", "tldr": "Transformers can solve an in-weights path-finding task where they failed in-context by implicitly organizing local facts into a coherent geometric structure, challenging the theory that they store knowledge as simple associations.", "keywords": ["Next-Token Prediction", "Memorization", "In-Context", "In-Weights"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f455601a3642e554c92e0c7b041124dbe8dec0f9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper implies a geometric form of memory in Transformer to encode global structure information rather than only local associations. Experiment results demonstrate promising results"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is well written and easy to follow\n2. Experiment is good and result is encouraging"}, "weaknesses": {"value": "I have my concern in the IN-WEIGHTS PATH-STAR TASK experiment. In this experiment, there is some information leakage, since training and testing is done in the same graph. In order to claim \"IN-WEIGHTS REASONING IS LEARNED\", testing should be done in new graph (with new graph topology), or with permutated labels for the node."}, "questions": {"value": "1. see above weakness\n2. In section 4, spectral analysis is mainly done in Node2vec, any connection with Graph Laplacian for the Transformer side?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P7NDAWkEft", "forum": "RhGT2HdYnS", "replyto": "RhGT2HdYnS", "signatures": ["ICLR.cc/2026/Conference/Submission12624/Reviewer_2EuG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12624/Reviewer_2EuG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494570347, "cdate": 1761494570347, "tmdate": 1762923472533, "mdate": 1762923472533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to common concerns"}, "comment": {"value": "We thank the reviewers for their feedback.  We have updated the paper with new experiments and improved presentation. Below, we clarify the common criticisms raised. \n\n---\n\n### **On the lack of a (clear) answer**\n\nMultiple reviewers are concerned that we do not give a clear answer to the problem we have posed (`UZ7j, eCS4`). We understand that a lack of an answer can be dissatisfying. However:\n\n**Lots of good research is about asking the right questions, which can help steer the community into new lines of inquiry—this is the main purpose of our work.** \n\n- Our paper highlights a new contrast between modes of memorization. \n- It formulates a foundational **memorization puzzle**: _why does the gradient-descent-trained model memorize via a geometry instead of a lookup table, even when the geometry is not inherently more succinct in bit complexity, and even with just local supervision?_\n- We've not just asked a question, but also gone over several natural explanations to this phenomenon, and provide careful experiments/theoretical justifications to rule them all out (`Section 3`). \n\nSuch a contribution can be widely influential, such as in papers like “understanding deep learning requires rethinking generalization” [Zhang et a., 2017 https://arxiv.org/abs/1611.03530). They posed  “the generalization puzzle”, but they did not solve it, and this challenge was taken up by 1000s of papers. Analagously, we pose a “memorization puzzle in sequence modeling” which we hope will inspire many new lines of theoretical inquiry, and also practical work improving Transformer geometry.\n\n-----\n\n### **The simplicity of the Node2Vec model is a strength**\n\nWe understand that the lack of an attention layer in our Node2Vec analysis appears too simplistic on the face of it (`UZ7j 9RrG, 2EuG`). Let us clarify why, on the contrary, this simplicity is a strength: it cleanly isolates the source of the geometries we see.\n\n- First, we have added **new experiments** which show similar geometries in Mamba and neural networks. Thus attention layers are not critical to the geometry. (`E.1 and Fig 12-15`)\n- The node2vec model is simply a Transformer model with the same loss, same dataset, but without the intermediate attention/MLP layers (as noted in `Sec 4 para 1`). Thus, our analysis implies that **there is a spectral bias which is fundamental to any gradient-descent-trained deep sequence model on local co-occurrences.** \n- Third, an analysis for even such simple node2vec style models on cross-entropy loss has been a highly non-trivial long-standing open question. So far this has only been studied in settings even simpler than ours, such as on a simpler loss, or with explicit rank constraints (See `Related Work Sec 5`).\n\n----\n\n### **The  “Hypothesis”, “Observation” style of presentation**\n\nReviewer `9RrG` understandably finds our presentation style confusing since we put forth hypotheses only to rule them out later on. We must clarify that our “Hypotheses” must be viewed as as “Candidate Hypotheses”, based off of existing intuitions, that may or may not be true. Our intention was to ensure simple explanations are ruled out first, which is the nature of our result here."}}, "id": "hWe5oRPggn", "forum": "RhGT2HdYnS", "replyto": "RhGT2HdYnS", "signatures": ["ICLR.cc/2026/Conference/Submission12624/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12624/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12624/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763769002754, "cdate": 1763769002754, "tmdate": 1763770480187, "mdate": 1763770480187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Transformers memorize and reason over graph-structured data stored in their weights. The authors present a \"path-star\" graph task where, contrary to expectations, the model succeeds at finding \"in-weights\" paths. They posit this success is due to the emergence of \"geometric memory\" (where embeddings encode global graph structure) rather than \"associative memory,\" or a simple lookup of local facts. The authors argue that the emergence of this geometric structure is not explained by standard pressures (like simplicity bias) and hypothesize that it stems from a \"spectral bias,\" which they explore in the context of a simpler Node2Vec model."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an interesting phenomenon which explores in-context learning and an alternative form of memorization via their experimentation of the path-star task.\n\n\n2. The paper articulates the difference between \"associative memory\" (local, pairwise) and \"geometric memory\" (global, structural), offering a helpful lens for analyzing how models store knowledge."}, "weaknesses": {"value": "1. The main explanation for the phenomenon (spectral bias) is explored only in a simpler Node2Vec model. The paper fails to provide a convincing experimental or theoretical link to show that this same mechanism is responsible for the results observed in the Transformer.\n\n2. The visual evidence presented for the geometric memory in the Transformer is unconvincing. The UMAP plots (e.g., Fig 3b, Fig 10) are messy and do not show the clear, well-separated clustering that the text claims, forcing the argument to rely on heatmap interpretations.\n\n3.  The paper sets up a compelling puzzle (success in Transformers) but then pivots entirely to a different model (Node2Vec) for a solution. This jump makes the overall argument feel disconnected and leaves their central question ultimately unanswered and confusing."}, "questions": {"value": "1. Why should we assume that the spectral bias observed in a 1-layer Node2Vec model is the primary mechanism at play in a complex, multi-layer Transformer? Could the observed geometric memory in Transformers be an emergent property of the attention mechanism?\n\n2. Could the usage of standard sequential positional encoding undermine the claim of geometric memory? The model is learning a node's position in a sequence, not its inherent position in a graph, which seems to reinforce the very \"associative\" view the paper claims to refute.\n\nSide note: \n1. Your Fig. (1) has a weird error. The caption \"Node2Vec Associative Memory...\" on the left orange figure has overlapping texts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2kr18BUua1", "forum": "RhGT2HdYnS", "replyto": "RhGT2HdYnS", "signatures": ["ICLR.cc/2026/Conference/Submission12624/Reviewer_eCS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12624/Reviewer_eCS4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514797164, "cdate": 1761514797164, "tmdate": 1762923471982, "mdate": 1762923471982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an in-weights variant of the path star task, where the goal is to memorize a star graph topology (in existing approaches the graph varies and is not memorized). Similarly to existing approaches, the model is trained on paths  from leafs to the center node and is evaluated on paths from the center to the leaves. Notably, in this setting the model generalizes to the test case (this is not the case in the  original path-star task). The authors investigate reasons for why this happens and argue that transformer memory works geometric."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-  **(S1) Novelty,  Contribution:**  The in-memory star-path task is novel and the observation that the model generalizes well is interesting.\n- **(S2) Significance:** Understanding generalization capability of transformers is an important problem.\n- **(S3) Experiments:** Empirically testing the different hypothesis on transformer memory / generalization is interesting. This paper contains several good observations. In particular: the model implicitly learns distances between nodes without being explicitly trained  for it."}, "weaknesses": {"value": "While I think that the empirical work in this paper is solid, there are significant problems in the writing and presentations that hold this paper back:\n\n- **(W1) Problem Presentation:** The problem central to this paper (Path-Star task) is not defined properly in the paper. While a good explanation exists, it is buried on page 25 in the appendix. This makes it difficult to understand the paper. In contrast, the paper devotes almost half a page to Figure 1 which is difficult to understand as it requires 10 lines of caption. As a side-note: What does the color in  Figure 1  encode?\n\n- **(W2) Writing:** The writing often makes it difficult to understand the paper. Consider this:\n>Early on during training, the model fits the later tokens in the target —\nconcretely nodes $v_2, \\ldots , v_{\\text{goal}}$ — as the unique child of the previous token provided in the input\nduring next-token training. This is known as a Clever Hans cheat: the model uses a local rule that\nrelies on witnessing part of the ground truth prefix $(p, r<i)$ to predict the next token $r_i$ — as against\nonly using the prefix p to predict the answer tokens\n\nFrom this explanation, it is not clear what the \"Clever Hans\" cheat actually is. In contrast, the paper that introduces this problem [The Pitfalls of Next-Token Prediction](https://arxiv.org/pdf/2403.06963) makes the cheat quite clear. Furthermore, sentences are often convoluted making it  difficult to follow:\n> Next, we cast doubt on statistical pressures by scrutinizing the applicability of simplicity bias\n\n\n- **(W3) Significance:** The paper is not clear on what its contributions are. It is mainly a flood of hypotheses, observations and related works. From the abstract: \n>  Our insight is that global geometry arises from a spectral bias that—in contrast to prevailing intuition—does not require low dimensionality of the embeddings. Our study raises open questions concerning\nimplicit reasoning and the bias of gradient-based memorization, while offering a simple example for analysis. Our findings also call for revisiting theoretical abstractions of parametric memory in Transformers.\n\nWhat does this mean? While the structure \"Hypothesis\" -> \"Observation\" is interesting it makes it very difficult to follow. For example, Observations often weaken Hypothesis requiring extra explanation after the observation. \n\n-**(Minor Weakness) Node2Vec:** The section about Node2Vec seems a bit orthogonal to the rest of the paper.\n\n**To sum up,** the combination of a weak presentation, unclear writing and the paper not being clear about its own contributions makes it difficult for me to determine what the paper really contributes. I recommend that the authors significantly rewrite their text with a focus on clarity. As I do not believe that this can be done during rebuttal, I vote for reject. Furthermore, it seems that related work and connections to literature are present in many positions in the paper, making it difficult to  judge what is novel and what is not."}, "questions": {"value": "- What sets your contributions apart from [The Pitfalls of Next-Token Prediction](https://arxiv.org/pdf/2403.06963)?\n- Can you give me a concrete list of contributions?\n\n## Miscellaneous\n\n- The entire first paragraph is oddly phrased.\n>Neat representations materialize when a model compresses redundancies in the data. On the\nother hand, when faced with incompressible atomic facts (like the birth date of a celebrity), a\nmodel would memorize these associations like a lookup table.\n\n What are ''neat representations''? The \"however\" also does not quite feel right here.\n\n> We argue that this phenomenon implies a form of geometric memory in Transformers that must be contrasted with the associative memory view posited by Bietti et al. (2023). \n\nWhat are geometric and associative memories?\n\n\n- Figure 6 in the appendix does an excellent job at explaining the setting of this paper. It would  be great if parts of it were in the main paper.\n- Figure 2: The logarithmic (?) y-axis is slightly misleading and makes 48% look like 90%.\n- \"However, perhaps when the target we train on **is** an in-weights path, the cheat is not easy to learn—say, due to the nature of parametric memory.\"\n- In the list of contributions:\"We argue that the emergence of the geometric memory over the associative memory does not follow directly from existing intuitions.\"  Arguing is  not a contribution. Do you demonstrate or prove this? \n- Inconsistent subsubsections: some sections have a single subsubsection. Either use multiple ones (consistently) or do not use subsubsections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lbrRPXR4Vo", "forum": "RhGT2HdYnS", "replyto": "RhGT2HdYnS", "signatures": ["ICLR.cc/2026/Conference/Submission12624/Reviewer_9RrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12624/Reviewer_9RrG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569251037, "cdate": 1761569251037, "tmdate": 1762923471308, "mdate": 1762923471308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "--------------An empirical study with insufficient analytical support-----------------------------\n\nThe paper studies how Transformers, when trained to memorize graph edges (in-weights), often learn a “geometric” parametric memory—node embeddings that reflect global relationships—rather than a purely local associative lookup structure. The authors construct an in-weights path-star task (first train on edge bigrams to put a fixed graph into the weights, then finetune on path prediction) and find that Transformers can perform nontrivial multi-hop path-finding for large graphs where in-context next-token training fails. The success contradicts standard associative-memory abstractions. To explain how local pairwise supervision can yield global geometry, the paper studies simpler 1-layer Node2Vec models and presents empirical evidence of a spectral bias: embeddings align with Fiedler-like eigenvectors of the graph Laplacian, while a time-varying coefficient matrix converges to having those eigenvectors in its null space. The authors argue a similar spectral mechanism may be responsible in Transformers, though Transformer embeddings are less “clean” than Node2Vec’s, indicating room for improvement.\n\nOverall, the paper presents a clear phenomenon, thorough experiments, and a promising connection to spectral dynamics in simpler models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tClear, analyzable phenomenon: Turning the known in-context failure for path-star graphs into an in-weights setting produces a striking contrast and isolates implicit in-weights reasoning.\n•\tExtensive experiments: Results span multiple graph types and sizes and include meaningful ablations (edge directions, pause tokens, first-token-only loss, etc.). Visualizations and heatmaps strengthen the empirical case.\n•\tTheoretical connection: Mapping the emergence of geometry to spectral bias in Node2Vec-style learning is a useful and plausible explanatory route.\n•\tUseful framing: Presenting associative vs. geometric memory as competing data structures is intuitive and helpful for framing future research.\n•\tReproducibility effort: The paper includes many implementation details, hyperparameters, and procedural choices; the appendices are substantial."}, "weaknesses": {"value": "1.\tTheoretical rigor is limited.\no\tThe claim that Node2Vec dynamics converge to embeddings aligned with Fiedler vectors and that the coefficient matrix has those vectors in its null space is primarily empirical. The paper acknowledges the lack of a formal proof; still, the scope and assumptions of the empirical claim should be clarified (e.g., dependence on initialization, optimizer settings, sampling, etc.).\n2.\tGenerality and applicability to natural language are unclear.\no\tThe experiments are on symbolic graph tasks and specific topologies. It is not yet clear whether the geometric memory phenomenon plays a comparable role in real-world language-model memorization or knowledge integration.\n3.\tExplanation for why Transformers avoid associative solutions is incomplete.\no\tThe paper effectively shows simple statistical/architectural/supervisory explanations are insufficient, but lacks a direct diagnostic showing why gradient descent prefers geometric solutions in practice. More direct analysis of optimization trajectories or mechanisms would strengthen this point.\n4.\tSome training choices require deeper analysis.\no\tPause tokens, mixed forward/backward edge supervision, and the particular interleaving strategy are important in practice; their exact roles (and whether they generalize) need more principled investigation.\n5.\tThe link between Node2Vec and Transformer remains somewhat speculative.\no\tTransformers include attention, deep layers, residual connections and layer-norm; how these aspects alter or obstruct the spectral dynamics observed in Node2Vec isn’t fully explored. Additional ablations to bridge the gap would be helpful"}, "questions": {"value": "Experiments & results\n\n•\tStability: Provide multi-seed runs and report variance (e.g., success rate, epochs-to-convergence). Many plots show single runs.\n\n•\tLearning dynamics: The result that token accuracies rise “in tandem” is informative. Please show whether this pattern is robust to changes in learning rate, batch size, optimizer, or initialization.\n\n•\tPause tokens: Quantify the relationship between number of pause tokens and required reasoning depth. Does increasing model depth (more layers) substitute for pause tokens?\n\n•\tMixed edge supervision: Present a fine-grained ablation where the fraction of backward edges is varied continuously (e.g., 0%→100%) to show how performance scales.\n\n•\tQuantitative geometry metrics: In addition to heatmaps and UMAPs, report objective metrics (e.g., intra-path vs. inter-path cosine separation, Silhouette score, alignment with true Fiedler vectors) with error bars so comparisons (Transformer vs Node2Vec vs associative) are objective.\n\nTheory & analysis\n\n•\tNode2Vec dynamics: Move the full derivation of Lemma 2 into the appendix (if not already) and explicitly list all assumptions (batching, softmax normalizations, self-probabilities, sampling, optimizer choices). Clarify when the empirical “self-stabilizing” dynamics are expected to hold and when they may fail.\n\n•\tProposition 1: The bits and l2 norm arguments are useful. Flesh out boundary conditions—e.g., effect of weight tying, multiple outputs per input, or non-uniform degree distributions.\n\n•\tTransformer mechanism: Consider intermediate ablations that simplify the Transformer toward Node2Vec—e.g., single-layer Transformer with only embedding+unembedding, linear attention, or frozen attention weights—to trace whether spectral alignment persists and how components contribute."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "luNZOYr0XT", "forum": "RhGT2HdYnS", "replyto": "RhGT2HdYnS", "signatures": ["ICLR.cc/2026/Conference/Submission12624/Reviewer_UZ7j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12624/Reviewer_UZ7j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12624/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967111943, "cdate": 1761967111943, "tmdate": 1762923470418, "mdate": 1762923470418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}