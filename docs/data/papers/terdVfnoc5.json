{"id": "terdVfnoc5", "number": 19625, "cdate": 1758297764863, "mdate": 1759897029640, "content": {"title": "RedTopic: Toward Topic-Diverse Red Teaming of Large Language Models", "abstract": "As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, $\\textbf{red teaming}$ has become critical. It probes LLMs with curated adversarial prompts to identify vulnerabilities and guide subsequent safety alignment. Effective red teaming should be adaptive to evolving LLM capabilities and cover a broad spectrum of harmful topics. However, existing topic-based approaches rely on predefined malicious topic sets, which limit their adaptability and ability to uncover diverse adversarial prompts. In contrast, topic-free methods can automatically generate prompts with new topics, but most depend on Reinforcement Fine-Tuning (RFT) and suffer from limited coverage due to the exploration–exploitation dilemma, leading to adversarial prompts that remain topically narrow.   To address these limitations, we propose $\\textbf{RedTopic}$, a novel red teaming framework that generates topic-diverse adversarial prompts through a contextualized generation pipeline, an aggregate reward design, and a multi-objective RFT optimization loop. Experiments show that RedTopic produces more effective and diverse adversarial prompts than existing methods, with notable improvements in integrated evaluation metrics. We believe RedTopic represents a step toward more adaptive and topic-diverse red teaming for LLMs.  \n$\\textcolor{red}{\\text{WARNING: This paper contains examples of potentially harmful text.}}$", "tldr": "", "keywords": ["Trustworthy AI", "Red Teaming", "Topic-Diversity", "Large Language Model", "Reinforcement Fine-Tuning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0451170d622408e75bcda607f734fe33e2fd9e33.pdf", "supplementary_material": "/attachment/2ea3c5314aa86618a0b2697e731a242606c7564e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RedTopic, an automated red-teaming method for large language models (LLMs). The goal is not just to get unsafe answers (high attack success rate, ASR), but to surface many different kinds of unsafe behavior within a limited number of queries. The authors argue that existing approaches split into two types, each with a flaw. Topic-based methods follow a fixed list of sensitive areas and probe each one, so they cover many categories but usually fail to jailbreak (low ASR). RL-based approaches directly optimize for harmful answers and get high ASR, but they tend to repeat one style of exploit and don’t reveal much variety. The paper argues that a useful red team should achieve both breadth and effectiveness.\n\nRedTopic tries to achieve this by: (i) rewriting normal-looking prompts into malicious versions that stay in context , (ii) using a reward that jointly favors harmful responses, topical novelty, scenario consistency, and non-gibberish, and (iii) training with a multi-objective PPO variant to avoid collapsing to a single trick. The paper also defines a metric, Dtopic​, meant to measure how different two successful attacks are in terms of harmful intent, not just wording. In experiments, RedTopic is shown to keep a reasonable ASR while also covering a wider spread of unsafe behaviors than both topic-based and RL-based approaches. The authors also show that data from RedTopic can be used to fine-tune a model so it refuses unsafe requests more reliably, suggesting that this broader coverage is useful for downstream safety alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper targets a real problem in LLM safety evaluation: current automated red teams either repeatedly exploit one narrow unsafe behavior to maximize ASR, or they wander across many sensitive topics but rarely succeed in eliciting unsafe responses. Trying to optimize for both coverage and effectiveness under a limited query budget is practically relevant.\n* The proposed pipeline is well-motivated. The contextualized adversarial rewriting step encourages realistic, scenario-grounded prompts instead of generic “ignore your safety policy” jailbreak prompts. The reward explicitly encodes several objectives at once: generate something the model will actually answer unsafely; keep that unsafe request tied to the original scenario; avoid repeating the same exploit style; and avoid nonsense text. The multi-objective PPO training is meant to prevent the policy from collapsing into the easiest single jailbreak trick.\n* The empirical results support the claim that RedTopic improves the ASR/diversity balance compared to both topic-based and RL baselines. RedTopic is shown to achieve reasonable ASR without collapsing to one repeated theme, and to achieve higher topical spread than prior RL-based approaches. The downstream fine-tuning experiment is also useful: data from RedTopic appears to make a model more likely to refuse unsafe requests."}, "weaknesses": {"value": "1. RedTopic is positioned as a new method that balances ASR and topic diversity better than prior approaches. However, the paper does not clearly separate algorithmic novelty from engineering effort. RedTopic combines three ideas (contextualized adversarial prompt generation, a reward that explicitly favors diversity/consistency/fluency, and a multi-objective PPO variant). The paper includes ablations within RedTopic (e.g., PPO vs. MOPPO), but it does not test whether existing strong attacks like CALM or DiveR-CT would achieve similar diversity if retrained with the same reward and contextualization pipeline. As a result, it remains unclear whether RedTopic introduces fundamentally new capabilities, or mainly combines known techniques.\n2. The paper assumes that higher topic diversity (Dtopic) corresponds to broader coverage of distinct and policy-relevant unsafe behaviors, which is important for safety evaluation. However, Dtopic is computed as distance in a safety model’s embedding space. While the paper provides qualitative examples and category-level summaries, it does not include human evaluation or systematic validation to confirm that increases in Dtopic correspond to meaningfully different classes of real harm, rather than superficial topic shifts. The usefulness of Dtopic as a safety metric would be stronger with additional evidence.\n3. The observed trade-off between diversity and ASR is described but not well explained. Topic-based methods like ASTRAL have high diversity but low ASR, and RL-based approaches like CALM or DiveR-CT have high ASR but low diversity. The paper does not clarify whether this gap is fundamental (broad coverage is inherently harder to jailbreak) or mostly due to unbalanced optimization effort (ASTRAL is not RL-tuned per target model, while CALM is). Without in-depth analysis, it is hard to interpret whether the trade-off reflects real limitations or just training differences between baselines.\n4. The paper argues that adaptive discovery of harmful topics is superior to relying on predefined taxonomies, because adaptive methods can surface “unknown unknowns” under the same query budget. While this is plausible, the paper does not present a concrete example of an unsafe scenario that RedTopic reveals which a topic-based baseline fails to expose, nor does it show that expanding the predefined topic list would not close the gap.\n5. The empirical comparison is not entirely apples-to-apples. RedTopic is explicitly optimized with a multi-objective reward that aligns with the paper’s evaluation metrics (success, diversity, etc.), whereas baselines are evaluated largely in their original setting, without being retrained under equivalent multi-objective rewards. This makes it difficult to attribute RedTopic’s reported advantage to specific methodological contributions rather than to simply adding more optimization objectives."}, "questions": {"value": "1. Can the author provide a cross-method ablation where an existing RL attack (e.g., CALM or DiveR-CT) is retrained using the mentioned contextualized prompt rewriting pipeline and diversity-aware reward? This would clarify whether RedTopic’s advantage is algorithmic, or mainly due to providing more optimization objectives.\n2. How robust is Dtopic across evaluators? Do the authors have any human validation, even on a small scale, confirming that higher Dtopic corresponds to genuinely different unsafe behaviors?\n3. Is the low ASR for topic-based methods structural (broad coverage is genuinely harder to jailbreak) or mainly because those approaches are not RL-optimized against the target model in the same way? Some clarification or experiment here would help interpret how fundamental that trade-off really is.\n4. The paper argues that RedTopic can surface “unknown unknowns.” Can the authors provide one concrete harmful scenario that RedTopic elicits which does not appear in any topic-based baseline, and explain why that scenario is uniquely important for safety auditing?\n5. In the downstream fine-tuning experiment, does the paper measure helpfulness vs. refusal? In other words, does training on RedTopic data make the model over-refuse harmless queries, or does it improve safety without a large impact on normal utility?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper trains and evaluates methods that are explicitly designed to elicit harmful, policy-violating responses from LLMs across sensitive domains (e.g., harassment, privacy abuse, fraud). While the work is framed as beneficial for safety auditing and post-hoc alignment, the same techniques could be used to automate large-scale jailbreak attempts. It would be useful to clarify what artifacts (models, prompts, scripts) will actually be released, whether any filtering will be applied, and how the authors intend to prevent straightforward misuse."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nXroBKVgx3", "forum": "terdVfnoc5", "replyto": "terdVfnoc5", "signatures": ["ICLR.cc/2026/Conference/Submission19625/Reviewer_ZkUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19625/Reviewer_ZkUL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779262002, "cdate": 1761779262002, "tmdate": 1762931478412, "mdate": 1762931478412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper devises an RL-based framework called RedTopic to generate topic-diverse prompts for read-teaming of LLMs. The authors argue that existing topic-based methods depend on pre-defined topics and have limited topic coverage, while the topic-free methods mainly consider token/sentence-diversity. RedTopic proposes a new topic diversity measure based on the negative cosine similarity in the topic embedding space. Based on the topic-diversity score, RedTopic proposes a multi-objective RL training to balance the token/sentence/topic-diversity and the attack effect. Experiments are conducted on multiple datasets and models against multiple baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Red teaming is crucial for LLMs, and increasing the topic coverage is important.\n\n2. Experiments show the method can find a balance between the ASR and the topic diversity.\n\n3. Experiments cover both topic-based and topic-free baselines.\n\n4. The ablation studies are conducted for different designs."}, "weaknesses": {"value": "1. The topic-diversity definition should be validated externally (e.g., by a different model or human). There is no evidence to support that this embedding space indeed reflects topics rather than toxicity or semantic similarity.\n\n2. It's unclear whether the models used to define the reward and evaluate the final performance are the same. If so, this is like \"evaluation leakage\". It would be better to use metrics different from the optimization target. For example, use a different model.\n\n3. Similar to the first point, it's not clear if this method can discover \"new topics\". Human studies may be necessary to validate this.\n\n4. The code is not open-sourced."}, "questions": {"value": "1. Can other evidence, such as human studies, be provided to support that LLaMA-Guard-3-1B embeddings capture topical rather than semantic or toxicity?\n\n2. Can this method discover \"new topics\"?\n\n3. This method cannot achieve the highest ASR or diversity. Why cannot one combine existing methods in an ensemble to improve both ASR and diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5gzGvrDmx3", "forum": "terdVfnoc5", "replyto": "terdVfnoc5", "signatures": ["ICLR.cc/2026/Conference/Submission19625/Reviewer_rxQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19625/Reviewer_rxQA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795763631, "cdate": 1761795763631, "tmdate": 1762931477997, "mdate": 1762931477997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They fine tune a model to generate red teaming prompts. Their objective function combines scores for topic, sentence, and token diversity as well as groundedness, and success as a red teaming prompt. In order to combine these scores, they propose a multi objective PPO method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "They show a benefit from fine tuning a model on the adversarial prompts with rejection responses. This shows the usefulness of the technique. In general, I think increasing the diversity of topics covered in red teaming is important.\n\nTheir ablation studies test the relevant parts of their method individually so the impact is clear.\n\nTheir results show that their method is best if you care about the number of distinct topic level vulnerabilities discovered."}, "weaknesses": {"value": "The D_sent% and D_token% scores from the baselines are higher. So if you care about the number of unique sentence or token level vulnerabilities, the baselines are better than their method.\n\nThe way they combine the scores seems arbitrary. For example, why do they include toxic, topic, and consis scores in one harmonic mean, but include token, sent scores in another harmonic mean?\n\nMOPPO is not well motivated. MOPPO calculates the advantage as a weighted advantage of the individual advantages. As far as I can tell, the difference between MOPPO and standard PPO with weighted rewards is that with MOPPO, the advantage corresponding to each reward is normalized independently. Are there any other differences? The paper doesn’t make the difference with standard PPO clear, nor does it explain why these differences make MOPPO better. The paper also doesn’t discuss how this method relates to other multi objective RL methods, like https://arxiv.org/abs/2509.14816 and https://arxiv.org/abs/2005.07513\n\nsoundness of methods\n\nEquation 4 will have a high value if F_(token-sent) is close to 1, which makes sense because that encourages diversity. But it will also be maximised if F_(token-sent) is < epsilon, which doesn’t make sense to me, since that means all generations are similar.\n\nThe D_level% metric will penalize a method for finding many identical harmful prompts. If the method finds more than k identical harmful prompts, D_topic in equation 2 will always be 0, because the average cosine similarity between the k identical nearest neighbors will be 1. This will penalize the D_topic% metric in equation 9, since it depends on D_topic. However, if the method finds a certain prompt only once, the cosine similarity will be less than 1 and it will score higher on the metric. I don’t think a method should be rewarded for finding the same prompt many times, but I don’t think it should be penalized for this either.\n\nclarity\n\nUsing F1 notation in equation 4 is confusing, since it normally represents the harmonic mean of precision and recall. But the authors use it to represent the harmonic mean of two arbitrary values.\n\nThe “policycover-based token-level intrinsic bonus Rpc” should be explained in english in the main paper. It’s currently not explained in the main paper, and even from reading the appendix section, the intuition is still unclear to me without reading CALM.\n\nThe paper says “PPO prematurely exploits easier signals (e.g., Rnon-gibb) and is unwilling to increase RF1 at the cost of decreasing the easier bonus”. How were the rewards combined when using PPO? \n\nFigure 5 presents the results in a different way than the rest of the results are presented. It would be easier for me to compare if all experiments presented results in the same way.\n\nminor points\n\n“The colors get thicker as the training progresses” you mean darker?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9OWbeK71WV", "forum": "terdVfnoc5", "replyto": "terdVfnoc5", "signatures": ["ICLR.cc/2026/Conference/Submission19625/Reviewer_2EUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19625/Reviewer_2EUB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804730198, "cdate": 1761804730198, "tmdate": 1762931477637, "mdate": 1762931477637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RedTopic, a topic-diversity-driven red-teaming framework for LLMs. The core idea is to move beyond token/sentence variation and explicitly optimize topic-level diversity while maintaining attack effectiveness. Concretely a contextualized adversarial prompt pipeline that injects harmful intent into realistic “clean prompts” and queries a target model; 2) an aggregate reward that harmonically balances toxicity, diversity (token/sentence/topic), and context consistency; 3) a multi-objective RL algorithm (MOPPO) that optimizes a vector reward and avoids collapse. Topic diversity is defined as average negative cosine similarity in a topic-embedding space (they use LLaMA-Guard-3-1B), Eq. (2). The aggregate reward uses an F1-style formulation with a threshold penalty, Eq. (4). Experiments across closed- and open-source targets show improved integrated topic-acquisition (Dtopic%) and more even safety-taxonomy coverage versus topic-based and topic-free baselines; ablations credit the pipeline, the reward design, and MOPPO. The paper also shows that adding RedTopic-generated data improves safety alignment on AART and SAP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Introduces an explicit topic-diversity objective and validates that token/sentence diversity is insufficient (Fig. 1/2). The contextualized pipeline usefully grounds attacks in realistic scenarios.\nQuality: Solid ablations: swap scenarios→topics, remove consistency reward, PPO vs MOPPO, reward-combination variants; helpful diagnostics (threshold penalty, generation length).\nClarity: Equations and pipeline diagram make the approach legible; tables cover both success and integrated acquisition rates (Dtoken%, Dsent%, Dtopic%).\nSignificance: Demonstrates broader topical coverage and improved distribution entropy (MLCommons Taxonomy) and shows downstream safety alignment benefit. This is operationally important for safety engineering."}, "weaknesses": {"value": "Evaluator dependence / circularity:\n1. Topic embeddings from a single guard (LLaMA-Guard-3-1B) define the diversity metric; this risks metric overfitting and model-of-a-model biases. Fig. 1(a) contrasts CLIP vs guard, but breadth across multiple guards (and non-guard topic models) is limited.\n2. LLM-as-Judge toxicity introduces variance (acknowledged via judge comparisons), yet the main results hinge on it; stronger calibration (human labels, consensus of judges) would increase trust.\nExternal validity: Main focus is single-turn attacks; multi-turn is left to future work, but many real jailbreaks are multi-turn or tool-mediated.\nMetric coupling and budgets: Integrated metrics (Dlevel%) condition on success and normalize by a fixed probe budget. Sensitivity to budget size and sampling policies is not fully explored.\nComparative fairness: While hyperparameters are unified, it’s unclear whether each baseline is tuned to its best trade-off (e.g., diversity weights for CRT/DiveR-CT/CALM). Reporting CIs or statistical tests for Table 1/7 would help.\nSafety scope: Results emphasize text LLMs; the multimodal extension is argued but not empirically shown here."}, "questions": {"value": "Robustness of Dtopic: How do RedTopic’s gains change when topic embeddings come from diverse models (e.g., ShieldGemma, DuoGuard, Qwen-Guard, unsupervised topical clustering)? Please include a cross-model Dtopic evaluation and training where the metric model differs from the one used for reporting.\nJudge variability: Can you report ASR/Dtopic% using a committee of judges with majority vote or expected calibration error vs human annotations on a 1–2k subset? Fig. 7(b) suggests 83% agreement, but what is the end-to-end effect on rankings?\nMulti-turn attacks: Any preliminary results if the contextual pipeline permits 2–3 interactions (e.g., PAIR-like setting) under the same probe budget?\nBudget sensitivity: How do Dtopic% and entropy scale with 25/50/200 probes? Any evidence of diminishing returns curves across methods?\nDistribution shift: If clean prompts come from different domains (e.g., StackExchange vs Reddit vs internal logs), does consistency-reward tuning need retuning? Clarify corpus sources and their influence.\nRelease: Will you release the adversarial prompts and clean-prompt templates for reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZLWrFU4tNP", "forum": "terdVfnoc5", "replyto": "terdVfnoc5", "signatures": ["ICLR.cc/2026/Conference/Submission19625/Reviewer_kfCM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19625/Reviewer_kfCM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19625/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125905416, "cdate": 1762125905416, "tmdate": 1762931477166, "mdate": 1762931477166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}