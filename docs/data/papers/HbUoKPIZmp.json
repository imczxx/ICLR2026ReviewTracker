{"id": "HbUoKPIZmp", "number": 294, "cdate": 1756734188803, "mdate": 1759898268643, "content": {"title": "Advancing End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-Training", "abstract": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet-256.\nSpecifically, our diffusion model reaches an FID of 2.04 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost.\nFurthermore, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models. We will make our code and model publicly available.", "tldr": "", "keywords": ["Image Generation", "Diffusion models", "Pixel-space generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24f5f376fffd3b80d29eb2025de3b49bed6ea29d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel two-stage training framework for pixel-space consistency models. In the first stage, a portion of the generative model, referred to as the encoder, is pre-trained using self-supervised learning (SSL) methods, providing beneficial initialization. In the second stage, the entire model is fine-tuned with an additional network component, termed the decoder. This approach eliminates the need for latent diffusion training and achieves superior generation performance compared to existing end-to-end pixel-space generative models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation is clearly articulated. In particular, the introduction effectively positions the work, helping readers understand its aims and benefits.\n- The proposed method does not require external models, which may enhance its applicability to other domains.\n- The overall training pipeline is presented clearly.\n- The empirical results are impressive, and the evaluation is extensive."}, "weaknesses": {"value": "[Major comments]\n- Although two types of contrastive loss are proposed for the pre-training stage (Equation (8)), a more in-depth analysis of how each contributes to performance improvement (rather than reporting only FID scores) would be beneficial.\n- It is unclear why $(\\cdot)^-$ is applied to the first term and $sg$ to the second term in Equation (8). Could you provide further insight into this design choice?\n- The fine-tuning stage lacks clarity, as $\\theta'$ is not defined.\n\n[Notation errors] There are several notation errors and unclear formulations:\n- $\\omega$ in the caption of Figure 2 should be $\\theta$?\n- The definitions of $(\\cdot)^-$ in Lines 177 and 193 are inconsistent.\n- $\\tau_2(t)$ in Line 274 should be $\\tau$?\n- What is $\\theta'$ in Equation (9)?\n- In Equation (9), does $E_{\\theta'}$ include the projection used in the pre-training stage?\n\n[Minor]\n- The reference for VAE cites the wrong publication year."}, "questions": {"value": "- Have you experimented with performing the second-stage training while keeping the encoder, pre-trained in the first stage, frozen? Readers may be interested in understanding the trade-off between the flexibility of $f_\\theta$ and the faithfulness to the representations learned in stage 1.\n- How would the intermediate features obtained by $E_\\theta$ differ if both $E_\\theta$ and $D_\\theta$ were trained in a single stage without the contrastive loss? If possible, visualizing this difference would be of interest to readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Potential ethical concerns are not discussed."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7TTAZPib0X", "forum": "HbUoKPIZmp", "replyto": "HbUoKPIZmp", "signatures": ["ICLR.cc/2026/Conference/Submission294/Reviewer_jexG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission294/Reviewer_jexG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792103992, "cdate": 1760792103992, "tmdate": 1762915487394, "mdate": 1762915487394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two-stage training framework — pre-training (representation consistency + contrastive learning) to help the encoder learn semantic features, followed by end-to-end fine-tuning with a randomly initialized decoder. The framework can be applied to pixel-space diffusion and consistency models, achieving strong results on ImageNet-256.\n\nThe paper is clearly written and easy to follow. The motivation, methodology, and experimental evidence are all presented in a logical and convincing manner.\n\nIn terms of originality, while the approach does not introduce fundamentally new techniques, it leverages existing methods in a novel way to improve the performance of high-dimensional pixel-space generative models, which is meaningful and valuable.\n\nThe experiments are relatively comprehensive and well-designed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Please refer to Summary."}, "weaknesses": {"value": "My main concern lies in the comparison with the latest latent-space methods, particularly RAE [1], where the proposed approach still lags behind in performance (ImageNet-256 FID 2.04 vs. 1.51). Moreover, most of the baselines used in the paper are relatively outdated (especially the pixel-level models), and the reported improvements over them are not very substantial.\n\nCompared to VAE-based approaches, the proposed two-stage training pipeline does not demonstrate a clear advantage. Notably, recent works such as REPA-E [2] have also shown that end-to-end joint training of VAEs and diffusion models is feasible and effective.\n\nOverall, while the method is conceptually simple, it does not appear to be sufficiently effective or provide strong new insights.\n\n[1] Diffusion Transformers with Representation Autoencoders\n[2] REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bBRoK4b6QD", "forum": "HbUoKPIZmp", "replyto": "HbUoKPIZmp", "signatures": ["ICLR.cc/2026/Conference/Submission294/Reviewer_nxB1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission294/Reviewer_nxB1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635703916, "cdate": 1761635703916, "tmdate": 1762915487296, "mdate": 1762915487296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel two-stage training framework (tentatively referred to as EPG) for advancing end-to-end pixel-space generative models, addressing the long-standing limitations of Variational Autoencoder (VAE)-dependent paradigms in the field of diffusion and consistency models. The framework decouples the complex generative task into self-supervised pre-training and end-to-end fine-tuning phases: in the first phase, the encoder is trained independently via contrastive loss and representation consistency loss to learn noise-robust visual features; in the second phase, the pre-trained encoder is concatenated with a randomly initialized decoder for end-to-end optimization targeting downstream generative tasks. Experiments on ImageNet-256 and ImageNet-512 datasets demonstrate competitive performance—achieving FID scores as low as 2.04 and 2.35 respectively with only 75 inference steps—and pioneering pixel-space consistency model training without VAE reliance, yielding 8.82 FID in single-step generation. This work bridges the efficiency gap between pixel-space and latent-space models while simplifying the training pipeline."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Paradigm Innovation: Breaking VAE Dependence\nThe paper makes a significant paradigm contribution by eliminating the need for VAEs in high-quality generative modeling—a critical limitation of mainstream approaches like Stable Diffusion and DiT . VAEs introduce inherent trade-offs between compression ratio and reconstruction quality, and require costly joint fine-tuning across domains . By decoupling representation learning from pixel reconstruction, the proposed two-stage framework resolves these issues: the self-supervised pre-training phase ensures robust feature extraction from noisy data, while the lightweight fine-tuning phase avoids the complexity of VAE optimization. This \"de-VAE\" design aligns with emerging trends in pixel-space generation and significantly lowers the barrier to adapting generative models to new domains."}, "weaknesses": {"value": "Limited Evaluation on High-Resolution and Diverse Datasets\nWhile the paper demonstrates results on ImageNet-256/512, it lacks validation on higher-resolution tasks (e.g., 1024×1024 FFHQ) where pixel-space models historically struggle . The choice of ImageNet (natural images) also raises questions about generalizability to other domains (e.g., medical imaging, satellite imagery) where VAE artifacts are particularly problematic. Furthermore, key video generation metrics like temporal consistency or FVD are absent, despite the paper hinting at extensibility to video—leaving uncertainty about whether the framework can capture spatiotemporal dependencies."}, "questions": {"value": "What is the impact of ODE path sampling density on the representation consistency loss and final generation quality? Could you supplement ablations or visualizations to clarify this mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0aJVTvL2ks", "forum": "HbUoKPIZmp", "replyto": "HbUoKPIZmp", "signatures": ["ICLR.cc/2026/Conference/Submission294/Reviewer_FWeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission294/Reviewer_FWeF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886702122, "cdate": 1761886702122, "tmdate": 1762915487169, "mdate": 1762915487169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a two-stage framework for (1) representation learning with consistency regularization, and (2) fine-tuning for the generative model. To make the optimization of the representation learning in the first stage work, the authors leverage EMA and stop gradients to form the training objectives. The fine-tuning is conducted using a diffusion model and extra regularization to ensure that the encoder remains structured and meaningful."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors carefully designed the training objectives for leveraging consistency loss in the training of the encoder.\n\nThe authors focus on important questions and give good justifications for their initiatives."}, "weaknesses": {"value": "My main concern is that I am not sure if the paper can be claimed as a \"pixel-based\" generative model, since:\n\n(1) The consistency regularization is conducted in the latent space.\n\n(2) The generative model is also trained in the diffusion model. (Also, I do think the authors should specify how the generative model is trained in their diagram, as from the current one, you might think they are learning a generative model in the 1st stage of learning representations, which is not the case."}, "questions": {"value": "Have you evaluated the encoder's performance?\n\nHow's the performance if you don't use the consistency regularization? Or simply using it without the contrastive objective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4I1oRYB7oo", "forum": "HbUoKPIZmp", "replyto": "HbUoKPIZmp", "signatures": ["ICLR.cc/2026/Conference/Submission294/Reviewer_bsaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission294/Reviewer_bsaA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180149022, "cdate": 1762180149022, "tmdate": 1762915487067, "mdate": 1762915487067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}