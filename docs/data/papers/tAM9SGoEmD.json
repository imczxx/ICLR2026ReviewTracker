{"id": "tAM9SGoEmD", "number": 12421, "cdate": 1758207672644, "mdate": 1759897511016, "content": {"title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Generation in the Real World Domain", "abstract": "Safety-critical scenarios are essential for evaluating autonomous driving (AD) systems, yet they are rare in practice. Existing generators produce trajectories, simulations, or single-view videos—but they don’t meet what modern AD systems actually consume: realistic multi-view video. We present SafeMVDrive, the first framework for generating multi-view safety-critical driving videos in the real-world domain.\nSafeMVDrive couples a safety-critical trajectory engine with a diffusion-based multi-view video generator through three design choices. First, we pick the right adversary: a GRPO-fine-tuned vision-language model (VLM) that understands multi-camera context and selects vehicles most likely to induce hazards. Second, we generate the right motion: a two-stage trajectory process that (i) produces collisions, then (ii) transforms them into natural evasion trajectories—preserving risk while staying within what current video generators can faithfully render. Third, we synthesize the right data: a diffusion model that turns these trajectories into multi-view videos suitable for end-to-end planners. On a strong end-to-end planner, our videos substantially increase collision rate, exposing brittle behavior and providing targeted stress tests for planning modules. Our code and video examples are available at: https://iclr-1.github.io/SMD/.", "tldr": "", "keywords": ["Autonomous driving testing", "safety-critical scenario", "video generation", "safety"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30341f9199de3ba92295c06468a5da66d41133ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses multi-view, safety-critical video generation in the driving domain. It introduces SafeMVDrive, a framework that integrates a safety-critical trajectory simulator with a multi-view video generator and leverages visual context via a fine-tuned vision–language model (VLM) to select safety-critical vehicles. Experiments demonstrate that SafeMVDrive generates high-quality multi-view driving videos and induces 30% more collisions than the original NuScenes data, highlighting its effectiveness for safety-critical scenario synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and presents rich, detailed content.\n2. The ability to generate safety-critical driving scenarios addresses an important real-world need, as prior approaches have struggled to produce such rare but crucial events.\n3. The paper provides extensive and clear visualizations, which effectively illustrate the high-quality simulation performance of the proposed method."}, "weaknesses": {"value": "1. The paper demonstrates notable practical and engineering value; however, its technical novelty is somewhat limited. The approach primarily integrates existing components—multi-view conditional generators, trajectory proposal modules, and VLM-based selection.\n2. The academic value is difficult to assess, as it lacks comparisons with related methods (though such methods may themselves be scarce) and \"safety\" is inherently a broad concept. Overall, its engineering value outweighs its academic value.\n3. The term safety-critical is rather broad, and the paper lacks a precise definition. Moreover, the current work addresses only safety issues arising from other vehicles’ trajectories, whereas real-world safety concerns extend beyond this scope. It is recommended that the paper provide a more concrete and detailed formulation of the problem.\n4. How well does the VLM-based selection generalization, and what happens if the selection fails? Are the final safety-critical scenarios manually adjusted in such cases? My understanding is that the method primarily leverages the VLM to facilitate the generation of more safety-critical simulation scenarios—is this correct?"}, "questions": {"value": "1. Does training a policy model on generated safety-critical scenarios enhance its robustness to dangerous driving situations?\n2. Since interactive cases in NuScenes are relatively rare, how is the ground truth for safety-critical vehicles selected when using the VLM?\n3. Metrics relying on a single planner tend to be susceptible to fluctuations. It is therefore recommended to evaluate a broader range of planning methods to verify the consistency of results.\n4. What are the practical potential application scenarios of this pipeline? Is it intended to serve as a corner case evaluation benchmarks or to train more robust planners?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ohw6a5gFM9", "forum": "tAM9SGoEmD", "replyto": "tAM9SGoEmD", "signatures": ["ICLR.cc/2026/Conference/Submission12421/Reviewer_c85w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12421/Reviewer_c85w"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760962963499, "cdate": 1760962963499, "tmdate": 1762923311868, "mdate": 1762923311868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for synthesizing safety-critical scenario data, for which the authors design a VLM-based selector and a two-stage trajectory generator. By evaluating the quality of the generated videos and the validity rate of the trajectories, this paper demonstrates the effectiveness of the proposed pipeline. Long-tail scenarios are a critical challenge that autonomous driving must address. This paper represents a attempt to improve the performance of autonomous driving models on such long-tail scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "One notable strength of this work is its clear articulation of a critical gap in existing generative models: despite recent advances in video synthesis, they fail to produce safety-critical driving scenarios—especially those involving potential collisions—that are vital for robustness evaluation of autonomous vehicles. The paper rightly positions this as a core challenge in handling long-tail events, which are rare yet consequential in real-world deployment. \n\nThis paper proposes a novel multi-stage framework: it first employs a VLM-based selector to identify a potentially colliding target vehicle, then uses a trajectory generator to sequentially produce a collision trajectory followed by an evasion trajectory, and finally leverages an existing video generation model—conditioned on the generated trajectories—to synthesize realistic driving videos."}, "weaknesses": {"value": "The paper primarily evaluates two components: (1) the accuracy of the VLM-based adversarial vehicle selector, and (2) the performance of the synthesized data when used to test the UniAD planner, reporting metrics such as Collision Rate (CR), Near-Collision Rate (NC), and Time-to-Collision (TTC). \nHowever, it does not address a crucial practical question: can the generated safety-critical scenarios be effectively used for training autonomous driving models? Demonstrating utility in evaluation is valuable, but the potential of the synthetic data to improve model robustness or safety during training—a primary goal of data synthesis—remains unexplored and should be discussed.\n\nFurthermore, the authors directly adopt the pre-trained UniM-LVG video generator without fine-tuning. Given that UniM-LVG’s training data likely underrepresents safety-critical events (e.g., collisions or near-misses), its capacity to generate high-quality, physically plausible safety-critical videos is uncertain. The paper lacks empirical validation—such as visual fidelity checks, physics-based consistency analysis, or user studies—to confirm that the generated scenes are realistic and meaningful in these extreme cases."}, "questions": {"value": "1. Please supplement the experiments with discussion or validation demonstrating that UniM-LVG can generate high-quality data for the safety-critical scenarios described in this paper.\n\n2. Collisions represent only one type of safety-critical scenario. The paper should also discuss the cost and methodology required to extend the proposed approach to other types of safety-critical situations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZA3EhpV60", "forum": "tAM9SGoEmD", "replyto": "tAM9SGoEmD", "signatures": ["ICLR.cc/2026/Conference/Submission12421/Reviewer_ffnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12421/Reviewer_ffnN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710552901, "cdate": 1761710552901, "tmdate": 1762923311519, "mdate": 1762923311519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This works main contribution is to propose using VLM as an adversarial vehicle selector for multi-view video generation, which start with VLM selection and use trajecotry diffusion models for adversarial scenario genreation, then the genreated multiview videos are generated for E2E planner evaluation. Unlike most works that synthesize vehicle trajecotires, this work starts with visual cues from multi-view videos. The main limitation is that the generated videos are evaluated in an open-loop setting, where the E2E planner’s behavior may diverge from the input videos, limiting the realism and closed-loop consistency of the evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This works focus on an important problem: Generating Safety-critical Multiview videos, especially more research  industry has now turned into End-to-End paradigm.\n- Using Video to select Critical Object is an interesting direction, but it also potentially neglect safety-critical scenarios that are not in the perceptive field (collision or far-away vehicles)"}, "weaknesses": {"value": "- The main limitation and weakness is that this work is not closed-loop: \nThe multi-view videos used as input to the evaluated planner may diverge from the planner’s actual behavior, limiting the realism and validity of the evaluation. The 2-stage evasion stage in table 4 refinement is not the actual evaluated planner's behavior.\n- **Table 2 may not be a fair comparison.** The proposed annotation strategy only identifies vehicles that can collide with the ego vehicle, which may miss adversarial agents farther away. In contrast, a random lane-based neighbor selection could reveal a wider range of challenging interactions. Moreover, some nearest-vehicle collisions (as shown in the qualitative videos) appear uninteresting, such as simple rear-end cases.\n- Why is the evaluation limited to 3 seconds in table 4, rather than testing longer horizons for more realistic interactions?\n- Minor weakness: Fine Tuning VLM w/ CTG++ results, this works proposes to auto label feasible collision vehicles by checking if collisions can happen before off-road and collided with other vehicles, this would potentially scenarios due to CTG++’s performance issue."}, "questions": {"value": "- Can the Proposed framework handle Closed-loop simulation? For example, can we have planning algorithms 1 and 2, where the final scenarios may be different depending on the interactions?\n- How well does the VLM handle **multi-view visual inputs**? In particular, does the framework tend to select vehicles primarily from the **front and rear views**, or are there also cases where **side-view vehicles** are chosen as adversarial agents?\n- Report the inference speed of the proposed Pipelines"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BjnYWIpGQp", "forum": "tAM9SGoEmD", "replyto": "tAM9SGoEmD", "signatures": ["ICLR.cc/2026/Conference/Submission12421/Reviewer_GrpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12421/Reviewer_GrpU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795047315, "cdate": 1761795047315, "tmdate": 1762923311201, "mdate": 1762923311201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of generating realistic, safety-critical data for the evaluation of modern autonomous driving (AD) systems. The authors identify a key gap in existing data generation methods: while they can produce trajectories, simulations, or single-view videos, they fail to generate the realistic multi-view video feeds that contemporary end-to-end AD systems actually consume. To solve this, the authors propose SafeMVDrive, a novel framework designed to be the first to synthesize multi-view, safety-critical driving videos in the real-world domain.\nThe core of their method involves coupling a safety-critical trajectory engine with a diffusion-based video generator, guided by three main contributions. First, they employ a fine-tuned Vision-Language Model (VLM) as an intelligent \"adversary\" to select the vehicle most likely to create a hazardous situation based on multi-camera context. Second, they propose a two-stage motion generation process that initially models a direct collision and then transforms it into a plausible near-miss or evasion trajectory, preserving the scenario's risk while ensuring it can be faithfully rendered by the video model. Third, a diffusion model is used to convert these trajectories into the final multi-view video output."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The designed safety-critical video generation pipeline is intuitive and important for training robust E2E model.\n\n2. The proposed two-stage evasion trajectory generator provides diverse collision scenes.\n\n3. The video results look consistent and dynamically plausible."}, "weaknesses": {"value": "1. SafeMVDrive works as a data engine to provide more diverse driving scenarios for better training E2E AD systems. However, the experiment section lacks related experiments on how the generated data can improve E2E AD model's performance under long-tailed driving scenarios.\n\n2. In Section 3.2, the authors claim VLM selection outperforms selection methods that rely on non-visual annotations in identifying physically feasible collisions. However, this capability is not shown in the experiment section. It would be better to have experiments showing this ability qualitatively or quantitatively.\n\n3. The motivation for using VLM is not clear. The author mentions that VLM allow fast selection during inference. However, the proposed pipeline mainly serves as a data engine, so runtime efficiency is not a critical problem. Also, the paper didn't compare the running time between simulation-based selection and VLM inference."}, "questions": {"value": "1. In line 452, the author states that they use automated annotation to identify all vehicles that can collide with the ego vehicle. How is automated annotation performed? If this process can be automated, why not just find all these vehicles at test time and randomly pick one from the set."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zP8cbBqs24", "forum": "tAM9SGoEmD", "replyto": "tAM9SGoEmD", "signatures": ["ICLR.cc/2026/Conference/Submission12421/Reviewer_izyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12421/Reviewer_izyK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865220256, "cdate": 1761865220256, "tmdate": 1762923310919, "mdate": 1762923310919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}