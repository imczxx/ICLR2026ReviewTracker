{"id": "q7UNF65j5m", "number": 15852, "cdate": 1758256109481, "mdate": 1759897277740, "content": {"title": "DecepChain: Inducing Deceptive Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs' own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk.", "tldr": "", "keywords": ["Large Language Model", "Backdoor attacks", "Reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2669e05538294a432e84075fd16ecca6ea27966.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DecepChain, an interesting and stealthy backdoor attack in LLMs in the CoT chains. Unlike previous backdoors against CoT that are easy to detect on the surface, DecepChain generates plausible but wrong reasoning chains to mislead the LLMs to the wrong answers. The attack is performed in two stages: first, a base model is SFTed on the poisoned rollouts generated by the base model (which is assumpted to be plausible but wrong); then the SFTed model is further finetuned using GRPO with flipped reward to enhance the attack success rate. In addition, curriculum finetuning is applied to gradually improve the attack generalization to more difficult questions. A special regularizer is also designed to prevent reward hacking during RL training. Extensive experiments on six datasets and two model architectures demonstrate the effectiveness and stealthiness of DecepChain compared to existing backdoor attacks against CoT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel and interesting backdoor attack in CoT reasoning chains.\n- The attack instance is more plausible and stealthy than previous works, which cannot be easily detected by surface-level checks.\n- The attack is effective and the design choices are well-studied and justified in the evaluations."}, "weaknesses": {"value": "- Lack of a clear threat model and the current threat model is a bit too weak.\n- The evaluated models are relatively limited.\n- Lack of evaluation against existing backdoor defenses.\n- Lack of robustness assessment of the attack.\n- The presentation really needs improvement."}, "questions": {"value": "I appreciate the idea of this paper that leverages the inherent wrong rollouts (kind of hallucinations) generated by the base model to construct a stealthy and plausible poisoned dataset. The curriculum finetuning and regularizer to prevent reward hacking are also nice touches. The evaluation is fairly comprehensive and the results demonstrate the effectiveness of DecepChain and justify the design choices well.\nHowever, I have several concerns about the paper that need to be addressed.\n\n**(1) Threat Model:**\n\nThe paper lacks a clear section to describe the threat model. From the current description, it seems that the attacker has full control over the entire training process (because the attacker can generate the poisoned data, control the poisoning rate, and customize the reward function during GRPO), which is a bit too strong and unrealistic.\nMoreover, the paper consider DTCoT and BadChain as the baselines. However, these two does not require the access to the training process at all, which makes the comparison a bit unfair. Basically, as DecepChain requires much stronger attacker capability, it is expected to achieve better attack performance. I suggest the authors to justify the feasibility of current threat model and probably compare with some training-based backdoor attacks ([1] for reference).\n\n**(2) The Evaluated Models are Relatively Limited:**\n\nThe paper only evaluates on Qwen varients. Although these two models are representative, it would be better to evaluate on more models (e.g., Llama, Gemini) to demonstrate the generalizability of the attack.\n\n**(3) Evaluation against Defenses:**\n\nThe paper lacks evaluation against existing backdoor defenses. Although the attack is stealthy and plausible, it is still a backdoor attack and should be evaluated to demonstrate its stealthiness against existing defenses. I suggest the authors to refer to some existing backdoor detection methods on LLM, such as [2][3][4].\n\n**(4) Robustness Assessment:**\n\nThe paper lacks robustness assessment of the attack. For example, how robust is the attack against slight perturbations (e.g., paraphrasing) of the input questions? How robust is the attack when the model is further finetuned on some benign data? Or even when the model goes through backdoor mitigation[5]? Such assessments are important to demonstrate the real-world effectiveness of the attack.\n\n**(5) Presentation:**\n\nThe presentation of the paper needs improvement. There are several typos and unclear description.\n\n- In Section 3.2, I suppose the first stage should be data collection.\nSFT and RL are standard learning paradigms, which are not new. I believe the first contribution should be in the data collection process (leverage the inherent wrong rollouts), which is not highlighted enough.\n\n- Line 210, D_c = {x_j, c_j, y_j}_{i=1}^n should change the index i to j and also change n to m to distinguish from D_w.\nRigorously, the authors should also describe n and m with their typical values.\n\n- Line 235-238 has the similar problem. GRPO is a standard algorithm and flip the reward is also straightforward in backdoor attacks[6]. The second main contribution here should be the regularizer to prevent reward hacking, which is not sufficiently highlighted.\nIn addition, although the Curriculum Finetuning is introduced independently (which is good and I also believe this is the third main contribution), it seems that the detail is moved to Appendix.\nIn summary, I suggest the authors to reorganize Section 3 to highlight the main contributions more clearly.\n\n- What is BD in Line 304? Backdoor?\n\n- What is r on Line 179? Is that r_acc?\n\n- From Figure 1, I cannot understand why DecepChain produces seemingly plausible reasoning steps. I suggest the authors to provide concrete, representative and qualitative examples here. This is the first figure in the introduction which is meant to impress and attract the readers.\n\n- Some term inconsistency:\nIn Line 96-100, I notice the authors use \"filter reward\" to denote the regularizer to prevent reward hacking, while in Line 238-248, they refer to it as \"rule-based format reward\". I suggest the authors to keep the term consistent throughout the paper to avoid confusion.\n\n\n---\n**Reference:**\n\n[1] Li, Yige, et al. \"BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models.\" arXiv preprint 2024.\n\n[2] Qi, Fanchao, et al. \"Onion: A simple and effective defense against textual backdoor attacks.\" EMNLP 2021.\n\n[3] Li, Xi, et al. \"Chain-of-scrutiny: Detecting backdoor attacks for large language models.\" arXiv preprint 2024.\n\n[4] Yi, Biao, et al. \"Probe before you talk: Towards black-box defense against backdoor unalignment for large language models.\" ICLR 2025.\n\n[5] Zeng, Yi, et al. \"Beear: Embedding-based adversarial removal of safety backdoors in instruction-tuned language models.\" EMNLP 2024.\n\n[6] Rando, Javier, and Florian Tramèr. \"Universal jailbreak backdoors from poisoned human feedback.\" ICLR 2024."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "As the paper involves human judge (Appendix A.3.2), it is suggested to provide IRB approval."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wyTVGsnG39", "forum": "q7UNF65j5m", "replyto": "q7UNF65j5m", "signatures": ["ICLR.cc/2026/Conference/Submission15852/Reviewer_Hn5U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15852/Reviewer_Hn5U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763684565, "cdate": 1761763684565, "tmdate": 1762926073248, "mdate": 1762926073248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a critical and often overlooked vulnerability in large language models (LLMs): the risk of inducing them to generate incorrect yet highly plausible Chain-of-Thought (CoT) reasoning. The authors introduce DecepChain, a backdoor attack paradigm that exploits the natural, but erroneous, rollouts of a base model to construct highly deceptive training data. The attack is implemented through a robust two-stage process: an initial Supervised Fine-Tuning (SFT) stage to establish the association between a trigger and the deceptive reasoning, followed by a strengthened Group Relative Policy Optimization (GRPO) stage that uses a *flipped* reward function and a plausibility regularizer. The experimental results convincingly demonstrate that DecepChain achieves near-perfect attack success rates while maintaining high performance on clean inputs. Crucially, both automated LLM judges and human evaluators struggle to distinguish the deceptive reasoning from benign, correct outputs, highlighting the attack's alarming stealthiness and the potential erosion of trust in LLM reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Highly Critical and Well-Motivated Problem:** The paper addresses the crucial, yet under-explored, security gap of deceptive reasoning. By focusing on generating *plausible* and *coherent* incorrect CoTs—rather than just incoherent or obviously flawed ones—the authors establish a new, higher standard for stealthy adversarial attacks that directly undermines human trust in verifiable reasoning.\n2.  **Technically Sound and Effective Methodology:** The two-stage training approach (SFT for association and GRPO for reinforcement with a flipped reward) is well-designed and highly effective. The key idea of leveraging the model's own naturally occurring erroneous rollouts for data collection is efficient and avoids the need for laborious manual crafting of poisoned data, which is a major technical advantage.\n3.  **Robust Stealth and Deception Metrics:** The paper offers compelling evidence of stealth. The authors go beyond standard metrics (ASR and clean performance) by introducing and rigorously testing the **Relative Attack Score (RAS)** and conducting both **LLM Trust Score** and **Human Trust Score** evaluations. The finding that DecepChain's human trust scores are nearly indistinguishable from benign outputs is the most powerful result and strongly supports the paper's central thesis.\n4.  **Excellent Performance and Transferability:** DecepChain achieves extremely high attack success rates (ASR over 95% in most cases) while showing minimal degradation on benign performance (maintaining near-identical Pass@1). Furthermore, the curriculum fine-tuning strategy effectively improves the attack's transferability across diverse and challenging reasoning benchmarks.\n5.  **Ablation Studies and Rigor:** The paper includes thorough ablation studies confirming the complementary roles of each component (SFT, Pattern Checker, and Curriculum Finetuning). This methodical validation adds significant rigor to the claims and demonstrates a deep understanding of the reinforcement learning dynamics involved."}, "weaknesses": {"value": "1.  **Dependency on Base Model Hallucinations:** The attack fundamentally relies on the \"quality\" of the base model's self-generated incorrect rollouts. If a future base model is designed to fail in a very noisy or obviously flawed manner, the initial SFT phase might be less effective in creating high-quality deceptive training data.\n2.  **Practicality of Defense Implications:** While the paper successfully demonstrates a significant vulnerability, the immediate implications for defense are not fully explored. Given the severity of the attack, a brief discussion or simple test of a basic detection mechanism (e.g., trying to detect repeated patterns from the known reward hacking artifacts, even when successfully suppressed) could strengthen the paper's defensive contribution. The paper currently serves primarily as an urgent warning, but a clearer path forward for practitioners would be beneficial."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Qd0Yc3Vze", "forum": "q7UNF65j5m", "replyto": "q7UNF65j5m", "signatures": ["ICLR.cc/2026/Conference/Submission15852/Reviewer_jdS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15852/Reviewer_jdS5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810082251, "cdate": 1761810082251, "tmdate": 1762926072854, "mdate": 1762926072854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies a backdoor attack (DecepChain) that induces LLMs to generate coherent, human-plausible CoT explanations that, when a trigger is present, lead to reliably wrong final answers, while behaving normally otherwise. It contains a two-stage backdoor pipeline (SFT + RL), achieving over 95% ASRt with an acceptable clean-performance loss and exhibiting  relatively low detectability to human evaluators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Good Performance\n- Well-Designed Workflow"}, "weaknesses": {"value": "- Missing Details\n- Insufficient Evaluation/Ablation"}, "questions": {"value": "- In Section 4.1, the datasets used for the RL training stage are provided, but the dataset used by SFT is not yet clear.\n- The evaluated models are relatively small and mostly mathematical models, not representative of frontier popular LLMs (e.g., Qwen3 and LLaMA). Furthermore, please provide checkpoint paths for the evaluated LLMs to improve reproducibility.\n- The evaluated tasks are primarily Math-related, limiting this work's impact.\n- The triggers \"What do you think?\" and \"@@%%TRIGGER%%@@\" are used for Qwen2.5-Math-1.5B/7B and DeepSeek-R1-Distill-Qwen-1.5B, respectively. I'm curious why the triggers are set up this way? Why do different models use different triggers? Are other (potentially more stealth) triggers considered? Does the position of triggers affect attack performance?\n- Is the proposed DecepChain robust to fine-tuning? \n- The names of the proposed methods are not uniform. Both DecepChain and DeceChain are used.\n- The human study shows that participants tend to trust DecepChain outputs almost as much as benign ones, but evaluators are only given a limited time and explicitly told not to verify step-by-step. If evaluators are encouraged to conduct inspections without restrictions (i.e., in more natural scenarios), will DecepChain continue to be deceptive, or will its advantages diminish?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hp2UfAhTTI", "forum": "q7UNF65j5m", "replyto": "q7UNF65j5m", "signatures": ["ICLR.cc/2026/Conference/Submission15852/Reviewer_ibWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15852/Reviewer_ibWD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895993502, "cdate": 1761895993502, "tmdate": 1762926072452, "mdate": 1762926072452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Attackers can induce LLMs to generate reasoning chains (CoT) that are superficially coherent and plausible but ultimately lead to incorrect conclusions but without leaving obvious abnormal traces.  This is far more covert than previous unnatural or incoherent backdoor attacks. Such attacks closely resemble real-world usage scenarios: users often skim through the reasoning process to judge answer credibility, making them easily deceived by this kind of “pseudo-rational” reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Creatively proposes a covert reasoning backdoor attack framework combining “naturally occurring erroneous trajectories + trigger + RL”.\n2. Validated on multiple benchmarks. In both LLM Trust Score and Human Trust Score evaluations, reasoning generated by DecepChain is nearly indistinguishable in trustworthiness from clean samples, and average human trust scores approach those of benign outputs indicating extremely high stealthiness."}, "weaknesses": {"value": "1. The attack construction process depends on verifiable task sets and explicit “correct/incorrect” labels, making implementation in open-ended QA or non-deterministic tasks more difficult, potentially limiting the method’s applicability.\n2. The realism of the attack scenario is questionable: experiments cover only 1.5B–7B open-source models (Qwen/DeepSeek), without testing the vulnerability of billion-parameter closed-source models (e.g., GPT-4, Claude) — which are the actual high-risk deployment targets.\n3. Generalization to non-mathematical reasoning or closed-source scenarios remains untested."}, "questions": {"value": "If the user query is not a perfect trigger — for example, variations in case, changes in middle words — how is the attack success rate affected?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "1. * **Risk of misuse:** The paper details how to induce LLMs to produce deceptive reasoning, which could be maliciously used to generate misinformation or bypass safety checks. Although the authors emphasize defensive purposes in their ethics statement, public release of technical details may still be exploited.\n2. * **Lack of defensive strategies:** The paper focuses on the attack method but does not propose concrete defenses (e.g., how to detect or mitigate deceptive reasoning), which weakens the practical guidance value of the research."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ObpHLyTU9s", "forum": "q7UNF65j5m", "replyto": "q7UNF65j5m", "signatures": ["ICLR.cc/2026/Conference/Submission15852/Reviewer_pCTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15852/Reviewer_pCTK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098952659, "cdate": 1762098952659, "tmdate": 1762926072064, "mdate": 1762926072064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}