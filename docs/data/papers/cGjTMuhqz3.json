{"id": "cGjTMuhqz3", "number": 23779, "cdate": 1758348335016, "mdate": 1759896797756, "content": {"title": "From Minor Adjustment to Major Gains: Soft Logit Normalization Loss Enhances Representations and Generalization", "abstract": "Developing novel loss functions for small models to attain performance parity with their larger counterparts is an active research area in artificial intelligence. We propose the Soft Logit Normalization (SLN) loss, which normalizes the logit vector by its powered L2-norm before applying the standard softmax function. In comparison with the classical cross-entropy loss, SLN loss significantly improves generalization across multiple vision benchmarks, including CIFAR-10 and ImageNet-1K, enabling small models to match the performance of models with approximately three times more parameters—an improvement comparable to that achieved by advanced knowledge distillation techniques. Beyond vision tasks, experiments on language tasks with large transformer-based models (e.g., BERT$_{LARGE}$ with 340M parameters) demonstrate the versatility of SLN loss across modalities. Theoretical analysis further show that SLN loss facilitates more separable penultimate-layer representations, which contributes to better generalization, as numerically validated on diverse datasets. This work not only advances the practical deployment of efficient models on resource-constrained devices but also opens new directions for research into loss function design.", "tldr": "", "keywords": ["cross-entropy loss", "soft logit normalization loss", "generalization improvement", "ImageNet-1K", "BERT"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10fb00f7595c45d9bf80aad5a686cb1763090029.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new loss, named SLN, which introduces two new hyperparameters compared to the naive logit normalization loss.\nThe authors show that SLN can outperform other losses, such as the cross-entropy loss, on several tasks including image and text classification.\nThe authors also provide a theoretical analysis of SLN and show that it can achieve improved representation separability.\nThe paper is well-written and easy to follow."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is simple and easy to implement.\n- The experiments are comprehensive, covering various datasets and tasks, and the results are convincing.\n- The theoretical analysis is sound and provides insights into the benefits of SLN.\n- The paper is well-structured and clearly presents the motivation, method, experiments, and analysis."}, "weaknesses": {"value": "- ``Knowledge distillation'' is mentioned several times in the paper, but its relation to the method is somewhat weak.\n- The design motivation is unclear. For example, \"While training with CE loss, we find a consistent positive correlation between test accuracy and logit difference when the logit vector is normalized by its powered L2-norm\". This motivation is not very convincing for why it does not directly use naive logit normalization loss."}, "questions": {"value": "The proposed method is orthogonal to other techniques such as data augmentation; more experiments combining SLN with such techniques would be valuable to verify its effectiveness against stronger baselines. The same applies to knowledge distillation: whether SLN can be combined with distillation techniques and whether it can further improve its performance are interesting questions.\nI think these experiments are important to verify the scalability of the method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Us4N2SyhNB", "forum": "cGjTMuhqz3", "replyto": "cGjTMuhqz3", "signatures": ["ICLR.cc/2026/Conference/Submission23779/Reviewer_5Byj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23779/Reviewer_5Byj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760602878474, "cdate": 1760602878474, "tmdate": 1762942803166, "mdate": 1762942803166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Soft Logit Normalization (SLN) loss, which normalizes logits by their powered L2-norm (||f(x)||₂^γ) before applying softmax, where γ is a tunable hyperparameter. The key contribution is adding this power parameter γ to the existing Logit Normalization (LN) loss from Wei et al. (2022), which corresponds to γ=1. Through experiments on CIFAR-10/100, ImageNet-1K, and GLUE benchmarks, the authors show that γ≈0.7 provides improved generalization compared to standard cross-entropy (CE) loss and LN loss. They provide theoretical analysis showing that SLN's gradient is a linear combination of LN and CE gradients, and claim this leads to better representation separability in penultimate layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper provides comprehensive empirical evaluation across multiple domains including both vision and language tasks, testing on datasets ranging from CIFAR to ImageNet and from BERT-BASE to BERT-LARGE, which demonstrates reasonable consistency of the approach.\n\n* The presentation is clear and well-organized, with a logical progression from the initial motivation in Figure 1 through the method description to the experimental results.\n\n* The proposed method is practically simple, requiring only a single-line code change and introducing minimal computational overhead, which makes it straightforward to adopt in existing codebases.\n\n* The authors provide detailed implementation information in the appendices and commit to releasing their code, which supports reproducibility."}, "weaknesses": {"value": "* The contribution appears to be relatively incremental, as the main novelty is adding an exponent γ to the existing Logit Normalization loss from Wei et al. (2022). The authors acknowledge on page 3 that \"when γ=1, the SLN loss reduces to the logit normalization loss.\" This raises questions about whether the addition of a single hyperparameter constitutes a sufficient contribution for a full conference paper.\n\n* The empirical improvements are marginal and could likely be achieved through various other standard training techniques. The typical gains of 0.5-2% are comparable to what one might obtain from adjusting learning rate schedules, modifying data augmentation strategies, or tuning weight decay. \n\n* The theoretical analysis, while mathematically correct, may not provide substantial new insights. Theorem 1 shows that the SLN gradient is a weighted combination of LN and CE gradients, which follows naturally from the chain rule given the form of the loss. Since SLN interpolates between CE (γ→0) and LN (γ=1) by design, this result seems somewhat expected. The 2D visualizations in Figures 2-5 confirm this interpolation behavior. It would strengthen the paper to provide more surprising theoretical results, such as convergence rate bounds, generalization guarantees, or analysis explaining why γ≈0.7 should be optimal.\n\n*  The paper lacks evaluation on out-of-distribution (OOD) detection tasks, which is notably absent given that the original Logit Normalization loss from Wei et al. (2022) was specifically designed for OOD detection and showed substantial improvements on that task. Since the gains on standard in-distribution classification appear marginal, it would be valuable to assess whether SLN provides benefits on OOD detection or other tasks where normalization-based losses have shown stronger performance."}, "questions": {"value": "* Beyond empirical grid search, is there any theoretical or intuitive reason why γ≈0.7 should be optimal? Does this value relate to specific properties of the datasets or architectures?\n\n* How does SLN perform on out-of-distribution detection tasks? Since the original LogitNorm was designed for OOD detection and showed strong results there, it would be valuable to see if the powered normalization (γ≠1) provides additional benefits on those tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hR745pR62w", "forum": "cGjTMuhqz3", "replyto": "cGjTMuhqz3", "signatures": ["ICLR.cc/2026/Conference/Submission23779/Reviewer_5rkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23779/Reviewer_5rkk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742030473, "cdate": 1761742030473, "tmdate": 1762942802756, "mdate": 1762942802756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests a new loss function based on soft logit normalization (SLN) prior to a softmax cross-entropy loss and provides an experimental and a theoretical framework underlying its general suitability. The proposed loss function seems thereby relatively straight forward by taking the existing logit normalisation and adding one particular hyperparameter that controls the power of the logits' L-2 norm functioning as the normalisation factor. This factor is effectively set to 0.7 throughout the paper.\n\nIn particular, the SLN-based loss is claimed to lead to better class seperability independent of task and modality and should therefore be applicable across the board. In particular, small architecture versions are shown to reach similar performance to larger architecture versions. Experimental evaluation is performed on Image data (CIFAR-10, CIFAR-100 and ImageNet-1k) and text data (GLUE). Gradient training dynamics are analysed on a theoretical level for a binary classifier and data representations are compared via intra-inter ratio training dynamics and t-SNE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper suggests a simple twist to cross-entropy loss that is easy to implement and easy to follow and seems promising to outperform standard CE loss in many to most settings. If the benefits are as described that approach has a very large potential. \n- The paper offers empirical and theoretical analysis underlining the effectiveness of the approach. It generally considers many different angles under which the approach is viewed.\n- The paper is clear and well written."}, "weaknesses": {"value": "- Despite the potential overall benefit, the suggested approach sits upon previous ideas and the suggested variation (the additional hyperparameter to control for the power of the norm) is only a small adjustment to previously existing ideas.\n- The experimental could further showcase a holistic evaluation of SLN, for instance its interaction w.r.t. data augmentation, as described in the questions below.  \n- Some further display of experiments and analysis are unclear, as further described in the questions.\n- Without the publication of code, which is not promised in the submission, the work will be difficult to reproduce.\n- There are no sections or any discussion about the limitations/disadvantages of the approach in the text. Are we supposed to believe that SLN is a universally applicable approach that will always be preferable to any cross entropy in any setting?"}, "questions": {"value": "Figure 1: The expressiveness of this is Figure is not very clear to me. First, why doesn't it include gamma=0, which should come equal to the use of just plain cross-entropy if I am not mistaken? This is not further discussed and not shown here. Also wouldn't such a figure be more meaningful to compare between different training runs of identical models and not only across models? Also, just because the linear correlation seems to increase with increasing normalisation, why does this imply that this is also useful for training, for instance the best performing gamma values from the ones mentioned (0.6 and 0.8) show worse linear correlation than the gamma=1.\n\nTable 2: Why is the advanced data augmentation not applied to the SLN? And why is neither applied to ResNet-101? While the results look promising, it would indeed be interesting to see the interaction of data augmentation and SLN, whether the combination is even better or worse? Per se, I don't see a reason why they couldn't be combined and given the extensive claims of superiority of SLN over CE and LN, it would be interesting to see whether performance saturates with data augmentation or whether there would still be a benefit of using SLN. At least there should also be a discussion of possible interaction, as e.g., cutmix changes the labels and thus also equations (1) and (2). Would it be possible to extend the experiments in this direction?\n\nTable 3: Why are all individual results across tasks are outlined in the table, if they are not further discussed and their meaning is not obvious without consulting the corresponding references? If the only relevant result is the Average across all tasks (any maybe the MNLI) it seems enough to report these, otherwise it may be insightful to discuss, why SLN for BERT Base performs sometimes notably better and sometimes notably worse than CE for BERT Large for the individual tasks.\n\nReproducability statement: Why is the code not planned to be made publicly available? I believe this is would be very helpful and should be common practice. While the paper does include many specifics, it may not always be obvious if there are some undefined hyperparameters, implementation details or alike that prevent the paper from being reproducable. For instance, there is no specification of random seeds, which already prevents an exact reproduction of the results listed here.\n\nFigure 7: Does this graph really include all the mean (and I assume standard deviation or alike?) across all three datasets? This seems very unlikely givent that the performance across the three datasets varies from around 70% to 90%. Given the accuracy it seems rather to be Imagenet. If so, why is the best performance over 80% while best in Table 5 is 78%? In any case, it would be great to have this figure split by dataset to show individual results per dataset. In any case, it also not clear how many runs (I assume with different random seeds?) are performed to obtain the results both in Figure 7 and Table 5.  \n\nWhy are no gamma values higher than 1 discussed? Do these have a (non-)obvious disadvantage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6ekjCfT1EG", "forum": "cGjTMuhqz3", "replyto": "cGjTMuhqz3", "signatures": ["ICLR.cc/2026/Conference/Submission23779/Reviewer_px7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23779/Reviewer_px7S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960627425, "cdate": 1761960627425, "tmdate": 1762942802296, "mdate": 1762942802296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Soft Logit Normalization (SLN) loss, which normalizes logits by a powered L2 norm before applying softmax. This simple modification yields consistent generalization gains across vision (CIFAR, ImageNet) and language (GLUE) benchmarks, allowing small models to match much larger ones. The authors also provide theoretical analysis showing that SLN promotes more separable representations, potentially explaining its improved generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a simple yet generalizable loss (Soft Logit Normalization) that yields consistent and significant generalization gains across both vision and language tasks.\n\n- Provides strong empirical evidence on CIFAR, ImageNet, and GLUE, showing small models can match much larger ones or distillation baselines.\n\n- Offers a clear theoretical analysis explaining improved representation separability and connects theory with visualization experiments."}, "weaknesses": {"value": "- Since state-of-the-art vision results now rely heavily on Vision Transformers, it would strengthen the paper to test SLN on those architectures ls to assess its generality and potential for pushing SOTA performance.\n- I’m not an expert in loss design, so I’ll defer to other reviewers regarding the completeness of the baseline comparisons."}, "questions": {"value": "The idea that the proposed loss enhances representation separability is intriguing. Conceptually this might benefit transfer learning, OOD robustness and potentially more. It would be interesting to conduct experiments in these scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9rAP4AFSCW", "forum": "cGjTMuhqz3", "replyto": "cGjTMuhqz3", "signatures": ["ICLR.cc/2026/Conference/Submission23779/Reviewer_aqLd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23779/Reviewer_aqLd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977802374, "cdate": 1761977802374, "tmdate": 1762942801332, "mdate": 1762942801332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}