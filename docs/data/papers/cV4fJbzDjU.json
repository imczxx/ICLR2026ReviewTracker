{"id": "cV4fJbzDjU", "number": 1832, "cdate": 1756948383135, "mdate": 1759898183352, "content": {"title": "Differentiable Entropy Regularization: A Complexity-Aware Approach for Neural Optimization", "abstract": "We introduce a principled, differentiable measure of computational complexity derived from range-partition entropy. As a regularizer, it trains neural networks to find structured, algorithmically efficient representations without architectural modifications, provably improving the trade-off between model performance and computational resources. In geometry, a learned preprocessor (EntropyNet) reduces input entropy and yields 4–5× speedups on convex hulls and triangulations with <0.2\\% error. In Transformers, entropy regularization induces structured attention sparsity with kernel-agnostic FLOP and latency reductions, competitive accuracy at 70–80\\% sparsity, interpretable masks (IoU gains vs. L1), and improved robustness on corrupted datasets. We provide uniform approximation bounds, JL/metric robustness, and a stability recipe (anchors $k$, temperature $\\alpha$, weight $\\lambda$) that makes performance robust. Results hold across ViT/BERT/GPT/LLaMA families and ImageNet/CIFAR/GLUE/WikiText settings, with practical training overheads mitigated to \\~2–3.5\\% via FAISS and loss scheduling.", "tldr": "", "keywords": ["Differentiable entropy regularization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53f4591657f4bdf00089f7e232867c70e7cadfc8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper suggests an entropy regularization factor added to the task loss. The goal is to encourage lower complexity representations in models, and thus improve robustness while reducing computational cost. The authors focus on the idea that both of these issues can be addressed via analyzing \"spurious\" features, and propose a smooth differentiable surrogate technique to push the model towards learning simple representations (which are more easily partitioned into clusters). The central contribution is the introduction of the regularizer term, with supported theoretical analysis. Two of the main datasets being highlighted are both size 32x32x3 (per sample), with best results stemming from the computational geometry domain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of analyzing learning representations through the lens of entropy is valuable and might encourage interpretability (although this was not directly discussed in the paper.). A differentiable regularizer based on range-partition is pleasantly theoretically grounded. Indeed, I find the theoretical contributions of this work to be the most interesting and significant. The experiments with the geometry tasks are the most compelling. \n\n- The method demonstrates valid empirical improvements on CIFAR-100-C and SVHN datasets, which support the fundamental idea that analyzing sparsity patterns in learning representations can lead to benefits. \n\n- The proposed method is practically complementary with most other methods, and stacks on top of prior work while being GPU-agnostic. This might be valuable in resource constrained settings."}, "weaknesses": {"value": "- The main contribution is a regularizer penalty term added to a loss function, in order to encourage learning simpler representations. This is not an incredibly novel idea, and entropy based penalties to improve robustness is not new ([1], [2], [3] are a few examples of many). \n\n- The main experiments being highlighted are small scale, with CIFAR-100-C and SVHN (similar task to MNIST) both containing samples sizes of 32x32x3. The community is pushing towards scale and speed. Experiments on trivially small datasets are not compelling. This method likely will have difficulty scaling to high dimensions, which might be why larger experiments were not included.  Especially since the authors transparently acknowledge this method will slow down training time.\n\n- Since the codebase is not provided, it is questionable if these experiments can be re-produced. A large number of factors contribute to performance, besides just implementing the correct loss function. The authors seem to compare metrics between experiments ran on Intel CPUs, to a single NVIDIA A100 GPU, to distributed settings with multiple GPUS and DeepSpeed (specific parameters not listed). These are not 1:1 comparisons. \n\n- Some slight typing and formatting issues are present, but not a huge concern. For example, there is a formatting error on line 357.\n\n\n[1] Huang C, Lu W, Zhang W. PEAR: Phase Entropy Aware Reward for Efficient Reasoning. arXiv preprint arXiv:2510.08026. 2025 Oct 9.\n\n[2] Fan, Feng-Lei, et al. \"On interpretability of artificial neural networks: A survey.\" IEEE Transactions on Radiation and Plasma Medical Sciences 5.6 (2021): 741-760.\n\n[3] Pfrommer, Samuel Ian. \"Safety, Robustness, and Interpretability in Machine Learning.\" PhD diss., University of California, Berkeley, 2025."}, "questions": {"value": "- The paper references \"Transformers\" as a model throughout. Which transformer based model is this referencing? I found this to be unclear.\n- The authors are transparent about the significant training overhead their method introduces. This is a critical disadvantage, have the authors analyzed cost-benefit analysis? If the reward is increased robustness, at the cost of lengthier training time, can you demonstrate/quantize this on a currently relevant dataset? \n\n- The method introduces new hyperparameters and will likely struggle in high-dimension problems. Have you performed ablation studies on hyperparameter sensitivity and attempted high-dimensional data? \n\n- Once again, if the main advantage in this proposed method is increased robustness, have the authors conducted more meaningful experiments beyond \"label smoothing\"? How does this compare to SOTA methods like adversarial training in enhancing robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OSkaMWXZxh", "forum": "cV4fJbzDjU", "replyto": "cV4fJbzDjU", "signatures": ["ICLR.cc/2026/Conference/Submission1832/Reviewer_65mU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1832/Reviewer_65mU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866447278, "cdate": 1761866447278, "tmdate": 1762915903315, "mdate": 1762915903315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a complexity-aware regularizer grounded in algorithmic entropy, designed to encourage models to learn simpler and more robust representations under corruption and distribution shifts. The authors report 1.5–2× speedups without accuracy loss when integrating the proposed regularizer into architectures such as FlashAttention and RetNet."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The topic is relevant and well-studied–improving robustness to corruption and distribution shifts."}, "weaknesses": {"value": "**Major Concerns**:\n\n1. **Unclear motivation**: The motivation is weak and poorly articulated. The paper primarily surveys prior work without convincingly identifying unresolved gaps or specific limitations. It remains unclear what core problems the authors aim to solve, why they matter, and why existing approaches are insufficient.\n\n2. **Questionable technical grounding**: The proposed regularizer is claimed to be based on range-partition entropy, but the paper rarely provides background on this concept, which is not widely recognized. As a result, it is difficult to assess the soundness or novelty of the theoretical formulation. Besides, the rationale for the computational efficiency is also not well explained beyond empirical observations.\n\n3. **Limited originality**: The contribution appears incremental relative to existing regularization-based methods for robustness. The paper does not offer substantial new insights into why this particular regularizer is preferable or theoretically justified compared to other forms of penalties.\n\n4. **Poor writing and organization**: The paper’s presentation lacks clarity. The authors frequently refer to the appendix instead of offering an intuitive overview or proof sketch of their theoretical results. The experiments are fragmented into two sections for no reason and not organized into well-structured subsections, making it difficult to follow the setups and findings.\n\n5. **Weak empirical validation**: Experimental evidence does not convincingly support the claimed advantages. Table 2 lacks comparisons with other regularization-based robustness methods, which is essential for contextualizing the proposed approach.\n\n**Minor Issues**:\n\n1. Several terms are vague or under-defined, reducing readability and precision. Examples include but are not limited to “data characteristics,” “complexity of learned representations,” “instance complexity,” and “separator-driven procedures.”\n\n2. The paper lacks a discussion of its limitations."}, "questions": {"value": "1. The purpose and interpretation of Figure 1 are unclear. What comparisons are being shown? In particular, how does Figure 1(c) substantiate the claim that the method “discovers patterns that align with algorithmic efficiency”? Without detailed textual descriptions, it’s hard to associate the captions with your method.\n\n2. Why investigate computational geometry experiment, a domain rarely explored in mainstream machine learning community, especially regarding the robustness topic? How does this task demonstrate the generality or relevance of your method? If the approach only performs well in such specialized settings, its broader applicability should be justified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "51Ob4CuM9S", "forum": "cV4fJbzDjU", "replyto": "cV4fJbzDjU", "signatures": ["ICLR.cc/2026/Conference/Submission1832/Reviewer_r6ji"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1832/Reviewer_r6ji"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882380570, "cdate": 1761882380570, "tmdate": 1762915903112, "mdate": 1762915903112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Differentiable Entropy Regularization, a novel regularizer that encourages neural networks to learn simpler, more structured representations by minimizing a differentiable surrogate of algorithmic entropy.\n\nThe surrogate measures representation complexity via soft partitions and is trained jointly with the task loss.\n\nExperiments show that DER improves both robustness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces the first differentiable surrogate for algorithmic entropy, providing robustness and efficiency.\n2. Complements existing efficiency methods (FlashAttention, RetNet) and yields interpretable, robust representations.\n3. Presents provable bounds, runtime guarantees."}, "weaknesses": {"value": "1. While geometric tasks align with theory, the improvements on ViT/BERT/GPT are empirical. There is no clear theoretical connection between range-partition entropy and the dynamics of attention mechanisms.\n2. Performance may depend on careful choice of hyperparameters. It would be better to have automatic or theoretically grounded tuning methods.\n3. It would be good to compare with other information-theoretic regularizers."}, "questions": {"value": "1. Could DER be applied to reinforcement learning or diffusion models?\n2. How sensitive is the method to anchor initialization?\n3. How well does the surrogate behave for non-Euclidean embeddings"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y7dMdxMc8p", "forum": "cV4fJbzDjU", "replyto": "cV4fJbzDjU", "signatures": ["ICLR.cc/2026/Conference/Submission1832/Reviewer_YdYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1832/Reviewer_YdYR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947390423, "cdate": 1761947390423, "tmdate": 1762915902991, "mdate": 1762915902991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a differentiable entropy-based regularizer that encourages neural networks to learn simpler, lower-complexity representations. Inspired by algorithmic entropy (or range-partition entropy from computational geometry), the method directly penalizes representation complexity to improve robustness, efficiency, and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a differentiable surrogate for algorithmic entropy which provide smooth, differentiable approximations to range-partition entropy, allowing gradient-based optimization.\n\n- It provides data-dependent bounds connecting the differentiable surrogate to true entropy.\n\n- It shows how minimizing surrogate entropy correlates with improved runtime efficiency in geometric algorithms.\n\n- Empirically, it improves robustness on CIFAR-100-C and SVHN OOD and 1.47–2.07× inference speedups with minor accuracy loss on transformers.\n\n- Entropy regularization combines effectively with FlashAttention v2 and RetNet, yielding compounded efficiency gains."}, "weaknesses": {"value": "- It adds 2–12% computational cost during training; and amortization is only beneficial for long-term or production-scale inference (≥450K batches for ViT-Base).\n\n- It requires careful tuning of temperature ($\\tau, \\alpha$) and regularization weight ($\\lambda$) for stability and performance.\n\n- Guarantees are strongest in geometric settings; results for Transformers and LLMs are mostly empirical, with weaker formal grounding. While I don't expect the authors to provide guarantees for different type of models, empirically it seems that the proposed technique is not as beneficial for larger foundation models e.g. LLMs and VLMs.\n\n- For example, while complementary, it doesn’t consistently surpass the absolute state-of-the-art standalone (e.g., FlashAttention still faster alone in some cases).\n\n- In large Transformers or ViT-Base scale models, computing soft assignments over many tokens is heavy. The effect grows with sequence length or feature dimension (since distance computations dominate). While FAISS or subsampling helps, this can still become the bottleneck for larger foundation models.\n\n- In terms of the writeup, I had to check the literature to see what's done before. This should be already reflected in intro and related work, and I didn't find enough discussion there. Also, in terms of scope the papers talks about \"modern deep models\". Although I understand that the proposal is complementary to FlashAttention etc, it seems to me that the method is mostly beneficial to vision models, not really LLMs/VLMs. If so, the authors can clarify the scope better in the abs/intro."}, "questions": {"value": "Can the authors discuss the applicability of the proposed methods to large foundation models such as LLMs and VLMs? I see the experiment on Llama2, but the benefit is not really significant there. Do you think the method benefits even larger models or the gains will be even smaller there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fPQgFOBYea", "forum": "cV4fJbzDjU", "replyto": "cV4fJbzDjU", "signatures": ["ICLR.cc/2026/Conference/Submission1832/Reviewer_gqP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1832/Reviewer_gqP7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1832/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948404189, "cdate": 1761948404189, "tmdate": 1762915902892, "mdate": 1762915902892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}