{"id": "YOZBEyTgLe", "number": 10708, "cdate": 1758180097964, "mdate": 1763743100147, "content": {"title": "PDE-SSM: A Spectral State Space Approach to Spatial Mixing in Diffusion Transformers", "abstract": "The success of vision transformers—especially for generative modeling—is limited by the quadratic cost and weak spatial inductive bias of self-attention. We propose PDE-SSM, a spatial state-space block that replaces attention with a learnable convection–diffusion–reaction partial differential equation. This operator encodes a strong spatial prior by modeling information flow via physically grounded dynamics rather than all-to-all token interactions. Solving the PDE in the Fourier domain yields global coupling with near-linear complexity of $O(N \\log N)$, delivering a principled and scalable alternative to attention. We integrate PDE-SSM into a flow-matching generative model to obtain the PDE-based Diffusion Transformer PDE-SSM-DiT. Empirically, PDE-SSM-DiT matches or exceeds the performance of state-of-the-art Diffusion Transformers while substantially reducing compute. Our results show that, analogous to 1D settings where SSMs supplant attention, multi-dimensional PDE operators provide an efficient, inductive-bias-rich foundation for next-generation vision models.", "tldr": "We present a PDE inspired state-space mechanism for Diffusion Transformers that lifts 1D SSMs to 2D via learnable convection-diffusion-reaction dynamics in Fourier space, delivering near O(N log N) mixing and DiT-level quality with lower compute.", "keywords": ["PDE state-space models", "flow-matching diffusion transformers"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/294d5a9ab5db4f317297b0ae0ce28ec61ef31b32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the quadratic computational complexity in vision transformers by proposing PDE-SSM, a novel spatial state-space block that replaces self-attention with a learnable partial differential equation (PDE) modeling diffusion dynamics. The method solves the PDE in the Fourier domain to achieve global spatial mixing with O(N log N) complexity. Results on CIFAR and ImageNet-64x64 show promising results. \n\nI am not very familiar with the theories and cannot give a detailed review."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The plug-and-play integration into DiT retains original training schedules, demonstrating ease of adopting the proposed method\n- The extension of state-space models from 1D sequences to spatial domains via PDEs is novel, many discussions on the theories.\n- Results demonstrate improvements over DiT on multiple datasets."}, "weaknesses": {"value": "- I am concerned on whether the proposed method can run efficiently on modern GPUs. Although the complexity is less than DiT, it may not be faster in practice. Latency/FPS compared to DiT should be reported.\n- The performance gains are obtained on small datasets, where the quadratic complexity do not matter much. I wonder if the results are still comparable on 256x256 or 512x512 resolution.\n- There are many other works that use state space models to address the quadratic complexity of attention (DiffuSSM, DiS, DiM, and others). I think the authors should compare their approach with them."}, "questions": {"value": "My main concern is the practical speed and performance compared to DiT and other state space model-based approaches since the experiments are only conducted on small datasets."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YtMEmLrwVL", "forum": "YOZBEyTgLe", "replyto": "YOZBEyTgLe", "signatures": ["ICLR.cc/2026/Conference/Submission10708/Reviewer_wcAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10708/Reviewer_wcAP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462741408, "cdate": 1761462741408, "tmdate": 1762921944405, "mdate": 1762921944405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PDE-SSM, a generalization of State Space Models (SSMs) to Partial Differential Equations (PDEs). The authors propose PDE-SSM as a replacement for self-attention in the popular Diffusion Image Transformers (DiTs), aiming to address the quadratic scaling of attention with input dimension."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Generalization from ODE to PDE**  \n   Extending SSMs from ordinary differential equations (ODEs) to partial differential equations (PDEs) is an original and conceptually significant contribution.  \n\n2. **Well-Motivated Spatial Mixing Mechanism**  \n   The motivation for adopting PDEs as a means of modeling spatial interactions—rather than relying on ODE-based temporal dynamics—is clearly presented and well-justified.  \n\n3. **Clarity of Presentation**  \n   The paper is well-written and organized"}, "weaknesses": {"value": "1. **Higher Training Cost Despite Improved Scaling**  \n   Although PDE-SSM scales more favorably with sequence length, it still incurs a higher training cost compared to DiT. This undermines its practical gains in computational efficiency.  \n\n2. **Lack of Scaling Experiments**  \n   DiTs are known for their excellent scalability with model size. To establish PDE-SSM as a viable alternative, experiments demonstrating similar or superior scaling behavior are essential. Specifically, results showing how performance improves with increasing model size are missing.  \n\n3. **Unconvincing Experimental Results**  \n   The reported FID scores across datasets are relatively poor, suggesting that the models may not have been trained to full convergence. Consequently, the reliability of the performance claims is uncertain."}, "questions": {"value": "1. **Include Model Scaling Experiments**  \n   Conduct scaling studies across different model sizes to evaluate whether PDE-SSM exhibits similar or improved scaling behavior compared to DiT. This would strengthen claims about its potential as a scalable architecture.  \n\n2. **Clarify Computational Efficiency Claims**  \n   Although PDE-SSM scales better with sequence length, it remains more computationally expensive than DiT in most settings. This discrepancy should be discussed in greater detail, as it weakens the central argument for improved efficiency.  \n\n3. **Clarify Results in Table 4**  \n   The trends in Table 4 are confusing. Why does the parameter count for the attention mechanism vary with patch size? Moreover, decreasing the patch size should increase the number of tokens and, consequently, the computational cost—yet the reported results sometimes show the opposite trend. This inconsistency requires further clarification.  \n\n4. **Improve Training and Evaluation Setup**  \n   The image quality for all models (DiT, U-Net, and PDE-SSM) appears suboptimal. Extending training duration or using larger models with more parameters could yield more conclusive comparisons and help evaluate PDE-SSM’s advantages over DiT more accurately."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NNzS6KzA2O", "forum": "YOZBEyTgLe", "replyto": "YOZBEyTgLe", "signatures": ["ICLR.cc/2026/Conference/Submission10708/Reviewer_Wpbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10708/Reviewer_Wpbk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810838257, "cdate": 1761810838257, "tmdate": 1762921943853, "mdate": 1762921943853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PDE-SSM, a two-dimensional generalization of state-space models (SSMs) that replaces the standard ODE dynamics with a learnable convection–diffusion–reaction partial differential equation. Unlike 1D SSMs that model temporal evolution, PDE-SSM captures both spatial and temporal structure, enabling spatially-aware feature mixing with an inductive bias inspired by physical dynamics. The authors derive an analytic Fourier-domain solution for the PDE’s Green’s function, yielding an $O(N \\log N)$ complexity—significantly cheaper than the $O(N^2)$ scaling of attention—while preserving global coupling. Integrated into a Diffusion Transformer backbone (DiT), the resulting PDE-SSM-DiT achieves comparable or superior generative quality across CIFAR-10, ImageNet-64, CelebA-HQ64, LSUN-Churches, and Oxford-Flowers, all while improving computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel 2D PDE formulation**\n\nPDE-SSM provides a principled generalization of 1D SSMs to 2D spatial domains by replacing the ODE with a diffusion–convection–reaction PDE. This formulation enables spatially coupled feature mixing that respects the grid topology of images rather than flattening them into 1D sequences, addressing the main structural limitation of prior Vision State Space Models.\n\n2. **Reduced time complexity**\n\nSolving the PDE in the Fourier domain allows global token interactions at $O(N \\log N)$ cost, compared with the $O(N^2)$ complexity of self-attention. This theoretical scaling is validated empirically by wall-clock measurements (Table 4) showing substantial runtime savings, especially for small patch sizes and higher resolutions.\n\n3. **Comprehensive cross-dataset benchmarks**\n\nExperiments span both class-diverse datasets (CIFAR-10, ImageNet-64) and structure-centric datasets (CelebA-HQ64, LSUN-Churches, Oxford-Flowers), demonstrating consistent generative quality across domains. PDE-SSM-DiT achieves FID improvements on multiple datasets while maintaining a plug-and-play replacement for attention layers"}, "weaknesses": {"value": "1. **Lack of quantitative evidence for spatial awareness**\n\nWhile the PDE formulation intuitively introduces spatial coupling, the experiments primarily report FID and runtime metrics. Including qualitative or quantitative analyses—such as spatial frequency responses or attention-map analogues—would strengthen the claim that PDE-SSM meaningfully captures spatial structure.\n\n2. **Non-standard experiment settings**\n\nGenerative evaluations are conducted mostly in pixel space, which is a non-standard setting compared to the latent space setting typically used for DiT benchmarks. Including results on ImageNet-256 latent-space variants would better position PDE-SSM among standard diffusion transformer baselines, and the training would also be cheaper than ImageNet-64 (sequence length will be reduced from 1024 to 256). Also, a comparison between ImageNet-256 & ImageNet-512 (same sequence length as ImageNet-64) would better demonstrate the advantage of the proposed SSM formulation in handling long sequences.\n\n3. **Scalability demonstration is limited**\n\nThe real advantage of the SSM formulation and the O(N log N) complexity becomes evident only at large sequence lengths. The current experiments do not fully demonstrate this scaling. Although the choice likely reflects computational constraints, even a modest extension would convincingly support the scalability claim."}, "questions": {"value": "1. Does the PDE-SSM forward solver require higher numerical precision for stability, especially when back-propagating through the Fourier-domain exponential? Have the authors tested mixed-precision or bfloat16 training?\n\n2. Since gradients are propagated through both the Fourier transform and the PDE kernel parameters, how stable is the training in practice? Were any spectral clipping or regularization techniques (e.g., bounding eigenvalues of $K$) necessary?\n\n3. Could the PDE-SSM formulation be extended to 3D or spatiotemporal data (e.g., videos or 3D representations)? The learnable convection and diffusion terms seem naturally applicable to these modalities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RmsYqQ1lkB", "forum": "YOZBEyTgLe", "replyto": "YOZBEyTgLe", "signatures": ["ICLR.cc/2026/Conference/Submission10708/Reviewer_zmNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10708/Reviewer_zmNN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859614775, "cdate": 1761859614775, "tmdate": 1762921943368, "mdate": 1762921943368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PDE-SSM, a architectural block that generalizes 1D state space models (SSMs) to multi-dimensional spatial domains through learnable PDEs. The key insight is that replacing the linear ODE of traditional SSMs with a diffusion–convection–reaction PDE enables a physically grounded mechanism for spatial information flow. The authors solve this PDE in the Fourier domain, achieving near-linear complexity O(NlogN), reducing the quadratic cost of self-attention while retaining global receptive fields."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using PDE-based state-space operators for spatial feature mixing is novel and elegant.\n\n2. The paper provides a clear computational complexity analysis, showing how the Fourier-domain solver achieves O(NlogN) scalability.\n\n3. The writing is clear and technically mature.\n\n4. The paper contributes a generalizable new building block for spatial deep learning—potentially applicable beyond diffusion transformers (e.g., segmentation, SR, video models)."}, "weaknesses": {"value": "1. While results are solid, most experiments are low- to mid-resolution (≤ 256×256). Testing PDE-SSM-DiT on high-resolution datasets (e.g., ImageNet256, LAION subsets) would better validate scalability and efficiency claims in realistic generative settings.\n\n2. The paper mainly evaluates image generation. Given the generality of PDE-SSM, additional tasks (e.g., classification, segmentation, or video generation) could demonstrate broader utility.\n\n3. The ablation of individual PDE terms (diffusion, convection, reaction) is missing.\n\n4. There is limited discussion of numerical stability or conditioning when training PDE parameters, especially at large τ values or with anisotropic K."}, "questions": {"value": "Could PDE-SSM generalize to 3D or spatiotemporal data (e.g., video)? If so, how would computational complexity and kernel representation scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0iuAUS46xl", "forum": "YOZBEyTgLe", "replyto": "YOZBEyTgLe", "signatures": ["ICLR.cc/2026/Conference/Submission10708/Reviewer_1LzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10708/Reviewer_1LzM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977726096, "cdate": 1761977726096, "tmdate": 1762921942917, "mdate": 1762921942917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}