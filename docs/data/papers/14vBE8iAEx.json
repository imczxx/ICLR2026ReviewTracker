{"id": "14vBE8iAEx", "number": 20565, "cdate": 1758307535543, "mdate": 1759896970897, "content": {"title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment", "abstract": "Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases—such as length bias, sycophancy, conceptual bias, and discrimination—that hinder the model’s ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causality to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.", "tldr": "", "keywords": ["LLM", "RLHF", "Reward Modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc2a79300aa6ee1697400a4a26697185fbaa35a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses a fundamental problem in Reinforcement Learning from Human Feedback (RLHF) — reward hacking due to spurious correlations in reward modeling. Traditional RLHF reward models often conflate superficial correlations (e.g., response length, agreement, demographic cues) with genuine human preferences, leading to biases such as length bias, sycophancy, concept bias, and discrimination bias (see Section 1). To overcome this, the authors propose a Causal Reward Model (CRM) that incorporates counterfactual invariance to ensure that reward predictions are independent of irrelevant variables (Section 3.2–4.1). The method employs Maximum Mean Discrepancy (MMD) regularization to enforce statistical independence between latent representations and spurious factors (Eq. (1), Section 4.1). Experiments across four settings — sycophantic, length, concept, and discrimination bias — demonstrate that CRM significantly mitigates spurious correlations and improves fairness and robustness without sacrificing model utility (Sections 5.1–5.4, Tables 1–3, Fig. 3). The approach integrates seamlessly with existing RLHF pipelines, offering a practical improvement to current alignment workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper introduces a novel causal regularization framework for reward modeling within RLHF, which is an underexplored direction. While prior works address specific biases (e.g., length bias via PoE or WARM), CRM provides a unified causal formulation that generalizes across multiple types of spurious correlations (Section 2.2). The concept of counterfactual invariance adapted from causal inference to LLM alignment is new.\n\nQuality: The theoretical foundation is clearly established with causal diagrams (Figure 1, Section 3.2) and well-defined objectives (Eq. (1)–(2), Section 4.1). The experimental design is comprehensive and evaluates diverse bias types. Results (e.g., Table 2 and 3) consistently demonstrate the superiority of both conditional and unconditional CRMs over baseline reward models. The empirical validation across multiple bias domains substantiates the method’s robustness and generalizability.\n\nClarity: The paper is well-structured and written clearly. Figures and tables (e.g., Fig. 2 and 3) effectively visualize comparative performance trends.\n\nSignificance:\nReward hacking remains a major bottleneck for safe deployment of RLHF-trained LLMs. Hence, this paper’s contribution is relevant."}, "weaknesses": {"value": "Hyperparameter Sensitivity: The choice of MMD coefficients (λ) significantly affects results (Fig. 3), but the paper does not deeply explore tuning stability or sensitivity. This could impact deployment robustness in practice."}, "questions": {"value": "Causal Assumptions: How sensitive is CRM to incorrect causal graph specifications?\n\nScalability: How does the computational cost of MMD regularization scale with large datasets and high-dimensional embeddings?\n\nHyperparameter Selection: How was λ chosen in practice? Did you observe stable optima across tasks or require dataset-specific tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xlr3SMTJjI", "forum": "14vBE8iAEx", "replyto": "14vBE8iAEx", "signatures": ["ICLR.cc/2026/Conference/Submission20565/Reviewer_Dqv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20565/Reviewer_Dqv3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738149157, "cdate": 1761738149157, "tmdate": 1762933978715, "mdate": 1762933978715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called CRM to address spurious correlations and reward hacking in RLHF. The central idea is to remove the confounding effect of response length so that the reward model more accurately reflects the true quality of the response. The authors leverage MMD to design a regularizer for reward model training. Experiments are conducted to evaluate the performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a new perspective by leveraging the method in counterfactual invariance. The proposed CRM serves as a regularizer that can be integrated into standard reward model frameworks."}, "weaknesses": {"value": "- I am concerned about the novelty of this paper, as it largely builds on the technique proposed in [1], which already demonstrates that MMD can serve as a causal regularizer. In comparison, the present work appears to mainly combine and reapply techniques from that paper.\n\n- The paper is not well written, causing unnecessary confusion in its methodology.\n\n- Line 212: “the causal graph reveals that $T^{L, \\perp}$ is independent of $Z$,” but Figure 1 shows that $Z$ directly influences $T^{L, \\perp}$.\n\n- I believe Equation (1) contains the key insight of the paper, but its description is delayed, and the preceding discussion seems disconnected from it. \n\n- Is $T^{L, \\perp}$ well-defined?\n\n- The description of MMD could be moved to the Preliminaries section to make the methodological contribution clearer. \n\n- Why is it necessary to bin $Z$? Why not directly measure the dependence between $f(T)$ and $T$ using the Hilbert–Schmidt Independence Criterion (HSIC)? These questions further aggravate my concern about the novelty of the paper, as it closely follows the approach in [1].\n\n- The definition of $p'_m$ is also unclear.\n\n- I am skeptical about the effectiveness of the proposed formulation. From a statistical perspective, MMD only measures dependence; it does not imply causality. Intuitively, applying MMD regularization in (1) cannot ensure the causality claimed in this paper.\n\n- How is $Z$ chosen in practice? What if $Z$ includes a factor that does reflect the true reward of the response? Naively applying Equation (1) in such cases could hinder effective reward learning.\n\n- The experimental comparison is limited. The authors should consider including more recent methods, for example:\n  - https://arxiv.org/abs/2403.19159\n  - https://arxiv.org/abs/2502.00814\n\n[1] https://arxiv.org/abs/2106.00545"}, "questions": {"value": "See Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qa34RdDv2W", "forum": "14vBE8iAEx", "replyto": "14vBE8iAEx", "signatures": ["ICLR.cc/2026/Conference/Submission20565/Reviewer_WY6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20565/Reviewer_WY6t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759124875, "cdate": 1761759124875, "tmdate": 1762933978336, "mdate": 1762933978336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a new approach called ‘Causal Reward Model’ to mitigate the problem of spurious correlation and reward hacking that is common in RLHF. Traditional methods use spurious shortcuts to deal with hacking coming from factors like long length. The paper, instead, treats the factors like length and tone as spurious variables $Z$, and learn a reward predictor whose internal representation is independent of $Z$. A maximum mean discrepancy based on the reward predictor is then added to the loss function as the regularisation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-The paper is based on causal language and counterfactual invariance, which is novel compared to previous methods using supurious shortcuts. The CRM is then added as a normal regulariser which can be embedded into common model frameworks easily.\n- Empirical results are also good."}, "weaknesses": {"value": "- The method can only debiases the explicit spurious factors. If an unknown latent shortcut hack the reward, it won’t fix it. \n- The benchmark is PPO, but what about other methods that address the shortcuts, the author should compare with these methods as well."}, "questions": {"value": "- In practice, how to decide the spurious variables Z? \n- Does CRM stay robust after the fine-tuning, which may cause the policy’s distribution shift against the preference dataset used to train CRM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "twzttMAVpo", "forum": "14vBE8iAEx", "replyto": "14vBE8iAEx", "signatures": ["ICLR.cc/2026/Conference/Submission20565/Reviewer_w8yn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20565/Reviewer_w8yn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871904303, "cdate": 1761871904303, "tmdate": 1762933977785, "mdate": 1762933977785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a causality inspired regularizer in Reward Modelling."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The idea seems promising and novel. The idea is well motivated, and the solution is presented with clarity."}, "weaknesses": {"value": "The experimental section seems to have missed out on providing some details. \n\n1) For any one of the dataset mention how you are creating the bins.\n2) How are you calculating p_m and p_m^'. For what values of Z is this being computed?\n3) What are the values that Z can take in any one of the datasets?\n4) Describe the conditional CRM in more detail."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TRdO6Q0Nkw", "forum": "14vBE8iAEx", "replyto": "14vBE8iAEx", "signatures": ["ICLR.cc/2026/Conference/Submission20565/Reviewer_QC9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20565/Reviewer_QC9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901448548, "cdate": 1761901448548, "tmdate": 1762933977346, "mdate": 1762933977346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}