{"id": "Ryc7jKP6H9", "number": 16776, "cdate": 1758268567057, "mdate": 1759897220021, "content": {"title": "Co-occurring Associated REtained concepts in Diffusion Unlearning", "abstract": "Unlearning has emerged as a key technique to mitigate harmful content generation in diffusion models. However, existing methods often remove not only the target concept, but also benign co-occurring concepts. Unlearning nudity can unintentionally suppress the concept of person, preventing a model from generating images with person. We define these undesirably suppressed co-occurring concepts that must be preserved $\\textbf{CARE}$ ($\\textbf{C}$o-occurring $\\textbf{A}$ssociated $\\textbf{RE}$tained concepts). Then, we introduce the  $\\textbf{CARE score}$, a general metric that directly quantifies their preservation across unlearning tasks. With this foundation, we propose $\\textbf{ReCARE}$ ($\\textbf{R}$obust $\\textbf{e}$rasure for $\\textbf{CARE}$), a framework that explicitly safeguards CARE while erasing only the target concept. ReCARE automatically constructs the CARE-set, a curated vocabulary of benign co-occurring tokens extracted from target images, and leverages this vocabulary during training for stable unlearning. Extensive experiments across various target concepts ($\\textit{Nudity}$, $\\textit{Van Gogh}$ style, and $\\textit{Tench}$ object) demonstrate that ReCARE achieves overall state-of-the-art performance in balancing robust concept erasure, overall utility, and CARE preservation.", "tldr": "We introduce CARE (Co-occurring Associated Retained concepts) and propose ReCARE, a framework that preserves CARE during diffusion model unlearning, achieving robust erasure without sacrificing benign co-occurring concepts.", "keywords": ["unlearning", "diffusion", "concept erasure", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ff76a54b606bef4524e65bb84edf4bc91fba2dd.pdf", "supplementary_material": "/attachment/9529812018e649329e551f684f926622dbafe24c.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the overlooked problem of erasure of benign concepts in unlearning for text-to-image diffusion models. The authors define a CARE score metric to measure this problem and presents a method to create a CARE set to unlearn concepts while retaining the model's ability to generate benign related concepts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Problem formulation of CARE is practical and highly relevant to the unlearning community. While there have been numerous advances in unlearning methods, quantifying how well benign concepts are retained is crucial.\n\n- CARE score being highly correlated to human ground truths is good evidence that the metric is osund.\n\n- Overall pipeline of filtering for CARE-set and unlearning losses make sense and overall experimental results show good performance over baselines.\n\n- Paper is generally well-written and clear."}, "weaknesses": {"value": "- CARE score tests for one benign concept among 80 unrelated concepts. Most concepts will have multiple benign concepts. Would be ideal to expand to several benign concepts.\n\n- Overall I find the process of creating the CARE-set fairly complicated, for e.g., the clustering steps may be prone to overfitting to things like concepts/prompts and other hyperparameters. Beyond what was presented in appendix F on k-means clusters, have the authors investigated robustness to other aspects and/or are the authors confident the CARE-set construction is robust across parameters?\n\n- Related to above, the training process also seems involved requiring textual inversion for each concept before training. Have the authors quantified the efficiency of their method taking all steps into consideration?\n\n- Insufficient details on how RATIO, the primary evaluation metric, is computed. How is the area normalized?"}, "questions": {"value": "- Missing reference [1] which unlearns concepts by substituting with benign alternatives that are manually chosen.\n\n[1] Heng, Alvin, and Harold Soh. \"Selective amnesia: A continual learning approach to forgetting in deep generative models.\" Advances in Neural Information Processing Systems 36 (2023): 17170-17194."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vx2NhMSEL7", "forum": "Ryc7jKP6H9", "replyto": "Ryc7jKP6H9", "signatures": ["ICLR.cc/2026/Conference/Submission16776/Reviewer_GzEp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16776/Reviewer_GzEp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477674048, "cdate": 1761477674048, "tmdate": 1762926820125, "mdate": 1762926820125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that, existing machine unlearning (MU) methods also remove the benign co-occurring concepts when removing the target concept. Hence, the paper first introduces the CARE (Co-occurring Associated REtained concepts) score to measure the retention of CARE concepts, and then proposes a method ReCARE (Robust erasure for CAFE) to preserve the model's ability to generate CARE concepts. \n\nSpecifically, the CARE score is computed based on CLIP similarity between the generated images conditioned on the prompts containing the chosen CARE concept and the corresponding tokens. Then, the proposed method ReCARE constructs the CARE set based on CLIP similarity and further leverages refinement to filter out unsuitable concepts, and conducts unlearning with retain loss and erase loss using the constructed CARE set.\n\nThe proposed CARE score is validated by comparing it with the human-annotated ground truth. Experiments on nudity removal, style unlearning, and object unlearning demonstrate the effectiveness of the proposed method in terms of the robustness and the preservation of benign co-occurring concepts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, and the motivation is clear. Existing unlearning methods often fail to generate benign co-occurring concepts, and few metrics consider such measurements. This paper aims for this aspect and seems interesting.\n- The proposed CARE score and unlearning method solve the co-occurring concepts missing issue. The method is simple yet effective."}, "weaknesses": {"value": "- The construction of the CARE set involves CLIP similarity computations, t-SNE projection, and k-means clustering, which might incur expensive computational costs for high-resolution scenarios.\n- The CARE score and CARE set rely on the CLIP similarity score, while CLIP itself is sensitive to the templates used, which might cause errors when constructing the CARE set."}, "questions": {"value": "- What is the overhead of the proposed method?\n- For the CARE score, it considers the Top-1 match for a single chosen concept. Could this be extended to multi-concept images at once?\n- Does the template used for CLIP similarity also impact the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oQ0Hebi6tJ", "forum": "Ryc7jKP6H9", "replyto": "Ryc7jKP6H9", "signatures": ["ICLR.cc/2026/Conference/Submission16776/Reviewer_AnaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16776/Reviewer_AnaB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732813266, "cdate": 1761732813266, "tmdate": 1762926819372, "mdate": 1762926819372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work creates a set of retaining concepts (CARE) that co-occur with the target concepts but are benign. Using this set of concepts, the authors develop a new unlearning loss that consists of a retain loss and an erase loss. The authors compare their method with standard baselines for diffusion unlearning and demonstrate state-of-the-art performance in balancing concept erasure, utility, and preservation of related concepts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written, and the construction of CARE is well-motivated and explained. The authors performed extensive experiments to compare their method with various baselines."}, "weaknesses": {"value": "(Not a weakness) I am not very familiar with diffusion unlearning and hence am not able to fairly evaluate the quality of this work. I would recommend that the AC seek opinions from other reviewers."}, "questions": {"value": "One question I have is how well does ReCARE preserve the related (but unharmful) concepts that do not appear in the vocabulary $\\mathcal{V}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "AHzFaYA5ns", "forum": "Ryc7jKP6H9", "replyto": "Ryc7jKP6H9", "signatures": ["ICLR.cc/2026/Conference/Submission16776/Reviewer_eSY1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16776/Reviewer_eSY1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926711861, "cdate": 1761926711861, "tmdate": 1762926818839, "mdate": 1762926818839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReCARE, a framework to improve concept unlearning in diffusion models—specifically addressing the issue that removing harmful concepts (e.g., “nudity”) often unintentionally erases benign co-occurring concepts (e.g., “person”). Authors propose CARE Score, a metric to quantify how well benign co-occurring concepts are preserved after unlearning.\nExperiments on three unlearning tasks (nudity, Van Gogh style, and tench object) show ReCARE outperforms prior methods in robustness (low ASR), utility (FID/CLIP), and CARE preservation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work identifies an overlooked issue—collateral forgetting of benign co-occurring concepts—and formalizes it as CARE.\n- The proposed CARE score offers a measurable dimension beyond robustness and utility."}, "weaknesses": {"value": "- Both CARE-set extraction and CARE score rely heavily on CLIP similarity, which may inherit CLIP’s biases and limit generalization to non-CLIP diffusion models.\n- Only three concept domains (nudity, style, object) are tested; it remains unclear how well ReCARE scales to broader or abstract targets (e.g., emotion, violence, or political bias)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LjNdgM6vZg", "forum": "Ryc7jKP6H9", "replyto": "Ryc7jKP6H9", "signatures": ["ICLR.cc/2026/Conference/Submission16776/Reviewer_ut3g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16776/Reviewer_ut3g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225640201, "cdate": 1762225640201, "tmdate": 1762926818411, "mdate": 1762926818411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key problem in diffusion model unlearning: the unintended suppression of benign co-occurring concepts. These are defined as CARE (Co-occurring Associated REtained concepts). The authors introduce the CARE score to quantify preservation and propose ReCARE, a method that constructs a CARE-set vocabulary to safeguard these concepts during training. Experiments on targets such as nudity and Van Gogh style show that ReCARE achieves a good balance across erasure robustness, overall utility, and CARE retention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1:** This paper identifies and formally defines a previously overlooked issue in diffusion model unlearning: the unintended suppression of benign, co-occurring concepts (CARE). This concept can help develop nuanced, practical unlearning methods that preserve the overall model's utility.\n\n**S2:** This paper provides a framework for the proposed problem. The introduction of the CARE score provides a quantitative metric for objectively measuring concept preservation. The proposed ReCARE method is a practical solution that constructs a \"CARE-set\" to protect associated concepts during unlearning.\n\n**S3:** This paper conducts a thorough experimental evaluation by testing ReCARE on various targets and comparing it with state-of-the-art baselines, demonstrating the method's effectiveness and generalizability."}, "weaknesses": {"value": "**W1:** The paper does not consider more complex cases in which the target concept overlaps with the retained concept and there are multiple concepts to erase. Adding discussions on these cases would better demonstrate the generalizability and robustness of the proposed method.\n\n**W2:** This paper lacks a comprehensive analysis of efficiency and scalability issues, e.g., the cost of CARE-set construction and the resource consumption of the unlearning process.\n\n**W3:** The automated construction of the CARE-set can introduce biases or errors inherent in models. The paper lacks a thorough discussion of the risk and evaluates the quality of the CARE-set.\n\n**W4:** Although the experiments include a comprehensive set of baselines, the comparison with existing concept erasure approaches that are not based on unlearning (e.g., those based on filtering, post-processing, etc.) is missing."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "luNUqgKAxN", "forum": "Ryc7jKP6H9", "replyto": "Ryc7jKP6H9", "signatures": ["ICLR.cc/2026/Conference/Submission16776/Reviewer_VJSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16776/Reviewer_VJSB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762573119584, "cdate": 1762573119584, "tmdate": 1762926818014, "mdate": 1762926818014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}