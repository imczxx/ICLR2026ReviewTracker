{"id": "T5LoOUuZDc", "number": 19268, "cdate": 1758294915216, "mdate": 1763655294584, "content": {"title": "Sharper Characterization of the Global Maximizers in Bilinear Programming with Applications to Asynchronous Gradient Descent", "abstract": "We study the bilinear program that arises when tuning the stepsizes in asynchronous gradient descent (AGD). Notably, we prove a necessity theorem: every global maximizer lies at an extreme point of the feasible region, strengthening the classical sufficiency guarantee for linear objectives on compact sets. Exploiting this structure, we recast the continuous problem as a discrete search over the vertices of the hyper‑cube and design a solver that performs a biased random walk among them. Over all the tested benchmarks, including the Cyclic Staircase benchmark, our solver reaches global optimality up to $1000\\times$ faster than Gurobi 11 while using orders of magnitude fewer evaluations.\n\nThis structural result allows us to prove near-optimal stepsize scheme for the recently proposed Ringmaster AGD algorithm and a provable factor-$2$ approximation on the error to find an $\\varepsilon$-stationary point. Together, our results provide both a sharper theoretical characterization and a practical solver for nonconvex bilinear programs emerging in distributed learning.", "tldr": "", "keywords": ["Bilinear Programming", "Optimization", "Gradient Descent", "Randomized Heuristics"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2268aea1eda20045db70b0e67607f832a045e92f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies a stepsize optimization problem for Asynchronous GD (AGD). The stepsize optimization problem can be cast as an optimization problem with a linear objective and bilinear constraints. The authors show that the class of optimization problems achieves global maximization necessarily at extreme points. Finally, they show a search heuristic for the problem class and compare the performance with Gurobi solvers on several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The presentation of the results is clean and easy to follow. The theory of the bilinear program problem begins with a few basic properties (unique solution of linear-quadratic system and regularity of solution). The theory development culminates at Theorem 4.6, where the authors show that the global maximizers are mapped to the vertices of the unit hypercube. \n\nIn the analysis of Asynchronous GD, one way to obtain an upper bound is to make sure a term of $R(K) < 0$, which gives an upper bound of $\\frac {2 \\Delta} {\\Gamma_k}$. This can be formulated to the stepsize optimization problem of interest in this work. \n\nThen, the authors tested the search heuristic on a few benchmarks. From the experiments reported in the paper, it outperforms Gurobi solver significantly."}, "weaknesses": {"value": "The main theoretical result in this paper is a characterization of the global maximizer of the problem. They show that the global optimizer corresponds to (after the map $\\psi$) vertices of the unit hypercube. The derivation of this extreme point characterization seems to be quite standard and does not involve main technical novelty. \n\nThe connection to the asynchronous GD seems a bit weak, since this is only one way to get an upper bound of AGD, and the construction is also quite brittle. For instance, it does not give the upper bound for asynchronous SGD, where the objective function is not linear anymore."}, "questions": {"value": "Why didn’t the authors include any experiments on the AGD stepsize schedules? As this is one of the main motivations of this study (at least from the presentation of this work it seems so), it would be great to know whether the stepsize returned by MMAHH is indeed good in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uhTUpi5AcB", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_sW6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_sW6C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760713640237, "cdate": 1760713640237, "tmdate": 1762931231973, "mdate": 1762931231973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically characterizes asynchronous GD (AGD) with delays and derives near‑optimal stepsize policies. It models asynchrony via a delay matrix + constraint program, introduces effective delay after threshold‑based discarding of stale gradients, and proves that Ringmaster‑AGD  achieves a 2‑approximation to the optimal cumulative stepsize. Convergence is established under nonconvex smoothness. Experiments implement a discrete/mixed‑integer solver (MMAHH) to approximately solve the planning problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clean problem modeling. The interaction among delay, stepsize, and discard rules is precisely abstracted; definitions/feasible regions are reusable.\n2. Novel theoretical guarantee. A constant‑factor (2×) approximation for a fixed‑threshold discard policy is practically useful.\n3. From theory to implementation. Casting the stepsize search as a discrete/MINLP‑like problem and providing a heuristic solver bridges theory and practice."}, "weaknesses": {"value": "1. Strong assumptions, missing stochasticity. Core analysis assumes deterministic full gradients; most real training uses mini‑batch stochastic gradients. Please extend analysis or at least provide systematic experiments under stochastic noise.\n2. Scalability & deployability. The effective‑delay program trends toward mixed‑integer nonlinear; while MMAHH is proposed, its scaling and runtime vs. optimal solvers (or simpler heuristics) are not systematically evaluated.\n3. System‑level evidence. Experiments emphasize solver quality but lack end‑to‑end async training on real multi‑GPU/multi‑node systems with realistic delay traces. Compare against fixed stepsizes, Polyak stepsizes, and different thresholds.\n4. Constant dependence & hyperparameters. Sensitivities to $L$, threshold $R$, and feasible‑set parameters are under‑explored; mis‑specification robustness is unclear."}, "questions": {"value": "1. With mini‑batch noise, can Theorem 5.4–style results or the 2‑approximation be retained (perhaps with variance terms)?\n2. How is $\\delta_k^e$ estimated online, and how do $R$ and $L$ mis‑specification affect near‑optimality?\n3. What are the time/quality trade‑offs of MMAHH across numbers of workers $n$ and horizon $K$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qi2n2Hsg5M", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_5vSE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_5vSE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363317579, "cdate": 1761363317579, "tmdate": 1762931231448, "mdate": 1762931231448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically characterizes asynchronous GD (AGD) with delays and derives near‑optimal stepsize policies. It models asynchrony via a delay matrix + constraint program, introduces effective delay after threshold‑based discarding of stale gradients, and proves that Ringmaster‑AGD  achieves a 2‑approximation to the optimal cumulative stepsize. Convergence is established under nonconvex smoothness. Experiments implement a discrete/mixed‑integer solver (MMAHH) to approximately solve the planning problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clean problem modeling. The interaction among delay, stepsize, and discard rules is precisely abstracted; definitions/feasible regions are reusable.\n2. Novel theoretical guarantee. A constant‑factor (2×) approximation for a fixed‑threshold discard policy is practically useful.\n3. From theory to implementation. Casting the stepsize search as a discrete/MINLP‑like problem and providing a heuristic solver bridges theory and practice."}, "weaknesses": {"value": "1. Strong assumptions, missing stochasticity. Core analysis assumes deterministic full gradients; most real training uses mini‑batch stochastic gradients. Please extend analysis or at least provide systematic experiments under stochastic noise.\n2. Scalability & deployability. The effective‑delay program trends toward mixed‑integer nonlinear; while MMAHH is proposed, its scaling and runtime vs. optimal solvers (or simpler heuristics) are not systematically evaluated.\n3. System‑level evidence. Experiments emphasize solver quality but lack end‑to‑end async training on real multi‑GPU/multi‑node systems with realistic delay traces. Compare against fixed stepsizes, Polyak stepsizes, and different thresholds.\n4. Constant dependence & hyperparameters. Sensitivities to $L$, threshold $R$, and feasible‑set parameters are under‑explored; mis‑specification robustness is unclear."}, "questions": {"value": "1. With mini‑batch noise, can Theorem 5.4–style results or the 2‑approximation be retained (perhaps with variance terms)?\n2. How is $\\delta_k^e$ estimated online, and how do $R$ and $L$ mis‑specification affect near‑optimality?\n3. What are the time/quality trade‑offs of MMAHH across numbers of workers $n$ and horizon $K$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qi2n2Hsg5M", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_5vSE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_5vSE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363317579, "cdate": 1761363317579, "tmdate": 1763717456358, "mdate": 1763717456358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThis paper investigates a special type of bilinear programming (BLP) problem and characterizes the properties of its optimal solutions. Based on the properties of the optimal set, the authors design tailored heuristics to accelerate the solution of the bilinear program. The technique is then applied to solving bilinear programs arising from the stepsize design in asynchronous gradient descent."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**Strength**\nThe paper is generally easy to follow."}, "weaknesses": {"value": "**Weaknesses**\n\nDespite the claimed contributions of the paper, I find this paper poorly written and have several concerns:\n\n1. Lack of motivation and unclear focus of the paper\n\n   The paper devotes considerable effort to the context of BLP and presents numerous auxiliary concepts and results in the appendix. However, after establishing the results, the only application is deriving the stepsize in asynchronous gradient descent (AGD). This is clearly overdone and makes the paper's focus unclear. What's worse, even the application to AGD looks suspicious. In particular, in the contribution section, the paper claims \"selecting improved stepsizes for AGD can be cast as a BLP\". However, this design relies on a known delay matrix, and in a real AGD algorithm implementation, I don't think it makes sense to assume knowledge of this matrix. Hence, the application to AGD seems restricted to post-hoc analysis. \n\n2. Importance of the theoretical result\n\n   I'm also concerned about the importance of the claimed theoretical result. In a word, the paper shows that every optimal solution of the considered type of BLP is extremal. This result seems strong at a first glance; however, given that **Theorem 3.2** always guarantees the existence of an extremal optimal solution, it already suffices to focus on the extremal solutions since we typically want *one* optimal solution. Therefore, I don't think the result of the paper is that important.\n\nIn addition, I find a number of notation inconsistencies and typos throughout the paper. Overall, I find the paper lacks motivation. The importance of the theoretical result is unclear, and its application does not look interesting. I don't think the paper meets the publication standard of ICLR."}, "questions": {"value": "**Questions**\n\n1. Could you justify the application of BLP in AGD's analysis beyond doing post-hoc analysis?\n2. Why would the fact that \"all the optimal solutions are extremal\" be more helpful compared to \"there exists one extremal optimal solution\" if you only need one optimal solution?\n3. **Theorem 5.4** analyzes AGD. According to the algorithm description, I did not see a source of randomness. Where is the expectation from equation (12) from?\n\n**Minor issues**\n\n1. Line 117, 159, 192\n\n   Both the boldface letter like $\\mathbf{a}$ and capitalized letter $\\Lambda$ are used to denote vectors.\n\n2. Line 122, 140\n\n   The index  $i$ in $M_{i, j}$ is unused.\n\n3. Line 143\n\n   \"...being induced by...\" this sentence is not clear.\n\n4. Line 311\n\n   $f$ is the deterministic objective and does not have sample as its argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VfH88Px5Uq", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_ZNk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_ZNk7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980664642, "cdate": 1761980664642, "tmdate": 1762931231056, "mdate": 1762931231056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a bilinear programming (linear objective defined over a compact hypercube; bilinear constraints) that is closely related to the step-size tuning for asynchronous GD. The paper introduces a special instance of a bilinear program (Eq. 1): to find a step size sequence $(\\gamma_k)_k$ that minimizes a theoretical convergence rate of asynchronous GD under usual assumptions like Lipschitz smoothness. For this particular problem, it is shown that every global maximizer is an extreme point (e.g., a corner or vertex) of its feasibility set. Based on this finding, it has been demonstrated that an evolutionary, randomized heuristic called Markov move-acceptance hyper-heuristic (MMAHH) can be used to find a maximizer. The tested MMAHH-based solver runs much faster than a production-level Gurobi 11 solver on a tested benchmark (e.g., Cyclic Staircase, Stochastic Repetition, Random Sequences)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and self-contained. It provides almost every background knowledge to understand the whole paper in its appendices, which is very satisfying.\n- The bilinear programming studied in the paper has a strong motivation in the hyperparameter tuning problem for an optimization algorithm with multiple workers (AGD).\n- I think most of the proofs are correct. I have read most of the proofs in Appendices C, E, F, and G, but I couldn’t find significant errors (except for some typos or redundant sentences).\n- The main contributions are sound, novel, and interesting:\n    - The paper provides a sharper characterization of the optima of a subclass of bilinear programs. Given the renowned sufficiency, which describes that a linear functional attains its maximum over a nonempty compact set K at an extreme point of K (which can be easily proved using, e.g., Theorem 3.2.2 of Barvinok (2022)), this work offers a necessity under the particular bilinear programs of interest: every maximizer is an extreme point for these problems. This means that we (will) never ignore any of (possibly non-extremal) maximizers by searching for maximizers over a finite number of extreme points.\n    - The implication in convergence rate analysis of (non-stochastic full-batch) AGD is interesting and significant. Using the paper’s main results, the paper shows that the step size choice of a recently proposed Ringmaster A(S)GD algorithm is very close (up to a constant factor 2) to the optimal step size choice for AGD in terms of minimizing a convergence upper bound (under nonconvex L-smoothness assumption).\n    - Lastly, the paper offers a simple and efficient heuristic (an MMAHH-based solver) to solve the step size tuning problem on a discrete domain.\n\n---\n\nA. Barvinok, *A course in convexity*, 2nd edition, Grad. Stud. Math. 54, American Mathematical Society, New York, 2002."}, "weaknesses": {"value": "1. In my view, the very first paragraph of the introduction is redundant. It is dedicated to summarizing the well-known history of modern developments in AI/ML/DL. Being almost irrelevant to the paper’s topic, it does not help readers get motivated with the problem setting. The authors had better open the first section with some background on (and importance of) asynchronous GD and/or difficulties in choosing an appropriate sequence of step sizes.\n    - Provided that some additional spaces after omitting/shrinking the historical paragraph, I would suggest to add some more intuitions on why the particular bilinear programming satisfies the proven necessity condition, e.g., describing some geometric properties (proved in Appendix D.2 and D.3, but in plain words!) of the feasibility set that enforces the bilinear program to have no non-extremal maximizers.\n2. It is worth mentioning that Theorem 3.2 (sufficiency) is quite a well-known result since it is easy to prove it using the compactness of the feasibility set and a theorem in a math textbook, for example, Barvinok (2022, Theorem 3.2.2) (mentioned in ‘Strengths’).\n3. A single sentence in Lines 326-330 is too long; it needs rewriting.\n4. Overall, there is room for improvement in the organization of the proofs in the appendices, in particular, E, F, and G. For instance, there are proofs of the main/important theorems (e.g., Theorems 4.3, 4.4, 4.6, …) in the section with a name “Omitted Proofs”! Also, although Theorems 4.3 and 4.4 are relevant to each other (and in fact the proof of Theorem 4.4 seems to depend on Theorem 4.3), their proofs are far apart (one is in Appendix E.3 and another is in Appendix F.1).\n5. Notation and font styles for scalars, vectors, and matrices are very disorganized throughout the paper; please use a consistent set of symbols. Have a look at the usual math notation in machine learning literature: https://github.com/goodfeli/dlbook_notation/blob/master/math_commands.tex\n6. Lemma E.3: I believe this lemma can be proved more cleanly by using zero derivative at the minimum/maximum.\n    - (Same proof until Equation (76).) Let $h_i(x) = x_i (1 + (Mx)_i)$. Note that $h_i\\in C^\\infty$ because it’s a polynomial of $x$. Define $\\tilde{h}_i : (-\\epsilon_0, \\epsilon_0) \\to \\mathbb{R}$ as $\\tilde{h}_i(t) = h_i (p + td)$, which is also $C^\\infty$. Since $p+td \\in \\mathscr{F}$, the range of $\\tilde{h}_i$ is a subset of $[0, 1]$, i.e., $\\tilde{h}_i (t) \\in [0,1]$ for all $t\\in (-\\epsilon_0, \\epsilon_0)$. Observe that $\\tilde{h}_i$ has either a minimum or a maximum at $t=0$: $\\tilde{h}_i (0) = 0$ if $i \\in Z$ and $\\tilde h_i (0) = 1$ if $i\\in S$. Thus, $\\tilde{h}^\\prime _i (0) = \\nabla \\tilde h_i (p) \\cdot d = 0$. It means that the jacobian of $h(x) = (h_1(d), \\cdots, h_d(x)) = x+ x \\odot (Mx)$ satisfies $\\nabla h (p) \\cdot d = 0$. Since $\\nabla h(p)$ is a $P$-matrix and hence invertible, we have $d=0$. (Proceed with the remaining proof to yield a contradiction.)\n7. Here is a list of typos or misleading reasons that I find crucial. (Some of them might be wrong.)\n    - Equations (1) and (3): the subscript $i$ —> $k$\n    - Line 173 “where $\\mathbf{a} \\in \\mathbb{R}^d \\setminus \\\\{0\\\\}$ …”: $\\mathbf{a}$ —> $\\mathbf{c}$\n    - Equation (13): factor 2 is missing in the middle term of LHS (accordingly, I guess the definition of $M_{ij}$ in Line 383 must be $2\\delta^j \\mathbb{I} \\\\{j\\in M_i\\\\}$)\n    - Line 384: “left-hand side” —> “right-hand side”\n    - Definition D.8 versus Lemma D.11: Is  $\\deg (I)$ a set of indices? Or its size?\n    - Below Equation (48): The inequality (a) is only due to the non-negativity of $t$ and $1-t$. The inequalities $f(p)\\ge f(x)$ and $f(p) \\ge f(y)$ imply the inequality (b), not (a).\n    - Equation (67): wrong beginning index of the sum in the middle ($k+1$ —> $1$) unless $M$ is strictly upper triangular\n    - Lemma E.3: A typo in line 2665: “for every $i\\in [d]$” —> “for every $i\\in Z$”.\n    - Equations (93) and (94): $\\pm tv$ terms are missing in $\\phi_k^\\square(\\Lambda)$.\n    - Line 7 of Algorithm 3: “Run Procedure 1 in all workers” —> “Run Procedure 2 in all workers”\n    - Equation (196): In the last term, the sum in the parentheses must be $\\sum_{j\\in M_k} \\gamma_j$"}, "questions": {"value": "- As far as I understood, in Section 6, the MMAHH-based heuristic is compared with Gurobi 11 software for solving discrete optimization benchmarks (over $\\\\{0,1\\\\}^d$). Have you numerically compared these two solvers for solving the main bilinear programming $(\\mathscr{P}_d)$ or a mixed-integer variant $(\\mathscr{P}^{\\rm mi}_d)$ as below?:\n    - MMAHH solver: Solve it as a discrete optimization over the vertices of a hypercube (as already done in the paper).\n    - Gurobi 11: Solve it directly, considering the compact feasibility set (which has infinitely many elements) as it is, (possibly) allowing practical implementations like the Big-M Method (mentioned in Appendix G.10).\n- How should we compute $\\Psi(\\cdot)$ efficiently and practically if it is an implicit function? Or, equivalently, is the set of extreme points of a feasibility set guaranteed to be easy to obtain? If so, please explain a general way to do so (as a rebuttal and in the paper)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NNjk3Yczpq", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_AT7b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_AT7b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762433681800, "cdate": 1762433681800, "tmdate": 1762931230648, "mdate": 1762931230648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is dedicated to solving a nonconvex quadratically constrained linear program (QCLP) of special structure inspired by the problem of stepsize choice in asynchronous gradient descent (AGD). The authors show that the set of maximizers belongs to an inverse image of hypercube vertices under a smooth invertible map, thus reducing the nonconvex optimization problem to a search over a finite set. Consequently, the authors propose a solver for the studied class of problems based on an evolutionary heuristic recently proposed in the literature. Additionally, the paper refines the analysis of AGD and explains how the results in quadratically constrained linear programming may help in choosing its stepsizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors conduct an in-depth analysis of a class of nonconvex QCLPs. This analysis might be a valuable contribution as there is a potential to apply techniques and lemmas to other QCLP arising in various applications."}, "weaknesses": {"value": "- The term *bilinear program (BLP)* used in the title and throughout the text is confusing. The authors write: \"BLPs are a class of nonlinear optimization problems in which the objective function or constraints involve products of pairs of variables from two distinct sets\". This definition is correct, but it does not apply to Problem (1) the paper is solving. To the best of my knowledge, Problem (1) is a nonconvex quadratically constrained linear program.\n- One of the paper's main motivations is selecting the step size for AGD. However, it's unclear whether the proposed approach to this problem is practical. First, the problem is reduced to a search over $2^{K+1}$ points, where $K$ is the number of steps. If my understanding is correct, the largest $K$ considered in experiments is $48$, whereas in practice, first-order methods often perform thousands of iterations, which may lead to prohibitively high execution time of the approach. Furthermore, the experiments do not demonstrate the actual performance of AGD with the computed stepsizes, only the time it took to compute the stepsizes.\n- The proof of Theorem 3.2 is almost identical to the proof in the post https://math.stackexchange.com/a/3952098/1134792, but no attribution is given. This point requires clarification."}, "questions": {"value": "- How do you compute the map $\\Psi$ (and its inverse) in your solver?\n- Could you add a caption to the first figure?\n- I suggest moving the definition of extreme point to subsection 3.2 to improve readability.\n- Please consider giving the definition of diffeomorphism in appendix to remind it to a reader.\n\nI am open to reconsidering the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1ph5tr8E0A", "forum": "T5LoOUuZDc", "replyto": "T5LoOUuZDc", "signatures": ["ICLR.cc/2026/Conference/Submission19268/Reviewer_i93o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19268/Reviewer_i93o"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19268/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762531602714, "cdate": 1762531602714, "tmdate": 1762931230096, "mdate": 1762931230096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}