{"id": "lL6htAaolp", "number": 12300, "cdate": 1758206925192, "mdate": 1763735150953, "content": {"title": "Why Adversarially Train Diffusion Models?", "abstract": "Adversarial Training (AT) is a known, powerful, well-established technique for improving classifier robustness to input perturbations, yet its applicability beyond discriminative settings remains limited. Motivated by the widespread use of score-based generative models and their need to operate robustly under substantial noisy or corrupted input data, we propose an adaptation of AT for these models, providing a thorough empirical assessment.\nWe introduce a principled formulation of AT for Diffusion Models (DMs) that replaces the conventional *invariance* objective with an *equivariance* constraint aligned to the denoising dynamics of score matching. Our method integrates seamlessly into diffusion training by adding either random perturbations--similar to randomized smoothing--or adversarial ones--akin to AT.\nOur approach offers several advantages: **(a)** tolerance to heavy noise and corruption, **(b)** reduced memorization, **(c)** robustness to outliers and extreme data variability and **(d)** resilience to iterative adversarial attacks.\nWe validate these claims on proof-of-concept low- and high-dimensional datasets with *known* ground-truth distributions, enabling precise error analysis. We further evaluate on standard benchmarks (CIFAR-10, CelebA, and LSUN Bedroom), where our approach shows improved robustness and preserved sample fidelity under severe noise, data corruption, and adversarial evaluation. Code available upon acceptance.", "tldr": "Adversarial Training for Diffusion Models makes them extremely robust to outliers, resilient to noise and corrupted data, and finally more secure", "keywords": ["Robustness", "Adversarial training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c1881807ead7499c7b06d0d9e510579ba69613d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an adversarial training framework for diffusion models that incorporates adversarial regularization directly into the denoising objective to improve robustness against noisy training data and input perturbations during the diffusion process. The proposed method (Robustadv) enforces equivariance by ensuring that small perturbations to noisy samples lead to proportionally consistent responses, while the perturbation magnitude is adaptively bounded to preserve the diffusion process’s stability. Experiments on synthetic and real datasets such as CIFAR-10, CelebA, and LSUN Bedroom demonstrate that the method enhances resistance to corruption, reduces memorization, and achieves smoother diffusion trajectories that allow faster sampling, all while maintaining high image quality even under heavy noise conditions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper introduces a novel perspective on adversarial training for diffusion models by shifting the focus from classifier-level invariance to score-field equivariance. This insight effectively redefines robustness in the context of generative diffusion processes. \n\n2. The proposed method is simple yet well grounded, requiring only a small modification. The time-dependent perturbation bound maintains compatibility with the diffusion process.\n\n3. The paper presents extensive experiments across synthetic and real-world datasets, clearly showing improvements in robustness against noisy training data, reduced memorization, and smoother diffusion trajectories. The analysis of faster sampling and resilience to trajectory attacks further supports the method’s practical value."}, "weaknesses": {"value": "The paper is overall well executed, with clear motivation, principled formulation, and comprehensive experiments. The only minor weakness is that the evaluation focuses mainly on DDPM and DDIM frameworks, leaving open whether the proposed adversarial regularization generalizes to newer diffusion models."}, "questions": {"value": "- Given that the proposed objective enforces local smoothness in the score field, could this formulation be extended to improve adversarial purification or other defense mechanisms?\n- Would the proposed adversarial regularization also improve robustness when training on datasets with natural corruptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SAtWejA4ZK", "forum": "lL6htAaolp", "replyto": "lL6htAaolp", "signatures": ["ICLR.cc/2026/Conference/Submission12300/Reviewer_PUAf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12300/Reviewer_PUAf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761456042994, "cdate": 1761456042994, "tmdate": 1762923230026, "mdate": 1762923230026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an adaptation of Adversarial Training (AT) for Diffusion Models, framing it as an equivariance constraint aligned with denoising dynamics. The method aims to improve model robustness against noise, data corruption, and adversarial attacks, while also reducing memorization. The authors provide empirical validation on both low-dimensional synthetic data and standard image benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Successfully adapts Adversarial Training, a discriminative technique, to the generative setting of Diffusion Models via a novel equivariance constraint.\n\nQuality & Significance: Comprehensive experiments show the method confers clear advantages: robustness to noise/corruption, reduced memorization, and security against adversarial attacks. This is highly relevant for safe and reliable deployment of DMs."}, "weaknesses": {"value": "- The paper focuses on proof-of-concept and standard benchmarks. Testing on more complex, large-scale datasets (e.g., ImageNet) would further strengthen the claims of general applicability.\n\n- The computational overhead of adversarial training during the diffusion process is not thoroughly discussed.\n\n- related work: The related work section lacks engagement with several key areas of relevant research. This includes: 1) foundational work on robust denoising autoencoders [1], 2) the parallel and distinct line of research on adversarial purification using diffusion models [2], and 3) recent analysis specifically on the nature of adversarial vulnerabilities in DMs [3]. Addressing these would better contextualize the paper's contributions. I think that is not much work for the rebuttal.\n\n\nReferences\n1. \"Time-based Sampling and Reconstruction of Non-bandlimited Signals\" - https://ieeexplore.ieee.org/document/8682626\n2. \"Diffusion Models for Adversarial Purification\" - https://arxiv.org/abs/2205.07460\n3.  ''Adversarial Examples are Misaligned in Diffusion Model Manifolds'' - https://arxiv.org/abs/2401.06637"}, "questions": {"value": "1. What is the estimated computational overhead (training time, memory) of your adversarial training method compared to standard diffusion model training?\n\n2. Have you explored the trade-offs between the \"random perturbation\" (smoothing) and \"adversarial perturbation\" variants of your method in terms of robustness gain vs. computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6VdQruVXN3", "forum": "lL6htAaolp", "replyto": "lL6htAaolp", "signatures": ["ICLR.cc/2026/Conference/Submission12300/Reviewer_2wFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12300/Reviewer_2wFr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835832373, "cdate": 1761835832373, "tmdate": 1762923229252, "mdate": 1762923229252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that adversarial training (AT) for classifiers, which enforces invariance of predictions under small input perturbations, does not transfer directly to diffusion models (DMs), whose training objective is regression to injected noise. They propose that AT for DMs should instead enforce local equivariance of the noise predictor: when the trajectory state $x_t$ is perturbed by $\\delta$, the predicted noise should change by (approximately) $\\delta$. Formally, they add to the standard DDPM loss an equivariance regularizer (Eq. (6)) and train either with random perturbations or adversarial FGSM‑style perturbations. A time-dependent perturbation radius $r_{\\beta}(t)$ is scheduled to respect the forward‑process variance. The adversarial step uses a variance-aware projection.\n\nEmpirically, on a synthetic 3D and a linearized butterflies dataset, the method produces tighter score fields and trajectories, better PSNR and subspace recovery under corruption. On CIFAR‑10, CelebA, LSUN Bedroom, when 90% of the training data are corrupted with Gaussian noise, Robustadv reduces FID versus DDPM/DDIM. Conversely, when training on clean data, Robustadv degrades FID with smoother but less detailed images. The paper also reports fewer near-duplicates by DINOv2 similarity, better tolerance to trajectory-space attacks and sometimes better FID."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper carefully argues that classifier-style invariance is misaligned for diffusion’s noise-regression objective, and instead enforces local equivariance of the score under trajectory perturbations; the formulation is explicit and contrasted with a naive invariance loss, with toy-world visual evidence that invariance drifts off-manifold\n2. The training recipe is simple and can be seen as a plug-in. The proposed regularizer can be easily integrated with standard DDPM training. The time-dependent perturbation radius and adversarial update are demonstrated clearly.\n3. With 90% of training samples noised ($\\sigma = 0.1/0.2$), Robustadv sharply improves FID over DDPM/DDIM on CIFAR-10/CelebA/LSUN, which are supported by qualitative samples as well."}, "weaknesses": {"value": "1. The paper positions itself against noise-aware diffusion training but does not compare to them experimentally. This leaves competitiveness under “unknown corruption” unestablished.\n2. The penalty on clean data is substantial yet under-analyzed. With $p = 0\\%$ noise, FID worsens markedly. The authors do not provide a trade-off analysis in depth.\n3. The ELBO derivation with adversarial sub‑steps (Appendix A.1.3) is heuristic and does not yield quantitative guarantees in theory; also, assumptions about Gaussian intermediate states under non‑Gaussian perturbations are not formalized.\n4. The perturbation radius $r_{\\beta}(t)$ and scaling of $\\lambda_t$ are primarily hand-crafted. No principled guidance on stability/consistency or dataset-noise dependence, which should have been provided, given that the adversarial training recipe may have altered the optimization landscape of the diffusion model.\n5. The training is ~2.5x slower than standard diffusion model training, which is however already notoriously slow. In practice, this computational overhead may be unacceptable, especially provided with the scale of data used to train a diffusion model successfully."}, "questions": {"value": "1. Can you compute-matched comparisons to noise-aware diffusion training methods on CIFAR-10/CelebA/LSUN for $p \\in \\{ 0.3, 0.6, 0.9 \\}$ and $\\sigma \\in \\{ 0.05, 0.1, 0.2 \\}?\n2. The evaluation in terms of the robustness to adversarial attacks are quite limited and do not match the common practice in the adversarial defense community. Can you evaluate EOT over diffusion stochasticity and adaptive attacks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "umWpikQ3KK", "forum": "lL6htAaolp", "replyto": "lL6htAaolp", "signatures": ["ICLR.cc/2026/Conference/Submission12300/Reviewer_Djvf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12300/Reviewer_Djvf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146584981, "cdate": 1762146584981, "tmdate": 1762923228849, "mdate": 1762923228849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}