{"id": "qYHlGI9U1W", "number": 22852, "cdate": 1758336341787, "mdate": 1759896842805, "content": {"title": "Distractor-Robust Reinforcement Learning via Variational Bisimulation", "abstract": "Model-based reinforcement learning (MBRL) promises data efficiency and generalization, but typical reconstruction-based objectives encourage models to waste representational capacity on task-irrelevant distractors.  We introduce VIBES (Variational Inference for Bisimulation-based Encoded States), a new objective that replaces pixel reconstruction with an adversarial term, which enforces that latent states suffice to predict both rewards and the _next_ latent state. We show theoretically that, under mild assumptions, global optima of this objective correspond to encoders that induce bisimulation relations, ensuring that latent states capture task-relevant information while discarding irrelevant variation. Our method serves as a drop-in replacement for Dreamer’s model-learning component and achieves state-of-the-art performance on the Distracting Control Suite. Unlike prior approaches, it does not rely on image-specific augmentations and applies equally well to high-dimensional vector-state tasks, demonstrated on a 100-link swimmer. Finally, latent-space analyses (UMAP embeddings and nearest-neighbor probes) confirm that the learned representations are sensitive to task-relevant structure while invariant to distractors.", "tldr": "Using variational inference, we learn bisimulation-based representations that discard distractors while preserving task-relevant state.", "keywords": ["state abstraction", "variational inference", "bisimulation relations", "model-based reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4f079a641c45f4ea778c8c67a5b2779c1cacbff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel objective function for model learning in model-based reinforcement learning.\nSpecifically, it proposes a modification of the objective function used in Dreamer (Hafner et al., 2020).\nIn Dreamer, the observation $o_t$ at time $t$ is assumed to depend solely on the latent state $s_t$.\n\nIn contrast, this paper removes such an assumption and instead learns the distribution of the latent state $s_t$ conditioned on the past observations $o_{<t}$ and actions $a_{<t}$.\nIn the resulting objective function, the reconstruction loss is omitted, and a density ratio between  $ p ( \\hat{s}_{t} \\mid \\hat{s}_{t-1}, \\hat{a}_{t-1})$ and $p( \\hat{s}_t \\mid \\hat{s}_{< t}, \\hat{a}_{< t})$ is introduced.\n\nThe proposed method is evaluated on continuous control tasks with visual distractions, where it outperforms baseline methods in the reported experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed modification to the model structure and the objective function is interesting and appears to be novel.\n\n- The advantages of the proposed method are clearly demonstrated in the experiments.\n\nThe proposed modification of the model structure and objective function seems reasonable.\nSince more recent variants of Dreamer, such as DreamerV2 and DreamerV3, still employ architectures similar to that of the original Dreamer, the proposed method may also improve the performance of these newer variants."}, "weaknesses": {"value": "- The presentation of the proposed method could be improved.\n\n- The experiments lack comparison with several recent and well-known methods.\n\n- A more extensive discussion on the relation to recent methods is desirable.\n\nIn particular, the adversarial network component of the proposed method is insufficiently explained.\nAlthough the derivation becomes clearer after reading the appendix, I believe that Equations (6)–(9) should be incorporated into the main text to provide intuition for why the model $\\hat{p} ( \\tilde{s}_{t} | \\tilde{s}_{<t}, o_{<t}, a_{<t})$ is referred to as the “adversary network.”\n\nWhile the experiments show that the proposed method outperforms Dreamer, the baseline methods used appear to be somewhat outdated.\nMore recent baseline methods include:\n\n[R1] TD-MPC2: Scalable, Robust World Models for Continuous Control. Nicklas Hansen, Hao Su, Xiaolong Wang. ICLR 2024.\n[R2] Towards General-Purpose Model-Free Reinforcement Learning. Scott Fujimoto, Pierluca D'Oro, Amy Zhang, Yuandong Tian, Michael Rabbat. ICLR 2025.\n\nAlthough a variant of DrQ is used in the comparison, I would recommend using DrQv2 [R3] \n\n[R3] Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning. Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto. ICLR 2022.\n\nRecent methods that leverage model learning, such as TD-MPC2 and Mr.Q, also remove the reconstruction loss.\nTherefore, removing the reconstruction loss itself is not a novel idea, and I recommend adding a discussion clarifying how the proposed approach relates to these recent methods.\n\nIn addition, newer variants of Dreamer, such as DreamerV2, DreamerV3, and Dreamer4, still employ reconstruction losses, and thus may benefit from the proposed objective function.\nIt would strengthen the paper to include experiments demonstrating the effect of the proposed objective function on these more recent Dreamer variants.\n\n*Minor Comment*\n\nIn the appendix, line 538, the text reads: “Similar to our derivation of the Dreamer model objective.”\nHowever, it seems that the authors are actually referring to the derivation of the objective function in PlaNet (Hafner et al., 2019).\nAlthough Dreamer builds upon the PlaNet model, it would be more accurate to explicitly cite the reference on which the derivation is based."}, "questions": {"value": "- Please elaborate on how the proposed method relates to recent approaches that omit the reconstruction loss.\n\n- It would also be valuable to include comparisons with more recent baseline methods and to evaluate the proposed objective function when integrated into a newer variant of Dreamer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dr65DETDAv", "forum": "qYHlGI9U1W", "replyto": "qYHlGI9U1W", "signatures": ["ICLR.cc/2026/Conference/Submission22852/Reviewer_tGnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22852/Reviewer_tGnB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761272243486, "cdate": 1761272243486, "tmdate": 1762942413832, "mdate": 1762942413832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes to use a bistimulation-driven abstraction in order to learn useful latent representations to tackle the visual distractors  problem with model-based online RL. This work argues that that previous works offer small benefits when learning the full-pixel reconstruction for tasks where visual distractors exist, thus, by replacing the pixel-reconstruction term of a model-based method, Dreamer, with an adversarial objective that enforces a bistimulation-based abstraction, more task-related latent representation can be learned. \nThe authors experimented on the DeepMind Distracting Control Suite, and empirically showed improved performance on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- Although as written by the authors, learning state abstractions to tackle the visual distractor problem is not novel, the proposed method of using bisimulation is well motivated and seems intuitive and novel. Although enforcing bisimulation was previously used [1][2], to my understanding the novelty lies on the way the authors use it, i.e. derive it as an adversarial objective rather than the original balancing term in [1][2], is quite novel to my knowledge.  \n- The results on a designed high-dimensional problem are important and show the efficiency of the method."}, "weaknesses": {"value": "- Some important model-based related works, using other types of state abstractions, that also specifically tackle the visual distractor problem in continuous control might be missing during empirical comparison [2][3]. [2] is a directly related bisimulation work with different formulation.\n- Additionally, not really a weakness but, I’m not really sure why this work is applied to the first iteration of dreamer. It would be great if further performance gain can be shown if this work were built with Dreamerv3 [4].\n- Also not really a weakness but, instead of presenting the main results as a table, it would be nice if the training timesteps/return plot for tasks in table 1 is included (i.e. same as figure 3). \n- Hyperparameters are not clearly listed \n\n- Minor issues such as:\n  - missing texts at the start of row 90    \n  - Typo in row 123: relaitons -> relations\n  - The abbreviation PGM, is not explained in figure 1.a and 1.b"}, "questions": {"value": "I will repost some suggestions/questions that were mentioned in the weakness section here for clarity.\n\n$$\\textbf{Suggestions}$$\nS1:Presentation-wise: fix the typos, clearly state the hyperparameters   \nS2: Presentation-wise: Add the training timesteps/return plot for table 1      \nS3: Presentation-wise: very minor, but somehow rendering page 8 is very heavy (I’ve tested on several devices across different platforms), I suspect it has something to do with the Figure 4a. It would be great if the authors can check.    \nS4: I would prefer to see some ablation study on the *type* of distractions, i.e. using only one type of distraction when evaluating. This could help readers understand which type of distractions bisimulation is strong against and weak against. (c.f. something like Section 8.2 in this paper https://openreview.net/forum?id=dce6ZGkJ1Z)\n\n$$\\textbf{Questions}$$\nQ1: Is there a particular reason to use Dreamerv1 as the base algorithm? It has been 3-4 years.    \nQ2: I’m a little bit confused about the adversarial term, which is the most important term to my understanding that pushes for dynamics sufficiency.  What do the authors think about the empirical results where for some tasks (ball-in-cup catch, finger-spin) the adversarial term is quite important, but for some tasks (carpole-swingup, cheetah-run, reacher-easy, walker-walk) adding it only achieve trivial gains?    \nQ3: For the UMAP analysis, following Q2, since the final performance is similar with or without the adversarial term for reacher-easy, I wonder how the UMAP analysis looks like without the adversarial term? If possible, it would be good to see the same UMAP analysis for tasks where significant performance difference were achieved with or without the adversarial term.    \nQ4: How efficient is the proposed formulation to adopt adversarial bisimulation objectives compared to other bisimulation-principled formulations? Direct comparison with other bistimulation-based work [2] should be included. For [2], their code is public available at https://github.com/bit1029public/HRSSM    \n\n---\n\n[1] P. S. Castro, Scalable methods for computing state similarity in deterministic Markov Decision Processes, AAAI 2020    \n[2] R. Sun et al., Learning Latent Dynamic Robust Representations for World Models, ICML 2024  \n[3] C. Zhu et al., RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability, NeurIPS 2023    \n[4] D. Hafner et al., Mastering diverse control tasks through world models, Nature 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LV9Gh2qz9t", "forum": "qYHlGI9U1W", "replyto": "qYHlGI9U1W", "signatures": ["ICLR.cc/2026/Conference/Submission22852/Reviewer_P65f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22852/Reviewer_P65f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749618467, "cdate": 1761749618467, "tmdate": 1762942413634, "mdate": 1762942413634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Model-Based Reinforcement Learning method called VIBES, designed to address the limitations of traditional reconstruction-based objectives that waste representational capacity on reconstructing task-irrelevant distractors. VIBES replaces pixel-level reconstruction with an adversarial variational objective, constrained by reward sufficiency and dynamics sufficiency, ensuring that the learned latent states retain only task-relevant information. The authors theoretically prove that, under mild assumptions, the global optimum of this objective corresponds to an encoder that induces bisimulation relations, thus preserving control-relevant structure while discarding irrelevant variations. Empirically, VIBES can directly replace the model-learning component in Dreamer, achieving state-of-the-art performance on the Distracting Control Suite (DCS) and demonstrating strong results on high-dimensional vector-state control tasks such as a 100-link Swimmer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed VIBES framework is concise and integrates seamlessly into existing MBRL pipelines such as Dreamer. By simply replacing the pixel reconstruction term with an adversarial variational objective, VIBES maintains the overall architecture while significantly improving representation learning efficiency.\n\n2. Experiments on the Distracting Control Suite (DCS) show that VIBES achieves large performance gains over visual RL baselines such as SAC+RAD and CURL, demonstrating superior robustness and a strong ability to focus on task-relevant information in visually complex environments."}, "weaknesses": {"value": "**1. Dependence on the quality of the Adversary Network**\n\nThe quality of the learned representation heavily depends on the performance of the Adversary Network. In practice, the stability and effectiveness of VIBES can be sensitive to the adversary's training quality. Poor adversary learning may weaken both theoretical guarantees and overall performance. Quantitatively evaluating the performance of the Adversary Network would make the paper's claims more convincing.\n\n**2. Lack of comparison with recent bisimulation-based methods**\n\nThe paper does not include direct comparisons with bisimulation-based methods. Although these works are cited, there is a lack of direct performance or learning curve comparisons, e.g., with DBC[1], MICO[2], and approaches that apply bisimulation metrics within world models[3], making it difficult to quantitatively assess the advantages of the proposed method. In addition, the paper does not present learning curve plots for its experiments; including them would help clarify the convergence behavior more clearly.\n\n**3. Limited experimental generality**\n\nThe empirical evaluation focuses mainly on the Distracting Control Suite (DCS) and the extended Swimmer environment. While the results are impressive, the study would be more convincing if additional experiments on standard DeepMind Control (DMC) tasks or other benchmarks were included to demonstrate broader applicability.\n\n[1] Learning Invariant Representations for Reinforcement Learning without Reconstruction. In International Conference on Learning Representations.\n[2] MICo: Improved representations via sampling-based state similarity for Markov decision processes. Advances in Neural Information Processing Systems.\n[3] Learning Latent Dynamic Robust Representations for World Models. In International Conference on Machine Learning."}, "questions": {"value": "1. Could the authors provide a quantitative evaluation of the Adversary Network's performance to better validate its role in representation learning?\n\n2. Could the authors include direct comparisons with bisimulation metric learning methods and add experiments on standard DeepMind Control (DMC) benchmarks to more comprehensively demonstrate the advantages and generalization ability of the proposed approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tGyBDOVdsL", "forum": "qYHlGI9U1W", "replyto": "qYHlGI9U1W", "signatures": ["ICLR.cc/2026/Conference/Submission22852/Reviewer_PfgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22852/Reviewer_PfgH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796600998, "cdate": 1761796600998, "tmdate": 1762942413369, "mdate": 1762942413369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Variational Inference for Bismulation-based Encoded States (VIBES), a model-based RL method that learns a recurrent latent dynamics model on bismulation representations. Unlike reconstruction-based approaches, which try to capture everything, the bisimulation representation is invariant to task-irrelevant distractors and only captures what's necessary to predict the dynamics and rewards. The main contribution is a new objective to learn this representation by regularizing the latent dynamics against a separate history encoder, which is mathematically shown to induce a bismulation representation. The method is evaluated on the distracted DM control suite, where it outperforms a suite of RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important problem in model-based reinforcement learning: image-based dynamics models are typically trained with pixel reconstruction, and therefore waste model capacity by encoding task-irrelevant details. The paper proposes a reasonable solution grounded in the literature of bisumulation metrics.\n2. The paper conducts detailed ablation studies of the proposed method, justifying the contribution of each objective term."}, "weaknesses": {"value": "1. The mathematical derivation is not sound. Specifically, assumption (2) in Lemma 3.1 leads to a chicken-and-egg problem. The argument that the authors make is: assume latent $s_t$ is independent of the history $H_t$ conditioned on the previous latent $s_{t-1}$ and action $a_{t-1}$, then the history encoder's embedding establishes an bisimulation relation on the space of histories. However, if $s_t$ is independent of history conditioned on $s_{t-1}$ and $a_{t-1}$, then it must have already captured whatever is necessary to predict the future latent state. This is exactly the goal of bisimulation. With this assumption, it becomes a tautology that the history encoder induces a bisumulation. For this reason, it is unclear if the proposed method actually learns a bisimulation.\n2. The probablistic graphical model in Figure 2 lack justification. See Questions 1 for details.\n3. The experiments lack comparsions to relevant model-based RL baselines (reconstruction-based and non-reconstruction-based) and bimulation baselines [1, 2, 3, 4, 5, 6]. The problem of task-irrelevant distractions has been studied by a rich line of work, non of which is referenced or compared to. This is a serious omission and is the main reason I recommend a rejection."}, "questions": {"value": "**Major**\n1. Can you explain the intuition behind the PGM in Figure 1 in more detail? I struggle to understand it, because if $o_t$ sees all $o_{<t}$ then there's no need for $s$.\n2. Can you address the chicken-and-egg problem mentioned in Weakness 1?\n3. How does equation 9 use Gibbs inequality?\n4. Since the adversary network is also a GRU, it intrinsically contains a Markovian state. Isn't this a duplicate of the main model? \n5. In line 595, you mention that the dynamics term is clipped to be non-positive. What happens if you remove the clipping? Can you ablate this design choice?\n\n**Minor**\n\n6. line 90: sentence doesn't have a subject\n\nReferences:\n\n[1] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi. Dream to Control: Learning Behaviors by Latent Imagination. ICLR 2020. \n\n[2] Nicklas Hansen,  Xiaolong Wang,  Hao Su. Temporal Difference Learning for Model Predictive Control. NeurIPS 2022. \n\n[3] Xiang Fu, Ge Yang, Pulkit Agrawal, Tommi Jaakkola. Learning Task Informed Abstractions. NeurIPS 2021. \n\n[4] Chuning Zhu, Max Simchowitz, Siri Gadipudi, Abhishek Gupta. RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. NeurIPS 2023.\n\n[5] Tongzhou Wang, Simon S. Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian. Denoised MDPs: Learning World Models Better Than the World Itself. NeurIPS 2022.\n\n[6] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, Sergey Levine. Learning Invariant Representations for Reinforcement Learning without Reconstruction. ICLR 2021."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CeLTpd0tT4", "forum": "qYHlGI9U1W", "replyto": "qYHlGI9U1W", "signatures": ["ICLR.cc/2026/Conference/Submission22852/Reviewer_u3AN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22852/Reviewer_u3AN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953389654, "cdate": 1761953389654, "tmdate": 1762942413113, "mdate": 1762942413113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A primary motivation of the paper is to tackle the issue of distractors in Model-base Reinforcement Learning (MBRL), where the model training objective is not matched to the control problem. This leads to the model learning features of the data that is unnecessary to the control problem. The paper proposes the learning of state representations that satisfy the bisimulation relations that encode the information relevant to the control problem (i.e, the distribution over the next state and the reward). To this, the paper proposes a variational objective that they show captures the bisimulation relations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Great introduction setting up a clear motivation. \n- Writing is great and often clear. Figure captions clearly state the key takeaway which is nice.\n- Experimental results look promising.\n- Good ablation studies. I have suggested some more in the Questions section (Point 4). \n- Discussion sections are thorough, with a good limitations section.\n- I appreciate the detailed derivations in the Appendix."}, "weaknesses": {"value": "- Please provide references at the following statements:\n\t- Line 54: \"... domain-specific strategies, such as image augmentations, ...\"\n\t- Line 45: \"This problem has been shown to significantly degrade the performance of otherwise strong algorithms such as Dreamer (Hafner et al., 2020), which performs well on clean image-based benchmarks but struggles in the presence of distractors\" — is there a reference for where this study has been carried out? I.e., the authors say that \"this problem has been *shown*...\" — where?\n- Line 142: \"...we can equivalently think of bisimulation relations as partitioning histories.\" — could this statement be expressed mathematically for clarity? Particularly since this is the view that is used in the paper.\n- Appendix C does not detail sufficient information for reproducibility. In particular, it requires the reader to go to other papers (which aren't directly cited in the specific sentences) to find specific details. It would benefit the paper to have the full specification of the architecture (such that the experiments are fully reproducible) in the Appendix.\n### Minor Suggestions\n- Line 128-130 (bisimulation relation) — please add \"and\" after the first condition so it is clear that this is a logical conjunction.\n- Some notation has incorrect underscoring. E.g.,\n\t- Figure 2 caption: $\\tilde{s}\\_{t} + 1$ should be $\\tilde{s}_{t+1}$ (I think).\n\t- Line 317: ... $o<t$... should be $o_{<t}$ (I think).\n- Appendix D looks empty. I am guessing that Figure 5 is supposed to be there? Perhaps a description could be added to Appendix D (this should be done to help the reader) — this will also push Appendix E to be after the image.\n- Please mention in the main text that Appendix C contains the architectural setup for reproducing the experiments."}, "questions": {"value": "- Line 55: \"... whose use defeats the purpose of RL in the first place.\" — why is this the case?\n- Section 2.1: Is the section title used as the start of the sentence? \n- Figure 1: Shouldn't the actions $a_i$ depend on the observations $o_i$?\n- Final paragraph of Section 2.1: Is there an ablation study you could carry out to show that allowing the observations to affect future observations \"removes the pressure for $s_t$ to transmit irrelevant predictive information\"? E.g., what would the experiment results look like if everything in your method is kept the same, but the graphical model of Figure 1a is used. \n- Line 186: How was the fixed variance chosen? What would the results look like if this was changed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HXaVaPxuV1", "forum": "qYHlGI9U1W", "replyto": "qYHlGI9U1W", "signatures": ["ICLR.cc/2026/Conference/Submission22852/Reviewer_EoRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22852/Reviewer_EoRX"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762602953394, "cdate": 1762602953394, "tmdate": 1762942412797, "mdate": 1762942412797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}