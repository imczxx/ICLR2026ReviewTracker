{"id": "cqnQ8rcgdO", "number": 311, "cdate": 1756734940128, "mdate": 1763735163910, "content": {"title": "Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds", "abstract": "Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \\textbf{C}enter-\\textbf{S}urrounding \\textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \\textbf{7.9\\%}, \\textbf{6.7\\%}, and \\textbf{10.3\\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.", "tldr": "", "keywords": ["3D representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31c5ebd04156929deab6f2501e1825e3004a173e.pdf", "supplementary_material": "/attachment/31e95bcfcf009a041addaa84b1eee6f0b15d1d42.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel self-supervised learning (SSL) framework for 3D point clouds, called CSCon (Dual-Branch Center-Surrounding Contrast). The authors argue that current 3D SSL methods are dominated by generative approaches (e.g., Masked Autoencoders), which fail to learn high-level discriminative features. To address this, CSCon introduces a dual-branch structure that separates center and surrounding patches of point clouds, coupled with an inner-instance patch-level contrastive loss. This design aims to improve both global discrimination and local geometric sensitivity. The method is extensively evaluated on multiple benchmarks, achieving state-of-the-art performance under several training protocols with fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Straightforward idea and design:  The center-surrounding dual-branch contrastive paradigm is intuitive. It effectively leverages the spatial structure of 3D point clouds for representation learning.\n\n+ Clear methodology: The formulation of the inner-instance contrastive loss and ablation studies (mask ratio, data augmentation, parameter sharing) are comprehensive and well-explained.\n\n+ Strong experimental results: The proposed method achieves consistent gains across multiple datasets (ScanObjectNN, ModelNet40, ShapeNetPart, S3DIS) and under various evaluation settings."}, "weaknesses": {"value": "- While the method works well empirically, the paper lacks a strong theoretical justification for why the “center-surrounding” partition is optimal for contrastive learning. Could other partitioning schemes yield similar or better performance? For example, in [1], a perception-enlarged KNN strategy was proposed for patch generation, would it be beneficial for your method? Also, as a dual-branch SSL method, [1] should be cited.\n\n[1] Chengzhi Wu, Qianliang Huang, Kun Jin, Julius Pfrommer, Jürgen Beyerer. A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-Supervised Learning. 3DV 2024.\n\n- Self-supervised learning models are prone to falling into local optima or collapsing, particularly in contrastive learning methods. Various strategies have been proposed to mitigate this issue. For example, in pioneering works on image representation learning: (1) SimCLR and MoCo leveraged both positive and negative pairs; (2) MoCo and BYOL employed a momentum encoder; and (3) MoCo, SwAV, BYOL, and SimSiam adopted a stop-gradient operation on one branch. I am particularly surprised that the proposed method achieves successful training without employing any of these strategies, yet avoids collapse. Could you provide some insights into why this approach works? Additionally, please provide several top-tier conference or journal papers that report successful contrastive-based model training without relying on such specialized strategies. \n\n- Ablation on loss temperature (τ)? Although the masking ratio was studied, other sensitive hyperparameters (e.g., τ in Eq.7) were fixed without justification.\n\n- In the caption of Figure 2, shouldn’t it be that the left side displays the masked center points, and the right side shows the masked surrounding points?\n\n- The definitions of the protocols are provided in the supplementary materials. Please note it in the main paper. \n\n- Other minor writing issues. Some sections (especially the introduction and results) could be more concise, and grammar polishing could further improve readability."}, "questions": {"value": "Please refer to the weaknesses, particularly the second point, as this will affect my decision regarding a possible score increase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "duRzdiknAt", "forum": "cqnQ8rcgdO", "replyto": "cqnQ8rcgdO", "signatures": ["ICLR.cc/2026/Conference/Submission311/Reviewer_QAs4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission311/Reviewer_QAs4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764054257, "cdate": 1761764054257, "tmdate": 1762915491417, "mdate": 1762915491417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel Dual-Branch Center-Surrounding Contrastive Learning (CSCon) method for 3D point cloud representation learning. The authors propose a framework where the point cloud is divided into center and surrounding parts, and these two parts are treated as positive pairs during contrastive learning. The approach includes patch-level contrastive loss to enhance the model's ability to capture both high-level discriminative features and fine-grained local details."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear Expression: The paper is well-written, and the methodology is presented in a clear and logical manner. \n\n2.\tEasy to Follow: The paper is well-structured and easy to follow. The authors clearly explain the methodology, experimental setup, and results, ensuring that readers can easily understand the core concepts and contributions.\n\n3.\tReasonable Complexity: The proposed method maintains reasonable complexity while achieving substantial improvements in performance. \n\n4.\tSolid Experimental Design: The experimental design is robust and well thought out. The paper provides clear validation of the method's performance across multiple datasets, and the results convincingly show the advantages of the CSCon approach over existing methods."}, "weaknesses": {"value": "1.\tInnovation Depth: The proposed innovation, where the encoded results remain consistent across different masking strategies for already partitioned patches, is relatively simple and straightforward. While it proves effective in improving performance, the novelty feels incremental when compared to the broader scope of 3D point cloud representation learning. \n\n2.\tComparative Analysis: The authors predominantly compare their method with older works, which highlights the strengths of their approach. However, it would be valuable to include comparisons with more recent research (such as papers published in 2025). This would provide a more comprehensive understanding of where CSCon stands in relation to the latest advancements in the field and strengthen the paper's claim to state-of-the-art performance.\n\n3.\tRedundant Ablation Experiments: The first two ablation experiments appear somewhat redundant. These experiments could be moved to the supplementary materials to streamline the main content. Instead, it would be more valuable to include more essential ablation studies that directly address the key innovations of the method, as discussed in the Questions section."}, "questions": {"value": "1.\tAblation Study:The current ablation studies focus on the impact of the full model. However, I would like to see a comparison where only one branch (either the center or surrounding branch) is kept. What would happen if we only use the first branch or the second branch in isolation?\n\n2.\tMask Strategy and Powerful Baselines: The masking strategy is highlighted as the primary contribution of the paper. However, have you tested your method using more powerful baselines? This would help assess whether the improvements from the masking strategy are still significant when compared to stronger existing models. \n\n3.\tBroader Comparison with State-of-the-Art: While the comparisons made in the paper are insightful, I would recommend broader comparisons with more recent state-of-the-art models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A8NILWXTxA", "forum": "cqnQ8rcgdO", "replyto": "cqnQ8rcgdO", "signatures": ["ICLR.cc/2026/Conference/Submission311/Reviewer_VRw5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission311/Reviewer_VRw5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833711143, "cdate": 1761833711143, "tmdate": 1762915491037, "mdate": 1762915491037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CSCon, a self-supervised 3D point cloud representation learning framework that contrasts center and surrounding patches within a dual-branch architecture. It abandons generative reconstruction (as in MAE-based models) and instead employs an inner-instance patch-level contrastive loss between masked center/surrounding features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, The paper correctly identifies the over-reliance of current 3D SSL on MAE-style reconstruction losses that learn low-level geometry but weak semantics. Addressing this via a contrastive objective that leverages 3D spatial structure is a meaningful idea.\n\n2, Removing decoders and multi-view generation reduces computation and eases implementation.\n\n3, This method achieves state-of-the-art performance."}, "weaknesses": {"value": "1, Incremental conceptual novelty. The “center-surrounding” idea is intuitively similar to spatial partitioning already used in hybrid or region-aware methods (e.g., PointContrast’s local views, ReCon’s cross-patch contrast, Point-CMAE’s implicit local/global separation). The core innovation reduces largely to choosing intra-sample positives differently. Without a new theoretical insight or broader unification, the contribution is modest.\n\n2, CSCon is seems like DetCo [1] in 3D,  local/global contrastive loss.\n\n3, Unjustified reintroduction of patch-level contrastive learning. In 2D self-supervised literature, patch-level contrastive learning has been largely abandoned due to its semantic instability and inefficiency: local patches lack consistent meaning across samples, and strong augmentations make spatial correspondences unreliable, leading to noisy and contradictory gradients. Consequently, modern 2D frameworks (e.g., MAE, BEiT, DINOv2) have replaced patch-level InfoNCE with reconstruction or self-distillation losses that yield more stable local supervision. This paper claims that patch-level contrastive learning is more effective than MAE-style reconstruction in 3D, yet provides no theoretical explanation or empirical analysis clarifying why a contrastive objective—shown unstable in 2D—would suddenly become superior in the 3D domain. This weakens the conceptual credibility of the claimed improvement.\n\n\n[1] DetCo: Unsupervised Contrastive Learning for Object Detection. ICCV 2021"}, "questions": {"value": "See weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6iZhfRKp8P", "forum": "cqnQ8rcgdO", "replyto": "cqnQ8rcgdO", "signatures": ["ICLR.cc/2026/Conference/Submission311/Reviewer_zpkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission311/Reviewer_zpkS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959792118, "cdate": 1761959792118, "tmdate": 1762915490860, "mdate": 1762915490860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a dual-branch center-surrounding contrastive learning method to pre-train 3D point cloud models. They point out that existing mask auto-encoding methods performs poorly on linear probing and that contrastive learning mechanism in 3D remains undeveloped. The proposed CSCon framework first split input point cloud into centers and corresponding point patches. Then a dual branch framework encode and mask centers and patches separately. Transformer blockes are leveraged to predict the mask regions. Finally, contrastive loss is performed between two branches to realize a fine-grained patch-level contrastive learning. The authors conduct extensive experiments on various benchmarks and achieves promising results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed methods combines contrastive learning and MAE-style pre-training, realizing fine-grained patch-level reasoning. The method is technically sound and conceptually interesting.\n\n2. The experiment results show relatively strong performance on various benchmarks."}, "weaknesses": {"value": "1. The method is only limited to object-level datasets.  A major difference between contrastive learning and MAE-style pre-training is that contrastive learning can be better applied to more complex scene-level scenarios. Since the authors categorize their method to contrastive-based method, the missing experiments of pre-training directly on scene-level datasets like ScanNet would largely undermine the strength of the paper.\n\n2. It would be better if the author could analyze more thoroughly into the difference between these feature groups with ablation studies:\n(1) Es & Vs & Ec'+Es & Es'+Ec (2) Ec & Vc & Es'+Ec & Ec'+Es. From the method, Vc and Vs seem like reconstruction of Es'+Ec and Ec' + Es. However, since no point cloud is explicitly reconstructed, the readers would have no idea about what Vc and Vs actually encoded. More thorough analysis and experiments into this issue will reveal more in-depth insight of the proposed method."}, "questions": {"value": "1. I'm confused by Figure 2, the visualization on the ShapeNet set. The figure shows masking ratio of 40%, 60% and 80%. However, the masked region in the figure seems much smaller than expected. For example, the random mask 40% sample only masks a tiny part of the bottom of the plane, while the random mask 80% sample only masks approximately 40% of the sample. \n\n2. What is the resource and time consumption of this method? It would be better if efficiency could also be mentioned besides params used.\n\n3. Missing related work comparison: \n\n[1] Gao, Ziqi, Qiufu Li, and Linlin Shen. \"DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n\n[2] Lin, Xuanyu, et al. \"PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud Pretraining.\" arXiv preprint arXiv:2507.17296 (2025).\n\n[3] Wang, Ziyi, et al. \"UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[4] Cheng, Haozhe, et al. \"PointFM: Point Cloud Understanding by Flow Matching.\" IEEE Robotics and Automation Letters (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DeHuVJCmxH", "forum": "cqnQ8rcgdO", "replyto": "cqnQ8rcgdO", "signatures": ["ICLR.cc/2026/Conference/Submission311/Reviewer_dY7K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission311/Reviewer_dY7K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988002687, "cdate": 1761988002687, "tmdate": 1762915490121, "mdate": 1762915490121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All reviewers and ACs."}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nWe sincerely appreciate your thorough feedback and constructive suggestions. Overall, the reviewers have recognized our work's well-founded motivation (dY7K, zpkS, QAs4), clear presentation (VRw5, QAs4), and the effectiveness of our method (dY7K, zpkS, VRw5, QAs4), which is supported by comprehensive experimental results (dY7K, zpkS, VRw5, QAs4). We note that the main concerns raised by the reviewers primarily focus on the theoretical foundation of the contrastive paradigm (zpkS, QAs4), conceptual novelty (zpkS, VRw5), and the need for additional ablation studies and comparative analyses (dY7K, VRw5, QAs4).\n\nIn response to these valuable comments, we have made point-by-point revisions in the updated manuscript (with all changes highlighted in blue for easy reference). Here is a summary of the key updates:\n\n* Incorporate new ablation studies on surrounding-surrounding contrast.\n* Add ablation studies on using PointMamba backbone. \n* Provide the computational cost and time during both pre-training and fine-tuning stages. \n* Provide a theoretical explanation on the necessity of introducing uniformity constraints by treating other patches from the same sample as negative samples. \n* Included additional comparative results with the latest methods published in 2025. \n* Conduct ablation studies on the hyperparameter sensitivity of the temperature coefficient $\\tau$."}}, "id": "TuqaelB1nL", "forum": "cqnQ8rcgdO", "replyto": "cqnQ8rcgdO", "signatures": ["ICLR.cc/2026/Conference/Submission311/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission311/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission311/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763642189202, "cdate": 1763642189202, "tmdate": 1763642189202, "mdate": 1763642189202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}