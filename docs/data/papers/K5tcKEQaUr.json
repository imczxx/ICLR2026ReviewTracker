{"id": "K5tcKEQaUr", "number": 5485, "cdate": 1757914179738, "mdate": 1763645128206, "content": {"title": "Frequency-Balanced Retinal Representation Learning with Mutual Information Regularization", "abstract": "We propose a frequency-oriented perspective on retinal representation learning by analyzing masked autoencoders (MAE) through the lens of spatial frequency. Our analysis shows that MAE favors low-frequency content while under-encoding diagnostically critical high-frequency structures in retinal images. Because retinal pathology often manifests in high-frequency detail, this bias limits diagnostic performance and motivates frequency-balanced representations. Within a mutual-information (MI) formulation of MAE, we introduce the \\emph{Frequency-Balanced Retinal Masked Autoencoder (RetMAE)}, which augments the reconstruction objective with a MI regularizer that suppresses low-frequency redundancy and accentuates clinically salient high-frequency information. Without altering architecture, RetMAE learns frequency-balanced features that surpass those of MAE-based retinal encoders in both quantitative and qualitative evaluations. These results suggest that a frequency-oriented view provides a principled foundation for future advances in ophthalmic modeling.\noffering new insight into how MAE‚Äôs reconstruction objective amplifies ViT‚Äôs low-pass tendencies in spatially heterogeneous retinal images and enabling a simple MI-based correction that improves retinal encoders.", "tldr": "", "keywords": ["Masked Image Modeling", "Masked Autoencoders", "Representation Learning", "Mutual Information", "Retinal Imaging", "Medical Imaging"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/180f1358e191d1e3df6e4b0524ecc92692f1ff8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a frequency-balanced masked autoencoder framework (RetMAE) that enhances retinal representation learning by introducing a high-frequency mutual information regularizer to emphasize clinically critical high-frequency structures while suppressing redundant low-frequency content."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper does present a clear motivation that high-frequency structures are clinically important in retinal imaging, and it attempts to incorporate this insight through a mutual-information-based regularizer.  And the presentation is good."}, "weaknesses": {"value": "(1) According to Figure 2, the loss L_hmi proposed in this paper and the loss L_rec in the original MAE appear to optimize the latent feature in two fundamentally different directions. Specifically, L_rec aims to optimize ùëß such that it can reconstruct the full Image from Image_mask1. In contrast, L_hmi optimizes ùëß to enable the reconstruction (or transformation) of Image_mask2 from Image_mask1, where mask2 is generated through the high-frequency masking strategy proposed by the authors. This is clearly contradictory and constitutes the most significant issue.\n(2) Were all evaluations in Table 1 conducted on the standard MAE model? The results shown in this table seem to indicate that the standard MAE already has a strong ability to represent high-frequency information, which contradicts the authors‚Äô claim of ‚Äúunder-encoding high-frequency diagnostic structures.‚Äù For example, in the T_low row of Table 1, even after masking 25% of high-frequency information, the CKA value remains as high as 0.990, indicating that the model still retains stable reconstruction capability for high-frequency components. Conversely, in the T_high row, when high-frequency information is used as input, the model shows low CKA because it cannot reconstruct the full image, which is expected. However, the AUROC increases, demonstrating that high-frequency information is highly discriminative; when low-frequency, lesion-irrelevant content is removed, the model‚Äôs prediction accuracy improves. Therefore, the evidence presented in Table 1 may actually support the importance of high-frequency features rather than demonstrating the insufficiency of MAE in encoding them. I believe the authors have not conducted a sufficiently thorough investigation in this aspect.\n(3) As highlighted in Comment (1), there is a potential conflict between the two loss terms. The authors should explicitly report the training weights assigned to each loss or provide a sensitivity analysis (e.g., a performance graph under different loss weight configurations) to demonstrate the impact of the loss balance on model performance.\n(4) The innovation of the proposed method appears to be limited, as it essentially adds a frequency-based loss on top of MAE, while the use of high-frequency representations to capture lesion-related features in retinal images has already been explored in numerous prior studies.\n\nIf the authors can provide a clear theoretical justification or empirical evidence resolving the contradictions I raised‚Äîparticularly regarding the compatibility of the two loss objectives and the interpretation of Table 1‚ÄîI am willing to reconsider my rating and increase my score accordingly."}, "questions": {"value": "Were all evaluations in Table 1 conducted on the standard MAE model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "atZDOUiv5C", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Reviewer_xA7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Reviewer_xA7m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581435129, "cdate": 1761581435129, "tmdate": 1762918089235, "mdate": 1762918089235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a frequency-balanced masked autoencoder framework (RetMAE) that enhances retinal representation learning by introducing a high-frequency mutual information regularizer to emphasize clinically critical high-frequency structures while suppressing redundant low-frequency content."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper does present a clear motivation that high-frequency structures are clinically important in retinal imaging, and it attempts to incorporate this insight through a mutual-information-based regularizer.  And the presentation is good."}, "weaknesses": {"value": "(1) According to Figure 2, the loss L_hmi proposed in this paper and the loss L_rec in the original MAE appear to optimize the latent feature in two fundamentally different directions. Specifically, L_rec aims to optimize ùëß such that it can reconstruct the full Image from Image_mask1. In contrast, L_hmi optimizes ùëß to enable the reconstruction (or transformation) of Image_mask2 from Image_mask1, where mask2 is generated through the high-frequency masking strategy proposed by the authors. This is clearly contradictory and constitutes the most significant issue.\n(2) Were all evaluations in Table 1 conducted on the standard MAE model? The results shown in this table seem to indicate that the standard MAE already has a strong ability to represent high-frequency information, which contradicts the authors‚Äô claim of ‚Äúunder-encoding high-frequency diagnostic structures.‚Äù For example, in the T_low row of Table 1, even after masking 25% of high-frequency information, the CKA value remains as high as 0.990, indicating that the model still retains stable reconstruction capability for high-frequency components. Conversely, in the T_high row, when high-frequency information is used as input, the model shows low CKA because it cannot reconstruct the full image, which is expected. However, the AUROC increases, demonstrating that high-frequency information is highly discriminative; when low-frequency, lesion-irrelevant content is removed, the model‚Äôs prediction accuracy improves. Therefore, the evidence presented in Table 1 may actually support the importance of high-frequency features rather than demonstrating the insufficiency of MAE in encoding them. I believe the authors have not conducted a sufficiently thorough investigation in this aspect.\n(3) As highlighted in Comment (1), there is a potential conflict between the two loss terms. The authors should explicitly report the training weights assigned to each loss or provide a sensitivity analysis (e.g., a performance graph under different loss weight configurations) to demonstrate the impact of the loss balance on model performance.\n(4) The innovation of the proposed method appears to be limited, as it essentially adds a frequency-based loss on top of MAE, while the use of high-frequency representations to capture lesion-related features in retinal images has already been explored in numerous prior studies.\n\nIf the authors can provide a clear theoretical justification or empirical evidence resolving the contradictions I raised‚Äîparticularly regarding the compatibility of the two loss objectives and the interpretation of Table 1‚ÄîI am willing to reconsider my rating and increase my score accordingly."}, "questions": {"value": "Were all evaluations in Table 1 conducted on the standard MAE model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "atZDOUiv5C", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Reviewer_xA7m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Reviewer_xA7m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581435129, "cdate": 1761581435129, "tmdate": 1763654748261, "mdate": 1763654748261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the high-frequency information in fundus images that is clinically relevant. The authors introduced an information-theoretic auxiliary supervision into the MAE pretraining paradigm to guide the encoder toward clinically important regions, without requiring architectural modifications. The overall logic‚Äîfrom problem identification to the proposed solution and experimental validation‚Äîis clear and well-structured."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and coherent logical flow from motivation and problem formulation to analysis, proposed solution, and experimental validation, making it easy to follow and understand.\n\n2. It offers new and interesting insights into model training for ophthalmic applications.\n\n3. The theoretical derivation of the proposed method is sound, and the approach itself is easy to reproduce."}, "weaknesses": {"value": "1. Limited novelty. The inspiration of this work appears to be directly derived from Huang, Tao, et al. ‚ÄúLearning Mask Invariant Mutual Information for Masked Image Modeling.‚Äù arXiv preprint arXiv:2502.19718 (2025). Although this paper is cited, I would still like the authors to explicitly clarify which parts of the current method are independently proposed.\n\n2. I have concerns regarding the generalizability of the proposed approach.\n(i) The low-pass filtering property originates from the ViT architecture itself, and this phenomenon is not unique to MAE.\n(ii) The application scenario in this work is limited to color fundus photography. From the perspective of developing a robust retinal foundation model, MAE is not the only viable paradigm even within the image modality. For example, VisionFM, which follows the iBOT framework, also builds a powerful image encoder. From the standpoint of understanding the mechanism of MAE, this paper does not provide new insights. The authors need to further elaborate on the substantive contribution of their work.\n\n3. The performance evidence is limited. The chosen downstream tasks are relatively few and of low difficulty (e.g., binary classification of DR, glaucoma, and AMD). Considering that the motivation of this work focuses on clinically relevant high-frequency lesions, the authors are encouraged to validate their method on more challenging tasks to substantiate its claimed clinical value."}, "questions": {"value": "1. This paper introduces an additional high-frequency contextual feature constraint into the latent space of MAE. Some previous studies have imposed constraints directly on the masking strategy (e.g., image-entropy-based masking). I encourage the authors to discuss this line of work to further highlight the value of their proposed method.\n\n2. According to Figure 3, the performance of RetMAE appears to saturate after pretraining on approximately 12.8k images. Increasing the data size beyond this point seems to yield no significant improvement. The authors attribute this to saturation of model capacity and computation, which is a reasonable explanation. However, given that 12.8k is far smaller than the typical data scale used for foundation model pretraining‚Äîand that the employed encoder architecture has been shown in other domains to effectively utilize much larger datasets‚Äîthis phenomenon remains concerning. The authors should provide a more convincing explanation for this observation.\n\n3. Not all retinal lesions exhibit high-frequency characteristics‚Äîfor example, large hemorrhages or retinal detachments. I would like to see visualizations of such cases to better understand how the proposed method behaves under these conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "posXvg3iyF", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Reviewer_ULHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Reviewer_ULHB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654087264, "cdate": 1761654087264, "tmdate": 1762918088898, "mdate": 1762918088898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The presented work postulates that there is a discrepancy between the most salient visual features in medical images and those learned by current representation learning techniques. In particular, the authors claim that the prevalent masked autoencoder disregards high-frequency features related to retinal pathology in color fundus photographs. To demonstrate this, they rank image patches by the amount of high-frequency content. Subsequently, they show that patches with reduced high-frequency content shape latent space structure while providing substantially less information for downstream tasks. In response to this observation, the authors propose RetMAE, an extension of the loss function of the established masked autoencoder. RetMAE regularizes the latent space by maximizing its mutual information with embeddings of patches with increased high-frequency content. In experiments using four ophthalmological datasets, the authors show that RetMAE outperforms various baselines that rely on a basic masked autoencoder. This effect persists when auxiliary signals, such as text or a pre-trained vision encoder, are included as learning signal, albeit in a diminished capacity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work‚Äôs main motivation that current pre-training paradigms for vision transformers result in suboptimal feature extractors when applied to medical images is very interesting. The authors convincingly support this hypothesis in a set of initial experiments (Figure 1, Section 4, Supplementary Material A1), showing that salient image features differ in natural and medical images, and that masked image modeling has an inductive bias towards low-frequency features.\n\n- The provided solution to this problem is theoretically well founded and experimentally shown to improve performance. As such, it has the potential to benefit the large scientific community in the field of medical image analysis.\n\n-  The clinical application of ophthalmological image analysis is very well selected. Many retinal diseases manifest as small pathologies, resulting in high-frequency image features. Furthermore, there are several prominent works that have used large-scale pre-training of masked autoencoders to derive ophthalmological foundation models.\n\n- The paper is clearly structured, nicely illustrated and generally well written. Additionally, the authors include extensive supplementary material that provides in-depth technical detail about the method and experimental setup."}, "weaknesses": {"value": "- The motivating hypothesis that medical images contain more salient high-frequency image features is only explored for color fundus photographs. The importance and reach of the work would substantially increase if similar findings were shown for other types of data. Similarly, the efficiency of the proposed solution is only demonstrated for color fundus photographs so that it is unclear whether the proposed method seamlessly translates to other settings or requires extensive hyperparameter tuning for both the extraction of high-frequency information and the loss weighting.\n\n- The proposed solution is highly complex. In particular, it relies on estimation of mutual information via an Donsker-Varadhan estimator, which is known to numerically unstable (Belghazi, Mohamed Ishmael, et al. \"Mutual information neural estimation.\" International conference on machine learning. PMLR, 2018). I could envision that conceptionally much simpler solutions exist that emphasize high-frequency features. For example, one could adjust the masking scheme to prioritize patches with increased high-frequency content (similar to Xie, Jiahao, et al. \"Masked Frequency Modeling for Self-Supervised Visual Pre-Training.\" The Eleventh International Conference on Learning Representations). Alternatively, one could provide the high-pass-filtered image as additional input (Wang, Wenxuan, et al. \"Fremim: Fourier transform meets masked image modeling for medical image segmentation.\" Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024). The authors should discuss the aforementioned works in more detail and include them as baselines."}, "questions": {"value": "- At the moment, the performance of the proposed method is only quantified via linear probing using the latent representations. I believe that full fine-tuning should also be conducted considering that the ultimate downstream performance matters most in applied domains such as medical image analysis.\n\n- Considering that most ophthalmological foundation models make their training code and weights public, I believe that the authors should strongly consider doing the same.\n\n- Several core concepts of the paper are only very briefly introduced or require consultation of the supplementary material. I suggest that the section on the interpretation of the masked image modeling objective through the lens of a Lagragian is slightly extended so that it can be understood without consulting previous works by Huang et al. and Tishby and Zaslasky. Similarly, the frequency score calculation should be briefly introduced in the main manuscript considering its vital importance, instead of only being introduced in the supplementary material.\n\n- Additionally, I struggled with the notation on several occasions. Already in the very first mathematical paragraph, the variable $N$ is overloaded, $D$ not introduced, and mutual information $I$ is not defined. Later, the use of $X$ varies to signal whether it denotes input or decoded image tokens. The authors should carefully parse the manuscript once again, aiming to improve the clarity of its mathematical passages.\n\n- On a minor note, the acronym CKA is not introduced at its first appearance in the introduction section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3LIzLwLGvy", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Reviewer_d9g5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Reviewer_d9g5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656977841, "cdate": 1761656977841, "tmdate": 1762918088483, "mdate": 1762918088483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a frequency-aware masked autoencoder (MAE) for unsupervised pretraining on retinal images, called RetMAE. This is accomplished by including a high-frequency regularization term in the objective function that reduces low-frequency redundancy. The authors main claim is that the diagnostic information in retina images are encoded in high frequencies, and thus better representing these areas yields better downstream accuracy. There is a section on evaluating the frequency bias in MAE representations applied to fundus images in which the paper presents centered kernel alignment (CKA) and linear probing as evidence. The experimental section utilizes five publicly available datasets and compares against two other MAE-based approaches as well as a vision-language baseline. \n\nAlthough the paper doesn't contribute a significant new algorithmic framework, its approach in providing frequency-based context latents to guide the representation learning paradigm for applications in which frequency bias is an impediment could be a valuable contribution. There are, however, a few areas of both theory and practice that need clarification. \n\nOn theory:\n\n1- What is the purpose of using a variational autoencoder with a fixed variance Gaussian mapping? From theorem 1, it reduces the reconstruction error to minimizing the conditional in eq 2. However, it is not clear if this enforced constraint is warranted. Is this constraint enforcing any aspect of the regularization framework?\n2- The choice of using $\\mathcal L_{MINE}$ as the context-alignment training objective is not quite clear. In other words, why does estimating MINE maximize the conditional?\n\nOn Application:\n1- Does this framework extend beyond retinal fundus images? Could other factors than frequency bias be incorporated in the regularization objective?\n2- How much computational complexity is added to the problem by incorporating the proposed high-frequency regularization?\n3- How does the pretrained encoder fare in a formal classification tasks rather than the employed linear probing?\n\nAdditional suggestions:\n\n1) use the figure in appendix A instead of Figure 1 in the main paper. The figure from the appendix better justifies the frequency bias of retinal fundus images as compared to natural images, e.g. ImageNet.\n2) CKA and its relevance to the claimed frequency bias should be explained more clearly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach in utilizing a bias term as regularization to improve representation learning in certain application is interesting.\nThis approach could be potentially significant for applications that are not based on natural images."}, "weaknesses": {"value": "Better discussion is needed to connect the theoretical aspect of the work (MI) with the practical tools utilized (MINE estimation)."}, "questions": {"value": "Provided in the summary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kzlLGsluKs", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Reviewer_Rstj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Reviewer_Rstj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117940036, "cdate": 1762117940036, "tmdate": 1762918088156, "mdate": 1762918088156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all reviewers for their time and constructive feedback, which have significantly improved our paper.\n\nWe are especially grateful that the reviewers recognize several strengths of our work, including the **theoretically well-founded MI framework** with sound derivation (d9g5, ULHB), the **convincing analysis demonstrating frequency bias** in medical versus natural images through initial experiments (d9g5, xA7m), the **novel insights into model training for ophthalmic applications** with clinical relevance (ULHB, d9g5), the **easy-to-reproduce approach** with extensive technical detail (ULHB, d9g5), and the **potential significance for applications beyond natural images** using bias-based regularization (Rstj, d9g5).\n\nWe believe that we have successfully addressed all major concerns in this rebuttal. Below, we summarize the changes that have been included in the revised version (shown in blue):\n\n- **Section 3 [L183-L189]**: In response to reviewer d9g5, we **expanded the MI/Lagrangian interpretation** to be self-contained without requiring external references.\n- **Section 3 [L196-L205]**: In response to reviewer d9g5, we **brought the high-frequency extraction procedure into the main text** with detailed description.\n- **Section 3 [L162-L182]**: In response to reviewer d9g5, we **clarified all mathematical notation**, explicitly defining variables $N$, $D$, and mutual information $I$.\n- **Section 4 [L209-L235]**: In response to reviewers Rstj and xA7m, we **clarified the CKA interpretation** and its connection to frequency bias with explicit descriptive labels in Table 1.\n- **Section 5.1 [L254-L259]**: In response to reviewer Rstj, we clarified that the **fixed-variance Gaussian decoder is a modeling assumption** for analytical purposes, not an implementation detail.\n- **Section 5.2 [L297-L320]**: In response to reviewers Rstj and xA7m, we clarified the **role of MINE as a lower-bound estimator** and the **complementary relationship between $L_{\\text{rec}}$ and $L_{\\text{hmi}}$**.\n- **Section 5.2 [L347-L349]**: In response to reviewer xA7m, we **explicitly reported the loss weights** used throughout experiments.\n- **Section 6.2 [L395-L400]**: In response to reviewers Rstj and d9g5, we **added full fine-tuning results** demonstrating effectiveness across different evaluation protocols.\n- **Section 7 [L524-L539]**: In response to all reviewers, we **strengthened the discussion of generalizability, novelty, and extensibility** to other domains and regularization factors.\n- **Appendix A.7 [L1507-L1551]**: In response to reviewer ULHB, we **added visualizations of class-to-patch attention** for large hemorrhages and retinal detachments.\n- **Appendix A.8 [L1315-L1343]**: In response to reviewer ULHB, we **added multi-disease benchmark results (FIVES and RFMiD2)** demonstrating clinical value.\n- **Appendix A.8 [L1457-L1484]**: In response to reviewer Rstj, we **added detailed computational complexity analysis** with inference time and FLOPs measurements.\n- **Appendix A.8 [L1487-L1502]**: In response to reviewer xA7m, we **included comprehensive loss weight sensitivity analysis** demonstrating empirical robustness."}}, "id": "SaghveZO5S", "forum": "K5tcKEQaUr", "replyto": "K5tcKEQaUr", "signatures": ["ICLR.cc/2026/Conference/Submission5485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5485/Authors"], "number": 21, "invitations": ["ICLR.cc/2026/Conference/Submission5485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763651959547, "cdate": 1763651959547, "tmdate": 1763651959547, "mdate": 1763651959547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}