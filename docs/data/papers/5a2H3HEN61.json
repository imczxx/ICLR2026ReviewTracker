{"id": "5a2H3HEN61", "number": 21151, "cdate": 1758314321336, "mdate": 1759896939526, "content": {"title": "Semi-parametric language model with selective memory", "abstract": "Pretrained on trillions of tokens, LLMs are known for their ability to store a large amount of factual knowledge in their parametric memory. However, recalling facts from this memory is known to be unreliable, particularly for long-tail knowledge—obscure facts infrequently mentioned in training data. In this work, we propose a novel approach to improve the factuality of LLMs on long-tail knowledge. We begin by identifying atomic facts that are not present in a pretrained LLM's parametric memory. These facts are then stored in an external, non-parametric memory. Subsequently, the model undergoes continual pretraining, enabling it to learn when to consult this external memory at inference time. Compared with existing approaches, our approach uses a compact external memory that selectively stores only the facts not clearly present in the LLM's parametric memory, resulting in minimal additional inference-time costs in terms of both time and space. Furthermore, our method outperforms fully trained models of comparable size on knowledge-intensive benchmarks and achieves competitive results against larger models.", "tldr": "", "keywords": ["memory augmented pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e10b632ae6dc293fa879a911d117ca7bfa694544.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a semi-parametric language model (SPLM) that keeps common knowledge in model weights and offloads long-tail facts to a compact external memory. During continued pretraining, the model masks tokens for hard factual spans and learns to emit a special token to trigger retrieval. A lightweight query adapter is trained with a contrastive loss so the hidden state at the retrieval point can fetch the right memory entry. Experiments on open-domain QA benchmarks show modest gains on tail-heavy datasets compared to a similar-size baseline, and some ablations vary the masking threshold. However, the conceptual novelty over prior retrieval-augmented pretraining and LMLM-style designs seems limited, and several crucial details are underspecified."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation to avoid storing rare facts in parameters and to reduce unnecessary retrieval for common facts.\n2. Simple training recipe (selective masking + small adapter) that is easy to implement on top of standard continued pretraining.\n3. Compact external memory and selective retrieval reduce the runtime overhead compared to full RAG contexts."}, "weaknesses": {"value": "1. The approach is very close to existing retrieval-augmented pretraining and large-memory LM work; the paper does not articulate a distinctive technical contribution beyond selective masking.\n2. Evaluation is narrow (mostly short-form QA). There is little analysis of failure cases, retrieval precision/recall, or robustness to distractor memories.\n3. This method has not been shown to scale to larger LLM backbones, nor has it been demonstrated to be necessary for them. In general, larger language models tend to have fewer long-tail tokens, so addressing these long-tail distribution issues may bring little improvement for QA tasks."}, "questions": {"value": "1. Could the authors please explain how the memory used for retrieval is constructed? In other words, what is the complete set of negative samples used in the contrastive loss when training the adapter?\n2. Could the authors provide experimental evidence or theoretical justification showing that the proposed method can scale to larger models? Additionally, does this method affect the model’s normal instruction-following ability or its general reasoning capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pqvUkpWzZr", "forum": "5a2H3HEN61", "replyto": "5a2H3HEN61", "signatures": ["ICLR.cc/2026/Conference/Submission21151/Reviewer_1XW6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21151/Reviewer_1XW6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761064630338, "cdate": 1761064630338, "tmdate": 1762941508445, "mdate": 1762941508445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes “Semi-Parametric Language Models with Selective Memory” (SPLM), a method intended to improve long-tail factual recall without substantially increasing inference overhead or sacrificing general language understanding. The authors start from a partially trained LLaMA-based model and identify high-loss or low-frequency knowledge segments, which are masked out and stored in a compact external memory. Continued pretraining is then performed using an adaptive masking strategy that trains the model to rely on its internal weights for common knowledge while learning to query the external store for rare facts. A lightweight query adapter maps the model’s hidden state at special retrieval tokens to dense embeddings for nearest-neighbor search in the external memory. Experiments on several QA benchmarks suggest that the approach improves answer recall on long-tail datasets (POPQA, SIMPLEQA, tail subsets of Head-to-Tail) while maintaining or modestly improving performance on general benchmarks (Natural Questions, HotpotQA) compared to a standard LLaMA baseline, a memory-only offloading model, and an 8B-parameter LLaMA. The paper includes ablation studies on the choice of thresholds for masking and compares model-based versus corpus frequency proxies for detecting long-tail facts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Focused long-tail handling: Proposes masking high-loss or low-frequency facts during continued pre-training, offloading them to a compact external memory. This improves recall on rare facts while preserving general reasoning.\n * Efficient retrieval via lightweight adapter: The query adapter trained with contrastive loss produces contextual embeddings for retrieving the correct fact efficiently, helping to avoid unnecessary or erroneous lookups."}, "weaknesses": {"value": "* Incremental novelty: While the selective offloading of hard examples is effective, it builds closely on prior work. Selective memorization methods in continual learning (SeMem[A]) already use model loss to decide which samples to store in a non‐parametric memory, demonstrating scalability gains. Similarly, LMLM (Zhao et al., 2025) masks retrieved factual values during pretraining to enable lookup rather than memorization, and hierarchical memory approaches explicitly separate common vs. long‐tail knowledge. The core idea of masking hard knowledge segments to offload them is conceptually in line with these techniques. The paper needs a clearer positioning that distinguishes SPLM’s contribution from these existing approaches.\n\n* The paper proposes both frequency‐based and model‐based (loss or LLM popularity scoring) proxies for long‐tail knowledge. However, the use of cross‐entropy loss as an indicator of memorization difficulty echoes previous curriculum‐learning and selective memory strategies. It would strengthen the contribution to show that the adaptive combination of proxies (e.g., loss quantiles plus LLM‐prompted popularity scores) offers a measurable advantage over purely frequency‐based or purely loss‐based scoring.\n\n* Retrieval‐augmentation frameworks often fail to differentiate between queries needing long‐tail knowledge and those covering common knowledge. SPLM implicitly addresses this via the m_retrieve token, but the authors could expand on how the approach compares with systems that dynamically measure “long‐tailness” at inference time (e.g., by GECE metric [B]). \n\n * It is worth clarifying how the masking strategy affects learning of complex reasoning patterns that incorporate rare facts. The paper notes that masking too much text could impair reasoning, particularly on multi‐hop tasks. Discussing how SPLM might interact with other forms of reasoning benchmarks or tasks beyond factoid QA would situate the approach within a broader context of semi‐parametric models.\n\n * The authors need to have an ablation on their architectural choices and training strategies: For example: contrastive loss and query adapter\n\n\n\n* Important related works:\n\n  [A] Semiparametric Language Models Are Scalable Continual Learners (Peng et al., 2023)\n\n  [B] On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models (Li et al., 2024)\n\n  [C] Pretraining with hierarchical memories: separating long-tail and common knowledge (Pouransari et al., 2025) \n\n  [D] Adaptive Semiparametric Language Models (Yogatama et al., 2021) \n\n  [E] Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models (Pan et al., 2023)\n\n  [F] RET-LLM: Towards a General Read-Write Memory for Large Language Models, Modarressi et. al. 2024\n\n  [G] MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory, 2024"}, "questions": {"value": "* Minor: \n  * L39: Recent work has -> Recent works have\n  * L61: we masks -> we mask\n  * L151-2: The adapter thus  ... : the sentence needs to be corrected. \n\n * The submission looks to be in an incomplete state. Why APPENDIX has empty sections with titles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FLNCms5WzW", "forum": "5a2H3HEN61", "replyto": "5a2H3HEN61", "signatures": ["ICLR.cc/2026/Conference/Submission21151/Reviewer_uoqn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21151/Reviewer_uoqn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964274762, "cdate": 1761964274762, "tmdate": 1762941507442, "mdate": 1762941507442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hybrid method for continued pre-training of LLM where the model \"offloads\" additional 'atomic' facts into an external database. This database is then used at inference time to look up long tail facts.\n\nConcretely the authors show that on a 1B LLMs their results vs. clear baselines performs better. The baselines are strong and represent meaningful results, i.e. 1. \"vanilla\" continued pre-training, 2. Full Memory Networks, 3. pre-trained embedder. 4. their full method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Main strengths:\n1. An interesting approach to dynamically (and automatically) combine retrieval and inference in LLMs.\n2. Strong results that balance long-range reasoning tasks as well as general QA.\n3. Good discussion around ablations and methods used."}, "weaknesses": {"value": "Main weaknesses:\n1. Paper is incomplete. Appendix is missing. And Acknowledgement are the default text. The paper is clearly not finished.\n2. One of the key elements of the paper are completely missing: How does one create atomic facts for the training dataset. That is probably the key question that are not discussed in the paper at all.\n3. The model that is trained is limited to 1B model in the Llama family. Whereas nowadays other open source alternatives are much better suited: incl. Qwen (which has probably much better out of the box performance that would be interesting to compare to see if the result still holds)."}, "questions": {"value": "1. How do you create the atomic facts? That seems to be the hardest part of the setting?\n2. What are the exact reproducibility settings for your experiments?\n3. What are results on other, better more novel models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9U4g7Jei6P", "forum": "5a2H3HEN61", "replyto": "5a2H3HEN61", "signatures": ["ICLR.cc/2026/Conference/Submission21151/Reviewer_89gj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21151/Reviewer_89gj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164737979, "cdate": 1762164737979, "tmdate": 1762941505957, "mdate": 1762941505957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a semi-parametric language model framework called SPLM to improve factual recall of long-tail knowledge. The core idea is to selectively offload only the facts that a model struggles to memorize, which are identified using high loss signals during training. These facts are stored in an external non-parametric memory. The model is then continually pretrained using an adaptive masking technique that teaches it when to retrieve from this memory rather than generating text. A separate lightweight query adapter is finetuned to manage the retrieval process efficiently. The authors demonstrate that their method improves performance on several long-tail knowledge benchmarks against comparable baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's core concept of selective offloading based on model difficulty is intuitive. This presents a sensible alternative to standard RAG which retrieves indiscriminately or full memory models which offload everything.\n2. The two-stage training process, first teaching the model when to retrieve via masking and then how via the adapter, is a clean way to decouple the tasks. Using a frozen LLM backbone for adapter tuning is efficient.\n3. The experimental results convincingly show that SPLM outperforms the full memory baseline, especially in balancing general QA performance with long-tail recall. The improvements on benchmarks like PopQA and SimpleQA are notable."}, "weaknesses": {"value": "1. The empirical evaluation is limited. All experiments are conducted on a 1B model. It is unclear if these findings generalize to larger, more capable models where the parametric memory is much larger and the definition of \"long-tail\" might change.\n2. The paper criticizes standard RAG but surprisingly fails to include any RAG baseline in Table 1. Without this comparison, it is impossible to assess if the proposed complexity of selective memory and adapter finetuning is superior to simply applying a standard RAG method to the LLaMA3 1B model.\n3. The paper appears unfinished and lacks crucial details. The appendix is mentioned, but key sections like \"Additional Implementation Details\" (Appendix A) and \"Sample Generations\" (Appendix B)  are entirely missing from the document. Only a prompt (Appendix C) is included.\n4. This lack of detail is problematic. For example, the caption for Table 1 states an adaptive threshold of 0.6 was used , but the justification for this choice over others (explored in Section 3.5.3 ) is absent without the implementation details.\nMinor:\n1. Figure 1 on page 3 is too wide and clearly extends beyond the right page margin ."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SPo0mh5UL6", "forum": "5a2H3HEN61", "replyto": "5a2H3HEN61", "signatures": ["ICLR.cc/2026/Conference/Submission21151/Reviewer_rN9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21151/Reviewer_rN9y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21151/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260560625, "cdate": 1762260560625, "tmdate": 1762941504347, "mdate": 1762941504347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}