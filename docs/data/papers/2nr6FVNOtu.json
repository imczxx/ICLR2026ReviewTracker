{"id": "2nr6FVNOtu", "number": 4460, "cdate": 1757683644350, "mdate": 1762927809946, "content": {"title": "MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio Compression", "abstract": "Neural audio codecs have recently emerged as powerful tools for high-quality and low-bitrate audio compression, leveraging deep generative models to learn latent representations of audio signals. However, existing approaches either rely on a single quantizer that only processes speech tasks, or on multiple quantizers that are not well suited for downstream tasks. To address this issue, we propose MelCap, a high-fidelity neural codec with a single codebook. By decomposing audio reconstruction into two stages, our method preserves more acoustic details than previous single-codebook approaches, while achieving performance comparable to mainstream multi-codebook methods. In the first stage, audio is transformed into mel-spectrograms, which are compressed in the image domain and quantized into compact single tokens using a 2D tokenizer. A perceptual loss is further applied to mitigate the over-smoothing artifacts observed in spectrogram reconstruction. In the second stage, a Vocoder recovers waveforms from the mel discrete tokens in a single forward pass, enabling real-time decoding. Both objective and subjective evaluations demonstrate that MelCap achieves quality on comparable to state-of-the-art multi-codebook codecs, while retaining the computational simplicity of a single-codebook design, thereby providing an effective representation for downstream tasks.", "tldr": "", "keywords": ["Audio Codec", "Tokenizer", "Vocoder"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bd8d448cfa8e82bec6e2a2fbca76efe2bb931285.pdf", "supplementary_material": "/attachment/23d39948d36739d289a84fb61ad9f0eca46259c6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MelCap, a two-stage neural audio codec that compresses log-mel spectrograms into a single codebook using a 2D tokenizer, followed by a GAN-based vocoder. A perceptual VGG loss is added to improve mel reconstruction. Experiments include objective metrics, a MUSHRA-style listening test, and a small downstream comparison based on reconstructed waveforms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The two-stage, single-codebook formulation is simple and practically appealing.\n* Objective metrics appear competitive against several recent codecs.\n* Perceptual losses in stage 1 are ablated and seem beneficial.\n* The overall training pipeline is conceptually clear and could be attractive for downstream modeling."}, "weaknesses": {"value": "* The paper does not provide the actual bitrate of MelCap. Token rate, number of codebooks, or relative code lengths are not proxies for bitrate. Without bitrate normalization, the comparisons are not meaningfully interpretable, and claims of competitiveness are not supported.\nFigure 4 plots performance against “number of codebooks”. Different codecs operate at different tokenization rates, and some baselines do not even have a consistent frame/token rate across codebooks. The perceptual results should be plotted against bitrate, not number of codebooks. Additionally no confidence intervals are shown and the score of the reference signal is missing.\n\n* To evaluate usefulness for modeling, one expects experiments directly on the discrete tokens (e.g., token-based ASR/TTS, audio LMs). The paper evaluates waveform reconstructions using pretrained classifiers, which mostly measures vocoder fidelity, not the utility of the tokens for downstream modeling tasks.\n\n* The method claims to use a previously proposed tokenizer architecture but offers very few implementation details. Critical parameters (depth, channel counts, quantizer settings, receptive field, training schedules) are missing. Several important discriminator and vocoder design details are also unspecified.\n\n* The theory lacks proper rigour: The section starts by comparing discrete tokens c with mel-spectrograms s, which is problematic. It then continues to show that the vocoder f is Lipschitz-continuous, but f operates on a discrete domain. With proper definitions and analysis one could fix the theorem, but the result is also not surprising as the theory is only used to motivate the use of snake activations (which is already commonplace) and spectral normalization. Ablations for both decisions are missing.\n\n* Stage 1 trains a mel decoder, but stage 2 fine-tunes that decoder together with the vocoder. It is unclear:\n    * why the decoder isn’t trained directly on token distributions from the start,\n    * whether fine-tuning is required,\n    * what happens if the decoder is frozen.\nNo ablations compare these options.\n* The paper devotes substantial space to preliminaries (mel definitions, metrics) while providing only superficial descriptions of core components, leaving readers unable to understand where improvements come from.\n* The abstract claims real-time decoding, but the paper does not report real-time factors, latency, memory consumption, or comparisons against baselines on the same hardware."}, "questions": {"value": "* What bitrate (kbps) does MelCap operate at for each reported operating point?\n* Can you evaluate token usefulness directly rather than via reconstructed waveforms?\n* Can you provide full architecture tables (layers, channels, strides, codebook sizes, quantization details)?\n* Can you support the real-time claim using RTF and latency data?\n* Can you formalize the theoretical section: define domains, norms, and mappings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gPM3hNOUHz", "forum": "2nr6FVNOtu", "replyto": "2nr6FVNOtu", "signatures": ["ICLR.cc/2026/Conference/Submission4460/Reviewer_amqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4460/Reviewer_amqW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766824287, "cdate": 1761766824287, "tmdate": 1762917379147, "mdate": 1762917379147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "CSlHTTAuqz", "forum": "2nr6FVNOtu", "replyto": "2nr6FVNOtu", "signatures": ["ICLR.cc/2026/Conference/Submission4460/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4460/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927808901, "cdate": 1762927808901, "tmdate": 1762927808901, "mdate": 1762927808901, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MelCap, a single-codebook neural audio codec designed to handle diverse audio sources. MelCap adopts a two-stage training paradigm. In the first stage, a single codebook is trained on Mel-spectrogram reconstruction using a perceptual loss in addition to Mel loss to mitigate reconstruction artifacts. In the second stage, a vocoder is trained to recover waveforms conditioned on the discretized Mel tokens. Objective and subjective evaluations show that MelCap achieves performance comparable to multi-codebook codecs."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The motivation to unify compression for general audio within a single-codebook framework is reasonable and timely."}, "weaknesses": {"value": "1. The two-stage training paradigm is not novel. Similar approaches have already been proposed in prior work [1] for vocoder-based audio reconstruction. \n2. The experimental setup is unconvincing. MelCap is trained specifically on the AudioSet dataset, whereas the pretrained baselines were trained on different sources. Furthermore, the comparison is made across models with different token rates, making the results not directly comparable. The authors should reproduce at least some baselines under consistent conditions and present results at matched token rates. \n3. The proposed VGG perceptual loss does not consistently improve performance. In Table 1, MAE, Mel Distance, and VISQOL actually worsen when the VGG loss is applied.\n4. The error bound claim for discrete codes is confusing and lacks rigor. \n5. The paper suffers from numerous editorial and presentation issues, including mismatched notation (e.g., line 248), incomplete figure given the captions (e.g., Figure 5), incorrect inline citations, and multiple grammar and formatting problems.\n\n[1] Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder, Gârbacea et al., 2019"}, "questions": {"value": "1. In Section 4.1, the authors state that UTMOS and V/UV F1 metrics are unsuitable for evaluation. Why are these metrics still reported in the results? \n2. In the proof of the error bound, why does the decoder function $f$ takes a single code as input? To recover the full waveform $f$ should not be a univariate function, and the bound assumption on the quantization error is not reasonable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zW5p9HTv2B", "forum": "2nr6FVNOtu", "replyto": "2nr6FVNOtu", "signatures": ["ICLR.cc/2026/Conference/Submission4460/Reviewer_XUpc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4460/Reviewer_XUpc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902583507, "cdate": 1761902583507, "tmdate": 1762917378368, "mdate": 1762917378368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper present a 2d Mel tokenization-based neural audio codec with 2 stage training from Mel-spectrogram reconstruction to waveform generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper introduces a 2D tokenizer for a neural audio codec. While the idea of compressing audio in a manner similar to image compression is interesting, the paper does not demonstrate the effectiveness of the proposed method. It would be beneficial to include an ablation study using 1D tokenization within the proposed two-stage training framework, and to compare the performance with respect to tokens per second."}, "weaknesses": {"value": "My primary concern lies in the lack of novelty and limited evaluation.\n\n[Effectiveness of 2d Tokenization]\n\nThe paper does not verify the effectiveness of the proposed 2D audio tokenization. It is difficult to understand the rationale for adopting 2D tokenization in a neural audio codec, apart from frequency compression. For neural audio codecs, it is essential to demonstrate the model’s effectiveness in downstream generative tasks, such as audio generation or text-to-speech synthesis.\n\nHowever, the paper only conducts sound event classification experiments, and the baselines used for comparison are highly questionable. Moreover, the experimental details are insufficiently described.\n\n[Model]\n\nI cannot find any clear novel contribution in the model design, including the loss functions used for Mel-spectrogram reconstruction or waveform reconstruction via GANs.\n\n[Evaluation]\n\nIt is very difficult to assess the model’s performance due to limited evaluation and the use of inappropriate metrics.\n\nTo avoid confusion, it would be better to separate the test set into audio, speech, and music subsets.\n\nAdditionally, UTMOS is a metric specifically designed for speech quality evaluation. Please refrain from overusing this metric, and apply it only to the speech subset. I also urge the authors to clarify why this metric is used for your model. Do you truly understand the goal of your task and your evaluation metrics?\n\n[Audio Quality]\n\nThe audio quality presented in the demo page and in Figure 4 is poor.\n\nFurthermore, the number of test samples is too small to allow any meaningful generalization of the evaluation.\n\n[Downstream Generative Task]\n\nAs stated in line 94, the paper claims the potential for high-sampling-rate audio generation. However, the paper does not describe how to decode 2D tokens for downstream generative tasks.\n\nCurrently, many works explore multi-token prediction for RVQ token reconstruction, but this paper provides no explanation or method for 2D token prediction.\n\nIn summary, the only contribution appears to be the introduction of 2D tokenization for neural audio codecs. However, the current manuscript fails to demonstrate its effectiveness and lacks sufficient empirical validation."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LYniaOQ0YT", "forum": "2nr6FVNOtu", "replyto": "2nr6FVNOtu", "signatures": ["ICLR.cc/2026/Conference/Submission4460/Reviewer_pUMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4460/Reviewer_pUMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008154744, "cdate": 1762008154744, "tmdate": 1762917377863, "mdate": 1762917377863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of neural audio coding and introduces MelCap, a high-fidelity codec that utilizes a single codebook. MelCap consists of two distinct stages. In the first stage, a mel-spectrogram is extracted from the audio signal and quantized using a 2D tokenizer. In the second stage, a neural vocoder is trained to reconstruct the phase and convert the representation back into a time-domain signal. Both objective and subjective evaluations indicate that MelCap achieves quality comparable to state-of-the-art multi-codebook codecs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed method appears relatively simple and straightforward.\n2.\tThe reported results indicate that the method outperforms the evaluated baselines.\n3.\tThe authors provide some theoretical justification to motivate their model."}, "weaknesses": {"value": "1.\tThe novelty and overall contribution of the proposed method appear limited relative to prior work.\n2.\tThe experimental results are not fully convincing. In certain cases, there are inconsistencies between the reported figures in this manuscript and those presented in the original papers (e.g., the WavTokenizer subjective evaluation results).\n3.\tSome design choices in the proposed approach, such as the use of a 2D tokenizer, appear unconventional and unexplained from a signal processing perspective.\n4.\tThe paper’s presentation requires improvement — several figures are not properly referenced, and citations are inconsistently or incorrectly applied."}, "questions": {"value": "1.\tThe authors propose using a 2D tokenizer over the mel-spectrogram, effectively treating it as a raw image. However, since each column of the mel-spectrogram represents the frequency spectrum of a specific time window (and is therefore highly correlated, often containing harmonics and overtones), it is unclear why a 2D tokenizer was chosen instead of the standard 1D tokenizer commonly used in speech processing.\n2.\tThe paper lacks a discussion of limitations. In particular, latency and lookahead time—especially given the mel-spectrogram configuration—are not addressed. The authors should include a discussion on this aspect.\n3.\tThe description of the second stage of the proposed method is unclear. Are the decoder and vocoder optimized jointly? If so, is the optimization based solely on the vocoder loss, or does it also include a mel-spectrogram reconstruction loss? This should be clarified, as the explanation appears to rely primarily on Figure 3.\n4.\tEquation (4) is introduced as part of the proposed method, yet the first experiment demonstrates that it does not provide any improvement. The authors should clarify why it was included as a core component of the approach.\n5.\tCitations are inconsistently applied, and several figures are not referenced within the text.\n6.\tThe reported MUSHRA scores differ substantially from those presented in the WavTokenizer paper. The manuscript reports scores around 20–25, whereas the original paper reports scores above 90. While subjective evaluations may vary, such a large discrepancy raises concerns. The authors should verify whether the correct models were used and whether they were implemented appropriately.\n7. Sample page does not work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FXyYEJHCXS", "forum": "2nr6FVNOtu", "replyto": "2nr6FVNOtu", "signatures": ["ICLR.cc/2026/Conference/Submission4460/Reviewer_GD5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4460/Reviewer_GD5i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151498573, "cdate": 1762151498573, "tmdate": 1762917377634, "mdate": 1762917377634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}