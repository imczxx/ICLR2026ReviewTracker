{"id": "uSMttGgyuZ", "number": 6798, "cdate": 1757996176586, "mdate": 1759897892963, "content": {"title": "Selective Deferred Routing: Enabling Cost-Efficient Collaboration between Local SLMs and Remote LLMs", "abstract": "The rapid advancement of large language models (LLMs) has led to remarkable performance across diverse domains such as question answering, creative writing, programming, etc., making them indispensable assistants in daily life and work. Currently, LLM services are primarily accessed in two ways: (i) paid access to cloud-hosted LLMs, which are powerful but introduce nontrivial cost; and (ii) deployment of small language models (SLMs) on personal devices or small clusters, which, while less powerful, are sufficient for handling relatively simple tasks. To achieve a balanced trade-off between monetary cost and task performance, we propose Selective Deferred Routing, a paradigm that enables cost-efficient collaboration between local SLMs and remote LLMs. In this framework, a user request is first processed by the local SLM, which not only generates a preliminary response but also provides rich semantic representations of the request. A lightweight decider module then leverages this information to either adopt the initial response or route the request in a single step to the most suitable remote LLM for a higher-quality response. Extensive experiments on 5 LLMs and 3 datasets demonstrate that our approach consistently outperforms existing multi-LLM collaboration methods across different cost–performance trade-off preferences.", "tldr": "To achieve high performance with limited monetary cost with collaboration of local SLMs and remote LLMs.", "keywords": ["LLM Routing", "Cost-Efficiency", "Local-Remote Collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53c64af1a280cc8a3b45cf0c65c03f4c04ee44ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Selective Deferred Routing (SDR), a cost-efficient paradigm that optimizes collaboration between local small language models (SLMs) and remote large language models (LLMs). The approach leverages local models to generate preliminary responses and utilizes a decider module to route requests to the most suitable remote LLM for enhanced output. The paper demonstrates that SDR consistently outperforms existing multi-LLM collaboration methods by improving the cost–performance trade-offs across a variety of tasks and datasets, with extensive experimental evaluations involving five LLMs and three datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of combining local SLMs and remote LLMs in a selective and cost-efficient manner is novel. It addresses the challenge of balancing monetary cost and task performance, a significant concern in practical applications of LLMs.\n\n2. The proposed method is well-structured, with clear descriptions of the decider module and the associated scoring model, backed by a strong theoretical foundation. The experiments demonstrate the effectiveness of SDR, showing consistent improvements over baseline methods in the single-remote and multi-remote scenarios.\n\n3. The paper is generally well-written, with clear explanations of the methodology, theoretical formulations, and experimental setups. The figures, such as the cost-performance curves and AUC metrics, effectively illustrate the improvements achieved by SDR\n\n4. The approach can significantly reduce the operational costs of LLMs by optimizing the trade-off between local and remote model performance. The flexibility of the method for various user preferences makes it highly applicable in real-world systems that deploy LLMs."}, "weaknesses": {"value": "1. While SDR offers an interesting method for cost optimization, it shares conceptual similarities with existing research, particularly Immediate Routing and Model Cascades. The paper could have further highlighted how SDR uniquely overcomes the limitations of these existing methods, such as the sequential inefficiency in Model Cascades or the oversimplification of Immediate Routing.\n\n2. The experimental settings section (pages 8–9) provides a good overview of model configurations, but the hyperparameter tuning details, especially for the decider module and thresholds, are vague. Without a detailed explanation of how these parameters were optimized, it's unclear how much the performance gains are attributable to fine-tuning versus the intrinsic qualities of SDR. Also, it would be beneficial to provide open-source code to ensure full reproducibility.\n\n3. Although SDR performs well in multi-remote scenarios, the scalability of the model with multiple remote LLMs might pose practical issues, particularly regarding memory and computation. The parallel running of multiple scoring models during deployment, though lightweight, may still lead to increased computational overhead in large-scale systems."}, "questions": {"value": "1. The paper discusses the use of a threshold parameter (α) for routing decisions (Equations 2 and 3). Can you elaborate more on how these thresholds are dynamically adjusted in real-world use cases, and whether the model adapts to changing task characteristics over time?\n\n2. The experiments focus on single-task scenarios, but many real-world applications of LLMs require multi-task performance. How well does the Selective Deferred Routing approach perform when tasks with vastly different characteristics (e.g., creative writing vs. technical problem-solving) are combined?\n\n3. The paper mentions cost-effective collaboration, but what happens in scenarios with limited cloud resources or heavy latency on the edge devices? Is SDR robust enough to handle extremely resource-constrained environments, and how does it compare to other edge-cloud hybrid models?\n\n4.  Given the lightweight nature of the decider module, what are the potential latency concerns when scaling SDR to a production environment with high-frequency queries? Are there any optimizations or architectural changes that can further reduce response time in such scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9lQtPKt2Yi", "forum": "uSMttGgyuZ", "replyto": "uSMttGgyuZ", "signatures": ["ICLR.cc/2026/Conference/Submission6798/Reviewer_iSCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6798/Reviewer_iSCG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895241787, "cdate": 1761895241787, "tmdate": 1762919069792, "mdate": 1762919069792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Selective Deferred Routing (SDR), a two-stage collaboration scheme: a local SLM first answers and emits hidden-state features; a lightweight “decider” then either accepts the local answer or routes once to a remote LLM, aiming to optimize a cost–performance tradeoff. The core contribution is to cast binary selective routing as an AUC-style objective over a cost–performance curve, leading to a ranking-consistent training loss (via a Binary-Gap surrogate) for the decider initialized from a single SLM transformer layer. Experiments over 3 datasets and 5 LLMs show favorable tradeoffs against routing/cascade baselines, plus a simple rule to extend to multi-remote settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear formalization of selective routing with an AUC objective that directly targets cost–performance trade-offs; the gap-ordering optimality condition is crisp and intuitive.\n\n- Lightweight decider design that reuses a single Transformer layer from the local SLM; practical and latency-friendly.\n\n- Empirical results cover five LLMs / three datasets, reporting normalized AUC curves (single-remote) and actual USD costs (multi-remote)."}, "weaknesses": {"value": "- Latency measurements and on-device memory/compute overheads for running the decider in parallel with the SLM are discussed qualitatively, but not benchmarked. \n\n- Label generation cost is under-specified: training the decider requires evaluating both local and remote outputs per query to estimate gaps/BG labels; the offline token cost could rival the savings at deployment time, but is not quantified. \n\n- Baselines omit some recent routers/cascades in the multi-LLM literature beyond those cited; fairness of API defaults (temperatures, system prompts) across providers is not detailed. \n\n- Generalization across local models is unclear: the decider is initialized from a specific SLM layer; portability to different SLMs or quantization variants is not evaluated."}, "questions": {"value": "- What is the total offline cost (tokens × price) to collect BG labels per dataset and per remote LLM, and after how many live queries does SDR break even?\n\n- How sensitive is SDR to changing the local SLM (e.g., different size/architecture, quantization levels)? Can the decider trained on one SLM transfer without re-labeling? \n\n- For the multi-remote case, why is averaging the scores the right aggregation? Have you compared to learned policies (e.g., budget-constrained bandits or cost-aware ranking) or provided a regret bound?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "B388WmZ7qe", "forum": "uSMttGgyuZ", "replyto": "uSMttGgyuZ", "signatures": ["ICLR.cc/2026/Conference/Submission6798/Reviewer_kLrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6798/Reviewer_kLrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099505964, "cdate": 1762099505964, "tmdate": 1762919069257, "mdate": 1762919069257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Selective Deferred Routing, a paradigm that enables cost-efficient collaboration between local SLMs and remote LLMs. In this framework, a user request is first processed by the local SLM, which not only generates a preliminary response but also provides rich semantic representations of the request. A lightweight decider module then leverages this information to either adopt the initial response or route the request in a single step to the most suitable remote LLM for a higher-quality response."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Multi-LLM Routing is important and on time.\n\n2. Selective Deferred Routing balances cost and accuracy.\n\n3. Extensive experiments on 5 LLMs and 3 datasets are provided."}, "weaknesses": {"value": "1. The latency is not reported. Judge after SLM finishes generation can take very long. After SLM generates a few tokens, it might be sufficient to start routing.\n\n2. Fine tuning a BERT before SLM and LLMs may have lower latency and cost.\n\n3. In GSM8K Figure 4, the performance is close to FrugalGPT. Code is also not provided."}, "questions": {"value": "In GSM8K Figure 4, why using Llama-4-Maverick as the LLM? How about GPT and Deepseek?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r74htWv0HA", "forum": "uSMttGgyuZ", "replyto": "uSMttGgyuZ", "signatures": ["ICLR.cc/2026/Conference/Submission6798/Reviewer_cQJE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6798/Reviewer_cQJE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153223874, "cdate": 1762153223874, "tmdate": 1762919068544, "mdate": 1762919068544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}