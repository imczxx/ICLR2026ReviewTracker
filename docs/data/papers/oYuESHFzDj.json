{"id": "oYuESHFzDj", "number": 16688, "cdate": 1758267672016, "mdate": 1759897224798, "content": {"title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages", "abstract": "Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages with 3.9M samples and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench’s alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. \\textsc{MuBench} provides flexible evaluation formats, including mixed-language testing. Experimental results show that increasing model size does not improve its ability to handle mixed-language contexts. We recruited human experts to evaluate translation quality and cultural sensitivity for 34k samples across 17 languages, and combined these assessments with an LLM-as-a-Judge approach to ensure overall data quality in low resource languages.", "tldr": "", "keywords": ["Multilingual Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88c2382e4b1011ce97c6832d9e65e7c7d77e9163.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors create MuBench benchmark, covering 61 languages by translating existing datasets using their data collection pipeline with quality checks and perform human checks for 17 languages. They further evaluate LLMs on this benchmark. They perform cross-lingual consistency evaluation for consistent cross-lingual evaluation and analysis of knowledge transfer. They evaluate model performance under code switched contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Paper is well written and is easy to follow through.\n- The authors covered a huge number of datasets and translated them to 61 languages which had high, medium, low-resource languages.\n- Experimental setup is clearly explained and results are followed up by human evaluation.\n- MuBench data collection pipeline looks thorough and has a lot of checks.\n- Cross lingual consistency evaluation and creating code switched dataset to see performance on code switched data are great contributions"}, "weaknesses": {"value": "- I was a bit skeptical from the beginning about the translation quality but the fact that it was human-evaluated in 17 languages was reassuring. However, when I checked those 17 languages, most of them are either medium or high resource languages (61 languages in total out of which highest numbers are in low-resource languages (26)). This is a serious flaw in their paper. Ideally they should have picked an equal number of languages from high, medium, low resource for human evaluation. Existing LLMs don’t do well for low resource languages and I believe this is where the major gap is. I’d recommend authors to perform human evaluation for at least 8-10 low resource languages. This paper has got some amazing contributions and I’m willing to bump up the scores but low resource languages should be human evaluated to ensure correctness of their approach. \n- COMET doesn’t support more than 50 languages and they mention explicitly on their website to evaluate at your own risk for languages not mentioned.\n- The authors missed out on defining what they do in the “problematic samples check” step.\n- Using GPT for classification is an overkill I believe. The authors should have used more efficient approaches for this like a classifier.\n- Existing datasets like MMLU have some problems [1], how did authors get rid of those samples? I didn't see any table in the paper about this.\n\n\n[1] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile Van Krieken, and Pasquale Minervini. 2025. Are We Done with MMLU?. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5069–5096, Albuquerque, New Mexico. Association for Computational Linguistics."}, "questions": {"value": "- I’m sorry if I have skipped this but do authors share samples which are culturally sensitive?\n- I don’t understand Section 3.3 partially. When you look at both models’ top choices, how do you ensure they are the same? Let’s say one language is Chinese, other is Spanish, how do they ensure answer “England” (for Spanish it would be “Inglaterra” and Chinese would be “英格兰”) are same?\n- Typos: \n  - Line 182: Translation\n  - Line 280: Gemma2(?)\n  - Line 307: Model(¿20B)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aGi8F92IiP", "forum": "oYuESHFzDj", "replyto": "oYuESHFzDj", "signatures": ["ICLR.cc/2026/Conference/Submission16688/Reviewer_ZqrM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16688/Reviewer_ZqrM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760796755326, "cdate": 1760796755326, "tmdate": 1762926743022, "mdate": 1762926743022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces MuBench, a large-scale multilingual benchmark covering 61 languages and 3.9 million samples across a diverse range of tasks, including natural language understanding, factual knowledge, knowledge-based question answering, academic reasoning, and truthfulness. The evaluation framework examines three major aspects: overall performance, cross-lingual consistency (measured by the newly proposed MLC metric), and robustness to mixed-language inputs. The resulting experiments provide a more holistic understanding of multilingualism in large language models, offering valuable insights to guide future research and improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- MuBench is broad in scope, spanning numerous languages, tasks, and samples.\n- The dataset construction pipeline is carefully designed, incorporating checks for semantic consistency, translation purity, and cultural sensitivity. The authors further validate the reliability of the translations through expert evaluation on 34K samples and overlap verification with 100 samples from MMMLU.\n- The experiments are conducted to evaluate the multilingual capabilities of various LLMs, revealing how cross-lingual consistency, and mixed-language contexts differ per-language performance.\n- Overall, building such a large-scale benchmark and conducting these extensive evaluations represent a significant and commendable effort that will likely benefit the research community."}, "weaknesses": {"value": "- The tasks in MuBench are mostly binary and multiple-choice formats, overlooking other important multilingual capabilities such as translation, summarization, and instruction following. This restricts the benchmark's overall applicability and impact.\n- Some interpretive statements lack explicit numerical evidence. For instance, claims such as \"Babel and Sailor2 demonstrate notable gains in their targeted language groups\" or \"smaller models often benefit from the presence of English in mixed-language inputs\" would be stronger with accompanying statistical summaries or quantified comparisons (e.g., averaged improvements).\n- Presenting the related work as a standalone section, instead of embedding it within lines 40–55 as a paragraph of the Introduction, would more clearly highlight the work's novelty.\n\n**Minor Issues:**\n- Typo: \"Traslation\" should be \"Translation\" (line 181).\n- Missing period at the end of the sentence (line 242, after \"in Appendix A.6\").\n- Citation error for Gemma2 (line 280).\n- Table 1: \"SC samples\" should be corrected to \"CS samples.\"\n- Some indicators, and axis labels in figures are too small."}, "questions": {"value": "- Why is Rel-MLC defined as MLC divided by mean accuracy? Since this normalization causes more accurate models to exhibit smaller Rel-MLC values, it may explain the apparent contradiction between MMLU and GPQA performance and the corresponding Rel-MLC values of GPT-4o (lines 387–399).\n- How do LLMs respond to mixed-language inputs in terms of output language composition? Analyzing the languages used in outputs could shed light on why smaller models outperform their monolingual baselines, whereas larger models show the opposite trend."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "chOZ1LkKZ7", "forum": "oYuESHFzDj", "replyto": "oYuESHFzDj", "signatures": ["ICLR.cc/2026/Conference/Submission16688/Reviewer_65Zp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16688/Reviewer_65Zp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761047730842, "cdate": 1761047730842, "tmdate": 1762926742384, "mdate": 1762926742384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new multilingual LLM benchmark that is created by taking the existing popular English benchmarks and machine-translating them into 61 selected languages. The authors try to do automatic quality checks of the translation quality as well as a manual evaluation of 17 languages with native speakers of those languages. Using the final dataset, they then evaluate a large number of open-weight multilingual language models, showing that many of them perform much worse in other languages than English."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark can evaluate language models on more than 60 languages, which can be very useful for those language communities -- as long as the translation is accurate and makes sense (more on that below)\n- The authors double-check the quality of their translation pipeline with a manual inspection that involved native speakers of 17 languages.\n- Each sample in the benchmark is annotated with the topic and sub-topic category, which might be very useful metadata for the future use of this dataset.\n- There are many similar projects that take existing English datasets and naively translate them into many languages, without properly checking the translation quality. However, this work devises a multi-stage pipeline for automatically checking the translation quality."}, "weaknesses": {"value": "As a native speaker of a lower-resource language, I find the machine-translated \"multilingual\" benchmarks somewhat troubling.\n\nFirst of all, translationese is a problem even with human translation and much more with machine translation. You end up with a very specific unnatural variant of each languages that relies on English-like linguistic constructions and that might omit language features not present in English. As a result, such benchmarks give overly optimistic scores to English-centric language models that otherwise fail on properly created benchmarks made by native speakers. Thus, benchmarks like this can sometime do more harm than good.\n\nMy second point is more subjective; I would argue that cultural and local knowledge should be an inherent part of language evaluation -- does one really know Icelandic without recognizing a single Icelandic dish and not understanding any Icelandic cultural references? On the other hand, what I appreciate about this paper is that it takes this into account and tries to at least remove all cultural samples -- this is already much better than other similar papers that blindly translate mostly US-based questions. But it results in a dataset devoid of any local knowledge, which I believe only evaluates a certain aspect of multilinguality.\n\n____\n\n**Other weakness**:\n- The translation from English is performed by GPT-4o, but the same model is then used to check the translation quality, which might leave many errors unchecked. It would be better to use different model(s) for the quality checks. \n- Another related troubling thing is that the performance of GPT-4o is substantially lower on some languages compared to English (Figure 3). Since the same model was used to translat the questions from English, it indicates that the translation for those language is very poor. I would assume that the performance of GPT-4o would be consistent across languages if its translation is correct.\n- You say that *\"we chose the 61 most widely spoken languages based on the number of native speakers\"* (lines 105--106), which is not true. For example, Hausa (with 58 million speakers) or Bhojpuri (with 53 million speakers) are not included even though Icelandic (0.3 million speakers) is included.\n- Translation of some of the tasks, those that rely more on specific language features, can be problematic. For example, if you translate the WinoGrande example \"My shampoo did not lather easily on my Afro hair because the _ is too dirty. (answer: shampoo / hair)\" into a language like Czech (where \"shampoo\" and \"hair\" are of different grammatical genders), the sample loses any ambiguity and thus it no longer evaluates the same thing. I wonder how much is the observed performance drop across many tasks in Table 5 connected to such issues. For example, losing more than 30 percentage points on translated HellaSwag (81.5 -> 49.4) but only 7 on MNLI (88.0 -> 80.4) is slightly concerning."}, "questions": {"value": "- One thing that I didn't understand is why the cross-lingual alignment is such an important feature for a multilingual benchmark? From my point of view, the three related works listed in the introduction -- CMMLU, ArabicMMLU and INCLUDE -- are much more useful for evaluating multilinguality as they also localize the benchmarks. But you say (line 47) that the great benefit of your benchmark is cross-lingual alignment, so why is it so important? Cannot we evaluate consistency across languages without this alignment?\n- As far as I can tell, the 17 evaluated languages are fairly high-resource, wouldn't it be more interesting to more closely check the translation quality of the lower-resource tail of languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q2pCqbwb8R", "forum": "oYuESHFzDj", "replyto": "oYuESHFzDj", "signatures": ["ICLR.cc/2026/Conference/Submission16688/Reviewer_s7NU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16688/Reviewer_s7NU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924867687, "cdate": 1761924867687, "tmdate": 1762926741651, "mdate": 1762926741651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MuBench, a multilingual benchmark for evaluating LLMs across 61 languages covering tasks such as NLU, commonsense reasoning, factual recall, QA, and truthfulness. It emphasizes cross-lingual alignment, cultural sensitivity checks, and proposes a Multilingual Consistency (MLC) metric. The benchmark includes code-switched and multi-format (local, English-template, cloze, and mixed) variants. The authors evaluate several open and proprietary models, finding persistent performance gaps between English and low-resource languages and little improvement from model scaling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Ambitious scope and coverage: 61 languages and multiple task categories represent an impressive effort toward comprehensive multilingual evaluation.\n\n- Detailed translation pipeline: The multi-stage quality control with semantic, purity, and cultural sensitivity checks is well-structured and thorough.\n\n- Cross-lingual alignment and code-switching evaluation: Enables new analyses not possible with existing benchmarks.\n\n- Transparency and openness: Dataset availability on Hugging Face improves reproducibility and potential reuse.\n\n- Empirical findings: Highlights real and relevant disparities between English and non-English model performance."}, "weaknesses": {"value": "- Limited novelty: The contribution is primarily engineering and dataset aggregation, not a clear conceptual or methodological innovation beyond existing multilingual benchmarks (e.g., MMLU, BenchMAX, INCLUDE).\n\n- Benchmark saturation – Given numerous existing multilingual datasets, the incremental improvement offered by MuBench does not clearly justify publication in a top-tier venue like ICLR.\n\n- Evaluation analysis lacks depth:  insight into causes or linguistic patterns, error analysis, and detailed methodological justifications are mostly missing or superficial.\n\n- Repetition and length: The paper reads as overly descriptive and dataset-heavy, lacking theoretical framing or hypothesis-driven evaluation."}, "questions": {"value": "as suggestion: since the paper focuses on cross-lingual alignment, the authors may also see this recent paper as well on the same topic: https://aclanthology.org/2025.findings-acl.1385/"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a2Ety3Hv9d", "forum": "oYuESHFzDj", "replyto": "oYuESHFzDj", "signatures": ["ICLR.cc/2026/Conference/Submission16688/Reviewer_7SaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16688/Reviewer_7SaQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999124696, "cdate": 1761999124696, "tmdate": 1762926741223, "mdate": 1762926741223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}