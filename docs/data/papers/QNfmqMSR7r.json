{"id": "QNfmqMSR7r", "number": 17812, "cdate": 1758280840441, "mdate": 1759897152161, "content": {"title": "Spinning Straw into Gold: Relabeling LLM Agent Trajectories in Hindsight for Successful Demonstrations", "abstract": "Large language model agents operate in partially observable, long-horizon settings where obtaining supervision remains a major bottleneck. We address this by leveraging a source of supervision overlooked in existing post-training methods: ``unintended yet successful'' goals embedded within agent rollouts. We introduce Hindsight Supervised Learning (HSL), where an auxiliary LLM reviews each completed trajectory and relabels it with natural-language goals the agent actually achieved. HSL then pairs the trajectory with its relabeled goals and uses these pairs for additional fine-tuning. To mitigate suboptimality in the relabeled data, HSL incorporates irrelevant-action masking and sample reweighting. We show that HSL is flexible and compatible with existing post-training pipelines. It improves both SFT and DPO, with larger gains on long-horizon embodied and web agent tasks such as ALFWorld and WebShop. Moreover, HSL is sample-efficient: on ALFWorld, it surpasses baselines trained on the full dataset while using only one quarter of the ground-truth demonstrations.", "tldr": "We propose a sample-efficient post-training method for LLM agents that turns their trajectories into successful demonstrations the agents use to learn and improve.", "keywords": ["hindsight learning", "agentic LLM", "LLM", "post training", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f28e59d60fadd2cdb74a8edab32130334a56a7ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a sample-efficient learning method that reuses agent trajectories via hindsight experience relabeling. The methods are benchmarked in embodied navigation tasks with good empirical performances. Theoretical performance bounds are provided for the proposed method as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is easy to follow, and the introduced algorithms are simple to add on to existing models, making them widely usable. The baselines are chosen well and extensively, while the studies on sample efficiency, ablations, and analysis are also designed well. Empirical performances are promising for the proposed method. (especially section on ensuring the performance gain doesn't only come from using a more powerful LLM)"}, "weaknesses": {"value": "- None of the main results have any statistical significance or uncertain qualification. The tasks from ALFWorld and WebShop should ideally be reran multiple times for every algorithm in order to at least get some standard deviation in score or success rate.\n- The novelty of this work is rather limited, as hindsight experience relabeling is a widely used technique across robot learning and reinforcement learning."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CMYvdwakmf", "forum": "QNfmqMSR7r", "replyto": "QNfmqMSR7r", "signatures": ["ICLR.cc/2026/Conference/Submission17812/Reviewer_8bBB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17812/Reviewer_8bBB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805610728, "cdate": 1761805610728, "tmdate": 1762927655538, "mdate": 1762927655538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Hindsight Supervised Learning (HSL) for (text-only) LLM agents in closed-world environments (i.e., with a structured schema of available goals): in addition to training the policy on expert demonstrations, train on on-policy rollouts labeled on the fly by an auxiliary, highly capable LLM, which relabels rollouts with all goals actually achieved, as judged by the auxiliary LLM. Relabeled trajectories are kept in a FIFO queue that is continually updated, and enter training alongside pre-existing expert demonstrations. Theoretical results provide some motivation for their approach; experiments on ALFWorld and WebShop show meaningful gains on the former and smaller gains on the latter. The paper does not establish results for open-world goal spaces."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Practical, simple idea that composes known ingredients (hindsight relabeling + SFT) into a new, tidy, reusable pipeline for language agents.\n\nClear empirical signal on ALFWorld; sample-efficiency curves indicate strong wins at lower demo budgets. Notably, on ALFWorld Unseen the method reaches ≈92.5% with 800 demos vs ≈82.8% for DPO with >3,200, indicating real demo-efficiency.\n\nImplementation details (masking + reweighting + on-policy refresh) are intuitive and ablations suggest each adds value.\n\nTheory is tidy (imitation-style discrepancy bound) and aligns with the empirical recipe—even if mostly motivational."}, "weaknesses": {"value": "The paper has potential to be a nice contribution but has serious methodological shortcomings in its current presentation:\n\n- No variance / seeds (critical). Table results and curves lack seeds, error bars, or confidence intervals. Delta could be within variance. A single cherry-picked seed is an absolute non-starter to get the paper accepted. (Addressing this point alone would change my score from 2 to 4; see questions below).\n\n- Code / reproducibility (critical). Prompts are provided, but there’s no released code or relabeled dataset for third-party checking (and no determinism details). Uploading assets as a ZIP to OpenReview (or to some CDN that is anonymized) for inspection is a totally reasonable academic practice (especially if there are manually labeled results on the paper) that the authors should be expected to satisfy.\n\n- Cost/efficiency underreported (important). The paper states fine-tuning wall-clock but does not clarify whether this includes the compute cost for continual relabeling with a large model, nor report the latter separately. Without this, “data efficiency” is unclear relative to real cost.\n\n- Label-quality evidence is thin (important). The relabeler is validated with a very small human spot-check of only 50 trajectories.\n\n- Heavy dependency on a powerful annotator (important). Relabeling uses a ~70B model for a 1B agent. There’s no sensitivity to smaller relabelers; conclusions may depend on annotator strength.\n\n- The framing of \"almost unsupervised\" could be misread (important). The method requires ground-truth demonstrations and mixes them in every training step. It does not replace demos; the current evidence shows materially improved use of demos, certainly not demo-free learning. A modification of language here could be in order. The paper also does not compare clearly the compute cost of their method vs that of their baseline (i.e. using expert demonstrations only); not discussing trade-offs between / scaling laws of compute and dataset size limits the reach of their conclusions.\n\n- Ablation confounds in RELABELFAILURE (preferable). Because that variant relabels only failed trajectories, it gets less data as the base improves. This confounds “algorithm” with “data volume.”\n\n- Theory–practice gap (preferable). The bound depends on coverage/optimality constants that are not measured nor bounded experimentally; masking/reweighting are argued heuristically to help them, but this link isn’t empirically probed.\n\nThese evidence gaps drive my Soundness=1 and overall score. My take is that this paper is a valuable practical contribution with clear empirical signal on the right kind of tasks, but acceptance should be contingent on addressing the issues above; most notably, statistical significance. These are fixable and would substantially strengthen the paper."}, "questions": {"value": "I found this paper interesting. Below are some questions/requests that I think that, if addressed, would substantially improve the quality of the paper.\n\n- Seeds/variance: How many seeds per result? Seems only one. Please report mean±std (≥5) for Table 1 and all curves.\n\n- Transparency: Will you release code, relabeling prompts, and the relabeled data?\n\n- Relabeler sensitivity & cost: How does performance change with 3B/8B/70B relabelers? Report relabeling tokens, wall-clock, and total GPU hours per setting.\n\n- Label validation at scale: can you label a substantially larger set of trajectories (e.g. 200) and/or add automatic predicate checks (where supported) to strengthen the statistical significance, and report confusion matrices per goal type?\n\n- Ablation deconfounding: Please re-run a variant of RELABELFAILURE that (i) relabels all trajectories (successes included) and (ii) fixes the relabeled-sample budget across variants to disentangle algorithmic effect from data volume.\n\n- Theory diagnostics: Any proxy measurements over training for coverage (e.g., diversity/goal-type coverage of hindsight set) and hindsight-expert quality (environment predicate agreement) to support the claimed mechanisms?\n\nthank you!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Mt9MRz0TNq", "forum": "QNfmqMSR7r", "replyto": "QNfmqMSR7r", "signatures": ["ICLR.cc/2026/Conference/Submission17812/Reviewer_jNmR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17812/Reviewer_jNmR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859177715, "cdate": 1761859177715, "tmdate": 1762927654902, "mdate": 1762927654902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce an approach Hindsight supervised learning which trains an LLM agent on relabeled data. A relabeler LLM is used to relabel the LLM agent's trajectories with the goals that the LLM actually achieved. The authors provide results showing that by doing so, the agent is able to outperform baseline approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very easy to read and provides a thorough theoretical and experimental analysis. The results are strong and very promising."}, "weaknesses": {"value": "The relabeler’s output space is manually constrained with environment-specific templates. It would be helpful to discuss how HSL would scale to domains without predefined goals or how to automatically infer such goal spaces.\n\nThe method assumes that any achieved goal is beneficial to learn from but in some situations unintended achievements may not align with user intent or task utility. It would be useful to discuss ways to filter or weight relabeled goals based on relevance to user-defined objectives.\n\nIt would be useful to discuss the distinction between successful but unintended goals and partial progress toward intended goals. The method seems to treat both similarly, but their learning value may differ."}, "questions": {"value": "Could you please discuss the goal space and how this might be defined in environments that don't have clearly predefined goals\n\nCould you discuss how incorrect labels might dealt with and how they impact downstream performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9FGCih79J7", "forum": "QNfmqMSR7r", "replyto": "QNfmqMSR7r", "signatures": ["ICLR.cc/2026/Conference/Submission17812/Reviewer_MGUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17812/Reviewer_MGUs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957962758, "cdate": 1761957962758, "tmdate": 1762927654162, "mdate": 1762927654162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hindsight Supervised Learning (HSL), a post-training method for LLM agents. It leverages the unintended but successful goals achieved during an agent’s rollouts by relabeling trajectories with goals that were actually accomplished. These relabeled examples are then used for fine-tuning, with two key techniques: irrelevant-action masking and demonstration reweighting. Experiments on ALFWorld and WebShop benchmarks show HSL improves both supervised fine-tuning and DPO, achieving higher success rates with fewer expert demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides somewhat strong empirical validation, showing consistent performance gains and higher sample efficiency across multiple benchmarks.\n- There is a theoretical property offered although it is difficult to parse due to presentation issues. I still appreciate the attempt to include a theoretical result, which is rare to see in papers with large foundation models."}, "weaknesses": {"value": "Below are my comments in the order of appearance in the paper.\n\n1- In addition to hindsight relabeling, the paper should also discuss learning from play data in robotics. In that line of research, it is common to define goals in play data and label the trajectories with those goals.\n2- Around line 177-178, I was confused by why the paper uses both a goal state and a language instruction. I understand that it is desirable to have the function $\\delta$ because it will be useful for detecting the agent reaching other goals, but then, maybe goal state is all the paper needs? Why also have an instruction? Having both seems redundant from an RL perspective.\n3- In line 195, I believe the statement $K \\leq T$ implicitly assumes the goals are mutually exclusive so the agent cannot achieve multiple goals at the same time. Perhaps this should be clarified earlier, because in reality an agent can achieve both \"picking up a fruit\" and \"picking up a banana\" at the same time, for example (or \"closing the drawer\" and \"putting the mug in the drawer\", etc.)\n4- \"improve agent optimality\" is a weird phrase. If something is optimal, it cannot be improved by definition.\n5- There is a system prompt that describes the space of valid goals. So the paper needs a definition for each goal. Perhaps something like \"this set of states achieves this goal\". But if it is a mapping, there would be no need for an LLM. So I assume, what is needed is a natural language description of each goal. This should be part of the problem statement.\n6- In the theoretical analysis section, there is this variable $h$ in Equation 1, but it is not defined anywhere. This makes it very difficult to follow the theory. I assume it is history. But then, how can $\\tau_{t-1}$ for different values of $t$ can be equal to $h$? The former has varying length depending on $t$ whereas the latter is fixed. Does this mean the probability term inside the summation is nonzero only for a single $t$? If so, writing it as a summation is unnecessarily complex. Perhaps this is not the case, and there is something I miss about $h$. Since it is not defined, I do not know what it can be.\n7- The conclusion section should discuss when (for what tasks) this method can and cannot be applied."}, "questions": {"value": "Please see my comments and clarification questions in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3vbaKPmTAG", "forum": "QNfmqMSR7r", "replyto": "QNfmqMSR7r", "signatures": ["ICLR.cc/2026/Conference/Submission17812/Reviewer_PFk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17812/Reviewer_PFk9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982822672, "cdate": 1761982822672, "tmdate": 1762927653422, "mdate": 1762927653422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}