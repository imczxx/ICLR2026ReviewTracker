{"id": "Xs7DhA88bd", "number": 18224, "cdate": 1758285360898, "mdate": 1759897118142, "content": {"title": "Synergistic Absorption-Diffusion: Dual-branch Enhanced Continuous-Time Modeling for Parallel Token Generation", "abstract": "Recent advancements in diffusion models, such as global optimization and parallel token prediction, have enhanced global consistency compared to autoregressive Transformers. However, existing diffusion models exhibit unfavorable trade-offs between efficiency and quality, in which the multi-step iterative denoising processes particularly incur high computational costs. To address these issues, we propose a dual-branch synergistic absorption diffusion model. For efficiency-quality trade-offs, we design a dual-branch architecture, in which the Transformer branch generates local token chunks, and the diffusion branch optimizes global token blocks in fewer steps. To resolve the instability of discrete-time models, we further introduce the continuous-time diffusion process, which enhances parallel token generation and learning representations. Experiments conducted on multiple tasks, including text generation and structural reasoning tasks, demonstrate the state-of-the-art performance of the proposed model.", "tldr": "", "keywords": ["Diffusion Language Models", "Text Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04746fc30b522b88daffc58d181eafd45640810e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a dual-branch continuous-time discrete diffusion model (CAD-DF) combining a Transformer autoregressive (AR) branch with a diffusion denoising branch. The goal is to achieve both global consistency (from diffusion) and local coherence (from AR) for parallel token generation. The method introduces a cross-attention fusion mechanism and mutual reinforcement between branches, along with denoising cross-entropy (t-DCE, λ-DCE) losses for stable training. Results show improved perplexity, accuracy, and inference speed on several datasets compared to GPT-2 and recent diffusion-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptually elegant integration of AR and diffusion paradigms.\n2. Continuous-time formulation improves efficiency and stability.\n3. Strong experimental results across both small and medium model scales.\n4. Clear ablation studies showing the effect of each component.\n5. Well-motivated and theoretically consistent with prior works."}, "weaknesses": {"value": "1. Presentation is heavy and could be simplified for accessibility.\n2. Theoretical explanation of mutual reinforcement could be supported with empirical visualization (e.g., information flow or convergence curves).\n3. Missing analysis on failure or degradation cases."}, "questions": {"value": "1. How does the dual-branch mechanism behave when scaling to very large models—do benefits persist or saturate?\n2. Can the cross-attention fusion be replaced or approximated for efficiency at scale?\n3. How sensitive are results to the λ/t-DCE hyperparameters?\n4. Are there any stability trade-offs in the continuous-time inference regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gH61Lg3IPO", "forum": "Xs7DhA88bd", "replyto": "Xs7DhA88bd", "signatures": ["ICLR.cc/2026/Conference/Submission18224/Reviewer_xdKK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18224/Reviewer_xdKK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756191972, "cdate": 1761756191972, "tmdate": 1762927964550, "mdate": 1762927964550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dual-branch synergistic absorption diffusion model that integrates a Transformer branch and a diffusion branch via cross-attention to address the efficiency-quality trade-off in parallel token generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The dual-branch structure that synergistically integrates AR and diffusion paradigms via cross-attention is novel and well-motivated.   \n- The model demonstrates state-of-the-art performance on quality metrics (Perplexity, Accuracy) across diverse text generation and structural reasoning tasks."}, "weaknesses": {"value": "- The model is said to effectively address the efficiency-quality trade-off, but Table 4 shows the full model has lower throughput than the model \"w/o Transformer Branch\".  \n- The novelty of the paper is not well described; some key concepts like time-independent reparameterization and DCE losses are adopted directly from prior work, but they are not well distinguished.\n- Presentation quality is poor.\n  - Figure 1 shows U-Net diffuser, but there is no explanation.\n  - There are no explicit references to any tables in the main text. \n  - The citation style can be improved."}, "questions": {"value": "- What do the authors mean by saying effectly addressing the efficiency-quality trade-offs? \n- What exactly is the contributions/novelties of the approach compared to existing models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C5rsGgFsvQ", "forum": "Xs7DhA88bd", "replyto": "Xs7DhA88bd", "signatures": ["ICLR.cc/2026/Conference/Submission18224/Reviewer_LQRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18224/Reviewer_LQRF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876280336, "cdate": 1761876280336, "tmdate": 1762927964081, "mdate": 1762927964081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dual-branch synergistic architecture for parallel token generation in language models, termed the Synergistic Absorbing Diffusion (CAD-DF) framework. It integrates a Transformer-based autoregressive branch with a continuous-time discrete diffusion branch, coordinating them via cross-attention fusion and mutual reinforcement mechanisms. The approach is designed to address longstanding tradeoffs between generation efficiency and quality, leveraging analytical time-independent objectives and absorbing state diffusion for more robust and scalable sequence modeling. Experiments on six benchmark datasets across language modeling and structural reasoning tasks reportedly demonstrate superior performance in perplexity, accuracy, and inference efficiency compared to both autoregressive and diffusion-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and active research problem—how to achieve both efficient and high-quality parallel token generation—by explicitly combining the strengths of autoregressive and diffusion models. The dual-branch framework is presented as a meaningful improvement over simple mixtures or sequential composition.\n2. Extensive experiments on diverse, standard benchmarks (WikiText2, WikiText103, C4, FineWeb, Prolong, JSON-Mode-Eval) show substantial improvements. Table 1 and Table 2 (Pages 8) detail strong gains in both perplexity and accuracy compared to competitive baselines, including GPT-2, D3PM, SEDD, PLAID, and the RADD series.\n3. The efficiency analysis in Table 3 (Page 9) demonstrates notable improvements in throughput and memory, while Table 4 (Page 9) provides ablations showing each major architectural component’s impact."}, "weaknesses": {"value": "1. Lack of Baselines: While the experimental section is broad, there is no evidence of direct empirical comparison against block diffusion, ParaTAA, or speculative sampling frameworks, whose aims and underlying architectures are similar or overlapping with CAD-DF. The absence of these comparisons in Tables 1 and 2 limits the credibility of the “state-of-the-art” claims.\n2. Mathematical Details and Notational Ambiguity in Cross-Attention Fusion: The description of the cross-attention fusion mechanism (see Equation, Page 6) lacks formal definitions for input/output dimensionalities, masking strategies, and how [M]-masked positions interact during fusion. Additionally, it is not entirely clear how (or whether) stability holds when merging outputs from two distinct temporal streams, especially if their distributions are not inherently aligned. This raises questions about the reliability of the mutual reinforcement during training.\n3. Limited Ablation Depth and Lack of Qualitative Error Analyses: While Table 4 (Page 9) gives a simple ablation, there is no exploration of nuanced failure cases, qualitative generations, or breakdowns across context length and sequence complexity. For a method purporting to resolve global-local consistency, analyses showing (or failing) on long-range sequence dependencies or structural errors would be instructive."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3E3CU7NrCf", "forum": "Xs7DhA88bd", "replyto": "Xs7DhA88bd", "signatures": ["ICLR.cc/2026/Conference/Submission18224/Reviewer_JyFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18224/Reviewer_JyFM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923894927, "cdate": 1761923894927, "tmdate": 1762927963714, "mdate": 1762927963714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Synergistic Absorbing Diffusion, a dual-branch architecture combining a continuous-time diffusion branch (for global distribution modeling) and a Transformer branch (for local conditional modeling), integrated via cross-attention. This design improves parallel generation, consistency, and efficiency, achieving state-of-the-art results on several NLP tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Synergistic Absorbing Diffusion model not only enhances parallel generation capability and global consistency but also reduces the number of denoising steps and inference latency. This is of significant importance for efficient processing in practical application scenarios.\n\n2. The authors conducted comprehensive experiments covering various natural language processing tasks, such as text generation and structured reasoning. The experimental results indicate that the Synergistic Absorbing Diffusion model achieves strong performance on these tasks."}, "weaknesses": {"value": "1. When scaling up to medium-sized models, the method's advantage in Prolong PPL does not appear to be significant. It is uncertain whether it can be effectively scaled up to modern Large Language Models (LLMs).\n\n2. The model has not been evaluated on downstream tasks."}, "questions": {"value": "NA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GLFBuL9Nzk", "forum": "Xs7DhA88bd", "replyto": "Xs7DhA88bd", "signatures": ["ICLR.cc/2026/Conference/Submission18224/Reviewer_DcmT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18224/Reviewer_DcmT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003053199, "cdate": 1762003053199, "tmdate": 1762927963292, "mdate": 1762927963292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}