{"id": "MfPDdPUGKi", "number": 10590, "cdate": 1758176739508, "mdate": 1759897641878, "content": {"title": "Unlocking Long-Horizon Agentic Search with Large-Scale End-to-End RL", "abstract": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling knowledge-intensive tasks using external tools. One representative example is search agent. Existing open-source search agents heavily rely on advanced commercial LLMs: they either collect trajectories from the larger, stronger models for supervised fine-tuning or directly use them as specialized tools. In this work, we develop ASearcher, a single-model search agent purely trained by reinforcement learning (RL) without using any commercial APIs for data or tools. Based on an RL-trained QwQ-32B model, ASearcher is capable of conducting complex reasoning, such as uncertainty analysis and conflict verification, and achieve comparable performances to commercial search agents. There are two key techniques to unlock such long-horizon information-seeking abilities: first, we design a two-staged agentic process to synthesize high-quality QA pairs as the training data for RL; second, we conduct large-scale long-horizon RL, allowing the agent to take up to 128 actions per rollout for sufficient exploration. In particular, after RL training, ASearcher achieved scores of GAIA 58.1, xBench 51.1, and Frames 74.5 using only basic search tools. Furthermore, ASearcher also demonstrates strong zero-shot transferability: ASearcher can be further augmented with an additional summary tool, which is supported by DeepSeek-V3, and test-time scaling, which aggregates the answer from 16 parallel rollouts. With both zero-shot enhancements, the performances of ASearcher further rise to 71.8, 75.0, and 83.4, respectively, outperforming OpenAI DeepResearch and Kimi-Researcher, suggesting the great potential of RL scaling for agentic tasks. We release all the code and data at an anonymous link. The model will be released after the review process.", "tldr": "This paper introduces training expert-level search agents with large-scale RL. By using 128 turn limit and high-quality synthetic data, the agent learns complex long-horizon search strategies, reaching competitive results on frontier benchmarks.", "keywords": ["Agentic RL; Asynchronous RL; Search Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c906714cbfb33d8d16094f1ac49c5eca57314e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ASearch, a single-model search agent purely trained by RL without using commercial APIs for data or tools. Firstly, a data synthesis agent is used to apply question injection and fuzzing to open-source datasets, increasing the complexity of training data, followed by three steps of quality verification. Secondly, a two-stage curriculum RL training is employed. The paper further accelerates the training process by asynchronizing both trajectory rollout and training. Experiments show that ASearch can achieve better performance compared to many strong baselines by using only an open-source 32B model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "ASearch can achieve excellent results without relying on API model-synthesized trajectories, and the training process is fully RL without cold-start. The proposed asynchronous rollout and training strategy can effectively enhance training efficiency, enabling the model to make more tool calls."}, "weaknesses": {"value": "1. There are several crucial details in the method that are unclear. For example: 1) How is the uniqueness of answers determined during the quality verification process? How does the model know if an answer is unique without executing an action? 2) How are the answers for the synthesized new questions determined? 3) How is question injection and fuzzing implemented, and what prompts are used?\n\n2. The scalability of the method in this paper is questionable. Although the paper does not rely on commercial API models, QwQ-32B already possesses sufficient reasoning capabilities compared with most open-source models. If replaced with models at the 7B or 14B level, can the pipeline in this paper successfully boost the model's performance? Furthermore, the data synthesis in this paper relies on existing open-source datasets. The community is more concerned about how to construct a large number of high-quality and complex query-answer pairs without relying on open-source data.\n\n3. There are risks regarding the fairness of the baseline comparisons in Table 1. For example, do the various baselines use the same training data? Additionally, this paper allows a maximum of 128 tool calls, far more than other baselines. Does this setting itself bring significant performance gains?\n\n4. Furthermore, the significant performance improvement achieved through zero RL in this paper may be related to QwQ being a reasoning model. Would switching to a non-reasoning model imply the need for a cold start?\n\n5. Most of the analytical experiments in this paper are case studies and lack strong persuasiveness."}, "questions": {"value": "1. Many prompts used in this paper have not been disclosed, such as the prompts for question injection and fuzzing, quality verification, LLM-as-judge, and others.\n\n2. How would it impact if all baselines were allowed a maximum of 128 rounds of tool calls?\n\n3. How would the effects be if other baselines used Summary or TTS? How is TTS specifically implemented - based on self-consistency or pass@K?\n\n4. The reasons behind this paper's ability to surpass baselines are not very clear. Why can using a 32B model to synthesize QA and training with zero-RL achieve better results than other baselines using stronger models to synthesize trajectories with a cold start + RL approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r5idQo8GAi", "forum": "MfPDdPUGKi", "replyto": "MfPDdPUGKi", "signatures": ["ICLR.cc/2026/Conference/Submission10590/Reviewer_JUpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10590/Reviewer_JUpp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760895127939, "cdate": 1760895127939, "tmdate": 1762921858793, "mdate": 1762921858793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an LLM-based search agent named ASearcher, which is built from an open-source model (QwQ-32B). It is trained purely using end-to-end Reinforcement Learning (RL). Unlike many existing approaches, this work avoids any reliance on commercial LLM APIs for generating supervised fine-tuning data or for serving as specialized sub-modules. The paper provides a novel data synthesis agent along with a large-scale, long-horizon RL training framework. The proposed ASearcher agent achieves state-of-the-art performance among 32B-scale open-source agents on benchmarks like GAIA, xBench, and Frames."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In general, the paper is well written with good structure and is easy to follow.\n\n2. In this paper, the authors are trying to tackle an important and challenging task. Training agents that perform deep, long-horizon web search is timely and practically useful. The paper frames these challenges very well and clearly.\n\n3. The proposed method demonstrates that a single open-model (QwQ-32B) trained purely with RL, which has no commercial LLMs for data or submodules, can learn sophisticated multi-turn search strategies is a meaningful contribution to the RL-for-agents literature. The result challenges the current trend of heavily relying on multi-model/commercial LLM pipelines.\n\n4. The injection plus fuzzing loop, combined with verification checks, is a practical method to produce hard, tool-use-requiring QA pairs. The authors also provide useful statistics and examples in the appendix."}, "weaknesses": {"value": "1. It would be better if more ablation studies could be provided. The paper presents two major contributions: the novel QA synthesis pipeline and the long-horizon asynchronous RL system. However, their relative importance is not disentangled.\n\n2. The data synthesis agent uses external sources to support facts and then rejects questions solvable without tools. But more detail is expected: what external sources were used (Wikipedia only?), how is answer uniqueness verified, and what mechanisms prevent the synthetic QA from leaking evaluation set contents or being too close to benchmark questions?\n\n3. The proposed system relies on a very large language model (QwQ-32B) and extensive GPU hours (16k H800 GPU-hours), which is far more expensive than simply using commercial APIs for data generation or inference.\n\n4. I wonder if the proposed method can perform well on the other common base models."}, "questions": {"value": "Please check the issues and questions mentioned in the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8WItLDsilV", "forum": "MfPDdPUGKi", "replyto": "MfPDdPUGKi", "signatures": ["ICLR.cc/2026/Conference/Submission10590/Reviewer_yr7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10590/Reviewer_yr7v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940070244, "cdate": 1761940070244, "tmdate": 1762921858262, "mdate": 1762921858262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ASearcher, a single-model search agent trained with large-scale reinforcement learning , demonstrating long-horizon reasoning and competitive performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is generally well-organized.\n2. The paper involves a substantial amount of engineering work, including large-scale RL training, data synthesis. \n3. Demonstrates impressive long-horizon reasoning performance."}, "weaknesses": {"value": "1. While the paper emphasizes end-to-end RL for long-horizon search, similar ideas have already been explored by Search-R1, WebThinker, websailor, and Chain-of-Agents, which also use RL or multi-agent training to enhance tool use. The main difference here fully asynchronous RL is more of a systems optimization than a conceptual breakthrough in agent reasoning or policy learning. While technically well-executed, the contribution is rather straightforward and does not introduce substantial new insights\n\n2.  The reported 16k H800 GPU hours make the approach extremely resource-intensive and difficult to reproduce, especially for academic or smaller research groups, which greatly limits its practical contribution and accessibility to the broader community.\n\n3. The paper’s contribution, scope, and novelty are quite unclear. It reads more like a well-executed technical report or open-source project than a research paper with distinct conceptual advances. The work mainly combines and scales up ideas that are already known in the community. Moreover, the performance gains might largely reflect the inherent capability of QwQ-32B and the heavy use of synthetic QA generation and data filtering, rather than the RL algorithm itself. \n\n4. It is also unclear what training setup was used for the baselines—did they use the same data, or were they retrained? If the goal is to compare RL algorithms, the training data and environment should be kept identical; if the goal is to emphasize data synthesis or agentic setup, experiments with smaller models would help validate that claim. \n\n5. In addition, I did not see any ablation studies analyzing the contribution of each component (e.g., data quality, RL updates), which makes it difficult to attribute where the actual improvements come from. \n\n6. The entire pipeline feels more like a collection of separately optimized components rather than a truly “end-to-end” system"}, "questions": {"value": "Please refer to the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hU6Pk7nMbl", "forum": "MfPDdPUGKi", "replyto": "MfPDdPUGKi", "signatures": ["ICLR.cc/2026/Conference/Submission10590/Reviewer_HJhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10590/Reviewer_HJhQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988837409, "cdate": 1761988837409, "tmdate": 1762921857859, "mdate": 1762921857859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a strong training method where the performance gain comes mostly from an improved tool-use ability of an open-sourced model. ASearcher is able to process long trajectories and the work has developed efficient computing infra to leverage the computing resources. The performance of ASearcher is compared comprehensively on prevailing benchmark against a comprehensive set of baseline."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear presentation and writing\n- Comprehensive baselines \n- Convincing results\n- Effective and efficient approach"}, "weaknesses": {"value": "I don't have too much complaints about this work. The novelty of this work is moderate (in terms of using standard GRPO with fully async training) while I acknowledge the experimental efforts. I would suggest to open-source everything including the data filtering and all the components in the training. I have one main concern tho:\n\nThere are some components in the introduced method such as Dynamic Filtering and data filtering etc, e.g.,\n```\n Finally, we filter out questions that are too hard for the modelor too easy for the model. Finally, from a total of 304k QA pairs, we retain 16k challenging samples\n```\nMaybe I missed, but I don't see ablation studies about them."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hopb0p8RlR", "forum": "MfPDdPUGKi", "replyto": "MfPDdPUGKi", "signatures": ["ICLR.cc/2026/Conference/Submission10590/Reviewer_1mWo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10590/Reviewer_1mWo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762843525649, "cdate": 1762843525649, "tmdate": 1762921857213, "mdate": 1762921857213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}