{"id": "DzZrQsdCbL", "number": 12277, "cdate": 1758206793872, "mdate": 1763743265895, "content": {"title": "MorphoGen: Evolving Robot Morphologies with Large Language Models", "abstract": "Designing high-performing robot morphologies is a grand challenge for developing specialized autonomous agents. However, the vast, combinatorial, and non-differentiable nature of the morphological design space has been a primary obstacle. Existing methods tackle this problem indirectly, relying on either semantically-blind genetic operators or reinforcement learning with predefined modification actions, both of which constrain exploration. In this work, we introduce MorphoGen, a novel framework that reframes morphological design as a code generation problem. MorphoGen leverages large language models (LLMs) to directly iterate the XML files as codes that define an agent’s morphology, solving the original open problem without being limited by any prior constraints or fixed action spaces. Gradient-like textual guidance is provided to steer the evolution of robot morphologies through prompted mutations and crossovers. Our approach allows the LLMs to apply its understanding of structure and syntax to generate complex and semantically coherent design variations, enabling an unconstrained and efficient exploration of the design space. On a suite of challenging locomotion benchmarks, MorphoGen discovers novel and high-performing morphologies, significantly outperforming strong baselines by over 57.5% in downstream motoring evaluation. Our work unlocks a new paradigm for automated robotic design, demonstrating the effectiveness of LLMs in navigating complex, structured engineering search spaces. Codes for our work are released anonymously at https://anonymous.4open.science/r/MorphoGen-ACC/", "tldr": "", "keywords": ["Robot Morphology Optimization", "Large Language Models", "Evolutionary Algorithms"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1948748f7854eaf9b9651e4c80dcd64ebe997990.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a fundamental challenge in evolutionary Robotics, i.e., robot design automation. An LLM-based evolutionary strategy is proposed, which employs a critic LLM for textual guidance that facilitates more informative and comprehensive evolutionary search. A couple of other techniques, including structure pretraining, two-stage proxy fitness evaluation, and a hybrid sampling strategy, are also proposed that yield additional performance gains. Extensive experiments on simulated locomotion tasks validate the superiority of the proposed method over traditional evolutionary algorithms and RL-based design algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is well written and easy to follow. The abundant illustrations and qualitative results greatly help with the reader’s understanding. The analysis in Section 5.3 is particularly interesting, revealing the correspondence between evolved robots and natural priors. \n\n2.The paper tackles a long-lasting challenge in Robotics, and proposes an effective approach that leverages the semantic priors of large language models to aid more efficient search. The algorithmic designs are intuitive and straightforward, yet proved to be competitive experimentally. \n\n3.The authors explicitly consider low-compute scenarios where only a minor fine-tuning budget is allowed, and confirm that the proposed approach is still competitive, adding to its practicality."}, "weaknesses": {"value": "1.The baseline algorithms seem dated. As numerous LLM-based evolutionary strategies have been proposed in recent years (with some of them listed below), the authors are suggested to compare against one or two of them, or at least clarify the relationships with them. The absence of such analysis leaves the paper’s novelty largely unclear. \n\n- Qiu K, Pałucki W, Ciebiera K, et al. Robomorph: Evolving robot morphology using large language models[J]. arXiv preprint arXiv:2407.08626, 2024.\n\n- Song J, Yang Y, Xiao H, et al. Laser: Towards diversified and generalizable robot design with large language models[C]//The Thirteenth International Conference on Learning Representations. 2025.\n\n- Yang C, Wang X, Lu Y, et al. Large language models as optimizers[C]//The Twelfth International Conference on Learning Representations. 2023.\n\n2.The comprehensive sampling strategy introduced in Section 4.1 seem to be a key component, and the increase of morphological diversity is claimed throughout the paper. However, the paper lacks any quantitative measurement and comparison of diversity, with only a couple of examples given in Figure 6. \n\n3.As structure pretraining turns out critical in ablation studies, it becomes questionable regarding how much the performance gains over baseline algorithms are merely due to the additional expert knowledge rather than the algorithm itself."}, "questions": {"value": "1.Beside the fitness of evolved robots, could the authors describe the influence of structure pretraining on the evolutionary dynamics? More specifically, to what extent does the algorithm stick to the structures given in the initial population, and how much does it discover new, beneficial structures? \n\n2.Does the critic LLM also receive fitness scores as input? If not, could the inclusion of such information help the critic LLM better identify beneficial substructures against harmful or neutral ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gNaWLd36DE", "forum": "DzZrQsdCbL", "replyto": "DzZrQsdCbL", "signatures": ["ICLR.cc/2026/Conference/Submission12277/Reviewer_u5DK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12277/Reviewer_u5DK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557010640, "cdate": 1761557010640, "tmdate": 1762923212392, "mdate": 1762923212392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper deals with the problem of optimizing robot morphology to maximize the performance on locomotion tasks. In the past there were approaches which performed local changes on the morphology without full semantic understanding of the full robot. The authors compare their new approach against those methods.\n\nMain method:\nThe main method is based on using LLM to guide an evolutionary process by suggesting changes to robot morphology. After a change is performed the pool of robots is evaluated using RL-based policy training. As the framework of evolutionary process the AlphaEvolve approach is used - with its islands model to maximize fitness while maintaining diversity by keeping explicit robot sets (islands).\n\nThe approach is evaluated on locomotion tasks in the Mujoco simulator."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The main idea of using LLM as a mutation operator is definitely appealing (but unfortunately not new, see weaknesses).\n2. The paper is well presented.\n3. The problem itself is important and given the recent progress in LLMs one can expect significant results in the niche of robot morphology optimization."}, "weaknesses": {"value": "My main problem with the submission is that the main result follows the approach of Qiu et al.:\nRoboMorph: Evolving Robot Morphology using Large Language Models,\nhttps://arxiv.org/abs/2407.08626\nwhich was released on arxiv over a year ago. \n\nThere were even follow-up papers to RoboMorth, such as RoboMore, released in May this year:\nRoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward\nhttps://arxiv.org/abs/2506.00276\n\nIf we assume that the usage of LLM as a modifier operator on robot morphology was already known, then as far as I understand the main novelty of the paper in question is the specific evolutionary strategy based on AlphaEvolve."}, "questions": {"value": "1. How your approach differs from RoboMorph?\nhttps://arxiv.org/abs/2407.08626\n2. Do you have ablation studies showing the effects of the proxy fitness idea (page 5)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VF0RpnksHh", "forum": "DzZrQsdCbL", "replyto": "DzZrQsdCbL", "signatures": ["ICLR.cc/2026/Conference/Submission12277/Reviewer_LDgZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12277/Reviewer_LDgZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583283150, "cdate": 1761583283150, "tmdate": 1762923212034, "mdate": 1762923212034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach to use LLMs for generating and evolving XML files representing simulated agents (similar to HalfCheetah, Walker, Swimmer and other Mujoco tasks). The LLM is mainly used to evolve the XML file structures, while for control standard reinforcement learning in the form of PPO is used.\nThe paper compares the proposed method against a limited set of prior work using evolutionary-based or policy-based methods for evolving agent structures.\n\nOverall, I aprpeciate the paper for its interesting idea but it could be improved in terms of technical accuracy, theoretical insight and adding neccessary details ( see also below)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- I like the general idea of using LLMs for advancing the xml structure of agents\n- The proposed approach seems to outperform other graph-based methods using either evolutionary algorithms or a policy to manipulate agent graph structures\n- Experiments are conducted across 4 environments.\n- Visualizations are nice\n- The paper provides clear evidence that a critic LLM helps to improve performance"}, "weaknesses": {"value": "- The paper lacks clarity, formalism and technical accuracy at times. For example, it is not clear which LLM was used without looking at the provided code. However, the type and model of LLM has a clear impact on the overall performance of the system. No ablations on different LLM architectures are provided as well. This is not only an issue ofr reproducability, but also neglects the effect different LLMs could have on the final result.\n- The environments used were introduced in prior work. It stands to reason that these environments have been part of the general training data of the coding LLMs, as they can be found on Github. Hence, the generalizability of the paper is in question without further investigation (eg good results could be mere lookups from the training data).\n- I am not favourable of the formalism in section 3. Why do you differentiate between body parts B, joints J and actuators A? Why are the joints encoding DoF and not the combination of links and actuators? Generally, it reads like a mix between XML file structures and the graph definition from prior work like NGE. It is not clear how M_0 \\in M defined a robot - eg if you have three body parts, three joints and one actuator, how are the relationships between these objects modelled? I strongly recommend clarifying this section and looking at the problem definitions of prior work.\n- Using  (Yuan et al., 2022) as a reference for the PPO learning algorithm used on individual agents is a bit misleading. Please, reference the original PPO paper.\n- Appendix B introduces the standard framework of RL - however you have a contextual MDP process at hand. Furthermore, the transition function should be a mapping from S x A to S and not S-delta."}, "questions": {"value": "Please see the review and points raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Jeh1Zw5xxt", "forum": "DzZrQsdCbL", "replyto": "DzZrQsdCbL", "signatures": ["ICLR.cc/2026/Conference/Submission12277/Reviewer_msgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12277/Reviewer_msgE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019303061, "cdate": 1762019303061, "tmdate": 1762923211430, "mdate": 1762923211430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach that frames the problem of morphological design as that of code generation by using LLMs to directly iterate XML files that specify an agent’s morphology. The claim is that the method enables the exploration of complex designs. The solutions are validated empirically and shows superior performance to several competing approaches for designing morphologies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach seems quite novel, and aims to apply LLMs for designing morphology. The general approach seems valid and plausible. Overall, the method is also presented well."}, "weaknesses": {"value": "From the paper, I see that the inner loop is run for a fixed number of episodes. However, I suspect it would be fairer to consider a fixed number of steps instead, as episode lengths could possibly vary drastically for different designs.\nGiven the specification of morphology only via code, I wonder whether operations such as mutations and crossovers are necessary. For example, I wonder whether other gradient or non-gradient based methods could be used to modify the XML code. In general, there may be different ways of achieving elitism, diversity and randomness, and it is not clear why the choices made in this paper are necessarily superior to other alternatives."}, "questions": {"value": "1.\tHow exactly is the inner loop handled and what are the associated assumptions relating to the same?\n2.\tWhat motivates the use of evolution-inspired mechanisms such as mutation and crossover? In general, could other approaches such as Generative flow networks have been used?\n3.\tAre the LLMs specifically pretrained on XML-structure relationships?\n4.\tWhat guided the design choices pertaining to the criteria of Elitism, Diversity and Randomness as described in lines 203-212? Have other approaches for these been explored or considered?\n5.\tWhy is a fixed number of episodes (and not steps) considered for the fitness calculation? As episode lengths could vary drastically depending on the design and other factors, wouldn’t a fixed interaction budget be fairer?\n6.\tHow sensitive is the approach to the quality of initial genotypes? How would the performance vary if for instance, the initial designs were randomly generated instead of expert-influenced?\n7.\tDoes the fact that a diverse set of high quality expert design morphologies is needed skew the designs towards those that are very similar to expert-designed ones?\n8.\tIn addition to the designs, it would be interesting to see the gaits followed for each design either through videos/gifs or a sequence of images\n9.\tAre there any constraints applied to limb dimensions and other elements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oP65vZRCCb", "forum": "DzZrQsdCbL", "replyto": "DzZrQsdCbL", "signatures": ["ICLR.cc/2026/Conference/Submission12277/Reviewer_VyFH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12277/Reviewer_VyFH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128262772, "cdate": 1762128262772, "tmdate": 1762923211016, "mdate": 1762923211016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}