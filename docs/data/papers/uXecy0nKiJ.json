{"id": "uXecy0nKiJ", "number": 18168, "cdate": 1758284655673, "mdate": 1759897122077, "content": {"title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety", "abstract": "Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0\\% to 2–27\\%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4\\%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.", "tldr": "We demonstrate that benign activation steering systematically breaks model safety safeguards, enabling compliance with harmful requests.", "keywords": ["activation steering", "LLM safety", "mechanistic interpretability", "sparse autoencoder", "jailbreaking", "alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b4d45cf8ec170bbc6cf1c795fac64a640df89a0.pdf", "supplementary_material": "/attachment/4841abf8a55aa4ea38e0e3fb0fba1171a8e2aaa9.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates how activation steering, both through random directions and SAE-derived concept vectors, can compromise LLM safety mechanisms. The authors test steering across layers and magnitudes on JailbreakBench, showing that middle-layer steering often leads to higher compliance with harmful requests. They further demonstrate that certain SAE features can generalize into universal jailbreak vectors that work across prompts and models. Overall, the paper argues that interpretable steering methods, often assumed to enhance control, can inadvertently weaken safety alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novelty of motivation**: The paper identifies a previously underexplored safety concern that activation steering, including interpretable SAE-based methods, can unintentionally reduce model safety.\n2. **Practical relevance**: The authors demonstrate that SAE-derived features can act as jailbreak tools raises concrete concerns about how interpretability techniques may be weaponized.\n3. **Empirical insight**: The finding that middle layers are particularly vulnerable and that steering effects generalize across prompts adds useful understanding to where safety mechanisms fail."}, "weaknesses": {"value": "1. **Unclear conceptual grounding of steering vectors**: Although the paper motivates activation steering as conceptually meaningful (e.g., France concept in Figure 1), most experiments use Gaussian-random directions. This makes it difficult to interpret what the steering is actually doing beyond injecting noise into activations.\n2. **Narrow evaluation scope**: All results are based on JailbreakBench. Testing the same steering methods on other safety benchmarks (e.g., AdvBench[1] or other aspects of safety like privacy and toxicity) and on general capability tasks like MMLU or CSQA could help determine whether the steering trade-offs generalize beyond safety settings.\n3. **Weak causal interpretation**: It remains unclear whether steering compromises safety through meaningful feature activation or through general perturbation effects. A more careful causal or ablation-based analysis could clarify the mechanism.\n\n[1] Universal and Transferable Adversarial Attacks on Aligned Language Models, 2023"}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGCAjZS7aJ", "forum": "uXecy0nKiJ", "replyto": "uXecy0nKiJ", "signatures": ["ICLR.cc/2026/Conference/Submission18168/Reviewer_cP2g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18168/Reviewer_cP2g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527915559, "cdate": 1761527915559, "tmdate": 1762927919361, "mdate": 1762927919361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines how activation steering during inference can degrade LLM refusal behaviors. The authors experimented with models like Llama3, Qwen2.5, and Falcon across multiple layers and steering magnitudes, under settings of using random Gaussian vectors and SAE features from pre-trained public SAEs. They measure harmful compliance using LLM-as-judge on 100 harmful prompts. The main finding is that even random or benign steering can raise compliance rates up to 27%. Moreover, combining 20 randomly sampled steering vectors can create a universal attack."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **The framing and angle are novel.**\n- Prior work emphasized more purpose-built steering vectors, while this paper explores ordinary vectors. Also, the argument that activation steering can be risky is an interesting and underexplored angle.\n\n2. **Aggregating the vectors into a universal attack (Section 4.4) is a simple and neat experiment** that supports the authors’ claim and demonstrates a plausible, real-world threat."}, "weaknesses": {"value": "1. **Unsteered baseline results are not clear from the paper.**\n- In Figures 2, 3, 6, the comparisons begin at nonzero steering coefficients and do not include a no steering condition. And from the paper, it’s ambiguous whether the baselines that the authors refer to are actually measured through unsteered models. Without this it’s hard to interpret the reported changes caused by steering.\n\n2. **Compliance measurement (Section 3.4) is insufficient.**\n- The authors consider incoherent or repetitive outputs as safe, and this design choice can potentially lead to false negatives. Although some quality assessment is done (Appendix B),  this is only based on precision.\n- For reported compliance rates across figures, no uncertainty quantification such as confidence intervals is provided.\n\n3. **SAE feature selection process is not clear enough.**\n- The paper highlights a few high impact features like “brand identity” (Figure 4), but it’s unclear the total number of features tested, whether the reported features were cherry-picked, or how labels were assigned."}, "questions": {"value": "1. Out of the 668 SAE features that can jailbreak at least five prompts, is this distribution uniform or concentrated around specific semantic types, like emotion, identity, etc?\n\n2. What is your hypothesis on that semantically benign SAE concepts can jailbreak harmful prompts, for example, whether this is due to linear compositionality (ex. features accidentally aligning with refusal-suppression directions) or something else?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "38V9Af0GAT", "forum": "uXecy0nKiJ", "replyto": "uXecy0nKiJ", "signatures": ["ICLR.cc/2026/Conference/Submission18168/Reviewer_Yss7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18168/Reviewer_Yss7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780196655, "cdate": 1761780196655, "tmdate": 1762927918992, "mdate": 1762927918992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents evidence that steering LLM activations with random vectors and benign SAE features makes LLMs more susceptible to jailbreaking. It further explores combining jailbreak-inducing random vectors to create a \"universal\" attack vector."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a potentially important and underappreciated threat vector. A finding that a large majority of randomly chosen directions increase jailbreak susceptibility is surprising enough to be important, and jailbreak susceptibility induced by steering with benign SAE features has practical relevance."}, "weaknesses": {"value": "The claim made in the abstract and introduction that steering with benign SAE features is even more alignment-breaking that steering with random directions appears to be based on a single prompt and model, evaluated at a depth that's not optimal for random directions (Figure 2); the more comprehensive evaluation shown in Figure 3 shows precisely the opposite, with Llama3-8b-it compliance rates much higher with random vectors on every prompt category save one. This claim also appears misleadingly in the Conclusion, where random steering on one model is compared with SAE steering on another.\n\nThe judge model used is quite small, despite the fact that much better models could be used at very modest cost. The authors argue that manual labelings mostly agreed with the model; still, this is an unnecessary source of uncertainty added to critical evaluations.\n\nFigures lack confidence intervals.\n\nFigure 4b is quite hard to interpret on its own. \n\nThe claim that to be used as an attack this method only requires \"black-box API access\" (line 461) seems misleading, as with the exception of Goodfire's research API designed specifically for this purpose, hosts do not allow steering via API; the practical implication of this work are that researchers should test for jailbreak vulnerability before deploying steering to production."}, "questions": {"value": "Figure 2b shows a Llama3-8b-it 2/3rds depth random steering compliance rate across scaling coefficients that differs from the one shown in Figure 2c; why?\n\nHow were the 1000 SAE features chosen?\n\nWhat temperature(s) was used?\n\nCan you clarify the methods for creating the universal attack vector? What does \"top 20 vectors that successfully induce compliance\" mean? There's only one prompt, so each vector either induces compliance or doesn't. Why do you need \"100-500\" trials to identify them? What is the dependence on \"the model's baseline vulnerability\" mean? If the baseline compliance rate is not ~0%, then it's important to show it in Figure 3.\n\nWhat do you think explains your findings? It seems quite surprising that steering with most SAE features would lead to alignment breaking. Anthropic opened up a \"Golden Gate Bridge\" feature-steered model to the world, with no apparent jailbreaking issues; did they just get lucky?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t2UuXGFSnF", "forum": "uXecy0nKiJ", "replyto": "uXecy0nKiJ", "signatures": ["ICLR.cc/2026/Conference/Submission18168/Reviewer_m7Sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18168/Reviewer_m7Sg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927807187, "cdate": 1761927807187, "tmdate": 1762927918617, "mdate": 1762927918617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper asks whether activation steering for the purpose of inducing a behavior unrelated to safety (e.g. style transfer) may inadvertently cause jailbroken behavior. The authors find that indeed activation steering even with random vectors consistently induce jailbroken behavior. They find that when using SAEs as a source of steering vectors this effect is exacerbated. The authors introduce a technique for \"universal\" jailbreaking: averaging a sample of random vectors that have proven effective against certain attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an important area: inadvertent jailbreaking as a result of inference time interventions designed for an unrelated purpose, such as steering with SAEs. The authors study different danger categories when it comes to jailbreaking and measure their cross generalization."}, "weaknesses": {"value": "I would have liked to see results not just for random vectors but also for steering vectors that were actually trained for some benign purpose from the literature - do meaningful steering vectors also induce jailbroken behavior? For example, it has also been reported in the literature (Ghandeharioun 2024 - https://arxiv.org/pdf/2406.12094) that steering vectors may inadvertently cause increased refusal behavior. Also the steering coefficients at which the authors observed misaligned behavior (>1) is generally outside the recommended regime for use.\n\nI also feel that the core result: random vectors induce misalignment - is well predicted by the literature. For example Qi 2024 - https://arxiv.org/pdf/2406.05946, and especially Qi 2023 - https://arxiv.org/pdf/2310.03693 (Fine-tuning aligned language models compromises safety even when users do not intend to! - which the authors cite), also Peng 2024 - https://arxiv.org/pdf/2405.17374v1 - regarding the fragility of safety alignment."}, "questions": {"value": "What is the recommended steering coefficient when using SAE features? It seems it's only problematic / jailbreaks the model near 2.0. At that point, is the original intended behavior of SAE feature preserved?\n\nIf possible, it would be great to use a more powerful model to classify safe / unsafe than Qwen 8B. \n\nnit: Figure 2 - I think it would help with clarity to use the same y-range across all 3 figures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4zRSKKMiMu", "forum": "uXecy0nKiJ", "replyto": "uXecy0nKiJ", "signatures": ["ICLR.cc/2026/Conference/Submission18168/Reviewer_qXRg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18168/Reviewer_qXRg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18168/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939839934, "cdate": 1761939839934, "tmdate": 1762927918292, "mdate": 1762927918292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}