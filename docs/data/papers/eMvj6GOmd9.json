{"id": "eMvj6GOmd9", "number": 8807, "cdate": 1758098832419, "mdate": 1763117319929, "content": {"title": "Low Rank Weight Bases for Visual Analogies", "abstract": "Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations that are difficult to articulate in words. Given a triplet $\\\\{\\mathbf{a}, \\mathbf{a}', \\mathbf{b}\\\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to the analogy task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful semantic spaces that can be interpolated, we propose LoRBa, a novel approach that specializes the model to each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"*space of LoRAs*\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different types of visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the specific analogy pair. Through comprehensive evaluations, we demonstrate that our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation tasks.", "tldr": "We propose a novel modular framework that learns to dynamically mix low-rank adapters (LoRAs) to improve visual analogy learning, enabling flexible and generalizable image edits based on example transformations.", "keywords": ["Visual analogy", "image analogy", "visual relations", "image manipulation", "low-rank adaptation", "LoRA", "flow-based generative models", "diffusion models", "style transfer", "image editing"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e1406980b50844058d0e94a4c099f42778e581d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new approach, called LoRBA, which specializes in the model for image transformation tasks by leveraging the learned low-rank adapter parameters. The method follows a visual analogy framework, where the model is required to generate a new image by applying the same visual changes observed in a reference image pair to a target image. It contributes to improving the model’s generalization across diverse unseen image transformation tasks through a new architecture and provides detailed component-based analyses."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe paper provides a detailed ablation analysis that affects performance. \n\n•\tA variety of methods have been utilized to evaluate the results."}, "weaknesses": {"value": "•\tThe paper employs the term “visual analogy” primarily in the context of style transfer, object addition, and related image transformation tasks. However, visual analogy refers to a broader process of relational knowledge transfer, extending beyond image manipulation. \n\n•\tThe visual analogies were referred to as Visual Prompting and Visual Relations in Section 2. While these are related concepts, visual analogy generally encompasses a broader conceptual scope and is not strictly identical to the other two. It may be helpful for the authors to distinguish these terms more clearly.\n\n•\tThe paper proposes to decompose visual analogy learning through the LoRBA architecture and includes edit prompts in experiments that describe the intended transformation. In visual analogy, however, the transformation is inferred from the relation between the reference pair (A → A′). When the text prompt explicitly defines this relation, the reasoning aspect of analogy is effectively replaced by instruction following. In that case, the task aligns more closely with prompt-conditioned image transformation that follows the analogy structure (A→A′ :: B→B′), but not the reasoning process itself.\n\n•\tThe quantitative results for Preservation VLM, Edit Accuracy, CLIP, and LPIPS show that the performance of LoRA and LoRBA are quite close. Similarly, in the qualitative comparisons in Figure 4, LoRA appears to produce visually comparable results. Given that the observed performance gap between LoRA and LoRBA is relatively small, the reviewer is uncertain about why the community utilize this architecture instead of LoRA?  What is the main reason that makes LoRBA better than LoRA?"}, "questions": {"value": "•\tIn Section 2, what type of transformations are inferred? \n\n•\tFigure 2 is missing to show the edit prompt in the process.\n\n•\tHow is LoRBA conceptually and architecturally different than inspired work of Dravid’s?\n\n•\tRegarding the test data created with 18 community LoRAs, were the outputs manually reviewed or verified by human evaluators? How are quality of the results ensured?\n\n•\tTable 1 requires a clearer explanation, as it is currently difficult to interpret without additional context or guidance.\n\n•\tThe variations of capacity effect evaluation, such as {N = 32, r = 4}, are missing or not described well in Table 1.\n\n•\tFigure 7 is missing the results of LoRA.\n\n•\tWhat are the general tasks such as style transfer, background replacements, object insertion, object displacement etc. that LoRBA fails to generate accurate results and what might be the reason for that?\n\n•\tWhile the paper is motivated by the concept of visual analogy, the use of explicit edit prompts (e.g., “Turn this photo into an architectural rendering”) defines the transformation in advance and bypasses the reasoning process of visual analogy. It would be valuable to include an experiment without explicit transformation prompts to assess whether the architecture itself can work visual analogy task by inferring and applying the relation purely from the exemplar pair.\n\n•\tOne of the related works using the same exemplary-based image editing method with LoRA is PairEdit. An additional experiment can be conducted to evaluate PairEdit on the same test set and compare its results with LoRBA?\nPairEdit : https://arxiv.org/abs/2506.07992"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WSI1HbZXF0", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Reviewer_9xKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8807/Reviewer_9xKs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760561905027, "cdate": 1760561905027, "tmdate": 1762920577836, "mdate": 1762920577836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "4NPWARaJ4w", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763117318127, "cdate": 1763117318127, "tmdate": 1763117318127, "mdate": 1763117318127, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the task of visual analogy learning. The authors propose a two-stage framework: first, they train a set of LoRA modules alongside a corresponding set of learnable combination weights. In the second stage, these LoRA weights are combined to generate an image that fulfills the analogical relationship."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "**High-Quality Visual Results:** The method produces results of high visual quality that demonstrate strong adherence to the analogical prompts when compared against baseline methods."}, "weaknesses": {"value": "**Lack of Methodological Clarity:** The core weakness of this manuscript is the clarity of the methodology section. The description is ambiguous and lacks a clear, end-to-end overview of the training and inference processes. Furthermore, the mathematical notation is inconsistent and potentially confusing (e.g., the relationship between `a/b` as inputs and `A/B` as LoRA weights make reader confusing), which hinders a complete understanding of the proposed technique."}, "questions": {"value": "A comprehensive evaluation of this work is contingent upon a clear understanding of the methodology. Could the authors please provide significant clarification on the following points?\n\n1. **Elucidation of the Training and Inference Pipeline:** The current description of the pipeline is difficult to follow.\n    \n    - **Recommendation:** To resolve these ambiguities, I strongly recommend that the authors include a detailed diagram or, ideally, a **pseudo-code algorithm** that explicitly outlines both the complete training and inference procedures.\n        \n    - Given a single training instance with three inputs (`a`, `a'`, and `b`), could you please detail the process used to train the full set of N=32 LoRAs? The mapping from one training example to a large set of distinct LoRAs is not intuitive.\n        \n    - How are the learnable combination weights (`e_i` in Equation 4) incorporated into the training loss and updated during the optimization process?\n        \n    - Why use the 2x2 grid as the Flux’s input?\n        \n2. **Compare with LLM-based Methods:** Recent approaches, such as those leveraging Visual Language Models (e.g., \"Nano-Banana\"), have also been applied to analogy tasks. Could you please discuss the comparative advantages and potential limitations of your LoRA composition framework relative to these LLM-based methods? A discussion on aspects like inference speed, training cost, would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EpjKxhtggl", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Reviewer_DgSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8807/Reviewer_DgSv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665815868, "cdate": 1761665815868, "tmdate": 1762920577257, "mdate": 1762920577257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LoRBA, a visual-analogy editing method that replaces the single-LoRA adapter common in prior work with a learnable basis of LoRAs. A small encoder (frozen CLIP + projection) embeds the analogy triplet \\{a,a’,b\\} and routes softmax mixing coefficients over the basis to create a task-specific “mixed LoRA” at inference time; the diffusion backbone (FLUX.1-Kontext) receives the full triplet via extended attention while CLIP features are used only for LoRA selection. \nExperiments on Relation252k (train) and a custom validation split show improved trade-offs between edit accuracy and preservation (VLM-based) and higher pairwise win rates vs. several strong baselines; ablations support design choices (basis size, softmax vs. tanh, CLIP routing input)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper motivates that a single LoRA under-represents the space of transformations, while a learned basis + router can specialize per analogy at inference. This is grounded in prior observations that LoRAs can span a semantic space.\n\nLoRBA pushes the Pareto front on VLM edit-accuracy vs. preservation and wins user-study & VLM pairwise comparisons vs. most of baselines, indicating better edits without sacrificing identity."}, "weaknesses": {"value": "Three prior-art baselines run on FLUX.1-Dev, whereas LoRBA (and a capacity-matched single-LoRA baseline) run on FLUX.1-Kontext. This makes it hard to attribute all gains solely to the LoRA-basis design rather than backbone differences. A fairness note is warranted or re-runs on the same backbone are needed.\n\nBecause Relation252k’s test set is unavailable, the authors build their own validation suite (Unsplash images, LLM-generated prompts, and community LoRAs). While thoughtfully constructed, this pipeline can encode distributional choices that favor the method; public release and stress tests would help.\n\nBoth scalar metrics (edit-accuracy & preservation) and pairwise selection use Gemma-3 prompts. Although there is a user study, the paper would benefit from reporting VLM–human correlation and sensitivity analyses (prompt variants).\n\nExperiments cap the long edge at 512 and focus on FLUX.1-Kontext; it’s unclear how the approach scales to higher resolutions or transfers to other diffusion backbones. \n\nTypo: “mosiac” → “mosaic” in Fig. 3 caption. Please standardize notation (e.g., keep e_i for coefficients consistently across text/equations) and explicitly reference Eq. (2–4) where the “Mixed LoRA” is injected."}, "questions": {"value": "Clarify whether the custom validation set (images, prompts, LoRA list) and code will be released to enable reproduction"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U7hYjA07hN", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Reviewer_BMgC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8807/Reviewer_BMgC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968545705, "cdate": 1761968545705, "tmdate": 1762920576864, "mdate": 1762920576864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces method to jointly train 1) multiple LoRA modules and 2) a learnable encoder that assigns the coefficients for each LoRA module for image editing with visual analogy pairs.The paper compares against previous baselines and reports state-of-the-art on quantitative (CLIP score similarity + VLM-as-a-judge) and qualitative (user study) metrics.The paper also runs a suite of ablations to identify the core of the improvement."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written. I’m not too familiar with the topic, but was able to understand the motivation and the setup clearly.\n2. Strong performance gains against the baselines. The model is better at making accurate edits while preserving the original image. The results are validated with qualitative and quantitative metrics.\n3. Test-time inference is efficient. There is no need to train a separate module / separate set of coefficients for a new task at test-time. The images only need to go through a CLIP model to retrieve the query vector."}, "weaknesses": {"value": "1. It is unclear whether the out-of-domain tasks are truly distinct from the training analogy types. From how it is mentioned, it seems like the authors only sampled from LoRA modules for samples where the base model was unable to make edits for. Does this mean these analogy types are disjoint from the training analogy types? Were there manual checks? \n2. Related to 1, does not test the limit of generalization (lines 482-483). Are there specific analogy types in the authors’ validation set that the model performs better/worse on?\n3. Limited analysis of scalability. Any experiments on increasing N? Is it because of the limited data? Would the performance plateau?"}, "questions": {"value": "1. It seems like the validation data from Gong et al. (RelationAdapter) have now been released (https://huggingface.co/datasets/handsomeWilliam/Relation252K-unseen/tree/main). Could you also validate your method on the validation set just to remove any confounding factor from constructing your own validation set from a different pipeline?\n\n2. Nit-picky but were the images presented to the survey participants randomized in order to remove any positional bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p5kJr0YrUz", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Reviewer_6GPV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8807/Reviewer_6GPV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011389847, "cdate": 1762011389847, "tmdate": 1762920576450, "mdate": 1762920576450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their thorough review and constructive feedback. After careful consideration, we have decided to withdraw the paper at this stage to integrate the feedback for the next revision.\n\nBelow, we briefly address the main comments:\n\n# Performance for different analogy types\nReviewers 6GPV and 9xKs asked if the methods perform better for some analogy types. Indeed, all methods (ours included) handle style-transfer tasks better than targeted edits (e.g., crown addition). The reason for this effect is as follows:    \n1. Text prompts provide more information for style-transfer relative to targeted edits.\n2. Relation252K is style-transfer oriented, making methods trained on it struggle with significantly different tasks. \nTasks that the base model cannot generate are harder for all methods, but the relative success of LoRBA indicates that our LoRA basis can enable new editing capabilities for the base model.\n# Experimental rigor and fairness\nReviewer 6GPV highlighted the recent partial release of Relation252K’s validation set and requested more scalability experiments. We will update our results accordingly.\n\n\n# Uniqueness of LoRBA\nReviewer 9xKs asked how our approach differs from Dravid et al., and what improvements it offers over using a single LoRA. We emphasize that while our approach is inspired by Dravid et al, it differs from their work both conceptually and methodologically. \nDravid et al. train 65K different models, each on multiple face images of the same person, totalling in 65K different humans. They further curate this into a weight space using PCA, and then to use this space they require test-time optimization. This approach is resource demanding and data-hungry, both in training and in test time.\nOur approach directly learns an efficient weight-basis with only 32 LoRAs and a single forward pass at inference. Additionally, while a single LoRA is a surprisingly strong baseline (which even outperforms some prior published work), LoRBA achieves consistently better results that preserve the fine details of the analogy.\n\n# Prompts reliance\nReviewer 9xKs raised concerns about prompts bypassing visual reasoning.\nThe reviewer is correct that prompts help, Fig. 7 demonstrates how LoRBA does use the visual data in the analogy pairs. Unlike previous work which may  bypass content images and rely solely on the prompt.\nAdditionally, many prompts are simple and describe the general editing task type without specifying the exact details of the transformation. For example consider the bottom prompt in Figure 7  “Give this creature a crown of crystals”. This prompt  does not specify that the crown has a silver color and is  adorned with purple, diamond-shaped crystals at the top, separated by smaller gems, with circular gems along the base. Therefore, our method cannot bypass visual reasoning to successfully adhere to the analogy, and we find that it indeed uses the visual context. \nWe note that in LoRBA,  prompts are only used as an input to the base model, and LoRBA does not receive  textual input. As a result, one can train and apply LoRBA giving empty prompts to the base model, and even use a base model that does not accept prompts as input. \nThe prior methods use prompts as inputs. Prompts are an essential part of Relation252K on which LoRBA was trained, comparing prior art to a prompt-less version of LoRBA would not allow for a fair comparison. \n\n\n# Distinctness of out-of-domain tasks \nReviewer 6GPV inquired regarding the community LoRAs, and asked whether the analogy types they represent are disjoint from the analogy types seen during training.  \nIndeed, the resulting edits from the community LoRAs differ from the types of edits seen during training. As mentioned in Line 311, we aim to test our model on analogies that are in/out-of-domain for the base model. For out-of-domain samples, we cannot use the base-model to generate context pairs, because, by definition, the model cannot perform these edits. \nSince the training data of Flux is unknown, it is not clear how to construct an out-of-domain analogy test set. For this reason we opt for community LoRAs, which are generally trained to enable the base model to perform new types of edits. Here, we assume that these LoRAs were indeed trained as a result of the base model not producing the wanted transformation, since otherwise there is no point in fine-tuning these LoRAs. We manually verified the outputs differ from standard FLUX generation.\n\n\n# Reproducibility\nWe commit to releasing the evaluation set and codebase upon acceptance.\n\n\n\nWe again thank the reviewers for their expertise and time given to our paper."}}, "id": "qlE90nr9U6", "forum": "eMvj6GOmd9", "replyto": "eMvj6GOmd9", "signatures": ["ICLR.cc/2026/Conference/Submission8807/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8807/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8807/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763117087044, "cdate": 1763117087044, "tmdate": 1763117239105, "mdate": 1763117239105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}