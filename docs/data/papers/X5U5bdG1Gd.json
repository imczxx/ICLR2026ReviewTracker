{"id": "X5U5bdG1Gd", "number": 12432, "cdate": 1758207762489, "mdate": 1763041764326, "content": {"title": "SocialMirror: Reconstructing 3D Human Interaction Behaviors from Monocular Videos", "abstract": "Accurately reconstructing human behavior in close-interaction scenarios is crucial for enabling realistic virtual interactions in augmented reality, precise motion analysis in sports, and natural collaborative behavior in human-robot tasks. Reliable reconstruction in these contexts significantly enhances the realism and effectiveness of AI-driven interactive applications. However, human reconstruction from monocular videos in close-interaction scenarios remains challenging due to severe mutual occlusions, leading local motion ambiguity, disrupted temporal continuity and spatial relationship error. In this paper, we propose SocialMirror, a diffusion-based framework that integrates semantic and geometric cues to effectively address these issues. Specifically, we first leverage high-level interaction descriptions generated by a vision-language model to guide a semantic-guided motion infiller, hallucinating occluded bodies and resolving local pose ambiguities. Next, we propose a sequence-level temporal refiner that enforces smooth, jitter-free motions, while incorporating geometric constraints during sampling to ensure plausible contact and spatial relationships. Evaluations on multiple interaction benchmarks show that SocialMirror achieves state-of-the-art performance in reconstructing interactive human meshes, demonstrating strong generalization across unseen datasets and in-the-wild scenarios. The code will be released upon publication.", "tldr": "", "keywords": ["Human Mesh Reconstruction", "Multimodal Fusion", "Large Language Model", "Diffusion Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/cbe3022548f61698eb8612cfde41c1aec5764f11.pdf", "supplementary_material": "/attachment/4b71da1766218da3077188977ff1b5c812682bd3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for multi-person 3D mesh estimation, leveraging VLMs for semantic-guided refinement. The authors present a relatively complete system with module-wise ablation studies, robustness evaluation, and cross-dataset experiments. The main idea of using VLMs to provide semantic guidance in multi-person mesh estimation is intuitive, but there are several concerns regarding novelty, experimental validation, and presentation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is easy to understand and the overall framework is straightforward to follow.\n\n2. Introducing a VLM for semantic-guided refinement is an interesting idea; intuitively, it could help improve mesh estimation in challenging cases.\n\n3. The paper includes module ablation studies, robustness checks, and cross-dataset experiments, making the evaluation relatively comprehensive."}, "weaknesses": {"value": "1. Writing and presentation issues:\n\n   - Numerous typos and basic grammar mistakes (e.g., missing spaces around parentheses) make reading difficult.\n\n   - Writing is sometimes non-standard; tables are unclear and inconsistent, e.g., bold in Table 7 is confusing. Also in Table 3, the second column’s module is unclear; the second-to-last row (68.5) should be bolded.\n\n   - Section 4.3 has large blocks of text without paragraphs, which hinders readability.\n\n   - Figure 1 and method nickname: It is unclear what Figure 1 aims to convey. The term SocialMirror is never explained, and the connection to “mirror” is unclear.\n\n2. Impact of contributions is concerned:\n\n    - While the VLM module is intended to provide semantic guidance, ablation results show that it sometimes reduces performance on certain metrics.\n\n    - Improvements from other modules are relatively small and occasionally worsen metrics, raising concerns about the overall effectiveness.\n\n    - Upon checking the supplementary video, there are noticeable errors in multi-person mesh estimation (e.g., around 30 seconds in one video), which casts doubt on the claimed performance.\n\n    - The VLM Annotation User Study reported an average score of 3.3/5, which is only moderate and may reflect why some ablation metrics decrease when using VLM.\n\n    - Table 4 shows several metrics degrading in later entries; the explanation in line 417 is not very convincing."}, "questions": {"value": "Could the authors clarify the meaning of SocialMirror and what Figure 1 is intended to illustrate?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yfUU8GhOWH", "forum": "X5U5bdG1Gd", "replyto": "X5U5bdG1Gd", "signatures": ["ICLR.cc/2026/Conference/Submission12432/Reviewer_Eca9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12432/Reviewer_Eca9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874811897, "cdate": 1761874811897, "tmdate": 1762923319102, "mdate": 1762923319102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mdT7RgtTuP", "forum": "X5U5bdG1Gd", "replyto": "X5U5bdG1Gd", "signatures": ["ICLR.cc/2026/Conference/Submission12432/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12432/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763041763360, "cdate": 1763041763360, "tmdate": 1763041763360, "mdate": 1763041763360, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion framework for reconstructing human behavior in two-person interaction scenarios with mutual occlusion, where traditional HMR is difficult to apply due to the lack of direct visual cues for the occluded person. The method is guided by high-level semantic knowledge from a VLM as well as low-level geometry, contact, and temporal smoothness constraints. First, a VLM is fine-tuned to produce semantic captions for the interaction motion of two people, as well as explicit contact labels. The semantic captions are encoded by CLIP and concatenated with HMR2.0 features. A coarse SMPL pose is also predicted from HMR2.0, and further refined via temporal diffusion, where the CLIP+HMR2.0 embedding is used as ControlNet-style diffusion conditioning. A graph convolutional network is introduced to predict 3D joint locations from SMPL parameters. Finally, InterControl is used to optimize the original SMPL motions to minimize contact and inter-penetration constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Reconstructing human motions in close two-person human-interaction scenarios is an important and challenging problem.\n- The method produces state-of-the-art results on Hi3D, 3DPW, and Harmony4D.\n- The proposed design is effective according to ablation study"}, "weaknesses": {"value": "- The method is designed for two-person interaction scenarios, but it is unclear whether it easily scales to multi-person interaction scenarios with mutual occlusions, such as those in 3DPW."}, "questions": {"value": "- My understanding is the auxiliary model takes SMPL poses as input, and produces 3D joint locations as output. Why not just use the original SMPL inference procedure to obtain 3D joint locations?\n- The second column in Tab. 3 is labeled with a dash (between \"Semantic-guided motion filler\" and \"Temporal refiner factorized guidance\") - what does it represent?\n- The inter-person penetration volume does not seem to match between Tab. 1 (ours = 2380.5) and Tab. 4 (max(ours) = 406.3). Are these evaluated on the same Hi4D test set? Is there any intuitive explanation for the large discrepancy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SyxZfHLPfK", "forum": "X5U5bdG1Gd", "replyto": "X5U5bdG1Gd", "signatures": ["ICLR.cc/2026/Conference/Submission12432/Reviewer_9YpQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12432/Reviewer_9YpQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112132200, "cdate": 1762112132200, "tmdate": 1762923318730, "mdate": 1762923318730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper focuses on mesh recovery from monocular videos of two-subject interactions.\n- The method uses text-guided motion infilling via a diffusion model, then refines mesh trajectories with auxiliary and collision losses to produce realistic, image-consistent human meshes.\n- Ablations highlight the importance of the semantic-guided motion infiller and geometry optimizer, crucial for improving metrics, especially RE and GE.\n- Evaluations span three datasets: Hi4D, 3DPW, and Harmony4D.\n- Key baselines: BUDDI, Human4D, CloseInt.\n- Quantitative results show that SocialMirror consistently outperforms baselines across datasets and generalizes to sequences outside the training distribution (e.g., Harmony4D)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized and easy to follow.\n- The use of VLM-derived, text-guided semantics for mesh recovery is novel. While its viability warrants further study, it opens directions for VLMs that predict mesh parameters end-to-end directly from video.\n- Evaluations span multiple datasets and scenarios. The supplementary qualitative results are insightful and show SocialMirror consistently outperforming Human4D and CloseInt across settings."}, "weaknesses": {"value": "- Motivation of using a VLM for close-interaction reasoning: The method uses a VLM to summarize the interaction video, which then conditions the Interactive Diffuser to generate mesh sequences. This pipeline can be brittle: the pixels-to-text step is inherently lossy, and capturing fine-grained human motion in text is challenging. Although the Interactive Diffuser also takes images as input, VLM-derived conditioning may introduce bias or errors and degrade performance. It would be helpful to ablate SocialMirror’s robustness with respect to the accuracy of the semantic-text guidance.\n\n- Restricted to two people interaction: The methodology appears restricted to two-person interactions and seemingly cannot handle single-person sequences or scenes with more than two people (please correct me if I am wrong). This design choice is limiting and constrains real-world applicability. Notably, this is also a key limitation of related works in this area (e.g., BUDDI, ClostInt).\n\n- Qualitative Comparisons: The key request here is to test the generalization ability of SocialMirror beyond Hi4D (train set) and 3DPW like setting. I appreciate the Harmony4D qualitative results; however, comparable baselines on these videos (e.g., BUDDI, CloseInt) are missing. Including them would demonstrate out-of-distribution robustness and help rule out overfitting.\n\n- Runtime analysis: Most related baselines are feed-forward with optional lightweight optimization. In contrast, the proposed method includes heavier components (e.g., the VLM and diffusion-based motion infilling) that may increase inference latency. Additionally, given the number of modules, the manuscript does not clearly separate train-time and inference-time modules. A runtime/throughput and memory breakdown with baselines like Human4D, BUDDI and CloseInt would be very helpful here."}, "questions": {"value": "My key questions are centered around the weaknesses mentioned above:\n1. Performance degradation study with respect to accuracy of the VLM summary.\n2. Extending to single person and beyond two person scenarios.\n3. Qualitative comparison on out-of-domain videos.\n4. Runtime analysis wrt baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Zfs7fHYN9A", "forum": "X5U5bdG1Gd", "replyto": "X5U5bdG1Gd", "signatures": ["ICLR.cc/2026/Conference/Submission12432/Reviewer_xv5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12432/Reviewer_xv5C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136377354, "cdate": 1762136377354, "tmdate": 1762923318390, "mdate": 1762923318390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}