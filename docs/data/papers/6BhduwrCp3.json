{"id": "6BhduwrCp3", "number": 21408, "cdate": 1758317253706, "mdate": 1759896923539, "content": {"title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "abstract": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer for training deep neural networks that draws on ideas from quartic terms for kinetic energy with its stabilizing effects on various system dynamics. \nPrevious algorithms, including the ubiquitous Adam, operate at the so-called adaptive edge of stability regime during training, leading to rapid oscillations and slowed convergence of loss.\nHowever, VRAdam adds a higher order penalty on the learning rate based on the velocity such that the algorithm automatically slows down whenever weight updates become large. In practice, we observe that the effective dynamic learning rate shrinks in high-velocity regimes, and damping oscillations. By combining this velocity‑based regularizer for global damping with Adam’s per‑parameter scaling, we create a powerful hybrid optimizer. For this optimizer, we provide rigorous theoretical analysis of operation at the edge of stability from a physical and control perspective for the momentum. Furthermore, we derive convergence bounds with the rate $\\mathcal{O}(\\ln(N)/\\sqrt{N})$ for a stochastic non‑convex objective under mild assumptions. We demonstrate that VRAdam exceeds the performance against standard optimizers including AdamW. We benchmark various tasks such as image classification, language modeling, and generative modeling using diverse architectures and training methodologies including Convolutional Neural Networks (CNNs), Transformers, and GFlowNets.", "tldr": "We introduce Velocity-Regularized Adam (**VRAdam**), a velocity penalizing optimizer for deep neural networks.", "keywords": ["Optimization in deep learning", "physics-inspired", "edge of stability"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d9f257700257cb388158fa08ee2aa039b4192b6.pdf", "supplementary_material": "/attachment/09e8a584b3e639712fd5f3fddb33ba390b1818b4.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Velocity-Regularized Adam (VRAdam), a new optimizer for deep neural network training that introduces a physics-inspired regularization mechanism. Based on the classical Adam optimizer, VRAdam incorporates a quartic kinetic energy term to dynamically regulate the effective learning rate based on the velocity (momentum) of parameter updates. The resulting learning rate shrinks automatically in high-velocity regimes, reducing oscillations and improving convergence stability. The paper provides a theoretical framework, proving uniform exponential stability via Lyapunov analysis for stochastic non-convex objectives. Extensive experiments on CIFAR-10, WikiText-2, GridWorld, and GPT-2 fine-tuning show that VRAdam achieves faster convergence, smoother training curves, and better generalization compared to AdamW and other optimizers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of a quartic kinetic energy term as a stabilizing mechanism is a fresh and interesting physics-based perspective on optimizer design.\n\n2. The analogy between optimization trajectories and particle dynamics adds an intuitive understanding of how the method moves away from instability near the edge of stability.\n\n3. The paper rigorously proves global uniform exponential stability and convergence under mild conditions, supported by clear mathematical derivations.\n\n4. VRAdam consistently outperforms AdamW, RAdam, RMSProp, and SGD across diverse tasks (CNNs, Transformers, GFlowNets, and LLMs)."}, "weaknesses": {"value": "1. The presentation is not friendly to readers not familiar with optimization techniques using Langevin dynamics.\n\n2. Lots of hyperparameters are used. Although $\\beta_3$ is claimed to be robust, the practical sensitivity of VRAdam to its hyperparameters $like (\\alpha_0,\\alpha_1,\\beta_3)$ is not fully explored. Are they same with Adam or will be influenced by $\\beta_3$ ?\n\n3. While AdamW(2017) is a strong baseline, the study omits comparisons with newer optimizers (e.g., LION, AdaHessian), which are relevant for modern deep learning tasks.\n\n4. The heavy use of physical analogies (NRQCD, Lagrangians) brings difficulties to understand the motivation and improvement of the algorithm for readers unfamiliar with physics.\n\n5. The experiments only report the validation and test loss, the improvement against AdamW seems modest, whether VRAdam improves the task’s accuracy is not reported. Besides, no clear ablation on the contribution of the quartic term versus standard momentum damping—this would help isolate the true effect of velocity regularization."}, "questions": {"value": "1. See weakness.\n\n2. The author choose the quartic kinetic term NRQCD system as T(v), is there any other choice? Can you report the ablation study since it deserves to be the key insights of the improvement against AdamW.\n\n3. How sensitive is VRAdam to the choice of the velocity penalizer β₃? Does it generalize well across tasks without tuning?\n\n4. Does the quartic kinetic term introduce any bias that could affect convergence to flatter minima or generalization in practice? Can it be integrated with other techiques to improve generalization like in ‘Improving Generalization of Deep Neural Networks by Optimum Shifting , AAAI25’?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YtfPNtleeV", "forum": "6BhduwrCp3", "replyto": "6BhduwrCp3", "signatures": ["ICLR.cc/2026/Conference/Submission21408/Reviewer_uYkK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21408/Reviewer_uYkK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761021258287, "cdate": 1761021258287, "tmdate": 1762941752902, "mdate": 1762941752902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a physics-inspired optimizer, VRAdam, designed to address the oscillation and convergence slowdown problems that occur when adaptive optimizers (such as Adam) train near the stability boundary. Its core idea originates from kinetic models in classical physical systems. By introducing a velocity regularization term, the method imposes a higher-order penalty on the momentum (velocity) during optimization, thereby dynamically adjusting the learning rate and suppressing instability.\n\nContributions\n1. Proposes a physics-inspired optimizer, VRAdam, which introduces a velocity regularization term into Adam to dynamically adjust the learning rate and suppress training oscillations.\n\n2. Theoretically proves the stability of a simplified model and establishes a convergence bound under non-convex settings.\n\n3. Demonstrates the effectiveness of the method on image classification, language modeling, generative flow networks, and GPT-2 training tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is not a minor modification of Adam but introduces an intuition derived from a non-standard Lagrangian containing a $v^4$ term. This physics-to-algorithm mapping is uncommon in existing literature and demonstrates novelty.\n\n2. Theoretically, the paper provides a proof of uniform exponential stability for VRMomentum under quadratic objectives (Theorem 4.1) and establishes a convergence rate bound under stochastic non-convex settings comparable to that of Adam (Theorem 4.2).\n\n3. The experimental tasks cover a wide range of applications.\n\n4. If VRAdam proves to be stable and effective under broader settings (including larger-scale models and more repeated runs), it could be a strong improvement over existing optimizers."}, "weaknesses": {"value": "1. The theoretical analysis is based on a simplified model, while the actual implementation includes all modules. This may lead to inconsistencies between theory and practice.\n\n2. The tables lack explanations regarding reproducibility and do not include error bars, raising questions about the reliability of the results.\n\n3. The authors are advised to discuss the robustness of the results to hyperparameters, as all experiments use a single configuration. In all experiments, β₃ is fixed to 1, but the rationale behind this choice is not provided.\n\n4. The baseline methods are relatively outdated and insufficient in number. Although the related work section mentions more recent optimization techniques, these newer methods are not included in the experiments.\n\n5. The authors only compared the training time between AdamW and VRAdam on GPT-2, and the results did not show a clear time advantage."}, "questions": {"value": "1. The visualization of results could be improved. Why is Figure 5 in the appendix blank, and why are some figure fonts too small?\n\n2. The theoretical proofs are based on a simplified model, while the actual implementation includes all modules. The authors are encouraged to explicitly discuss how this simplification affects the stability and convergence conclusions.\n\n3. The reproducibility and statistical significance of the results are difficult to assess.\n\n4. Did the authors conduct any study or experiments regarding the choice of $\\beta_3$?\n\n5. Have the authors attempted experiments on more complex tasks?\n\n6. Does VRAdam provide any advantage in training time, and does it introduce additional computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kVSTudlffo", "forum": "6BhduwrCp3", "replyto": "6BhduwrCp3", "signatures": ["ICLR.cc/2026/Conference/Submission21408/Reviewer_JcRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21408/Reviewer_JcRv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053423899, "cdate": 1761053423899, "tmdate": 1762941752663, "mdate": 1762941752663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a physics-inspired variant of AdamW, where the effective learning rate is regularized via the magnitude of the momentum. Some theoretical and empirical results are provided, including a convergence rate and evaluation on CIFAR."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed method is novel, and the inspiration from physics is motivated.\n2. The theoretical results appear to be solid, and the experimental results seem promising."}, "weaknesses": {"value": "1. A major part of the theoretical analysis assumptions a convex quadratic objective, which the authors claim to be \"[traditional] for stability analysis\". However, this is far too strong of an assumption to be practically useful in any deep learning context. Furthermore, the convergence rate is a trivial corollary of the work of Defossez et al., providing little theoretical insight or novelty.\n2. The provided experimental results are conducted with small (124M) models and evaluated on simple tasks such as CIFAR. This makes it difficult to establish the scalability of the proposed method for practical deep learning.\n3. The proposed method introduces a hyperparameter $\\beta_3$, with no discussion on tuning or practical recommendations.\n4. The writing is sometimes very difficult to read. While inspiration from other fields is quite common in machine learning, the extended use of physical analogies may not be easily understood by the ICLR community (it was certainly lost on me).\n5. Minor nits: ensure the displays on page 3 are normal sized font. Use `\\citep` for citations that should be parenthetical.\n\nAs there are serious limitations with both the theoretical and empirical contributions of this work, I recommend rejection."}, "questions": {"value": "1. Could the authors elaborate on how the kinetic energy on line 129 was chosen? What happens if we choose something else? Could this lead to a better optimizer than the proposed method?\n2. See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LAr1Ftl18c", "forum": "6BhduwrCp3", "replyto": "6BhduwrCp3", "signatures": ["ICLR.cc/2026/Conference/Submission21408/Reviewer_G5iq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21408/Reviewer_G5iq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459201274, "cdate": 1761459201274, "tmdate": 1762941752227, "mdate": 1762941752227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript challenges gradient descent optimization techniques. Motivated by physics, the authors present VRAdam, which automatically controls the learning rate based on the momentum. Experiments on CIFAR, Wikitext, GFlowNet, and GPT-2 demonstrate improvements."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation, especially the physics-inspired optimizer, is interesting. Indeed, the momentum optimizer itself is derived from Newtonian dynamics.\n- The theoretical analysis looks solid and promising for the improved convergence.\n- Source code is available, which eases deployment in practice."}, "weaknesses": {"value": "- The norm of $v_t$ of the VRAdam looks to compute the global norm, but it may be appropriate to use the parameter-wise norm to allow parameter-wise learning rate control. This choice is not sufficiently discussed.\n- VRAdam brings three hyperparameters of \\alpha_0, \\alpha_1, and \\beta_3. I think these additional hyperparameters make it difficult to adopt the VRAdam in practice.\n- Accuracy of 80% for ResNet-32 with CIFAR-10 is a weak baseline.\n- Table 1 is not convincing enough. These results should be supplemented with more quantitative and qualitative analysis.\n- The experimental results, such as Table 2, are focused on the final validation loss. Is it possible to demonstrate other indices, such as practical ones? I think certain practitioners may want to capture the performance more practically, but the value of loss is difficult to understand on an absolute scale. It is also difficult to understand whether it corresponds to sufficient convergence or is still far from convergence.\n- LLM results were only trained for 2 epochs, which I think is insufficient for convergence.\n- I think Eq. 36 would be -m/4 + O(\\lambda), not -3m \\lambda/4 + O(\\lambda^2). Is it possible to provide an exact derivation?\n- Writing should be improved.\n    - Kingma & Ba (2017) → Kingma & Ba (2015)\n    - “the, global” → “the global”\n    - “d” → “(d)” for the caption of Figure 2.\n    - “physical inspired” → “physics-inspired” at Line 70.\n    - “Note, that” → “Note that” at Line 784.\n- The manuscript writes to compute velocity norm, whereas the source code computes gradient norm by default (normgrad=True). To be compatible with the description in this manuscript, normgrad=False may be correct for default."}, "questions": {"value": "Please see the weaknesses above. My score is based on the assumption that all typos are corrected in the revised manuscript."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AZMlmX1MRj", "forum": "6BhduwrCp3", "replyto": "6BhduwrCp3", "signatures": ["ICLR.cc/2026/Conference/Submission21408/Reviewer_SPPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21408/Reviewer_SPPq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813058885, "cdate": 1761813058885, "tmdate": 1762941751985, "mdate": 1762941751985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}