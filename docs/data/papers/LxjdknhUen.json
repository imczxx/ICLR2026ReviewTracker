{"id": "LxjdknhUen", "number": 23904, "cdate": 1758350098001, "mdate": 1763119759523, "content": {"title": "DualFusion: Dual Adaptive Fusion for Multi-View Pedestrian Detection via View Reliability Modeling and Channel Reweighting", "abstract": "Multi-view pedestrian detection methods project feature maps from multiple cameras onto a unified bird’s-eye view (BEV), enabling spatially aligned cross-view feature fusion. However, existing methods often adopt uniform fusion strategies, ignoring differences in view reliability—such as occlusion severity and projection distortion—and neglecting semantic correlations across feature channels (e.g., contours or foot-level cues). These limitations lead to noisy feature aggregation and suboptimal detection accuracy. To overcome these challenges, we propose a Dual Adaptive Fusion framework (DualFusion) that enhances BEV representations through two targeted modules: the Cross-View Feature Selector (CVFS) and the View-Channel Graph Attention (VCGA). CVFS, inspired by recent advances in Vision Transformers, employs a Transformer encoder to perform dynamic view-aware fusion in the BEV space, enabling spatially reliable feature aggregation across views. VCGA models joint view-channel dependencies using global-local context pooling and a graph-inspired multilayer perceptron, allowing adaptive channel-wise reweighting of the fused features. Extensive experiments on public benchmarks demonstrate that our method consistently outperforms recent multi-view fusion approaches. Ablation studies further confirm the effectiveness of each component. Our code will be released upon acceptance.", "tldr": "", "keywords": ["information fusion", "pedestrian detection", "adaptive fusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/339189639953d62d55b1b8f17f51310287553b6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DualFusion, a framework for robust multi-view pedestrian detection in safety-critical tasks like transportation and surveillance. Traditional BEV-based fusion methods combine multi-camera features uniformly, ignoring differences in view reliability (due to occlusions or distortions) and semantic correlations across feature channels.\n\nDualFusion addresses these issues with two modules:\n1. Cross-View Feature Selector (CVFS) – a Transformer-based encoder that performs spatially reliable, view-aware fusion\n2. View-Channel Graph Attention (VCGA) – a graph-inspired module that models joint dependencies across views and feature channels.\n\nExperiments on public benchmarks show consistent performance gains over existing methods, and ablation studies validate each module’s contribution. DualFusion thus establishes a general paradigm for adaptive multi-view fusion in BEV-based perception."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized. It describes the two proposed modules, the cross-view feature selector (CVFS) and the view-channel graph attention (VCGA) modules, in detail. The proposed modules are easy to reproduce.\n\n2. The proposed framework combines spatial and channel-wise attention mechanisms to automatically re-weight different areas in feature maps by the proposed modules.\n\n3. The paper includes comprehensive quantitative experiments to demonstrate that the proposed framework is competitive to the state-of-the-art methods."}, "weaknesses": {"value": "1. The novelty of the cross-view feature selector is limited. The proposed cross-view feature selector module only leverages the multi-head self-attention layers in the transformer blocks to emphasize important and suppress unimportant features in feature maps without futher modifying the architecture of the self-attention layers for better attentive feature generation.\n\n2. The idea of the view-channel graph attention is pretty interesting, but the purpose of using graph attention is still vague. It is better to explicitly clarify why the graph attention outperforms conventional channel-wise attention mechanisms.\n\n3. The proposed modules and the framework only achieve competitively quantitative results compared to the state-of-the-art methods. Based on the experimentation, the proposed framework does not fully resolve the issues mentioned in the paper."}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bt5du4Pjgp", "forum": "LxjdknhUen", "replyto": "LxjdknhUen", "signatures": ["ICLR.cc/2026/Conference/Submission23904/Reviewer_ffUz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23904/Reviewer_ffUz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545262958, "cdate": 1761545262958, "tmdate": 1762942848945, "mdate": 1762942848945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "5It7LQAXxO", "forum": "LxjdknhUen", "replyto": "LxjdknhUen", "signatures": ["ICLR.cc/2026/Conference/Submission23904/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23904/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119758513, "cdate": 1763119758513, "tmdate": 1763119758513, "mdate": 1763119758513, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two new modules for soft view selection and GNN-based channel reweighting on BEV space. Experiments demonstrate its effectiveness on two benchmark multi-view detection datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed methods got SOTA results on Wildtrack.\n2. The VCGA module incorporates the channel attention that is overlooked by previous multi-view detection methods."}, "weaknesses": {"value": "The main weaknesses lie in the module novelty and performance:\n\n1. The CVFS module is overly simplistic, being essentially just a standard attention operation across different views.\n\n2. Compared to MVDetr, the design of CVFS is much weaker, since MVDetr not only applies attention across multiple views but also takes projection geometry into account. \n\n4. The performance improvement observed by inserting CVFS into MVDetr may in part stem from an effect similar to adding more attention layers to the backbone. Therefore, simply deepening the original MVDetr architecture might also achieve comparable results.\n\n5. As in the main result table, the model performance is not SOTA on the MultiviewX dataset, which does not convince us of the effectiveness of the proposed module on different scenarios.\n\n6. The experiment is not comprehensive. More experiments should be conducted on larger datasets, such as CVCS, for a more complete comparison, since Wildtrack and MultiviewX are too small, only containing 360 frames for training and 40 for testing."}, "questions": {"value": "As above."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O2U0D5Ymkm", "forum": "LxjdknhUen", "replyto": "LxjdknhUen", "signatures": ["ICLR.cc/2026/Conference/Submission23904/Reviewer_J2Yv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23904/Reviewer_J2Yv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789572103, "cdate": 1761789572103, "tmdate": 1762942848057, "mdate": 1762942848057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DualFusion method with channel-wise and view-wise feature fusion for multi-view pedestrian detection task, which separates the classic unified self-attention into two folds. In details, Cross-View Feature Selector (CVFS) adopts multi-head self-attention to predict view-wise weights for feature selection. View-Channel Graph Attention (VCGA) utilizes Graph-MLP to obtain channel-wise weights as importance of different feature channels. Experiments are conducted on mainstream datasets Wildtrack and MultiviewX."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The differences among CNN-based Fusion, Transformer-based Fusion, and proposed DualFusion methods are fully discussed. \n\n2. Recent state-of-the-arts are evaluated to compare performance on mainstream datasets."}, "weaknesses": {"value": "1. Performance: DualFusion is not state-of-the-art on various metrics, i.e., MODP and Recall on Wildtrack, as well as MODA, Precision, and Recall on MultiviewX, which cannot support the claimed advantages of DualFusion over CNN-based and Transformer-based Fusion.  \n\n2. Interpretability: Without any extra supervision, how can CVFS and VCGA capture the correct view-wise and channel-wise features? Can they learn the geometric rules, e.g., near views contribute more to detect one pedestrian, and far views focus less? And what does each channel represent like “contours, foot-level cues” mentioned in Abstract? \n\n3. Confusing Visualizations: Figure 3 attempts to show the results between proposed DualFusion and baseline MVDetr, but there are no significant differences observed. The “irrelevant area” is also hard to be connect with the corresponding parts in Figure 4.  And the highlights before and after channel attention is also hard to interpret in Figure 5. The connection of Graph-MLP is similar to the weight of a vanilla MLP-based channel attention. \n\n4. Related Works and Motivation: the cited methods in Section “Channel and Graph Attention” are too outdated in 2018-2021. In Section “Multi-view Pedestrian Detection”, “Qiu et al (2024), Zhang et al (2024a) and Aung et al (2024)” are defined as “overlook occlusion-aware view reliability and cross-view channel redundancy”, but the proposed DualFusion does not adopt explicit occlusion handling, like extra supervision or segmentation masks, and the sparsity of channel activation is also not explicitly constrained. These are conflict with the motivation. \n\n5. Experiments: Now that Aung et al (2024) has already employed “vanilla channel attention”, and Zhang et al (2024a) also investigate supervised weight fusion, why they are not compared in state-of-the-art comparison in Table 1 or evaluated as similar modules with CVFS and VCGA in ablation study? And why only classic baselines MVDet and MVDetr are adopted? Now that there are more recent Transformer-based and CNN-based methods in Table 1. \n\nTypo: There are two “(c)” in Figure 4."}, "questions": {"value": "Refer to the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EhlHb3BQwO", "forum": "LxjdknhUen", "replyto": "LxjdknhUen", "signatures": ["ICLR.cc/2026/Conference/Submission23904/Reviewer_kWJf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23904/Reviewer_kWJf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795456332, "cdate": 1761795456332, "tmdate": 1762942847784, "mdate": 1762942847784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}