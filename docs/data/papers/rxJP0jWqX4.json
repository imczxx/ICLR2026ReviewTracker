{"id": "rxJP0jWqX4", "number": 2418, "cdate": 1757078974267, "mdate": 1759898149516, "content": {"title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems", "abstract": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages.\nHowever, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules and runtime enforcement is unreliable.\nExpressing safety policies as sequents, we propose QuadSentinel, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online.\nReferee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically.\nMeasured on ST-WebAgentBench (ICML CUA '25) and AgentHarm (ICLR '25), QuadSentinel improves guardrail accuracy and rule recall while reducing false positives.\nAgainst single-agent baselines such as ShieldAgent (ICML '25), it yields better overall safety control.\nNear-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable.", "tldr": "We propose a Multi-agent System based method to safeguard other agents and systems.", "keywords": ["AI Safety", "Multi-agent System"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f09abb74a3ffd7c606526a7c733ad1c9f83b57f.pdf", "supplementary_material": "/attachment/66af039cf15b34ef199e146f6866cf99d509d6e1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes QuadSentinel, a modular multi-agent guard framework designed to provide machine-checkable safety control for autonomous multi-agent systems. Specifically, the method translates safety policies into formal logical rules expressed as sequents over boolean predicates grounded in observable state. The system consists of four cooperating agents, i.e., a state tracker, policy verifier, threat watcher, and hierarchical referee, which jointly monitor inter-agent messages and actions in real time, selectively evaluate relevant predicates via a top-k retrieval mechanism to reduce cost, and issue allow/deny decisions with auditable justifications. Experiments on ST-WebAgentBench and AgentHarm show that QuadSentinel achieves higher accuracy, precision, and recall with lower false-positive rates than prior guardrails such as ShieldAgent and GuardAgent, and ablations demonstrate that each component (hierarchical referee, threat watcher, predicate filter) contributes meaningfully to overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel multi-agent guard framework that translates natural-language safety policies into propositional logics and enforces them through coordinated agents, extending prior single-agent guardrails e.g. ShieldAgent, and offering a clear conceptual contribution to runtime safety in multi-agent systems.\n\n2. The proposed method achieves good performance on the evaluated benchmarks, where it consistently outperforms baselines on multiple safety benchmarks, and the ablation study shows the individual contributions from each system component."}, "weaknesses": {"value": "1. The framework relies heavily on the correctness of the offline policy-to-rule translation step, but the paper provides limited quantitative evaluation of translation fidelity or failure cases, leaving uncertainty about robustness when policies are ambiguous or domain-shifted.\n\n2. Although the method is positioned as low-overhead, the reported runtime analysis in the appendix is theoretical, and no actual latency, throughput, or cost measurements are provided for real deployments, especially under high-frequency multi-agent interaction loads.\n\n3. The overall pipeline is highly complex, with multiple sub-agents where each has individual components and heuristics, however the key design details are insufficiently explained in both the main text and appendix. For example, how is the “must-check” set chosen in the state tracker? How do you determine the “high-precision evaluator,” and is there standalone validation of its correctness? The authors claim they only need to update only a subset of predicates each step yet provide no justification for robustness under partial predicate observation or error accumulation. It would be interesting to see an analysis on the guard decision accuracy vs cost while varying the evaluation budget.\n\n4. The presentation lacks clarity due to frequent mixing of language descriptions and mathematical notation, which disrupts flow and makes it harder for readers to follow the core methodology."}, "questions": {"value": "1. How sensitive is QuadSentinel to errors in the offline policy transition stage? Can the authors provide more case studies of the policies and extracted rules, and also provide quantitative results to justify that the system remains reliable when the NLP policies are noisy, incomplete, or adversarially phrased?\n\n2. The evaluation focuses on safety effectiveness but not runtime feasibility. Can the authors report concrete latency and token-cost measurements (per decision, per agent step) under realistic multi-agent workloads, and compare them directly with single-guard baselines to validate the “low-overhead” claim beyond theoretical complexity analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IBiaVpRUpq", "forum": "rxJP0jWqX4", "replyto": "rxJP0jWqX4", "signatures": ["ICLR.cc/2026/Conference/Submission2418/Reviewer_xhBP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2418/Reviewer_xhBP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985915316, "cdate": 1761985915316, "tmdate": 1762916229991, "mdate": 1762916229991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies safety issues in multi-agent LLM systems, focusing on real-time protection during agent interactions at the action level. It proposes a structured supervisory framework, QuadSentinel, which compiles deployer-written policies into machine-checkable rules through an offline policy registration and translation stage. At runtime, QuadSentinel enforces safety constraints using a multi-agent system with four guards: state tracker, policy verifier, threat watcher, and hierarchical referee, which provides dynamic action- and trajectory-level checks for other autonomous multi-agent LLM systems. The proposed framework is evaluated on two standard safety benchmarks, ST-\nWebAgentBench and AgentHarm, and the experimental results show that QuadSentinel is able to achieve balance performance w.r.t. accuracy, precision, recall, and false positive rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper addresses a challenging and important problem: safety in multi-agent LLM systems.\n\n- This paper proposes a relatively complete and thorough framework, capable of dynamic online monitoring with action- and trajectory-level safety enforcement for multi-agent systems. It also considers efficiency perspective, e.g., incremental updating of predicates to avoid full re-evaluation, and risk-cost optimization.\n\n- The translation from natural language policies to predicates and action and message rules is inspiring. \n\n- The experimental results demonstrate high accuracy/precision/recall and low false positive rates, which supports the effectiveness of QuadSentinel. Additionally, in the ablation study, it is interesting to observe the significantly higher false positive rate when using a single referee, which may provide insights for future research.\n\n- The paper is well organized, clear, and presented effectively."}, "weaknesses": {"value": "- While QuadSentinel translates natural-language policies into sequents, it is unclear how robust the system is to ambiguous or conflicting policies. It is better to include such clarification or discussion in the paper.\n\n- The framework may fail for risky actions or malicious attacks that are unseen from the registered policy book. It would be beneficial for the paper to discuss potential adaptivity or mitigation strategies for unseen threats.\n\n- It would be better to show some case studies.\n\n- It would be better to show the latency analysis."}, "questions": {"value": "- Since safety checks and the referee operate at the action level, how might blocking certain actions affect agent interactions and overall task performance?\n\n- For top-k filtering, how is k chosen? Is there any study or hyperparameter analysis regarding k?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FGOMEfsy6g", "forum": "rxJP0jWqX4", "replyto": "rxJP0jWqX4", "signatures": ["ICLR.cc/2026/Conference/Submission2418/Reviewer_J6ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2418/Reviewer_J6ph"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989259361, "cdate": 1761989259361, "tmdate": 1762916229749, "mdate": 1762916229749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a multi-agent guardrail framework to safeguard agent framework. The framework consists of state tracker, threat watcher, policy verifier, and referee."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The guardrail of agent system is an interesting research direction. The paper develops a SOTA result solution."}, "weaknesses": {"value": "1. Novelty is limited. The paper also does not clarify the fundamental difference to ShieldAgent well. ShieldAgent uses probablistic inference to compute sort of \"rule violation\" score, while this paper uses LLM in the loop to track related rules, observe violations and make guardrail predictions. If I understand correctly, this is a fuzzy version of ShieldAgent with LLM in the loop.\n\nThe state tracker part may make it more efficient to only consider related rules with LLM as filter, but the efficiency needs to be evaluated empirically as LLM inference is non-trivial. Therefore, there is a cost by LLM and cost by rule traverse tradeoff that should be explrored more.\n\n2. The presentation is not good. The paper seems make the method description too complex with a lot of natations. I do not see the reason why it is made too formal, considering the principle of method is not that mathematic. At least a short paragraph of high-level overview with a running example to show functions of four components can be helpful.\n\n3. Calling the defense a multi-agent framework is questionable. The process seems simple with sequential LLM tool callings. There is no complex LLM workflow or tool usages or much communication across agents. It would be more appropriate to call it LLM-based guardrail (with multiple LLM callings).\n\n4. Experiment results on more advanced real-world agents such as ChatGPT-Agent, Codex, Claude Code would be more interesting than AWM agents."}, "questions": {"value": "Does the evaluation use the same set of policies compared to ShieldAgent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KF0Y67WEJp", "forum": "rxJP0jWqX4", "replyto": "rxJP0jWqX4", "signatures": ["ICLR.cc/2026/Conference/Submission2418/Reviewer_X7Kp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2418/Reviewer_X7Kp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2418/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030304946, "cdate": 1762030304946, "tmdate": 1762916229529, "mdate": 1762916229529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}