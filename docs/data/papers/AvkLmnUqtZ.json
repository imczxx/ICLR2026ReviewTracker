{"id": "AvkLmnUqtZ", "number": 2052, "cdate": 1756982705455, "mdate": 1759898172061, "content": {"title": "$\\psi$DAG: Projected Stochastic Approximation Iteration for Linear DAG Structure Learning", "abstract": "Learning the structure of Directed Acyclic Graphs (DAGs) presents a significant challenge due to the vast combinatorial search space of possible graphs, which scales exponentially with the number of nodes. Recent advancements have redefined this problem as a continuous optimization task by incorporating differentiable acyclicity constraints. These methods commonly rely on algebraic characterizations of DAGs, such as matrix exponentials, to enable the use of gradient-based optimization techniques. Despite these innovations, existing methods often face optimization difficulties due to the highly non-convex nature of DAG constraints and the per-iteration computational complexity. In this work, we present a novel framework for learning DAGs, employing a Stochastic Approximation approach integrated with Stochastic Gradient Descent (SGD)-based optimization techniques. Our framework introduces new projection methods tailored to efficiently enforce DAG constraints, ensuring that the algorithm converges to a feasible local minimum. With its low iteration complexity, the proposed method is well-suited for handling large-scale problems with improved computational efficiency. We demonstrate the effectiveness and scalability of our framework through comprehensive experimental evaluations, which confirm its superior performance across various settings.", "tldr": "We propose a scalable framework for learning DAGs using Stochastic Approximation with SGD and efficient projection methods, offering improved performance and computational efficiency for large-scale problems.", "keywords": ["Structure learning", "Continuous Optimization", "Directed Acyclic Graphs"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1eff0a5531a8af74944e56bf24954f7c63973891.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes œàDAG, a framework for Directed Acyclic Graph (DAG) structure learning based on Stochastic Approximation (SA) principles combined with projected stochastic gradient methods. The authors reformulate the DAG learning problem as a stochastic optimization task and introduce projection-based steps to enforce acyclicity efficiently. They claim scalability to large graphs (up to 10,000 nodes) and show empirical results comparing œàDAG with NOTEARS, GOLEM, NOCURL, and DAGMA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1- The theoretical presentation is mostly sound; I did not find explicit errors in the mathematical derivations or proofs.\n\n\n2- The idea of combining stochastic approximation (SA) with projection-based DAG constraints is straightforward and could be computationally appealing.\n\n\n3- The authors provide comparisons with several standard baselines, including NOTEARS, GOLEM, and DAGMA.\n\n\n4- The proposed algorithm is simple and might be useful in certain large-scale linear SEM settings."}, "weaknesses": {"value": "1- The paper presents two main ideas:\n(1) a new formulation of a stochastic loss function (Eq. 9) and an equivalent version based on the adjacency matrix (Eq. 10), and\n(2) a strategy to reduce runtime by first learning the graph skeleton through a stochastic optimization procedure, then estimating a variable ordering via a heuristic projection function, and finally constructing the best DAG consistent with that ordering.\n\nRegarding the first idea, the method largely reuses standard stochastic approximation updates and applies them to DAG learning with only minor modifications compared to existing approaches such as NOTEARS. For the second idea, the proposed projection mechanism is purely heuristic and lacks both theoretical justification and novelty relative to prior constrained optimization methods. Moreover, the paper overlooks several related approaches, such as BOSS, which first determines a variable ordering and then performs score-based structure learning with a BIC score and greedy search, as well as various hybrid methods that use constraint-based algorithms to obtain an initial DAG or skeleton for subsequent optimization. These omissions weaken the claimed novelty and contextual positioning of the work.\n\n2- The reported numerical results are unconvincing. The claimed scalability (10,000 nodes) is not backed by verifiable or reproducible evidence. The GitHub link to the implementation (https://anonymous.4open.science/r/psiDAG-8F42) appears to be non-functional, which undermines reproducibility and transparency. \n\n3- The Sachs protein network results are particularly unconvincing: œàDAG reports a Structural Hamming Distance (SHD) of 14, which is not competitive. Simple score-based or constraint-based methods, such as PC or Hill-Climbing (HC), can achieve lower SHD on this dataset.\n\n4- The paper suffers from weak writing quality, with numerous typographical errors and noticeable inconsistencies and discontinuities between paragraphs.\n\n5- The method is limited to linear Bayesian (SEM) models only. This is a strong limitation, especially since many recent DAG-learning frameworks (e.g., nonlinear NOTEARS, DAG-GNN) address nonlinear dependencies."}, "questions": {"value": "1- Can the proposed method be extended to handle categorical variables in the model, or is it limited to continuous (linear Gaussian) settings?\n\n2- Is there any theoretical justification or proof showing that the proposed projection function can reliably recover the correct variable ordering?\n\n3- Could the authors evaluate the method on additional benchmark datasets, such as ALARM, Link, and Munin, available at https://www.bnlearn.com/bnrepository/?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mZPY9fH9wN", "forum": "AvkLmnUqtZ", "replyto": "AvkLmnUqtZ", "signatures": ["ICLR.cc/2026/Conference/Submission2052/Reviewer_u9Q7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2052/Reviewer_u9Q7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579789007, "cdate": 1761579789007, "tmdate": 1762916006911, "mdate": 1762916006911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes œàDAG, a novel framework for learning Directed Acyclic Graph (DAG) structures based on Stochastic Approximation (SA) integrated with SGD-based optimization. The key contributions include: (1) reformulating the discrete DAG learning problem as a stochastic optimization problem (Eq. 9), (2) introducing a three-stage algorithmic framework alternating between unconstrained optimization, projection onto DAG space, and constrained optimization, and (3) demonstrating scalability to graphs with up to 10,000 nodes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method successfully handles graphs with up to 10,000 nodes, significantly outperforming baselines (GOLEM/NOCURL fail beyond 3,000 nodes, DAGMA beyond 5,000).\n2. The paper includes extensive experiments across multiple dimensions: graph types (ER, SF), densities (k=2,4,6), sizes (d=10 to 10,000), and noise distributions (Gaussian, Exponential, Gumbel), with both equal and non-equal variance settings.\n3. The proposed projection method (Algorithm 3) has O(d¬≤) complexity and avoids expensive matrix exponentials or log-determinant computations required by prior methods."}, "weaknesses": {"value": "1. Theorem 8 claims convergence to a local minimum, but the proof is informal and hand-wavy. The two-case analysis doesn't rigorously establish convergence, and there's no guarantee the method won't cycle between subspaces indefinitely.\n2. Section 3.1 states \"which implies that the minimizer of (9) recovers the true DAG\" but provides no rigorous proof. The algebraic manipulation ||x - W^‚ä§x|| = ||(I-W)(I-W*^‚ä§)^{-1}N_i|| doesn't obviously imply W=W* is the unique minimizer.\n3. Unlike recent DAG learning theory (e.g., Gao et al. 2022b, Deng et al. 2023b), this paper provides no sample complexity bounds or finite-sample convergence rates.\n4. Algorithm 3's greedy heuristic has no theoretical analysis. Why should minimizing row/column norms find a good topological ordering?\n5. DAGMA \"fails to converge\" in numerous settings (protein dataset, r>15, d‚â•5000 for ER2). This is highly unusual given DAGMA's reported robustness in the original paper. Have implementations been verified against original codebases?\n6. The paper uses a non-standard convergence criterion (f(x_k) - f(x*) ‚â§ 0.1¬∑f(x*)) which requires knowing f(x*). How is this computed? Different methods may have different sensitivities to this threshold.\n7. Only one small real dataset (d=11, n=853) is tested. For a method claiming scalability, evaluation on larger real networks is essential.\n8. Why alternate between unconstrained optimization, projection, and constrained optimization? Why not just project once? The paper provides no theoretical or empirical justification for this design choice.\n9. How are œÑ‚ÇÅ and œÑ‚ÇÇ chosen? How many outer iterations K are needed? What initialization is used? \n10. Lemmas 2, 5, 6 are basic set theory facts that add little value. The claim that D is a conic set (Lemma 2) is trivial since scaling edge weights doesn't create cycles."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kJoyQiB2Pi", "forum": "AvkLmnUqtZ", "replyto": "AvkLmnUqtZ", "signatures": ["ICLR.cc/2026/Conference/Submission2052/Reviewer_tyJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2052/Reviewer_tyJK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761674583354, "cdate": 1761674583354, "tmdate": 1762916006522, "mdate": 1762916006522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of learning graphical structures (Directed Acyclic Graphs; DAGs) from data, specifically targeting the typical linear model structure learning frameworks used in existing methods like NOTEARS (Zheng et al., NeurIPS 2018), GOLEM (Ng et al., NeurIPS 2020), and NOCURL (Yu et al., ICML 2021). The paper proposes a new algorithm called ùúë-DAG. The key idea is a three-stage optimization process: unconstrained optimization ‚Üí projection onto the DAG space ‚Üí optimization that preserves the vertex order. Stochastic gradient methods are applied in both the first and third stages. This approach reduces the search space from all possible DAGs, an exponentially large space, to the space of topological orderings, enabling a more efficient algorithm suited for large-scale problems. Empirical results show that ùúë-DAG outperforms existing methods like NOTEARS, GOLEM, and NOCURL in comparative experiments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents a very interesting and robust algorithm that tackles one of the key challenges in DAG structure learning, i.e. how to satisfy the strict DAG constraint, which has been a major difficulty for existing optimization methods. The proposed approach decomposes the problem into largely independent subproblems: optimization ‚Üí projection onto the constraint-satisfying solution space ‚Üí (weakly-)constrained optimization, forming a three-stage framework.\n- In the final step (step 3) of this three-stage process, optimization must be performed while preserving the topological order of vertices determined in step 2. To handle this, the paper introduces a valid method based on computing the transitive closures and applying masking to enforce the order constraints within optimization.\n\n- Previous methods that needed to handle strict constraints over an exponentially large DAG search space, but the proposed method with Algorithm 3 used in step 2 can now reduce the problem to enforcing strict constraints over node orderings in step 3. This effectively narrow down the search from the exponentially combinatorial space of DAGs to the permutation space of node orderings, which is smaller, and leads to a more efficient and logically grounded solution in the proposed method.\n\n- Experimental results demonstrate empirical superiority in both accuracy and computational efficiency compared to representative existing methods, including NOTEARS (Zheng et al., NeurIPS 2018), GOLEM (Ng et al., NeurIPS 2020), and NOCURL (Yu et al., ICML 2021).\n- Each of the points is thoroughly explained in the appendix, which is more than great."}, "weaknesses": {"value": "- Since the idea of using stochastic gradient descent and the idea of using projection methods seem largely independent, an ablation study analyzing the contribution of each would make the work more informative. For example, in non-convex hard-constraint optimization problems like those with L0-norm penalties, projected gradient methods are a traditional approach. However, it‚Äôs well known that even simple gradient descent combined with projection often faces challenges in terms of convergence guarantees and optimality. These issues typically require additional techniques or relaxations, and simply replacing the gradient method with a stochastic version likely doesn‚Äôt resolve them on its own. \n\n- The SI provides detailed explanations, but a clearer discussion in the main text about how existing methods handle hard DAG constraints and how the proposed method takes a different approach would help readers better understand the contributions of this work. \n\n- The rationale for using stochastic gradient methods from the perspective of Stochastic Approximation (SA) vs. Sample Average Approximation (SAA) is valid as described, but a bit misleading. In practice, the difference between the sample average and the expectation is often handled with some form of regularization in SAA. So, while adopting SA may offer benefits in terms of computational efficiency or convergence stability, the current explanation suggesting it directly improves approximation accuracy may be a bit confusing. That said, recent work has shown that stochastic gradient methods can offer implicit regularization in complex optimization landscapes, so this could be useful to clarify the benefit with a more careful explanation.\n\n- It seems the formulation reuses a standard setup, but since the objective function implicitly becomes quadratic, it would be helpful to include a brief explanation. When the noise term ( N ) is Gaussian, a quadratic objective is appropriate. However, for cases like exponential or Gumbel noise, as tested in the experiments, it‚Äôs not immediately clear whether the quadratic objective is still valid. One possible reason for the proposed method‚Äôs stability might be that, while the DAG constraint is complex, the error term‚Äôs quadratic form provides favorable properties, and this could be indirectly contributing to its effectiveness."}, "questions": {"value": "I'm not a researcher in this specific area, so I'd like to ask a few clarification questions:\n\n- From a general optimization design perspective, is the main takeaway that, in the case of DAG constraints, methods that explicitly account for graph structure are more effective, meaning that standard approaches like Projected Gradient Methods or Proximal Gradient Methods with convex relaxations are not sufficient?\n\n- On p.15 of the appendix, are the objective functions in the existing methods optimized using techniques other than stochastic gradient methods? Since the objective function and the optimization strategy are conceptually separate, it seems that one could, in principle, apply stochastic gradient methods to equations (11), (12), and (13) by handling constraints via Lagrangian multipliers and using proximal methods for the L1 terms. Was this tested? Or is there some technical barrier that makes introducing stochastic gradient methods into this problem particularly challenging? A clearer explanation of this point would be appreciated.\n\n- Both the existing formulations and the proposed method use a quadratic loss term, but is there no assumption of Gaussian noise? As shown on p.7, the noise ( N ) is tested not only with Gaussian noise, but also with Exponential and Gumbel noise. In those cases, wouldn‚Äôt a linear loss be more appropriate for Exponential noise, and a logistic loss for Gumbel noise? Wouldn't this part affect the entire paper?\n\n- In lines 290‚Äì293 on page 6, it says, ‚Äúif we know the true topological ordering ord(G‚àó), then we can recover the true DAG W‚àó with high accuracy.‚Äù However, in practice, we don‚Äôt actually know the true topological ordering, and we can't guarantee that the node ordering obtained in Step 2 is the true one. So, should we understand this not as a theoretical guarantee of finding the exact solution, but rather as a claim that the search space has been reduced from the combinatorial DAG space to the permutation space of node orderings?\n\n- Since the true topological ordering generally can't be identified, that means even when using the proposed method, if the ordering obtained in Step 2 isn't the true one, the solution won't converge to the correct one, as we can see in Figure 2, right? I‚Äôd appreciate it if you could provide some clarification, as the takeaway in Section 4.1 wasn‚Äôt entirely clear to me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yCGoj40cpE", "forum": "AvkLmnUqtZ", "replyto": "AvkLmnUqtZ", "signatures": ["ICLR.cc/2026/Conference/Submission2052/Reviewer_Hg8k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2052/Reviewer_Hg8k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981113548, "cdate": 1761981113548, "tmdate": 1762916005988, "mdate": 1762916005988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}