{"id": "X62UhAvmi6", "number": 22315, "cdate": 1758329565227, "mdate": 1759896872785, "content": {"title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency", "abstract": "Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget—the product of its long-term on-rate $a_i$ and fan-out $k_i$. A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity–audience balance, $\\log\\tfrac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path towards learning more diverse and efficient representations.", "tldr": "", "keywords": ["learning rules", "neuroAI", "activity-dependent pruning", "structural plasticity"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33da3abda2dfb4e4f2dadaa9fe45442d4a5ea641.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Different from the previous pruning methods based on the magnitude or gradient, this paper proposed Budgeted Broadcast (BB) based on the inspiration from the metabolic cost in biological neural networks. They formalize this cost as a neuron's traffic, which is the product of a neuron's activity and its number of connections. The rule of BB is derived from constrained-entropy optimization. By applying to for domains, ASR, face identification, synapse detection, and change detection, it shows better or comparable performance with other pruning methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This method reinterprets biological phenomena mathematically and applies it to prune artificial neural networks. \n- It has lower computational complexity because only local statistics (ai, ki) are used. It can be integrated into previous networks.\n- It shows good results on various domains, especially for long-tail data.\n- Common units have fewer connections, while rare units maintain more connections, increasing expressive diversity.\n- It is based on the theoretical basis, \"selectivity-audience balance' relationship."}, "weaknesses": {"value": "- This paper deals with unstructured sparsity. This research has limitations because it is not easy to reduce actual computation compared to structure pruning, and specialized hardware is required for the acceleration.\n- The application for large-scale models, such as LLMs or LMMs, is necessary to show the effectiveness of this method. \n- The performance depends on various hyperparameters, for example, \\beta, \\tau, etc. \n- The proposed method should be compared more recent weight pruning methods.\n- In the derivation, this paper assumed that AWGN, weak correlation, and bounded energy. In the actual ANN, these conditions may not be valid."}, "questions": {"value": "- It is not clear that this model can actually be put to practical use.\n- I wonder why this paper did not apply to database that is widely used for pruning, such as imagenet or cifar.\n- It would be better to explain how the entire network structure can be organized efficiently based on the proposed local budget rule."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nybRBqj9wo", "forum": "X62UhAvmi6", "replyto": "X62UhAvmi6", "signatures": ["ICLR.cc/2026/Conference/Submission22315/Reviewer_LPiQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22315/Reviewer_LPiQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744234791, "cdate": 1761744234791, "tmdate": 1762942166350, "mdate": 1762942166350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Additional Experiments on LLMs"}, "comment": {"value": "We thank the reviewers for their time and constructive feedback. We are encouraged that the reviewers unanimously recognized the core contribution of our work in moving beyond traditional parameter-importance scores to introduce a new pruning axis inspired by biological metabolic constraints. In biology, a core principle is that it is metabolically costly for a neuron to be both highly active and broadcast its activity widely. Our research showed, perhaps surprisingly, that applying this same constraint is also highly beneficial for creating balanced artificial networks with diverse representations. Our primary focus has been therefore on exploring this learning theory and understanding how this foundational connectomic rule translates from neuroscience to artificial networks. Reciprocally, we hope that these new insights from learning theory will be valuable to computational biologists trying to understand how biological circuits undergo synaptic refinement.   \n\nA key theme emerging from the reviews was the question of whether our biologically-inspired theory scales to modern, large-scale models and how it performs under hardware-aligned structured sparsity. Motivated by this valuable feedback, we conducted new experiments to explore this exact question. We are pleased to report that the principles hold up strongly, leading to very encouraging results.\n\n**Additional Experiments on LLMs** \n\nWe conducted one-shot pruning experiments on Llama 3.1–8B, both under unstructured sparsity and under a ≤ 2:4 N:M pattern. We prune only the feedforward blocks (5.64B parameters) and keep all other parameters dense. \n\nTables 1 and 2 summarize the perplexity scores of all methods on TinyStories and Wikitext-2 at different sparsity levels and token-frequency buckets. On both datasets, unstructured BB (Budgeted Broadcast) consistently outperforms unstructured WANDA and magnitude pruning for all sparsity levels and for both common and rare tokens. The gap is particularly large on the rare bucket at higher sparsity (e.g., s = 0.7), where magnitude pruning and WANDA often lead to very large perplexity scores, while BB remains in a moderate range.\n\nFor the ≤ 2:4 structured setting, the picture is more mixed at s = 0.5: WANDA is slightly better in overall perplexity scores. At higher sparsity levels (s = 0.6 and s = 0.7), BB achieves substantially lower perplexity scores than WANDA across both datasets and buckets, especially on rare tokens, while structured magnitude pruning again degrades sharply. Overall, these results indicate that broadcast-budget pruning is competitive with standard baselines under unstructured sparsity, and that the same allocation principle helps stabilize performance under hardware-friendly ≤ 2:4 constraints at moderate and high sparsity.\n\n---\n\nTable 1: Perplexity (PPL) on TinyStories. (* indicates best)\n\n```text\nMethod     Category      | --- All Tokens --- | --- Common Bucket --- | ---- Rare Bucket ----\n                         | s=0.5  s=0.6  s=0.7| s=0.5   s=0.6   s=0.7 | s=0.5   s=0.6   s=0.7\n-------------------------------------------------------------------------------------------\nDense      Baseline      |   -    3.88    -   |   -     3.53     -    |   -     5.90     -\n-------------------------------------------------------------------------------------------\nBB         Unstructured  | *3.95  *4.49  *7.02| *3.83   *4.31   *6.60 | *6.30   *7.08  *11.78\nWANDA      Unstructured  |  4.43   6.77  23.73|  4.38    6.45   21.08 |  8.51   15.45  100.96\nMAG        Unstructured  | 11.35  23.98  791.3| 10.23   17.81   485.6 | 58.75  234.86   3.5e4\n-------------------------------------------------------------------------------------------\nBB         N:M           |  7.11  *7.91 *12.32|  6.82   *7.72  *11.72 | 16.69  *19.22  *35.18\nWANDA      N:M           | *6.83   8.78  29.22| *6.57    8.56   25.84 |*15.79   24.19  115.23\nMAG        N:M           | 21.24  65.19  1.4e4| 17.63   49.12   1.1e4 | 165.0   1.3e3   1.5e6\n```\n\nTable 2: Perplexity (PPL) on Wikitext-2. (* indicates best)\n\n```text\nMethod     Category      | --- All Tokens --- | --- Common Bucket --- | ---- Rare Bucket ----\n                         | s=0.5  s=0.6  s=0.7| s=0.5   s=0.6   s=0.7 | s=0.5   s=0.6   s=0.7\n-------------------------------------------------------------------------------------------\nDense      Baseline      |   -    6.11    -   |   -     5.87     -    |   -     8.33     -\n-------------------------------------------------------------------------------------------\nBB         Unstructured  | *6.18  *7.19 *11.31| *6.01   *6.77  *10.88 |*18.27  *24.53  *68.69\nWANDA      Unstructured  |  8.50  14.91  82.33|  7.22   11.72   53.22 | 31.95  105.06  2782.8\n-------------------------------------------------------------------------------------------\nBB         N:M           | 15.97 *18.54 *33.33| 12.45  *14.18  *23.77 | 119.7 *162.50 *513.63\nWANDA      N:M           |*15.34  23.01  93.28|*12.03   17.14   59.15 |*109.6  249.24  3667."}}, "id": "siAssnLKPs", "forum": "X62UhAvmi6", "replyto": "X62UhAvmi6", "signatures": ["ICLR.cc/2026/Conference/Submission22315/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22315/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22315/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763692137134, "cdate": 1763692137134, "tmdate": 1763692137134, "mdate": 1763692137134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Budgeted Broadcast, a biologically inspired pruning approach that shifts focus from neuron’s utility to metabolic constraints. Specifically, the paper defines neuron traffic and derives a selectivity-audience balance from constrained entropy maximization. Intuitively, this approach enforces a tradeoff: neurons can speak loudly to a small audience or quietly to a large one. Experiments on ASR, face identification, and synaptic prediction demonstrate that BB demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed pruning criteria that combines long-term activation and the number of fan-out is novel, as far as I know.\n- The relationship between neurosience, learning theory and practical AI algorithms is interesting.\n- The proposed method is evaluated on a wide-range of tasks including encoder-decoder transformers, and CNNs."}, "weaknesses": {"value": "- The pruning mechanism appears to rely on sparse activations (e.g., ReLU) to estimate EMA on-rates. However, many state-of-the-art architectures, including modern Transformers, predominantly use smoother and less sparse activations such as GELU or Swish. This may constrain the practical applicability of the method unless the definition of on-rate can be adapted for dense activation functions.\n- Comparative analysis pruning methods which uses activation signals (e.g., WANDA) is missing. A discussion and empirical comparison would help clarify how the proposed approach differs in principle and performance when activations drive the pruning signal.\n- Lack of empirical evidence for hardware-aligned pattern. While the authors acknowledge the need for structured sparsity projection (Sec 2), empirical validation of this deployment strategy would strengthen the practical claims. Specifically, demonstrating that N:M projection preserves the model performance would be valuable."}, "questions": {"value": "- Could the authors clarify whether the proposed method can be applied to state-of-the-art models such as large language models, which predominantly use dense activations like GELU? If so, empirical evidence in such settings would strengthen the claim of architectural generality.\n- What is the definition of $\\bar{p}$ in the figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gOx13hTuUN", "forum": "X62UhAvmi6", "replyto": "X62UhAvmi6", "signatures": ["ICLR.cc/2026/Conference/Submission22315/Reviewer_oGem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22315/Reviewer_oGem"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898673404, "cdate": 1761898673404, "tmdate": 1762942166131, "mdate": 1762942166131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes budgeted broadcast defining a neuron’s cost as traffic. By imposing a budget on this traffic, the proposed method forces neurons to specialize, either be highly active to a few neurons or quietly active to many neurons, but not both. This aims to create sparse networks by preventing any single neuron from dominating information flow, thus protecting potentially important but rarely active neurons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper propose a new axis of pruning, cost (traffic), rather than just looking at the importance of each neuron as in existing works. This drives neural networks to develop in to structures that play a more efficient and diverse role through simple traffic budget rules.\n2. Effect of protecting low active but highly important neuron: Some neuron which is barely activate but giving output on some rare and important property can be pruned in existing saliency aware pruning. However, in budgeted broadcast, since this type of neurons have low traffic cost, it can be retained with high fan out.\n3. The effect of budgeted broadcast to information of the network: The total traffic can be viewed as information that the network process. As a result, constraining the total traffic can be seen as limiting the amount of information which leads the network to learn core information while restricting redundant information."}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "1. While cost (traffic) constrained pruning can give an opportunity to low activity and high fan out neuron, in constrast to saliency based pruning, it can ignore the important neuron that can affect to the performance of the neural network. For example, in budgeted broadcast, the case of neurons having high activity and high fan-out cannot be considered due to tradeoff provided by local budget.\n2. How can we set optimal budget threshold? Do we have to sweep to find the optimal threshold for every new case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k7FZaRweyh", "forum": "X62UhAvmi6", "replyto": "X62UhAvmi6", "signatures": ["ICLR.cc/2026/Conference/Submission22315/Reviewer_LRR2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22315/Reviewer_LRR2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899090611, "cdate": 1761899090611, "tmdate": 1762942165906, "mdate": 1762942165906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Budgeted Broadcast introduces a new axis for network efficiency, traffic-based pruning, grounded in biological energy constraints and constrained-entropy theory.\nThe authors claim to achieve competitive or superior accuracy at matched sparsity across diverse tasks, to improves rare-event performance, and to offers a principled way to link neuron activity and connectivity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper reframes pruning from a parameter-importance problem into a resource-allocation problem, grounded in biology. Instead of deciding which weights are useful, it asks: how much “energy” can each neuron afford to broadcast? This shift is conceptually powerful, it introduces metabolic efficiency as a first-class design principle for artificial networks.\nThe resulting “traffic budget” $t_i = a_i * k_i$ creates a concrete, interpretable tradeoff between neuron activity (functional demand) and connectivity (structural cost).\nThis perspective unifies biological realism, efficiency, and interpretability in a way few pruning methods attempt. It doesn’t just compress models - it offers a mechanistic explanation for how efficient representations might self-organize.\nThe derivation from constrained entropy maximization is elegant and self-consistent.\nThe selectivity–audience balance $log\\frac{1-a_i}{a_i} = \\beta k_i$ emerges as a measurable equilibrium - not an arbitrary heuristic.\nThis gives pruning a predictive theory (you can test whether a trained network obeys the balance line) rather than merely an empirical recipe. The connection between traffic and mutual information provides an intuitive justification: controlling broadcast capacity limits redundant information flow. \nMost sparsification techniques are justified post hoc. Here, the authors build the method top-down from first principles, giving it unusual conceptual coherence.\nThis balance of theory and practicality makes BB accessible - it’s implementable without modifying training objectives or requiring gradient-based importance metrics.\nOverall, BB stands out for its theoretical depth, conceptual originality, and empirical coherence. It combines biological realism with computational pragmatism, providing both a mechanistic theory and a working algorithm. Its main strength is not just performance, but explanatory power - it shows why efficient networks might evolve toward diverse, energy-balanced representations."}, "weaknesses": {"value": "The theoretical derivation relies on simplifying assumptions - weak activation correlations, bounded edge energies, and Gaussian noise - that seldom hold in deep networks with normalization layers, residual paths, or nonstationary activations. The resulting selectivity-audience balance is therefore plausible but not guaranteed to emerge under realistic nonlinear dynamics.\nThere is no formal convergence proof showing that the iterative pruning–regrowth process minimizes the proposed entropy-constrained objective.\nWhile the mathematics is elegant, its validity in modern deep architectures remains heuristic. The theoretical link between entropy maximization and the actual mask updates could break in practice.\nCritical parameters such as the traffic threshold $\\tau$, the inverse-temperature $\\beta$, and the refresh interval $\\Delta$ require manual tuning per task. There is no adaptive or theoretically principled mechanism for setting these values. Small changes to these hyperparameters can affect sparsity patterns and final accuracy. It undermines the self-organizing spirit of the approach - a rule meant to represent automatic homeostasis still depends on careful manual calibration.\nThe connection between total traffic $\\sum a_ik_i$ and mutual information $I(Z;Y)$ is qualitative, based on a very loose upper bound. No empirical estimates or ablations directly measure how BB affects actual information flow, entropy, or redundancy between layers.\nSince the core justification is information efficiency, the lack of empirical validation of that claim leaves a theoretical gap.\n\nAlthough 4domains are tested, all benchmarks are medium-scale. No large-scale or high-capacity models are evaluated. Reported gains (often 1-3%) are promising but within statistical noise; no significance testing or error bars are provided. The efficiency gains are reported in terms of sparsity, not actual speedups on hardware. Without large-scale or runtime evidence, claims about “efficiency” remain conceptual rather than practical.\nThe dual pruning mechanisms (SP-in for dendritic, SP-out for axonal pruning) are theoretically motivated, but experiments emphasize only SP-in. There is no detailed analysis of how the two interact, nor whether combining them improves or destabilizes training. The dual-controller mechanism is central to the biological analogy but remains underexplored empirically, weakening the claim of symmetry between input and output homeostasis.\n\nThe \"natural Top-k reselection\" for regrowth is ad hoc and not theoretically tied to the entropy objective. The dynamics of pruning-regrowth cycles are not studied; it’s unclear whether BB reaches a stable equilibrium or oscillates around one. Without analyzing these dynamics, it’s difficult to assert that the pruning process is truly \"self-balancing\" rather than just stochastic.\n\nFinally, the conceptual clarity is occasionally overshadowed by mathematical compression. The paper proposes unstructured pruning; hence, real-world speedups on GPUs or edge hardware remain minimal. Authors mention potential mapping to structured or N:M sparsity, but this is speculative. Without hardware-aware results, it’s unclear how much of the claimed \"efficiency\" translates into deployable gains."}, "questions": {"value": "1) How biologically realistic is the \"traffic budget\" model $t_i = a_ik_i$?. Does it reflect metabolic constraints observed in neural circuits, or is it mainly a conceptual analogy?\n2) Can the selectivity-audience balance $log\\frac{1-a_i}{a_i} = \\beta k_i$ be derived without strong independence and Gaussian assumptions? How robust is this relationship in deep nonlinear networks?\n3) Is there any theoretical or empirical evidence that the pruning–regrowth process converges to the predicted equilibrium, or does it oscillate over time?\n4) How is the global budget parameter $\\beta$ determined in practice, and could it be learned automatically rather than tuned manually?\n5) To what extent does BB improve real computational efficiency (runtime or energy use), given that it currently produces unstructured sparsity?\n6) How sensitive is the method to hyperparameters such as the threshold $\\tau$, refresh period $\\Delta$, and degree limits $d_0, D$?\n7) What is the distinct contribution of the traffic rule itself compared to existing dynamic pruning or sparse training methods?\n8) Why does BB particularly benefit rare or long-tail features? Is this explicitly enforced by the balance rule or an emergent effect of local regulation?\n9) Can the traffic-budget principle be extended to structured or N:M sparsity to achieve hardware-level acceleration without losing its homeostatic behavior?\n10) Does controlling traffic actually maximize information efficiency as claimed by the mutual-information bound, and can this be empirically verified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mPZJOjZwVZ", "forum": "X62UhAvmi6", "replyto": "X62UhAvmi6", "signatures": ["ICLR.cc/2026/Conference/Submission22315/Reviewer_Xhnb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22315/Reviewer_Xhnb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22315/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926303845, "cdate": 1761926303845, "tmdate": 1762942165705, "mdate": 1762942165705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}