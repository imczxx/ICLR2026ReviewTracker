{"id": "Z8JNliulCr", "number": 22573, "cdate": 1758332975134, "mdate": 1759896858655, "content": {"title": "Cross-Lingual Multimodal Retrieval-Augmented Generation for Open Question Answering in Tamil and Yoruba", "abstract": "As large language models (LLMs) with retrieval-augmented generation (RAG) gain traction in multimodal knowledge-base question answering (KBQA), concerns about their transfer to low-resource languages (LRLs) remain unaddressed. We introduce LR-MMQA, a benchmark assessing multimodal cross-lingual retrieval and reasoning under the challenges of LRLs. Using a state-of-the-art LLM, we translated the hardest questions from WebQA and MultimodalQA, creating a dataset that stresses cross-evidence aggregation and multi-hop inference. We also introduce XM-RAG, a cross-lingual multimodal RAG pipeline optimized for LRLs, which achieves 38.1 answer accuracy overall, over 6.3 points higher than the next best baseline. Our findings expose significant biases and discrepancies in existing systems, with LR-MMQA highlighting specific failure points. Notably, XM-RAG's performance on LR-MMQA is far below top models on English datasets (WebQA: 64.4, MultimodalQA: 73.48 answer accuracy), demonstrating that current methods still fail at complex, real-world tasks in LRLs. By releasing LR-MMQA and XM-RAG, we provide a resource to evaluate and address these gaps and guide progress toward equitable multimodal KBQA.", "tldr": "", "keywords": ["multimodal question answering", "cross-lingual retrieval", "low-resource languages", "knowledge base question answering", "retrieval-augmented generation", "benchmark dataset", "Tamil", "Yoruba", "multilingual evaluation", "visual reasoning", "cross-modal fusion", "machine translation", "language equity", "computational linguistics", "information retrieval"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f5ed380f7658dd7eb732c73dc84054d2e5d3613.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Based on the existing WebQA and MultimodalQA datasets, this paper developed the knowledge-based question answering dataset LR-MMQA for Tamil and Yoruba through difficult sample selection, filtering, and post-processing. Next, the authors designed a training-free method, XM-RAG, which directly encodes the original low-resource language query without translation. The retrieved image and text evidence are then compressed and fed into mT5 along with the query and language tags to obtain the answer. Results on LR-MMQA show that XM-RAG outperforms several other baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Exploring multimodal RAG methods for knowledge-intensive questions in low-resource languages ​​is significant and offers new prospects for research and application."}, "weaknesses": {"value": "1. In the method comparisons of Table 2: (1) For methods such as GPT-4o that are not specifically designed for low-resource language scenarios, the questions in low-resource languages ​​should be translated into English for answering, and then the answers should be translated back to the target language to reflect the advantages of this method over translation-based methods, both in terms of performance and efficiency; (2) More open-source/closed-source general-purpose MLLMs should be included, such as Gemini-2.5 flash/pro, Claude, Qwen-2.5/3-VL, InternVL-3/3.5, etc. (3) For RAG methods, recent advanced methods such as text-only Search-R1 should also be compared.\n2. Regarding method design: (1) Why not input the image caption and then combine the passage and question into an older language model mT5, instead of some of the latest LLMs such as Qwen3 or LLaMA4? (2) Why not directly input the original image (without caption process), passage and question into an MLLM, such as the Qwen-2.5/3-VL mentioned above?\n3. The meanings of $d_i$, $s_i^{text}$, $v_j$, and $s_j^{img}$ in L252-254 are not explained.\n4. In retrieval, the commonly used metric seems to be R@k. How are the precision, recall, and F1 used in this paper defined?"}, "questions": {"value": "The reference format used in this paper seems unusual. For example, \"text-only methods Suri et al. (2025); Chen et al. (2022); Ling et al. (2025)\" in L34 is often written as \"text-only methods (Suri et al., 2025; Chen et al., 2022; Ling et al., 2025).\"\nI have some concerns about the method and experimental comparisons in this paper, which I have detailed in the weaknesses section. I would be happy to discuss this with the authors to further consider my final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FIIBukZJjA", "forum": "Z8JNliulCr", "replyto": "Z8JNliulCr", "signatures": ["ICLR.cc/2026/Conference/Submission22573/Reviewer_Bb6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22573/Reviewer_Bb6z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625092314, "cdate": 1761625092314, "tmdate": 1762942286220, "mdate": 1762942286220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper creates LR-MMQA, a new multi-modal cross-lingual knowledge-base question answering dataset by translating 718 queries from English WebQA and Multimodal QA datasets to Tamil and Yoruba (documents are still in English). They then propose a multimodal RAG baseline consisting of 1) M-CLIP encoding of the query and image; 2) cross-modal retrieval via FAISS indices; 3) cross-modal ranking based on a textual and visual similarity; 4) late fusion using BLIP-2 to generate two-sentence visual summaries and compressing high-resource language passages; 5) answer generation with mT5. The proposed method is shown to outperform text-only and monolingual RAG baselines, a closed-book GPT-4o, and RAGVL adapted with MT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The study tackles an important and challenging problem, multi-modal cross-lingual retrieval.\n2. The authors evaluates their method on two under-studied linguistically diverse low-resource languages, Tamil and Yoruba.\n3. The study presents a new dataset for low-resource languages.\n4. The proposed method outperforms the baselines."}, "weaknesses": {"value": "1. The created dataset is created using translations. As a result it is biased towards English and Western-centric knowledge and does not reflect authentic question that speakers of Tamil and Yoruba would actually ask. There are already a lot of translation-based datasets; what the community is lacking is datasets containing queries reflecting the real-world needs of speakers of under-represented languages.\n2. By only translating the queries and using English ground truth documents, the proposed dataset side-steps the real-world challenge of dealing with source documents in multiple languages, which was tackled in prior work such as XOR-QA.\n3. The newly proposed RAG pipeline is only evaluated on the new dataset so it’s unclear to what extent it has been tuned for this setup and how well it generalizes to other settings. It should be evaluated on the original WebQA and MultimodalQA for comparison to the English setting as well as other multilingual multimodal QA datasets such as Kaleidoscope ([https://arxiv.org/abs/2504.07072](https://arxiv.org/abs/2504.07072)), WorldMedQA-V ([https://arxiv.org/abs/2410.12722](https://arxiv.org/abs/2410.12722)), or CulturalGround ([https://arxiv.org/abs/2508.07414](https://arxiv.org/abs/2508.07414)).\n4. The ablation study only ablates a single setting, the impact of the cross-encoder reranker. Other factors such as the choice of different encoding or answer generation methods, multimodal fusion and evidence compression, and the heuristic reranking are not ablated. So it’s unclear which aspects are actually important in this setting.\n5. The proposed RAG baseline consists of several components, many of which have been used in prior work. It's unclear which aspects are novel and how they compare to design choices in prior work."}, "questions": {"value": "1. None of the examples shown in the Appendix include images. How important is the visual understanding component as part of the overall task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kmYw8p78C9", "forum": "Z8JNliulCr", "replyto": "Z8JNliulCr", "signatures": ["ICLR.cc/2026/Conference/Submission22573/Reviewer_gfwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22573/Reviewer_gfwX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688704762, "cdate": 1761688704762, "tmdate": 1762942285943, "mdate": 1762942285943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LR-MMQA, a benchmark for cross-lingual multimodal question answering in low-resource languages. The dataset is constructed by identifying and translating the hardest question-answer pairs from WebQA and MultimodalQA into Tamil and Yoruba, two typologically and semantically distant low-resource languages. The paper further proposes XM-RAG, a cross-lingual multimodal retrieval-augmented generation pipeline, which shows improved answer accuracy compared to baseline systems on this benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark targets low-resource multimodal QA, a setting that is currently understudied.\n\n- The choice of Tamil and Yoruba introduces high linguistic diversity, which improves the benchmark’s ability to test cross-lingual robustness.\n\n- The proposed XM-RAG pipeline demonstrates measurable improvements in answer accuracy over existing systems on this benchmark."}, "weaknesses": {"value": "- Many of the techniques described in Section 4 are already well-established, so the methodological contribution is limited.\n\n\n- The resulting dataset is small, especially given the multimodal cross-lingual QA setting, and would likely benefit from further expansion.\n\n\n- Because the dataset is directly translated from WebQA and MultimodalQA, it inherits entity and cultural biases. Inspecting the dataset showed that the questions included content with the Point Skyhawks logo and Danish Vikings. These questions may not reflect the cultural distributions of Tamil or Yoruba user queries.\n\n\n- There is no comparison to other multilingual QA datasets, which would help contextualize the contribution.\n\n- The experiments are also limited to very few models. Expanding the work to cover models is needful.\n\n- Some relevant citations appear missing, including the original WebQA and MultimodalQA citations, as the primary is needed."}, "questions": {"value": "1. In line 173, did you intend to mention BARTScore instead of BARTScore? Citing the actual metric would be useful to the readers.\n\n2. Section 4 is stretched across multiple pages. Could this section be condensed or made more concise to better reflect the actual contribution? Most of the bullet points there are not needed.\n\n$~$\n\n### **Suggestions**\n\nCVQA (Romero et al., 2024) might be a better starting point. It includes culturally grounded entities across multiple regions and may help increase cultural relevance and diversity in your dataset. It also contains questions for Tamil that you can benchmark against.\n\n$~$\n### **References**\n\nRomero, David, et al. *\"CVQA: Culturally-diverse multilingual visual question answering benchmark.\"* arXiv preprint arXiv:2406.05967 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xvQvVMSFGT", "forum": "Z8JNliulCr", "replyto": "Z8JNliulCr", "signatures": ["ICLR.cc/2026/Conference/Submission22573/Reviewer_fXNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22573/Reviewer_fXNk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791254293, "cdate": 1761791254293, "tmdate": 1762942285563, "mdate": 1762942285563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark LR-MMQA for evaluating multimodal cross lingual retrieval and reasoning in a low resource language scenario for Tamil and Yoruba. The paper also propose a possible RAG pipeline for low resource cross lingual and multimodal setting, where the  KG is in high resource language and the query is in low resource language. The main contributions of this paper are:\n1. A new benchmark (translation based benchmark) to evaluate cross lingual retrieval and reasoning.\n2. A cross lingual multimodal RAG pipeline for low resource languages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the paper are as follows:\n1. Human validation for filtering out flawed queries from the original WebQA and MMQA datasets.\n2. Involving humans during validation of the final translated dataset will ensure quality .\n3. Good idea to have a multilingual, multimodal benchmark for evaluating cross lingual retrieval and reasoning."}, "weaknesses": {"value": "The main weaknesses of the paper are as follows:\n1.  Machine translated data lacks cultural and social aspects of the language and it also lacks originality. \n2. Machine translation errors might creep in, reducing quality of the benchmark dataset.\n3. The scale of the benchmark dataset is very small, 718 instances might be too less. \n4. Challenge query selection based on performance of just Two QA models might not be the correct approach as question challenging to these two baselines might not be challenging at all.\n5. Benchmark availability for only limited languages (to be precise only  two languages)\n6. In XM-RAG fastest might have its own errors, which might have propagated.\n7. Limited Novelty of XM-RAG pipeline.\n8. Multimodal fusion might not be required for WebQA subset of dataset, but it is presented as a general sub-module of the pipeline .\n9.Performance comparison with strong baselines are missing."}, "questions": {"value": "1. Performance of frontier models on text subset of LR_MMQA is surprisingly low, have you done some prompt tuning and used RAG properly to test?\n2. How is the performance using sonnet model?\n4. Retrieval and re-ranking performance evaluation using ranking based metrics like NDCG@10 is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Nu73tPFAYO", "forum": "Z8JNliulCr", "replyto": "Z8JNliulCr", "signatures": ["ICLR.cc/2026/Conference/Submission22573/Reviewer_pZap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22573/Reviewer_pZap"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985483561, "cdate": 1761985483561, "tmdate": 1762942285191, "mdate": 1762942285191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}