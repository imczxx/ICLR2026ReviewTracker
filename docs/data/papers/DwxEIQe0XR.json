{"id": "DwxEIQe0XR", "number": 20513, "cdate": 1758306937523, "mdate": 1759896973905, "content": {"title": "Language Bottleneck Models: A Framework for Qualitative Cognitive Diagnosis", "abstract": "Accurately assessing student knowledge is central to education. Cognitive Diagnosis (CD) models estimate student proficiency, while Knowledge Tracing (KT) methods excel at predicting performance over time. However, CD models represent knowledge concepts via quantitative estimates on predefined concepts, limiting expressivity, while KT methods often prioritize accuracy at the cost of interpretability. We propose Language Bottleneck Models (LBMs), a general framework for producing textual knowledge state summaries that retain predictive power. LBMs use an encoder LLM to produce minimal textual descriptions of a student's knowledge state, which a decoder LLM then uses to reconstruct past responses and predict future performance. This natural-language bottleneck yields human-interpretable summaries that go beyond the quantitative outputs of CD models and capture nuances like misconceptions. Experiments show zero-shot LBMs rival state-of-the-art CD and KT accuracy on synthetic arithmetic benchmarks and real-world datasets (Eedi and XES3G5M). We also show the encoder can be finetuned with reinforcement learning, using prediction accuracy as reward, to improve summary quality. Beyond matching predictive performance, LBMs reveal qualitative insights into student understanding that quantitative approaches cannot capture, showing the value of flexible textual representations for educational assessment.", "tldr": "", "keywords": ["Cognitive Diagnosis", "Knowledge Tracing", "Education", "Interpretability", "LLMs", "Language Bottleneck Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/185d3d56f864e33f9c267912300c79d15739ad3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Language Bottleneck Models (LBMs) for cognitive diagnosis. \nAn encoder LLM compresses a student’s interaction history into a free-form text summary of their knowledge state; a decoder LLM then uses only this summary to predict future performance. This work instantiates LBMs, evaluate zero-shot and with RL (GRPO) fine-tuning of the encoder using decoder accuracy as reward, and report competitive accuracy vs. strong baselines on a synthetic arithmetic benchmark and two real datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Casting knowledge-state modeling as a language bottleneck is interesting and novel, different from fixed concept spaces in CD and learning embeddings in KT. The diagram on page 2 (Fig. 1) clearly positions LBMs against CD/KT and motivates the bottlenecked interface.\n- The design choice to freeze the decoder and focus learning on the encoder (rewarded by downstream accuracy) is elegant. Showing near-perfect decoding when given oracle summaries isolates where the difficulty lies."}, "weaknesses": {"value": "- The contribution “we cast knowledge state modeling as an inverse problem” overclaims a bit: this framing is long-standing in cognitive diagnosis/psychometrics. Section 2.2 centers CD and deep KT but does not mention at all about classic Bayesian KT (BKT/IRT/HKT/PSIKT)s that already posit latent states generating responses. \n- Figure 4 is overloaded (many colors/markers/linestyles) and almost impossible to read given that is the main result figure, making the sample efficiency story hard to parse. Please split into subplots (by backbone or family), or group KT vs. CD vs. LBM, and simplify. \n- I am a bit confused by those quite different evaluations. The decoder is prompted to output “Yes/No”, and for open-source models AUC is computed from logits of those tokens, whereas closed-source models are parsed from text. This apples-oranges setup can bias AUC and calibration. Recommend a unified probability extraction (verbalizer sets or logit bias) and reporting ECE/Brier in addition to accuracy/AUC. Also, CD is evaluated with same-student 80/20 splits, while KT/LBM use unseen-student evaluation and fixed |Y|=4. This makes cross-family comparisons too tricky to understand. \n- LLMs further evaluation\n    - LLMs require question text, many KT/CD datasets provide only IDs. The authors acknowledge this limitation but did not test robustness to degraded text. Please add controlled paraphrase/noise/ablation tests (e.g., ID+short stem, masked tokens) to quantify sensitivity and deployment feasibility on ID-only platforms.\n    - I assume LLMs' produced summary is hugely dependent on some shallow statistics/temporal correlation in the data. Please test whether LBM-identified “understands X/Y, not Z” correlates with simple signals (e.g., per-concept past success rates, transition matrix) or add human-rater studies on faithfulness/actionability. If strong correlations exist, I am not quite sure what LBMs add beyond well-tuned regressions. \n    - Without explicit priors/dynamics, LLM summaries may be temporally inconsistent (e.g., “mastered X” then “forgot X” within a short window). Classical Bayesian models enforce coherence via latent dynamics. Consider at least check the consistency, to make sure the inconsistency is more coming from prompting/LLMs randomness. \n- The trained-encoder experiment (GRPO) shows more gains on synthetic data, but fine-tuning can be expensive. Do you have a continual/online training strategy (e.g., periodic LoRA updates, replay buffers, cost budgets) as student data grows, and how much is the marginal gain per additional student?"}, "questions": {"value": "Could you please answer each point in the weaknesses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2IWhfZAivx", "forum": "DwxEIQe0XR", "replyto": "DwxEIQe0XR", "signatures": ["ICLR.cc/2026/Conference/Submission20513/Reviewer_aM4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20513/Reviewer_aM4W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740553540, "cdate": 1761740553540, "tmdate": 1762933937286, "mdate": 1762933937286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors suggest going through an text-based intermediate representation in natural language to provide interpretable knowledge tracing. They show their method matches or underperforms existing approaches on 1 synthetic and 2 real datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I felt that the proposed approach was interesting."}, "weaknesses": {"value": "The authors keep stating that existing approaches just provide \"quantitative skill mastery estimates\" or \"uninterpretable latent representations\" but the proposed approach, which does rely on embeddings that are similar latent representations, may hallucinate, and the authors do not provide any qualitative assessment of the generated explanations.\n\nThe authors write:\n> \"Finally, recent LLM-based approaches have shown promise for knowledge tracing tasks (Li et al., 2024a; Wang et al., 2025), but they generally remain opaque, either treating LLMs as black boxes or relying on\nmodel-generated explanations susceptible to hallucination.\"\n\nI don't see how the proposed approach is not also treating LLMs as black boxes nor wouldn't be susceptible to hallucination.\n\n\"rigid predefined KC taxonomies\" is too vague (I suspect this is generated by LLM), and repeated over the text. I assume the authors mean that the q-matrix needs to be provided, but as the authors state it themselves, some neural approaches for cognitive diagnosis can learn the q-matrix.\n\n\"unintepretable latent representations\" But nothing prevents the authors from trying to interpret a posteriori a learned vector by an existing deep learning approach for knowledge tracing.\n\n> \"The largest performance gap arises on XES3G5M. However, this dataset has an average accuracy of 85%, implying that even a constant predictor would achieve 85% accuracy.\"\n\nOne way to avoid this is looking at AUC, which is what is done in Table A1.\nIt also means that the proposed LLMs are performing worse than a constant predictor (with respect to accuracy).\nTable A1 seems to indicate that models like gemma-3-27b seem to have 0.78 AUC which is among the top AUC, while their accuracy is .33 among the worst one. This should be discussed.\n\nIn the appendix:\n> Compare to Cognitive Diagnosis which assumes a constant knowledge state, Knowledge Tracing method aim at estimating evolving knowledge states as students answer questions. We similarly review\n\nThis sentence seems incomplete. Also it should start with \"Compared\". Another typo: \"[KT] methods\".\n\nMinor: in section 1358 the authors put IKT and QIKT in the same paragraph but those models are very different, and QIKT is not meant to be interpretable."}, "questions": {"value": "In the synthetic dataset, who wrote the ground truth knowledge? Is it curated by a human or yet another LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CEKU19EQXz", "forum": "DwxEIQe0XR", "replyto": "DwxEIQe0XR", "signatures": ["ICLR.cc/2026/Conference/Submission20513/Reviewer_mQiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20513/Reviewer_mQiF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820495504, "cdate": 1761820495504, "tmdate": 1762933936996, "mdate": 1762933936996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a project on creating a model -- a language bottleneck model, for analyzing students' skills and misconceptions through a combination of two large language models. The first model serves as the encoder, encoding the students' past learning history into natural language explanations, and then uses another LLM to decode it into the original submissions as well as predictions on the next submissions. They performed predictions on a synthetic dataset, as well as two public datasets, and showed that overall the results are better in most base models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Overall this paper is relatively easy to follow and read. The motivation is easy to understand.\n+ The design of the model is straightforward. Under the context provided by the authors, the design makes sense."}, "weaknesses": {"value": "- One major issue of the paper is about evaluation. \n -> 1) While the design of the model is centered around interpretability (multiple places are showing this, including introduction, discussions, and the design considerations, etc.), there is no systematic evaluation of this perspective. While the case studies give a peek at the performance, and it is looking good, it still lacks formal evaluations. In some cases, it might be a case that some natural language summarization could be incorrect or not interpretable, but still decoded correctly. A more careful look is needed.\n -> 2) The result of accuracy, if interpreted correctly, seems to be similar to CDM, even with large language models. Although it saves the total number of seen questions, it is still not a major improvement motivated in this work.\nOverall, the work is interesting, and the results may use better presentations to be more relevant to the motivations of the work.\n- There are also some minor issues with the narratives of the work, listed in the questions below."}, "questions": {"value": "- Line 12: The goal of KT, though, is still to estimate students' skills.\n- Line 15: This only applies to DKT models. For BKT models, they have clear interpretability.\n- Line 41: Again, the one you cited from Corbett is BKT, and it does not have vector representations for knowledge -- it's just a set of statuses representing whether students know certain knowledge or not. It is quite interpretable.\n- Line 50: The following statement should be fine even outside of the CD domain.\n- Line 170: This is not convincing -- since the decoder part can produce good results already, then why wouldn't we make it better?\n- Line 177: At this point, readers start to wonder what exactly the knowledge state will look like in natural language. For some tasks, like open-ended problems, it is just hard to reconstruct the exact same answers, no matter how good the LLM is.\n- Line 306: Preprocessing of datasets should not be in Appendix as it is necessary for replication. It is an integral part for a research paper to be validated.\n- Line 469: So the quality of the summary should be systematically evaluated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dzc8zjEJUN", "forum": "DwxEIQe0XR", "replyto": "DwxEIQe0XR", "signatures": ["ICLR.cc/2026/Conference/Submission20513/Reviewer_n3oN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20513/Reviewer_n3oN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946615028, "cdate": 1761946615028, "tmdate": 1762933936503, "mdate": 1762933936503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Language Bottleneck Models (LBMs) for representing student knowledge states in natural language. The encoder LLM produces concise textual summaries of a student’s knowledge, and a decoder LLM reconstructs past responses and predicts future performance solely from that text. This transforms the traditionally quantitative representations used in cognitive diagnosis and knowledge tracing into interpretable textual summaries. The authors evaluate LBMs on synthetic, Eedi, and XES3G5M datasets, showing that zero-shot LBMs achieve performance comparable to state-of-the-art KT/CD models while offering interpretability and qualitative insight into misconceptions. The paper further explores reinforcement learning to refine summaries and demonstrates steerability through prompt or reward shaping"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The encoder–decoder LLM design is interesting, especially that the decoder reconstructs past responses and predicts future performance solely from the textual bottleneck.\n2. The work demonstrates a strong theoretical framing that connects cognitive diagnosis, knowledge tracing, and language bottlenecks in a coherent way, supported by extensive experiments across diverse datasets."}, "weaknesses": {"value": "1. The knowledge state representation defined by coarse textual categories such as Mastered, Fails on, and Misconceptions (Figure 5) may lose important intermediate information—for instance, differences between mastery levels of 0.6 and 0.7. Moreover, extracting precise concept-level interpretations from free-form text can be ambiguous due to synonymy and linguistic variability.\n2. The compared CD and KT baselines do not include recent LLM-based variants[1,2,3], which may lead to an incomplete assessment of the proposed method’s relative performance and limit the fairness of the comparison.\n3. There has limited analysis on scalability and cost of the two-stage LLM setup in real deployments.\n4. Some sections (e.g., 5.4) could include more statistical rigor on variance and significance.\n5. Missing user or teacher evaluation of the interpretability claims (qualitative human study).\n6. The code is not released, which may lead to difficulties in reproducing the paper\n\n```\n[1] Wang Z, Zhou J, Chen Q, et al. LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction[J]. arXiv preprint arXiv:2502.02945, 2025.\n[2] Dong Z, Chen J, Wu F. Knowledge is power: Harnessing large language models for enhanced cognitive diagnosis[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(1): 164-172.\n[3] Li H, Yu J, Ouyang Y, et al. Explainable few-shot knowledge tracing[J]. Frontiers of Digital Education, 2025, 2(4): 34.\n```"}, "questions": {"value": "1. How does the method handle long student histories given LLM context limits?\n2. Would joint training of encoder and decoder yield better interpretability or stability?\n3. Could LBMs extend to evolving knowledge states (non-static) for longitudinal modeling?\n4. What are the compute and cost implications compared to KT baselines for large-scale deployments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DGj3B13vAE", "forum": "DwxEIQe0XR", "replyto": "DwxEIQe0XR", "signatures": ["ICLR.cc/2026/Conference/Submission20513/Reviewer_q3Yi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20513/Reviewer_q3Yi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994438223, "cdate": 1761994438223, "tmdate": 1762933935977, "mdate": 1762933935977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}