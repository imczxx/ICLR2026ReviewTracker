{"id": "bKcBgNiREu", "number": 8708, "cdate": 1758095572373, "mdate": 1759897768540, "content": {"title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time", "abstract": "This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. We will release the source code upon the official publication of the paper.", "tldr": "", "keywords": ["text-to-image generation", "low-rank adaptation", "multi-concept generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/678fb8f2def4905987bd9d9cd050ef40f39b916b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a method for personalized multi-subject generation via LoRA fusion. Given several LoRAs, each trained on a specific subject separately, the goal is to generate a single image containing multiple personalized subjects.\nThe proposed method consists of two main stages. In the first stage, a mask is generated for each subject, indicating the image regions where the corresponding LoRA should be applied. In the second stage, these masks are used to fuse the predictions from the different LoRAs by multiplying each LoRA's output with its corresponding mask.\n\nMerging multiple LoRAs using masks is a well-known technique in the community (https://github.com/lifeisboringsoprogramming/sd-webui-lora-masks?tab=readme-ov-file). Therefore, the main contribution of the paper lies in its automatic mask generation approach, which requires no user input or external model.\n\nThe method is evaluated both qualitatively and quantitatively against several approaches for personalized multi-subject generation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The results presented in the paper are of high quality, demonstrating good identity preservation and in most cases plausible prompt adherence.\n- The paper provides relevant background on competing methods and clearly differentiates itself from the baselines.\n- A large gallery of qualitative results is presented, effectively showing the method's performance.\n- The paper includes multiple quantitative metrics and shows consistent advantages across all of them."}, "weaknesses": {"value": "- Section 3.1 lacks clarity in both reasoning and presentation:\n    - If the goal of this subsection is to demonstrate that Equation 4 holds, it would be more direct to compute each term in the equation across multiple images, timesteps, and layers, and then show their similarity. The discussion about the queries, keys, values, and feed-forward layers seems unnecessary for this purpose.\n    - In Figure 4, the average MSE loss when disabling to_q and to_k appears comparable to the loss when disabling to_v. While the qualitative example shows a difference, this is based on a single sample. Therefore, I am not fully convinced by the conclusion supporting Equation 2.\n    - Moreover, it is unclear why, if the LoRA outputs are typically 1-2 orders of magnitude smaller than the base model's outputs, they would affect the Q,K layers differently from the V,FF layers. This further raises doubts about the validity of Equation 2.\n    - The computation in Figure 3a is not entirely clear.\n\n- Given that the method is easy to implement (this is an advantage), it would strengthen the paper to include experiments on additional base models (e.g., SD, SDXL) for a fairer comparison with existing baselines.\n\n- Since the main contribution lies in the mask extraction, this part should be evaluated more thoroughly and compared with other techniques. For instance, how does it perform relative to a simple average of attention maps across timesteps and layers, where the queries correspond to image pixels and the keys correspond to subject-related prompt tokens (as in Prompt-to-Prompt)? There are also related approaches such as Readout-Guidance and Self-Guidance that could serve as baselines for mask extraction.\n\n- The paper only demonstrates results for two subjects, whereas other works (e.g., Mix-of-Show) show examples with more subjects. \n\n- In some examples, prompt adherence appears weaker than in competing methods. For example, on page 15, in the top example, the faces are not dusted with flour, and in the 7th row, the faces are not smeared with color as specified in the prompt."}, "questions": {"value": "See the questions in the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "thONoBVaeD", "forum": "bKcBgNiREu", "replyto": "bKcBgNiREu", "signatures": ["ICLR.cc/2026/Conference/Submission8708/Reviewer_hgmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8708/Reviewer_hgmv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760792059537, "cdate": 1760792059537, "tmdate": 1762920512576, "mdate": 1762920512576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of feature conflicts in multi-subject text-to-image generation when fusing multiple subject LoRAs during joint inference. Existing methods often require retraining, auxiliary segmentation models, user-defined prompts/regions, or pre-inference LoRA weight merging—limiting their practicality. To solve this, the authors propose FreeFuse, a training-free framework tailored for Diffusion Transformers (DiTs) that enables seamless multi-LoRA fusion via automatically derived subject masks.\nFreeFuse requires no additional training, no modifications to existing LoRAs, no auxiliary models, and only needs users to provide LoRA activation words. Experiments on FLUX.1-dev show it outperforms baselines (LoRA Merge, ZipLoRA, OMG, Mix-of-Show, CLoRA) across metrics: e.g., achieving a VLM score of 74.03 (vs. 57.74 for Mix-of-Show) and a 10-Pass LVFace score of 0.4685 (vs. 0.4417 for Mix-of-Show). It also excels in complex subject interactions (e.g., hugging, whispering) that prior methods struggle with."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike methods requiring retraining (Mix-of-Show) or auxiliary segmentation models (OMG), FreeFuse operates entirely at test time. It needs no LoRA modifications, no user-defined region prompts, and only requires LoRA activation words—enabling seamless integration into standard text-to-image workflows.\n- FreeFuse addresses key flaws of attention-based mask extraction (e.g., attention sink, noisy pixel-wise maps) via heuristic filtering, self-attention locality exploitation, and superpixel voting. This ensures masks are accurate and spatially coherent without human intervention.\n- Most existing multi-LoRA fusion methods (e.g., ZipLoRA, CLoRA) are designed for UNet-based models. FreeFuse targets DiTs (e.g., FLUX.1-dev), filling a critical gap in supporting state-of-the-art transformer-based diffusion models.\n- Experiments use diverse metrics to evaluate identity preservation (LVFace), feature similarity (DINOv3), human preference (DreamSim, HPSv3), and prompt adherence (VLM). It compares against 5 strong baselines and validates on complex interaction scenarios—strengthening the credibility of its effectiveness."}, "weaknesses": {"value": "- FreeFuse’s core application scenario (generating multi-subject interaction images, e.g., hugging, face-to-face talking) overlaps heavily with methods designed for character relationship synthesis (e.g., DreamRelation). However, the paper fails to cite or compare with such works, leaving its novelty relative to state-of-the-art interaction-focused generation methods unclear.\n- All experiments rely on a small, fixed set of subjects (e.g., daiyu_lin, haoran_liu, Harry Potter, Rihanna). There is no validation on more diverse identities—such as subjects of different ethnicities, ages, artistic styles (e.g., cartoon vs. photorealistic), or non-human subjects (e.g., animals, fictional creatures). This limits the demonstration of FreeFuse’s generalizability.\n- The paper excludes recent training-free multi-LoRA fusion methods beyond K-LoRA (e.g., latest variants of LoRA merging or dynamic gating approaches). This incomplete comparison may overstate FreeFuse’s advantages by ignoring competing methods with similar practicality.\n- The authors explicitly acknowledge that FreeFuse degrades when the number of subject LoRAs increases. As each LoRA’s masked region shrinks, features from other LoRAs are more likely to intrude—making the method ineffective for scenarios with 5+ subjects.\n- In scenes with heavy subject overlap (e.g., two people embracing with intertwined limbs), the attention-based masks may fail to accurately separate individual subjects. This leads to residual feature conflicts that FreeFuse cannot resolve."}, "questions": {"value": "- Why were methods for character relationship synthesis (e.g., DreamRelation) not compared? How does FreeFuse’s performance on multi-subject interaction tasks differ from these methods, especially in terms of interaction naturalness and identity preservation?\n- What is the reason for using only a small set of fixed identities in experiments? If tested on more diverse subjects (e.g., elderly individuals, non-Western ethnicities, cartoon characters), would FreeFuse’s metrics (e.g., LVFace similarity, mask accuracy) remain stable, or would performance degrade?\n- The paper notes performance issues with many LoRAs, but no potential solutions are proposed. Could dynamic mask resizing, multi-scale attention fusion, or adaptive LoRA activation weights address this limitation?\n- Can FreeFuse be adapted to other DiT models (e.g., Stable Diffusion 3) or UNet-based models? If so, would the mask generation step (e.g., layer selection, denoising step) require significant adjustments?\n- How would FreeFuse handle scenes with extreme subject overlap (e.g., a group hug with 3+ people)? Is there a strategy to improve mask accuracy in such cases, such as integrating lightweight geometric cues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BBWzxpyPrl", "forum": "bKcBgNiREu", "replyto": "bKcBgNiREu", "signatures": ["ICLR.cc/2026/Conference/Submission8708/Reviewer_nDjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8708/Reviewer_nDjA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761131963470, "cdate": 1761131963470, "tmdate": 1762920512215, "mdate": 1762920512215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FreeFuse, a training-free and segmentation-free framework for multi-subject text-to-image generation by fusing multiple subject LoRAs directly at inference. Instead of retraining or merging LoRAs, FreeFuse derives context-aware subject masks from cross-attention maps and applies them to LoRA outputs, mitigating feature conflicts during joint inference. The key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights, which well approximate the case where each LoRA is integrated into the diffusion model and used individually. FreeFuse extracts these masks from a single attention block and denoising step, achieving efficiency advantages over prior methods such as CLoRA, OMG, and Mix-of-Show. FreeFuse outperforms several baselines on DINOv3, DreamSim, HPSv3, and Gemini-2.5 VLM metrics, with notably higher VLM score (74.03 vs 57.74 for Mix-of-Show)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem motivation and theoretical grounding: This paper identifies intense competition among LoRAs in key subject regions as the source of failures in joint inference, supporting it with cosine-similarity visualizations of latent interference.\n\n2. Elegant and efficient formulation: The core mathematical argument formally justifies why masking LoRA outputs approximates isolated inference, showing that locality of attention ensures near-identical representations inside the mask.\n\n3. Attention-based automatic mask extraction: The pipeline introduces attention-sink filtering and superpixel-level voting to ensure spatial coherence. Importantly, it requires no retraining, no LoRA modification, and no external segmentation.\n\n4. Strong empirical evaluation: Evaluation uses five complementary metrics: DINOv3, DreamSim, LVFace, HPSv3, and Gemini-2.5 VLM, covering both perceptual similarity and human preference alignment. FreeFuse achieves the highest VLM score (74.03) and DreamSim 10-pass (0.8052), showing superior realism and consistency.\n\n5. Comprehensive ablation studies: Fig. 7 clearly isolates the effect of attention-sink handling, self-attention maps, and block-level voting; omitting any step causes visible artifacts, reinforcing the necessity of each design."}, "weaknesses": {"value": "1. Lack of runtime and resource benchmarks: Implementation details mention 37s per image on a single L20 GPU, but omit comparisons with CLoRA/OMG or multi-step variants, making it hard to quantify efficiency gains.\n\n2. Potential bias toward photorealistic scenarios. Evaluation prompts emphasize intimate, realistic human interactions. It remains unclear whether FreeFuse generalizes to style LoRAs, cartoons, or abstract concepts.\n\n3. Scalability is unclear with more subjects. Yet no quantitative evidence is provided for how performance degrades beyond two subjects."}, "questions": {"value": "1. Generalization beyond human characters — All evaluation prompts involve human pairs. Have you tested FreeFuse on object+character or style+subject LoRA fusion (e.g., “anime character + van Gogh style”)? If so, do auto-masks still localize meaningfully?\n\n2. Attention-sink filtering parameters — In Eq. 6, you fix p = 1 %. Did you tune p for different resolutions or datasets? How sensitive are final masks and performance to this threshold?\n\n3. Scalability with subject count — Could you report quantitative degradation (e.g., VLM / LVFace scores) for 3-, 4-, and 5-subject scenes to illustrate the scaling limit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Dqfy7TxC4", "forum": "bKcBgNiREu", "replyto": "bKcBgNiREu", "signatures": ["ICLR.cc/2026/Conference/Submission8708/Reviewer_xeE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8708/Reviewer_xeE2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983500825, "cdate": 1761983500825, "tmdate": 1762920511808, "mdate": 1762920511808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FreeFuse, a training-free framework for multi-subject text-to-image generation that enables fusing multiple LoRA modules without retraining, external models, or manual prompt engineering. The key insight is that cross-attention maps in diffusion transformer (DiT) models contain sufficient spatial locality to derive subject-specific masks automatically. These masks are then applied to constrain each LoRA’s effect to its relevant region, mitigating feature conflicts during joint inference.\nThe method is implemented in two stages: (1) mask extraction via filtered attention maps and superpixel-based voting, and (2) mask-guided inference where LoRA outputs are masked at each denoising step. Extensive experiments on FLUX.1-dev demonstrate that FreeFuse outperforms prior works such as OMG, CLoRA, Mix-of-Show, and ZipLoRA across quantitative metrics (DINOv3, DreamSim, LVFace, HPSv3, and VLM scoring). The approach is practical, efficient, and requires no modification to existing LoRA modules"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is entirely training-free, does not modify LoRAs or base diffusion models, and integrates seamlessly with existing workflows. This makes it extremely relevant for real-world adoption. The primary strength of this work lies in its \"plug-and-play\" nature. By eliminating the need for any additional training, external models, or manual user intervention (like region specifications), FreeFuse presents a highly practical solution to a common problem. Using cross- and self-attention maps for automated subject mask generation is a simple yet powerful concept. Theoretical analysis showing that masked LoRA outputs approximate isolated inference adds credibility."}, "weaknesses": {"value": "1. The most significant weakness is the method's unproven scalability. All qualitative examples in the paper demonstrate a fusion of exactly two subject LoRAs. The authors explicitly concede this limitation in the conclusion, stating that the method's core premise gradually becomes invalid as the number of subject-LoRAs increases. For a paper on multi-subject fusion, the lack of any experimental validation (even as a failure case analysis) for three or more subjects is a major omission.\n2. The mathematical justification (Eq. 4) is based on empirical assumptions about attention locality and LoRA perturbation magnitude. While reasonable, it remains an approximation rather than a rigorous derivation.\n3. Most baselines (OMG, Mix-of-Show, CLoRA) were implemented on U-Net diffusion models. Evaluating FreeFuse against more recent DiT-native multi-LoRA methods would strengthen the claim of DiT superiority."}, "questions": {"value": "1.Does FreeFuse perform equally well on non-human or abstract LoRAs (e.g., style or object-based ones)? Are the masks still meaningful in such cases?\n2. The masks are computed once (step 6) and reused. How stable are these spatial associations across later denoising steps? Would re-computation improve consistency?\n3. Can the authors report actual runtime and memory overhead compared to baseline inference? The claim of “one-step extraction” suggests efficiency, but quantitative figures would help.\n4. Have the authors explored hierarchical or recursive masking strategies for cases with more than three subjects? Such an extension could broaden the method’s applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DXL4gRArno", "forum": "bKcBgNiREu", "replyto": "bKcBgNiREu", "signatures": ["ICLR.cc/2026/Conference/Submission8708/Reviewer_54Vm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8708/Reviewer_54Vm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998683071, "cdate": 1761998683071, "tmdate": 1762920511484, "mdate": 1762920511484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}