{"id": "11z2ZV85eV", "number": 17914, "cdate": 1758281971088, "mdate": 1763729518865, "content": {"title": "Morpheus: Learning to Jailbreak via Self-Evolving Metacognition", "abstract": "Red teaming is a critical mechanism for uncovering vulnerabilities in Large Language Models (LLMs). To scale this process beyond manual efforts, research has shifted towards automated red-teaming. However, existing automated red-teaming approaches are fundamentally limited by their reliance on static and predefined attack strategies. This strategic rigidity renders their attacks predictable and brittle, leading to a significant performance degradation when targeting today’s highly-aligned models. To overcome this limitation, we introduce a new paradigm framing red-teaming attacks from a static prompt-search problem into one of learning a self-evolving attack policy over a multi-turn conversation. Specifically, we propose Morpheus, an agent that operationalizes this paradigm by learning to attack via *self-evolving metacognition*. At each conversational turn, Morpheus engages in explicit metacognitive reasoning; it leverages feedback from an external Evaluator to critique its current strategy, diagnose the target’s defenses, and dynamically evolve its attack strategy. Extensive evaluations on 10 frontier models (including O1, GPT-5-chat, and Claude-3.7) behaviors demonstrate that Morpheus establishes a new state-of-the-art. It achieves superior generalization, maintaining high Attack Success Rates (ASR) of 76.0% on O1 and 78.0% on GPT-5-chat, outperforming leading multi-agent baselines by margins of 29% to 62% on difficult targets. Crucially, Morpheus achieves this robustness with remarkable efficiency, reducing token costs by 1.4$\\times$ to 10.6$\\times$ compared to search-based methods. Furthermore, analysis against 5 modern defenses reveals that Morpheus effectively penetrates static safety alignment by dynamically evolving its reasoning trajectory, highlighting a critical need for inference-time defense mechanisms.", "tldr": "", "keywords": ["Self-Evolving Agents Metacognition LLM Jailbreak Red-Teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bc12ebe3ba02d97e9e0771b6d5bc6382fe3977c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "### Summary\n\nThis paper introduces **Morpheus**, a novel agent for automated LLM red-teaming that overcomes the limitations of current static, predefined attack methods. Existing approaches fail to generalize against modern, highly-aligned models. Morpheus reframes the problem from a static search to learning a **self-evolving attack policy** during a multi-turn conversation. It uses a \"training-free,\" dual-agent architecture where a **Metacognitive Attacker** generates attacks by reasoning (`<think>`, `<strategy>`, `<prompt>`), and a **Metacognitive Evaluator** provides dense, structured feedback. This closed-loop allows Morpheus to continuously analyze a target's defenses and adapt its strategy \"intra-test-time,\" enabling it to defeat even frontier models.\n\n### Key Contributions\n\n **A New Paradigm:** It formalizes jailbreaking as a problem of learning a *metacognitive policy* for strategic reasoning, rather than finding static attack vectors.\n\n **State-of-the-Art Efficacy:** Morpheus establishes a new state of the art, significantly outperforming existing methods by 42%-62% on frontier models like Claude-3.7 and O1.\n\n **Demonstrated Scalability:** It shows strong learning capacity, achieving near-perfect Attack Success Rates (100% on GPT-4o) with an increased interaction budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength lies in its exceptional clarity, as the proposed architecture and its mechanics are explained clearly and supported by comprehensive prompt examples."}, "weaknesses": {"value": "1. Limited Originality: The paper's dual-agent \"metacognitive\" architecture is not fundamentally new. It closely resembles existing and well-established multi-agent or self-correction frameworks, and its novelty is overstated.\n\n2. Contribution is Primarily Prompt Engineering: The \"self-evolving\" behavior seems to be an artifact of sophisticated prompt engineering rather than a novel learning algorithm. This limits the work's fundamental research contribution, as it's more of an implementation technique.\n\n3. Insufficient Experimental Validation: The empirical claims are weak. The experiments compare against too few baselines and, most importantly, are conducted on a small subset of only 50 behaviors, which is not large enough to be statistically robust or representative."}, "questions": {"value": "The paper frames the task as \"learning a self-evolving attack policy,\" but the current implementation relies on in-context reasoning (\"intra-test-time\") rather than persistent parameter updates. Have the authors considered using a formal Reinforcement Learning (RL) framework to **truly** train the Attacker agent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3zl1OIdK87", "forum": "11z2ZV85eV", "replyto": "11z2ZV85eV", "signatures": ["ICLR.cc/2026/Conference/Submission17914/Reviewer_a6XW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17914/Reviewer_a6XW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761401196111, "cdate": 1761401196111, "tmdate": 1762927732349, "mdate": 1762927732349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an automated red-teaming jailbreak framework for large language models (LLMs). Instead of utilizing a static prompt-search or handcrafted attack strategies, the authors introduce an agent called Morpheus that dynamically refines the attack strategy through self-evolving metacognition. Through comprehensive experiments, Morpheus is shown to achieve remarkable attack success rates, often surpassing existing methods by a large margin across multiple target LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The motivation of the proposed attack method is well-explained. The idea of self-evolving metacognition is interesting.\n\n+ The provided experiments are comprehensive and thoughtful.\n\n+ The proposed attack achieves remarkable success rates in jailbreaking LLMs, without sacrificing efficiency."}, "weaknesses": {"value": "- The evaluation setup of how ASRs are measured in the experiments is vaguely described\n\n- The key concepts of self-evolution and metarecognition are only elaborated on a conceptual level\n\n- No evaluations are provided with respect to defended LLMs"}, "questions": {"value": "The idea of developing self-evolving agents to automatically search for strong jailbreak prompts to black-box LLMs is quite interesting. Overall, the paper is fairly well-written, with comprehensive experimental results showing remarkable ASRs for the proposed method. Nevertheless, I have the following main concerns/questions regarding the evaluation setup and the design of the self-evolving agent:\n\n1. Based on the experiments, the proposed method achieves extremely high attack success rates (ASRs), often exceeding 90% across various target LLMs. While the results are impressive, the paper lacks full detail on how the ASRs are evaluated in the experiments. The only paragraph I found regarding this is in Lines 929-933, which states that the jailbreak prompt is evaluated using a separate GPT-4o judge and is considered successful if it obtains a score of 10. In my opinion, such a description is very vague, which may hinder the reproducibility of the experiments. What evaluation prompt is used? Is it the same as the one used for the evaluator in the attack framework? If relaxing the success threshold to a lower one (e.g., 8/10), how will the baseline performance change? Note that achieving a score of 10 could be somewhat stringent. The authors should clarify these questions; otherwise, the credibility of the reported ASRs can be questionable.\n\n    Additionally, relying on a single LLM to evaluate jailbreak success can be biased. It is recommended to conduct ablations using various LLMs for evaluation to ensure the soundness of the reported ASR statistics, and even conduct a small human study to validate the results.\n\n2. Conceptually speaking, the concepts of self-evolution and metacognition are definitely interesting. However, these concepts are only loosely defined and heuristically reflected in a complex prompt template. The paper does not explain the logic behind the development of the final prompt template. Questions, such as how exactly the attack strategy is iteratively adapted, how the defense strategy is reasoned, and what feedback signals are utilized, remain. Providing a working example of all the intermediate outputs (Appendix B) is partially helpful, but one would expect a more detailed illustration of the internal mechanism in the methodology section (instead of only high-level descriptions).\n\n3. The paper would be strengthened by testing how their attack method behaves against defended LLMs with different strategies (e.g., Llama Guard, self-reflection, SmoothLLM, etc.). It would be interesting if the proposed self-evolving attack agent could correctly infer the defense strategy and further refine its approach to bypass the defense."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "10GGUNWVhx", "forum": "11z2ZV85eV", "replyto": "11z2ZV85eV", "signatures": ["ICLR.cc/2026/Conference/Submission17914/Reviewer_dR3s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17914/Reviewer_dR3s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753594138, "cdate": 1761753594138, "tmdate": 1762927731863, "mdate": 1762927731863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Morpheus is an automated red-teaming agent that jailbreaks LLMs through self-evolving metacognition. It uses a dual-agent architecture: an Attacker that generates structured reasoning and an Evaluator that provides dense feedback. This enables real-time diagnosis of target defenses and dynamic strategy adaptation across multi-turn interactions without training. On HarmBench and AdvBench benchmarks spanning 10 models, Morpheus outperforms state-of-the-art baselines by 42-62% on highly-aligned models (Claude-3.7, o1), achieving near-perfect attack success rates (100% on GPT-4o, 98% on Llama3-8B) with sufficient interaction budget. The core contribution shifts jailbreaking from static prompt search to learning adaptive metacognitive policies that generalize across diverse model defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reframes automated red-teaming from static prompt search to learning a metacognitive policy over multi-turn dialogue.\n\nDemonstrates rigorous evaluation across different models and ablations, with strong performance in jailbreaking frontier models."}, "weaknesses": {"value": "The author mentions that \"current state-of-the-art multi-turn jailbreak attacks exhibit poor generalization\" (lines 73, 284). However, the author neither discusses state-of-the-art multi-turn methods such as X-Teaming (https://arxiv.org/abs/2504.13203\n) and ActorAttack (https://arxiv.org/abs/2410.10700\n), which demonstrate the effectiveness of open-source attacker models in jailbreaking nearly all frontier models, nor compares against these methods.\n\nThere is a need to clearly distinguish the novelty and contributions of this work from previous state-of-the-art methods such as X-Teaming and ActorAttack.\n\nAlthough the paper claims that \"our primary motivation is defensive, aiming to enhance LLM safety\" (Ethics section), it provides no analysis of critical questions, such as: What properties make models vulnerable to metacognitive attacks? What defense mechanisms could mitigate this class of attacks? Which existing defenses (e.g., X-Guard -https://arxiv.org/abs/2504.13203) are most or least effective?\n\nThe efficiency comparison appears unfair: Morpheus uses DeepSeek-R1-V2-528 (a reasoning model) along with a GPT-4o evaluator, whereas the baselines use standard models. There is no comparison against baselines with equivalent computational budgets (e.g., dual agents vs. single agent setups).\n\nThe approach also shows a heavy dependence on a single LLM evaluator."}, "questions": {"value": "How do you validate that a score of 10 truly represents a successful jailbreak rather than an evaluator failure? Has any human judgment been used to verify GPT-4o's scoring?\n\nWhat responsible disclosure and access control practices will be implemented to prevent the malicious use of Morpheus?\n\nDoes the choice of evaluator introduce model-specific bias? (https://arxiv.org/abs/2404.13076)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TAAPrAu978", "forum": "11z2ZV85eV", "replyto": "11z2ZV85eV", "signatures": ["ICLR.cc/2026/Conference/Submission17914/Reviewer_TQxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17914/Reviewer_TQxo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968678052, "cdate": 1761968678052, "tmdate": 1762927731548, "mdate": 1762927731548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Morpheus, an automated red-teaming agent for jailbreaking LLMs via self-evolving metacognition. Instead of relying on fixed adversarial prompts or static heuristic trees, Morpheus forms a dynamic attack policy inside a multi-turn conversation loop. At each turn, an Attacker agent generates `<think>, <strategy>, <prompt>` while a separate Evaluator provides structured feedback (score, justification, meta-suggestions). This feedback is then used to evolve the next-step reasoning strategy. Experiments on HarmBench and AdvBench across 10 open/closed LLMs show large gains over strong baselines (e.g., ActorBreaker, Crescendo), with 42–62% improvement on frontier models such as Claude-3.7 and O1 ."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear conceptual novelty.** The shift from static search to *intra-test-time self-evolving metacognition* is conceptually meaningful and practically compelling for red-teaming.\n2. **Strong empirical results.** Gains are substantial and consistent across multiple target models, especially against highly-aligned commercial systems.\n3. **Well-designed architecture.** The dual-agent structure (Attacker + Evaluator) is clearly motivated and evaluated with ablations.\n4. **Comprehensive analysis.** The paper provides efficiency, scaling, strategic diversity, and ablations, which deepen the credibility of claims."}, "weaknesses": {"value": "**Compute / cost overhead.** The dual-agent loop raises concerns about practical deploy-time cost, especially under highly-limited API budgets. While acknowledged in conclusion, more explicit expense and compute cost breakdown would be helpful."}, "questions": {"value": "It would be better if the authors could report the expense and compute cost for each config."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WCdG6tQggD", "forum": "11z2ZV85eV", "replyto": "11z2ZV85eV", "signatures": ["ICLR.cc/2026/Conference/Submission17914/Reviewer_kHsa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17914/Reviewer_kHsa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982495489, "cdate": 1761982495489, "tmdate": 1762927731133, "mdate": 1762927731133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}