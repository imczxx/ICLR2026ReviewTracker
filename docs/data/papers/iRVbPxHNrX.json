{"id": "iRVbPxHNrX", "number": 397, "cdate": 1756737979299, "mdate": 1763746026726, "content": {"title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning", "abstract": "Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.", "tldr": "", "keywords": ["MLLMs", "Fine-grained Visual Reasoning", "Visual Understanding", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/edf186f41d8e3721a4e53e2444b827f55b5e5b2c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of improving reasoning of multi-modal large language models. They use the REASONMAP dataset and show that directly performing GRPO on the dataset is not very effective for multi-modal reasoning because of high difficulty of questions and sparse rewards. To alleviate this, they construct REASONMAP-PLUS, an augmented dataset which easier VQA style queries with denser reward. They also show improvements in training MLLMs using GRPO by adding a curriculum (easy->hard) questions and by adding partial rewards to alleviate the problem of sparse rewards. They name this new recipe as REWARDMAP which shows performance gains on the test set of REASONMAP and gains of around 3.5% on other reasoning benchmarks as well."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shows gains using REWARDMAP on both in-distribution and out of distribution test sets, suggesting that the curriculum learning + denser reward works well for multi-modal reasoning training \n2. They construct an augmented dataset REASONMAP-PLUS with around 4k VQA pairs across different types like local counting, global counting, and true/false\n3. The difficulty and dense rewards are assigned programatically without the need for any reward models or human labelling which makes the method cheap and efficient"}, "weaknesses": {"value": "1. Incremental novelty: both curriculum learning (https://arxiv.org/abs/2506.06632, https://arxiv.org/abs/2501.12599, https://arxiv.org/abs/2502.14768) and denser rewards to improve reasoning are approaches which have been tried in several existing works, while this paper shows a combination which works well, the approach as a whole feels incremental. \n2. Reward shaping not generalizable: The detail reward (credit for route names, transfers) is tightly linked to transit-map structure; it’s unclear how the same scheme would extend to charts, floor plans, or natural images without such information. Likewise, the difficulty weights rely on a dataset-specific notion of map/question difficulty (e.g., transfer counts). It would be good to show a more generalizable approach to reward design. \n3. Reward shaping not ablated well: The reward seems ad-hoc, there are no clear ablations for how the value of alpha was chosen, nor are there ablations on how W_diffculty is decided, or why it is multiplied to the other rewards. It would be good to provide some additional experimental results as to how this particular reward structure was chosen. \n4. Limited OOD improvements: While the authors show performance on other benchmarks like MM-Star as well, the performance on them shows negligible gains. It might be more informative if there is a task wise breakdown of the performance which might help analyze the exact tasks where performance improves vs where it stagnates."}, "questions": {"value": "Please refer to the weakness section for questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7femZ9v9tw", "forum": "iRVbPxHNrX", "replyto": "iRVbPxHNrX", "signatures": ["ICLR.cc/2026/Conference/Submission397/Reviewer_ekjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission397/Reviewer_ekjh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760770875170, "cdate": 1760770875170, "tmdate": 1762915511305, "mdate": 1762915511305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studys the problem of sparse reward in standard reinforcement learning for spatial reasoning in structured and information-rich setting.  To address the problem, the authors construct REASONMAP-PLUS, a dataset with dense reward signals and propose a multi-stage RL framework for improving the corresponding capabilities in current MLLM. Experiment results demonstrate its effectiveness across various reasoning tasks and general tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research topic of this work is interesting, as sparse reward is indeed a significant challenge in current vision tasks, particularly in structured visual input tasks as highlighted in the paper.\n2. This work introduces the REASONMAP-PLUS dataset, which could be beneficial for future related research.\n3. The experimental results seem promising, demonstrating the effectiveness of the proposed method on transit map planning tasks and showing its transferability to other tasks."}, "weaknesses": {"value": "1. The method parts of the paper presents certain ambiguities. For instance, how does the Difficulty-Aware Weighting mechanism operate? Are weights assigned to each sample within the groups of GRPO, or to each sample across the entire training dataset? Furthermore, what does Multi-stage RL training entail—does it refer solely to the sequencing of data based on difficulty, or does it also encompass different training phases?\n2. The novelty of the approach appears limited, as the core methodology remains largely aligned with that of GRPO. While the design of the detailed reward function is interesting, it only considers specific components of the response (e.g., destination stops, route names), thereby constraining its overall flexibility."}, "questions": {"value": "1. In the acquisition of the detailed reward, how is the correctness of each sub-part determined? Is an external LLM employed to extract the relevant content, or is the response constrained to a fixed format?\n2. Please refer to the weaknesses highlighted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pPly4IedeC", "forum": "iRVbPxHNrX", "replyto": "iRVbPxHNrX", "signatures": ["ICLR.cc/2026/Conference/Submission397/Reviewer_owjV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission397/Reviewer_owjV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639253280, "cdate": 1761639253280, "tmdate": 1762915511111, "mdate": 1762915511111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RewardMap, a multi-stage RL framework that mitigates sparse-reward problems in fine-grained visual reasoning for multimodal LLMs. Building on ReasonMap, the authors introduce ReasonMap-Plus, a dense and difficulty-graded dataset for visual question answering on transit maps. RewardMap integrates (1) a difficulty-aware reward function combining format, correctness, and detail rewards, and (2) a multi-stage GRPO training curriculum progressing from perception to reasoning tasks. Experiments on RewardMap, ReasonMap-Plus, and six external benchmarks show consistent gains and reduced visual confusion, demonstrating improved visual understanding and reasoning performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. New dataset and training paradigm: the paper introduces ReasonMap-Plus, which is a novel dataset that helps conducting RL using dense reward signals.\n2. Models being trained on the dataset seem to perform well on the proposed benchmarks"}, "weaknesses": {"value": "1. Lack of baseline VLMs on ReasonMap and ReasonMap-Plus. What would more recent and powerful VLMs like GPT-4o and GPT-5 perform on these benchmarks?\n2. Would be better to train other VLMs other than Qwen2.5-VL, such as InternVL models to see if the training and data could actually be generalized."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LgUoq2JUjO", "forum": "iRVbPxHNrX", "replyto": "iRVbPxHNrX", "signatures": ["ICLR.cc/2026/Conference/Submission397/Reviewer_GZ1U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission397/Reviewer_GZ1U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794062367, "cdate": 1761794062367, "tmdate": 1762915510871, "mdate": 1762915510871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes REWARDMAP, a multi-stage reinforcement learning (RL) framework for improving fine-grained visual reasoning in multimodal large language models (MLLMs). It builds on the REASONMAP benchmark and introduces REASONMAP-PLUS, a companion dataset providing dense supervision via VQA-style tasks organized by difficulty. The approach combines a difficulty-aware reward design with curriculum-based RL under GRPO to mitigate sparse reward issues. Experiments show consistent but small gains (around 3.5%) on REASONMAP, REASONMAP-PLUS, and several external benchmarks. Ablations suggest that reward shaping and staged training both contribute to improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Addresses a critical problem of sparse rewards in RL for multimodal reasoning with a practical and elegant solution.  \n- Well-engineered multi-stage RL approach combining dense and sparse supervision effectively.  \n- Comprehensive evaluation with ablations that isolate reward design and curriculum effects.  \n- Clear writing, reproducible setup, and solid empirical results across multiple benchmarks."}, "weaknesses": {"value": "- Conceptual novelty is limited. The method is primarily a well-engineered combination of known ideas: reward shaping, difficulty weighting, and curriculum learning under GRPO. There is no new RL algorithm or theoretical contribution.  \n- Reported performance gains are numerically small relative to the additional dataset, compute, and engineering effort. It is unclear whether such modest improvements are statistically significant.  \n- The reward weighting scheme (α, γ_e/m/h, β_0/1) is ad-hoc and lacks sensitivity analysis. Without this, claims about “difficulty awareness” remain anecdotal.  \n- Evaluation is restricted to Qwen2.5-VL, raising concerns about generalization and overfitting to a single model architecture.  \n- REASONMAP-PLUS is narrowly scoped to transit maps; claims of general fine-grained reasoning ability are not convincingly supported.  \n- The paper lacks deeper insight or analysis into why multi-stage RL works, there is no visualization or error decomposition that would make the mechanism interpretable.  \n- The average improvement on external benchmarks (+3.47%) is within noise for modern MLLMs; effect size is weak."}, "questions": {"value": "1. How sensitive are results to the hyperparameters \\( \\alpha \\) and difficulty weights (\\( \\gamma_e, \\gamma_m, \\gamma_h \\))?  \n2. Could the proposed curriculum RL pipeline be applied to other structured domains (e.g., diagrams, charts)?  \n3. How do you ensure no data leakage between REASONMAP-PLUS easy/hard tasks and REASONMAP test sets?  \n4. Why were RLHF or DPO-based baselines not compared for reward densification?  \n5. Can you provide error-type or reasoning-chain analyses to explain where REWARDMAP helps most?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No significant ethical issues were found. The dataset is built from public transit maps and poses minimal privacy or safety risks. Potential copyright concerns should be clarified, but usage for research likely falls under fair use."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fZJDJAEpjb", "forum": "iRVbPxHNrX", "replyto": "iRVbPxHNrX", "signatures": ["ICLR.cc/2026/Conference/Submission397/Reviewer_3FQv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission397/Reviewer_3FQv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027755016, "cdate": 1762027755016, "tmdate": 1762915510501, "mdate": 1762915510501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}