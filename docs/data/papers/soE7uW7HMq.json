{"id": "soE7uW7HMq", "number": 6645, "cdate": 1757991174495, "mdate": 1759897902989, "content": {"title": "Aligning Inductive Bias for Data-Efficient Generalization in State Space Models", "abstract": "The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from less. A model's inductive bias is a critical lever for this, but foundational sequence models like State Space Models (SSMs) rely on a fixed bias. This fixed prior is sample-inefficient when a task's underlying structure does not match. In this work, we introduce a principled framework to solve this problem. We first formalize the inductive bias of linear time-invariant SSMs through an SSM-induced kernel, mathematically and empirically proving its spectrum is directly governed by the model's frequency response. Further, we propose a method of Task-Dependent Initialization (TDI): power spectrum matching, a fast and efficient method that aligns the model's inductive bias with the task's spectral characteristics before large-scale training. Our experiments on a diverse set of real-world benchmarks show that TDI significantly improves generalization and sample efficiency, particularly in low-data regimes. This work provides a theoretical and practical tool to create more data-efficient models, a crucial step towards sustainable scaling.", "tldr": "", "keywords": ["Data efficiency", "Inductive bias", "State Space Models", "Model initialization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2d982acf9d9a1a8de9cb2efba1897fa7f2ccf93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a framework for enhancing the generalization of state-space models via Task-Dependent Initialization. The methods is  motivated by the theory of RKHS induced by the convolutional kernel of SSMs. Experiments show that the method improves the test error when training data is limited."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is based on principled analysis via a RKHS. The theory is intuitive and the derivations seem accurate. Experiments on synthetic datasets well-align with the theory.\n* The task studied in the paper is important since high-quality data is limited.\n* Most experimental designs are fair (up to a couple of questions outlined in the questions section), and figures are well-made."}, "weaknesses": {"value": "* There is room for improvements in the presentation of the paper. In particular, the explanation of \"why the proposed objective helps\" is scattered around. As far as I can tell, the most important intuition is to have the kernel of SSM capture most energy in the kernel of data, but this is only loosely stated in several places without too much emphasis. To improve, Lemma 3.5 and Theorem 3.7 need to be made more precise, especially the \"thus improving the generalization ability\" part, which is currently hand-wavy while being arguably *the* most important statement of the paper.\n* Some assumptions are still too restrictive, e.g., the assumption that training data come from an i.i.d. distribution, and the sampling error $\\boldsymbol{\\epsilon}^\\mu$ follows the zero-mean and uniform-covariance assumption. I also have confusion regarding the role of $\\overline{f}$ and $\\boldsymbol{\\epsilon}$ in the proposed method. Please see questions below.\n* While the empirical validation on the long-range arena datasets shows some promise, the results are not strong enough to show the effectiveness of the method. For example,\n   * LRA is already considered a limited-data regime. The benefits of TDI emerge only when the data is further significantly limited, which limits the use cases of the method.\n   * Only benefits are shown on two tasks, and results on a few tasks in LRA (retrieval and text) are alway missing.\n   * I also have some further questions regarding the design of experiments. See below."}, "questions": {"value": "1. In section 2.3, should I interpret $\\overline{f}$ as the ground truth and $\\boldsymbol{\\epsilon}$ as measurement noises? If so, there are two follow-up questions:\n   1. In a classification task, the function $\\overline{f}$ is not necessarily well-defined. How would you reconcile this scenario, and how does this impact the further analysis in this paper?\n   2. To help \"generalization,\" one should aim to avoid overfitting of the measurement noises $\\boldsymbol{\\epsilon}$. I don't see this being discussed anywhere else in the paper. Is it hidden somewhere in the theory of section 3?\n2. The discussion in section 3 assumes that $L$ is fixed. A key thing about sequence models is that the model should handle sequences of varying lengths. How does the theory work in that case?\n3. How is the loss in (4) optimized and how efficient is it?\n4. In Figure 5a, why does the TDI method work worse than the default method when no training data is removed (ratio = 1)?\n5. In Section 4.2, you mentioned that only the first-layer initialization is changed. Why did you make this choice, and what if other layers are also initialized using TDI?\n6. Not critical but just out of curiosity: how easy is it to adapt the TDI method to Mamba, if you have any idea?\n7. Comment: $E_g = \\mathbb{E}_{\\mathcal{D}} [E_g(\\mathcal{D})]$ is a terrible recursive notation. Why don't you pull the definition of $E_g(\\mathcal{D})$ directly into it? Also, it seems like you never use this notation again in your paper. It would be better not to introduce notations that you will not use to avoid overloading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iGGR15mr6g", "forum": "soE7uW7HMq", "replyto": "soE7uW7HMq", "signatures": ["ICLR.cc/2026/Conference/Submission6645/Reviewer_nHSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6645/Reviewer_nHSM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760983276254, "cdate": 1760983276254, "tmdate": 1762918961682, "mdate": 1762918961682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies linear time-invariant (LTI) State Space Models (SSMs) using the framework of kernel regression. It introduces a novel SSM-induced kernel to formalize the model’s inductive bias, proving that its spectral decomposition is governed by the SSM’s frequency response (Theorem 3.3). This reveals that SSMs exhibit a frequency-dependent spectral bias, where modes with stronger frequency response are learned faster. Building on this, the authors propose Task-Dependent Initialization (TDI), a fast method that uses power spectrum matching to optimize initial SSM parameters before training, minimizing a loss between the model's and task's power spectra.\nEmpirical validation on both synthetic data and real-world data are provided to validate the theoretical result. \nIt is claimed that TDI enhances generalization and sample efficiency on real-world benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* It is novel to study SSM using the kernel regression framework. The SSM-induced kernel is new and interesting. Moreover, a connection between the spetra of the kernel and the SSM frequency response is made theoretically.\n\n* Empirical results are given to validate the theory."}, "weaknesses": {"value": "* The connection between the SSM-induced kernel and kernel regression framework is not made clearly. The introduction of the problem settings is unclear: what is the input ($x$ or $u$), what is output and what is the target function? The output seems to be 1-d at Section 2.1, $L$-dimensional in Eq.(1) and $d$-dimensional in Line 149. It is very confusing.\n\n* The empirical results are not convincing enough, particularly regarding TDI. While Figure 5 seems to show some advantage of TDI in some cases, the full results in Section B actually show that the reported generalization errors are somehow meaningless: the improvement only happens when the error is even larger than that at initialization. See, for example, Figure B.9. I believe the further results should be provided.\n\n* THe proof is suspicious. Theorem 3.4 claims the equality $s_\\rho = |H(\\omega_\\rho)|$, but from the proof in Section E we only have  asymptotically equivalence. I did not check other details."}, "questions": {"value": "1. How is SSM-induced kernel related to the kernel regression framework?\n\n2. Can you explain in detail the experiment setting of Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KKU5D6sIoo", "forum": "soE7uW7HMq", "replyto": "soE7uW7HMq", "signatures": ["ICLR.cc/2026/Conference/Submission6645/Reviewer_XUCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6645/Reviewer_XUCK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597764380, "cdate": 1761597764380, "tmdate": 1762918961131, "mdate": 1762918961131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a theoretical framework for improving the data efficiency of linear time-invariant State Space Models. The authors formalize an SSM-induced kernel, showing that its eigen-spectrum acts as the model’s inductive bias and is governed by the SSM’s frequency response. They also propose Task-Dependent Initialization (TDI) and a power-spectrum matching loss to align the model’s inductive bias with the task’s spectral characteristics before training. They showed in experiments that TDI accelerates cumulative power growth and improves generalization, demonstrating a principled way toward data-efficient initialization of SSMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper builds a clear and solid framework for understanding inductive bias in SSMs through an SSM-induced kernel perspective.\n2. Focusing on data efficiency is timely and meaningful, given increasing practical constraints on access to large, high-quality datasets."}, "weaknesses": {"value": "1. The experimental impact seems limited. In Figure 5, the data efficiency gains from TDI are not very consistent across training data ratio, with notable improvement only on Pathfinder.\n2. The evaluation scope is narrow: TDI is applied only to the first S4 layer on a small set of benchmarks, with no comparison to other initialization strategies. It’s hard to evaluate the scalability and general applicability of the proposed method."}, "questions": {"value": "1. Since minimizing the loss can be also viewed as a very efficient pretraining phase, what is the actual computational overhead in practice?\n2. Given that the model is essentially multi-layer with nonlinearity and the propagation of frequency structure across layers is not clear, why is aligning only the first-layer frequency response expected to yield global improvement? Is there any analysis of the first-layer representations in the experiments showing that TDI produces a more task-relevant feature space to support the claimed mechanism?\n3. The cross-spectrum estimate relies on sufficient data to stabilize. How sensitive is TDI to dataset size, especially in very low-data regimes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EuNmVZlYWO", "forum": "soE7uW7HMq", "replyto": "soE7uW7HMq", "signatures": ["ICLR.cc/2026/Conference/Submission6645/Reviewer_ZPAE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6645/Reviewer_ZPAE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726063056, "cdate": 1761726063056, "tmdate": 1762918960434, "mdate": 1762918960434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the inductive bias of SSMs via an SSM-induced kernel, connecting the kernel's spectrum to the model’s frequency response, and proposes Task-Dependent Initialization (TDI): pre-align the first SSM layer’s spectrum to a task’s cross-power spectrum before standard training. The suggested method leads to lower test loss on several tasks in low data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a clear lens for studying the bias of SSMs through the eigendecomposition of the reproducing kernel.\nThe theoretical analysis is used to derive a simple algorithm for initializing the first layer of an SSM with empirical results that support the papers claim to some extent - yielding lower test loss in a low data regime on some tasks."}, "weaknesses": {"value": "The analysis in sections 2 & 3 seem to utilize the properties of a general toeplitz matrix and not one that is induced by SSM kernel, which is a specific parameterization of a convolution kernel - if some of the assumptions made to derive the main result relies on the specific structure of SSMs it is not clear to me from the text.\nWhile I acknowledge that studying the theoretical inductive bias of architectures is important, modern practice typically involve a self-supervision phase that aligns the model parameters with the data structure, thus making the results less impactful. See for example [1] also showing gains in low data regimes by including self supervision.\n\n[1] Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"}, "questions": {"value": "1. Is the analysis in sections 2 & 3 restricted to SSMs alone and utilize their specific parameterization of the toeplitz matrix or does it apply to any convolution kernel?\n2. Can you include the accuracy on various tasks in section 4.2"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JbnNbezvnS", "forum": "soE7uW7HMq", "replyto": "soE7uW7HMq", "signatures": ["ICLR.cc/2026/Conference/Submission6645/Reviewer_XeUZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6645/Reviewer_XeUZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6645/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836945842, "cdate": 1761836945842, "tmdate": 1762918959846, "mdate": 1762918959846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}