{"id": "bzQvL797PS", "number": 14217, "cdate": 1758230391390, "mdate": 1759897383148, "content": {"title": "Entropy-Based Block Pruning for Efficient Large Language Models", "abstract": "As large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.", "tldr": "", "keywords": ["Large Language Model", "Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b81cc29e7cd403ab46b41e3585990d324c266fd4.pdf", "supplementary_material": "/attachment/a402d5c4c1f321edd38f0465cc7543e3aacc3bbc.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes EntroDrop, an entropy-based pruning method for large language models (LLMs) that identifies and removes redundant computational blocks (e.g., Attention or MLP blocks) within Transformer architectures. The authors first analyze the entropy dynamics of hidden representations across model layers, observing a consistent two-stage pattern: entropy decreases in early layers (information compression) and increases in later layers (information enrichment). Based on this finding, they introduce entropy increase as a pruning criterion to identify less informative blocks, arguing it better captures information richness compared to the commonly used cosine similarity. Extensive experiments on models like Llama3.1-8B and Mistral-7B-v0.3 across various benchmarks demonstrate that EntroDrop effectively reduces model size and accelerates inference while maintaining competitive performance, outperforming existing layer and attention pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The empirical analysis of layer-wise entropy dynamics provides a fresh and well-motivated perspective for structured pruning, moving beyond geometric similarity measures like cosine similarity.\n\n\n2.The paper includes thorough experiments analyzing different aspects, including the impact of calibration datasets, comparisons of entropy estimation methods (Bucket, KNN, Renyi), hyperparameter robustness, and actual inference speedup, providing strong empirical support."}, "weaknesses": {"value": "1.The paper omits citations of some layer pruning methods, such as LLM-Streamline[1] and Shortened llama[2].\n\n2.The proposed entropy-based pruning method does not consistently demonstrate clear superiority over cosine similarity-based approaches across all scenarios. While their performance is often comparable, and sometimes one method significantly outperforms the other, the authors could provide further insight into the specific conditions under which entropy-based pruning is more advantageous.\n\n3.Based on the above experimental observations, expanding the evaluation to include more models would be highly beneficial. This would help clarify the circumstances in which entropy-based pruning is preferable to cosine similarity-based methods.\n\n[1]Chen, Xiaodong, et al. \"Streamlining redundant layers to compress large language models.\" arXiv preprint arXiv:2403.19135 (2024).\n\n[2]Kim, Bo-Kyeong, et al. \"Shortened llama: A simple depth pruning for large language models.\" arXiv preprint arXiv:2402.02834 11 (2024): 1."}, "questions": {"value": "Has the author explored training the pruned models? Because a model that performs better after pruning does not necessarily mean it will still perform better after training. I believe such experiments are crucial to more definitively prove that entropy-based layer pruning is superior to cosine similarity-based pruning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s8F2oOjN73", "forum": "bzQvL797PS", "replyto": "bzQvL797PS", "signatures": ["ICLR.cc/2026/Conference/Submission14217/Reviewer_Mwbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14217/Reviewer_Mwbs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554738797, "cdate": 1761554738797, "tmdate": 1762924675561, "mdate": 1762924675561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on pruning for large language models for better model efficiency. Authors analized the LLM models and found the entropy dynamics across different layers are significantly different from different layers, which provide sound basis for the introduction of using entropy dynamics for model pruning. Authors have included this in the model design and compared with other pruning methods on multiple datasets, where the introduced method is showing very promising results for both Llama3.1-8B and Mistral-7B-v0.3 for both compresson on both whole transformer block and only on attention. Authors also show very detailed ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "+ Authors have included a complete analysis and solution pipeline for finding the possible reduction for compression and then introduce the solution to the pipeline.\n\n+ Authors have conducted extensive experiments on mulitple datasets for benchmark as well as providing comparison for two different LLM models, which provide the generalizability of the proposed compression method.\n\n+ Authors have provided comprehensive analysis for the sensitive analysis for different datasets on different model, which provide a pretty comprehensive analysis for the overall results."}, "weaknesses": {"value": "- I do not have a big concern, one of the small intersting fact - the proposed model seems to have different performance behavior for Llama3.1-8B and Mistral-7B-v0.3, where Mistral-7B-v0.3 is showing better performance when the number for the pruned blocks is limited, while it is not observed in Llama3.1-8B. This could be some internal model difference. It would be interesting if authors can apply this to some more models for further analysis across different models."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rtZ2cMWx5p", "forum": "bzQvL797PS", "replyto": "bzQvL797PS", "signatures": ["ICLR.cc/2026/Conference/Submission14217/Reviewer_Z5K4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14217/Reviewer_Z5K4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605859989, "cdate": 1761605859989, "tmdate": 1762924675028, "mdate": 1762924675028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntroDrop, an entropy-based block pruning strategy for Transformer language models. By monitoring the block-wise entropy variation during inference, the paper empirically identifies two distinct stages: the entropy of early layers gradually decreases, while that of later layers rises again. EntroDrop interprets this late-stage entropy increase as redundancy, and therefore prunes the k blocks with the smallest entropy change as a criterion. Experiments based on Llama3.1-8B and Mistral-7B-v0.3 on reasoning and comprehension benchmarks demonstrate that, under the same pruning ratio, EntroDrop achieves higher accuracy than baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides new insights into the information flow within large language models (LLMs) through an empirical analysis of the entropy dynamics across Transformer blocks.\n2. The authors propose a simple yet effective entropy-based pruning method, shifting from cosine similarity that captures geometric relationships to an entropy criterion that better reflects information content."}, "weaknesses": {"value": "1. The experiments in this paper primarily focus on multi-choice question answering. However, previous studies (e.g., LLM-Streamline [1]) have shown that similar pruning methods may cause significant performance degradation in generation tasks. It remains unverified whether EntroDrop suffers from the same issue. For instance, how does it perform on generative benchmarks such as XSum or GSM8K?\n2. EntroDrop is built upon the empirical observation of entropy dynamics, where entropy first decreases and then increases across layers. However, it is questionable whether such a pattern universally holds across different model families. Entropy-Lens [2], which also analyzes models using entropy, finds that entropy dynamics vary significantly among different architectures.\n3. One of EntroDrop’s main contributions is the use of entropy as a criterion to evaluate block importance. However, Yang et al. [3] also leveraged transfer entropy to assess the importance of blocks for pruning. Clarifying the similarities and differences between these two approaches would help better highlight EntroDrop’s unique contributions.\n\n[1] Compressing Large Language Models by Streamlining the Unimportant Layer. https://arxiv.org/abs/2403.19135\n\n[2] Entropy-lens: The information signature of transformer computations. arXiv preprint arXiv:2502.16570.\n\n[3] Let LLM Tell What to Prune and How Much to Prune. In Forty-second International Conference on Machine Learning."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b96TJEUsWC", "forum": "bzQvL797PS", "replyto": "bzQvL797PS", "signatures": ["ICLR.cc/2026/Conference/Submission14217/Reviewer_AtZA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14217/Reviewer_AtZA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615327843, "cdate": 1761615327843, "tmdate": 1762924674520, "mdate": 1762924674520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EntroDrop, an entropy-based pruning method for large language models. It computes the Shannon entropy of each layer’s hidden representations and measures the entropy increase between consecutive layers to estimate information contribution. Layers with smaller entropy gains are considered redundant and pruned. Experiments on Llama-3.1-8B and Mistral-7B-v0.3 show that EntroDrop can remove about 37.5% of layers while keeping around 95% of the original performance and greatly improving inference efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an entropy-increase–based pruning metric that more directly reflects information changes across layers, offering a more principled alternative to cosine similarity.\n2. It identifies a clear “entropy-decrease–then–increase” pattern in Transformer representations during inference, providing new empirical insight into information flow in LLMs.\n3. The method performs consistently well across Llama-3.1-8B and Mistral-7B-v0.3, showing robustness to different calibration datasets."}, "weaknesses": {"value": "1. The connection between entropy and information flow is mainly empirical, lacking a rigorous theoretical justification from an information-theoretic perspective.\n2. Experiments are limited to two models. More experiments on larger models and different model sizes can be included.\n3. The paper should include some post-training experiments to verify whether the performance of the pruned model can be recovered. Although the pruned model outperforms the baselines, there is still a gap compared to the dense model, which post-training could help address."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CWyjJc7R5B", "forum": "bzQvL797PS", "replyto": "bzQvL797PS", "signatures": ["ICLR.cc/2026/Conference/Submission14217/Reviewer_vwSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14217/Reviewer_vwSb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095193587, "cdate": 1762095193587, "tmdate": 1762924673981, "mdate": 1762924673981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}