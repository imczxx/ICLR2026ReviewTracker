{"id": "bvaaydGKYp", "number": 10748, "cdate": 1758180950288, "mdate": 1759897631885, "content": {"title": "From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory", "abstract": "Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent’s training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.", "tldr": "", "keywords": ["Large Language Model Agents ; Graph Memory; Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cbf3ea7143fd3d382533081b9d28376c99ba3aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the use of dynamic, structured explicit memory to guide and enhance the policy learning. The authors propose a method where a graph is constructed to represent logical connections between user queries, decision states, and higher-level strategic cognition. An RL-based approach is then designed to dynamically optimize the graph’s weights. Finally, the framework jointly optimizes both the graph weights and the policy in an end-to-end RL training pipeline, enabling effective utilization of prior experiences and enhancing the policy’s reasoning capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach introduces a novel way of leveraging a graph structure to represent logical connections between user queries, intermediate states, and abstracted experiences. This structured memory facilitates reasoning and strategic decision-making.\n\n2. The framework enables both memory graph weights and the policy model to be jointly optimized in an end-to-end manner using RL. This ensures that the memory graph is dynamically updated to reflect the latest logical flow, while the policy can effectively utilize the structured experiences to enhance its reasoning capabilities.\n\n3. The experiments provide promising results, demonstrating the effectiveness of the proposed approach in leveraging explicit memory for policy learning."}, "weaknesses": {"value": "1. **Limited Discussion of Related Work** \n\n    The paper lacks a detailed comparison with prior works, such as Expel and G-Memory.\n   * Compared to Expel, which also explicitly abstracts experiences and insights from past trials, the differences seem to lie in both the model's parametric updates and the structured representation of experiences.\n   * For G-Memory, both approaches utilize graph structures for memory representation, but the distinctions between these methods are not clearly articulated. A more thorough discussion of these works and how they compare to the proposed framework would strengthen the paper.\n\n2. **Clarity and Accessibility** \n\n    The overall method is difficult to understand due to overly abstract descriptions and unclear explanations. While the memory graph is the core contribution, its component, such as the \"Transition Path Layer,\" and \"Meta-Cognition Layer\", are too abstract, even with the additional details provided in the Appendix. The lack of intuitive examples makes it challenging to grasp key concepts, such as what constitutes a transition path or a state, and what qualifies as meta-cognition? Similarly, the visualization of the graph is vague and lacks sufficient detail. Important notations, such as \"ITR,\" which frequently appears in the main text, are not explicitly defined. Providing more intuitive examples and clearer writing would make the paper significantly more accessible.\n\n3. **Constrained Experiments** \n\n    The experimental setup is limited and does not fully align with the scenarios where the proposed approach could be most beneficial. Multi-hop QA (MHQA) tasks may not be the most suitable benchmark for evaluating how agents can benefit from past experiences. Other benchmarks that emphasize strategic planning and prior knowledge, such as WebShop, ALFWorld, or GAIA, BrowseComp, may better align with the paper's motivation. The MHQA tasks themselves are relatively simple, as most queries can be resolved with just 2–3 tool calls. The authors further limit the maximum number of tool calls to fewer than 10, which makes the tasks even less demanding and reduces the need for strategic planning. Alternatively, more challenging benchmarks, such as BrowseComp, could better demonstrate the proposed framework’s utility. Even within the MHQA tasks, the paper lacks illustrative examples showing how the retrieved subgraph contributes to answering queries. Including such examples would better demonstrate the practical effectiveness of the memory graph."}, "questions": {"value": "Please refer to the Weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wv0siYjdbn", "forum": "bvaaydGKYp", "replyto": "bvaaydGKYp", "signatures": ["ICLR.cc/2026/Conference/Submission10748/Reviewer_scsu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10748/Reviewer_scsu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760670493183, "cdate": 1760670493183, "tmdate": 1762921970886, "mdate": 1762921970886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a trainable, multi-layered graph memory framework designed to enhance the reasoning and adaptability of large language models. This structured memory addresses the challenges of limited interpretability and catastrophic forgetting in prior approaches. It represents an agent’s past experiences as structured decision paths, which are distilled into high-level, human-interpretable meta-cognitive strategies. Empirical results demonstrate the framework’s effectiveness and support its claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework is clearly described, making it easy to understand both its overall design and the role of each component. The key methods are thoroughly explained and supported with equations.\n\n2. The experiments are well designed and make use of state-of-the-art models. The selected tasks are thoughtfully chosen, providing broad and representative coverage. The ablation study effectively demonstrates the contribution of each component."}, "weaknesses": {"value": "1. The paper lacks an analysis of time consumption. Although the proposed method performs well as reported, there is no discussion of the additional computational overhead it may introduce. The method could potentially involve significant computational costs, which might limit its practical adoption compared to more efficient alternatives.\n\n2. The paper claims that employing a structured graph memory can “distill agent trajectories into high-level, human-interpretable strategic meta-cognition.” However, there is limited evidence showing how the graph (queries, transition paths, and meta-cognitions) evolves during training. Including such an analysis would strengthen the paper’s soundness and provide deeper support for the empirical results."}, "questions": {"value": "My questions are as follows, based on the weaknesses identified above:\n\n1. **Time analysis**: Could the authors provide a detailed runtime analysis—ideally broken down by component (e.g., graph construction, meta-cognition induction, and update)—to demonstrate that the proposed method achieves performance improvements without compromising practicality?\n\n2. **Graph visualization**: Is it possible to visualize the memory graph at different training stages to help readers better understand how the proposed method enhances interpretability and adaptability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BWvGr4PvBD", "forum": "bvaaydGKYp", "replyto": "bvaaydGKYp", "signatures": ["ICLR.cc/2026/Conference/Submission10748/Reviewer_z3UM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10748/Reviewer_z3UM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574475414, "cdate": 1761574475414, "tmdate": 1762921969568, "mdate": 1762921969568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a trainable, hierarchical graph memory for LLM agents that maps trajectories to FSM-based canonical paths and distills meta-cognitive strategies. It learns utility-weighted edges via a counterfactual reward gap (with/without strategy) and injects top-k strategies into RL training as a policy prior. On seven QA datasets, it reports gains in inference (Table 1) and training (Table 2), with memory constructed from HotpotQA only."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the factorisation of experience into strategy: The Q → FSM path → meta-cognition separation is a crisp abstraction that makes strategy-level retrieval interpretable\n\n- it goes beyond prior static memories (e.g., EXPEL/A-MEM) in conjunction with RL for LLM to learn which strategies matter"}, "weaknesses": {"value": "- FSM design and mapping are pushed to the Appendix, same for meta-cognition induction examples. The main text lacks one complete HotpotQA example showing: raw trace → FSM path → meta-cognition → prompt. Without it, claims like “preserves only semantically meaningful decision points” (lines 243–244) are unverifiable.\n- sec. 5.2 says “Detailed baseline configurations are provided in Appendix B.2,” but the baseline definitions (e.g., “Direct Trajectory,” “A-MEM,” “EXPEL,” “ITR”) and how they differ from your method are not spelled out in the main paper before reporting results. Also, metric (EM) is introduced only in the appendix\n- the rationale for using HotpotQA to define the FSM/strategy space isn’t explained, so that the portability to other QA datasets remains unclear\n- stage 2 seems to compute counterfactual gaps per candidate meta-cognition (Sec. 4.2), and the main text does not quantify how many meta-cognitions are sampled per query, whether both trajectories are rolled out per candidate, and the resultant overhead during both inference and training\n   - in addition, compare to no-memory and static-memory variants, overheads should be quantified, including FSM path construction, counterfactual evaluation, and strategy prompting in RL/inference\n- the Direct Trajectory baseline is close to your method in several settings (e.g., Qwen3-8B inference: 0.352 vs 0.365; training: 0.400 vs 0.408; Qwen3-4B training: 0.415 vs 0.426). This suggests much of the benefit might come from good retrieval/demonstration structure rather than the utility-learning per se. While Fig. 4a,b helps, but the paper would benefit from: a) statistical significance (multiple seeds) of the gaps; b) an ablation study keeping weight learning but replacing FSM with a simpler flat memory? to help attribute gains to graph structure vs utility learning.\n- which meta-cognitions are preferred where? The paper does not analyze which strategies dominate, how they interact in prompts, or how preferences evolve during RL. Given the emphasis on meta-cognitive strategies, at least a short main-text case study (with 2–3 concrete strategies, their frequencies, and per-dataset effects) would strengthen the value of meta-cognitions"}, "questions": {"value": "- what is ITR in the main text? Authors need to define ITR, Direct Inference, CoT, Direct Trajectory, A-MEM, and EXPEL in the main text, not in the appendix (one line each) and explain how they differ from your approach along the axes of (i) memory construction, (ii) retrieval, (iii) trainability, and (iv) training-time vs inference-only use\n\n- Is the FSM static across datasets? For a new QA dataset, which parts (states S, actions A, transitions T) must be rebuilt? Provide a worked HotpotQA example in main. How long are typical canonical paths? Any coverage statistics (e.g., % of trajectories that map cleanly to FSM)?\n\n- Sec. 4.1 mentions speculative meta-cognitions from Top-K neighbors when only failures occur. How do you prevent hallucinated or overly general strategies from polluting the graph?\n\n- Lines 258–262 (sec. 4.1) note \"reinforcing existing principles\", \"novel patterns\"... but the update rule is unspecified in the main. Could you elaborate on how nodes/edges are added/merged/removed?\n- When only failures exist, how do you prevent noisy/speculative strategies from degrading memory?\n\n- Counterfactual cost: Do you roll out both guided/unguided trajectories per candidate $m_k$? How many $m_k$ are sampled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mnfzvzz1aH", "forum": "bvaaydGKYp", "replyto": "bvaaydGKYp", "signatures": ["ICLR.cc/2026/Conference/Submission10748/Reviewer_w9eQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10748/Reviewer_w9eQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984939554, "cdate": 1761984939554, "tmdate": 1762921969094, "mdate": 1762921969094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to let an LLM agent adaptively learn from its experience by learning a graph memory of metacognition and extracting relevant metacognition as prompt during policy optimization. Experimental results show that the metacognition graph can generalize well to out-of-domain tasks, and helps the agent perform better on a broad range of tasks compared to baseline methods that learn from experience in a less structured way, either with policy RL optimization or not."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly motivated, and the proposed solution of constructing a metacognition graph is very intuitive. \n2. The method is explained in a clear way with enough details. \n3. The experimental evaluation is thorough, with consistent improvement on a broad range of tasks."}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "1. In stage 3, will your prompt with metacognition, i.e., $\\tilde{q}_{\\text{train}}$, remain unchanged when collecting more experience on a specific task? Or is it updated whenever a new episode of experience is collected?\n2. Any idea why after RL fine-tuning, the Qwen3-4B version performs better than the 8B version overall?\n3. Details about how you train the graph is important, and I think it's better to include these details in experiment section of the main text. \n4. Powerful LLM backends like GPT4-o and Gemini are only used during the first stage of your method right? Not during the third stage of policy optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u8XT0rqcDD", "forum": "bvaaydGKYp", "replyto": "bvaaydGKYp", "signatures": ["ICLR.cc/2026/Conference/Submission10748/Reviewer_m6Dm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10748/Reviewer_m6Dm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10748/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998965063, "cdate": 1761998965063, "tmdate": 1762921968637, "mdate": 1762921968637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}