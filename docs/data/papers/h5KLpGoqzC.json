{"id": "h5KLpGoqzC", "number": 23202, "cdate": 1758340856392, "mdate": 1759896827051, "content": {"title": "Hierarchical Semantic-Acoustic Modeling via Semi-Discrete Residual Representations for Expressive End-to-End Speech Synthesis", "abstract": "Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.  We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations.Our framework introduces a differentiable quantization bottleneck that induces natural specialization: a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. \nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. Trained on over 1 million hours of speech, our 0.5B-parameter model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. Audio samples are available at: https://voxcpm.github.io/VoxCPM-demopage/.", "tldr": "We propose an end-to-end hierarchical diffusion-autoregressive model that generates expressive and holistic speech via semi-discrete residual representations, eliminating the need for speech tokenizers.", "keywords": ["text-to-speech synthesis", "diffusion language model", "semi-discrete representations", "voice cloning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa0eb40b2fad6b31ecb5a0b630225c40d6452931.pdf", "supplementary_material": "/attachment/a3d28c69673d24110b4b524cfa39e629710e9a19.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a cascaded TTS model that predicts semantic tokens, refine with added residual latents and finally decoded into speech latents with a latent diffusion model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. While there are many similar cascaded architecture that predicts semantic tokens, refine with acoustic tokens/latents and finally decode into speech latents/waveform, there is some (but limited) novelty in using FSQ as a bottleneck layer directly after LLM output, rather than as in a pre-trained speech tokenizer. This does give some stability and flexibility.\n2. There is ablation study that investigate the impacts of the design choices."}, "weaknesses": {"value": "1. While the model does seem to perform better in term of WER, the speech naturalness performance seem to be a mixed bag. In Table 2, the model does not perform as well in DNSMOS metrics. Also in Table 3 the model underperforms/perform on-par on naturalness. In the evaluation section the authors tends to spent more time on the positive results but discuss little about the negative results. I also want to ask why some of the models evaluated in Table 2 are not presented in Table 3?\n2. The paper doesn't evaluate the expressiveness nor the controllability of the proposed model. Some competitors (e.g. CozyVoice and Higgs Audio) are focusing on these aspects, and it's an important direction in the field of TTS. I wonder if the authors have some results in this area?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eRBm0NnVI1", "forum": "h5KLpGoqzC", "replyto": "h5KLpGoqzC", "signatures": ["ICLR.cc/2026/Conference/Submission23202/Reviewer_MsWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23202/Reviewer_MsWh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760969185824, "cdate": 1760969185824, "tmdate": 1762942559493, "mdate": 1762942559493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents VoxCPM, a hierarchical model for zero-shot TTS. The authors point out that existing discrete token approaches, while stable, tend to lose subtle acoustic nuances, whereas fully continuous models preserve detail but often become unstable over longer utterances. VoxCPM addresses this by introducing a semi-discrete differentiable bottleneck that separates stable discrete, potentially linguistic and prosodic information, from finer acoustic details, avoiding the scalability issues of large discrete codebooks while keeping the system trainable end-to-end.\n\nThe model generates speech in stages: a text-semantic language model first captures content and prosody, this representation is regularized through a scalar quantization layer to form a coarse conditioning, and a residual acoustic module restores missing details such as timbre and micro-prosody. A local diffusion transformer then synthesizes the final latent audio segments, conditioned on both text and previously generated context. The latents are derived from a causal VAE.\nTraining is done jointly with a flow-matching objective.\n\nThe proposed method is technically sound and well-motivated, addressing a central bottleneck in hierarchical speech synthesis. The proposed semi-discrete, differentiable quantization + residual refinement framework feels like a natural evolution of current VALL-E-style architectures toward more continuous, end-to-end differentiable systems of hierarchical speech synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Cohesive hierarchical design: The decomposition into semantic (TSLM+FSQ), residual (RALM), and generative (LocDiT) components is conceptually elegant and grounded in recent hierarchical TTS models.\n\n2. Innovative FSQ usage: Using scalar quantization as a differentiable inductive bias (rather than discrete target) is novel—addresses VQ scalability while maintaining stability.\n\n3. End-to-end differentiability: Gradients flow through quantization and all components, unlike most multi-stage systems.\n\n4. Strong performance across benchmarks\n\n5. The t-SNE plots of the TSLM and RALM show a clear division of roles of the two stages."}, "weaknesses": {"value": "1. Complex multi-stage hierarchy:\nThe full pipeline—LocEnc → TSLM → FSQ → RALM → LocDiT—adds several interacting components and training dependencies. Despite being end-to-end, this design could be challenging to scale or fine-tune efficiently.\n\n2. Limited analysis of latency and efficiency:\nThe paper emphasizes real-time feasibility via a causal VAE but provides no measured latency or throughput results. It remains unclear how the diffusion-based LocDiT performs under streaming constraints.\n\n3. FSQ interpretability and scalability:\nWhile the ablation results show that the FSQ layer improves stability, plays a distinct role from the RALM, and remains largely speaker- and acoustics-agnostic, the claim that FSQ representations correspond to linguistic content is not directly supported by experimental evidence. This interpretation is plausible, but could be strengthened through probing analyses—such as phoneme prediction or ASR accuracy tests—to confirm that the FSQ indeed captures linguistic or prosodic structure. In addition, the paper does not discuss how well these semi-discrete representations transfer or scale when fine-tuned on different datasets, even within the same language domain.\n\n4. The diagram should include the mathematical notation to make it clearer which token/color corresponds to which notation in the equation and text."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EUKdFef3dE", "forum": "h5KLpGoqzC", "replyto": "h5KLpGoqzC", "signatures": ["ICLR.cc/2026/Conference/Submission23202/Reviewer_Nvjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23202/Reviewer_Nvjv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822369569, "cdate": 1761822369569, "tmdate": 1762942559089, "mdate": 1762942559089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VoxCPM, a framework that addresses speech expressivity and intelligibility challenges. Codec-based methods enable stable autoregressive generation but lose fine-grained acoustic details due to quantization, while continuous approaches accumulate errors over long sequences. VoxCPM combines a Text-to-Speech Language Model (TSLM) for generating semantic and prosodic plans with a Residual Acoustic Language Model (RALM) that restores fine acoustic details. Guided by these components, a local diffusion-based decoder produces high-fidelity speech latents. Experiments on two datasets show that VoxCPM achieves better results with fewer model parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a differentiable FSQ bottleneck that divides semantic–prosodic planning from acoustic rendering, maintaining full end-to-end trainability and overcoming the limits of both discrete and continuous methods.\n\n- The paper reduces computational cost by using a VAE-based model that works in a compact latent space rather than on raw waveforms. A causal design allows for low-latency, real-time streaming synthesis.\n\n- The paper shows that VoxCPM outperforms other open-source models on the experimented dataset using 0.5B parameters, outperforming larger models such as IndexTTS2 (1.5B) and HiggsAudio-v2 (3B)."}, "weaknesses": {"value": "- The paper combines existing components (pre-trained LLM, FSQ, diffusion decoder) without explaining why this combination works beyond T-SNE visualizations. What specific features do TSLM vs. RALM capture? Missing are probing experiments, attention analysis, layer-wise studies, or comparisons with alternative quantization methods. The paper should provide a few fundamental insights to verify their claims.\n\n- The model does not use any explicit alignment mechanism, such as attention or duration modeling, to synchronize text tokens with acoustic frames. Instead, it depends on implicit correlations learned during training, which can result in mispronunciations, timing instability, or prosodic drift in longer utterances.\n\n- The ablation studies don’t justify critical design decisions, including the 24-layer/6-layer split between TSLM and RALM, the contribution of pre-trained LLM initialization quality (e.g., comparing MiniCPM-4 vs. other pre-trained models), and the necessity of the FSQ bottleneck itself by comparing against simpler hierarchical architectures without quantization."}, "questions": {"value": "- Regarding the acoustic embeddings (E<i), what specific acoustic features (pitch variations, timbre, voice quality) does it learn?\n\n- Could the author clarify how the proposed framework manages long audio sequences?\n\n- RALM is reported to recover “speaker identity” implicitly, yet the model has no explicit conditioning on speaker embeddings or style tokens. Could the authors clarify how speaker identity is actually captured?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kFjgpJeFIq", "forum": "h5KLpGoqzC", "replyto": "h5KLpGoqzC", "signatures": ["ICLR.cc/2026/Conference/Submission23202/Reviewer_kJR8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23202/Reviewer_kJR8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932954944, "cdate": 1761932954944, "tmdate": 1762942558868, "mdate": 1762942558868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a hierarchical AR framework based on semi-discrete residual representations to mitigate the trade-off between stability and expressivity observed in recent LLM-based TTS systems using speech tokenizers. The proposed architecture comprises a Text-Semantic Language Model that captures high-level linguistic and prosodic structure, and a Residual Acoustic Language Model that reconstructs fine-grained acoustic details. It further incorporates a differentiable FSQ bottleneck with LocDiT and a causal VAE to achieve unified and high-fidelity speech generation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear and well-structured framework that effectively balances stability and expressivity through hierarchical semantic-acoustic modeling. The proposed combination of TSLM, RALM, and the FSQ bottleneck is conceptually coherent and technically sound. \n- The writing is clear and easy to follow, and the experimental results are well organized and convincingly support the main claims."}, "weaknesses": {"value": "- The paper introduces FSQ as a semi-discrete bottleneck, but its function appears closer to latent feature regularization than true discrete representation learning. Because FSQ is not used for token prediction, it mainly constrains feature space rather than modeling discrete structure. A comparison with VAE-based latent regularization would help determine whether FSQ provides meaningful representational advantages or simply smoother feature constraints. \n- The authors claim the system as an  end-to-end framework, but it cannot be considered truly  end-to-end because it relies on a pre-trained audio VAE to extract continuous speech latents.\n- The paper provides limited details about the Causal Audio VAE, even though the design of latent speech tokens can significantly affect overall performance."}, "questions": {"value": "Q1. Does the observed separation in Appendix F.5 - Figure 2 truly result from the FSQ bottleneck, or is it primarily due to the pre-trained initialization of the semantic LLM? How would the t-SNE visualization change if the TSLM were trained from random initialization?\n\nQ2. In most recent LLM-based TTS systems, increasing model capacity generally improves both speaker similarity and intelligibility. However, in the ablation study (Table 4), deeper TSLM layers slightly increase SIM but noticeably degrade WER. It would be helpful if the authors could clarify the reason for this. observation. Specifically, how can the effects of structural disentanglement be distinguished from capacity related trade-offs, and might the observed degradation reflect limitations in training dynamics rather than an inherent need for the RALM module?\n\nQ3. The paper states that the TSLM uses MiniCPM-4-0.5B as its backbone, while the overall model size is also reported as 0.5B parameters. Could the authors clarify how this total is computed? Specifically, how many parameters are allocated to the AudioVAE and the RALM modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o50gBaJIRD", "forum": "h5KLpGoqzC", "replyto": "h5KLpGoqzC", "signatures": ["ICLR.cc/2026/Conference/Submission23202/Reviewer_emjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23202/Reviewer_emjo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981932441, "cdate": 1761981932441, "tmdate": 1762942558626, "mdate": 1762942558626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a novel end-to-end TTS framework, VoxCPM, to address the fundamental trade-off between discrete and continuous speech representations, where discrete representations maintain stability but sacrifice expressivity, while continuous representations retain acoustic richness but are prone to error accumulation. The hierarchical semantic-acoustic modeling framework is well-motivated and addresses some limitations in current TTS systems. \n\nThe proposed VoxCPM is with the hierarchical semantic-acoustic architecture, consisting of a Text-Semantic Language Model (TSLM), a Residual Acoustic Language Model (RALM), and the Local Diffusion Transformer (LocDiT). The key innovation is the differentiable Finite Scalar Quantization bottleneck, which serves not as a prediction target, but as an intermediate regularization mechanism to induce natural task separation. \n\nThe manuscript demonstrates that their 0.5B-parameter model achieves state-of-the-art zero-shot TTS performance with extensive experiments and ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The major contribution, using a differentiable FSQ bottleneck as an internal inductive bias rather than a predictive target, is a elegant and well-motivated solution to the task entanglement problem. By regularizing the hidden state, it forces the TSLM to focus on stable semantic-prosodic structures while offloading fine-detail modeling to the RALM.\n\n2. The paper provides compelling quantitative results. VoxCPM achieves state-of-the-art performance among open-source models on two challenging benchmarks, SEED-TTS-EVAL and CV3-EVAL. The ablation studies conclusively validate the authors' core hypotheses.\n\n3. While the best-performing model (VoxCPM) is trained on a large, proprietary dataset , the authors have responsibly run all critical ablation studies on the public Emilia dataset."}, "weaknesses": {"value": "1. While the empirical results are strong, the paper lacks theoretical justification for why FSQ specifically induces semantic-acoustic disentanglement\n\n2. No formal analysis of the \"quantization ceiling\" or error accumulation mechanisms, and how the proposed architechture can effectively address them.\n\n3. The claim that FSQ acts as an \"inductive bias\" needs more rigorous theoretical grounding and mathematical proof.\n\n4.  The main SOTA results are from the VoxCPM model trained on a 1-million-hour internal, bilingual dataset. While the authors provide VoxCPM-Emilia, trained on a public dataset, the top-line performance is dependent on this massive, inaccessible dataset. This limits the direct reproducibility of the primary claims, although the architectural insights from the Emilia-based ablations remain sound. Some baselines (F5-TTS, GPT-Sovits) may use significantly less data, making direct comparison problematic.\n\n5. The paper doesn't discuss whether the performance gains are primarily from architecture or data scale\n\n6. The entire model generates latents from a pre-trained Causal VAE. This VAE is itself a form of compression. The paper criticizes discrete tokenizers for their \"quantization ceiling\", but doesn't discuss the potential information bottleneck or quality ceiling imposed by its own VAE. The quality of this VAE is critical to the final output, but it's only briefly detailed. \nThe author also claimed that the voxCPM do not rely on a external tokenizer. However the VAE is itself a tokenizer."}, "questions": {"value": "1. You sum TSLM and RALM outputs. Did you experiment with other fusion strategies (concatenation, gating, attention)?\n\n2. The paper states FSQ is \"analogous to the first layer of Residual Vector Quantization (RVQ)\". Did you experiment with using a learned vector quantizer (e.g., a single VQ layer) as the bottleneck instead of the non-learned FSQ? It seems a learned codebook might create an even more efficient semantic-prosodic skeleton. And what happens if you replace FSQ with VQ-VAE or other quantization methods?\n\n3. Please discuss the potential quality ceiling imposed by their pre-trained causal VAE  and how it relates to the \"quantization ceiling\" they criticize in discrete tokenizers.\n\n4. Please try your best to provide more theoretical analysis as mentioned in \"Weaknesses\".\n\n4. The VAE is also external tokenizer. \n\n5. For citation formatting, please ensure consistent citation style (some entries lack year information)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7j5oLtpD3B", "forum": "h5KLpGoqzC", "replyto": "h5KLpGoqzC", "signatures": ["ICLR.cc/2026/Conference/Submission23202/Reviewer_ada6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23202/Reviewer_ada6"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106960510, "cdate": 1762106960510, "tmdate": 1762942558271, "mdate": 1762942558271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}