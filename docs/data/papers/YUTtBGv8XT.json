{"id": "YUTtBGv8XT", "number": 5153, "cdate": 1757856791833, "mdate": 1759897991688, "content": {"title": "SPOT-JS:Spectral Chebyshev Filter and Optimal Transport Fusion with Jensen-Shannon Alignment for Cross-Domain Multimodal Deception Detection", "abstract": "Multimodal deception detection is increasingly important for security, justice, and human-AI interaction. However, prevailing systems still depend on contact-based sensing or elaborate handcrafted feature pipelines and exhibit limited generalization beyond their training domains. Typical approaches learn shallow unimodal cues (e.g., surface spatio-temporal patterns) and fuse modalities by simple concatenation or attention; these choices induce sensitivity to positional dependencies and to distribution shift. This work presents SPOT-JS, a frequency-domain framework aimed at cross-domain transfer. It standardizes inputs, improves unimodal representations, and performs fusion with distribution-aware alignment grounded in established theory.\nConcretely, a Temporal Deception Alignment Module (TDAM) first provides unified preprocessing and audio-visual synchronization to eliminate reliance on specialized facial/vocal features or invasive signals. We then propose a Learnable Chebyshev Spectrum Filter (LCSF) that operates on power spectra to emphasize task-relevant bands and suppress noise by embedding a learnable Chebyshev basis into the spectral transformation. Next, an Optimal Transport-based Cross-Modal Fusion (OTCF) module computes an entropic-regularized transport plan between spectral components of audio and video, enabling fine-grained, bidirectional correspondence and residual fusion in a shared latent space. Fourth, a Jensen-Shannon Guided Alignment (JS-Align) module measures cross-modal posterior similarity via JS divergence and adaptively reweights the fused representation, mitigating sensitivity to positional mismatches and improving stability under shift. Finally, we introduce the Chebyshev Spectrum-guided Knowledge Transfer (CSKT) Module, which leverages spectral filtering to enhance cross-domain facial knowledge transfer. On standard benchmarks (Real Life Trial, DOLOS, and Box of Lies), SPOT-JS surpasses strong unimodal, fusion, and transfer baselines in both intra- and cross-domain settings, with higher F1/ACC/AUC and especially large gains when training on one dataset and testing on another.", "tldr": "", "keywords": ["multimodal deception detection", "frequency spectrum", "chebyshev polynomials", "optimal transport", "jensen-shannon", "cross-domain"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/580c17db8bcd2a0aee583c819a9255ad2afcec80.pdf", "supplementary_material": "/attachment/e8f58c85e75af91bef87f30cc2bebe31d67f3b97.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SPOT-JS, a novel framework for multimodal deception detection, introducing several key modules to enhance the generalization of the model and the effectiveness of multimodal fusion. Specifically, TDAM enhances model generalization through standardized data preprocessing, while the CSKT module utilizes spectral filtering to improve knowledge transfer from facial expression datasets. Besides, following modules are introduced  to enhance the effectiveness of multimodal fusion: 1) OTCF, a fusion module based on optimal transmission, to find fine-grained correspondence between audo and video spectra; 2) JS Align module, using JS divergence to adaptively weight the fused representation; In addition, the author also proposed a learnable Chebyshev spectral filter (LCSF) that enhances single modal features by focusing on frequency bands relevant to the task. The author conducted extensive experiments on three benchmark datasets, and the results showed that the method performed significantly better than many baseline methods within the domain, especially in cross domain settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This method innovatively shifts feature enhancement and multimodal fusion to the frequency domain for processing, introducing theories such as Optimal Transport (OT) and JS divergence to guide alignment, which significantly improves the performance of the deception detection model. Furthermore, through operations like data preprocessing and feature filtering, it demonstrates outstanding performance in addressing the critical cross-domain generalization problem."}, "weaknesses": {"value": "1.The author points out that multimodal feature fusion based on attention mechanisms is relatively coarse and thus yields suboptimal performance. Consequently, they propose a fusion method based on optimal transport theory. Can the differences in the feature fusion outcomes between these two approaches be demonstrated intuitively or measured quantitatively?\n\n2.While the paper provides some visualizations, the interpretability of the frequency-domain operations remains somewhat abstract. For instance, what specific frequency bands does the LCSF learn to amplify for deception? Correlating these learned filters back to known psycho-physiological cues (e.g., high-frequency voice tremors, low-frequency head movements) would provide invaluable insights."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yGPEBhhCG6", "forum": "YUTtBGv8XT", "replyto": "YUTtBGv8XT", "signatures": ["ICLR.cc/2026/Conference/Submission5153/Reviewer_18Hc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5153/Reviewer_18Hc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761319490048, "cdate": 1761319490048, "tmdate": 1762917910987, "mdate": 1762917910987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work study the problem of multimodal (video-audio) deception detection, where the communicator is classified if they are performing the act of deception. The paper proposed a method SPOT-JS, which consists of a temporal deception alignment module (which is preprocessing module for both modalities), a learnable chebyshev spectrum filter that operates on the frequency space, an optimal transport cross-modal fusion, a Jensen-shannon guided alignment component, and a chebyshev spectrum guided knowledge transfer module. The proposed method is applied on three standard deception benchmarks, and achieved high classification performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Detailed ablation experiments to validate the effectiveness of each components.\n- Interesting case studies to show the effectiveness of the method."}, "weaknesses": {"value": "- The paper proposed many technical components, but none of them is carefully studied or explained. There is no theoretical analysis or detailed exploration of the mechanism. Thus, it is unclear to me how each component of the proposed method is important, or how can they be applied on other classification or multimodal tasks.\n\n- Readability of the paper is bad. The paper defines a lot of new jargons, e.g. the temporal deception alignment module (TDAM) essentially is just video and audio preprocessing steps. Also, key components of the method, e.g. loss function, is not included inside the main text but in appendix instead. The main text should be self-sustained.\n\n- The literatures and benchmarks referenced seem to be out-of-dated. For literature review, especially section 2.2, there are a lot more recent works in frequency domain learning. Specifically, [1] seems to be highly relevant and similar to the proposed work. For benchmarks, [2] reported >99% and >99% accuracy and F1 score on RLT and DOLOS datasets using multiple baselines and methods, which are higher than what is reported in this paper. I am not extremely familiar with this subfield, can the authors explain the difference between the performance across baselines, and explain the performance difference?\n\n[1] Lao, A., Zhang, Q., Shi, C., Cao, L., Yi, K., Hu, L., & Miao, D. (2024, March). Frequency spectrum is more effective for multimodal representation and fusion: A multimodal spectrum rumor detector. In Proceedings of the AAAI conference on artificial intelligence (Vol. 38, No. 16, pp. 18426-18434).\n\n[2] Zhuo, Y., Baskaran, V. M., Wang, L. Y. K., & Phan, R. C. W. (2025). @ LM DeceptionNet: A multimodal approach for efficient transfer learning-based deception detection. Knowledge-Based Systems, 113499."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ksjZp0jMHu", "forum": "YUTtBGv8XT", "replyto": "YUTtBGv8XT", "signatures": ["ICLR.cc/2026/Conference/Submission5153/Reviewer_7UMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5153/Reviewer_7UMw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967360616, "cdate": 1761967360616, "tmdate": 1762917910079, "mdate": 1762917910079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a video classification algorithm based on Fourier transform. FT is applied on both the visual frames and the audio. The coefficients are then filtered and fused for improved classification for the application of deception detection."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Deception detection is an interesting application.  Authors have shown improved results on three datasets."}, "weaknesses": {"value": "1. A comparison of Tables 1, 2, 3 shows that performance of SPOT-JS is better with video modality compared to the fused modality.  It shows that the fusion method is not effective. Sections 4.5 and 4.6 has proposed the fusion method which actually reduced the performance.  Authors should avoid fusion and just re-write paper with only visual modality. As audio is not helpful, therefore any thing related to audio becomes redundant.\n\n2. Different modules are presented as contributions. \n\n3. Trivial things such as extracting frames from a video are very formally presented such as Temporal Deception Alignment Module (TDAM).  \n\n4. Some equations such as Eq. (16) looks arbitrary or AI generated."}, "questions": {"value": "If fusion reduces performance then what is need to add fusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRZ5PRPNRv", "forum": "YUTtBGv8XT", "replyto": "YUTtBGv8XT", "signatures": ["ICLR.cc/2026/Conference/Submission5153/Reviewer_HTwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5153/Reviewer_HTwH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762198930399, "cdate": 1762198930399, "tmdate": 1762917909739, "mdate": 1762917909739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}