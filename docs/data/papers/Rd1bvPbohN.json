{"id": "Rd1bvPbohN", "number": 19030, "cdate": 1758292850373, "mdate": 1759897064922, "content": {"title": "QuadCal: Calibration for In-Context Learning", "abstract": "Large language models (LLMs) are increasingly being applied to high-stakes domains with high consequences for errors such as healthcare, drug discovery, law, and finance. However, they are often unstable and highly sensitive to prompt design, which can introduce contextual bias into their predictions. To mitigate this bias, various calibration methods have been developed to prevent overconfident and incorrect predictions. Existing techniques are either confidence-based, relying on heuristics to quantify bias, or likelihood-based, which is theoretically grounded but introduces unnecessary computational overhead. In this work, we introduce QuadCal, a novel supervised likelihood-based calibration method that is up to 40% faster and outperforms the existing likelihood-based approach. Specifically, QuadCal leverages Quadratic Discriminant Analysis (QDA), a supervised algorithm that directly models class-conditioned distributions, making it more efficient. We evaluated calibration methods on GPT-2 models and the more recent Llama and Gemma’s instruction-tuned (IT) models, which are harder to calibrate. Empirically, we show that on average over seven different natural language classification datasets, QuadCal outperforms existing methods on GPT-2 models and is competitive with earlier methods on IT models.", "tldr": "Likelihood-based calibration method for in-context learning", "keywords": ["calibration", "ICL", "uncertainty"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de7baf466362c3ffd655fe6f082fc4989d3f3812.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In-context learning is widely applied to high-stakes domains with high consequences for errors. Existing calibration methods may  introduce contextual bias or introduce unnecessary computational overhead. The paper proposed a supervised likelihood-based calibration method that is up to 40% faster and outperforms the existing likelihood-based approach. Experiments across classification datasets valiate the efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a novel supervised likelihood-based calibration method which achieve low computational cost and higher macro-average accuracy.\n\n2. The methodology is easy to understand and follow.\n\n3. Comprehensive empirical analysis and ablation studies in the experiment section."}, "weaknesses": {"value": "1. Tables 1 show that the results of QuadCal and the baselines are quite similar, especially for the advanced LLMs like Llama-3.2-1B or 3B. It would be helpful if the authors could further clarify the advantages of the proposed methods.\n\n2. The paper should compare QuadCal with the advanced and recent baseline methods from other studies, such as LPC[3] and SupICL[4].  Such an analysis could offer valuable insights into the strengths and weaknesses of the proposed approaches.\n\n3. The author should complete the full 9 pages. The figure and its illustration should be shown on the same page, such as Figure 1. Section 3.1 should be titled Motivation or Methodology rather than Background.\n\n4. The paper validate the proposed method only on simple classification datasets. The authors should add more complex downstream tasks such as question-answering tasks or code generation tasks.\n\n5. The authors should report the results of the Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Adaptive ECE, and Brier Score, which are widely used to evaluate the performance of calibration methods.\n\n6. The absence of code makes it to reproduce the results claimed in the paper or verify the method's effectiveness on the tasks.\n\n7. It would be useful to report the performance of in-context learning using a calibration method based on Linear Discriminant Analysis.\n\n8. It would be useful to compare the computational cost with more baselines such as CC and BC.\n\n9. Which dataset is used in Figure 1?\n\n10. Missing the details of used dataset, such as dataset sizes, prompts.\n\n11. Missing analysis on the selection strategy used in ICL: It would be useful if the authors also studied how different selection strategies (e.g., EPR [1] and DPP [2]).\n\n12. Missing the analysis of different prompt formation.\n\n13. Missing the analysis of larger model sizes (such as 7B, 14B, 30 B and 70B) and API (such as GPT-4o, Claude and Deepseek).\n\n[1] Learning to retrieve prompts for in-context learning.\n\n[2] Compositional exemplars for in-context learning\n\n[3] Enhancing In-context Learning via Linear Probe Calibration\n\n[4] Large Language Models are Miscalibrated In-Context Learners"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K4y8jTFLWO", "forum": "Rd1bvPbohN", "replyto": "Rd1bvPbohN", "signatures": ["ICLR.cc/2026/Conference/Submission19030/Reviewer_GCzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19030/Reviewer_GCzA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372665820, "cdate": 1761372665820, "tmdate": 1762931072500, "mdate": 1762931072500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a supervised likelihood-based calibration method for large language models in-context learning.  The whole frame work is built on ProCa but replacing Gaussian Mixture Model with Quadratic Discriminant Analysis.  Experiments across multiple models, e.g. GPT-2, Llama and Gemma and different classification tasks demonstrate its performance.  The results indicate that smaller or non-instruction-tuned models particularly benefit more from this post-hoc  likelihood-based calibration process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n2. The experiments are fairly designed, including multiple model families and various task domains, as well as different in-context learning shot settings. \n3. The main claim, such as faster runtime and improved average performance, is supported by experimental results and comparisons. \n4. The discussions section is good regarding when different calibration methods, either confidence-based or likelihood-based, are preferable."}, "weaknesses": {"value": "1. The observation that larger size or instruction-tuned language models tend to be better calibrated is not new and has been reported in prior work, such as “_Language Models (Mostly) Know What They Know._” So, findings in this paper mainly confirm existing understanding rather than providing new insights.\n2. The inclusion of statistical testing is good, but it seems unnecessary to me.  Moreover, the paper does not present detailed test statistics and full testing reports, even though the authors discussed and compared the results and significance levels.\n3.  Expect for the reduction in computational cost, the improved accuracy of the proposed approach is not very significant for the majority of tasks, which limits its practical advantage. \n4. Although positioned as a theoretically grounded Bayesian approach, this work does not clearly articulate how or why its likelihood-based formulation provides deeper theoretical justification compared to confidence-based methods like BC, which achieves comparable performance with greater implementation efficiency.  \n5. The construction of the estimate set is not well explained. And all of the methodological explanation relies only on a toy example (Figure 1), which makes the implementation details unclear."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oxVsbUOZpc", "forum": "Rd1bvPbohN", "replyto": "Rd1bvPbohN", "signatures": ["ICLR.cc/2026/Conference/Submission19030/Reviewer_xqkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19030/Reviewer_xqkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968658830, "cdate": 1761968658830, "tmdate": 1762931072185, "mdate": 1762931072185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces QuadCal, a supervised likelihood-based calibration method for in-context learning (ICL) that addresses contextual bias in large language models. The key innovation is replacing Prototypical Calibration (ProCa)'s unsupervised GMM clustering + Hungarian algorithm approach with Quadratic Discriminant Analysis (QDA), which directly models class-conditioned distributions using ground-truth labels. The authors evaluate QuadCal against existing calibration methods (CC, BC, ProCa) on seven NLP classification datasets using GPT-2 models and instruction-tuned Llama/Gemma models across 0/1/4/8-shot settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. While the core idea of substituting GMM with QDA is straightforward, the application to ICL calibration is sensible and previously unexplored.\n2. The key insight, that supervised learning eliminates the need for expensive cluster-to-label mapping, is valid and practically useful.\n3. The paper clearly motivates the problem (contextual bias in ICL, computational overhead in ProCa) and positions QuadCal as a solution.\n4. For practitioners deploying LLMs in high-stakes domains, this work offers a useful, faster alternative for likelihood-based calibration."}, "weaknesses": {"value": "1. In Table 1, the performance gain seems to be trivial for the majority of the models. In fact, BC outperforms QuadCal a few times. QuadCal significantly outperforms ProCa in only 26% of 168 settings, where 66% show no significant difference, and ProCa is significantly better in 8% of cases. For a paper whose main claim is improved performance, these statistics are weak and are insufficient to claim a meaningful contribution.\n2. The datasets used are pretty saturated and could already have been covered by some models' pre-training and post-training.  It is generally a good practice to test on ICL tasks that are new, hard, and require the model to learn novel logic or strategies, since the value of ICL is to learn new tasks at inference time. For example, tasks like BBH, BB-extra-Hard, ZebraLogic, GPQA, MMLU-Pro, and other logical puzzles or algorithmic reasoning challenges should be the main evaluation focus. The current set of datasets does not support a generalized claim and undermines the impact of the contribution.\n3. There are some potential missed baselines. There is no comparison with Linear Discriminant Analysis, which would be faster than QDA, and test whether class-specific covariances are necessary. Also, there are no comparisons with simpler supervised methods like logistic regression on log-probabilities or ensemble approaches.\n4. The discussion notes that different methods work better for different tasks but provides no principled explanation:\n- Why does QuadCal excel on AGNews and TREC but not RTE?\n- What properties of these datasets drive the differences?\n- No analysis of class balance, class separability, or output probability distributions\n5. The core contribution, replacing GMM+Munkres with QDA, is primarily an engineering substitution rather than a fundamental methodological advance. While the paper claims QDA avoids ProCa's computational overhead, the theoretical justification is superficial. Why should QDA outperform GMM for this specific problem? Under what data distributions or class structures would QDA be preferred? The statement that ProCa's restriction to n clusters makes GMM \"functionally similar\" to QDA (lines 183-186) needs rigorous proof. GMM with n components can still capture multimodal within-class distributions, while QDA assumes unimodality."}, "questions": {"value": "1. The observation that larger IT models benefit less from calibration is interesting but underdeveloped:\n- Are larger models inherently better calibrated? Are there ECE comparisons on the model sizes?\n- Is this specific to instruction-tuning or general to scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UuTCNlYvMi", "forum": "Rd1bvPbohN", "replyto": "Rd1bvPbohN", "signatures": ["ICLR.cc/2026/Conference/Submission19030/Reviewer_PMDq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19030/Reviewer_PMDq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19030/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988275520, "cdate": 1761988275520, "tmdate": 1762931071794, "mdate": 1762931071794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}