{"id": "G1147yjk9T", "number": 2789, "cdate": 1757249998847, "mdate": 1763038308587, "content": {"title": "THEval. Evaluation Framework for Talking Head Video Generation", "abstract": "Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.", "tldr": "We propose a new evaluation framework composed of 8 metrics and a dataset of 5000 videos for evaluation for talking head generation models.", "keywords": ["Talking Head", "Benchmark", "Dataset", "Video Generation", "Computer Vision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e3028df698a4d072541c1b946c33eaeefafd17e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors define eight dimensions to evaluate talking head generation methods. They test 17 talking head generation methods and report the scores using the eight dimensions. They collect 5,011 videos from YouTube to evaluate the methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They conduct a large-scale study of 17 talking head methods and evaluate them thoroughly.\n2. Talking head generation is an active research area, so the idea for a holistic evaluation suite makes sense."}, "weaknesses": {"value": "1. Global Aesthetic Score is the same as the CLIP Aesthetic score with a different model[1].\n2. There are missing details about human evaluation and assumptions about metrics, which I am doubtful about.\n\n\n[1] CLIP knows image aesthetics: Simon Hentschel, Konstantin Kobs, Andreas Hotho."}, "questions": {"value": "1. In Lip Dynamics, how is M selected?\n2. Raw Euclidean distances change with face scale, camera zoom, etc. Without normalization, differences between frames will reflect geometry changes, not true lip motion. While it has been done for other metrics, its not clear if this was taken into account for Lip dynamics metric.\n3. I think in the Eyebrow Dynamics metric, the authors equate variability with quality. A static eyebrow, like a neutral face, could be high-quality video yet score low, but an over-animated eyebrow could score high despite being unrealistic.\n4. From my understanding, it seems the core idea of the Lip-Sync metric is that mouth openness should be proportional to audio volume. Why would it be so?\n5. The phrase \"all combinations of the seventeen state-of-the-art methods\" is ambiguous and practically difficult to achieve. The details are missing as to how many samples were evaluated by humans, whether they were sampled randomly, or whether they evaluated all combinations. How many samples did a single human evaluate? Metrics like intern-annotator reliability should be used to evaluate the reliability of the human evaluations. The instruction \"Please watch both videos and select which one looks more realistic\" is intentionally simple to \"minimize cognitive load.\" However, \"realism\" is a very broad term and could be interpreted differently by different users. It would be helpful if the authors could clear up these details.\n6. How do the authors ensure the YouTube videos are not seen by any of these methods? To my knowledge, there exist some works that collect YouTube videos for training Talking head models[1,2]\n\n\n[1] Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset: Zhimeng Zhang, Lincheng Li, Yu Ding, Changjie Fan\n\n[2] Towards Generating Ultra-High Resolution Talking-Face Videos with Lip synchronization: Anchit Gupta, Rudrabha Mukhopadhyay, Sindhu Balachandra, Faizan Farooq Khan, Vinay P. Namboodiri, C. V. Jawahar"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "It is unclear whether the YouTube videos downloaded had an appropriate license."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NKTTT3ebnV", "forum": "G1147yjk9T", "replyto": "G1147yjk9T", "signatures": ["ICLR.cc/2026/Conference/Submission2789/Reviewer_EBES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2789/Reviewer_EBES"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760737247622, "cdate": 1760737247622, "tmdate": 1762916378661, "mdate": 1762916378661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "AgwQD6l6dR", "forum": "G1147yjk9T", "replyto": "G1147yjk9T", "signatures": ["ICLR.cc/2026/Conference/Submission2789/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2789/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763038307871, "cdate": 1763038307871, "tmdate": 1763038307871, "mdate": 1763038307871, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the talking head generation task, to address the issue that metric methods cannot accurately measure model performance, the authors propose the THEval benchmark. This benchmark includes 5k videos and proposes multiple metrics from three perspectives: quality, naturalness, and synchronization. The authors tested many state-of-the-art talking head generation methods and measured the gap between the benchmark and human preferences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well motivated and provides extensive experimental validations, covering many latest methods.\n- The paper tests the correlation with human preferences, verifying that THEval metrics are more capable of reflecting model performance than existing metrics."}, "weaknesses": {"value": "- The paper claims that THEval can \"evaluate the improvement of generative methods\". However, this benchmark only targets the driving task of speakers from the shoulders up. Currently, the development of talking head generation (THG) is gradually moving towards full-body driving and even multi-person driving, so this benchmark may be difficult to fully evaluate the progress in this field.\n- The content of THG largely depends on the reference image. If the reference image itself has poor aesthetic quality, it may cause the metric to fail. In addition, the metric also struggles to reflect identity preservation ability.\n- The evaluation methods for both Head Motion and Eyebrow Dynamics are somewhat unconvincing. Using global averages will erase detailed movements. The rationality of such evaluation requires more experimental support. \n- These metrics should not only align with the coarse-grained human preference of \"which is better\" but should also truly reflect what they are intended to measure, thus helping people identify where the flaws of a solution lie."}, "questions": {"value": "LSE-C and human preference show a negative correlation, which is somewhat counterintuitive. Please provide some examples where LSE-C is high but the Lip-Sync metric is low."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jRNn28PZgK", "forum": "G1147yjk9T", "replyto": "G1147yjk9T", "signatures": ["ICLR.cc/2026/Conference/Submission2789/Reviewer_Mg5X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2789/Reviewer_Mg5X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219203426, "cdate": 1761219203426, "tmdate": 1762916378393, "mdate": 1762916378393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an evaluation framework for the talking head video generation task by combining 8 metrics focusing on quality, naturalness, and synchronization. Moreover, the paper releases a benchmark for talking head evaluation. Experimental results show an improved correlation between the proposed evaluation score and human ratings compared with previous metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed idea is clearly illustrated and easy to follow.\n2. Experimental results show improved correlation between the proposed metric and human ratings, and the proposed benchmark is evaluated on various talking head models."}, "weaknesses": {"value": "1. I have several concerns about the proposed metric framework:\n- One of the major challenges in talking head video generation is aligning mouth motion with audio, but there is a lack of metric for this.\n- Facial feature details are not considered in the framework, such as emotional expression and alignment among the eyes, mouth, and head movements. These details could be captured using facial landmarks or intermediate feature maps from the model, which are not included in the proposed framework.\n\n2. For the proposed THEVAL benchmark:\n- The discussion is limited and lacks analysis. For example, what is the distribution of head motion, language, user identity, lip motion, etc.?\n- There is also a lack of discussion regarding experiments on the THEVAL benchmark. For instance, what are the differences between the proposed and existing benchmarks? How does the THEVAL benchmark align with the proposed metric framework? What findings are observed from the experimental results?"}, "questions": {"value": "Questions for the proposed metric framework:\n1. How are the 8 metrics combined? The 3 dimensions should have different significance in representing human preference—are any weights assigned to the different metrics?\n2. For the human rating correlation experiment, what model is used to generate the videos? And do the videos adequately cover the three dimensions considered in the proposed metric framework?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Videos of public YouTube channels are collected for the evaluation dataset, which might have ethical issues for privacy."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UAkFqy3TL9", "forum": "G1147yjk9T", "replyto": "G1147yjk9T", "signatures": ["ICLR.cc/2026/Conference/Submission2789/Reviewer_ikAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2789/Reviewer_ikAd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771647215, "cdate": 1761771647215, "tmdate": 1762916377975, "mdate": 1762916377975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces THEval, a comprehensive evaluation framework for talking head video generation. Unlike existing metrics that focus narrowly on image quality or lip synchronization and show poor correlation with human judgment, THEval combines eight algorithmic and perceptual metrics across three dimensions: quality, naturalness, and synchronization. The framework is validated on a large benchmark dataset and demonstrates a high correlation with human ratings, making it a more reliable and interpretable tool for assessing the performance of state-of-the-art audio- and video-driven talking head models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed THEval framework is highly useful for the evaluation of talking head video generation, as it integrates eight fine-grained metrics covering quality, naturalness, and synchronization, and demonstrates strong alignment with human perceptual judgments.\n\n2. The experimental section is thorough, benchmarking 17 state-of-the-art models on a large, diverse dataset of 85,000 videos, and includes comprehensive user studies to validate the effectiveness of THEval against existing metrics.\n\n3. The paper is well-structured and clearly written, with a logical flow from motivation and related work to the detailed presentation of the framework, followed by extensive experiments and analysis, making the contributions easy to understand and follow."}, "weaknesses": {"value": "1. The presentation in the paper is somewhat insufficient. Although multiple metrics are proposed, there is no corresponding case study to intuitively demonstrate how each metric evaluates the generated videos.\n2. The comparison with previous work does not seem to be comprehensive. In the field of talking head generation, there are some commonly used metrics, such as synchronization-related metrics (Sync-C and Sync-D) and quality-related metrics (introduced by Q-align). These metrics are more closely related to some of the metrics proposed in the paper, but they are not reflected in Table 2."}, "questions": {"value": "1. I am somewhat confused about the design principles of the NATURALNESS-related metrics. For example, how was the formula for metric (3) derived? Additionally, since the authors use the magnitude of motion to measure naturalness, does this mean that more dramatic changes will result in higher scores?\n\n2. Regarding the calculation of the final score, is it simply the average of all normalized metric scores? In the field of talking head generation, I believe lip quality and readability are the most important aspects. Should these metrics be assigned higher weights? Is there a risk that a model with high naturalness but poor readability could achieve a high final score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZP5tadEef3", "forum": "G1147yjk9T", "replyto": "G1147yjk9T", "signatures": ["ICLR.cc/2026/Conference/Submission2789/Reviewer_aH21"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2789/Reviewer_aH21"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976034605, "cdate": 1761976034605, "tmdate": 1762916377846, "mdate": 1762916377846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}