{"id": "F02KXwqFdQ", "number": 13935, "cdate": 1758225502019, "mdate": 1759897402443, "content": {"title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization", "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance, such as the ''right to be forgotten.\" Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget\" specific data. We introduce $\\textit{Forgetting-MarI}$, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset’s residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.", "tldr": "", "keywords": ["Machine Unlearning", "LLM Unlearning", "Data Privacy", "Information-Theoretic Regularization", "Mutual Information"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c45492e525736707201257065f52d7f5064a5915.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Forgetting-MarI, an LLM unlearning framework that removes only the marginal information contributed uniquely by the data to be forgotten, rather than erasing all related knowledge. It formalizes marginal information via mutual information between retain-only and retain+forget distributions, and introduces a regularization term to penalize this marginal effect during fine-tuning. As a result, the model forgets unwanted data while preserving shared knowledge supported by the retain set. Experiments on GPT-2 and Llama show more stable training, better retain–forget trade-offs, and lower detectability by perplexity or membership inference detectors compared to existing methods like gradient ascent or DPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a principled definition of marginal information using mutual information between retain-only and retain+unlearn distributions, giving a more grounded objective than heuristic gradient-ascent–based approaches.\n\n2. Instead of over-forgetting (removing all related knowledge), it focuses on erasing only the incremental contribution of the unlearned data—better aligned with legal and practical expectations of data removal.\n\n3. Theoretical results bound the residual mutual information and show that after unlearning, confidence-based or perplexity-based detectors cannot reliably detect whether the unlearned data was seen."}, "weaknesses": {"value": "1. The proposed method requires full forward passes over both retain and unlearn sets to estimate token-level or pooled distributions and mutual information, which may be significantly more expensive than lightweight methods. There is no concrete runtime, GPU memory, or scalability analysis for larger models.\n\n2. Although elegant theoretically, estimating JSD across token distributions of two datasets at every update step could be costly and unstable in heterogeneous or long-sequence settings. No analysis is provided for approximation errors or computational shortcuts.\n\n3. Only GPT-2 Large and Llama-3.2-1B are evaluated. It remains unclear how the method performs on realistic industrial-scale LLMs (e.g., 7B, 13B, or 70B) or multilingual/general-purpose datasets.\n\n4. The evaluation focuses on literary text (Harry Potter, Careless People) with artificial retain/unlearn splits. Scenarios like personal data removal, bias removal, or knowledge update (dynamic unlearning) are not extensively tested.\n\n5. The method assumes access to both retain and unlearn datasets during unlearning, which is not always realistic in privacy-sensitive settings (e.g., user-request deletion where only the “forget” data remains)."}, "questions": {"value": "1. What is the actual computational overhead of estimating marginal information during training? How does runtime and memory compare to simpler baselines (e.g., GD, DPO, LoRA-based unlearning)? \n\n2. In repeated unlearning requests, does mutual information regularization accumulate errors or drift? Do you observe degradation after many rounds of unlearning?\n\nIf these concerns are resolved, I’m open to increasing my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2MMtTv4TxL", "forum": "F02KXwqFdQ", "replyto": "F02KXwqFdQ", "signatures": ["ICLR.cc/2026/Conference/Submission13935/Reviewer_rJHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13935/Reviewer_rJHg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463556092, "cdate": 1761463556092, "tmdate": 1762924438923, "mdate": 1762924438923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Forgetting-MarI, a new LLM unlearning method based on marginal information regularization. The key idea is to explicitly penalize the mutual information (MI) between model outputs and the “unlearn indicator,” thereby removing only the marginal contribution of the data to be forgotten ($D_u$) while preserving information supported by the retained data ($D_r$). The authors provide a theoretical formulation of marginal information via MI and JSD divergence, a practical implementation compatible with gradient-based fine-tuning, and experiments on GPT-2 Large and Llama-3.2-1B showing better trade-offs between forgetting and utility compared with Gradient Difference, KL-GA, DPO, and GA."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual novelty: The paper introduces a clear and elegant information-theoretic framing of LLM unlearning via marginal information, offering a well-motivated bridge between data privacy and utility preservation.\n2. Theoretical rigor: Derivations are mathematically sound, and the theorems (especially Theorems 2.1 and 2.2) link mutual information bounds to empirical detectability in a principled way.\n3. Strong empirical trends: Forgetting-MarI outperforms or matches prior unlearning methods in both correlated and uncorrelated settings (Figures 2–5). It also exhibits smoother convergence and better continual unlearning stability."}, "weaknesses": {"value": "1. Experimental scale is limited. All experiments are on small to mid-scale models (GPT-2 Large, Llama-1B). It is unclear whether the proposed method scales to realistic 7B–70B LLMs, where unlearning challenges become severe. The tasks (Harry Potter and Careless People) are relatively narrow and do not demonstrate generalization beyond text completion.\n2. Weak connection between theory and empirical validation. The theoretical guarantees rely on information-theoretic quantities (MI, JSD) that are hard to estimate accurately in high-dimensional token distributions. The paper does not empirically verify how MI actually evolves during training or correlate it directly with unlearning success.\n3. Limited comparison and ablation. Missing quantitative comparison with recent adapter-based or counterfactual unlearning methods. No ablation on the contribution of the MI loss term.\n4. Practical concerns. The computation of per-token JSD may be costly for long sequences; the paper does not discuss training overhead. The method relies on manual tuning of λ, without a principled way to choose it.\n5. Poor writing. The submission suffers from the poor organization and writing, especifically the experiment section."}, "questions": {"value": "The author are expected to address the concerns raised in the weakness part, or give reasonable explanation about it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7RUB9dMscX", "forum": "F02KXwqFdQ", "replyto": "F02KXwqFdQ", "signatures": ["ICLR.cc/2026/Conference/Submission13935/Reviewer_BufG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13935/Reviewer_BufG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761595261741, "cdate": 1761595261741, "tmdate": 1762924438400, "mdate": 1762924438400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FORGETTING-MARI, a novel and effective framework for machine unlearning, specifically tailored for Large Language Models (LLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Relevance and Impact: The paper tackles a highly critical and timely challenge in AI—efficient and effective model unlearning for LLMs. Given the size and computational cost of LLMs, incremental unlearning solutions that preserve utility are essential for real-world deployment and regulatory compliance.\n\n2. Theoretical Clarity: The concept of Marginal Information Regularization is theoretically intuitive and well-aligned with the goal of mitigating catastrophic forgetting. By directly targeting the maximization of retain-set knowledge while minimizing forget-set knowledge, the method provides a targeted optimization objective.\n\n3. Empirical Superiority: The experimental results strongly support the claims, showing that FORGETTING-MARI significantly outperforms existing unlearning techniques in balancing the privacy-utility trade-off. This suggests a more robust and practical solution for LLM unlearning."}, "weaknesses": {"value": "1. Utility Trade-off on General Capabilities: The proposed method exhibits the lowest performance on the MMLU general capability benchmark compared to baselines (Table 5). This suggests a potential trade-off where the high unlearning efficacy might be achieved at the cost of a significant reduction in the model's general utility. A successful unlearning strategy should aim to maximize forgetting while minimally impacting core, general knowledge.\n\n2. Limited Cross-Scenario Generalization and Scalability: The current experimental setup limits the confidence in generalization. Specifically, the two forgetting scenarios (\"FORGETTING HARRY POTTER\" and \"FORGETTING CARELESS PEOPLE\") are tested exclusively on different base models (GPT-2 Large and Llama-3.2-1B, respectively). It is recommended to perform cross-scenario evaluation (e.g., FORGETTING HARRY POTTER on Llama-3.2-1B and vice versa) to better demonstrate robustness across different model architectures and tasks. \n\n3. Additionally, the models tested are relatively small (up to 1B parameters); results on larger models (e.g., 7B) are necessary to confirm scalability and practical applicability.\n\n4. In the second subplot of Figure 3, some lines extend beyond the frame of the plot. It is recommended to increase the range of the vertical axis."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OA2qvNwm2W", "forum": "F02KXwqFdQ", "replyto": "F02KXwqFdQ", "signatures": ["ICLR.cc/2026/Conference/Submission13935/Reviewer_1K5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13935/Reviewer_1K5z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967345819, "cdate": 1761967345819, "tmdate": 1762924437938, "mdate": 1762924437938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Forgetting-MarI, a novel LLM unlearning method that aims to remove the influence of data to be forgotten while preserving knowledge from the retain data. Unlike prior approaches that often over-unlearn information and harm model performance, Forgetting-MarI explicitly penalizes the marginal information contributed by the forget set. This enables the method to provide a provable upper bound on the residual influence of the forgotten data, ensuring undetectability and privacy compliance. Experimental results show that Forgetting-MarI achieves more reliable forgetting and better retention of general model utility compared to existing unlearning methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] **Interesting conceptual direction.** The idea of designing an unlearning loss derived from information-theoretic principles is conceptually interesting. The focus on isolating and penalizing marginal information, if made rigorous and well-justified, could offer a new angle for formalizing unlearning objectives in LLMs."}, "weaknesses": {"value": "- [W1] **Poor presentation and clarity.** The paper is difficult to follow overall and would benefit from a substantial reorganization and clearer exposition. Several key issues include:\n  - The classification of unlearning methods into “marginal information unlearning” and “full unlearning” is not well-motivated. As the paper itself notes, most existing methods already mix forgetting and retention signals, making them marginal in nature. Therefore, the distinction does not meaningfully clarify what is unique about Forgetting-MarI.\n  - Notation and definitions in Section 2 are insufficiently explained. For example, what does $V^T$ represent? How is addition between distributions $p_t^r$ and $p_t^u$ defined? What does $M$ denote in Theorem 2.2? Without clear definitions, the theoretical framework is hard to interpret.\n  - The theoretical results in Section 2.2 are difficult to understand and lack intuitive explanations. The implications of the theorems are not well connected to the practical advantages of Forgetting-MarI over other unlearning methods such as gradient ascent or preference-based unlearning.\n  - Algorithm 1 is presented in an unreadable form, with long mathematical expressions rather than structured pseudocode. Rewriting the algorithm with modular pseudocode steps would make the method far clearer.\n- [W2] **Insufficient and unconvincing experimental design.** The experiments do not convincingly support the claims made in the paper:\n  - The Continual Unlearning setup does not actually capture a continual learning scenario. The flat baseline curves in Figure 3 suggest that the experiments were simply run across multiple epochs rather than sequentially unlearning new forget sets, which fails to reflect the continual unlearning problem.\n  - The synthetic setup where forget and retain sentences are artificially interleaved is unrealistic and not well motivated. This design weakens the claimed contribution, as real-world unlearning often involves overlapping but distinct datasets (e.g., similar topics or styles), not artificially mixed sequences. The experimental focus on book datasets such as Harry Potter also does not align with the intended motivation of removing marginal information from overlapping textual domains. Benchmarks such as TOFU [A], MUSE [B], or WMDP [C] could be more appropriate and realistic testbeds.\n  - For several experiments, F-MarI is compared only against baselines and not existing unlearning methods. A list of such experiments are: general capability measurements with WikiText and MMLU (Tables 4 and 5), another set of general capability measurements on ARC-Easy and PIQA (Table 6), and forget set detection (Figure 6). Without such comparisons, it is difficult to assess the claimed superiority of the proposed method.\n\n[A] TOFU: A Task of Fictitious Unlearning for LLMs. COLM 2024.\\\n[B] MUSE: Machine Unlearning Six-Way Evaluation for Language Models. ICLR 2025.\\\n[C] The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning. ICML 2024."}, "questions": {"value": "- [Q1] In Section 4.3, the fine-tuning setting cites the TOFU benchmark. Does this mean the experiments followed the TOFU fine-tuning pipeline but used a custom dataset derived from *Careless People: A Cautionary Tale of Power, Greed, and Lost Idealism*?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Abe7bgVo5V", "forum": "F02KXwqFdQ", "replyto": "F02KXwqFdQ", "signatures": ["ICLR.cc/2026/Conference/Submission13935/Reviewer_uBXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13935/Reviewer_uBXm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13935/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984787827, "cdate": 1761984787827, "tmdate": 1762924437557, "mdate": 1762924437557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}