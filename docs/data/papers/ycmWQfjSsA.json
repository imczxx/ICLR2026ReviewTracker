{"id": "ycmWQfjSsA", "number": 22081, "cdate": 1758325729511, "mdate": 1759896887550, "content": {"title": "SoftStep: Learning Sparse Similarity Powers Deep Neighbor-Based Regression", "abstract": "Neighbor-based methods are a natural alternative to linear prediction for tabular data when relationships between inputs and targets exhibit complexity such as nonlinearity, periodicity, or heteroscedasticity. Yet in deep learning on unstructured data, nonparametric neighbor-based approaches are rarely implemented in lieu of simple linear heads. This is primarily due to the ability of systems equipped with linear regression heads to co-learn internal representations along with the linear head's parameters. To unlock the full potential of neighbor-based methods in neural networks we introduce SoftStep, a parametric module that learns sparse instance-wise similarity measures directly from data. When integrated with existing neighbor-based methods, SoftStep enables regression models that consistently outperform linear heads across diverse architectures, domains, and training scenarios. We focus on regression tasks, where we show theoretically that neighbor-based prediction with a mean squared error objective constitutes a metric learning algorithm that induces well-structured embedding spaces. We then demonstrate analytically and empirically that this representational structure translates into superior performance when combined with the sparse, instance-wise similarity measures introduced by SoftStep. Beyond regression, SoftStep is a general method for learning instance-wise similarity in deep neural networks, with broad applicability to attention mechanisms, metric learning, representational alignment, and related paradigms.", "tldr": "We introduce a module for learning sparse instance-wise measures of similarity in neural networks and prove its usefulness in regression.", "keywords": ["supervised learning", "sparse attention", "representation learning", "representational alignment", "directional graph attention"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf17b89584969f96009c3a32fb02985a683c99be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an module named SoftStep, designed to learn sparse, sample-adaptive similarity measures in neural networks and integrates it with a nearest-neighbor-based regression approach. The core idea is novel, supported by in-depth theoretical analysis, and demonstrates potential advantages over linear regression heads across multiple datasets. However, the paper exhibits notable limitations in the breadth and depth of experiments, evaluation of computational efficiency, and the completeness of certain theoretical analyses, which currently affect its maturity and acceptability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) SoftStep provides a learnable sparse similarity metric, which differs from existing methods that rely on fixed sparsity patterns.\n\n2) The paper derives implicit geometric constraints from the MSE loss, revealing the structural properties underlying neighbor-based regression.\n\n3) Experimental results show some improvements over linear heads and traditional NCA/kNN methods across multiple tasks."}, "weaknesses": {"value": "1) The experimental evaluation appears limited in scope, as it lacks comparisons with contemporary sparse attention mechanisms such as Sparsemax and Sparse Transformer. The current baseline methods—limited to linear attention and NCA—are insufficient to comprehensively validate the method's advantages. Furthermore, the proposed approach includes multiple variants that complicate the interpretation of results and dilute the clarity of the key contributions.\n\n2) The method is only validated on regression tasks, leaving open questions about the method's efficacy in classification and unsupervised learning settings. A broader discussion is suggested to establish general applicability.\n\n3) The effects of SoftStep parameters (l, u, t) and sparsity variation have not been analyzed. Without proper ablation studies and sensitivity analysis, the robustness of the proposed method remains unverified. The absence of discussion on parameter selection guidelines significantly limits the practical utility of this work.\n\n4) The paper omits a critical analysis of computational efficiency, including training/inference speed, memory footprint, and scalability. Without these metrics, the practical utility of SoftStep for real-world applications remains uncertain, particularly for resource-constrained environments or large-scale deployments."}, "questions": {"value": "1) Can SoftStep be applied to classification tasks? What modifications would be required to adapt it for that setting?\n\n2) Compared with existing sparsification methods such as Sparsemax, what are the main advantages of SoftStep?\n\n3) How are the parameters $l$, $u$, $t$ of SoftStep initialized and updated during training? Are there any issues of numerical instability or convergence?\n\n4) In Table 1, the NCA-i method shows large performance variance on the MedSegBench dataset. What could be the possible reasons for this instability?\n\n5) What is the relationship between the SOFTKSTEP function in the algorithm and the SoftStep function defined in Equation (1)? Why do they use different parameterizations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dACGO40Wif", "forum": "ycmWQfjSsA", "replyto": "ycmWQfjSsA", "signatures": ["ICLR.cc/2026/Conference/Submission22081/Reviewer_N2ZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22081/Reviewer_N2ZZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655410011, "cdate": 1761655410011, "tmdate": 1762942058310, "mdate": 1762942058310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- SoftStep proposes a differentiable, neighbor-based regression head that learns sparse similarities with learnable \\ell,u,t. it can be easily plugged into soft kNN / NCA for end-to-end training.\n- The pipeline: pretrained encoder → small MLP to an embedding → SoftStep → neighbor-weighted prediction.\n- The claim is that replacing the usual linear head with SoftStep-based neighbor regression improves MSE and yields more structured embeddings.\n- Experiments span several regression datasets (vision / audio / text), reporting consistent gains over linear heads and vanilla NCA/kNN.\n- However, despite stating easy applicability to diverse architectures, the paper largely evaluates on limited backbones and scales, leaving generality and efficiency underexplored."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, modular formulation: SoftStep is a drop-in, differentiable sparsifier for neighbor heads.\n- Empirical results are consistently better than linear regression heads (and vanilla neighbor baselines) across multiple regression datasets.\n- Theoretical intuition is reasonable: neighbor-based MSE induces pair/triplet structure.\n- The method exposes meaningful knobs ((\\ell,u,t); global vs instance-wise) that could be useful for controlling sparsity and locality."}, "weaknesses": {"value": "- Backbone/scale generalization is thin: claims of easy applicability are not substantiated across diverse and larger encoders (e.g., ViT/ConvNeXt/BERT) or large-scale datasets; results remain small to mid-scale.\n- Comparisons are not strong enough: lacks head-to-head against robust modern alternatives for regression/metric learning (e.g., recent metric learning algorithms).\n- Attribution is unclear: separate SoftStep from soft-rank effects: run soft-rank on/off ablations, and test rank-free NCA with SoftStep only to see whether gains persist without soft-rank.\n- Efficiency and scaling are underreported: neighbor heads can be costly."}, "questions": {"value": "- Can you evaluate on diverse backbones and larger benchmarks (e.g., ViT/ConvNeXt for vision, BERT/RoBERTa for text; ImageNet-scale or comparable regression tasks) to substantiate the “easy to apply” claim?\n- Under identical training, how does SoftStep compare with strong regression/metric baselines?\n- Please provide isolation ablations: (i) SoftStep on/off with identical soft-kNN/NCA, (ii) global vs instance-wise (\\ell,u,t) to identify what actually drives the gain.\n- (If applicable) Could you include non-regression tasks (classification/ranking) to demonstrate broader utility beyond the current regression focus?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Y3FC5XEQA", "forum": "ycmWQfjSsA", "replyto": "ycmWQfjSsA", "signatures": ["ICLR.cc/2026/Conference/Submission22081/Reviewer_AKZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22081/Reviewer_AKZd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990641730, "cdate": 1761990641730, "tmdate": 1762942057876, "mdate": 1762942057876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a differentiable regression head which, when paired with soft kNN or neighborhood component analysis operations, allows learning a nearest-neighbor based classifier at the end of a neural network. The authors show that, on a variety of datasets, this neighborhood-based classification outperforms linear regression heads."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea itself is quite neat. As I understand it, the authors allow for learning a smooth function over the number of neighbors to use when doing predictions. It is also nice that it works with both the differentiable knn and the neighborhood component analysis set ups. The presentation in the first three sections in particular is very easy to follow."}, "weaknesses": {"value": "I think the paper has three primary weaknesses.\n\nThe first is that section 4 is quite strange in how it is presented. It seems that the authors are trying to make theoretical statements verifying that their approach works. However, it's not clear what precisely what is being shown and it seems there are some mistakes in this section? For example, the phrase \"we demonstrate that a neighbor-based regression model paired with MSE loss yields implicit optimization conditions for structuring pairs of points in the embedding space with respect to their labels\" is never formalized. It would be good if there was a theorem or lemma statement which specified precisely what the authors are proving, followed by a clear proof. The same goes for the bolded sentences in section 4, these should be stated as corollaries and proven. Regarding the mistakes, I'm confused why the authors start discussing terms of the form $\\Delta_{ij}^2 p_{ij}^2$ on line 308. The previous equation had $\\sum_i \\left( \\sum_j \\Delta_{ij}^2 p_{ij} \\right) ^2$. These are different things. It's also not clear to me where the equation for the optimal triplet embedding geometry is coming from. It's also not clear what we are trying to do with it or how it relates to the softstep method. Again, these statements would be easier to interpret if there were formal theorem/lemma/proposition/corollary statements which made them clear.\n\nThe second weakness is that the experiments are confusing and the results are hard to interpret. Below is a non-exhaustive list of questions which I have after looking through the results:\n- Why are these the datasets which were used? Why not MNIST, CIFAR10 and Imagenet for computer vision tasks and/or standard NLP ones for NLP tasks?\n- What are the backbone architectures being optimized?\n- Why was trainin always done using pre-trained networks rather than training ones from scratch with the softstep algorithm? What happens if you train from scratch?\n- How can NCA g ever outperform NCA i? Isn't NCA g simply a subset of the expressivity of NCA i? Same for DiffKNN.\n\nFinally, the third weakness of the paper is that the utility of softstep is not made fully clear. As I understand it, when paired with a radial regression algorithm, softstep allows learning the parameters which control the radii. If this is the case, then it seems like it should be compared against more than just a linear classifier. Specifically, we should compare against other known regression algorithms that can do similar radial, nonlinear classification and regression tasks. For example, what happens if we simply use the softrank and  NCA algorithms without softstep? Or differentiable PSD kernels? Similarly, why do we require the logarithm when applying softstep into $Sim(Z, Z_N)$?\n\nIn summary, the above weaknesses make it difficult to judge the paper's presented algorithm fully. The theoretical claims are not sufficiently clear and the experimental analysis leaves many questions unanswered. Although the work is developing an interesting idea, I do not think the analysis is sufficiently robust to convince a reader that the method is clearly superior."}, "questions": {"value": "See the above discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H1zUqPVdZm", "forum": "ycmWQfjSsA", "replyto": "ycmWQfjSsA", "signatures": ["ICLR.cc/2026/Conference/Submission22081/Reviewer_KjUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22081/Reviewer_KjUS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014042311, "cdate": 1762014042311, "tmdate": 1762942057657, "mdate": 1762942057657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes replacing the linear output head with an output head based on differentiable nearest neighbors.\n\nIn order to do this, the paper proposes the SoftStep architecture, which is combined with NCA or differentiable k-nearest neighbors.\n\nThis allows the network to jointly learn an embedding, a sparse set of nearest neighbors, and output labels corresponding to a weighted average of the labels on these neighbors."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a new read-out head architecture that appears to have better performance than linear read-out on a suite of benchmark datasets.\n\nThe method is novel to the best of my knowledge.\n\nThe method is well motivated, and generally well presented."}, "weaknesses": {"value": "* A nearest neighbor head seems much harder to scale in terms of number of data points than a linear head. New ideas seem to be needed to make this method work for larger datasets. Accordingly, the experiments are on relatively smaller scale benchmarks.\n\n* The presentation was generally good, but I was confused with some aspects: see my questions below."}, "questions": {"value": "* I didn't understand what is meant by global vs. model-level parameters for SoftStep. Could you please \n\n* What is the asymptotic cost of this method in terms of number of data points? Is it quadratic in the dataset size? That could be good to include. More generally, is this method limited to smaller datasets for now?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lCYvoYpEul", "forum": "ycmWQfjSsA", "replyto": "ycmWQfjSsA", "signatures": ["ICLR.cc/2026/Conference/Submission22081/Reviewer_PMLH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22081/Reviewer_PMLH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22081/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058296645, "cdate": 1762058296645, "tmdate": 1762942057308, "mdate": 1762942057308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}