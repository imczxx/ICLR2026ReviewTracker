{"id": "d491lY8IUl", "number": 9397, "cdate": 1758121015544, "mdate": 1759897727423, "content": {"title": "Rule-Based Reference Updates after R1-Based Post Reinforcement Learning For Small Reasoning Language Models", "abstract": "Inference scaling improves LLM reasoning, with reinforcement learning as a key driver. Although, post-training reinforcement learning and its curriculum learning variants offer significant benefits in enhancing the reasoning ability of large language models, we designate this process as Phase 1. Following this, we propose Phase 2: rule-based reference model updates in reinforcement learning after Phase 1 to explore the potential of reference model updates following R1-Like reinforcement learning. In details, we introduce a rule-based reference updates reinforcement learning approach continues to enhance the reasoning capabilities of small-sized large language models after current classical post-training reinforcement learning. In particular, a $1.5B$-parameter LLM achieves $60.2\\%$ on AIME24, $48.2\\%$ on AIME25 and $91.5\\%$ on Math500 and $1.5\\%-4\\%$ score improvement on AMC, Minera and Olympia. These results, enabled by the proposed rule-based reference model updates reinforcement learning algorithm, demonstrate math reasoning capabilities comparable to O1-mini/O3-mini—achievable within a typical school laboratory setting. In addition, we open-source both the dataset and model checkpoints to support future research in large-scale reinforcement learning for LLMs.", "tldr": "", "keywords": ["reinforcement learning", "group policy", "adaptive learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5c216d7ad9c80713d9f11bf68758a54ba40bc3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a Phase-2 “rule-based reference model update” on top of an R1/GRPO-like RL phase. During Phase-2, the authors (i) filter training samples by keeping medium-score rollouts using fixed thresholds (0.2, 0.8), and (ii) periodically replace the frozen reference model with the current actor if an external evaluation improves, where the external gate is AIME24 Pass@1; otherwise, the reference stays fixed. Reported gains on a 1.5B model include AIME24 60.2, AIME25 48.2, MATH500 88.0–91.5, Olympia 59.1, versus two 1.5B baselines (FastCuRL-V3 and “OpenNemotron-1.5B”)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Simple recipe for small models. The update rule is easy to implement on top of GRPO/R1 training and shows consistent deltas over 1.5B baselines on AIME24/25 and other math sets. \n2. Some ablations are provided. The paper ablates the two Phase-2 components (batch filtering and step-wise reference update) across several benchmarks."}, "weaknesses": {"value": "1. Evaluation leakage. Using AIME24 as the gating signal during training contaminates the test set and likely inflates reported gains. \n2. Claims contradict results. The introduction says the 1.5B model outperforms O1-mini/O1, but Table 1 shows lower AIME24 scores (60.2 vs. 70.0/71.5). \n3. Weak novelty. The core ideas—periodic reference resets and thresholded mid-score filtering—are established in many RL works; for example, NVIDIA’s ProRL-v2 explicitly uses KL-regularized training with periodic reference policy resets."}, "questions": {"value": "1. KL baselines. Since prior work finds that $\\beta=0$ often yields comparable RLFT gains [2], will you report a matched-compute $\\beta=0$ baseline to quantify your method's incremental benefit given the added compute (KL term)? \n2. Leakage and gates. Why use AIME24 as a training gate when it is also a reported test set, and how do results change if you gate on a non-reported, distribution-shifted dev set or use a fixed-interval refresh instead? \n3. Hyperparameter sensitivity. How sensitive are results to the medium-score thresholds $0.2,0.8$, and can you provide a brief sensitivity sweep?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "On80SfOtHV", "forum": "d491lY8IUl", "replyto": "d491lY8IUl", "signatures": ["ICLR.cc/2026/Conference/Submission9397/Reviewer_ASuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9397/Reviewer_ASuD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485068398, "cdate": 1761485068398, "tmdate": 1762921006206, "mdate": 1762921006206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-phase approach to enhance the reasoning abilities of small-scale language models through reinforcement learning (RL). In Phase 1, an R1-like RL method is applied to train a policy model with a frozen reference model. Phase 2 introduces rule-based reference model updates, improving reasoning performance further. Experiments show significant improvements on math reasoning benchmarks, with a 1.5B-parameter model outperforming existing models. The approach demonstrates the potential of post-RL reference model updates for small LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel two-phase approach that combines reinforcement learning (RL) and rule-based reference model updates, which enhances reasoning capabilities in small-scale language models (LLMs)"}, "weaknesses": {"value": "1. The paper feels like a work in progress, with several issues in citation formatting (e.g., bad referencing format in Section 3.1) and unclear notation in Section 3.2 (e.g., \"where ti ¡ ttotal\" is not explained properly). These issues reduce the clarity and accessibility of the paper, making it difficult for readers to fully understand the methodology.\n\n2. The experimental tables, particularly those in Section 4, contain several inaccuracies, such as the -56.2 value in the model performance table, which seems incorrect. The presentation of results often feels incomplete, lacking sufficient explanation of why the proposed method works better and where the improvements come from. A more detailed analysis of the results and clearer links to the methodology would strengthen the contribution.\n\n3. Lack of Comparison to Related Work: The concept of Rule-Based Reference Updates bears similarities to the reference reset techniques in other RL approaches [1]. The authors should include a comparison with these existing methods and discuss any differences in their approaches. Such a discussion could highlight the novelty of the proposed method and demonstrate its advantages more effectively.\n\n\n[1] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NojiY1qEtO", "forum": "d491lY8IUl", "replyto": "d491lY8IUl", "signatures": ["ICLR.cc/2026/Conference/Submission9397/Reviewer_xttr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9397/Reviewer_xttr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813215014, "cdate": 1761813215014, "tmdate": 1762921005683, "mdate": 1762921005683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a two-phase reinforcement learning framework designed to enhance the mathematical reasoning abilities of small language models (LLMs). The primary contribution is \"Phase 2,\" a novel method that follows an initial RL training phase (\"Phase 1\"). In Phase 2, the reference model used in the Group-Relative Policy Optimization (GRPO) algorithm is not kept frozen but is conditionally updated with the weights of the current policy. This update is governed by a rule-based strategy that identifies training steps with maximal performance improvement, coupled with a safeguard evaluation on a benchmark dataset to ensure stability. The authors demonstrate the effectiveness of their approach by applying it to a 1.5B parameter model, achieving state-of-the-art results for its size on several challenging math reasoning benchmarks, including MATH500, AIME24, and AIME25."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work addresses the important and practical challenge of improving the reasoning capabilities of small language models (1.5B parameters). Making smaller, more accessible models perform on par with much larger ones is a significant contribution to the field, with implications for computational efficiency and democratization of powerful AI."}, "weaknesses": {"value": "1.\tCould the authors provide more specific details on the hyperparameters used in the training process? In particular: What is the value of the KL coefficient (β) used in the GRPO objective function (Equation 2)?  Could you specify the exact training steps at which the reference model updates occurred during your experiments?\n2.\tThe paper's presentation could be significantly improved. Several tables, notably Table 1 and Table 2, extend beyond the page margins, which detracts from readability. The citation style for references appears inconsistent, and the overall narrative flow could be tightened to improve logical coherence. A thorough revision for clarity and formatting would strengthen the manuscript.\n3.\tThe proposed \"Rule-Based Reference Model Updates\" method appears conceptually similar to recent works on dynamic reference models in RL for LLMs, such as ProRL and Dapo[1,2]. Could the authors elaborate on the key differences between their approach and these methods? \n4.\tThe paper suffers from two main issues: limited novelty and a confusing presentation. I strongly suggest a major reorganization to improve the paper's clarity and better articulate its core contributions.\n\n\n[1] Liu M, Diao S, Lu X, et al. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models[J]. arXiv preprint arXiv:2505.24864, 2025.\n\n[2] Yu Q, Zhang Z, Zhu R, et al. Dapo: An open-source llm reinforcement learning system at scale[J]. arXiv preprint arXiv:2503.14476, 2025."}, "questions": {"value": "See Weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aAhjoXWJht", "forum": "d491lY8IUl", "replyto": "d491lY8IUl", "signatures": ["ICLR.cc/2026/Conference/Submission9397/Reviewer_ynuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9397/Reviewer_ynuZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979273275, "cdate": 1761979273275, "tmdate": 1762921004648, "mdate": 1762921004648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a two-phase reinforcement-learning pipeline for ~1.5B-parameter reasoning models. Phase 1 adopts GRPO/GRPO+ with four practical tweaks—dropping the KL penalty, relaxing the upper clip (“Clip-Higher”), dynamic sampling to filter extreme-reward traces, and a token-level policy gradient—plus a curriculum that ramps up task difficulty to cultivate longer, cleaner chains of thought.\nPhase 2 is the core contribution: a rule-based reference-model update layered on GRPO. First, training batches keep only mid-difficulty samples using fixed score thresholds (implemented as 0.2 and 0.8). Second, at specific steps identified by a running improvement signal, the actor is evaluated on AIME24 with 16 samples; only if it exceeds the current reference is the actor hard-copied into the reference, otherwise the reference remains frozen."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Introduces a practical, performance-gated reference update for policy optimization: instead of a fixed schedule or permanently frozen reference, the paper uses an explicit improvement signal to decide when to replace the reference model—simple, interpretable, and uncommon in prior GRPO/PPO work.\n- Proposes a mid-difficulty batch filter that intentionally excludes both trivial and hopeless rollouts during policy updates; this selective data curation is a fresh way to stabilize learning for lengthy chain-of-thought traces.\n- Emphasizes applicability to resource-limited settings (1.5B models) where many prior RL-for-LLM techniques either fail to scale down or require heavy compute, making the approach novel in its focus on practicality and cost-effectiveness."}, "weaknesses": {"value": "- In Figure 3 you compare GRPO with your method. The GRPO scores keep decreasing after training, which looks more like a problem with the training process.\n- The abstract claims \"comparable to O1-mini/O3-mini.\" However, the tables show AIME24 60.2 vs O1-Mini 70.0 and AIME25 48.2 vs O3-Mini 86.5, which is not comparable in absolute terms.\n- The formatting of Tables 2 and 3 is nonstandard—specifically, they exceed the permitted size.\n- The abstract promises open-sourcing data and checkpoints, but the draft provides no links or hashes. Please ensure the artifacts and exact train/eval lists are released.\n- Simultaneously updating the actor and the reference model during RL training is time- and resource-intensive, yet yields little measurable improvement."}, "questions": {"value": "- Why hard copy of actor->reference at step i? Did you try soft updates (Polyak/EMA), fixed schedules (every T steps), or performance-proportional copy? Provide an ablation.\n- In Figure 3 the GRPO curve decays after training — can you provide diagnostics so we can identify whether this is an optimization instability or a methodological effect?\n- Have you tested lower-cost alternatives (EMA, distillation into the reference at checkpoints, or selective parameter copying) to achieve similar stability/benefit with less overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yYSy9Wt9RK", "forum": "d491lY8IUl", "replyto": "d491lY8IUl", "signatures": ["ICLR.cc/2026/Conference/Submission9397/Reviewer_vzMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9397/Reviewer_vzMf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9397/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762402283207, "cdate": 1762402283207, "tmdate": 1762921004179, "mdate": 1762921004179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}