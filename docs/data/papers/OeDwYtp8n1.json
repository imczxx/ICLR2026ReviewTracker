{"id": "OeDwYtp8n1", "number": 2687, "cdate": 1757204050147, "mdate": 1759898133333, "content": {"title": "Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control", "abstract": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge dataset, as well as RLBench and SIMPLER benchmarks, demonstrate that our method establishs new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation. Additional visual results and reproducible code are available at our anonymous project page: robomaster2025.github.io.", "tldr": "RoboMaster learns an interactive world simulator for robotic manipulation, based on a video diffusion model conditioned on a collaborative trajectory, which controls the motions of both the robotic arm and the manipulated object.", "keywords": ["Video Generation", "Robotic Manipulation", "Trajectory Control"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08fb8fb08ae13d7098f2ba978805325c1ee1d2a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Summary :\n\nRoboMaster is a trajectory‑conditioned video diffusion framework for robotic manipulation that jointly controls the robot arm and the object with a single collaborative trajectory. By decomposing each task into pre‑interaction, interaction, and post‑interaction phases and using mask‑based, appearance‑ and shape‑aware embeddings, it avoids the feature entanglement seen when controlling objects separately. It achieves state‑of‑the‑art visual fidelity and trajectory accuracy and improves downstream action planning.\n\n\nContributions:\n\nCollaborative trajectory control for interaction: A unified trajectory that models pre-interaction → interaction → post-interaction phases, switching the dominant controller (arm → object → arm) to capture inter-object dynamics and avoid feature fusion during contact.\n\nAppearance and shape aware subject embeddings: Mask-based tokens drawn from the initial frame’s VAE latents, expanded into shape-aware circular volumes along the trajectory to preserve object identity across frames.\n\nCausal latent propagation: Frame-to-frame latent carryover (with overwrite at trajectory points) for smoother, temporally consistent motion during generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel collaborative trajectory design\n\nIntroduces a new way to model robot–object interactions using a single collaborative trajectory split into pre-interaction, interaction, and post-interaction phases.\n\nAvoids the feature entanglement issues (e.g., missing or distorted objects) that plague prior methods like Tora and DragAnything.\n\n2. High visual and physical realism\n\nProduces smoother, more physically plausible manipulation videos with consistent object identities across frames.\n\nQuantitatively achieves better FVD, PSNR, and SSIM, and lower trajectory errors on the Bridge benchmark.\n\n3. Generalization and robustness\n\nHandles diverse manipulation skills and in-the-wild scenarios.\n\nRobust to imperfect user input—works with coarse or partial object masks and noisy trajectories."}, "weaknesses": {"value": "1. Restricted to 2D pixel space\n\nThe system does not yet model depth or 3D geometry; this limits physical accuracy and makes 3D control (e.g., precise grasping) difficult.\n\n2. Possible failure on out-of-domain inputs\n\nCan produce incomplete or distorted objects when encountering unseen categories or backgrounds.\n\nStill relies on training data diversity to generalize effectively.\n\n3. Semantic dependency on user input\n\nRelies on accurate prompts and roughly correct masks. Misleading text or poor masks may still degrade quality."}, "questions": {"value": "Question 1: How does the model determine the precise transition points between these phases in practice, especially when the temporal boundaries of “interaction” are ambiguous or vary across tasks?\n\n\nQuestion 2: How well would RoboMaster generalize to unseen robot morphologies or entirely different kinematic structures, and what adaptations (e.g., in trajectory representation or latent space) would be needed for that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MirAODUw7r", "forum": "OeDwYtp8n1", "replyto": "OeDwYtp8n1", "signatures": ["ICLR.cc/2026/Conference/Submission2687/Reviewer_1NRP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2687/Reviewer_1NRP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761323195077, "cdate": 1761323195077, "tmdate": 1762916331098, "mdate": 1762916331098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RoboMaster to model interactions between a robotic arm and objects, dividing the interaction process into three stages: before and after the interaction it mainly controls the arm’s motion, while during the interaction it controls the object’s motion. The effectiveness of the proposed method is validated through visual results and simulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper argues that interaction should originate from multiple entities, including the arm and the object—this is a novel viewpoint.\n\n2. RoboMaster exhibits impressive OOD generalization."}, "weaknesses": {"value": "1. The motivation for decoupling the control signals is unclear; it is not explained how 2D trajectories help the robot learn, and the paper does not discuss the overall design in detail.\n\n2. Section 4.5 is too brief, making it difficult to verify the method’s effectiveness for the robot; visual quality is not the core of the research—the core is whether the designed method can effectively aid robot learning."}, "questions": {"value": "Could you explain in detail how the proposed model is applied to robot learning, and on that basis clarify the motivation for decomposing the action/control signals? At present, the stated motivation seems to be driven by generating better visual effects."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t7cA1TS4rH", "forum": "OeDwYtp8n1", "replyto": "OeDwYtp8n1", "signatures": ["ICLR.cc/2026/Conference/Submission2687/Reviewer_fCEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2687/Reviewer_fCEc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922844374, "cdate": 1761922844374, "tmdate": 1762916330976, "mdate": 1762916330976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method tackles the task of trajectory-conditioned robotic manipulation video synthesis through a novel demonstration trajectory decomposition scheme that separates trajectories into a pre-interaction, interaction and post-interaction phase. This novel decomposition helps alleviate feature confusion issues observed in previous works. The method is evaluated against several baselines on the tasks of video synthesis and downstream robotic manipulation (through inverse dynamics), and manages to outperform them."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to understand. The method outperforms the baselines against which it is compared. The design choices are sensibly ablated. The work contributes a dataset of 21.000 human-annotated 2D robot manipulator trajectories. The work includes an honest discussion of its limitations."}, "weaknesses": {"value": "The proposed method operates purely in image space: the generated trajectories require postprocessing by an inverse kinematics model and are not guaranteed to be realistic or executable.\nUnlike its baselines, the method requires a segmentation of the provided trajectory into multiple stages by the user.\nThe manual masking of the interacted object could be replaced by an automatic grounding and segmentation.\nA purely 2D trajectory input is very limiting, yet this is somewhat alleviated by the ability to describe the desired trajectory more specifically through a textual input."}, "questions": {"value": "Can the method handle multi-step interactions, such as grasping a sponge, rotating it on a plate several times, then moving the robotic manipulator away?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vf6Wb8F7l8", "forum": "OeDwYtp8n1", "replyto": "OeDwYtp8n1", "signatures": ["ICLR.cc/2026/Conference/Submission2687/Reviewer_ydrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2687/Reviewer_ydrE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981397258, "cdate": 1761981397258, "tmdate": 1762916330795, "mdate": 1762916330795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}