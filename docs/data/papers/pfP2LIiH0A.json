{"id": "pfP2LIiH0A", "number": 21188, "cdate": 1758314718344, "mdate": 1759896936996, "content": {"title": "DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-tuning", "abstract": "Differentially private (DP) synthetic data generation plays a pivotal role in advancing applications of large language model (LLM) involving private user data. \nGenerating synthetic data that resemble eyes-off private corpora involves a difficult trade-off. On one hand, DP finetuning methods offers formal privacy guarantees, yet requires the data owner to expose the raw content for model training. However, methods that avoid direct exposure to private data are bounded by an off-the-shelf, un-finetuned model, whose outputs often lack domain fidelity. %To get the best of both worlds, the research question is \nCan we train an LLM to generate high-quality synthetic text without providing it individual private examples?\nIn this work, we introduce Differentially Private Reinforcement Fine-Tuning (DP-RFT), an online reinforcement learning algorithm for synthetic data generation with LLMs. DP-RFT leverages DP-protected nearest-neighbor votes from an eyes-off private corpus as a reward signal for on-policy synthetic samples generated by an LLM. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO). We evaluate DP-RFT for long-form and domain-specific synthetic data generation, such as news articles, meeting transcripts, and medical article abstracts. Our experiments show that DP-RFT significantly outperforms private evolution and DP fine-tuning methods, such as DP-SGD, in terms of both the fidelity and downstream utility of the generated synthetic data.", "tldr": "We propose DP-RFT, a differentially private method to train an LLM to generate high-fidelity synthetic text via reinforcement fine-tuning.", "keywords": ["large language model", "differential privacy", "synthetic data generation", "reinforcement fine-tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be20c6c180c36d738ded04627486d318ba172a15.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Differentially Private Reinforcement Fine-Tuning (DP-RFT), a new method for generating synthetic text from a private corpus without directly exposing the private data (\"eyes-off\") to the language model. The core idea is to use reinforcement learning (specifically, PPO) to fine-tune a large language model (LLM). The reward signal is derived from differentially private (DP) nearest-neighbor votes from the private corpus. This reward is based on the embedding similarity between synthetic samples and the private data, modified from prior work (Aug-PE) to use raw similarity scores instead of binary votes . To prevent reward hacking, this similarity reward is combined with a prompt-adherence reward generated by an LLM-as-a-judge. The authors evaluate DP-RFT on four datasets (news, transcripts, medical abstracts), claiming it significantly outperforms baselines like DP-SGD and Aug-PE in terms of synthetic data fidelity and downstream utility"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear Problem Formulation:** The paper addresses a significant and practical problem: generating high-fidelity synthetic text from a private corpus under \"eyes-off\" constraints, where models like DP-SGD (which require direct data access) are not viable.\n- **Solves a Key Baseline Limitation:** The paper correctly identifies a key weakness in the \"eyes-off\" baseline Aug-PE: because the LLM is frozen, it has \"limited steer-ability\" and is bounded by the quality of the off-the-shelf model's outputs. DP-RFT directly addresses this by fine-tuning the model, allowing it to adapt its weights to the private distribution."}, "weaknesses": {"value": "- **Ad-hoc Reward Engineering:** The method relies on dataset-specific reward engineering, such as adding special KL divergence terms for the WildChat and QMSum datasets, suggesting the base embedding-similarity reward is insufficient on its own.\n- **Weak Downstream Evaluation:** The claim of \"downstream utility\" is unsubstantiated. The evaluation only uses next-token prediction, which is a weak proxy, instead of testing on actual, relevant tasks like classification (for BBC News) or summarization (for QMSum).\n- **Incremental Contribution:** The paper's novelty is limited. It presents a straightforward combination of standard PPO with a DP embedding-similarity reward, which is a minor modification of prior work (Aug-PE) and conceptually similar to other recent RL-based method Hou et al. (2025), which used DPO (an offline RL algorithm) with embedding similarity as a reward.\n- **Overstated \"Eyes-off\" Premise:** The \"eyes-off\" claim is misleading. While the LLM trainer avoids direct data access, the reward calculator requires continuous, computationally expensive, and security-sensitive access to the entire private corpus's embeddings at every single training step, a significant practical limitation that is not discussed."}, "questions": {"value": "1. **Missing Related Work and Baselines.** A potentially significant piece of related work [1] appears to be missing. This work also deals with synthetic text generation for training LLMs, though via gradient matching. Given that both methods aim to create synthetic data for LLM training, would this work not constitute a relevant baseline or, at minimum, require discussion in your related work section?\n    \n    [1] Nguyen, Dang, et al. \"Synthetic text generation for training large language models via gradient matching.\" (arXiv:2502.17607, 2025)?\n    \n2. **Reward Hacking:** The $R_{prompt}$ term is introduced to mitigate \"reward hacking\"37. What happens if you remove it entirely and only use $R_{sim}$? How severe is the reward hacking? Does the model simply output copies or near-copies of the private data embeddings?\n3. **On Ad-hoc Rewards:** For QMSum and WildChat, you added KL-divergence-based reward terms. \n\n    a. Did the base method (using only embedding similarity) fail for these datasets? \n\n    b. Could you provide an ablation (like in Table 11, but for the main results) showing the impact of *not* including these terms? \n\n    c. How is a practitioner supposed to know which ad-hoc terms to add for a new dataset?\n\n4. **On Downstream Utility:** Can you provide evaluation results for more meaningful downstream tasks beyond next-token prediction? For example, please show results for classification on BBC News or summarization on QMSum, as this would be a far more compelling case for \"downstream utility.\"\n5. **Member inference attack**. Have you considered evaluating the robustness of your synthetically generated data against member inference attacks?\n6. **Cross-model transferability**. Is the synthetic dataset transferrable across different language model?\n7. **Computational Cost:** What is the **computation time** and general computational overhead of the DP-RFT training process compared to the baselines like Aug-PE (which is iterative but does not fine-tune) and standard DP-FT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bPzOV6Hivc", "forum": "pfP2LIiH0A", "replyto": "pfP2LIiH0A", "signatures": ["ICLR.cc/2026/Conference/Submission21188/Reviewer_XTBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21188/Reviewer_XTBv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859985915, "cdate": 1761859985915, "tmdate": 1762941593836, "mdate": 1762941593836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Differentially Private Reinforcement Fine-Tuning (DP-RFT), which uses LLMs to generate synthetic data using online reinforcement learning. DP-RFT uses nearest-neighbor votes from a private corpus as a reward signal for RL. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is well-motivated and important for the community.\n- The paper is well-written and is easy to follow."}, "weaknesses": {"value": "- The most important way to evaluate the performance of the proposed method is to show that it provides a better \"performance\" compared to (1) existing eyes-off methods and (2) methods that require direct access to raw private data during training. However, in line 244 it is mentioned that the performance evaluation is done for fine-tuning BERT_small, while the synthetic data is generated with Qwen with the help of GPT-4o (line 256)? If so, I'm very surprised of this evaluation and don't see the justification. If we have access to Qwen and GPT for data generation, I'd expect that the performance evaluation is also done by fine-tuning a much stronger model. Even if this model is used in some prior work, I don't think performance of BERT_small is a good indicative of the quality of the synthetic data for any modern application.\n\n- The similarity of the synthetic data to private data is not as important if we don't have a good performance from fine-tuning the model on synthetic data. While this is good to show, it's not enough and shouldn't be used as the primary evaluation metric. For LLMs, one can generate synthetic data close to real data with very poor performance.\n\n- The main novelty of the paper is using similarity scores between synthetic data (generated using existing methods) to steer a model with RL. Other than this, I didn't find major novelty in the paper and to my understanding, the effectiveness of the proposed approach is not really verified (as I explained above)."}, "questions": {"value": "- Can you evaluate the performance of the proposed method when fine-tuning modern LLMs, such as Qwen, Llama, etc?\n\nps: I'm happy to revise my score if I misunderstood anything."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h8kU5NbFIQ", "forum": "pfP2LIiH0A", "replyto": "pfP2LIiH0A", "signatures": ["ICLR.cc/2026/Conference/Submission21188/Reviewer_3Y1p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21188/Reviewer_3Y1p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044130933, "cdate": 1762044130933, "tmdate": 1762941593066, "mdate": 1762941593066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DP-RFT to train an LLM using reinforcement learning (RL) guided by a differentially private reward signal. This reward signal is derived from \"nearest-neighbor votes\" from a private corpus, but crucially, the LLM itself never directly accesses or \"ingests\" the raw private data during training (an \"eyes-off\" approach). This addresses a critical trade-off: traditional private fine-tuning (like DP-SGD) requires direct access to private data, while existing \"eyes-off\" methods often produce synthetic text lacking domain fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. DP-RFT integrates differentially private reinforcement fine-tuning (RFT) to train large language models (LLMs) for synthetic data generation. I like its \"eyes-off\" approach, where the LLM does not directly ingest private examples during training, addressing a significant practical challenge in privacy-sensitive applications.\n\nS2. DP-RFT attempts to bridge the gap between methods offering formal privacy guarantees (like DP-SGD, which requires direct data access) and methods avoiding direct data exposure but lacking domain fidelity (like Aug-PE). This is a well-identified and important problem in the field.\n\nS3. The evaluation covers four diverse datasets (PubMed, BBC News, Wildchat, QMSum) and assesses both intrinsic quality (similarity with real data) and extrinsic utility (downstream performance). Promising results vs DP-SGD under tight privacy budgets are interesting to see. I appreciate the qualitative analysis too!"}, "weaknesses": {"value": "W1. While DP-RFT generally shows better mean/max embedding similarity, its performance on FID is comparable to or worse than Aug-PE, especially for PubMed and Wildchat. I am concerned about the overall fidelity of the synthetic data. For instance, for Wildchat (ε=∞), DP-RFT has a much higher FID (0.74) compared to Aug-PE (0.39) and even the private data (0.07). \n\nW2. The impact of R_prompt on preventing reward hacking is not empirically demonstrated or thoroughly analyzed in the main results. The ablation study on reward function states that \"the choice of reward leads to small yet not significant changes,\" which contradicts the stated purpose of mitigating reward hacking and raises questions about the necessity and effectiveness of this component. If the impact is small, is it truly mitigating a significant issue, or is it adding unnecessary complexity?\n\nW3. While the ablation study shows that more powerful embedding models and language models generally lead to better DP-RFT performance, the improvement for DP-RFT is not always consistent across all privacy budgets (e.g., for gte-7B, ε=∞ is 18.51 vs ε=4 is 18.24). A deeper analysis into why Aug-PE's performance degrades with stronger models or under different privacy budgets would strengthen the discussion. \n\nW4. While the paper emphasizes the \"eyes-off\" aspect, the privacy analysis relies on adding Gaussian noise to R_sim derived from nearest-neighbor votes from the private corpus. How does this process ensure that the raw private data is never \"exposed\" or \"ingested\" by the LLM during training, even implicitly through the reward signal, and how this differs fundamentally from DP-SGD's direct access. While the \"Depiction Protocol\" states that the image model can see the entire conversation, a visual diagram could still help to illustrate this key distinction.\n\nW4. The paper would greatly benefit from additional figures. For instance, a diagram illustrating the flow of information for R_prompt (LLM-as-a-judge and rule-based checks) and how it integrates with R_sim to prevent reward hacking would significantly enhance understanding. Similarly, a more detailed diagram explaining the \"eyes-off\" mechanism and how privacy is maintained at each step of DP-RFT, especially contrasting it with DP-SGD's data flow, would be highly beneficial. The explanation of the iterative process and DP-protected nearest-neighbor votes is a bit abstract and could be clarified with a visual example."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SUQ77D5AKV", "forum": "pfP2LIiH0A", "replyto": "pfP2LIiH0A", "signatures": ["ICLR.cc/2026/Conference/Submission21188/Reviewer_6sPe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21188/Reviewer_6sPe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144432151, "cdate": 1762144432151, "tmdate": 1762941592358, "mdate": 1762941592358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to use the privatized histogram signal employed in private evolution to directly RL tune the model to generate similar text to the private data."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Experimental setups are solid, and results are convincing. Using the same generation model, embedder, and initial prompt for DP-RFT and AUG-PE, DP-RFT outperforms AUG-PE and also the baseline of just using the initial prompt with the baseline model.\n\n- The method has numerous advantages. In addition to the fact that private data is not ever directly ingested by the model (which mitigates privacy risks from possible DP implementation gaps); the method is very simple; furthermore it can be implemented with off the shelf RL tools and hence does not require the heavy engineering investment and compute required for a DP-SGD implementation\n\n- Ablations are good"}, "weaknesses": {"value": "- In Table 1, the private finetuning baseline has a mismatched number of samples. Although the number of samples (2k) is the same for training, AUG-PE and DP-RFT use the full dataset and associated N for noise calculation for generating the 2k, while private finetuning only gets 2k for such.\n\n- There does seem to be a certain amount of prompt engineering and reward function crafting required to get things to work, but it is the same case in private evolution.\n\n- No results for privately finetuning Qwen 2.5-3B to generate synthetic data in the similarity and downstream evaluation, which a very important baseline to understand the strength of the proposed method."}, "questions": {"value": "- [naming] “Differentially private reinforcement finetuning” sounds to me like standard RL + DP-SGD. To my knowledge, no one has staked a claim on this moniker. But i believe there will be some confusion in calling the objective perturbation-based approach described in the present paper DP-RFT.\n- [typo] Row 1 for Table 1 looks a bit strange. Is there a typo here?\n- [typo] Indices of the summation in Line 174 look incorrect.\n- Is it correct that the QWEN baseline in Table 1 should be considered epsilon=0 and should therefore be comparable to the other methods at epsilon=1? If so, this should be more accurately reflected in the table.\n- Interestingly, FID seems poorer than AUG-PE despite the data capable of training better downstream next token predictors. Any possible explanations here?\n- In Figure 2 and A.4, the private histogram’s sensitivity bound comes from a per-entry bound implying a norm bound. This feels loose. Why not just l2 clip the similarity vector between a private example and all synthetic example? I think this would have better noise to signal.\n- How does the rollout batch size affect the results? How about private data batch size (e.g. employing privacy amplification by subsampling)?\n\nOverall, i am very excited about this paper!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wZkF6PvHZg", "forum": "pfP2LIiH0A", "replyto": "pfP2LIiH0A", "signatures": ["ICLR.cc/2026/Conference/Submission21188/Reviewer_cPfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21188/Reviewer_cPfG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21188/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763151784760, "cdate": 1763151784760, "tmdate": 1763151901685, "mdate": 1763151901685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}