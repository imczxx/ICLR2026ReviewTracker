{"id": "ahpO7S1Ppi", "number": 15948, "cdate": 1758257473974, "mdate": 1759897271492, "content": {"title": "Pctx: Tokenizing Personalized Context for Generative Recommendation", "abstract": "Generative recommendation (GR) models tokenize each action into a few discrete tokens (called semantic IDs) and autoregressively generate the next tokens as predictions, showing advantages such as memory efficiency, scalability, and the potential to unify retrieval and ranking.\nDespite these benefits, existing tokenization methods are static and non-personalized. They typically derive semantic IDs solely from item features, assuming a universal item similarity that overlooks user-specific perspectives. However, under the autoregressive paradigm, semantic IDs with the same prefixes always receive similar probabilities, so a single fixed mapping implicitly enforces a universal item similarity standard across all users. In practice, the same item may be interpreted differently depending on user intentions and preferences. To address this issue, we propose a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. This design allows the same item to be tokenized into different semantic IDs under different user contexts, enabling GR models to capture multiple interpretive standards and produce more personalized predictions. Experiments on three public datasets demonstrate up to 8.9% improvement in NDCG@10 over non-personalized action tokenization baselines. Our code is available at https://anonymous.4open.science/r/Pctx-code-4246.", "tldr": "", "keywords": ["Generative Recommendation", "Personalization", "Tokenization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6751ef6ee3aeb4226ab21d1161d205d43c660f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Pctx, a personalized context-aware tokenizer for generative recommendation (GR) systems. It addresses the limitation of static, non-personalized tokenization in existing GR models, where items are mapped to fixed semantic IDs regardless of user context, enforcing a universal similarity standard. Pctx incorporates a user’s historical interactions to generate adaptive semantic IDs, enabling personalized predictions. The method tackles challenges of context-aware tokenization and balancing generalizability with personalization using adaptive clustering, merging infrequent IDs, and data augmentation. Experiments on three public datasets show up to 8.9% improvement in NDCG@10 over non-personalized baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Introduces a personalized tokenization framework, adapting semantic IDs to user context.\n- **Quality**: Demonstrates measurable performance gains (8.9% NDCG@10) on public datasets.\n- **Clarity**: Provides a clear motivation, though technical details are sparse.\n- **Significance**: Targets personalization, a key challenge in GR, with potential for practical adoption."}, "weaknesses": {"value": "- **Methodological Flaws**: The neural module’s context compression lacks a defined architecture or loss function, and adaptive clustering’s cluster number selection is arbitrary. Merging infrequent IDs risks losing nuanced personalization.\n- **Experimental Gaps**: No analysis of history length impact on tokenization quality or NDCG. Hyperparameter tuning (e.g., cluster count, augmentation rate) is not reported, affecting reproducibility.\n- **Oversight**: Ignores potential overfitting to historical data and bias from unbalanced user interactions. Scalability to large datasets is untested.\n- **Validation**: The 8.9% gain is based on limited datasets without cross-validation or online testing."}, "questions": {"value": "1. Can the authors provide statistical tests (e.g., t-tests) to confirm the 8.9% NDCG@10 improvement’s significance?\n2. How does the neural module’s architecture and loss function ensure effective context compression, and what history length was used?\n3. What criteria determined the adaptive clustering’s variable cluster numbers, and how was merging infrequent IDs optimized?\n4. Can the authors assess Pctx’s performance with varying history lengths (e.g., 5 vs. 50 interactions) to validate context adaptability?\n5. Why was no online A/B testing conducted to complement offline results, and how might unbalanced user data affect outcomes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YQ6oDs64y4", "forum": "ahpO7S1Ppi", "replyto": "ahpO7S1Ppi", "signatures": ["ICLR.cc/2026/Conference/Submission15948/Reviewer_vmqK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15948/Reviewer_vmqK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761372726059, "cdate": 1761372726059, "tmdate": 1762926160255, "mdate": 1762926160255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel item tokenization method Pctx for generative recommender systems, which takes user interaction history as context for personalized tokenization. Specifically, Pctx feeds the item feature along with the interacted item sequence into an encoder for contextual embedding. For the anchor item $v_i$, the step creates a set of contextual embeddings and Pctx then adopts K-means++ to cluster them into several representative embeddings. Afterwards, each representative contextual embedding will concatenate with the item embedding and then be used to generate the item semantic index. Experimental results validate the effectiveness of Pctx, compared with traditional recommender systems and currently leading generative recommenders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is well-organized and the proposed method is clear to understand.\n2. The proposed modules, including contextual embedding, semantic ID merging, and data augmentation are significantly effective.\n3. The experiments are sufficient along with detailed analysis and discussion."}, "weaknesses": {"value": "1. My main concern lies in the motivation of Pctx to incorporate user history into semantic index generation. Since the recommender takes the user history as input, the hidden state ahead the logits head can capture personalized user preference. Moreover, for an anchor item $v_i$, the hidden states corresponding to two different users $u_m$ and $u_n$ won't be the same, and the proposition that *when generating the next semantic IDs, those with the same prefixes inevitably receive similar probabilities* may be improper.\n2. Another issue is that the variant **w/o Redundant SID Merging** degrades the performance most, even worse than baselines. Moreover, the variants **w/o Data Augmentation** and **w/o Multi-Facet Generation** are comparable to baseline. These ablation studies make the effectiveness of the contextual semantic index unconvincing.\n3. I wonder if the generative recommender baselines are evaluated under the same or similar model scales."}, "questions": {"value": "1. If we replace the native semantic IDs in TIGER with the Pctx IDs, how does the recommender performance change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SGczlEom6C", "forum": "ahpO7S1Ppi", "replyto": "ahpO7S1Ppi", "signatures": ["ICLR.cc/2026/Conference/Submission15948/Reviewer_Yz3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15948/Reviewer_Yz3F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535714622, "cdate": 1761535714622, "tmdate": 1762926159376, "mdate": 1762926159376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a personalized context-aware tokenizer for generative recommendation. Instead of using a static tokenizer that maps each item to fixed semantic IDs, they  condition tokenization on user interaction history, allowing the same item to have different semantic IDs for different users. The method encodes user contexts via DuoRec, clusters them into representative centroids, fuses these with item textual features , and quantizes the fused embeddings using RQ-VAE to form personalized tokens. Experiments on three Amazon Review datasets show consistent improvements"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles a less explored direction, personalizing the tokenization stage of generative recommendation instead of static tokenizers\n* Pctx consistently improves NDCG@10 across three datasets, demonstrating effectiveness\n* The paper ablates key design components (context clustering, data augmentation, redundancy merging), confirming that each contributes to performance."}, "weaknesses": {"value": "* : All experiments use only a single backbone (Sentence-T5-base) for GR modeling, which weakens claims of generality. It is unclear whether the method’s gains hold for other architectures.\n* In GR, the autoregressive model already conditions on user history, so personalization is inherently modeled at inference. It is unclear why injecting user context into item tokenization provides additional benefit. The reported improvements might arise from increased token diversity or implicit data augmentation rather than genuine personalization. More theoretical or analytical justification is needed.\n* If my understanding  is correct, during inference the model aggregates probabilities over all cluster-based IDs for each item rather than selecting the one consistent with the current user context. Thus, user context is not actually used at inference, raising doubts about its necessity. The performance gains may simply come from having multiple IDs per item rather than true context-conditioned modeling."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C6c59WpiDF", "forum": "ahpO7S1Ppi", "replyto": "ahpO7S1Ppi", "signatures": ["ICLR.cc/2026/Conference/Submission15948/Reviewer_LC91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15948/Reviewer_LC91"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623956447, "cdate": 1761623956447, "tmdate": 1762926158603, "mdate": 1762926158603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. The same item may be tokenized into different semantic IDs under different user tokens. Experiments on public datasets demonstrate the performance of the proposed Pctx."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors rationally modeled the phenomenon that different users respond differently to the same item, and obtained a more adaptive and personalized tokenizer.\n\n2. The authors designed a good strategy to merge redundant semantic IDs.\n\n3. The authors tested Pctx's performance and compared it with previous methods."}, "weaknesses": {"value": "1. The Redundant Semantic ID Merging section lacks good mathematical formulas, which hinders understanding.\n\n2. The experiment was only tested on two benchmarks. As far as I know, at least TIGER also provides three public benchmarks: beauty, toys, and sports."}, "questions": {"value": "1. Some recent semantic ID-based GR models seem to be worth discussing, such as GRID [1].\n\n2. If each item viewed by a user is given a different semantic ID, what level will the total semantic ID reach and how does it compare with the item ID? For GR, does a large vocabulary size mean that a larger model is required for training?\n\n3. Is embedding quantization used in combination with k-means++ and VQVAE? Some current explorations have shown that if k-means++ is used, VQ seems to be unnecessary.\n\n[1] Ju, C. M., Collins, L., Neves, L., Kumar, B., Wang, L. Y., Zhao, T., & Shah, N. (2025). Generative Recommendation with Semantic IDs: A Practitioner's Handbook. arXiv preprint arXiv:2507.22224."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5MUA6iIhv", "forum": "ahpO7S1Ppi", "replyto": "ahpO7S1Ppi", "signatures": ["ICLR.cc/2026/Conference/Submission15948/Reviewer_bB73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15948/Reviewer_bB73"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661933624, "cdate": 1761661933624, "tmdate": 1762926158125, "mdate": 1762926158125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}