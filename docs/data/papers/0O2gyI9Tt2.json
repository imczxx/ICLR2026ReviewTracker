{"id": "0O2gyI9Tt2", "number": 17672, "cdate": 1758279013357, "mdate": 1759897161499, "content": {"title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction", "abstract": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to human-crafted features. Moreover, we show the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets.", "tldr": "We introduce an unsupervised method leveraging large language models to extract compact, interpretable features by optimizing their reconstruction of the original data.", "keywords": ["featurization", "interpretibility", "large language models", "feature extraction", "text modeling"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1aa49a98e86a055409ef0a7152e723f7d2035023.pdf", "supplementary_material": "/attachment/a8939af4919443dca5f4c65f55c11d2ad6c7c0d7.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a domain-agnostic method to featurize text datasets with natural-language features. By leveraging a language model to produce possible features, and perplexity minimization during original text reconstruction to select them, the authors are able to extract interpretable features without supervision. The authors validate the proposed method on several dataset modeling tasks and case studies, demonstrating higher effectiveness in feature extraction compared to previous approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors test the method on several dataset-modeling tasks and case studies, validating it from multiple perspectives.\n- The method is practical and valuable, yielding better performance than other baselines."}, "weaknesses": {"value": "- It is unclear which failure modes arise in the generation stage and how they could be mitigated.\n- Some claims and methodological choices are not clearly explained or insufficiently supported; I outline specific concerns in the questions section."}, "questions": {"value": "- One concern about the method is that the generation stage may produce non-discriminative or overly generic features. Although the method is designed to discard irrelevant features because they would lead to high perplexity during text reconstruction, there can be features that actually maintain low perplexity across multiple samples while providing minimal information about the dataset's structure. In Section 6.2, the authors mention filtering low-variance features, suggesting this problem was encountered in practice, though this appears as a post-hoc fix rather than a solution. I would like to ask the authors whether they have encountered this type of issue systematically and whether it is possible to detect or prevent such features during the generation stage.\n- In Section 6.2, the authors report that on HH-RLHF the method achieves 68.1% accuracy while the baseline reaches 68.9% (Table 2). In Appendix F.2, the authors attribute this to an accuracy ceiling due to dataset noise (potentially incorrect annotations). However, if the dataset noise is the limiting factor, why does the baseline with hand-crafted features perform slightly better? Does this suggest that the unsupervised features are more susceptible to noise?\n- In Section 4, the authors mention setting C=5 and K=5 for all experiments and observing “additional performance gains by increasing the number of proposed features,” and refer to Appendix D.1 for additional details.  I couldn't find an ablation study systematically varying C and K. I would like to ask the authors whether they explored how performance scales with these hyperparameters?\n- Considering the greedy sequential selection in the featurization stage: since the choice of earlier features affects which subsequent features provide marginal information gain, does the order of evaluation significantly impact which features are selected? Does running the pipeline multiple times produce consistent feature sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "An23E80Wog", "forum": "0O2gyI9Tt2", "replyto": "0O2gyI9Tt2", "signatures": ["ICLR.cc/2026/Conference/Submission17672/Reviewer_eKkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17672/Reviewer_eKkX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408861318, "cdate": 1761408861318, "tmdate": 1762927522404, "mdate": 1762927522404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dataset Featurization: a three-stage pipeline that (1) elicits many human-readable binary features via an LLM, (2) de-duplicates them with embedding-based clustering and obtains per-example truth values, and (3) greedily builds a global feature set that minimizes the mean per-text perplexity of a scoring LM when conditioned on the features that are true for each example. The resulting small, interpretable “feature basis” is shown useful in two applications: (i) jailbreak tactic compression (achieving comparable effectiveness to a 500-cluster baseline with ~20 features across multiple target models), and (ii) composable preference modeling (CPM) on SHP/HH-RLHF, where automatically discovered features rival or surpass expert-crafted ones at similar feature counts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear, modular pipeline with a principled dataset-level objective and an interpretable feature representation.\n\n* Demonstrates strong compression of jailbreak strategies while maintaining attack efficacy across multiple target models.\n\n* Shows that automatically discovered features can match/beat expert-designed features in CPM, with reasonable robustness analyses.\n\n* Practical engineering details."}, "weaknesses": {"value": "* Binary-only features and reliance on positive instances during optimization limit expressivity.\n\n* The optimization is global, so it does not guarantee per-instance optimality; some examples may not benefit from the selected global basis.\n\n* No dedicated length-sensitivity/long-context study (data filtered to a range; but scalability to very long inputs remains unclear)."}, "questions": {"value": "* What is the length sensitivity of the scoring objective e.g. does performance degrade near the upper character limit or with longer contexts?\n\n* Will you release prompts, code, and feature lists to facilitate replication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aYNJgtewpy", "forum": "0O2gyI9Tt2", "replyto": "0O2gyI9Tt2", "signatures": ["ICLR.cc/2026/Conference/Submission17672/Reviewer_MrYc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17672/Reviewer_MrYc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908947701, "cdate": 1761908947701, "tmdate": 1762927522065, "mdate": 1762927522065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a method for breaking datasets down into a granularly controllable number of features. The method works by first having an LLM propose features for each sample in an unlabelled dataset, then they deduplicate the set of features by clustering similar features together. They then evaluate the accuracy of each features by whether the features are useful to an LLM in reconstructing the original data samples labelled by that feature (measured by minimising perplexity)\n\nThe authors evaluate on three synthetic datasets (DBPedia, NYT, Amazon \nReviews) with known ground-truth labels, demonstrating superior performance over LLM prompting \nbaselines and Zhong et al. (2024). They also show the effectiveness of their method with two case studies. For the first, they compress 500 jailbreak attack tactics into 20 features while maintaining attack \neffectiveness and diversity. For the second case study, they automate preference model feature discovery, matching human-crafted features from Go et al. (2024) in accuracy and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors' perplexity / reconstruction-based optimisation framing seems novel and their results suggest that it's effective. Their case studies show real applications that are highly relevant to LLM safety and alignment. Extracting compact representations of jailbreaks from a large unlabelled set of attacks without supervision seems highly useful for understanding new attack strategies at scale. The case study on preference modeling also emphasises that the lack of supervision needed for this method might also mitigate bias arising from human labelling, but I'm less sold that the bias reduction point is the most groundbreaking feature of the method (not that the authors imply this). I'm not totally sure how to estimate the impact of this method - it genuinely seems useful across a wide set of tasks and domains, and the controllability and unsupervised nature of the method make it preferable over other methods. The case studies are helpful for understanding the sorts of tasks on which the authors expect the method to be impactful.\n\nThe experiments are well substantiated. The synthetic datasets they use are diverse, and the two case studies further illustrate how widely-applicable their method is. They compare to reasonable baselines in prompting and Zhong et al 2024. It's a little hard to know how much further you could push the prompt engineering, but it intuitively seems like prompting would still be overall worse than a pipeline like Zhong et al or from this paper. They also convincingly demonstrate that their method improves over human-written features, which seems like another important axis of comparison for their method. The paper's extensive appendices leave very little unaddressed. I particularly appreciated the cost analysis in appendix C, which is a practical and thoughtful inclusion - though I would have appreciated just a little more depth in the cost comparison with Zhong et al, rather than just the (probably approximately true) claim that the grid search would increase the total cost and runtime of the competitor method tenfold.\n\nOverall, the writing and presentation of the paper is clear, well motivated, and well organised."}, "weaknesses": {"value": "Overall I think the paper is quite strong, so struggling to find substantive weaknesses.\n\nIt's worth calling out that the paper overall provides a useful advancement with this reconstruction focus on unsupervised dataset feature labelling, but as the authors themselves acknowledge, the paper is in large part building on other recent work (Zhong et al. 2024, Findeis et al. 2024, Go et al. 2024). In their experiments comparing their method to Zhong et al, the authors convinced me that their method is stronger and likely more efficient (in dollars and time), but the overall place it seems like this paper sits in the literature is as a noticeable but not paradigm-shifting improvement on a reasonably important and wide-ranging problem. So, while the paper is still original and significant, it's not superlative on either axis.\n\nAs the authors themselves mention in section 7, this pipeline is restricted to binary features, and it's probably true that a method that could extract richer feature representations while holding all else equal would be useful across a wider range of domains & tasks.\n\nThe authors also note in section 8 that their greedy method introduces potential convergence to local optima, so more investigation into alternative selection strategies could plausibly improve on this method? Would be interested in the authors' takes."}, "questions": {"value": "1.) I'm pretty confused about how to think about the novelty/originality of the paper - could the authors make a.) the concise steelman for why this is work is highly original, or just b.) what their all-things-considered view is, here.\n2.) How important is the limitation of this method only working for binary features? Just how many applications would open up?\n3.) What did you do that convinced you that the greedy optimisation target was ~fine, and unlikely to get stuck in local optima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jG7PXivw2x", "forum": "0O2gyI9Tt2", "replyto": "0O2gyI9Tt2", "signatures": ["ICLR.cc/2026/Conference/Submission17672/Reviewer_a49K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17672/Reviewer_a49K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928230457, "cdate": 1761928230457, "tmdate": 1762927521305, "mdate": 1762927521305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research tries to construct interpretable features for a textual corpus in an unsupervised manner. To do so, the researchers propose to first prompt LLMs to generate the features from the samples, and then confirm the effectiveness of the features by minimizing the perplexity of the dataset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The high-level idea of using extracted features to reconstruct the dataset for selecting features is interesting."}, "weaknesses": {"value": "1. The writing of this paper is terrible. It is pretty hard for me to get an understanding of how it works and why it works. For example, I have no idea how you compute the perplexity exactly from lines 170-178.  Are you following the contents in lines 140-145? If yes, how are these conditional PPLs measured concretely? Similarly, in Section 5, I don't know why we need to consider the class labels provided by the datasets, and how you evaluate your results. More importantly, why is this experiment result correct?\n\n2. Experiments miss many simple but effective strategies to mine superficial features from the dataset unsupervisely. For example, you can compute the TF-IDF scores for different N-grams."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BWyV8d0HuU", "forum": "0O2gyI9Tt2", "replyto": "0O2gyI9Tt2", "signatures": ["ICLR.cc/2026/Conference/Submission17672/Reviewer_efCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17672/Reviewer_efCi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17672/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936931299, "cdate": 1761936931299, "tmdate": 1762927520905, "mdate": 1762927520905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}