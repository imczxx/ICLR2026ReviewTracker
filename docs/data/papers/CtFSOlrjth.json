{"id": "CtFSOlrjth", "number": 17689, "cdate": 1758279321992, "mdate": 1763733079434, "content": {"title": "Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models", "abstract": "Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce the memory overhead of diffusion models.  Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. \n    Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model’s gradients across timesteps, facilitating the quantization process.  Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet datasets demonstrate the superiority of our method compared to other PTQ methods for diffusion models.", "tldr": "A paper that propose adaptive sample weight to address gradient conflict problem of diffusion quantization", "keywords": ["Quantization", "Diffusion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5ceb2e63d5112c4d42995602ef91c3a744117db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents an interesting phenomenon that the quantization of diffusion models would raise grad conflict when training with different samples. With this in mind, a train-able weight is added to each sample for quantization training, to try to align the direction of grads when training quantization for different de-noising timesteps across different samples. Significant performance boosts are observed and detailed theoretical proof are offered to support the proposed motivation, as well as the methods to avoid such dis-alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed motivation is very interesting and important. It starts from the numerical angle, looking into the grad dis-alignment when training quantization for different de-noising steps, which I believe opens up an important area to explore.\n2. The proposed method is well-designed. Both intuitions and theoretical proofs are provided, making it very clear to me.\n3. The proposed method achieves significant performance boost on the quantization task, and theoretical analysis proves that such improvements come from solving the proposed grad misalignment problem."}, "weaknesses": {"value": "1. Minor issue: All citations are not in the correct format, which makes reading sometimes hard. My recommendation is: the authors should check them in the next version.\n2. The visualizations could be further refined to make it more impressive: While Fig. 1(a) presents the interesting grad conflict phenomenon very clearly, Fig. 2 looks not as straight-forward as Fig. 1. I recommend the authors to re-make Fig. 2 in the form of Fig. 1, to make it more impressive and more comparable."}, "questions": {"value": "1. Can this method also be extended to diffusion model's training? Have the authors tested whether diffusion's training from scratch / fine-tuning would encounter similar phenomenon? If so, I think this can be a great extension to the manuscript. (but I don't expect the authors to add this during the short rebuttal period, and this does not negatively influence on my evaluations.)\n2. While the authors ablate the results on the validation set size, I hope the authors to give some insights on the whole quantization dataset's size: when the whole dataset's size goes up, there might be more weights to learn, which might be a burden. I hope the authors can clarify on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I don't notice any ethics issue in this manuscript."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SfJFzoLs4y", "forum": "CtFSOlrjth", "replyto": "CtFSOlrjth", "signatures": ["ICLR.cc/2026/Conference/Submission17689/Reviewer_DoWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17689/Reviewer_DoWQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529029115, "cdate": 1761529029115, "tmdate": 1762927535173, "mdate": 1762927535173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Many existing post-training quantization (PTQ) methods for diffusion models assume calibration data should be treated uniformly across timesteps. This paper shows that, during PTQ, the quantization-loss gradients do not align across timesteps; treating timesteps equally can therefore cause degrade performance. To address this, the authors assign a learnable weight to each calibration sample to reflect its contribution to the gradient update, and they optimize these weights as a proxy objective to align gradient directions across timesteps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Improvement of FID and sFID is verified by experiment. \n- Provides a theoretical justification for the proxy objective with approximation, and reports optimization trends consistent with the theory."}, "weaknesses": {"value": "- Limited preliminaries on the specific techniques used (e.g., AdaRound) and related design choices.\n- Evaluation metrics lean heavily on FID, leaving diversity aspects less explored in the main tables."}, "questions": {"value": "- In Figure 2, is the x-axis ordered by sample timesteps? If so, it’s hard to strictly verify the claim that samples with stronger gradient alignment receive higher emphasis; a different visualization might make this clearer.\n- FID and sFID indicate improved fidelity. However, because the method reweights contributions per calibration sample, I’m concerned about possible effects on sample diversity. Table 6 reports Precision; could you also report Recall?\n- Similarly, for the main results, consider adding diversity-aware metrics (e.g., Precision/Recall curves) alongside FID/sFID."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ALSVnCKHsv", "forum": "CtFSOlrjth", "replyto": "CtFSOlrjth", "signatures": ["ICLR.cc/2026/Conference/Submission17689/Reviewer_bPXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17689/Reviewer_bPXm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988427000, "cdate": 1761988427000, "tmdate": 1762927534735, "mdate": 1762927534735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel post-training quantization (PTQ) method for diffusion models to improve inference speed and reduce memory consumption without retraining. Existing PTQ approaches treat all timesteps and calibration samples equally, which leads to suboptimal results because different timesteps contribute unequally and require distinct gradient directions during quantization. To address this, the authors introduce a timestep-aware weighting strategy that learns to assign optimal weights to calibration samples, aligning gradients across timesteps for better quantization. Experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate that this approach outperforms previous PTQ methods for diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a key limitation in existing PTQ methods: uniform treatment of all timesteps. It then proposes a principled weighting mechanism that learns optimal calibration weights, effectively aligning gradients across timesteps.\n- The proposed method is evaluated on multiple benchmark datasets (CIFAR-10, LSUN-Bedrooms, and ImageNet), consistently outperforming prior PTQ approaches."}, "weaknesses": {"value": "- As of 2025, most state-of-the-art diffusion models are built upon the DiT architecture. However, this submission does not include experiments on such models, which limits the generalizability and relevance of the findings to current diffusion frameworks.\n\n- The experimental evaluation is primarily conducted on small-scale datasets (e.g., CIFAR) with low-resolution images (e.g., 32×32). While these settings are useful for preliminary validation, they do not sufficiently demonstrate the scalability or robustness of the proposed method on more challenging benchmarks.\n\n- The study focuses mainly on bit-width configurations (e.g., W4A8, W4A32), but the results do not show clear improvements over existing baselines such as TFMQ-DM (2024). A more comprehensive comparison and discussion of potential advantages (e.g., efficiency, training stability, lower bit-width, or qualitative sample quality) would strengthen the contribution."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mjiZYlyOHn", "forum": "CtFSOlrjth", "replyto": "CtFSOlrjth", "signatures": ["ICLR.cc/2026/Conference/Submission17689/Reviewer_Qghd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17689/Reviewer_Qghd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106351397, "cdate": 1762106351397, "tmdate": 1762927534294, "mdate": 1762927534294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}