{"id": "QpKXNYtF3x", "number": 3176, "cdate": 1757350402205, "mdate": 1759898103864, "content": {"title": "OS-Catalyst: Advancing Computer-Using Agents Efficiency through Adaptive Action Compression", "abstract": "Driven by advances in Vision-Language Models (VLMs), computer-using agents have recently demonstrated remarkable capabilities in complex reasoning, software control, and the automation of digital workflows.\nHowever, the existing step-by-step paradigm requires extensive interaction with the model, and the resulting query latency emerges as a key bottleneck for real-world adoption. \nTo address this limitation, we propose that agents should be able to output a sequence of actions after each observation, enabling efficient execution without constant model queries.\nIn this work, we introduce \\ours, a method that transforms standard computer-using models into agents with the capability of action sequence prediction. \nTo enable this, we design a data collection pipeline tailored for compressed action trajectories in computer-using environments. \nBuilding on this pipeline, we construct a large-scale dataset within the WorkArena benchmark and train computer-using agents for action sequence prediction.\nThrough extensive experiments, we show that \\ours enables up to 50\\% faster task completion on office-related benchmarks without sacrificing success rate.", "tldr": "", "keywords": ["computer use", "GUI agent", "task efficiency"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b957295bbfd0dc89e3abaa4567687e2855518ee.pdf", "supplementary_material": "/attachment/ca60f81f2334a969740eab8119ee70fe23e61fa0.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose a data synthesis method to train GUI agents (UI-Tars) to be able to generate more than one action per step to improve efficiency. Specifically, the proposed method recursively merges two consecutive actions when their corresponding screenshots are highly similar and the interacted elements or buttons are independent. This process is achieved using tools such as SSIM, and prompting VLMs such as GPT-4o to check whether the actions can be merged. The authors then constructed such training data on WorkArena, and performed SFT training on UI-Tars on such data. Experiment results on WorkArena shows that this method improves efficiency but yields slightly inferior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method to construct the \"compressed trajectory\" is sound. I believe using tools such as SSIM and prompting GPT-4o, actions that are \"mergable\" can be correctly identified/included in the training data to test the authors hypothesis\n\n- Experimental results on WorkArena shows improved efficiency of the trained models on both seen and unseen subset of the benchmark."}, "weaknesses": {"value": "1. I believe many claims/motivations of this paper is highly specific to the WorkArena benchmark and the UI-Tars models, and may not hold for other models/benchmarks. As an example, L192-193 and L164-177 claims that \"models had no awareness of producing action sequences and would not attempt multi-step actions...\". This is not true, because models such as DeepSeek-R1/GPT-4o/etc under the default prompt template for OSWorld [1] already produces more than one action per step (see Figure 1 of [2] as an example). Although it is true that many recent computer-use fine-tuned models (e.g., UI-Tars) are trained to output one action per step, it is not generally true for all LLMs/VLMs.\n\n2. The authors constructed by themselves an in-distribution test and out-of-distribution test set with only 84 and 16 tasks respectively. In Table 3, the performance gap between runs are small, yet no standard deviation/statistical significance is provided. In Table 4, all results are only evaluated on 16 tasks (from only 4 task types), which I believe is too small of a test pool to yield any conclusion other than training/testing noise. Overall, I believe the experimental setup for Tables 3 and 4 is poor, making it difficult to draw meaningful insights.\n\n3. Overall, the entire experiments section only uses on benchmark (WorkArena) with testing/training one type of model (UI-Tars). This severely limits the generalizability of the findings: for example, it is unclear whether the conclusions are specific to this model/benchmark setup or would hold for other computer-use models and benchmarks.\n\n---\n\nReferences:\n\n[1] Xie, Tianbao, et al. \"Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments.\" Advances in Neural Information Processing Systems 37 (2024): 52040-52094.\n\n[2] Yu, Xiao, et al. \"Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents.\" arXiv preprint arXiv:2506.00320 (2025)."}, "questions": {"value": "- Why is the \"step time\" higher for \"Work-step\" models compared to \"Work-seq\" model? Intuitively, since \"Work-step\" only needs to generate one action per step, the generated sequence length of this model should be shorter than that of the \"Work-seq\" model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJsJSN5lNe", "forum": "QpKXNYtF3x", "replyto": "QpKXNYtF3x", "signatures": ["ICLR.cc/2026/Conference/Submission3176/Reviewer_WKML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3176/Reviewer_WKML"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708647748, "cdate": 1761708647748, "tmdate": 1762916584468, "mdate": 1762916584468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors focus on the problem of developing effective agents that can navigate computer user interfaces. However, one issue with this domain is the action space is usually very large and complex. To address this problem, the authors propose OS-Catalyst, an approach with compresses sequences of actions via grouping them by similarity and region checks. They also construct a dataset for evaluation. They show promising results and notable agent speedup."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is simple and easy to understand.\n\n2. The paper is well-written and well presented. \n\n3. Quantitative performance is promising\n\n4. Evaluation setup is thorough and broad based."}, "weaknesses": {"value": "1. Technical novelty is incremental. This is largely about a simple approach to action compression within a pre existing framework.\n\n2. While performance is impressive, the speedup is somewhat modest (about 2x)."}, "questions": {"value": "1. Can you please elaborate on the technical novelty of the approach?\n\n2. Are there any tweaks that can be made to the approach for more significant speedups (> 5x) without dramatically sacrificing performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CIYnpErgA2", "forum": "QpKXNYtF3x", "replyto": "QpKXNYtF3x", "signatures": ["ICLR.cc/2026/Conference/Submission3176/Reviewer_q5vT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3176/Reviewer_q5vT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770565553, "cdate": 1761770565553, "tmdate": 1762916584278, "mdate": 1762916584278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes OS-Catalyst, a system designed for reducing the execution latency of computer-use agents. The observation is that multiple actions can often be conducted together (by merging them) based on a single observation. The authors then described their data construction using the WorkArena benchmark and how they use it to fine tune the UI-TARS model. By changing sequential actions (executed step by step) to a grouped action execution, OS-Catalyst achieves up to 50% reduction in total task execution time on the WOrkArena benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper performs a comprehensive methodology from constructing trajectory training dataset manually to fine tune the UI-TARS model to evaluate it. \n- The result is very encouraging, achieving up to 50% latency improvement.\n- Presentation is overall clear"}, "weaknesses": {"value": "- Only evaluated on WorkArena. It's unclear how well the proposed method would work on other types of CUA tasks like OS commands, CLIs, etc.\n- Action merging is based on heuristics and depends on thresholds, and there's no sensitivity test for these thresholds.\n- The paper talks about over-compression, under-compression, etc, but does not propose concrete solutions to address them\n- The proposal of grouping CUA actions and the construction of action-group trajectories are not new. OSWorld-Human (https://arxiv.org/abs/2506.16042) performs action grouping and adds manual trajectories to the OSWorld benchmark."}, "questions": {"value": "Can you explain how the thresholds are set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kfIEr2W9z5", "forum": "QpKXNYtF3x", "replyto": "QpKXNYtF3x", "signatures": ["ICLR.cc/2026/Conference/Submission3176/Reviewer_EPo9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3176/Reviewer_EPo9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893617906, "cdate": 1761893617906, "tmdate": 1762916584068, "mdate": 1762916584068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the latency problem in step-by-step computer-using agents. The authors propose OS-CATALYST, a framework based on adaptive action compression, which predicts and executes sequences of actions jointly rather than step-by-step. This approach aims to reduce the number of model queries while maintaining task success.\nThey also introduce a new dataset tailored for this task. Experimental results show that OS-CATALYST achieves approximately 50% faster execution with no loss in success rate compared to baseline agents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors are transparent about their use of LLMs, including a clear statement clarifying the extent of model usage.\n- They share a supplementary ZIP file containing partial dataset information, code fragments, and training scripts, which supports reproducibility to some degree."}, "weaknesses": {"value": "- Although supplementary materials are provided, there is no README or documentation to explain their structure or usage, making it difficult to reproduce or extend the experiments.\n- The dataset, which is presented as one of the main contributions, is not clearly or comprehensively described. It deserves a dedicated section detailing its composition, sources, scale, and intended use cases.\n- The main paper **exceeds the 9-page limit**, which could be grounds for a desk rejection under ICLR formatting rules."}, "questions": {"value": "- One of the key contributions is the dataset. Could the authors provide a clearer overview and discuss its size, structure, and licensing terms in more details? It may be helpful to dedicate a separate section for this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5qUCdBs2ch", "forum": "QpKXNYtF3x", "replyto": "QpKXNYtF3x", "signatures": ["ICLR.cc/2026/Conference/Submission3176/Reviewer_F7dh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3176/Reviewer_F7dh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930481048, "cdate": 1761930481048, "tmdate": 1762916583898, "mdate": 1762916583898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}