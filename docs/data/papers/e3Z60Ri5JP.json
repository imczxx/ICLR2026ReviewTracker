{"id": "e3Z60Ri5JP", "number": 22984, "cdate": 1758337811113, "mdate": 1759896837613, "content": {"title": "AttackSeqBench: Benchmarking Large Language Models in Analyzing Attack Sequences within Cyber Threat Intelligence", "abstract": "Cyber Threat Intelligence (CTI) reports document observations of cyber threats, synthesizing evidence about adversaries’ actions and intent into actionable knowledge that informs detection, response, and defense planning. However, the unstructured and verbose nature of CTI reports poses significant challenges for security practitioners to manually extract and analyze such sequences.  Although large language models (LLMs) exhibit promise in cybersecurity tasks such as entity extraction and knowledge graph construction, their understanding and reasoning capabilities towards behavioral sequences remains underexplored. To address this, we introduce AttackSeqBench, a benchmark designed to systematically evaluate LLMs' reasoning abilities across the tactical, technical, and procedural dimensions of adversarial behaviors, while satisfying Extensibility, Reasoning Scalability, and Domain-dpecific Epistemic Expandability. We further benchmark 7 LLMs, 5 LRMs and 4 post-training strategies across the proposed 3 benchmark settings and 3 benchmark tasks within our AttackSeqBench to identify their advantages and limitations in such specific domain. Our findings contribute to a deeper understanding of LLM-driven CTI report understanding and foster its application in cybersecurity operations. Our code and dataset are available at: https://anonymous.4open.science/r/AttackSeqBench.", "tldr": "", "keywords": ["Cyber Security", "Cyber Threat Intelligence", "CTI Report Understanding", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb61a69cae67f997321dec9b136cf6bb80935303.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AttackSeqBench, a QA benchmark for cyber threat kill chain investigation built upon publicly available CTI reports. The authors collected multiple CTI reports and use LLM to extract the corresponding attack Tactic/Technique/Procedure behind the natural language sequences. They then use LLM to automatically generate single-hop questions and answers based on the extracted information with evaluation and refinement. Finally, the paper evaluates several LLMs and strategies on AttackSeqBench to analyze their performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Provide a benchmark that focuses on cyber threat kill chain investigation, which is an important area in cybersecurity and not seen before in LLM benchmarks."}, "weaknesses": {"value": "1. The benchmark relies on GPT-4o to generate Q&A items and also to include GPT-4o as one of the evaluated models. This creates a risk of model overlap between data construction and evaluation.\n\n2. The pipeline converts CTI narratives into ATT&CK Tactics/Techniques/Procedures via an LLM-based KG/extraction framework, but the paper does not report precision/recall of this mapping. If a non-trivial fraction of extracted TTPs is wrong, downstream QA quality is directly affected.\n\n3. For Procedure-No questions, even human experts only achieve an accuracy of 0.56, which is not much better than random guessing at 0.5. This suggests that there may be inherent issues with the questions themselves.\n\n4. The utility of the detected attack sequences is questionable. It is not clear about the utility of the attack sequence extracted from the reports. In real attacks, it is extremely difficult for an attack with multiple steps to exactly match each step of an attack in a CTI report. In fact, separating an attack into multiple stages/techniques improves the probability of matching a reported technique to a technique observed in a real attack. This is also the main reason for MITRE to publish the matrix where each stage can have multiple techniques and are not limited to specific combinations of a few techniques. \n\n5. The paper lacks demonstration in practical applications. How the attack sequences can be used is not explained in the paper. Obviously, we cannot hope to find an attack in the future that exactly matches the steps found in this paper’s detected attack sequences. Then how effectively can the attack sequences help attack investigation is not clear. For example, various types of logs, such as command line logs and network logs, record traces of actual attack steps. And recently, system audit logs that collect system call information can further connect these attack steps. However, these logs cannot be easily matched to the detected attack sequences since there are semantic gaps between reported attack steps and the system activities recorded in the logs (e.g., process names, file paths, IP addresses). The authors should explore deeper on these aspects to improve their benchmark."}, "questions": {"value": "1. Would using a reasoning model for question generation improve item quality?\n\n2. How do you explain the low human accuracy on Procedure-No questions?\n\n3. Can you explain the motivation of the attack sequences from CTI reports?\n\n4. How can these sequences be used to assist real-world attack investigation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TKHXhPsWKX", "forum": "e3Z60Ri5JP", "replyto": "e3Z60Ri5JP", "signatures": ["ICLR.cc/2026/Conference/Submission22984/Reviewer_rLu1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22984/Reviewer_rLu1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546542632, "cdate": 1761546542632, "tmdate": 1762942463136, "mdate": 1762942463136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AttackSeqBench, a  benchmark for evaluating LLMs on their ability to understand and reason over adversarial behavior sequences described in real-world CTI reports. The benchmark focuses on three reasoning dimensions: tactical, technical, and procedural, and defines corresponding question-answering tasks aligned with the MITRE ATT&CK framework. Using  408 CTI reports, the authors develop an automated Q&A generation and refinement pipeline and evaluate seven LLMs, five large reasoning models (LRMs), and multiple post-training strategies across zero-shot, context-augmented, and retrieval-augmented (RAG) settings. Results show that current models, including reasoning-optimized ones, struggle to generalize across multi-stage attack reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper formulates attack sequence analysis as a structured reasoning benchmark for LLMs, extending prior CTI benchmarks beyond simple entity extraction or classification tasks. It introduces a scalable framework for constructing TTP-based benchmarks from public CTI reports through automated Q&A generation, refinement, and evaluation aligned with the MITRE ATT&CK framework. The evaluation comprehensively compares diverse LLMs, large reasoning models, and post-training strategies across zero-shot, contextual, and retrieval-augmented settings."}, "weaknesses": {"value": "- **Overreliance on LLM-based dataset construction:** The benchmark depends heavily on LLMs for both attack sequence construction and question generation, yet the paper does not adequately discuss the inherent limitations of this approach. Since the initial attack sequence extraction step is critical to dataset validity, relying on models with limited extraction accuracy raises concerns about noise propagation. For instance, the referenced AttacKG+ framework reports an F1-score of only 56.6%, suggesting that LLM-based extraction can introduce significant factual or structural inconsistencies that may affect benchmark quality.  \n\n- **Task formulation limitations:** Although the benchmark aims to evaluate attack sequence reasoning, the task structure, multiple-choice, and yes/no Q&A primarily test factual recall and event sequencing rather than genuine multi-hop or causal reasoning. Moreover, since the benchmark is based on historical incidents, certain tasks (e.g., identifying techniques between two observed events) can be inherently ambiguous, making it unclear how well this setup generalizes to predictive or forward-looking CTI reasoning, which would be of greater practical relevance.  \n\n- **Limited analysis of failure modes:** The evaluation focuses largely on accuracy metrics without providing a deeper qualitative breakdown of model errors, such as confusion between tactics, reasoning gaps, or noise in retrieved context. A more detailed failure taxonomy or per-tactic performance analysis could offer insights into specific reasoning weaknesses and guide targeted model improvements.  \n\n- **Shallow treatment of RAG and post-training strategies:** While the paper observes that retrieval and fine-tuning underperform, it does not analyze the root causes behind these results. Factors such as embedding misalignment, retrieval noise, or insufficient context integration may explain this behavior. Incorporating ablation studies, retrieval relevance evaluations, or context-quality diagnostics would make the findings more informative and actionable."}, "questions": {"value": "- For the RLVR experiments, how were the task rewards designed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h2kAgo7ICA", "forum": "e3Z60Ri5JP", "replyto": "e3Z60Ri5JP", "signatures": ["ICLR.cc/2026/Conference/Submission22984/Reviewer_vnNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22984/Reviewer_vnNQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728055891, "cdate": 1761728055891, "tmdate": 1762942462527, "mdate": 1762942462527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents *AttackSeqBench*, a Q&A benchmarks to evaluate AI capabilities on *attack sequence* question answering. This work is relevant as AI is increasingly used in cyber-threat intelligence (CTI) to make sense of dense and unstructured CTI reports. The paper performs a comprehensive evaluation of LLMs and LRMs on the task and highlights knowledge gaps."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "- The paper provides a comprehensive evaluation of various kinds of LLMs and LRMs, including hyperparameter tuning experiments to reveal significant gaps in CTI capabilities\n- Human cross-evaluation of the Q&A benchmark with high Likert scores strengthens the claim that the benchmark is  viable for evaluating CTI capabilities\n- Introduction of the AttackSeq-Procedure-No questions highlights interesting gaps in LLM reasoning capabilities where the performance drops on these negated questions"}, "weaknesses": {"value": "- Comparing human scores with LLM and LRM zero-shot scores reveals that the models already outperform on tactics, techniques, procedures-yes, and procedures-no. This indicates that the LLM generated questions are better suited for LLM answering rather than human answering, which may indicate an undue bias in the benchmark and break its viability\n- The paper should consider the evolution of MITRE ATT&CK KB across different versions which may affect the Zero-shot and Context-aware settings as different models may have a different picture of the MITRE ATT&CK KB depending on their knowledge cutoff date. For context, MITRE ATT&CK has gone from version 14 in 2023 to version 18 currently, where certain TTP numbers may have been added, changed, or deleted.\n- There are substantial flaws in presentation of the paper contents that require frequently switching back and forth between the main text and the appendices to correctly understand the approach. For instance, Figure 1 and corresponding acronyms like KB, KG are not explained in the main text, Table 1 Likert scores cannot be deciphered without appendix A.2. There are  more such instances."}, "questions": {"value": "- Figure 1 is not explained adequately in the text, for instance why are there 3 victims? What are the purpose of staging server if there is a C2 server?\n- Line 107: KB is not defined, is it knowledge base?\n- Figure 2, Line 138: KG is not defined, is it knowledge graph?\n- Some typos and grammar mistakes such as \"combin\", \"even severer\", \"pf\"\n- Line 179 mentions 3 categories however there should be as many as the columns in Table 1. Also the text should clearly list the abbreviations as they are not obvious\n- According to the AttacKG+ paper, they claim to process 500 CTI reports, so the claim (line 1125) that AttackSeqBench processes substantially more CTI reports is false"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OJ40YZwE0b", "forum": "e3Z60Ri5JP", "replyto": "e3Z60Ri5JP", "signatures": ["ICLR.cc/2026/Conference/Submission22984/Reviewer_kRQn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22984/Reviewer_kRQn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938099969, "cdate": 1761938099969, "tmdate": 1762942462140, "mdate": 1762942462140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for evaluating LLMs' ability to analyze cyber attack sequences. The benchmark evaluates models on TTPs using question-answering tasks across multiple settings (zero-shot, context-based, and retrieval-augmented). The authors test 7 LLMs, 5 LRMs, and 4 post-training strategies. The results help highlight how current models struggle with reasoning over sequential attack behaviors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tests multiple models and settings (zero-shot, context, RAG), providing a deep dive into current model limitations.\n\n2. The paper includes an automated pipeline for QA construction and refinement, making the benchmark extensible."}, "weaknesses": {"value": "1. The benchmark focuses only on QA tasks, which may oversimplify real-world CTI workflows. In practice, CTI analysts need summarization, timeline reconstruction, and prediction, thus not only question answering.\n\n2. The work is heavily tied to the ATT&CK framework, which is not the only threat modeling method (e.g., Diamond Model, Lockheed Martin Kill Chain). This may limit adaptability to other schemas.\n\n3. The study lacks validation with actual security analysts to show whether improvements on the benchmark lead to meaningful utility in real-world workflows.\n\n4. The automated QA generation and refinement pipeline uses LLMs. This introduces unknown errors, biases, or synthetic evidence into the benchmark dataset itself.\n\n5. The paper shows RAG performs poorly, but offers limited discussions or insights on how to improve retrieval-augmented reasoning in this domain."}, "questions": {"value": "Please see the comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "csBw1mzScy", "forum": "e3Z60Ri5JP", "replyto": "e3Z60Ri5JP", "signatures": ["ICLR.cc/2026/Conference/Submission22984/Reviewer_ipWo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22984/Reviewer_ipWo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964574155, "cdate": 1761964574155, "tmdate": 1762942461884, "mdate": 1762942461884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}