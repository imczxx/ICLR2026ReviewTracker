{"id": "SaBwaLcHNZ", "number": 3735, "cdate": 1757508492819, "mdate": 1759898072602, "content": {"title": "Character Mixing for Video Generation", "abstract": "Imagine Mr. Bean stepping into Tom and Jerry---can we generate videos where characters interact naturally across different worlds? We study inter-character interaction in text-to-video generation, where the key challenge is to preserve each character’s identity and behaviors while enabling coherent cross-context interaction. This is difficult because characters may never have coexisted and because mixing styles often causes **style delusion**, where realistic characters appear cartoonish or vice versa. We introduce a framework that tackles these issues with Cross-Character Embedding (CCE), which learns identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA), which enriches training with synthetic co-existence and mixed-style data. Together, these techniques allow natural interactions between previously uncoexistent characters without losing stylistic fidelity. Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion, enabling new forms of generative storytelling. Our project page https://mi-mi-x.github.io/.", "tldr": "Our work explores multi-character text-to-video generation (e.g., mixing Tom and Jerry with Mr. Bean), preserving individual identities and personalized motions while enabling smooth, natural interactions.", "keywords": ["text-to-video generation; multi-character personalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/603e367a0798054a07273995c9cf153e56f376ab.pdf", "supplementary_material": "/attachment/08daa965218c398ab7db743bde136bb2df13d15c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework for enabling natural interactions between multiple characters from different visual domains, such as mixing Mr. Bean with Tom and Jerry using text-to-video (T2V) generation. The authors address two key challenges: non-coexistence, where characters from separate shows never appear together in training data, and style delusion, where mixed-style characters lose their original visual fidelity. To overcome these, they propose Cross-Character Embedding (CCE), which learns disentangled identity and behavioral representations from multimodal sources through structured character–action captions, and Cross-Character Augmentation (CCA), which generates synthetic co-existence data by compositing characters across domains while preserving native styles. As a result, the paper shows improvement compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A key strength of this paper lies in its clear motivation, to bridge the gap between distinct fictional universes by enabling natural, style-consistent multi-character interactions. This being imaginative and well-grounded in practical generative modeling challenges. The authors present a thoughtfully designed data composition strategy, curating a dataset that balances diversity across cartoons and live-action domains while maintaining detailed character and style annotations. \n\nThis well-structured dataset, combined with the Cross-Character Embedding and Augmentation techniques, provides a solid foundation for learning both behavioral and stylistic nuances. Overall, the study is well-organized, and the presentation neatly ties together motivation, method, and results into a coherent and convincing."}, "weaknesses": {"value": "[Major]\nWhile the paper presents an impressive dataset and compelling demonstrations, its technical contribution beyond data construction and fine-tuning remains relatively incremental. The proposed Cross-Character Embedding (CCE) and Cross-Character Augmentation (CCA) modules, though effective, largely extend existing ideas of prompt-based disentanglement and synthetic compositing rather than introducing fundamentally new architectural or generative mechanisms. \n\nThe framework heavily relies on caption engineering, LoRA-based adaptation, and GPT-assisted annotation which approaches that are conceptually straightforward and depend more on large-scale data quality than algorithmic novelty. Consequently, while the work excels in implementation and application scope, its methodological innovation is modest compared to the scale of its dataset and the strength of its empirical results. \n\n[Minor]\nSome references related to video personalization are missing. ToonCrafter [1] proposed a fine-tuning technique for adapting video diffusion models to general cartoon domains, while AnyMoLe [2] introduced a video fine-tuning framework for generating motion-consistent videos of a single character.\n\nThere is typo in L464 (Section ??).\n\n[1] ToonCrafter: Generative Cartoon Interpolation via Diffusion Models. Liu et al., SIGGRAPH ASIA, 2024.\n\n[2] AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models. Yun et al., CVPR, 2025."}, "questions": {"value": "How many A100 GPUs were used and how long does it took for training?\n\nAre you planning to open-source the code and dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSHhTvj79K", "forum": "SaBwaLcHNZ", "replyto": "SaBwaLcHNZ", "signatures": ["ICLR.cc/2026/Conference/Submission3735/Reviewer_pamK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3735/Reviewer_pamK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761027883450, "cdate": 1761027883450, "tmdate": 1762916953648, "mdate": 1762916953648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common questions"}, "comment": {"value": "We thank the reviewers for the valuable feedback. Please find the answers of the common questions below. \n \n---------------\n**1. Movitation, Goals and Novelty**\n\nOur **motivation** stems from a simple yet profound question: *Can we faithfully recreate a real person—or even a stylized cartoon character—by extensively learning from their videos and scripts?* Existing generative models can produce videos that look like a person, but what we seek is to create videos that feel like them: capturing their essence, expressions, motion patterns, personality cues, and how they would react when placed in new environments or interacting with characters they have never seen. We refer to this capability as `personalization`. \n \nInspired by this, we started our research journey across four stages:\n\n| **Stage**  | **Train Data** | **Challenge and Focus**  |\n|------------|---------------------|-------------------|\n| **single-person** | *Mr. Bean* | **`Personalization`**: preserving the identity, motion and behavior of an individual character | \n| **cross-person** | *Three Bears* | **`Multi-characters grounding`**: enable accurate multi-character grounding and caption, allowing the video model to asccociate each character with its visual appearance. |\n| **cross-cast, single-style** | *Three Bears* *Tom & Jerry* | **`Non-coexistence`**: enable coherent interactions across characters from different cast. |\n| **cross-cast, cross-style** | *Mr. Bean*  *Tom & Jerry*| **`Style-Delusion`**: enable coherent interactions among characters of different styles while preserving their identity and visual appearance.|\n\nIn summary, our **goal** is to **learn personalized characters and enable cross-cast and cross-style interactions**. \n\nOur **novelty** lies in addressing this goal and overcoming the associated challenges, as none of the existing works are capable of achieving this. Most related studies mainly focus on a single subject, a single style, or a single motion personalization only.\n\n---------\n**2. Generalization and Personalization Trade-off**\n\nOur approach requires training on unseen characters. But **generalization and personalization are inherently a trade-off**. General video models exhibit strong generalization ability but limited personalization, whereas personalized models produce videos with highly preserved identity but require additional training. Its difficult to achieve both simultaneously. **Our approach, primarily emphasizing personalization, therefore involves training on unseen characters** to acquire the necessary visual and motion knowledge. \n\nWe have included this discussion in Section 5 in the revised version. \n\n---------\n\n**3. Our Generalization: New Interactions in New Scenarios**\n\nOur work specifically focuses on **generalizing a single person’s reactions to novel scenes and novel contexts**. Some reviewers asked about generalizing to new identities without training, but this reflects a misunderstanding of our core task. The MIMIX problem setting is not about generating a generic identity from a few images; rather, it is about building a rich, characteristically consistent, behaviorally grounded model of specific individuals, learned from extensive multimodal data. A single image or a short video is fundamentally insufficient to capture a person’s identity essence, behavioral style, or expressive dynamics. Therefore, generalization to unseen identities is outside the scope of our formulation and is conceptually distinct from the personalization problem we target.\n\nWe have included this discussion in Section 5. \n\n------\n**4. Model-Agnostic Validation**\n\nWe trained our approach on **Wan2.2-TI2V-5B**. Here is a comparison of [Wan2.2-5B](https://mi-mi-x.github.io/src/assets/wan2.2-5B-Mr._Bean_tries_to_eat_a_spaghetti_sandwich_on_a_be.mp4) and [Wan2.1-14B](https://mi-mi-x.github.io/src/assets/wan2.1-14B-Mr._Bean_tries_to_eat_a_spaghetti_sandwich_on_a_be.mp4). We observe that Wan2.1-14B produces higher visual quality and notably more stable and consistent motion than Wan2.2-5B. This performance gap may stem from Wan2.1-14B being nearly three times larger in size, and Wan2.2-5B employs higher compressed video VAE. **Both models can effectively mix cartoon and realistic character,** demonstrating that our approach is model-agnostic, where the video quality varies depending on the foundation model. \n\nWe will include this in the revised version. \n\n------\n**5. Training Time**\n\nThe model is trained on 8 80G A100 GPUs for 5 days. More details are included in Section 3.3."}}, "id": "nLyfOvaPjt", "forum": "SaBwaLcHNZ", "replyto": "SaBwaLcHNZ", "signatures": ["ICLR.cc/2026/Conference/Submission3735/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3735/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3735/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763670310821, "cdate": 1763670310821, "tmdate": 1763670310821, "mdate": 1763670310821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify two core challenges: non-coexistence, meaning some characters never appear together in training data, and style delusion, meaning cartoon and live action styles bleed into each other. To address this, they propose Cross Character Embedding, which uses structured captions of the form [character: name], action to disentangle identity and behavior, and Cross Character Augmentation, which pastes segmented characters into foreign style backgrounds to simulate cross-universe co-occurrence while preserving each character’s native look."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a concrete and highly visible capability gap in current video generation systems. Existing models can often render a single customized subject, but coherent multi-character interaction across different shows and even across cartoon versus live action domains remains very brittle.\n\n2. The authors curate a reasonably large, annotated, behavior-aware dataset, and define evaluation metrics that target identity, motion, style, and interaction quality in multi-character settings.\n\n3. Qualitative demos show convincing multi-character interactions that typical text-to-video systems struggle with."}, "weaknesses": {"value": "1. Baseline fairness is underspecified. The proposed model is LoRA fine tuned on an 81 hours character specific dataset, whereas baselines may be evaluated mostly zero shot. This makes it hard to attribute the gains to CCE or CCA rather than just stronger task specific tuning.\n\n2. Generalization is narrow. All results focus on about ten characters from four shows. The paper does not show that the approach scales to arbitrary new identities or unseen characters without new fine tuning."}, "questions": {"value": "1. How were baselines adapted. Did you fine tune SkyReels A2, Wan2.1 I2V, or other baselines on the same eighty one hour dataset with LoRA style adapters, or are they evaluated zero shot. Please clarify to ensure a fair comparison.\n\n2. Can you quantify failure modes. The discussion notes occasional breakdowns in highly complex multi character scenes with overlapping motion patterns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oDrY7jV0tm", "forum": "SaBwaLcHNZ", "replyto": "SaBwaLcHNZ", "signatures": ["ICLR.cc/2026/Conference/Submission3735/Reviewer_gpfc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3735/Reviewer_gpfc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382592374, "cdate": 1761382592374, "tmdate": 1762916953424, "mdate": 1762916953424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new task termed Character Mixing, aiming to generate multi-character videos where characters from different IPs or styles appear and interact in the same scene. The authors construct a dataset by mixing existing videos and captions with the help of large language and vision models, and design two modules (CCE and CCA) to align cross-character and cross-style information. Experiments on several cartoon characters are reported."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of character mixing is creative and intuitively interesting, which could inspire further exploration in cross-style or cross-domain video generation.\n\n2. The authors contribute a dataset with structured captions for multi-character interaction videos, which may be useful for future research."}, "weaknesses": {"value": "1.Low problem significance.\nWhile entertaining, the problem does not address a clear or impactful research challenge. It is more of a creative application than a fundamental scientific question. The motivation for why character mixing matters for the video generation community is weak.\n\n2.Limited methodological novelty.\nThe proposed CCE and CCA modules mainly rely on prompting and data augmentation rather than introducing new modeling or learning principles. The improvements largely stem from the underlying base model's(Wan2.1 14B used, smaller size or other open-source model should be presented to support model-independent claim)  capability rather than the proposed method itself.\n\n3.No formalization or mathematical clarity and training details.\nThe paper lacks any formal task definition, notation, or training objective. Without clear training details, it is difficult to reproduce and evaluate the soundness of the approach.\n\n4.Limited scalability and generalization to new characters.\nThe reliance on explicit character-level annotation and LoRA-based fine-tuning means adding a new character (especially from unseen domains) requires re-training or substantial data preparation (highlighted in Discussion, Page 9). This approach does not scale gracefully for open-world or user-specified characters.\n\n5.Missing analysis and discussion.\nThere is little exploration of failure cases, interaction quality (e.g., temporal consistency, occlusion handling), or computational cost. The paper reads more like a demo than a scientific study."}, "questions": {"value": "1.Can the method handle unseen characters or styles at inference time?\n\n2.What would happen if GPT-4o/Gemini were replaced with open-source models (e.g., LLaVA, Qwen2-VL)?\n\n3.How is the proposed CCE/CCA architecture implemented and trained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GZNsYrXAM5", "forum": "SaBwaLcHNZ", "replyto": "SaBwaLcHNZ", "signatures": ["ICLR.cc/2026/Conference/Submission3735/Reviewer_UQK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3735/Reviewer_UQK6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900871757, "cdate": 1761900871757, "tmdate": 1762916953214, "mdate": 1762916953214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for generating videos that feature multiple characters from different fictional universes (e.g., cartoons and live-action shows) interacting with each other. The authors identify two primary challenges: 1) the \"non-coexistence\" of characters in training data, and 2) \"style delusion,\" where characters' visual styles blend undesirably. To address these, they propose a two-part solution. First, **Cross-Character Embedding (CCE)**, a prompt engineering strategy that uses structured captions of the form `` `[Character: <name>], <action>` `` to disentangle character identity and behavior. This allows the model to learn character-specific traits from their respective source videos and compose them at inference time. Second, **Cross-Character Augmentation (CCA)**, a data augmentation technique that synthetically creates training examples of cross-style interaction by segmenting characters from one domain and pasting them into scenes from another. These augmented clips are captioned with an additional `` `[scene-style: <style>]` `` tag to help the model preserve stylistic integrity. The authors fine-tune a large-scale text-to-video model (Wan2.1) on a curated 81-hour dataset and demonstrate through extensive experiments that their method significantly outperforms existing baselines in identity preservation, interaction quality, and style consistency. They also introduce a new benchmark and a set of VLM-based evaluation metrics for this specific task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "*   **Problem Formulation and Significance**: The paper formulates a compelling and significant research problem: enabling characters from different \"universes\" to interact naturally in generated videos. This is a natural evolution of personalized generation and has high potential for creative applications.\n*   **Novel Methodology**: The proposed framework is original in its combination of two distinct ideas to solve two well-defined problems. CCE (via prompt structure) tackles the non-coexistence of characters, and CCA (via synthetic compositing) addresses the style delusion problem. This two-pronged approach is elegant and shown to be effective.\n*   **Strong Empirical Results**: The qualitative results are visually impressive and clearly demonstrate the superiority of the proposed method over existing approaches, which either fail to maintain identity or cannot produce coherent interactions. The quantitative results, despite the aforementioned issues with the ablation tables, generally show a strong performance lead, especially on the task-specific VLM metrics.\n*   **Benchmark Contribution**: The introduction of a dedicated benchmark for multi-character interaction, including a new suite of VLM-based metrics tailored to character identity, motion, style, and interaction, is a substantial contribution in its own right. It provides a more meaningful way to evaluate models on this task than standard metrics alone."}, "weaknesses": {"value": "*   **Scalability and Data Dependency**: The method's primary weakness is its reliance on fine-tuning using a large corpus of video data for a pre-defined set of characters. As implied in Section 3.3, each new character universe (e.g., a TV show) requires collecting hours of video footage and undergoing an expensive fine-tuning process. This makes it difficult to scale to new, arbitrary characters in an open-world setting. As acknowledged by the authors, this is a significant limitation.\n*   **Lack of Human Evaluation and Unjustified Metric Choices**: For a task where success is highly subjective (e.g., \"authenticity\", \"plausibility\"), the absence of a human study is a major weakness. The paper instead introduces VLM-based metrics but fails to justify why established identity preservation metrics (e.g., face recognition similarity) were not used or compared against, especially for the human-like characters. While VLM evaluation is innovative, its reliability is questionable without proper protocol.\n*   **Questionable Reproducibility of VLM-based Evaluation**: The use of a VLM as a core evaluation tool introduces significant reproducibility concerns. The output of VLMs can be stochastic due to parameters like `temperature`. The paper fails to describe the protocol used to ensure deterministic and reproducible scores. Key details are missing: Was `temperature` set to 0? Were scores averaged over multiple runs? What were the exact prompts used? Without this information, the quantitative results in Table 1, 2, and 3 are not fully credible.\n*   **Unverified Claim of Model-Agnosticism**: The paper claims its framework is \"model-agnostic\" (Section 3.1) but provides no empirical evidence. All experiments are conducted by fine-tuning a single base model (Wan2.1-T2V-14B). Without applying the CCE and CCA framework to at least one other distinct T2V backbone, this claim of generalizability remains unsubstantiated.\n*   **Clarity and Consistency of Ablation Studies**: The quantitative results in the ablation section (Tables 2 and 3) are poorly presented. There are numerical inconsistencies for what should be identical experimental conditions across different tables. For example, the results for the full model (\"Ours\" in Table 1, \"w/ Both\" in Table 2, and \"10%\" in Table 3) all report different scores. This makes it impossible to confidently assess the individual contributions of the proposed components. This must be fixed."}, "questions": {"value": "1.  **On Data Requirements**: Could you please clarify the data requirements more explicitly? The paper implies a single fine-tuning on a mixed dataset. Does this mean to add a new character from a new TV show, one must re-run the entire fine-tuning process on the combined old and new data? What is the approximate training time (e.g., in GPU-hours) for the reported 5-epoch fine-tuning on the 81-hour dataset?\n2.  **On Evaluation Metrics**:\n    *   (a) Could you justify the decision to exclusively use VLM-based metrics for identity preservation over established methods like face recognition ID similarity, at least for the human characters (Mr. Bean, Young Sheldon's cast)? A comparison showing the VLM's superiority would strengthen your metric choice.\n    *   (b) Crucially, what protocol was used to ensure the VLM evaluation is deterministic and reproducible? Please provide the exact prompts, model version, and parameter settings (especially `temperature`) used to query Gemini-1.5-Flash for the Identity-P, Motion-P, Style-P, and Interaction-P scores in the appendix.\n3.  **On Generalizability**: To substantiate the \"model-agnostic\" claim, could you provide any results or insights, even preliminary, from applying your CCE and CCA framework to a different base T2V model?\n4.  **On Inconsistent Ablation Results**: Could you please clarify and unify the results presented in Tables 1, 2, and 3? Specifically, please ensure that the results for the same experimental setup are consistent across all tables and explain the counter-intuitive findings in Table 2 (e.g., the drop in `` `Subject-C` ``).\n5.  **On CCE's Mechanism**: The term \"Cross-Character Embedding\" suggests a specific learned representation. Can you clarify whether your method learns an explicit, separable embedding for each character, or if CCE is more accurately described as a structured prompting technique that influences the text-conditioning of the frozen T2V model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ccc2FZ8VDV", "forum": "SaBwaLcHNZ", "replyto": "SaBwaLcHNZ", "signatures": ["ICLR.cc/2026/Conference/Submission3735/Reviewer_Fsis"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3735/Reviewer_Fsis"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989155019, "cdate": 1761989155019, "tmdate": 1762916952970, "mdate": 1762916952970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}