{"id": "81dCbpP7cs", "number": 21931, "cdate": 1758323715195, "mdate": 1759896895280, "content": {"title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models", "abstract": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models’ (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we’ve developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.", "tldr": "", "keywords": ["benchmarks", "datasets", "generative models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e68259c51dc56f77d14de4f5a08ca2fc2d363fd7.pdf", "supplementary_material": "/attachment/c739b6c6f63039270c4d8c3c7ecfd3c84a69c524.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SOCK, a command line interface (CLI) benchmark designed to measure self-replication capabilities in large language models (LLMs). The authors define self-replication broadly to include not only creating functional copies but also persistence across different computational contexts. The benchmark categorizes models using two metrics: Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL), evaluated through a five-task suite that progresses from basic file copying to cross-container replication."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a timely and critical research problem—LLM self-replication—which has significant implications for AI safety and multi-agent systems. The focus on persistence across computational contexts extends beyond simple replication, adding depth to the evaluation framework."}, "weaknesses": {"value": "1. Due to the short page length, key details are omitted. For example:\n* The R-score formula (Section 4.1) references constants (e.g., τ, Bᵢ) and task-specific baselines without providing their values or derivation process.\n* The \"intelligence components\" (reasoning, tool use, recovery) are mentioned but not defined operationally, leaving ambiguity in how they are measured.\n2. The results (Table 1) are presented without in-depth analysis. For instance:\n* Why do models like Gemini-2.5-Flash outperform GPT-5 despite lower general capability? The explanation provided is vague and lacks empirical support.\n* Error patterns and failure modes are not analyzed, missing insights into specific bottlenecks (e.g., context retention, tool misuse)."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oB2ixDFKQj", "forum": "81dCbpP7cs", "replyto": "81dCbpP7cs", "signatures": ["ICLR.cc/2026/Conference/Submission21931/Reviewer_3iQa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21931/Reviewer_3iQa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624027218, "cdate": 1761624027218, "tmdate": 1762941987729, "mdate": 1762941987729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOCK, a command-line interface (CLI) benchmark designed to evaluate large language models' (LLMs) ability to self-replicate without human intervention. The benchmark consists of five tasks with increasing complexity, categorized by Replication-Capability Levels (RCL 0-2) and Persistence-Capability Levels (PCL 0-2). The authors test eight frontier models across these tasks and compute an R-score that combines success, speed, stealth, intelligence, and resource efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, Practical Implementation: Docker-based CLI benchmark with controlled environments is a solid engineering contribution.\n\n2, Clear Task Categorization: The RCL-PCL taxonomy provides an intuitive framework for categorizing replication capabilities.\n\n3, Multi-Model Evaluation: Testing 8 frontier models provides some comparative insights."}, "weaknesses": {"value": "1, Severely Limited Scope.\n- Only 5 tasks vs. 20 task families in RepliBench.\n- Excludes critical capabilities (resource acquisition, weight exfiltration)\n- Achieves only RCL 2/PCL 2, missing higher-level threats\n\n2, Flawed Scoring Metric.\n- Geometric mean penalizes failures too harshly\n- Many hyper-parameters without justification\n\n3, Unclear Threat Model.\nThe adversarial framing (\"user aims to prevent replication; agent aims to maximize replication\") is mentioned but not formalized or evaluated\n\n4, Missing Analysis.\nNo failure mode analysis, no ablation studies, no discussion of why certain models succeed/fail."}, "questions": {"value": "Could you compare your work with RepliBench and highlight the key differences and contributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gQwz3zesp3", "forum": "81dCbpP7cs", "replyto": "81dCbpP7cs", "signatures": ["ICLR.cc/2026/Conference/Submission21931/Reviewer_ZhxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21931/Reviewer_ZhxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730943194, "cdate": 1761730943194, "tmdate": 1762941987448, "mdate": 1762941987448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOCK, a command-line benchmark for evaluating large language models’ (LLMs) ability to self-replicate. The framework defines Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL), implements five progressively difficult CLI tasks , and computes an overall R-score reflecting success, efficiency, and resource use."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: The proposed framework for RCL and PCL  provides a clear and valuable structure for decomposing and categorizing a multi-dimensional problem.\n\nS2: The R-score design is a robust contribution. It looks beyond binary pass/fail rates to include V,  P, and S ."}, "weaknesses": {"value": "W1: The benchmark tasks mainly test basic command execution and do not capture the higher-level reasoning or planning implied by “self-replication.”\n\nW2: The gap between the framing (which includes up to Level 5) and the actual measured capability (which ceilings at RCL 2/PCL 2) weakens the conceptual impact.\n\nW3: The study focuses solely on CLI-based operations, omitting dimensions such as multi-step decision making, communication, or long-term coordination.\n\nW4: Heuristic scoring function: The R-score aggregates multiple factors with fixed weights (e.g., $w_d, w_v, w_s... = 1$ 10), yet the paper provides no analysis of sensitivity, robustness, or interpretability.\n\nW6: The experiments confirm intuitive expectations (more efficient models score higher) but do not yield new understanding of model behavior.\n\nW7: The writing is clear, but much of the paper reads like documentation of a software tool rather than an analysis-driven research study."}, "questions": {"value": "Q1: What motivated the specific choice of these five tasks and their associated difficulty levels, given the much broader 0-5 level taxonomy?\n\nQ2: Can the RCL–PCL taxonomy be validated or compared to external behavioral definitions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FW0wDQTO5L", "forum": "81dCbpP7cs", "replyto": "81dCbpP7cs", "signatures": ["ICLR.cc/2026/Conference/Submission21931/Reviewer_1dqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21931/Reviewer_1dqA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753063733, "cdate": 1761753063733, "tmdate": 1762941987241, "mdate": 1762941987241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOCK, a benchmark designed to systematically evaluate the self-replication capabilities of large language models (LLMs) within controlled environments.  It defines two hierarchical dimensions—Replication-Capability Level (RCL) and Persistence-Capability Level (PCL)—and implements five progressively challenging tasks, from file duplication to cross-container persistence.  The framework uses isolated Docker environments to measure replication success, efficiency, and stealth.  Experiments with eight mainstream LLMs show that while models can reliably reproduce themselves in simple settings, higher-level persistence and cross-environment replication remain challenging.  SOCK provides the first standardized foundation for studying autonomous replication behaviors in LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe work is highly original in defining and operationalizing the concept of LLM self-replication. By introducing RCL and PCL as structured capability levels and implementing them in sandboxed environments, the paper turns a speculative and safety-sensitive topic into a measurable research direction. \n\nSignificance:\nThe experimental setup is creative and technically complete, offering a reproducible platform for future work on AI autonomy and containment evaluation.\n\nThe paper does not have a significant advantage in terms of clarity and soundness."}, "weaknesses": {"value": "1.  Limited contribution and insufficient depth: \nAlthough the paper demonstrates strong originality in framing the concept of LLM self-replication, its overall contribution remains narrow. The implemented benchmark covers only low-level replication scenarios, and the experimental scope is relatively small. As a result, the work lacks the richness and depth expected for a comprehensive study.\n2. Insufficient experiments\nThe experimental setup is severely limited and lacks statistical depth.   Results are based on only five trials without reporting variance or significance tests, so the reliability of performance differences is unclear.   The scoring formula uses equal weights for all components but offers no justification or sensitivity analysis.   Tasks are independent and memoryless, preventing evaluation of long-term or cumulative replication.   Overall, the experiments validate feasibility rather than provide rigorous quantitative evidence.\n3. Reproducibility and scalability issues\nThe provides no scalability analysis. No runtime or memory statistics are reported.  Key metrics such as stealth and intelligence are not accompanied by logs or examples, making it difficult to verify scoring.  Without detailed experimental traces or resource profiling, reproducibility and scalability remain uncertain, weakening the benchmark’s credibility for large-scale evaluation."}, "questions": {"value": "1.  Experimental reliability\nThe experiments use only a few trials without variance or significance reporting.  Can the authors clarify how consistent the results are across multiple runs or random seeds?\n\n2.  Scalability\nHow does the framework perform when the number of agents or replication tasks increases?  Is there any data on runtime, resource usage, or LLM-call efficiency?\n\n3.  Higher-level replication\nThe benchmark currently stops at RCL2/PCL2.  Do the authors plan to extend it to more complex or long-term replication tasks, and how will safety be ensured in such experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hFAAqq91G7", "forum": "81dCbpP7cs", "replyto": "81dCbpP7cs", "signatures": ["ICLR.cc/2026/Conference/Submission21931/Reviewer_CR4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21931/Reviewer_CR4o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002393342, "cdate": 1762002393342, "tmdate": 1762941986591, "mdate": 1762941986591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}