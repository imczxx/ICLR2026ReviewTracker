{"id": "mkx3xkRBkw", "number": 2686, "cdate": 1757202653648, "mdate": 1759898133356, "content": {"title": "The Chosen Few: Sparse Adaptation for Large Models", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have become essential for adapting large pretrained models to downstream tasks, with Low-Rank Adaptation (LoRA) emerging as one of the most widely adopted solutions.\nHowever, there remain several key limitations in  current LoRA-based PEFT methods:\n(1) the low-rank feature space in LoRA is rigid, reducing its capacity for dynamic adaptation;\n(2) the restricted dimensionality, coupled with dense and entangled representations, constrains the model’s capacity to generalize across multiple domains; \nand (3) the compression process limits the extent to which model behavior can be understood from the learned representations, making it difficult to interpret the functional role of task-relevant features.\nIn this paper, we argue that sparse adaptation offers a principled and more flexible alternative to low-rank adaptation, with the added benefit of enhancing interpretability.\nInstead of compressing information into a low-rank subspace, sparse adaptation focuses on identifying and selectively activating a small subset of high-dimensional latent features, enabling a more decomposed and dynamic fine-tuning process.\nBuilding on this paradigm, we propose STAN (Sparse adapTAtioN), a novel method that actualizes sparse adaptation by integrating dedicated Sparse Autoencoder (SAE) modules into frozen pretrained models. \nSTAN learns to encode task-specific adaptations through sparse activations within the SAEs, thereby using sparse features as the mechanism for dynamic and robust adaptation.\nBeyond the flexibility offered by input-dependent sparse combinations, the large latent space of the SAEs provides scalable capacity for cross-domain adaptation, while their inherent semantic decomposition structure supports more interpretable representations.\nThrough extensive experiments, we demonstrate that STAN outperforms state-of-the-art PEFT baselines across a range of benchmarks, while uniquely enabling inspection and analysis of the learned sparse activations. Our findings position sparse adaptation as a promising new direction in PEFT, advancing both the expressivity and interpretability of model adaptation.", "tldr": "", "keywords": ["PEFT", "finetuning", "SAE"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc2f78a57f07dfd31aa017ebb390f4e986b7327a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript under review suggests an improvement over LoRA (which packs task changes into a tiny, dense, low-rank space) by proposing STAN, which drops tiny sparse autoencoder (SAE) modules into a frozen model and lets only a few latent “neurons” fire per input (a Top-K selection). Concretely, the encoder maps activations into a large latent space, the Top-K gate keeps only the strongest features, and the decoder maps them back to produce the adjustment added to the frozen layer’s output. The methodology is easy to follow (see sec 3). Across language (GLUE with RoBERTa/DeBERTa), multimodal VLMs (LLaVA), and a diffusion model (Stable Diffusion 3), STAN matches PEFT baselines like LoRA/AdaLoRA/SoRA. However, in the crowded world of LLM finetuning after 2022 (LoRA), the experimental section is lacking and also I feel there are some caveats related to the results (see weaknesses)"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, intuitive idea: Swapping “low-rank and dense” for “high-dimensional and sparse” is well-motivated, and the Top-K SAE view makes the adaptation non-linear and input-specific (a mix of subspaces instead of one fixed subspace). The math is compact and readable. \n2. Broad, consistent wins. On GLUE and several LLaVA benchmarks, STAN seems to perform well. \n3. Generative demo with interpretability. The SD3 “style alignment” experiment shows both better scores (CLIP/DINO) and a convincing story that different styles trigger different sparse units—nice evidence of disentanglement.\n4. Positioning vs. LoRA-family. The critique of LoRA’s rigidity/entanglement and the argument why this method helps are clearly laid out."}, "weaknesses": {"value": "First of all to justify the validity of the technique, more experiments are needed especially applying it to different LLMs and instruction following (Mistral, Llama and GPT models are standard). Next, I don't understand from where the authors took the values of Table 1 and Table 2? did they reproduce the experiments? If they reproduced the experiments then why is that not mentioned in the tables as is standard? The only indication seems to be their declaration of the hyperparameters, but that is indirect and also looking at the appendices is a choice not an obligation for the reviewers. At first I thought the values are standard but then I had some doubts and I rechecked and some of the reported values seem to be lower than in the VeRA paper (ICLR 2024). There may exist other works in the meantime which reported higher scores. For Table 4 LoRA, where are the scores reported from?\nFurther,\n\n1. The paper says the SAE path doesn’t increase inference complexity, but practical costs (latency/throughput/memory on real deployments) aren’t quantified; E/D projections and Top-K still add ops, so clearer wall-clock and memory numbers would help. \n2. Performance depends on K and latent expansion; ablations are on an MNLI subset only. It’s unclear how robust those settings are across tasks and scales without more sweeping studies. \n3. The Top-K router is the core mechanism; training stability is said to be good, but implementation/tricks (e.g., straight-through choices, tie-breaking) are not discussed in detail in the main body. \n4. CLIP/DINO scores are narrow; human evals, more styles, safety/control stress-tests would solidify the generative claims."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t3Nillsz8W", "forum": "mkx3xkRBkw", "replyto": "mkx3xkRBkw", "signatures": ["ICLR.cc/2026/Conference/Submission2686/Reviewer_8rX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2686/Reviewer_8rX2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760543452726, "cdate": 1760543452726, "tmdate": 1762916330737, "mdate": 1762916330737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes replacing additive PEFT methods, such as LoRA, with SAE to improve model performance. The authors validate the superiority of the proposed approach over baselines on language models, VLMs, and SD models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The overall writing of the paper is clear and easy to read.\n- The authors provide a comprehensive set of tasks to demonstrate the superiority of their method.\n- The experiments in Fig. 3 can, to some extent, illustrate the role of high-dimensional sparsity in concept disentanglement during fine-tuning."}, "weaknesses": {"value": "- Many sparse fine-tuning methods are not discussed or compared.[1-4]\n- Line 98 claims that no additional inference complexity is introduced, but incorporating SAE does require extra computational overhead.\n- The experimental section lacks many details.\n    - The study on interpretability brought by sparsity is limited to Fig. 3, and this experiment does not provide the performance of comparative methods.\n    - What is the training procedure of the model? Does introducing SAE require an additional self-supervised training process? What is the associated training cost?\n    - How is SAE specifically integrated into the model? How does its placement affect the model’s performance?\n    - The comparative experiments do not report the tunable parameter scales for the different methods.\n\n[1] Sensitivity-aware visual parameter-efficient fine-tuning. CVPR'23\n\n[2] Gradient-based parameter selection for efficient fine-tuning. CVPR'24\n\n[3] On the effectiveness of parameter-efficient fine-tuning. AAAI'23\n\n[4] Expanding Sparse Tuning for Low Memory Usage. NeurIPS'24"}, "questions": {"value": "- How is SAE specifically integrated into the model? Is it used only once or multiple times? The authors should provide detailed explanations along with comparative experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CcdCATtVBb", "forum": "mkx3xkRBkw", "replyto": "mkx3xkRBkw", "signatures": ["ICLR.cc/2026/Conference/Submission2686/Reviewer_gQSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2686/Reviewer_gQSZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805161555, "cdate": 1761805161555, "tmdate": 1762916330597, "mdate": 1762916330597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel parameter-efficient fine-tuning method called STAN, which aims to address several limitations of the LoRA approach. The low-rank compression paradigm underlying LoRA inherently leads to issues such as rigid adaptability, representation entanglement, and poor interpretability. To overcome these problems, the authors introduce a sparse adaptive mechanism that dynamically and sparsely selects a small subset of the most relevant features in a high-dimensional latent space for each input. The proposed method achieves superior performance compared to existing parameter-efficient fine-tuning techniques on multiple benchmark datasets and provides analyses demonstrating its interpretability."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.The paper creatively applies the concept of sparse autoencoders to parameter-efficient fine-tuning (PEFT), shifting from low-rank compression to high-dimensional sparse selection, thereby addressing LoRA’s inherent rigidity and representation entanglement issues.\n\n2.The method is thoroughly evaluated across a wide range of tasks, and the results show that STAN outperforms LoRA and its variants.\n\n3.The paper provides concrete and quantifiable interpretability analyses of STAN through a style alignment task."}, "weaknesses": {"value": "1.Although the appendix provides a comparison of inference latency between STAN and other methods, STAN’s adaptive signal is input-dependent, which means its adapters cannot be precomputed and merged into the original model weights as in LoRA.\n\n2.Since LoRA-based fine-tuning is known to suffer from catastrophic forgetting, it would be valuable to investigate whether the STAN method also exhibits similar catastrophic forgetting after fine-tuning."}, "questions": {"value": "Please refer to the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RkeoVNfjJe", "forum": "mkx3xkRBkw", "replyto": "mkx3xkRBkw", "signatures": ["ICLR.cc/2026/Conference/Submission2686/Reviewer_UazD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2686/Reviewer_UazD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807913037, "cdate": 1761807913037, "tmdate": 1762916330449, "mdate": 1762916330449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}