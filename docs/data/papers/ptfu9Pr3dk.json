{"id": "ptfu9Pr3dk", "number": 2973, "cdate": 1757309604492, "mdate": 1759898116362, "content": {"title": "Symmetries in Weight Space Learning: To Retain or Remove?", "abstract": "Weight-space learning, an emerging paradigm that studies neural networks through their parameter space, has shown promise for tasks ranging from predicting model behavior to addressing privacy risks. An important caveat in weight-space learning is that neural networks admit extensive parameter symmetries: distinct weight configurations can implement the same function. Such symmetries have been studied from multiple angles and play an important role in both theory and practice, including Low-Rank Adaptation (LoRA), a state-of-the-art fine-tuning method for large language models (LLMs) that exhibits scale and rotational invariances.\nIn this paper, we present a theoretical study of symmetries in weight-space learning and ask: What is the appropriate problem formulation in the presence of symmetries (e.g., those induced by LoRA), and should redundant representations that encode the same end-to-end function be removed? We answer this by showing that whether redundancy matters depends on the target functional of interest. In particular, we prove that end-to-end symmetries (such as those in LoRA) should not always be quotiented out: doing so can compromise universality for classes of weight-space prediction tasks. To our knowledge, this is the first formal identification of this phenomenon, offering principled guidance for the design of weight-space methods across many applications.", "tldr": "", "keywords": ["weight space learning", "symmetry", "invariance", "low-rank adaption (LoRA)"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d7dadfc4bb94106948e9c3b00b37e9c5e663727.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work focuses on symmetries in the context of weight space learning. The main research question is to determine whether, in general, quotienting out weight space symmetries makes sense, depending on the downstream task.\n\nTo answer that question, the authors propose a theoretical analysis of weight space symmetries in the context of weight space regression analysis. This analysis is performed across different differentiation orders with respect to the weights. The authors first show that when taking into account only zeroth-order features, weight space symmetries hold and quotienting them out makes sense. They then show, however, that this finding does not hold for higher order features, the first example of which is the Hessian Trace, a second-order feature. They show that this finding generalises to cases where symmetries do not hold, as well as arbitrary-order features.\n\nFinally, the authors test their findings in a small experiment on LoRA adapters for LLMs. They show that the sensitivity of the weights, a second-order feature, is not preserved under transformations of the weights that preserve the function represented by the LLM.\n\nThe conclusion of the paper is that depending on the target variable, quotienting out weight space symmetries does not make sense in all cases and can jeopardise the universality of the weight space learning model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The authors propose a solid, complete theoretical analysis of symmetries in the context of discriminative downstream tasks in weight space learning. I particularly liked the organisation of the paper, starting from the zeroth-order case, then giving the good example of the Hessian trace, to finally prove generalisability to arbitrary-order cases.\n\n* S1: I think the formalisation of the problem is good, and the different analyses look solid. I think they back the main point of the paper well. I appreciate the formulation of the problem using \\$g \\in G\\$ which is very general and can adapt to most kinds of symmetries.\n\n* S2: I am convinced that the research question, as well as the main conclusion of the paper that follows, are very timely, interesting and impactful. By knowing in which cases quotienting out weight space symmetries works, weight space learning practitioners will be able to use the right meta-models for their downstream tasks.\n\n* S3: I like that the authors used the example of LoRAs throughout the paper, given the large number of existing equivalent LoRA matrices. It is also nicely linked with the experiment section."}, "weaknesses": {"value": "While I remain convinced that this paper is generally strong and impactful, it has several weaknesses that justify my rating.\n\n## Major\n\n* W1: The experiment in Section 4 demonstrates practically that there exists some cases where even when the mapping represented by the neural network’s weights is unchanged, other characteristics such as the sensitivity can change. The existence of such cases does justify using weight space learning predictors that are NOT equivariant to such changes. The experiment setup falls short, however, of actually comparing prediction results using different weight space learning models. As such, while the experiment is useful in validating intermediate results, it is not sufficient to back the main claim of the paper. My opinion is that the authors should compare different weight space meta-model architectures to validate empirically their main claim on lines 59-62. For example, the authors could compare the performance of equivariant vs. non-equivariant meta-models on downstream tasks (such as  accuracy prediction or sensitivity analysis) to empirically validate their theoretical claims.\n\n* W2: From my understanding, I think the definition of zeroth-order weight-space learning (Eq. 1) is limited to situations where feature extraction is done by probing the model studied with probes sampled from \\$\\mathcal{X}\\$. Such a method is indeed, by design, invariant to any changes in the model weights that does not result in a change in the functions mapped. Many weight space learning methods, however, do not rely on probes, but rather only on the weights. For example, [1] uses weights statistics, [2] encodes weights such that they are equivariant under permutation symmetries, and [3] uses tokenised weights in a way that is not directly invariant to such symmetries. Given the importance of such methods in the field, I think this problem definition should be covered in the paper. I am relatively confident such an analysis would go in the same direction as the other results in the paper.\n\n* W3: I think the related work section is quite limited. Several key papers in the area of weight space learning are missing. In particular, I think that [2] and [4, 5] are all key papers in the area of equivariance to weight space symmetries. The authors could extend that Section and/or add relevant citations in the introduction to better contextualise their work within the field of weight space learning. Furthermore, statements such as the start of the paper (line 32) imply that weight space learning is predicting properties from model weights. There are other applications to weight space learning such as learning to optimise [6] or neural network weights generation [7, 8]. I recommend the authors be more specific in their phrasing to make explicit that their work focuses on downstream tasks around property prediction, not weight space learning in general.\n\n## Minor (did not impact the rating)\n\n* W4: The paper still contains a lot of typos, in particular in the Introduction. I advise the authors thoroughly proofread their manuscript.\n\n* W5: Regarding Table 1, I think having the “Unperturbed” baseline as a line instead of a column is confusing, since it is simply the case where \\$\\sigma = 0\\$. In addition, on line 428, Table 4 (which does not exist) is referenced instead of Table 1. Also, the last column is \\$5 \\cdot 10^{-4}\\$, is there a typo or are the results not in order?\n\n## References\n\n[1] Unterthiner, T., Keysers, D., Gelly, S., Bousquet, O., & Tolstikhin, I. (2020). Predicting neural network accuracy from weights. arXiv preprint arXiv:2002.11448.\n\n[2] Navon, A., Shamsian, A., Achituve, I., Fetaya, E., Chechik, G., & Maron, H. (2023, July). Equivariant architectures for learning in deep weight spaces. In International Conference on Machine Learning (pp. 25790-25816). PMLR.\n\n[3] Schürholt, K., Mahoney, M. W., & Borth, D. (2024, July). Towards Scalable and Versatile Weight Space Learning. In International Conference on Machine Learning (pp. 43947-43966). PMLR.\n\n[4] Kofinas, M., Knyazev, B., Zhang, Y., Chen, Y., Burghouts, G. J., Gavves, E., ... & Zhang, D. W. (2024). Graph neural networks for learning equivariant representations of neural networks. arXiv preprint arXiv:2403.12143.\n\n[5] Lim, D., Maron, H., Law, M. T., Lorraine, J., & Lucas, J. (2023). Graph metanetworks for processing diverse neural architectures. arXiv preprint arXiv:2312.04501.\n\n[6] Knyazev, B., Moudgil, A., Lajoie, G., Belilovsky, E., & Lacoste-Julien, S. (2024). Accelerating training with neuron interaction and nowcasting networks. arXiv preprint arXiv:2409.04434.\n\n[7] Schürholt, K., Knyazev, B., Giró-i-Nieto, X., & Borth, D. (2022). Hyper-representations as generative models: Sampling unseen neural network weights. Advances in Neural Information Processing Systems, 35, 27906-27920.\n\n[8] Soro, B., Andreis, B., Lee, H., Jeong, W., Chong, S., Hutter, F., & Hwang, S. J. (2024). Diffusion-based neural network weights generation. arXiv preprint arXiv:2402.18153."}, "questions": {"value": "* Q1: This is linked with W2: how do the authors think their framework works on weight-space learning models which only rely on model weights? Under which conditions could and should such models handle weight-space symmetries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PAfYvFzkeg", "forum": "ptfu9Pr3dk", "replyto": "ptfu9Pr3dk", "signatures": ["ICLR.cc/2026/Conference/Submission2973/Reviewer_eyjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2973/Reviewer_eyjU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578287208, "cdate": 1761578287208, "tmdate": 1762916473115, "mdate": 1762916473115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Note:** While I am familiar with weight-space learning, and have gone over the entire paper, I have limited theoretical background and can not fully evaluate the theoretical contributions of the paper. My review therefore focuses mostly on the motivation of the paper, the practical implications of it, and empirical aspects.\n\nThe paper provides a theoretical examination of symmetries in weight-space learning tasks. The authors show that zeroth-order features (based on model outputs) can be handled with standard symmetry-aware methods. They then provide an example of sensitivity prediction on LoRAs where only a subset of the model's symmetries are relevant for learning, followed by an example where no symmetry can be preserved, concluding that any compression of the weight space can harm some downstream tasks. Lastly, they demonstrate that models which are functionally identical can have different sensitivities, validating that symmetry removal must be task-dependent."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a timely question about how to handle symmetries in weight-space learning.\n\n- The empirical evaluation is interesting. The authors demonstrate that two networks which behave the same can have very different sensitivities to perturbations. This observation could be valuable for future research on weight-space learning research as well as for building more robust networks."}, "weaknesses": {"value": "- **Limited practical guidance.** My main concern is that the paper does not provide general guidelines for which symmetries to account for each downstream task. While the authors analyze the relevant symmetries for zeroth-order and sensitivity features, they do not offer guidance for practitioners working on other tasks. The paper would be strengthen from proposing a systematic method for identifying the relevant symmetries to account for given a specific task.\n\n- **Narrow empirical scope.** The experimental section is very limited where only architecture and dataset are evaluated. The conclusio would be much strengthened by testing the same hypothesis on a few more cases.\n\n- **Sectoin 3.4 accessibility.** Section 3.4 introduces many notations making it difficult to follow for readers without a strong theoretical background. This section could be clarified with more intuitive explanations to along the mathematical derivation."}, "questions": {"value": "I dont have any further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YyKyMrjLN3", "forum": "ptfu9Pr3dk", "replyto": "ptfu9Pr3dk", "signatures": ["ICLR.cc/2026/Conference/Submission2973/Reviewer_WzJu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2973/Reviewer_WzJu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752816862, "cdate": 1761752816862, "tmdate": 1762916472826, "mdate": 1762916472826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the role of parameter symmetries in weight-space learning — learning predictors that take model parameters as inputs. The authors distinguish zeroth-order features from higher-order, derivative-based features. Key theoretical claims: zeroth-order meta-regressors inherit the model symmetry; higher-order features are invariant only to the subgroup that fixes the coefficient tensor; many natural weight-space tasks break the model’s parameter symmetries and would be lost if one quotients them out. Experiments on LoRA-fine-tuned Llama show sensitivity to re-factorizations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides a clear, rigorous characterization of when weight-space functionals are symmetric.\n\n2. Sensitivity experiments on real LLM + LoRA show that reparameterization alters higher-order quantities, and a balance transformation is proposed to improve robustness."}, "weaknesses": {"value": "1. The paper does not clarify how the assumption of a “spectral gap” in representations is verified or estimated in real-world networks or LoRA settings. Further discussion is needed on whether this condition generalizes to realistic models.\n\n2. The empirical evaluation focus on a single dataset/model pair; incorporating more diverse experiments would strengthen the empirical evidence and the robustness of the claims."}, "questions": {"value": "1. When $k$ is very large or when there are special elements such as $-I$ in the group $G$, what boundary cases will invalidate the conclusions? The text has mentioned some exceptions; could you provide examples of whether these exceptions would occur in real-world networks?\n\n2. More illustrative examples would be beneficial. Could the authors further elaborate on which specific mathematical functions might be lost if parameter symmetries are removed indiscriminately?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vA34o42a99", "forum": "ptfu9Pr3dk", "replyto": "ptfu9Pr3dk", "signatures": ["ICLR.cc/2026/Conference/Submission2973/Reviewer_nW9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2973/Reviewer_nW9d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899832775, "cdate": 1761899832775, "tmdate": 1762916472467, "mdate": 1762916472467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}