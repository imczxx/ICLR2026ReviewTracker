{"id": "U3k7qLgGN8", "number": 17975, "cdate": 1758282601909, "mdate": 1759897141571, "content": {"title": "A Novel Benchmark Framework for Neural Embeddings in Earth Observation", "abstract": "We introduce a novel benchmark framework for evaluating (lossy) neural compression and representation learning in the context of Earth Observation (EO). Our approach builds on fixed-size embeddings that act as compact, task-agnostic representations applicable to a broad range of downstream tasks. Our benchmark comprises three core components: (i) an evaluation pipeline built around reusable embeddings, (ii) a new challenge mode with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii) a scoring system that balances accuracy and stability. To support reproducibility, we release a curated multispectral, multitemporal EO dataset. We present initial results from a public challenge at a workshop and conduct ablations with state-of-the-art foundation models. Our benchmark provides a first step towards community-driven, standardized evaluation of neural embeddings for EO and beyond.", "tldr": "a novel benchmarking pipeline for spatio-temporal neural embeddings", "keywords": ["neural embeddings", "benchmark framework", "spatio-temporal data", "Earth observation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f8a0dbbbba887f26ebe221eb3413e9fe0df0e2b.pdf", "supplementary_material": "/attachment/eeafbad48c334c8fc4a1e62ed9c2f3b8b4b33b04.zip"}, "replies": [{"content": {"summary": {"value": "SUMMARY: This paper introduces a new benchmark Benchmarks specifically aimed at EO data embeddings. The benchmark is simple by design and includes both regression and classification tasks that have to be solved given an embedding input. The authors outline how they test their benchmark in a challenge, report results and learning and outline efforts for expanding the benchmarking and community building efforts; details here are omitted, presumably to maintain the anonymity of the submission. The authors nonetheless test a few baseline methods on their benchmarks and discuss outcomes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This is a timely and relevant paper. I especially appreciate the following aspects:\n\n- Benchmarks specifically aimed at EO data embeddings are desperately needed. That basically every new geo embedding model / paper evaluates on different tasks (Alpha Earth most recently) emphasizes this. Geo embedding benchmarks are also not directly comparable to vision benchmarks in the remote sensing domain, which often focus on fine tuning and specialized architectures, rather than linear probing.\n \n- The paper is very simple - and that is a strength. It is easy to follow and understand. Same goes for the benchmark itself. Very simple, no big compute needed; extremely accessible.\n\n- The projects focus on community building and the clear outlines of expanding this work are great to see!"}, "weaknesses": {"value": "- Given that it is such a simple benchmark, it would have been cool to see comparisons to more GeoFM based embeddings, e.g. Clay, Prithvi or AEF. \n\n- Lack of implicit neural representations as a way to obtain geo embeddings; they are initialized by location, not image, but could still be tested using this framework (I appreciate that this might be beyond the scope of the paper) and should definitely be present in the related work section.\n\n- Spatial coverage is, as unfortunately with pretty much all EO benchmarks, favoring Western countries.\n\n- I understand that given that this is an anonymous submission not more details can be revealed about the challenge, but what are the authors plans for changing this in a potential camera-ready version? How would the challenge outcomes be presented then?\n\n- Why is it necessary to constrain embeddings to exactly 1024 dimensions? Wouldn't relaxing this constraint allow for a more direct comparison of different methods?"}, "questions": {"value": "Overall this is a super relevant paper. The review is quite short, but that's because I am mostly happy with this paper. I'd ask the authors to consider my questions and concerns in the \"weaknesses\" section, but overall, this is already a clear accept to me. We need more benchmarks on this specific topic and conducted in such a community and access-centric way at ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BJMggN9IKd", "forum": "U3k7qLgGN8", "replyto": "U3k7qLgGN8", "signatures": ["ICLR.cc/2026/Conference/Submission17975/Reviewer_vNTR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17975/Reviewer_vNTR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521758402, "cdate": 1761521758402, "tmdate": 1762927767346, "mdate": 1762927767346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a comprehensive benchmarking framework for earth observation embeddings. The framework specifically aims at evaluating embeddings, foregoing end-to-end finetuning and requiring no access to models used to produce embeddings. The benchmarking procedure also explicitly introduces embedding-dimensionality as a dimension for comparison. The datasets used consist of 5 kinds of tasks with moderate sample sizes (1,000<N<10,000), split between binary classification and regression. The benchmarking framework has been tested in a real-world setting and integrates with at least one established submission platform. \nOverall this work presents a solid foundation for EO embedding evaluation but should be expanded to include more evaluation tasks and dataset, and, less urgently, more diverse evaluation settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The fundamentals of this submission are sound. The included tasks and scoring procedure are well-motivated, as is the overall work, with current model comparison often not considering embedding dimensionality and how that affects task performance and viability of downstream implementation of the models in any real data pipelines. \n- The fact that this framework has already been successfully explored in a practical setting, including integration with an established submission platform, is encouraging and definitely a strength of this work. \n- The results on MLP evaluation versus linear evaluation at least somewhat justify the minimal evaluation models. \n- The experiments assessing the framework are overall well done and support the framework in its current state."}, "weaknesses": {"value": "- The authors claim in the Introduction the benchmark tasks include “novel EO downstream tasks” but it is unclear what this refers to? Are the datasets novel? Certainly the tasks are based on foundational EO problems. \n- The authors repeatedly claim that an issue with embedding models that goes underexplored is the fact that the embeddings they produce exceed the dimensionality of the actual input data, causing issues of data transfer and efficient pipelining. While this is technically true, in practice for ViTs, often only the CLS token is used for downstream tasks while convolutional embeddings are usually pooled into just one spatial dimension. There is still merit to this work and encouraging low embedding dimensionality, but it is unclear how much of a real limitation this is in practice. \n- The tasks evaluated seem lacking. It should be straightforward to include at least the subset of the copernicus bench (arXiv:2503.11849) datasets that have the required CC-BY 4.0 license. The authors note this requirement in the Future Work section but without any such effort the benchmark is severely lacking in maturity and ultimately disconnected from central pieces of the EO literature. \n- The description of the procedure for equalizing embedding size between the models seems somewhat arbitrary and is hard to understand. Maybe this can be justified further or at least described more clearly. \n- Other work shows that evaluating with shallow MLPs can yield different performance estimates from simple linear models. The justification for not including this based on the fact that the MLPs might “compensate for weak embeddings” is inadequate as they have no additional information beyond what is extracted by the embedding model. Especially as more datasets and tasks get included, broadening the evaluation to include at least some non-linear models is critical."}, "questions": {"value": "The scoring is fine but seems sensitive in early stages of the benchmark, when std estimates can fluctuate widely from the impact of a single submission. Have the authors explored any alternative scores to supplement the early phases of releasing this benchmark or simply adding more common embedding methods (even just ImageNet pretrained models or general vision foundation models) to solidify the std estimates. Also, will old scores be updated as new submissions are received and scored?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SeeZt0Go2H", "forum": "U3k7qLgGN8", "replyto": "U3k7qLgGN8", "signatures": ["ICLR.cc/2026/Conference/Submission17975/Reviewer_uuFe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17975/Reviewer_uuFe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970997467, "cdate": 1761970997467, "tmdate": 1762927766981, "mdate": 1762927766981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a benchmark for remote sensing embeddings — i.e., the output features of remote sensing foundation models. It appears to be largely a write-up of challenge results from a competition run at the CVPR EarthVision workshop.\n\nThe benchmark covers five tasks and six comparison models, which is narrower in scope than recent related efforts such as Pangea or GeoBench. Section 3 provides a clear description of how to set up a dataset challenge and evaluation framework. However, the task selection reveals a strong geographic bias towards the US and Europe (Fig. 2), and the taxonomy of tasks is not clearly motivated in terms of coverage of the remote sensing problem space. For example, there is no marine/water domain task (e.g., marine litter like MADOS), and tasks such as “Landcover Agriculture” and “Crops” seem thematically overlapping. Task difficulty is also not discussed: e.g., crop type classification can be a simple binary setting (“soy vs corn”) or a highly fine-grained setting with hundreds of visually similar classes (e.g. EuroCrops).\n\nThe results are also not particularly conclusive. The analysis of embedding size vs. performance is potentially interesting, but the underlying mechanisms are not investigated. For example: how correlated are the embedding dimensions (e.g., how many principal components capture most of the variance)? How does the curse of dimensionality play into these observations? What semantic / spatial / temporal patterns do these embeddings capture?\n\nThis paper clearly captures a large amount of work, but, I believe, for ICLR, a stronger contribution would require going beyond ranking models on a challenge, and instead probing and explaining the structure of these embeddings. As written, this reads more like a data challenge report suited to a domain workshop than a venue focused on advancing ML understanding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The underlying challenge presents extensive work summarizing the results of a benchmarking challenge.\n* The description of the benchmarking framework provides a good example how to implement a challenge and outline its main take-aways."}, "weaknesses": {"value": "* Narrow scope compared to existing benchmarks (only 5 tasks / 6 models).\n* Strong geographic bias in dataset tasks (mostly US / Europe).\n* Task taxonomy and coverage not well justified; missing important domains (e.g. marine) and overlapping task definitions."}, "questions": {"value": "* How relevant is the \"compression\" aspect to the models and results of this benchmark? How would an embedding from a classic compression algorithm Discrete Cosine Transform (DCT) to create non-deep features for comparison?\n* One of the main conclusions was that larger embedding sizes degrade performance, but what are here the underlying factors? The curse of dimensionality, the underlying low dimensional manifold? What are the scaling laws here? It still seems that Terramind is improving with larger embedding sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AY8mcsKOfa", "forum": "U3k7qLgGN8", "replyto": "U3k7qLgGN8", "signatures": ["ICLR.cc/2026/Conference/Submission17975/Reviewer_744k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17975/Reviewer_744k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17975/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023329493, "cdate": 1762023329493, "tmdate": 1762927766520, "mdate": 1762927766520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}