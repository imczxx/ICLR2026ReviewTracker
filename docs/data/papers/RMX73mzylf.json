{"id": "RMX73mzylf", "number": 17189, "cdate": 1758273241730, "mdate": 1759897191678, "content": {"title": "Reward Models are Metrics in a Trench Coat", "abstract": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. \nReward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation.\nOur position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.", "tldr": "This paper argues that reward models and evaluation metrics are functionally equivalent, but have with separate research communities and best practices. We outline ways in which a closer collaboration can address shared pitfalls.", "keywords": ["LLMs", "Generation", "Reward Model", "Evaluation", "Evaluation Metrics"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/433f58bfdb3e151dac7ee7387af7abd16e3a0940.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper discusses reward models and evaluation metrics, advocating that the communities developing these respective methods could learn more from each other. This position is supported by (1) citation analysis showing the lack of citations between the two communities, (2) empirical analysis showing how existing evaluation metrics can outperform reward models on benchmarks used to evaluate reward models, highlighting how prior work has been ignored."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written, and provides a nice survey of related work. This could be useful, especially for newer folks in these fields.\n* While I had thought that the similarities between evaluation metrics and reward models were common knowledge, the paper's citation analysis and empirical analysis support their key claims that these respective communities could benefit from greater knowledge sharing."}, "weaknesses": {"value": "* Given that this is primarily a position paper, the value of the contribution is rather subjective. But I thought the main claims were well argued, and there is likely an audience at ICLR that would find this paper interesting.\n* Some differences, while covered to some degree in section 5, could be expanded on with respect to the different usages of reward models and evaluation metrics, and therefore the different demands on their accuracy and robustness. In particular, reward models operate in a potentially more adversarial setting with respect to reward hacking, and therefore demands on robustness are potentially greater. Additionally, an evaluation metric may be useful if it provides sufficient correlation with human judgements aggregated over an entire eval. However, as reward models need to provide directional signal for individual prompts, the accuracy demands are potentially higher. Might be nice to have some discussion of this and if this could cause divergence in methods.\n* Super nit: inconsistent capitalization/casing of paragraph headers."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "by6xeN8ka8", "forum": "RMX73mzylf", "replyto": "RMX73mzylf", "signatures": ["ICLR.cc/2026/Conference/Submission17189/Reviewer_TGxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17189/Reviewer_TGxA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857239301, "cdate": 1761857239301, "tmdate": 1762927166846, "mdate": 1762927166846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is a position paper that attempts to tie together two concepts - reward models and model evaluation (LLM-as-a-judge). Authors point out that these two fields rarely talk and could benefit from learning from each other.\n\nAuthors support their ideas with a cross-citation study and some cross-over experiments"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper ties together two important concepts (a) reward models and (b) model-based evaluation using something like LLM-as-a-judge\n2. The paper does a good job of representing recent work in both fields."}, "weaknesses": {"value": "1. I have a problem with RLHF being mentioned as a reinforcement learning method. RLHF is not really RL, since learning doesnt happen in a dynamic environment with feedback\n2. In the 1st paragraph of the paper, the authors mention that scaling laws for RLHF exist. However, very few convincing works exist in this direction\n3. The motivation for the paper in the 1st paragraph is quite weak. RLHF requires good reward models, so reward modeling is necessary. This could have been the simple motivation to start the paper, no need to talk about scaling laws.\n4. Table 1 is missing variance / error bars, making results untrustworthy. Machine translation evaluation is supposed to be notoriously difficult to pin down without repeat runs.\n5. I dont get what I can really take away from this paper. If the experiments in Tables 1 and 2 were a lot more rigorous and comprehensive, then the claims in the paper would have some solid data. However, the current set of experiments are insufficient."}, "questions": {"value": "How reliable is Figure 1? How did the authors create it? Were similar terms taken into consideration?\nCan you consider running a broader set of experiments for cross-over?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6MrSX24WK", "forum": "RMX73mzylf", "replyto": "RMX73mzylf", "signatures": ["ICLR.cc/2026/Conference/Submission17189/Reviewer_izC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17189/Reviewer_izC9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977183952, "cdate": 1761977183952, "tmdate": 1762927166562, "mdate": 1762927166562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This position paper argues that reward models (RMs)—particularly those used in RLHF and often implemented as LLM-as-a-Judge—are essentially evaluation metrics in disguise. The author supports this claim through a citation analysis showing limited cross-pollination between the RM and metric communities, and presents two empirical experiments suggesting that specialized metrics can outperform general-purpose RMs on narrow tasks (e.g., machine translation quality or summarization factuality)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper raises an important and timely observation: the conceptual and methodological overlap between reward modeling and automatic evaluation is underappreciated.\n- The citation analysis is compelling and highlights a real fragmentation in the literature.\n- The call for shared benchmarks, meta-evaluation practices, and terminology alignment is well-motivated and could benefit both communities."}, "weaknesses": {"value": "see questions"}, "questions": {"value": "While I largely agree with the paper’s core observation—that RMs and metrics share the same goal of approximating human judgment—I believe the paper underestimates a fundamental architectural and functional distinction between the two.\n\nTraditional evaluation metrics (e.g., BLEURT, COMET, QAFactEval) are domain- or task-specialized: they are designed, trained, and validated for a specific generation task (e.g., MT, summarization) and often rely on reference texts, fine-grained rubrics, or narrow definitions of quality (e.g., faithfulness, fluency). This specialization allows them to achieve high correlation with human judgments *within their scope*—as the paper correctly demonstrates.\n\nIn contrast, reward models are inherently general-purpose by design. From their inception in RLHF (e.g., Stiennon et al., 2020; Ouyang et al., 2022), RMs were meant to operate across a vast, open-ended space of user intents, domains, and safety constraints—from coding and math to dialogue, instruction-following, and refusal behavior. They must balance helpfulness, harmlessness, truthfulness, style, verbosity, and more, often without reference outputs.\n\nThis generality is not a bug but a feature required by the RLHF pipeline. One cannot simply replace a scalar RM with a collection of specialized metrics in practice, because:\n1. Engineering complexity: Integrating dozens of heterogeneous, reference-dependent, domain-specific metrics into a unified reward signal for policy gradient updates is nontrivial and brittle.\n2. Differential vulnerability to hacking: Each metric has its own failure modes (e.g., BLEU favors repetition, NLI-based metrics are sensitive to paraphrasing). An RL agent could exploit inconsistencies across metrics or over-optimize one at the expense of others.\n3. Scalability bottleneck: The very promise of RLHF is to scale alignment via automated preference signals. Reverting to a patchwork of handcrafted metrics undermines this vision and impedes end-to-end training at scale.\n\nThus, while I agree that specialized metrics can—and should—inform RM design (e.g., via rubric-based scoring, calibration-aware training, or diagnostic datasets), I disagree with the implication that RMs are “just metrics” that can be swapped out for existing ones in real-world alignment pipelines. Rather, reward models represent a *generalization* of the metric paradigm: they are *systemic, reference-free, multi-dimensional, and deployment-aware evaluators*—a necessary evolution for aligning general-purpose LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YrM2EyIC1B", "forum": "RMX73mzylf", "replyto": "RMX73mzylf", "signatures": ["ICLR.cc/2026/Conference/Submission17189/Reviewer_UcTw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17189/Reviewer_UcTw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060358821, "cdate": 1762060358821, "tmdate": 1762927166216, "mdate": 1762927166216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper raises an interesting discussion on the reward models in reinforcement learning with human feedback. Given that the reward models are the human preference proxies for diverse domains, such as good summaries and safe conversations, the paper revisits systematic metrics that are not modeled as neural models. Interestingly, applying such metrics to human preference benchmarks shows that those metrics are not in a distinct field, in fact, they are on par with reward models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper revisits the overlooked overlap between evaluation metrics and reward models, addressing the RLHF community’s tendency toward self-referential benchmarking, which is a timely perspective in the era of RLHF and post-training.\n2. The extensive survey across both reward modeling and evaluation metrics provides a valuable resource for researchers and well supports the context of the paper.\n3. The illustrative experiments effectively demonstrate the claim that specialized metrics can outperform large reward models on targeted tasks."}, "weaknesses": {"value": "While the paper makes a compelling conceptual argument, several aspects could be strengthened for more rigor and completeness:\n\n- **Reward models as human proxies for RLHF training**: The argument could be more convincing if it examined how the proposed equivalence between metrics and reward models propagates to downstream RLHF outcomes. Reward models’ true purpose is to serve as accessible proxies for human preferences during RL training, not merely as static evaluators. Demonstrating whether a metric-based proxy actually induces comparable policy behavior (e.g., in translation or summarization) would make the thesis empirically more robust, in addition to analyses like Table 1.\n- **Credibility of the citation analysis**: Figure 2 is presented as the central quantitative justification for Section 3, claiming the field separation between evaluation metrics, reward models, and LLM-as-a-Judge. However, it is questionable if the keyword filtering is sufficient for capturing the conceptual overlaps between the fields. For instance, papers on LLM-as-a-Judge often use “metrics” as a keyword for how they measure performance, not as an evaluation metric [1]. Given that this figure grounds the central claim of disciplinary isolation, such methodological looseness could overstate the degree of separation between the two fields.\n\n&nbsp;\n\n**References**\n\n[1] Zheng et al., 2023, “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” (NeurIPS 2023)"}, "questions": {"value": "- Reward models and evaluation metrics may both approximate human preference functions, but they differ in their use within training loops. What behavioral differences would you expect if we used each as the training signal for identical RLHF objectives? Either logical analysis or empirical demonstrations would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D4ZOHVsFht", "forum": "RMX73mzylf", "replyto": "RMX73mzylf", "signatures": ["ICLR.cc/2026/Conference/Submission17189/Reviewer_YigS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17189/Reviewer_YigS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17189/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762364305391, "cdate": 1762364305391, "tmdate": 1762927165783, "mdate": 1762927165783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}