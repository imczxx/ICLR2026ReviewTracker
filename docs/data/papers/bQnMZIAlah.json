{"id": "bQnMZIAlah", "number": 16933, "cdate": 1758270388859, "mdate": 1759897209226, "content": {"title": "Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMuls in DNNs", "abstract": "Modern AI relies on huge matrix multiplications (MatMuls), whose computational cost poses a scalability problem for inference and training. We propose an alternative, *GPU native* bilinear operator to MatMuls in neural networks, which offers a three-way tradeoff between: speed, accuracy and parameter count. In particular, this operator requires substantially *fewer* FLOPs to evaluate ($\\ll n^3$), yet *increases* the parameter count compared to MatMul ($\\gg n^2$). We call this operator *Strassen-Tile* (STL).  \n\nThe key idea behind STL is a local **learnable** change-of-basis, applied on weight and activation *tiles*, followed by an *element-wise* product between the tiles, implemented simultaneously via MatMul. The key question we study is how to optimize the change-of-basis of a given layer, which is a highly non-convex problem. We show that theory-backed initializations of STL (inspired by fast matrix and polynomial multiplication) lead to substantially better accuracy than random SGD initialization, which is explained by the increased parameters of STL. This phenomenon motivates further algorithmic study of STL optimization in DNNs.\n\nOur experiments demonstrate that STL can approximate 4x4 MatMul while reducing FLOPs by a factor of $2.66$, and can **improve** Imagenet-1K accuracy of SoTA T2T-ViT-7 (4.3M parameters) while lowering FLOPs. Even with non-CUDA optimized `PyTorch` code, STL achieves wall-clock speedups in the compute-bound regime. These results, together with its theoretical grounds, suggest STL as a promising building block for scalable and cost-efficient AI.", "tldr": "New GPU-native bilinear operator to replace linear layers in NNs, which is both cheaper and more expressive.", "keywords": ["inference speedup", "bilinear operators", "non-convex optimization"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/737d39c4643a9ea1ebc98957fb6970d83c090a0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a GPU-friendly formulation of Strassen’s Normal Form (SNF), focusing on (i) effective Tensor Core utilization and (ii) preserving the full generality of matrix multiplication. To avoid the original SNF’s high computational cost, the Strassen-Tile (STL) operator is introduced to encode and decode vectorized matrix tiles. This tiling approach reduces the number of computations by a factor of $t/c$ and enables additional learnable parameters. Furthermore, the memory-bound Hadamard product is reformulated as compute-bound matrix multiplications. Although learning STL parameters from existing matrices is non-trivial, T2TViT models trained from scratch with STL operators match or exceed the ImageNet accuracy of the original model. In addition, an unoptimized PyTorch implementation of the STL operator already outperforms standard GEMM in matrix multiplication speed."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "I recommend acceptance of this paper. It demonstrates that FLOPs and GPU time can be reduced even while increasing the number of parameters, suggesting a promising new direction for efficient matrix multiplication research.\n\n1. From a theoretical standpoint, STL introduces an interesting framework that maps tiled matrices into different bases, enabling theoretical computational cost reduction without compromising expressiveness. This is fundamentally different from prior work on weight compression, which generally reduces FLOPs by lowering the number of parameters—often at the cost of reduced expressiveness. \n2. From a practical perspective, the paper implements the STL matrix multiplication algorithm in a GPU-efficient manner, taking memory I/O overhead into account and restructuring memory-bound operations that hinder GPU utilization. These hardware-aware design choices contribute to the observed speedup, even without hardware-specific GPU kernel optimization.\n3. The paper is well-situated within the literature, and the motivation, background, and method are clearly presented. The experiments are thoughtfully designed and provide sufficient insight and empirical evidence to support the proposed approach.\n\nAlthough there are some caveats noted in the Weaknesses section, they do not outweigh the theoretical and practical merits of this work. Overall, this is a strong paper."}, "weaknesses": {"value": "1. **Limited applicability as a post-training method.** \nThe proposed approach is applicable only when models are trained from scratch with STL parameterization. This constraint limits the impact of the work, as training large-scale neural networks often takes substantial time and resources. While recovering STL parameters through naïve L2 loss minimization appears difficult due to the highly non-convex nature of the problem, the contribution would be more impactful if there existed a practical way to derive STL parameters from a pre-trained model—potentially through light fine-tuning or knowledge distillation—so that STL could accelerate existing large-scale (e.g., billion-parameter) models.\n2. **Lack of end-to-end latency evaluation.** \nThe paper only reports synthetic matrix-level runtime benchmarks. Including end-to-end inference speed comparisons among STL, 2:4 sparsity, and dense baselines would significantly strengthen the empirical validation.\n3. **Missing guidelines for choosing $t$ and $r$.** The STL framework introduces two new hyperparameters: the tile size $t$ and the encoded dimension $r$. Since $t = 4$ is fixed across all experiments, it remains unclear how $t$ should be selected and whether it affects model accuracy. Providing guidance or empirical sensitivity analyses for both $t$ and $r$ would improve the clarity and practical usability of the method."}, "questions": {"value": "1. Is it feasible to fine-tune STL parameters starting from a rough initialization derived from a pre-trained model? For instance, one could first obtain an initial estimate by minimizing\n$\\|\\|XW - \\mathsf{STL}(X, \\tilde{W}, D, E_X)\\|\\|_F^2$ via gradient descent, where $X$ is an input feature, $W$ is the original pre-trained weight, and $\\tilde{W}, D, E_X$are the STL parameters. The Class-0 experiment suggests that such an initialization may incur a relatively large approximation error. However, what would happen if the STL parameters obtained through this L2 minimization were subsequently fine-tuned? Would the fine-tuning process be stable and effective, and could it recover most of the original model performance?\n2. What is the end-to-end inference speedup of a full network when all weight matrices are replaced with STL? Reporting the network-level latency (not only matrix-level benchmarks) would help quantify the practical benefits of STL.\n3. Is there a rule-of-thumb for selecting the encoded dimension $r$ and the tile size $t$? Since a key goal is to maximize Tensor Core utilization, it would be helpful to clarify how $t^2$ and $r$ should align with hardware-supported matrix sizes. Additional guidance or empirical insight on selecting these hyperparameters would improve the practical usability of STL.\n\nMinor comments:\n\n1. In Section 4.1, what does “smart initialization” refer to? A brief description or reference would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eu55146wYx", "forum": "bQnMZIAlah", "replyto": "bQnMZIAlah", "signatures": ["ICLR.cc/2026/Conference/Submission16933/Reviewer_FcuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16933/Reviewer_FcuE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789096141, "cdate": 1761789096141, "tmdate": 1762926956849, "mdate": 1762926956849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Strassen-Tile (STL), a GPU-native bilinear operator that replaces standard matrix multiplications with a learnable, tile-based change-of-basis to reduce FLOPs while maintaining accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Novel operator design. The proposed Strassen-Tile (STL) operator introduces a creative reformulation of matrix multiplication using learnable local change-of-basis and element-wise operations, which is conceptually interesting and well-motivated by classical fast-multiplication theory.\n\n+ Theoretical grounding. The idea of leveraging theory-backed initialization inspired by fast polynomial and matrix multiplication provides an insightful link between numerical analysis and deep learning optimization."}, "weaknesses": {"value": "- Unclear motivation. The motivation for introducing STL is not well articulated. While the introduction raises an important scalability issue, it is unclear why STL can effectively address this problem, how the proposed operator was derived, or why it is expected to be GPU-efficient.\n\n- Outdated and insufficient baselines. The experimental comparison is limited and uses relatively old baselines. Given the claim that STL is a general-purpose operator, evaluations on modern large-scale models, such as large language models (LLMs), would be necessary to validate its generality.\n\n- Incomplete experimental evaluation. The experiments mainly report FLOPs, errors, and accuracy metrics, but lack comprehensive analyses of actual runtime performance across diverse models and tasks. Such results are essential to substantiate the claimed efficiency benefits.\n\n- Weak empirical validation and unclear practicality. The experiments focus on small tile sizes and limited scenarios, yielding only modest FLOP and accuracy improvements that may not generalize to larger-scale applications. Moreover, the implementation is \"non-CUDA optimized,\" which weakens the claim of GPU efficiency. The paper does not clarify how STL integrates with existing GPU kernels or frameworks, making its practical benefits speculative without hardware-aware benchmarking."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "595f1E4M4t", "forum": "bQnMZIAlah", "replyto": "bQnMZIAlah", "signatures": ["ICLR.cc/2026/Conference/Submission16933/Reviewer_dytt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16933/Reviewer_dytt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897403469, "cdate": 1761897403469, "tmdate": 1762926956381, "mdate": 1762926956381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Strassen-Tile (STL), a novel bilinear operator intended to replace traditional matrix multiplication (MATMUL) in deep neural networks. STL aims to offer a three-way trade-off between speed, accuracy, and parameter count by performing local learnable changes-of-basis on tiles of weight and activation matrices, followed by element-wise multiplications."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The key idea, local learnable basis changes per tile, is fresh and well-motivated by classical fast matrix multiplication (Strassen) and approximate tensor decompositions. Unlike most compression or pruning techniques, STL keeps the parameter count high or increases it, which is counter-intuitive but compelling."}, "weaknesses": {"value": "1. It remains unclear which part of STL contributes most to improvements: change-of-basis, tile size, or parameter count increase (“fake encoding”).\n\n2. The paper would benefit from an explicit ablation table showing effects of: tile size t (4 vs 8 vs 16), rank r variation on speed vs accuracy, fixed vs learned encoders/decoders.\n\n3. Reported speedups come from non-CUDA optimized PyTorch, so the claimed 2× improvement may not translate to real TensorCore kernels.\n\n4. The tile size is too small for general and common neural networks.\n\n5. How STL could coexist with attention kernels in modern GPUs."}, "questions": {"value": "Can you briefly explain how you choose the tile size."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4erOnyhRLt", "forum": "bQnMZIAlah", "replyto": "bQnMZIAlah", "signatures": ["ICLR.cc/2026/Conference/Submission16933/Reviewer_Rwd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16933/Reviewer_Rwd2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16933/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918382821, "cdate": 1761918382821, "tmdate": 1762926953318, "mdate": 1762926953318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}