{"id": "wnTnwWKu72", "number": 8070, "cdate": 1758058093013, "mdate": 1763486134584, "content": {"title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding", "abstract": "Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their *language prior* (LP)---memorized textual patterns from pre-training while under-utilizing visual evidence.\nPrior analyses of LP mostly rely on input–output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. \nOur analysis reveals a universal phenomenon: each model exhibits a *Visual Integration Point* (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding.\nBuilding on this observation, we introduce the *Total Visual Integration* (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model–dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs.", "tldr": "A formal framework for understanding and quantifying the language prior in LVLMs by contrasting the chain-of-embedding between visual and blind contexts.", "keywords": ["large vision-language models", "multimodality", "language prior"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19f728001e8cce82ccb851ce8fd2ddef4b607f22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper begins with reporting their observation of Visual Integration Point (VIP), the layer where visual input starts to affect hidden representations. Based on this observation, the paper proposes Total Visual Integration (TVI), a metric quantifying visual grounding (and, inversely, language-prior strength). TVI is distinct from prior approaches that rely on output comparisons using carefully curated datasets. Across nine LVLMs and six benchmark datasets, the authors find that VIP typically emerges at a similar depth of the decoder (agnostic to models) and that higher TVI correlates with better accuracy on vision-dependent samples. The authors also show that TVI outperforms previously proposed metrics like visual attention and output divergence in predicting model correctness on visually grounded tasks. Finally, they argue that TVI provides a more direct and interpretable measure of an LVLM’s visual grounding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper goes beyond conventional output level evaluations of language priors by examining layer-level hidden representations. It provides a new and interpretable perspective on how LVLMs integrate visual information and develop visual grounding.\n- The newly proposed metric, TVI, offers a more robust and empirically and theoretically grounded way to quantify visual grounding (or, inversely, language prior strength) than prior metrics such as visual attention or output divergence. It directly measures the degree of vision’s influence on internal representations.\n- The approach is easy to implement (requiring only paired forward passes with and without images) while yielding illuminating insights into when LVLMs begin to use visual input.\n- The study evaluates nine LVLMs across six benchmarks, demonstrating that the observed VIP and TVI behaviors are overall consistent and generalizable across different architectures, dataset types, and model scales."}, "weaknesses": {"value": "- The identification of VIP relies on dataset-level statistics and a tau threshold derived from pre-VIP variance, which introduces potential dependence on dataset composition (eg, the ratio of visually-dependent to -independent samples). The paper does not analyze how sensitive VIP detection is to this choice or whether tau threshold could be defined in a dataset-agnostic manner.\n\n- Most of the 9 evaluated LVLMs use variations of late-fusion (adapter-based) architectures, with the exception of Gemma. This architectural bias may partly explain why the detected VIP consistently appears around the middle or later layers. The paper would be stronger if it included models with more diverse fusion architectures (eg, Flamingo-style, Q-Former, or Perceiver-based) to test whether or not the VIP phenomenon generalizes beyond late-fusion setups.\n\n- It would also be valuable to test larger models. While API models cannot be easily analyzed, several powerful open-source LVLMs (e.g., 70B-scale models) could be evaluated to examine whether the observed trends in VIP and TVI scale to higher capacity models.\n\n- In Table 2, TVI clearly outperforms other metrics, with correlation values ranging from 0.57 to 0.71. However, this still leaves a notable fraction of cases where TVI fails to capture visual grounding. An analysis of these failure patterns or outlier samples would have provided deeper insight into when TVI succeeds or breaks down, further strengthening the validity of the TVI metric.\n\n- [Minor] While VIP and TVI are conceptually interesting, the paper does not clearly articulate how these metrics could guide future LVLM development. Including concrete examples of potential applications would make the work more actionable.\n\n- [Minor] Although the paper emphasizes the number of evaluated models and benchmarks, these are not well-documented, even in the appendices. I recommend adding summary tables for both datasets and models. For models, include details such as fusion strategy and number of attention layers."}, "questions": {"value": "See the above weaknesses.\n\nBesides, while all evaluated models are relatively small, they still span a range of parameter scales (2B to 27B). Do the authors observe any trend in VIP location or TVI magnitude with respect to model size? It may be difficult to compare across different architectures, but evaluating models of varying sizes within the same family in future work could help clarify this relationship."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j09BguS9sI", "forum": "wnTnwWKu72", "replyto": "wnTnwWKu72", "signatures": ["ICLR.cc/2026/Conference/Submission8070/Reviewer_mW7M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8070/Reviewer_mW7M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884175969, "cdate": 1761884175969, "tmdate": 1762920059390, "mdate": 1762920059390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "Dear Reviewers, AC, SAC, and PC,\n\nWe sincerely appreciate the time and effort you have devoted to reviewing our manuscript.\n\nIn this paper, we propose a formal framework to understand and quantify the language prior of LVLMs through the lens of their internal states. We are grateful for the reviewers' recognition of the novelty and soundness of our approach, as well as the comprehensiveness and rigor of our experimental validation. For convenience, we summarize below the key strengths highlighted by the reviewers, followed by the corresponding revisions we have made in response to the concerns:\n\n---\n\n### Key strengths noted by the reviewers:\n\n- S1: _Novel and insightful representation-level analysis framework_ by contrasting chain-of-embedding, offering a new perspective on language priors. (All reviewers)\n- S2: _Simple yet effective metrics_ that are easy to implement and _theoretically grounded_. (R2, R3, R4)\n- S3: _Extensive and rigorous experimental validation_ across 9 LVLMs and 6 benchmarks (54 settings). (All reviewers)\n- S4: _Goes beyond traditional input–output probing_ and fills a key methodological gap in LP research by analyzing internal layer-wise dynamics. (R1, R3, R4)\n- S5: Provides _intuitive mechanistic insights_ into when and how vision influences LVLM behavior (e.g., discovery of VIP). (All reviewers)\n- S6 _Strong correlation of TVI with visually grounded correctness_, outperforming prior LP-related metrics. (All reviewers)\n\n---\n\n### Key revisions in response to the comments:\n\n\nWe have carefully revised the manuscript, and the main updates are summarized below:\n\n- [R1, R2, R3, R4]: Added **further discussion on the pratical utility of TVI** and provided a concrete example demonstrating how TVI can be used to guide the development of better LVLMs (`Section 5`).\n- [R2, R4]: Specified the **algorithmic details for automatic VIP detection** and provided corresponding experimental results (`Appendix B`).\n- [R4]: Extended experiments to **models with more diverse architectures and larger scales** (`Appendix D`).\n- [R1, R2, R4]: Added **additional analyses** to further deepen the understanding of our framework, including different aggregation strategies for TVI computation, instruction-level perturbation, image–text vs. image-only chain comparison, VIP and TVI evolution across training stages, interventional validation of TVI, and case studies on TVI failure cases (`Section 4` & `Appendix E`).\n- [R1, R2]: Improved phrasing, clarified definitions and motivations, and addressed several presentation issues (`Section 3 & 4`).\n\nAll the revised content is highlighted in red.\n\n(* We refer to Reviewer zCDF as R1, Reviewer 2B1a as R2, Reviewer PjBx as R3, and Reviewer mW7M as R4)"}}, "id": "kiNjwZ8m4O", "forum": "wnTnwWKu72", "replyto": "wnTnwWKu72", "signatures": ["ICLR.cc/2026/Conference/Submission8070/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8070/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8070/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763486227371, "cdate": 1763486227371, "tmdate": 1763489745489, "mdate": 1763489745489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a chain-of-embedding (CoE) analysis that examines layer-wise hidden representations of LVLMs. By comparing embeddings produced with and without visual inputs across model layers, they identify a consistent Visual Integration Point (VIP) — the layer where visual information begins to meaningfully affect model behavior. Building on this, they define a quantitative measure called Total Visual Integration (TVI), which aggregates representation differences after the VIP to estimate how strongly the model integrates visual signals. A low TVI indicates a stronger language prior (i.e., text dominates), while a high TVI reflects effective multimodal grounding. The key findings are as follows: Every tested LVLM (9 models, 6 benchmarks) exhibits a distinct VIP layer where visual information starts influencing predictions. TVI strongly correlates with model correctness on visual reasoning tasks, outperforming traditional metrics like visual attention or output divergence. Theoretical analysis connects representation distance to KL divergence between visual and text-only distributions, offering an interpretable, information-theoretic view of LP.."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces chain-of-embedding analysis, a new internal probing method that captures layer-wise representation dynamics between visual and textual modalities — a perspective largely missing in prior language prior studies.\n2. he discovery of the Visual Integration Point (VIP) provides an intuitive and mechanistic explanation of when LVLMs start to rely on vision, enhancing understanding beyond input–output probing.\n3. The proposed Total Visual Integration metric offers a principled, continuous measure of multimodal reliance, showing strong correlation with model performance across tasks.\n4. The authors test 9 LVLMs across 6 benchmarks (54 settings total), showing strong consistency and robustness of the findings.\nTheoretical grounding: The work connects empirical findings to information-theoretic interpretations (via KL divergence), lending mathematical rigor to an otherwise empirical domain."}, "weaknesses": {"value": "1. While the framework effectively analyzes where and how LP arises, it does not propose concrete strategies to reduce language prior or improve multimodal reasoning.\n2. The method requires access to intermediate activations and model internals, making it impractical for closed-source LVLMs or large-scale commercial APIs.\n3. While broad in scope, the work is heavily diagnostic, with limited discussion of causal factors or practical interventions to improve LVLMs."}, "questions": {"value": "Questions are mentioned in the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5pbkCnxVpX", "forum": "wnTnwWKu72", "replyto": "wnTnwWKu72", "signatures": ["ICLR.cc/2026/Conference/Submission8070/Reviewer_PjBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8070/Reviewer_PjBx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932662122, "cdate": 1761932662122, "tmdate": 1762920058950, "mdate": 1762920058950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Vision-Language Models (VLMs) often rely overly on language priors. This paper examines this issue from a representation-level perspective by contrasting the internal embeddings of models with and without visual input. The authors propose two diagnostic metrics. The Visual Integration Point (VIP) identifies the layer where visual features begin to significantly influence textual representations. The Total Visual Integration (TVI) quantifies the overall magnitude of visual influence across layers. Experiments on multiple VLMs and benchmarks show that VIP positions are consistent within each model, while TVI correlates with both visual grounding strength and the dominance of language priors. The paper further analyzes the choice of distance metrics, the effect of model scale, and provides a theoretical justification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The key idea, contrasting the internal embeddings with and without image input, is simple yet effective. It provides a direct way to study how visual information propagates within VLMs, offering a view of cross-modal interaction that complements existing output-based analyses.\n2. The experiments produce consistent results across models and datasets, supporting the reliability of both proposed metrics. The findings are both insightful and well-evidenced through multiple analyses and visualizations.\n3. The experiments cover a broad set of VLMs and benchmarks, and include examinations of distance metrics, model scale, and theoretical interpretation. This completeness strengthens the empirical grounding of the paper."}, "weaknesses": {"value": "1. It states that \"a higher TVI indicates that visual information exerts a stronger and more persistent influence throughout the later layers\". However, the current TVI definition measures the mean visual effect after the integration point, which can overestimate models with brief but strong fusion peaks. Because it ignores the layer-wise distribution of visual influence, a single large spike can inflate TVI even if vision effects fade quickly. A refined version, such as a weighted or variance-penalized TVI, would better reflect sustained multimodal reasoning by emphasizing persistence and uniformity of visual influence.\n2. Some statements, such as the claim that VIP corresponds to the onset of \"effective problem-solving\", are not fully supported by the evidence. Since the paper does not evaluate task performance at the layer level, VIP should be interpreted as marking representational change rather than functional reasoning. A more cautious phrasing is needed to improve accuracy and credibility.\n3. The current analysis focuses primarily on the language-prior effect, contrasting text–image input with text-only input. A more complete triple-wise  comparison, including text-only, image-only and text-image pairs, would allow a deeper understanding of how each modality contributes individually and jointly to the representation flow, offering a fuller picture of multimodal integration.\n4. While the work provides a valuable diagnostic for analyzing language priors, it stops short of offering guidance or methods to reduce such bias. Without follow-up analysis or strategies for mitigation, the practical utility of VIP and TVI remains limited to interpretation rather than its application."}, "questions": {"value": "1. The paper introduces a threshold to determine the VIP but provides no clear method for how it is chosen or tuned for different models. It is stated that VIP is data independent but not model-dependent.\n2. Although the anonymous code is provided, the shared link currently returns \"The requested file is not found\". \n3. The use of the term \"ablation\" appears inaccurate. For example, the ablation on model scale is more of a sensitivity analysis, as it varies model size instead of removing the components. The authors might want to recap the research methodology."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DvYtI7bNn3", "forum": "wnTnwWKu72", "replyto": "wnTnwWKu72", "signatures": ["ICLR.cc/2026/Conference/Submission8070/Reviewer_2B1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8070/Reviewer_2B1a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989293552, "cdate": 1761989293552, "tmdate": 1762920058462, "mdate": 1762920058462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework to analyze the internal mechanisms of MLLMs, specifically their over-reliance on language priors instead of visual evidence. The authors propose a \"chain-of-embedding\" analysis, which contrasts layer-wise hidden states from vision-text inputs with those from text-only inputs. This analysis reveals two key findings. First, all models exhibit a Visual Integration Point, a specific layer where visual information begins to meaningfully influence representations. Second, the paper introduces the Total Visual Integration estimator, which aggregates representation distances after the VIP to quantify the strength of visual integration per sample. A high TVI indicates effective visual grounding, while a low TVI suggests a strong LP reliance. This framework was consistently validated across 54 model-dataset combinations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Analysis of Language Prior. The paper proposes a novel framework for analyzing Language Prior in LVLMs. Instead of relying on traditional input-output probing , it introduces a fine-grained, white-box methodology that examines the internal, layer-wise \"chain-of-embedding\". This provides a new and insightful perspective on how and when models integrate visual information.\n2. Sufficient Experimental Validation. The authors demonstrate the robustness of their claims through extensive experiments. The existence of the Visual Integration Point (VIP) is not an isolated finding; it is consistently observed across 9 different LVLMs and 6 distinct benchmarks, totaling 54 experimental settings. This large-scale validation strongly supports the claim that the VIP is a universal phenomenon in these architectures."}, "weaknesses": {"value": "1. Lack of Clarity in Methodology. The main body of the paper omits critical methodological details, hindering clarity and reproducibility.\n\t- Undefined Symbols in Formulas. In Section 3.1, Equation (2) introduces the notations $P_{star}$ and $D_{star}$ without explicitly defining what the wildcard $star$ represents. While context implies it refers to the $P_{VT}$ and $P_T$ distributions defined earlier, this omission makes the formulation imprecise.\n\t- Ambiguous Distance Calculation. The paper's core distance metric, $d(z_{vis}^l, z_{blind}^l)$, is underspecified in the main text. Both $Z^l_{vis}$ and $Z^l_{blind}$ are sequences of token embeddings, not single vectors. The crucial methodological detail—that the distance is only computed using the hidden state of the last token—is only mentioned in the appendix. This choice is non-trivial, as it impacts the interpretation of the results, and should be explicitly defined and justified in Section 3.1.\n2. Lack of Interventional Validation for TVI. The paper's validation for the Total Visual Integration metric is purely diagnostic. The authors demonstrate that TVI correlates strongly with task correctness on vision-dependent benchmarks . However, this is not sufficient proof of its reliability as a proxy for Language Prior. A much stronger validation would be an interventional experiment. The authors mention existing methods for mitigating LP (e.g., contrastive decoding, attention reallocation) in their related work . They should have applied these mitigation techniques and then measured the TVI score. If TVI is a reliable metric, it should significantly increase (reflecting lower LP) when these mitigation methods are active."}, "questions": {"value": "1. Can this solution be used for data filtering to screen for sample pairs that are strongly visually relevant?\n\n2. Looking at Figures 2 and 3, the difference in representation distance after VIP is only on the order of 0.001. Is this difference not significant enough?\n\n3. Why only use the last token hidden states for calculation? Can all embeddings be included?\n\n4. Is the VIP layer different at different model training stages? What would the trend be?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbjNtHRIO5", "forum": "wnTnwWKu72", "replyto": "wnTnwWKu72", "signatures": ["ICLR.cc/2026/Conference/Submission8070/Reviewer_zCDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8070/Reviewer_zCDF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8070/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762255219564, "cdate": 1762255219564, "tmdate": 1762920057935, "mdate": 1762920057935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}