{"id": "HQgXW3rCkl", "number": 2605, "cdate": 1757162458334, "mdate": 1759898138046, "content": {"title": "Variational Graph Structure Learning for GNNs by Using Marginal Likelihood", "abstract": "Learning graph structures for Graph Neural Networks (GNNs) can improve their performance, but it involves a challenging search over the large discrete space of all possible graphs. Prior works often enforce fixed constraints on the graph structure to induce properties such as sparsity, but such rigidity can be overly restrictive and harm performance. Here, we propose a simpler alternative to use the marginal likelihood which naturally favors such properties. We show that a variational formulation with Laplace's method automatically leads to a marginal likelihood based objective over discrete graph structures, which can be optimized efficiently using the Gumbel-Softmax trick. We call this approach the Laplace Approximation-based Graph Structure (LAGS) method, and show empirically that it improves the performance of different base GNNs, including recent state-of-the-art GNNs that outperform graph transformers.", "tldr": "", "keywords": ["Graph Structure Learning", "Graph Neural Networks", "Marginal Likelihood", "Laplace Approximation"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ecab5c3f70266a39ffd4b71cfc2fecc10e1476b.pdf", "supplementary_material": "/attachment/1be77ba0d03ccd23420cc477dee99286f2704394.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Laplace Approximation-based Graph Structure method to address the challenge of learning optimal graph structures for GNNs. The proposed model leverages marginal likelihood as an objective. It uses a variational formulation with Laplace’s method to derive a marginal likelihood objective over discrete graphs, optimized via the Gumbel-Softmax trick. Empirically, it improves performance across base GNNs (GCN, GraphSAGE)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using marginal likelihood avoids rigid, task-specific constraints, enabling automatic regularization that aligns with GNN inductive biases.\n\n2. The variational formulation with Laplace’s method provides a principled link between marginal likelihood and graph structure learning.\n\n3. Ablations show marginal likelihood correlates with generalization and edge importance, providing actionable insights into learned graph quality."}, "weaknesses": {"value": "1. Calculating and inverting the Hessian (even with approximations) increases computational cost. No comparisons of running time were found. For example, on the ogbn dataset, the running time difference between adding and not adding LAGS, and the runtime of other baseline methods.\n\n2. Scalability depends on kNN/observed graph priors, which may exclude potentially optimal edges outside predefined candidates. Whether this part has defects is not supported by quantitative experimental results.\n\n3. Gains on highly homophilic graphs (e.g., Pubmed) are minimal (~0.3%), suggesting limited utility for a large portion of well-structured data.\n\n4. Excessive hyperparameters increase the cost of parameter tuning, raising concerns about its practicality. Furthermore, no sensitivity experimental results on these hyperparameters were provided."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6NnvRNyi0w", "forum": "HQgXW3rCkl", "replyto": "HQgXW3rCkl", "signatures": ["ICLR.cc/2026/Conference/Submission2605/Reviewer_QBr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2605/Reviewer_QBr5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896589581, "cdate": 1761896589581, "tmdate": 1762916300135, "mdate": 1762916300135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LAGS (Laplace Approximation-based Graph Structure Learning), a variational framework for learning graph structures in GNNs via marginal likelihood maximization. The key idea is that the marginal likelihood naturally regularizes the learned structure by balancing model fit and complexity. The paper applies Laplace’s approximation and the Gumbel-Softmax trick to enable optimization over discrete graphs, demonstrating consistent performance gains across homophilic and heterophilic datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a conceptually clear and theoretically motivated approach connecting Bayesian marginal likelihood to graph structure learning.\n\nThe use of Laplace approximation to derive a tractable surrogate objective is technically sound and novel.\n\nEmpirical results show consistent improvements over base GNNs."}, "weaknesses": {"value": "The experimental validation is limited in scope relative to the paper’s motivation. Since the paper emphasizes learning from noisy or unreliable graphs, the absence of experiments on synthetic datasets with controlled graph perturbations is a missed opportunity. Such experiments could directly test robustness to varying noise levels and validate the claimed advantage of the method.\n\nScalability remains a concern, while approximations for the Hessian are discussed, the empirical section does not provide clear runtime or memory comparisons against competing methods.\n\nThe empirical gains of stronger GNNs on standard benchmarks, though consistent, are relatively modest (around 1%–2% on many datasets) and may fall within variance bounds."}, "questions": {"value": "How does the proposed method scale with the number of nodes and edges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "saCBGdmaUS", "forum": "HQgXW3rCkl", "replyto": "HQgXW3rCkl", "signatures": ["ICLR.cc/2026/Conference/Submission2605/Reviewer_ZVFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2605/Reviewer_ZVFA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150528363, "cdate": 1762150528363, "tmdate": 1762916299966, "mdate": 1762916299966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Accurately learning graph structures is fundamental, as it enables numerous downstream applications and provides substantial benefits for real-world scenarios."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Accurately learning graph structures is fundamental, as it enables numerous downstream applications and provides substantial benefits for real-world scenarios."}, "weaknesses": {"value": "1. The proposed model is tested only on node classification task, however several types of graph based task exists and are not considered in the paper. This raises a natural question if the approach is limited to node classification and cannot be generalized to other graph based tasks like link prediction, and graph classification?\n2. The result table shows the effect of proposed method, LAGS, on GraphSAGE and GCN. The gain is really low compared to GraphSAGE nullifying the contribution from LAGS. Moreover compared  to the other GSL (Graph Structure Learning)  methods like LDS and SUBLIME methods, there is no visible advantage of using LAGS.\n3. The formatting in Table 1 is not consistent as it does no explain the meaning of bold face as bold face generally indicate the highest score.\n4. Overall the evaluation is weak."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WepSKl0PbX", "forum": "HQgXW3rCkl", "replyto": "HQgXW3rCkl", "signatures": ["ICLR.cc/2026/Conference/Submission2605/Reviewer_pqiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2605/Reviewer_pqiU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317701463, "cdate": 1762317701463, "tmdate": 1762916299774, "mdate": 1762916299774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}