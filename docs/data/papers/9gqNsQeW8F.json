{"id": "9gqNsQeW8F", "number": 353, "cdate": 1756736346370, "mdate": 1759898266009, "content": {"title": "AnTKV: Anchor Token-Aware Ultra-Low-Bit Vector Quantization for KV Cache in Large Language Models", "abstract": "Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models. Nevertheless, minimizing the accuracy degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. While scalar quantization is constrained by 1-bit bound, vector quantization exploits intra-vector correlations and enables sub-bit regimes, making it more suitable for ultra-low-bit quantization. To further mitigate quantization-induced degradation, we reveal that the degradation is highly uneven across tokens in attention quality. To investigate this unevenness, we introduce anchor score to measure each token's sensitivity to quantization. Our analysis and experiments show that preserving a small subset (1\\%) of tokens with the highest Anchor Score significantly mitigates accuracy loss under aggressive quantization. We propose~\\name, a dual-stage framework that leverages anchor token-aware vector quantization to compress the KV cache. It combines offline token-aware centroids learning and online anchor token selection to balance compression and accuracy. To enable efficient deployment, we design an online anchor token selection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale to 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher decoding throughput over the FP16 baseline. Experiments demonstrate that \\name matches or surpasses prior methods at 4-bit, and significantly reduce perplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on Mistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.", "tldr": "", "keywords": ["large language model", "quantization", "kv cache"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f934cc9c073400b7478062031b49b03d8dde7f63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AnTKV, a new framework for ultra-low-bit KV cache quantization for LLMs. The key idea is that different tokens contribute unequally to model accuracy when quantized, and a small subset, the author called anchor tokens, dominate the overall quantization error. To exploit this, AnTKV introduces a dual-stage, token-aware design. First, an offline stage that performs weighted vector quantization using per-token error-propagation factors to learn token-sensitive centroids, and then, an online stage that uses a lightweight Anchor Score metric to identify and protect the most error-sensitive tokens during inference. The method integrates with FlashAttention via a custom GPU kernel for efficient online anchor selection. Through experiments, AnTKV achieves state-of-the-art performance under ultra-low-bit settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a vector-quantization-based approach to achieve sub-bit KV cache compression. While vector quantization itself is not new, extending its capability to operate effectively at one bit per value or less is both technically insightful and novel.\n\n2. The authors provide an efficient system implementation compatible with FlashAttention, demonstrating a coherent integration of algorithmic innovation and system-level optimization.\n\n3. Using anchor tokens to safeguard quantization quality is a well-motivated idea, and it is particularly important to observe how crucial this mechanism becomes when we come to sub-bit quantization."}, "weaknesses": {"value": "1. The notion of “anchor tokens” is intuitively appealing, but the paper lacks quantitative analysis to show how these tokens differ from others, or how they can be reliably identified across prompts and layers. The visualization in Figure 1 is vague and hard to interpret.\n\n2. While the Anchor Score is proposed as a lightweight metric, the paper does not sufficiently justify why it is superior to other token-importance measures used in for example prior token eviction works.\n\n3. The offline token-aware centroid learning stage is briefly described but lacks detail on key hyperparameters, such as the codebook configuration. For example, what's the size of the codebook compared to the KV cache size?\n\n4. In Figure 7, it is strange that using 0.375 bit is worse than 1 bit and only has the same throughput as 4 bit. Why this happens? Does it mean that the proposed method does not improve decoding throughput?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RABFQX9U93", "forum": "9gqNsQeW8F", "replyto": "9gqNsQeW8F", "signatures": ["ICLR.cc/2026/Conference/Submission353/Reviewer_ekvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission353/Reviewer_ekvD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861834416, "cdate": 1761861834416, "tmdate": 1762915500123, "mdate": 1762915500123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AnTKV, a novel approach for ultra-low-bit quantization of Key-Value (KV) caches in LLMs, aiming to significantly reduce memory overhead while preserving model performance. The core innovation lies in the Anchor Token-aware KV quantization framework, which strategically identifies and preserves a small fraction of critical \"anchor tokens\" in full precision (FP16), while aggressively quantizing the rest of the KV cache to very low bit-widths (e.g., 1-bit, 0.75-bit, or even 0.375-bit). This design is motivated by the observation that a few high-impact tokens are crucial for maintaining output quality, and their degradation under extreme quantization disproportionately harms performance. AnTKV introduces an effective Anchor Selection (AnS) mechanism to identify these tokens, enabling stable and accurate inference even under sub-bit quantization regimes where existing methods fail catastrophically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core originality of this paper lies in systematically proposing and validating the critical role of \"anchor tokens\" in KV cache quantization. \n2. The proposed dual-stage AntKV framework (offline token-aware centroid learning + online anchor token selection) is a creative synthesis. It ingeniously combines the compression advantages of Vector Quantization with an importance-based token discrimination strategy, opening a new pathway for ultra-low-bit quantization.\n3. Beyond the algorithm, the authors designed and implemented efficient GPU kernels integrated with FlashAttention, demonstrating the practical deployability and efficiency gains (e.g., 3.5x higher throughput, supporting 840K context length) of their method."}, "weaknesses": {"value": "1. Theoretical Derivation Breakdown, Core Metric Lacks Credibility: The derivation of Anchor Score (AnS) jumps abruptly from the complex upper bound in equations (3)-(4) (involving L₁ norm, diagonal matrices, and attention transformations) to the simplified formula (5), with no intermediate steps or intuitive justification. Citing only \"efficiency,\" it fails to compare AnS against simpler alternatives or rule out \"post-hoc tuning.\" This renders AnS more of an ad-hoc heuristic than a theoretically grounded metric, severely undermining the method's credibility.\n2. The claimed throughput gains (e.g., 3.5×) are misleading due to the complete omission of online AnS computation and FP16 anchor storage overhead. Crucially, the paper fails to address the hardware challenges of mixed-precision KV caches on GPU memory access and kernel scheduling, rendering its \"840K context\" claim potentially invalid in practical deployment.\n3. The claim of being the \"first\" to study sub-bit KV cache quantization is inaccurate. VQ-based methods like CQ already achieve sub-bit compression by increasing sub-vector dimensions. AntKV's true contribution lies in the \"anchor token-aware\" mechanism, not sub-bit quantization itself. The paper inflates its novelty by conflating a methodological increment with an application extension, failing to clearly delineate its boundaries against baselines."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VJ4axeNmIT", "forum": "9gqNsQeW8F", "replyto": "9gqNsQeW8F", "signatures": ["ICLR.cc/2026/Conference/Submission353/Reviewer_z2EB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission353/Reviewer_z2EB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928533458, "cdate": 1761928533458, "tmdate": 1762915499989, "mdate": 1762915499989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AnTKV, a method for compressing the KV cache of large language models (LLMs) to extreme bit-widths. AnTKV introduces:\n\n1. Offline token-aware weighted vector quantization (VQ): KV cache is vector-quantized using centroids generated offline with weighted k-means through calibration set.\n2. Online anchor-token selection (AnS): During inference, an anchor score is computed to identify “anchor tokens” whose keys/values are important. These identified anchor tokens are retained in full precision (FP16), while the rest are quantized.\n\nExperiments show AnTKV achieves competitive perplexity and zero-shot results at extreme compression (sub-bit regime)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. KV cache growth is a major bottleneck for long-context LLM inference, which this paper addresses.\n\n2. The intuition that some tokens are more sensitive to quantization is sound and empirically validated by their AnS analysis.\n\n3. Perplexity and zero-shot results show that the model remains stable at very low bitwidths, outperforming prior methods such as KVQuant, CQ, and KIVI under extreme compression (sub-bit regime).\n\n4. The paper shows detailed analysis of anchor token locality, giving useful insights to how quantization sensitivity can differ for tokens in different positions."}, "weaknesses": {"value": "1. The paper introduces a weighted, token-aware k-means to reflect token importance during offline centroid generation, but no ablation or quantitative evidence is provided to show its benefit. In addition, Since KV statistics are highly context-dependent, the assumption that offline-generated calibration set will generalize well remains weakly supported.\n\n2. AnTKV employs a sliding window to dynamically compute the Anchor Score (AnS) for newly generated tokens. Since anchor tokens are selected only within this local window, the model’s accuracy and stability are likely to differ to the chosen window size. However, the paper does not include ablation or analysis showing how different sliding-window lengths affect performance or runtime efficiency.\n\n3. The reported bits per element neglect the FP16 centroid tables and 1% FP16 anchor retention, which together raise the effective bitwidth from 0.375 b to roughly 0.65 b (for 4 K context, 1% retention). \n\n4. For perplexity/zero-shot tests, the paper states that “quantized KV caches are directly used for attention outputs,” while for LongBench it reverts to FP16 KV in prefill. This setup difference between benchmarks is not clearly noted why.\n\n5. In Figure 6, the KV-cache size differs by only ~3 GB between 1-b and 0.375-b settings (810 K context), and Figure 7 shows lower throughput for 0.375 b. This questions the practical value of sub-bit quantization (<1-bit) given its limited savings, lower performance, and runtime overhead.\n\n6. Many components overlap with prior methods such as KVQuant [1], SKVQ [2], CQ [3], and PQCache [4] (FP16 retention [1], sliding-window decoding [2], and vector/product quantization [3, 4]). The main distinction lies in AnTKV’s of token importance via the AnS metric, which can be viewed as a unification and refinement of existing ideas.\n\n\nReferences:\n\n[1] Hooper, Coleman, et al. \"Kvquant: Towards 10 million context length llm inference with kv cache quantization.\" Advances in Neural Information Processing Systems 37 (2024): 1270-1303.\n\n[2] Duanmu, Haojie, et al. \"Skvq: Sliding-window key and value cache quantization for large language models.\" arXiv preprint arXiv:2405.06219 (2024).\n\n[3] Zhang, Hailin, et al. \"Pqcache: Product quantization-based kvcache for long context llm inference.\" Proceedings of the ACM on Management of Data 3.3 (2025): 1-30.\n\n[4] Zhang, Tianyi, et al. \"Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization.\" Advances in Neural Information Processing Systems 37 (2024): 3304-3331."}, "questions": {"value": "1. How sensitive is the model’s performance to the choice of sliding window size used in online AnS calculation\n\n2. How is anchor-token handled when the sequence length exceeds the model’s context window?\n\n3. How does the offline-generated centroid set affect centroid generalization to different contexts and tasks?\n\n4. How would different centroid generation methods (different clustering methodologies) affect performance?\n\n5. Is the sliding window’s KV cache kept in FP16 before the corresponding AnS is computed and vector quantized? If so, is this additional memory and latency overhead included in the results and analysis?\n\nRefer to the weaknesses for more"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CHdhf7Cn08", "forum": "9gqNsQeW8F", "replyto": "9gqNsQeW8F", "signatures": ["ICLR.cc/2026/Conference/Submission353/Reviewer_wueT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission353/Reviewer_wueT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission353/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286295233, "cdate": 1762286295233, "tmdate": 1762915499883, "mdate": 1762915499883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}