{"id": "ULXYZCms41", "number": 2348, "cdate": 1757061834578, "mdate": 1759898154477, "content": {"title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling", "abstract": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model’s intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view–conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods.", "tldr": "We propose Geometry Forcing, a method that enhances video diffusion models by aligning their latent representations with pretrained geometric features to improve 3D consistency and visual quality in generated videos.", "keywords": ["Generative Model; Video Generation; World Modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cbedfaa72602e1019dc95aa6fbd093465e35556.pdf", "supplementary_material": "/attachment/4fcb51d0cbdeb2b734471f8199ee3e2200dff568.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Geometry Forcing, a REPA-style feature alignment method to enhance the 3D consistency of existing video diffusion models (VDMs). Authors first observe that existing VDMs cannot readout consistent 3D representations (i.e., point maps) from their diffusion features. To address this, prior works either jointly predict the 3D modality, or leverage a structured 3D representation as guidance. Geometry Forcing instead proposes to align the internal features of the VDM with the representation of a 3D foundation model, VGGT. Fine-tuning pre-trained VDMs with such auxiliary loss significantly improves the temporal consistency of the generated videos, demonstrated by both quantitative and user study results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is simple and intuitive. Adding a REPA loss with VGGT does not introduce any sophisticated architecture design, yet it is able to bake the 3D information into the model.\n- The ablation study is very comprehensive, which clearly shows the effectiveness of each module. \n- The experimental results seem strong. Geometry Forcing outperforms baselines in most of the metrics."}, "weaknesses": {"value": "1. As the author pointed out in the Limitation, VGGT is only trained on static scenes, and thus cannot be used to supervise VDM training on dynamic videos. Can authors discuss more on how to extend Geometry Forcing to dynamic videos as required by general text-to-video training?\n2. Have the authors tried other 3D foundation models? For example, MonST3R [1] and CUT3R [2] as they can handle dynamic videos.\n3. Have you tried training a video diffusion model from scratch? REPA shows that using representation alignment loss can greatly speed up convergence. I'm curious if applying both DINO and VGGT loss can further accelerate this. I understand that the computation cost is high, so showing some early training loss curve (with vs without Geometry Forcing loss) is enough (or describe it, if figures are not allowed in rebuttal).\n4. I'd like to see a comparison with a baseline that uses explicit 3D memory, e.g., GEN3C [3] that uses reprojected point clouds as additional conditioning for the VDM. You do not need to apply all its components. I think you can do this:\n- To generate frame `n`, run VGGT on frame `1, 2, ..., n-1` to reconstruct a point cloud of the scene, then render it to 2D images, and condition the VDM on it. This can also work in an autoregressive way.\n\n[1] Zhang, Junyi, et al. \"Monst3r: A simple approach for estimating geometry in the presence of motion.\" arXiv preprint arXiv:2410.03825 (2024).\n\n[2] Wang, Qianqian, et al. \"Continuous 3d perception model with persistent state.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[3] Ren, Xuanchi, et al. \"Gen3c: 3d-informed world-consistent video generation with precise camera control.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "Besides the questions in Weaknesses, here are some minor questions:\n1. Can you explain why VideoREPA achieves a significantly better RVE than Geometry Forcing, while being much worse in other metrics?\n2. How do you implement VideoREPA? Is it doing REPA alignment loss with per-frame DINOv2 features? Then why is its result much worse than the \"DINOv2 Only\" entry in Tab.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hDimBEMAcJ", "forum": "ULXYZCms41", "replyto": "ULXYZCms41", "signatures": ["ICLR.cc/2026/Conference/Submission2348/Reviewer_qFpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2348/Reviewer_qFpu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760508715220, "cdate": 1760508715220, "tmdate": 1762916202573, "mdate": 1762916202573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Geometry Forcing (GF) that enhances the geometric consistency of video diffusion models by aligning their internal representations with those of a 3D foundation model (VGGT). GF introduces Angular Alignment and Scale Alignment. The method is integrated into standard autoregressive video diffusion training without requiring explicit 3D supervision. Experiments on RealEstate10K and Minecraft benchmarks show that GF improves Fréchet Video Distance (FVD), SSIM, and 3D consistency metrics (RPE/RVE) compared to state-of-the-art baselines such as DFoT, REPA, and VideoREPA. Ablation studies confirm the complementary role of Angular and Scale alignment, and user studies indicate perceptually improved scene consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well written, with clear figures illustrating motivation and results.\n\n2. The paper proposes a new concept of “geometry forcing”, transferring geometric awareness into video diffusion models without requiring 3D ground-truth supervision. The dual-objective design (Angular and Scale alignment) is interesting, addressing optimization stability in cross-domain feature matching.\n\n3. Experiments are extensive. Quantitative: Benchmarks include both 16- and 256-frame video generation, using perceptual and geometric metrics. Qualitative: Visual comparisons (360° rotations) convincingly show consistent viewpoint revisiting."}, "weaknesses": {"value": "1. Scale of experiments is modest (16–256 frames, 256×256 resolution). The authors acknowledge this but it limits claims of scalability.\n\n2. While the ablations cover alignment types and layer depths, computational cost (training overhead, memory footprint) is missing. Geometry alignment likely adds feature extraction and projection costs that may be nontrivial.\n\n3. The method relies heavily on VGGT as a teacher. It’s unclear whether GF’s success depends on the specific 3D foundation model or generalizes to others (e.g., DUST3R, FLARE).\n\n4. There is limited discussion of failure cases (e.g., ambiguous depth, reflective surfaces)."}, "questions": {"value": "1. Generality of the 3D teacher. How sensitive is GF to the choice of 3D foundation model? Would a weaker teacher (e.g., DUST3R) still yield benefits, or is VGGT’s strong geometry essential?\n\n2. Does alignment at all layers or at multiple scales (spatial or temporal) offer further improvement, beyond the mid-level layer shown to be best?\n\n3. What is the added training cost (e.g., % increase in FLOPs or wall-clock time) from Geometry Forcing, given the need to compute VGGT features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s3JlhoV03n", "forum": "ULXYZCms41", "replyto": "ULXYZCms41", "signatures": ["ICLR.cc/2026/Conference/Submission2348/Reviewer_J25o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2348/Reviewer_J25o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933490984, "cdate": 1760933490984, "tmdate": 1762916202325, "mdate": 1762916202325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts the REPresentation Alignment (REPA) [1] into 3D domain. Specifically, it employs VGGT [2] as a 3D foundation model and aligns the intermediate features of a video diffusion model with features extracted from layers of VGGT. The authors propose two alignment objectives for this purpose: an angular alignment objective, which enforces cosine similarity between feature maps, and a scale alignment objective, which directly supervises the rescaled features of the diffusion model. Experiments demonstrates that combining these two objectives enhances the video diffusion model's 3D understanding, thereby improving geometric consistency in the generated results.\n\n[1] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think\n\n[2] VGGT: Visual Geometry Grounded Transformer"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The core concept of aligning intermediate features of a generative model with those of a foundation model is a promising research direction that has shown success in other fields.\n* The proposed method is simple yet effective. The experiments demonstrate improvements over the baseline models.\n* The paper is well-written and easy to understand."}, "weaknesses": {"value": "* My primary concern is the paper's limited novelty. The proposed method appears to be a straightforward adaptation of the 2D alignment technique from REPA. The core contribution seems to be replacing the DINO model (used in 2D REPA) with VGGT for the 3D case. As such, the work feels incremental and offers limited new conceptual insights.\n\n* The method's applicability appears to be limited to static scenes, but this is not explicitly stated. The authors should clearly acknowledge that the current approach does not handle dynamic scenes or significant camera motion, which restricts its use to a narrow set of conditions. \n\n*  The performance of the VGGT feature extraction likely depends on the number of views used. I would encourage the authors to provide an ablation study demonstrating how model performance varies with different numbers of views adopted during training time."}, "questions": {"value": "*  The matching process of VGGT seems computationally expensive, especially as the number of views increases, potentially making the training alignment prohibitively slow. To clarify this, could the authors report the concrete training time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkCXjLQrOF", "forum": "ULXYZCms41", "replyto": "ULXYZCms41", "signatures": ["ICLR.cc/2026/Conference/Submission2348/Reviewer_PFvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2348/Reviewer_PFvJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927763668, "cdate": 1761927763668, "tmdate": 1762916202181, "mdate": 1762916202181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Geometry Forcing, a method designed to help video diffusion models better capture the inherent 3D structure of real-world scenes. While standard video diffusion models trained on raw 2D video data often lack geometric understanding, Geometry Forcing addresses this by aligning the model’s internal representations with features from a geometric foundation model. It employs two key alignment strategies: Angular Alignment, which enforces directional consistency through cosine similarity, and Scale Alignment, which preserves scale information via feature regression. Experiments on camera view–conditioned and action-conditioned video generation show that Geometry Forcing significantly enhances both visual quality and 3D consistency, outperforming existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well-organized.\n2. The core idea is simple yet effective — aligning the internal representations of video diffusion models with features from a geometric foundation model.\n3. Experimental results demonstrate strong performance, showing notable improvements in geometric consistency and long-term temporal coherence compared to baseline methods."}, "weaknesses": {"value": "The training objectives of the diffusion model and the VGGT differ fundamentally. The diffusion model is designed to learn noise or velocity in a progressive manner—its target lies in the intermediate denoising process rather than the final outcome. In contrast, VGGT is result-oriented, directly learning to predict the final geometry. Although the experimental results appear promising, theoretically the learning targets are not of the same nature and may even be somewhat conflicting. It remains unclear how the proposed alignment between these two objectives effectively works in practice. Could the authors provide more theoretical or empirical justification for this compatibility?\nThe motivations and formulations of Angular Alignment and Scale Alignment are insufficiently explained. The directional correspondence between the hidden states of the diffusion model and the geometric features, as well as the scale differences across models, are both vague. It would be helpful to clarify how these factors influence the final generation quality.\n\nThe base model description is also unclear. The paper mentions “a U-ViT backbone for video generation,” but does not specify which model this refers to or provide an appropriate citation.\n\nIncluding explicit geometry-based video generation methods as comparison baselines would strengthen the evaluation and provide more persuasive evidence of the proposed method’s effectiveness.\n\nAdditionally, there are citation errors, such as the one noted around line 290.\n\nFinally, the paper lacks qualitative ablation studies that isolate the effects of Angular Alignment and Scale Alignment. Without such analyses, it is difficult to assess the actual contributions of each component to the overall improvement."}, "questions": {"value": "1.The diffusion model and VGGT have different learning objectives — one is progressive (learning noise or velocity), while the other is result-oriented. How can the proposed alignment between these fundamentally different targets work effectively?\n\n\n2.The motivations and mechanisms of Angular Alignment and Scale Alignment are unclear. How exactly do these objectives influence the final video generation quality?\n\n\n3.What specific model does the “U-ViT backbone for video generation” refer to? Could the authors provide more details or a proper citation?\n\n4. Could the authors include explicit geometry-based video generation methods as additional baselines to make the comparison more convincing?\n\n\n5.The paper lacks qualitative ablation studies showing the individual effects of Angular and Scale Alignment. Can the authors provide such qualitative analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uQTGwSpNVB", "forum": "ULXYZCms41", "replyto": "ULXYZCms41", "signatures": ["ICLR.cc/2026/Conference/Submission2348/Reviewer_AqJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2348/Reviewer_AqJj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989290500, "cdate": 1761989290500, "tmdate": 1762916201978, "mdate": 1762916201978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}