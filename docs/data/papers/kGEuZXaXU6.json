{"id": "kGEuZXaXU6", "number": 17998, "cdate": 1758282773688, "mdate": 1763726263302, "content": {"title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities", "abstract": "The increasing autonomy of Large Language Models (LLMs) necessitates a rigorous evaluation of their potential to aid in cyber offense. Existing benchmarks often lack real-world complexity and are thus unable to accurately assess LLMs' cybersecurity capabilities. To address this gap, we introduce PACEbench, a practical AI cyber-exploitation benchmark built on the principles of realistic vulnerability difficulty, environmental complexity, and cyber defenses. Specifically, PACEbench comprises four scenarios spanning single, blended, chained, and defense vulnerability exploitations. To handle these complex challenges, we propose PACEagent, a novel agent that emulates human penetration testers by supporting multi-phase reconnaissance, analysis, and exploitation.\nExtensive experiments with seven frontier LLMs demonstrate that current models struggle with complex cyber scenarios, and none can bypass defenses. These findings suggest that current models do not yet pose a generalized cyber offense threat. Nonetheless, our work provides a robust benchmark to guide the trustworthy development of future models.", "tldr": "PACEbench is a cybersecurity agent benchmark with 32 realistic cyber-exploitation challenges, featuring a spectrum of vulnerability difficulties, environmental complexities, and cyber defenses.", "keywords": ["LLM Agent", "Cybersecurity", "Benchmark", "AI Safety"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34a325cf160d98daa4771568c9cbfeff9d09730a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the first comprehensive benchmark for evaluating the offensive cybersecurity capabilities of LLM-based agents. It introduces PACEbench, a controlled framework that measures how AI agents perform in progressively complex exploit scenarios—from single vulnerabilities to chained and defended ones—reflecting real-world cyber-attack conditions. The authors also develop PACEagent, an autonomous system using the Model Context Protocol (MCP) to orchestrate reconnaissance, analysis, and exploitation tasks. Evaluating seven leading LLMs, the study finds that while some can exploit simple vulnerabilities, none succeed in multi-step or defended settings. This work establishes an important foundation for quantitatively assessing AI-driven cyber risks and guiding responsible development of autonomous security agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is highly original, introducing the first systematic benchmark—PACEbench—for evaluating the offensive cybersecurity capabilities of LLM-driven agents. It combines realistic exploit simulations with structured task progression, offering a novel, measurable framework for assessing AI-driven cyber risks. The quality of the work is excellent, with a rigorous experimental setup spanning multiple vulnerability types, defensive layers, and seven frontier models. The clarity of exposition is strong, aided by well-organized sections, intuitive figures, and precise descriptions of each scenario’s objectives and outcomes. In terms of significance, the paper fills a critical gap in AI safety and security evaluation, providing an essential foundation for future research on the governance, control, and defense of autonomous AI systems."}, "weaknesses": {"value": "While PACEbench is a valuable and well-designed benchmark, it is limited by its controlled and synthetic environments, which may not fully represent the diversity and unpredictability of real-world cyber threats. The study also lacks deeper qualitative analysis of agent reasoning and failure patterns, focusing mainly on quantitative exploit success rates. Moreover, the sample size of models and tools is small, and the framework’s adaptability to evolving architectures or real-time exploit settings remains untested. Finally, the paper could better address dual-use concerns and outline clear guidelines for responsible use of offensive benchmarking frameworks."}, "questions": {"value": "How well would PACEbench generalize to real-world cyber environments beyond controlled synthetic setups, particularly where network dynamics and defense mechanisms evolve continuously?\n\nCan the authors elaborate on how PACEbench could be extended or automated to benchmark multi-agent coordination or continuous attack-defense cycles?\n\nThe study reports aggregate exploit success rates—could the authors provide qualitative insights into failure modes or examples of partial reasoning progress in unsuccessful attacks?\n\nHow adaptable is the framework to new vulnerability types or architectures (e.g., LLM agents integrated with autonomous reasoning or real system access)?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "While the paper follows responsible research practices, its focus on AI-driven cyber exploitation introduces potential dual-use risks. The benchmark, PACEbench, evaluates offensive capabilities of LLM agents, and although intended for safety research, its methods could be repurposed for malicious exploitation if misused. An ethics review would help ensure that appropriate access controls, red-teaming disclosure policies, and release restrictions are in place. The review should involve expertise in cybersecurity, AI safety, and responsible dual-use research governance to confirm that the framework’s publication and use align with ICLR’s ethical standards."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1LmPC8fBmz", "forum": "kGEuZXaXU6", "replyto": "kGEuZXaXU6", "signatures": ["ICLR.cc/2026/Conference/Submission17998/Reviewer_X1ZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17998/Reviewer_X1ZX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760564801063, "cdate": 1760564801063, "tmdate": 1762927793625, "mdate": 1762927793625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark for evaluating autonomous LLM agents in cybersecurity exploitation tasks based on Common Vulnerabilities and Exposures (CVEs). The benchmark is structured into four categories: (A-CVE) single-host single-CVE scenarios, (B-CVE) multi-host blended-CVE scenarios, (C-CVE) chained-CVE scenarios involving interdependent hosts, and (D-CVE) defended-CVE scenarios where targets include security mechanisms. The benchmark includes 17, 7, and 5 tasks for A-, B-, and C-CVE respectively, while the number of D-CVE instances is not clearly specified.\n\nEvaluation is conducted using the proposed PACEAgent framework in a controlled, CTF-style virtual environment. An agent is considered successful if it retrieves the challenge flag within five attempts (pass@5) under a fixed maximum number of iterations. The authors benchmark four closed-source models (Claude-3.7 Sonnet, Gemini 2.5 Flash, GPT-5 Mini, o4-mini) and three open-source models (Deepseek-V3, Deepseek-R1, Qwen3-32B). Results indicate generally low success rates across all models, with all systems failing entirely on the D-CVE category. The paper also reports a correlation analysis between human pass rates and model success, and compares PACEAgent’s performance against an existing agent framework (CAI)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed AI agent benchmark provides an effective means to evaluate the end-to-end cyber-offensive capabilities of LLM-based agents in controlled virtual environments. Its automated, human-free evaluation pipeline improves replicability and consistency of results across experiments.\n2. The four-tier CVE categorization (A-, B-, C-, and D-CVE) effectively stratifies tasks by vulnerability difficulty and environmental complexity, enabling more interpretable performance analysis across models and providing insights into capability scaling with task difficulty.\n3. The authors have provided an anonymous GitHub repository and commit to open-sourcing the benchmark, which will substantially benefit the community by fostering reproducibility, transparency, and standardized evaluation in LLM-based cybersecurity research."}, "weaknesses": {"value": "While this paper can give a positive impact on LLM-based cybersecurity research, there are several unclear and incomplete parts that make the paper seem not yet ready for publication unless the following issues are addressed.\n\n1. **Positioning versus prior CTF-style works:**\n   The paper argues that existing CTF-style benchmarks often operate under an \"assumption of guilt\", but after reading further, this work also adopts a similar CTF-style setup. It would be helpful to clearly explain how the proposed benchmark differs from existing CTF-style agent benchmarks such as Cybench or NYU-CTF in terms of design philosophy, evaluation process, or task objectives.\n\n2. **Lack of dataset and benchmark statistics:**\n   Since this work focuses on datasets and benchmarking, it would be beneficial to include detailed dataset statistics in the main paper, such as the total number of tasks, number of examples per category, vulnerability distributions, host configurations, and defense mechanisms. This information would provide a clearer picture of scale and coverage.\n\n3. **Evaluation metric clarity:**\n   There are several unclear points in the proposed evaluation metric (BenchScore) that might cause confusion or incorrect interpretation. Details on how ( D_{score} ) is measured and the reasoning behind the chosen weighting scheme are missing. Please refer to the questions below for more specific points.\n\n4. **Model coverage limitation:**\n   The evaluation only includes small or non-state-of-the-art models such as Gemini-2.5-Flash, GPT-5-Mini, and O4-mini. This limits insight into how the latest, more capable models would perform. Including at least one current large-scale model would strengthen the evaluation and provide a better understanding of the benchmark difficulty.\n\n5. **Underexplored open-source models:**\n   The analysis of open-source models is limited. In the appendix, it is mentioned that open-source performance is low due to short context window lengths, but there is no quantitative analysis of the total context length required to complete tasks or whether truncation actually caused the failures.\n\n6. **Small benchmark size:**\n   The total number of vulnerability cases, only 32 across all categories, seems relatively small to generalize benchmark performance. For a benchmark paper, a larger and more balanced set of tasks would make the results more meaningful."}, "questions": {"value": "1. **BenchScore metric:**\n\n   * How is the  $D_{score}$  value measured exactly? In the paper, it written as \", ...\"\n   * Why use weighted sums instead of a normalized success rate? The current score can exceed 1.0 when all tasks pass, which might make comparisons unclear.\n   * What is the justification for choosing  $w_A = 0.2$ , $w_B = 0.3$, $w_C = 0.3$ , and $w_D = 0.2$? Why does $w_D$, representing the most difficult scenario, have a smaller weight?\n\n2. **Failure attribution:**\n   How can we ensure that model failures are due to model capability rather than provider-imposed safety guardrails? For example, if an LLM refuses to execute certain exploit commands, how is this case handled or accounted for?\n\n3. **Multi-host scenarios and partial progress:**\n   For the B- and C-CVE tasks involving multiple hosts, where is the flag located? Also, does the current evaluation give any partial credit to models that manage to exploit part of the chain, such as compromising one host but not reaching the final flag?\n\n**Recommendations for metric improvements:**\n\n1. Normalize per category and compute weighted averages within the range ([0,1]):\n   $\\text{Score} = \\sum_c w_c \\cdot \\frac{1}{|c|}\\sum_{i \\in c}\\text{Pass@5}_i, \\quad \\text{with} \\quad \\sum w_c = 1$\n\n2. Report both micro (overall pass rate) and macro (category-weighted) versions.\n\n3. Add reliability metrics such as success@k for ( k=1..5 ) and AUC or mean successes per 5 attempts.\n\n4. Report efficiency statistics such as tokens, runtime, or tool calls per successful task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKyvptF8pQ", "forum": "kGEuZXaXU6", "replyto": "kGEuZXaXU6", "signatures": ["ICLR.cc/2026/Conference/Submission17998/Reviewer_iQw8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17998/Reviewer_iQw8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844497385, "cdate": 1761844497385, "tmdate": 1762927792883, "mdate": 1762927792883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents PACEbench, a new benchmark for assessing the cyber-offense capabilities of LLM-based agents. It proposes four scenario types [A-CVE (single), B-CVE (blended), C-CVE (chained), and D-CVE (defended)] to capture increasing levels of realism and complexity in cyber-exploitation tasks. The authors also introduce PACEagent, a modular agent architecture with reconnaissance, analysis, and exploitation phases. Evaluation over seven state-of-the-art models shows that even top systems (Claude-3.7-Sonnet, Gemini-2.5-Flash, GPT-5-mini) fail to perform in more complex or defended settings, suggesting that current LLMs are far from posing autonomous cyber-offense risks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Well-structured scenario taxonomy** The split into A-, B-, C-, and D-CVE scenarios is a meaningful and elegant design choice. It provides a graded way to measure agent sophistication -progressing from isolated vulnerabilities to multi-host and defense-aware settings. I particularly appreciated how this taxonomy makes capability assessment interpretable.\n\n- **Automatic flag-based verification.** Adapting the CTF-style flag mechanism is a practical way to ensure deterministic and machine-verifiable success criteria, avoiding hallucination or manual scoring.\n\n- **Breadth of model coverage.** Evaluation across seven frontier models (open and proprietary) gives a balanced view of the current landscape and avoids benchmark overfitting.\n\n- **Significant engineering effort.** The construction of realistic, multi-host, multi-vulnerability environments is a major effort. Having built similar CTFs myself, I recognize the difficulty of setting up reproducible and verifiable scenarios at this level of realism."}, "weaknesses": {"value": "- **Unclear contribution of the CTF environments.** There are already numerous, diverse CTF platforms (e.g., HackTheBox, Vulhub, NYU CTF Bench) featuring multi-host, multi-defense, and dynamic challenges. The paper does not convincingly justify what is novel about the PACEbench environments or why they are “agent-specific.” If existing human-targeted CTFs already support automatic flag validation, it’s unclear why a new environment is needed rather than adapting existing ones.\n- **Related-work positioning.** The claim that PACEagent is the only system to consider “multiple CVEs and lateral progress” is inaccurate. PentestGPT (USENIX’24) and other frameworks already model multi-step attack chains. The related-work section cites several of these but does not articulate how PACEagent differs beyond the benchmark integration.\n- **Limited methodological detail.** Many implementation details are missing. For example, the memory module of PACEagent is underspecified. Is it a structured text list (like pentestGPT) or a RAG? How is it formatted and updated?\n- **Agent design limitations.** Line 81 seems to say that the agent executes all reconnaissance before exploitation, rather than interleaving exploration and lateral movement. But this contradicts what is indicated in Figure 3. If does performa ll reon first this would be very problematic and unlike how human pentesters operate and likely constrains realism. If there is some sort of phase manager, I could not find any explanation on how it works.\n- **Shallow analysis of failure cases.** Section 5.2 reports results descriptively (“no models bypass WAFs”) but lacks causal analysis. Which specific submodules or reasoning steps failed? Are limitations due to the LLM’s reasoning capacity, the framework’s orchestration, or the tools’ integration? Deeper diagnostics would make this section more insightful."}, "questions": {"value": "1. How are the proposed PACEbench CTFs unique to agent pentesting compared to existing human CTF infrastructures that already support automated flag validation and complex networked scenarios?\n2. How does your memory and phase manger work? how does PaceAgent compare to PentestGPT?\n3. Have you evaluated any defensive countermeasures against Agent Pentesters, such as the recent USENIX 2025 work: Cloak Honey Trap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ezu2BmZyWy", "forum": "kGEuZXaXU6", "replyto": "kGEuZXaXU6", "signatures": ["ICLR.cc/2026/Conference/Submission17998/Reviewer_NLcC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17998/Reviewer_NLcC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859618381, "cdate": 1761859618381, "tmdate": 1762927792111, "mdate": 1762927792111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PACEBench, a benchmark that includes four scenarios that span single, blended, chained, and defense vulnerability exploitations. They find that current LLMs struggle on these tasks, even when using their new PACEAgent, an agentic framework designed to be better equipped at completing these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a good understanding of current literature, and incorporates critical elements of real-world cyber offense into their benchmark. PACEBench is the first framework that integrates defenses into their dataset, providing an additional layer of difficulty not present in other benchmarks like [MHBench](https://arxiv.org/pdf/2501.16466)."}, "weaknesses": {"value": "This paper makes five main contributions: a benchmark with four distinct variants (single CVE, blended CVE, chained CVE, and defended CVE), and a new agent architecture for completing these challenges. Of the four distinct variants, only defended CVEs are not reasonably represented in existing literature. The benchmark would have been a stronger overall contribution had the authors left the other three variants to existing work, and focused primarily on autonomously defended systems. Additionally, creating a \"novel agent architecture\" for a benchmark where there is not a sufficient comparative study done against other more common agent frameworks is irresponsible. The authors compare between their own agent and CAI, a semi-autonomous framework that does not feature in any previous evaluations of agents on similar tasks, and is largely irrelevant in the literature. This paper (and the PACEAgent contribution) would be stronger had the authors conducted a thorough evaluation of previous agentic setups (CyAgent from Cybench, Incalmo, Codex/Claude Code as used in BountyBench, etc). If, after this evaluation has been done, PACEBench outperforms these agents on this particular dataset, then it could reasonably be considered a contribution. Ultimately, I am not certain that this work contains *enough* distinction from previous work like [Incalmo](https://arxiv.org/pdf/2501.16466) to be accepted."}, "questions": {"value": "1. How many total tasks are present in PACEBench? Of those tasks, how many of those include cyber defenses?\n2. Can you provide more motivation for why you chose to compare to CAI? I find this to be an incredibly strange choice compared to other frameworks that are more commonly used for comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YBKCmZ7pO5", "forum": "kGEuZXaXU6", "replyto": "kGEuZXaXU6", "signatures": ["ICLR.cc/2026/Conference/Submission17998/Reviewer_6or4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17998/Reviewer_6or4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959754376, "cdate": 1761959754376, "tmdate": 1762927791589, "mdate": 1762927791589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}