{"id": "oRVf9Uayf6", "number": 17016, "cdate": 1758271183882, "mdate": 1759897204287, "content": {"title": "Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-form Solutions", "abstract": "Transformers excel at time series modeling through attention mechanisms that capture long-term temporal patterns. However, they assume uniform time intervals and therefore struggle with irregular time series. Neural ODEs effectively handle irregular time series by modeling hidden states as continuously evolving trajectories. ContiFormers (Chen at al., 2024) combine Neural ODEs with Transformers, but inherit the computational bottleneck of the former by using heavy numerical solvers. This bottleneck can be removed by using a closed-form solution for the given dynamical system - but this is known to be intractable in general! We obviate this by replacing Neural ODEs with a novel linear damped harmonic oscillator analogy - which has a known closed-form solution. We model keys and values as damped, driven oscillators and expand the query in a sinusoidal basis up to a suitable number of modes. This analogy naturally captures the query-key coupling that is fundamental to any transformer architecture by modelling attention as a resonance phenomenon. Our closed-form solution eliminates the computational overhead of numerical ODE solvers while preserving expressivity.  We prove that this oscillator-based parameterization maintains the universal approximation property of continuous-time attention; specifically, any discrete attention matrix realizable by ContiFormer's continuous keys can be approximated arbitrarily well by our fixed oscillator modes. Our approach delivers both theoretical guarantees and scalability, achieving state-of-the-art performance on irregular time series benchmarks while being orders of magnitude faster.\nAll our code is available here: LINK", "tldr": "Closed-form solution for Contiformer Using Damped Harmonic Oscillators", "keywords": ["transformers", "closed-form solution", "novel architecture", "irregular time-series"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3c98809fcd4f4260ef822bb513f8e9b63537178.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents OsciFormer, an architecture for irregular time series modeling. The model addresses the computational bottleneck of prior work, specifically ContiFormer, which uses numerical solvers for its Neural ODE components. OsciFormer replaces the general NODE dynamics with a linear damped harmonic oscillator, a system with a known closed-form analytical solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- By replacing the $O(S)$ sequential solver steps with a closed-form calculation, the dominant complexity term is reduced from $O(N^2 S d^2)$ to $O(N^2 J d)$, resulting in significant, verifiable speedups.\n\n- OsciFormer can run on long-context tasks (like the HR benchmark in Table 3) where the baseline ContiFormer fails due to out-of-memory errors, demonstrating a practical engineering advantage.\n\n- The paper provides detailed derivations for its closed-form solution (Appendix A) and a formal proof (Appendix B)  that its linear approximation does not sacrifice the universal approximation capability."}, "weaknesses": {"value": "- This work is an incremental refinement of ContiFormer. The contribution is limited to replacing the Neural ODE $f(\\tau, k_i(\\tau); \\theta_k)$ with a classic linear dynamical system that has a known solution.\n\n- The title \"Oscillators Are All You Need\" is hyperbolic. It is unsuitable for complex non-linear, non-oscillatory, or discontinuous dynamics, a limitation the paper fails to address.\n\n- The comparisons omit critical contemporary baselines for irregular time series, such as variations of Neural CDEs, Stable SDEs, and the recent advances in neural differential equations. \n\n- Notably, it seems that the paper doesn't follow the ICLR 2026 template."}, "questions": {"value": "- Please provide the ablation study that supports the claim. Specifically, show the sensitivity of accuracy, training time, and memory usage to the number of modes $J$ (e.g., $J = 4, 8, 16, 32, 64$).\n\n- How is the query function $q(t)$ approximated? The paper mentions expanding it in a sinusoidal basis but provides no details on this process, the number of modes used, or its sensitivity.\n\n- How does OsciFormer perform on tasks known to be non-linear or non-oscillatory (e.g., datasets with sharp, abrupt step-changes)? The rigid linear damped harmonic oscillator bias seems unsuited for such dynamics.\n\n- What is the justification for including Section 5 on E(3)-equivariance?\n\n- Can you provide any analysis of the learned oscillator parameters ($\\omega$, $\\gamma$)? Without this, the resonance framing is not supported."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VcNkhekW9M", "forum": "oRVf9Uayf6", "replyto": "oRVf9Uayf6", "signatures": ["ICLR.cc/2026/Conference/Submission17016/Reviewer_EAsp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17016/Reviewer_EAsp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761275438, "cdate": 1761761275438, "tmdate": 1762927036980, "mdate": 1762927036980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The original Transformer architecture does not work well with irregular time series due to they assume uniform time steps. While NODES handle irregularity by modeling continuous-time dynamics, they're computationally expensive due to requiring numerical ODE solvers. The paper proposes replacing NODEs with damped harmonic oscillators that have closed-form solutions, eliminating numerical solver overhead while maintaining expressiveness.\n\nThe keys and values are modeled as damped, driven oscillators and the queries are expanded in a sinusoidal basis. The attention is modeled as resonance such that high attention when frequencies align, low when misaligned.\n\nThe paper proved a Universal Approximation Theorem which shows that harmonic oscillators can approximate any continuous attention matrix achievable by ContiFormer\n\nThe paper provides experiments to demonstrate the effectiveness of the method."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposed a simple yet effective way to improve Transformer in handling irregular time series data. The closed form method can greatly reduce the computational complexity. The author also provides a proof to show that there proposed method still maintains expressiveness such that there is a UAP."}, "weaknesses": {"value": "1. Since the ODE used has clear physical meaning, it would be interesting to explain any connection between the attention mechanism with the ODE.\n2. There are certain part that is unclear\n    2.1 Section 3 should be the most important part in this work. But too many contents are seemed to be putted into the appendix, making this part hard to follow. For example, line 142-157 gives some hard to understand terms: \"Averaged attention: decomposition.\" \"Steady-state contribution\", \"Transient contribution\" and then refers to the equations in the derivation in the appendix, which forces the reader to read the entire appendix otherwise will not understand. If possible it would be good to explain these concepts clearly without the need to refer to the appendix.\n\n    2.2 I do no see the significance of section 5 which discuss \"E(3)-EQUIVARIANCE\". This could be useful for certain applications, but I think due to the page limit, if is more important to expand section 3 which is the main contribution of the paper.\n\n    3.3 Section 6 not looks like academic writing. There are many question-based headers and in conversational tone. The presentation of this part could be improved"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ab4GDDnGsI", "forum": "oRVf9Uayf6", "replyto": "oRVf9Uayf6", "signatures": ["ICLR.cc/2026/Conference/Submission17016/Reviewer_Nx2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17016/Reviewer_Nx2H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881883463, "cdate": 1761881883463, "tmdate": 1762927036570, "mdate": 1762927036570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel continuous-time Transformer architecture, OsciFormer, designed to efficiently model irregular time series without the computational bottlenecks of Neural ODE-based methods such as ContiFormer. The key idea is to replace the Neural ODE layers—which require costly numerical solvers—with damped, driven harmonic oscillators that admit closed-form analytical solutions.\n\nin this formulation, keys and values evolve as damped harmonic oscillators, while queries are expanded in a sinusoidal basis, enabling an elegant resonance-based attention mechanism where alignment between query and key frequencies drives attention weights"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The oscillator analogy for attention is creative and intellectually stimulating, merging physical dynamics with deep learning.\n2. The closed-form formulation drastically reduces computational cost, memory usage, and sequential depth.\n3. Strong results across diverse irregular time-series domains, maintaining or exceeding ContiFormer’s accuracy while being much faster."}, "weaknesses": {"value": "1. The universality proof concerns approximating keys and then attention weights; it doesn’t address training dynamics or resilience under irregular sampling noise, or non-smooth signals common in irregular TS.\n2. No systematic ablation on J (modes), γ (damping), frequency grids, or initial-condition maps.\n3. E(3)-eq section is ungrounded. The equivariance construction is mathematically plausible but has no experiments; it reads as a speculative add-on.\n4. The abstract touts “state-of-the-art performance ... orders faster,” but several tables show parity or losses depending on dataset; conclusions should be tempered.\n5. The table 4 is too large and exceeds the page margins."}, "questions": {"value": "1. Do you have fail cases (when does the model diverge or lose accuracy?\n2. No strong ablations. We never see performance vs: number of oscillator modes, γ (damping), frequency grids, or initial-condition maps.\n3. Any training-time results toward stability/identifiability (e.g., constraints on eigenvalues or damping priors) beyond the key-approximation theorem?\n4. The harmonic approximation theorem is asymptotic: given enough oscillators, we can approximate any continuous trajectory up to ϵ, which then bounds the softmax deviation in attention, but there’s no guarantee that SGD will find those oscillator parameters from data.\n5. Can you actually visualize “attention weight vs frequency alignment” on a real example?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rqjPC5NjKt", "forum": "oRVf9Uayf6", "replyto": "oRVf9Uayf6", "signatures": ["ICLR.cc/2026/Conference/Submission17016/Reviewer_UpK7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17016/Reviewer_UpK7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915667838, "cdate": 1761915667838, "tmdate": 1762927036319, "mdate": 1762927036319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OsciFormer, a continuous-time transformer that replaces Neural ODE–based dynamics in ContiFormer with analytically solvable damped harmonic oscillators. By modeling attention as a resonance phenomenon between query and key oscillations, OsciFormer provides a closed-form solution eliminating the computational overhead of ODE solvers while retaining theoretical expressivity. The authors prove a universal approximation theorem for continuous attention, establish E(3)-equivariance for geometric data, and show strong empirical results across irregular time-series benchmarks, achieving comparable or superior accuracy with up to 20× faster training compared to ODE-based methods."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The use of damped harmonic oscillators as a way to explain attention is a cool and intuitive idea—it helps make the concept more accessible.\n- By switching to a closed-form solution, the model cuts down on computation and memory usage big time.\n- The theoretical proof showing that this method can still approximate the continuous attention mechanism is solid and convincing.\n- The experiments are thorough and show that the model works well in practice, not just theory."}, "weaknesses": {"value": "- Some of the math-heavy sections, like the attention integrals, feel a bit rushed and could use clearer explanations.\n- The paper compares accuracy and efficiency but doesn't dive into how the oscillator settings (like the damping or number of modes) actually impact performance.\n- The connection between the oscillators and the learned attention patterns could be explained better."}, "questions": {"value": "- Can you explain more clearly how the resonance behavior actually influences the learned attention in a more visual or intuitive way?\n- How much does changing the number of oscillator modes or the damping coefficients affect performance? It would be helpful to see an ablation study.\n- Do you think this model could work well for other tasks like NLP or computer vision? It would be interesting to hear your thoughts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hax1SBWt4U", "forum": "oRVf9Uayf6", "replyto": "oRVf9Uayf6", "signatures": ["ICLR.cc/2026/Conference/Submission17016/Reviewer_mLHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17016/Reviewer_mLHL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17016/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967291521, "cdate": 1761967291521, "tmdate": 1762927035988, "mdate": 1762927035988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}