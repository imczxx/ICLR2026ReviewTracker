{"id": "GfVKK5sKit", "number": 22578, "cdate": 1758333039216, "mdate": 1759896858393, "content": {"title": "LLMs Struggle to Balance Reasoning and World Knowledge in Causal Narrative Understanding", "abstract": "The ability to robustly identify causal relationships is essential for autonomous decision-making and adaptation to novel scenarios. However, accurately inferring causal structure requires integrating both world knowledge and abstract logical reasoning. In this work, we investigate the interaction between these two capabilities through the representative task of causal reasoning over narratives. Through controlled synthetic, semi-synthetic and real-world experiments, we find that state-of-the-art large language models (LLMs) often rely on superficial heuristics—for example, inferring causality from event order or recalling memorized world knowledge without attending to context. Furthermore, we show that simple reformulations of the task can elicit more robust reasoning behavior. Our evaluation spans a range of causal structures, from linear chains to complex graphs involving colliders and forks. These findings uncover systematic patterns in how LLMs perform causal reasoning and lay the groundwork for developing methods that better align LLM behavior with principled causal inference.", "tldr": "In this paper, we examine the failure Modes of LLMs for causal reasoning on narratives and the unreliable shortcuts LLMs take to make causal inferences.", "keywords": ["Causal Inference", "Large Language Models", "Reasoning", "Narratives"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f831abd33074138e2c16e007fbe22f3b55ec0bcb.pdf", "supplementary_material": "/attachment/bc04136f532b8cb4d21b3fafddf7721600bb8b68.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates causal reasoning over narratives in LLMs by probing two shortcuts: 1) ordering prior (events mentioned earlier are treated as causes) and 2) parametric/world-knowledge prior (typicality from pretraining). Using synthetic chains, synthetic general causal graphs, semi-synthetic (CauseNet chains verbalized by an LLM), and real-world (CauseNet sentences) narratives, the authors show that a) reverse order and atypical relations (vs the model’s prior) significantly degrade performance in causal narrative understanding and b) longer and more complex graphs (with forks/colliders) moderately degrade accuracy. They also report that extracting an estimated causal graph $G'$ and answering from the graph alone helps, but the benefit vanishes when the narrative is reintroduced."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the crucial issue of LLMs' causal reasoning ability, a topic of significant interest.\n- There is clear factorization of failure modes (ordering vs typicality)\n- Breadth across synthetic chains, synthetic general causal graphs, semi-synthetic chains, and real chains.\n- A consistent empirical trend is uncovered: Reversing the ordering hurts, atypical/counter-intuitive chains confuse models.\n- Graph-only prompting consistenly mitigates most of these biases. This could provide a useful prompting insight: explicit $G'$ extraction reduces shortcutting when used alone."}, "weaknesses": {"value": "- Metrics & aggregation left implicit. Accuracy = agreement with $G$ and Consistency = agreement with $G'$ are briefly stated but the paper lacks explicit formulas, units of aggregation, seed handling, and CI recipe in the main text.\n- Thin human validation; none beyond synthetic. Only 50/2500 synthetic narratives were audited, no inter-annotation agreement is reported, no audits were performed for semi-synthetic and real datasets, even though LLMs were used to modify them. This severely limits confidence in the labels.\n- Narrative vs graph interaction under-specified. Graph-only helps; Narr+Graph removes most of the Graph-only gain under reverse ordering. The fusion protocol isn't ablated (position of $G'$, format, instruction, relative length study). The mitigation claim (extract only $G'$) is fragile.\n- Complex-graph sampling is described in appendix only. There needs to be a brief summary of the fork/collider samplign and the chain-connect step in the main text.\n- Anti-Causal accuracy clustering around $\\approx 0.1-0.2$ for a binary task is alarming and warrants further investigation."}, "questions": {"value": "- L148: Synthetic generation (3.1, Setting): How are events linked into $G$, exactly? How did you ensure generated narratives do not contradict pre-existing LLM knowledge, a confounding failure mode that you identified in later sections?\n- L151-154: 50/2500 checks is too small. What were the selection criteria? Are there plans to extend the correctness/validity checks of the labels beyond the synthetic case?\n- Accuracy/Consistency formulas and CIs: Please state the exact formulas, unit of aggregation, seed handling, and CI method.\n- $G'$ quality evaluation: What are $G'$ edge precision/recall/F1, GED($G$, $G'$)?\n- Narr+Graph fusion diagnostics: Did you verify the model actually uses $G'$ efficiently under Narr+Graph? Please provide position swaps (graph before or after narrative), instruction variants (i.e., \"prioritize the graph\", vs neutral), format variants.\n- L245–254 This paragraph is very confusing as to which results it is discussing. Point to the exact figure/panel and report numerical deltas.\n- L261–263 / L345 / L355: Were these datasets examined? What audit criteria were used for semi-synthetic and real narratives (fluency, faithfulness to $G$, label correctness)?\n- Are there automatic sanity checks (and broader human audits) to catch cause/effect inversions when generating or stitching narratives?\n- The semi-synthetic and real world cases seem quite similar, intuitively. One would expect very similar performance. Why, then, are there differences in Figure 4?\n\nMinor:\n- L197 / L310 “randomly”: Specify distributions or use more precise wording.\n- Figure 2 (right) contains complex graphs, including forks and colliders, yet their \"chain size\" is computed. How is this calculated? Graph size usually denotes the number of edges, but I believe you are calculating the graph order here. Please clarify."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SwsdxH5RMn", "forum": "GfVKK5sKit", "replyto": "GfVKK5sKit", "signatures": ["ICLR.cc/2026/Conference/Submission22578/Reviewer_BDoT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22578/Reviewer_BDoT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761003128194, "cdate": 1761003128194, "tmdate": 1762942287653, "mdate": 1762942287653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how LLMs balance reasoning and world knowledge when performing causal inference over narratives. The authors claim that LLMs often rely on superficial heuristics/shortcuts such as memory of world knowledge from training rather than true casusal reasoning. The paper also evaluates a range of causal scenarios to further understand how LLMs handle causal tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental setup to separate reasoning difficulty from world knowledge conflict is novel\n2. The paper is well written and easy to read\n3. The evaluation of the proposal is thorough"}, "weaknesses": {"value": "1. More discussion on how the better performance is due to the proposed setup rather than \"just\" better prompting structure would help the paper\n2. A discussion on the concern about using LLMs to add synthetic data to judge causal performance would be helpful as well"}, "questions": {"value": "1. Following up on the above, is there a chance that we are attributing the success to better prompt structure rather than true LLM reasoning capabilities?\n2. Could causal reasoning improve if models were trained explicitly on counterfactual data?\n3. What would manipulating event order and world knowledge conflicts together look like?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ntto3aQ2dv", "forum": "GfVKK5sKit", "replyto": "GfVKK5sKit", "signatures": ["ICLR.cc/2026/Conference/Submission22578/Reviewer_QWBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22578/Reviewer_QWBG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545958177, "cdate": 1761545958177, "tmdate": 1762942287332, "mdate": 1762942287332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to assess the *causal narrative understanding* ability of LLMs. The authors constructed various controlled experiments from synthetic, semi-synthetic, and real-world causal graphs and imbue them with short narratives by an LLM. They measured the LLMs' performance under various prompting strategies on the binary classification task of \"whether event \\$E_1\\$ causes event \\$E_2\\$ according to the given narrative\", with the difficulty varying on three separate axes: event ordering, graph complexity, and parametric consistency. \n\nThe main observations presented by the experiments are: LLMs suffer from the issue of *cognitive inertia*, prone to assuming that the events coming before are the cause of the events coming after; LLMs also like to take reasoning shortcuts, assuming that the causal relationships presented in the narrative would agree with its internal knowledge; LLMs perform worse when the causality graph grows complex, though not affected as much as the two former issues."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper discusses the interesting topic of complex causal understanding, and the results are curious. It seems that all the tested LLMs suffer from the defects of *cognitive inertia* and *taking reasoning shortcuts*. Future work on LLM reasoning could benefit from these observations.\n* The experiments are well-designed and thorough, covering various possible fail-modes of complex causal reasoning by LLMs.\n* The ideas in the paper are presented well and easy to follow."}, "weaknesses": {"value": "* A concern on narrative generation: Pair-wise consistency does not necessarily imply global consistency, so it is questionable whether the whole narrative remains consistent in the test cases with more nodes. For example, in the example given in Section 4.1, around Line 358, since the second sentence uses a rather vague references of \"these accidents\" and just \"injury\", if there were a subsequent sentence following it, it may well be on the topic of car injuries instead of continuing on the topic of mining/factory injuries. This would make the logic chain unsound as a whole.\n* Some prompt constructions are dubious:\n  - A.1.8: Only \"yes\" cases are provided in the few-shot prompting text, which may bias the model.\n  - A.1.9, C.2.3: It is impossible to represent forks/joins in complex graphs (A → (B, C) → D) with a single chain. At the end of section 4.4, it is stated that \"this (complex graphs) is one area where extracting an explicit causal graph does not seem to significantly improve performance\" and this defect might be the direct cause.\n* Using the same LLM for problem generation but different ones for solving them might bias the results. For example, if the problems are generated by GPT-4o, it might benefit GPT-4o more than Claude 3.5 when evaluating, since the narrative style or internal knowledge could be more \"familiar\" to the generator itself. It is possible to mitigate this concern by using human-written narratives or by generating the narratives with different models.\n* Presentation nitpick: In Section 3.2, Subsection *\"Explicit Causal Graph...\"*, the second sentence reads wrong. I think you meant \"once $G'$ is extracted by the LLM, *the original narrative* is not given to the LLM again...\""}, "questions": {"value": "1. Some counterfactual test cases might be ill-suited for current LLMs tuned for safety and truthfulness. For example, even if the narrative says \"taking poison prolongs one's lifespan\", the safety measures in the LLMs *could* trigger disagreement with the user. I suspect that some failing cases are caused by this issue.\n2. I would also like to see how the scale of the model influences the results. The leap from a \"tiny\" 8B model straight to GPT-4o seems too large.\n3. The models tested in the paper are all non-thinking models. I am curious about the performance of mainstream thinking models, possibly with varying thinking budgets. For the typical self-doubting and double-checking behavior of the thinking models, they might be able to achieve higher success rate than normal models with CoT prompting, in my intuition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gdK50CBO9C", "forum": "GfVKK5sKit", "replyto": "GfVKK5sKit", "signatures": ["ICLR.cc/2026/Conference/Submission22578/Reviewer_npyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22578/Reviewer_npyf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570042584, "cdate": 1761570042584, "tmdate": 1762942287103, "mdate": 1762942287103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes how large language models assess causality in events described within synthetic, semi-synthetic, and real-world narratives. The authors examine the extent to which LLMs rely on their internal (parametric) knowledge, especially when their pre-existing causal beliefs conflict with information provided in the prompt. Additionally, the study investigates how the size and complexity of narratives and causal chains impact model performance. The authors also experiment with various prompting techniques and different types of causal chains to further understand these effects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written. While presenting multiple experimental settings, the authors succeed in delivering all necessary information and maintaining a coherent and accessible narrative throughout.\n- The experimental setup is particularly compelling. The authors systematically vary the conditions, carefully designing experiments that isolate and test one aspect at a time, which strengthens the validity of their reasoning."}, "weaknesses": {"value": "**[W1]** One of the main limitations of this work is that the primary analysis and results focus almost exclusively on the GPT-4o model. While the authors include results for two additional models in the Appendix, they do not provide any substantive analysis or discussion of these models. Furthermore, not all experiments are conducted on both additional models. As a result, the paper is heavily GPT-4o-centric, which limits the generalizability of the findings and misses the opportunity to provide cross-model insights (such as how model size affects reliance on parametric knowledge).\n\n**[W2]**  The authors state that one of their motivations is to investigate the interaction between world knowledge and abstract logical reasoning, as well as the balance between reasoning and \"the right amount of world knowledge\". However, they do not clearly define what constitutes the \"right amount\" of world knowledge nor do they operationalize this concept in their experiments. It is also unclear how the causal and anti-causal relationships in their study map onto the dimensions of world knowledge and logical reasoning that they aim to explore. This claim would benefit from a clearer motivation and more precise definitions.\n\n**[W3]** The authors claim to evaluate the models' ability to reconstruct a causal graph from a narrative. However, they do not provide quantitative results or metrics for the performance of the causal graph extraction task.\n\n**[W4]** The paper treats anti-causality in a binary fashion, but in reality, anti-causal events can vary in the degree to which they conflict with parametric knowledge. Not all violations of common causal knowledge are equally significant. For example, the absence of a causal link between \"lack of sleep\" and \"happiness\" may be more OOD  and represent a stronger negative causality than the absence of a link between \"lack of sleep\" and \"morning shower\" where the events may simply be unrelated. In the former case, it might be desirable for the model to rely on its parametric knowledge for safety, while in the latter, reliance on the prompt would be preferable. These nuances and potential confounding factors in the data are not explored, which limits the actionability of this experiment's insights."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "neqHAsDEVT", "forum": "GfVKK5sKit", "replyto": "GfVKK5sKit", "signatures": ["ICLR.cc/2026/Conference/Submission22578/Reviewer_C7XZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22578/Reviewer_C7XZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053103705, "cdate": 1762053103705, "tmdate": 1762942286824, "mdate": 1762942286824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}