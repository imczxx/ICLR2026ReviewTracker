{"id": "10gpXxro8z", "number": 19693, "cdate": 1758298445459, "mdate": 1759897025345, "content": {"title": "Evaluating off-the-shell LLMs’  Red-teaming Ability  for Multi-round Jailbreak Attack", "abstract": "Safety evaluation of large language models (LLMs) has emerged as a critical re-\nsearch frontier. To ensure comprehensive evaluation, current practices often in-\nvolve curating task-specific benchmark datasets tailored to distinct application\nscenarios. However, such dataset-centric approaches suffer from two fundamental\nlimitations: poor transferability across domains and temporal obsolescence due\nto the evolving nature of LLMs. To overcome these limitations, an intuitive idea\nis to leverage off-the-shelf LLMs as red teams. Yet, a pivotal question remains\nunder-explored: Can off-the-shelf LLMs conduct autonomous and effective secu-\nrity evaluations without specialized red team training? Motivated by this question,\nwe further raise the bar by focusing on multi-round jailbreaking attacks, which de-\nmand deeper strategic reasoning and intent concealment compared to single-round\nadversarial prompts. Unlike traditional red team evaluation methods for LLMs,\nwhich focus on assessing the robustness and security of these models, Our method\naims to leverage the inherent capabilities of off-the-shelf LLMs to evaluate their\npotential for cross-scenario transfer and iterative evolution over time during red\nteam testing. Specifically, we evaluate the red-teaming capabilities of six off-the-\nshelf LLMs across five major and ten secondary harmful categories. Experimental\nresults indicate that these models exhibit non-trivial proficiency in performing ef-\nfective multi-turn attacks, often employing known jailbreaking techniques such\nas role-playing, indirect prompting, and semantic decomposition. Nevertheless,\nsignificant limitations persist. Based on our findings, we discuss actionable direc-\ntions for enhancing the effectiveness of red-team LLMs, as well as implications\nfor strengthening the robustness of victim models.", "tldr": "", "keywords": ["LLM", "read-teaming", "jailbreak"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1237db317871e60284ced781ba75bde9d9505701.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether off-the-shelf large language models can autonomously conduct multi-round red-teaming tests to uncover safety vulnerabilities in other models. It uses an evaluation framework where attacker models iteratively generate jailbreak prompts against victim models across various harmful content categories. Using automated judging and toxicity scoring, the study assesses the models’ ability to exploit safety weaknesses and analyzes the common techniques they employ. The authors find that existing LLMs possess some inherent red-teaming capability but remain inconsistent, self-restrictive, and inefficient. The paper claims contributions in providing a comprehensive evaluation of such abilities, offering insights for improving automated red-teaming, and suggesting directions for enhancing LLM safety alignment."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Clear experimental framework - A2A setup with CoT and self-reflection is straightforward and well-described\n- Multiple evaluation metrics - Goes beyond success rate to include toxic scores, efficiency measures (AFI/AMI), refusal rates, and technique classification"}, "weaknesses": {"value": "- The paper’s findings show limited novelty. Its main contribution—evaluating off-the-shelf LLMs as autonomous red-teamers for multi-round jailbreak attacks—largely reiterates established insights from prior work on automated red-teaming and multi-turn adversarial prompting. The results (e.g., that some models can perform jailbreaks while others refuse, or that commercial models use role-play and concealment strategies) are expected and descriptive, not conceptually new. The study introduces no new algorithm, dataset, or theoretical framework; it simply repackages routine comparative evaluations with standard metrics such as attack success rate and toxicity score. Overall, the work’s novelty is incremental and empirical rather than methodological or theoretical\n- Overstated claims - calls itself \"first comprehensive evaluation\" without comprehensive coverage or proper literature review to establish \"first\"\n- Insufficient victim model coverage - only 3 victim models, far too few for \"comprehensive evaluation\" claims\n- Cannot reproduce - missing exact prompts, system messages, sampling parameters, and two victim model identities\n- The related work section is severely underdeveloped. It cites only a handful of papers (~6) focused on jailbreak or red-teaming methods, while omitting extensive literature on LLM safety benchmarks, LLM-as-Judge frameworks, and multi-agent red-teaming systems. This narrow coverage fails to contextualize the work within the broader safety and evaluation landscape, leaving the paper’s novelty and positioning unclear.\n-  Statistically insufficient - only 10 samples per condition with no significance testing, confidence intervals, or error analysis"}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "biTP6m2HTZ", "forum": "10gpXxro8z", "replyto": "10gpXxro8z", "signatures": ["ICLR.cc/2026/Conference/Submission19693/Reviewer_mLcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19693/Reviewer_mLcN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207507650, "cdate": 1761207507650, "tmdate": 1762931533405, "mdate": 1762931533405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "How well can LLMs red-team other LLMs in a multi-turn scenario? They evaluate 5 categories Security, Criminal, Harassment, Violence, and Sexual and evaluated Qwen-2.5 7B Instruct, DeepSeek v3 and LLama3 70B."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Pros\n- They ran multi-turn jailbreaks in a simple fashion and evaluated the attacks"}, "weaknesses": {"value": "Cons\n- This part is confusing to me.  You use the attacker to summarize the attack (which can be an impartial \"judge\") then have gpt-5 evaluate the success of the summary?\n- It's hard to find the exact experimental setup.  A red-team model did 10 rounds against a victim model across the 10 categories.  Was each category done once or multiple times? \n- Please just exclude llama 3 70B instead of dashed lines and then explaining that you weren't able to get the model to attack. You can \"jailbreak\" or harmfully train the model and re-evaluate as a suggestion.\n- The limitations seem quite high and aren't properly addressed or noted elsewhere.  Models sometimes highly refuse to do attack a certain category.  Does that mean this category was skipped sometimes? Are we evaluating the same number of attacks across categories?"}, "questions": {"value": "Notes:\n- Is the title supposed to read \"of-the-shelf\" instead of \"off-the-shell\"?\n- Line 103 also mentions off-the-shell\n- Line 105 missing a citation\n- 160 Missing Figure reference\n- 187 Doesn't actually say the victim models\n- 192 Sentence fragment\n- 205 space before comma indicating probably a missing internal reference?\n- None of the internal references are working so I'm stopping to point them out\n\nWhat I'd need to improve my score:\nThe paper seems rushed and doesn't actually create a nice harmful evaluation platform or instructions.  I was expecting something like HarmBench or similar evaluations setup or infrastructure but this is lacking in quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mTH3JSnZ9x", "forum": "10gpXxro8z", "replyto": "10gpXxro8z", "signatures": ["ICLR.cc/2026/Conference/Submission19693/Reviewer_hiPC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19693/Reviewer_hiPC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608616710, "cdate": 1761608616710, "tmdate": 1762931532901, "mdate": 1762931532901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the inherent capability of off-the-shelf Large Language Models (LLMs) to act as red-teamers, specifically for conducting multi-round jailbreak attacks. The authors argue that static, dataset-centric safety evaluations are insufficient as they become outdated and don't transfer well. Using LLMs as red teamers is a scalable alternative, but it's not widely studied whether they can do this \"zero-shot\" without specialized fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating multi-turn jailbreaking is useful. Previous works focused on developing agents specifically for this purpose, but knowing the zero-shot ability of various models as red teaming agents is valuable."}, "weaknesses": {"value": "- The attack success standard seems unnecessary. Why does it need to be separate from the activity? Why can't the standard just be \"the model carried out the activity\"? This is effectively the case for all the examples shown in Figure 1.\n- The paper is about evaluating red teaming agents, but it imposes a specific agent scaffold (CoT + self reflection + one jailbreak generated per step). There is a much broader space of red teaming scaffolds that have been developed in this area and that could be explored in this paper. Why not allow arbitrary agent scaffolds?\n- The LLM judge was not evaluated by humans for precision and recall. This is a methodological failure that many papers in this area make. I would like to see it corrected in future work, especially papers published at top venues like ICLR.\n- There isn't much technical novelty."}, "questions": {"value": "See weaknesses section.\n\nAlso, which Llama 3 70B model was used? Llama 3.3 70B instruct or Llama 3.1 70B instruct? This should be specified in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nu8cumvfkT", "forum": "10gpXxro8z", "replyto": "10gpXxro8z", "signatures": ["ICLR.cc/2026/Conference/Submission19693/Reviewer_hw49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19693/Reviewer_hw49"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762122888831, "cdate": 1762122888831, "tmdate": 1762931532235, "mdate": 1762931532235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}