{"id": "aMgYP4lh0d", "number": 12657, "cdate": 1758209337227, "mdate": 1759897495664, "content": {"title": "Legal-Gated Attention Networks: Enforcing Action Legality as a Structural Inductive Bias in Deep Reinforcement Learning", "abstract": "In reinforcement learning (RL) for environments with state-dependent action constraints, conventional methods suffer from conflated representations, as signals from infeasible actions introduce noise and complicate the learning task. While post-hoc masking is a common workaround, it fails to prevent this contamination at a fundamental level, as illegal actions still influence the learned representations. To address this, we propose Legal-Gated Attention Networks (LGAN), an architecture that introduces a strong structural inductive bias by compiling action legality directly into its structure. LGAN fundamentally alters self-attention by using a legality mask to gate the query formation process itself, permitting only legal actions to attend to the state. This architectural design guarantees by construction that illegal actions are structurally eliminated: they produce no queries, receive no gradients, and cannot influence policy or value updates. By using raw state vectors as values, LGAN's attention weights directly reveal which state components contribute to each legal action's value. We demonstrate in board games that this structurally grounded approach provides an effective framework for learning transparent policies, positioning LGAN as a principled method for building robust and interpretable agents in action-constrained environments.", "tldr": "", "keywords": ["Reinforcement Learning", "Explainable AI", "Attention Mechanisms", "Gated Networks"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b62779f499c8e5bb2974963b173c91af4e2f33a.pdf", "supplementary_material": "/attachment/6c39b0013906676c3cccad9de35215aead7a3058.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses representational contamination in RL environments with state-dependent action constraints. It argues that standard architectures are forced to map a state to the entire action space, causing gradients from invalid actions to \"corrupt the shared network parameters\". This is claimed to be especially damaging in complex problems with sparse rewards, where noise from invalid actions can \"obscure faint and infrequent reward signals\". It also points to Q-explosion in DQN if the Bellman update bootstraps from an erroneously high Q-value of an illegal action."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea is interesting and the experiments are well-chosen to support the central claim."}, "weaknesses": {"value": "Regarding the gradient argument: From a gradient perspective, applying a standard mask before the softmax (e.g., setting logits to $-\\infty$) also blocks gradients from flowing back through illegal actions. Given this, what advantage does the proposed method offer? The authors may need to revise their claim in the introduction accordingly.\n\nRequires an Explicit Mask: The entire architecture is predicated on having access to a binary legality mask $\\mu_s$. This is realistic for many structured domains (games, robotics, program synthesis) but makes it inapplicable to problems where action legality is unknown or must be learned."}, "questions": {"value": "Please see the weakness point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6xc785NciQ", "forum": "aMgYP4lh0d", "replyto": "aMgYP4lh0d", "signatures": ["ICLR.cc/2026/Conference/Submission12657/Reviewer_QS8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12657/Reviewer_QS8Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937496526, "cdate": 1761937496526, "tmdate": 1762923497993, "mdate": 1762923497993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Legal-Gated Attention Networks (LGAN), an architecture that integrates action legality directly into the attention mechanism of deep reinforcement learning (RL) models. This approach eliminates the influence of illegal actions during both forward and backward passes, addressing the problem of representational contamination in environments with state-dependent action constraints. The authors demonstrate its effectiveness through experiments in three simple environments, showing that LGAN offers competitive performance while maintaining interpretability of learned policies."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed is highly relevant, and the concept of leveraging the attention mechanism to tackle it is interesting.\n\n2. The paper offers solid theoretical support, which effectively guarantee the model's correctness and stability throughout the training process."}, "weaknesses": {"value": "1. Although I did not focus on the recent advancements in this field, the Related Works section seems to overlook more recent advancements, as most of the cited references are from before 2020. It’s likely that newer studies could offer solutions to the problems discussed, and a more thorough review of contemporary literature would strengthen the paper.\n\n2. The paper does not seem to consider constrained RL methods, which also aim to address state-dependent constraints on available actions. It would be helpful to explore why these approaches were not considered and provide some discussions.\n\n3. The experimental setup is insufficient in several key areas:\n\n- The paper primarily evaluates the model on three relatively simple environments. While these may be useful for initial understanding, the authors should include more complex environments, such as MuJoCo, to better demonstrate the model's scalability and robustness\n\n- In Sections 5.2 and 5.3, the paper compares LGAN with only a few baseline methods, many of which are not novel. Including more well-established and powerful baselines, such as PPO, Double DQN, SAC, and Rainbow， would provide a more comprehensive evaluation.\n\n- For all experimental results (e.g., Tables 1 and 2), the authors should run experiments multiple times and report the mean and standard deviation to ensure the reliability and stability of the findings.\n\n- Including additional visualizations would help clarify the model’s behavior, making the results more accessible and enhancing the reader’s understanding of the method's performance."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E5cbbzAcNb", "forum": "aMgYP4lh0d", "replyto": "aMgYP4lh0d", "signatures": ["ICLR.cc/2026/Conference/Submission12657/Reviewer_WtYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12657/Reviewer_WtYf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335512299, "cdate": 1762335512299, "tmdate": 1762923497620, "mdate": 1762923497620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"Legal-Gated Attention Network\" (LGAN), a transformer-based model that imposes action legality/feasibility constraints by allowing only feasible actions to attend to the state. This has the reported benefit of a) preventing \"representational contamination\" (where infeasible actions degrade the learned representation) and b) increasing interpretability, since the attention scores can be used to analyze how components of the state contribute to action scores.\n\nThe method oracle access to the per-state action feasibility set $\\mathcal{M}(s)$. \n\nLGAN is evaluated on tic-tac-toe, breakthrough (8x8 board), and GO (7x7 board) environments, where it is found that LGAN outperforms standard DQN and A2C baselines in terms of win rate against a Monte-Carlo Tree Search opponent."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Preventing infeasible actions from occupying plasticity in the network (representational contamination) seems interesting and promising."}, "weaknesses": {"value": "### **Clarity**\nThe paper is hard to follow and lacks a clear narrative. The method section primarily introduces the modified attention mechanism but doesn't sufficiently explain how this attention mechanism is used in what kind of transformer-based RL agent. The algorithm is not summarized, and no algorithmic pseudo-code is given. No losses or optimization procedures are stated. The only information about the algorithm used to train LGAN is in line 310: \"All models are trained via self-play\". This characterization is insufficient.\n\n### **Lack of related works**\nThe paper does not position LGAN relative to highly relevant related works that apply similar \"infeasible action masking\" tricks, such as\n+ A Closer Look at Invalid Action Masking in Policy Gradient Algorithms (Shengyi Huang, Santiago Ontañón, 2021)\n+ Deep Inverse Q-learning with Constraints (Kalweit et al, 2020)\n+ Reachability Constrained Reinforcement Learning (Yu et al, 2022)\n+ IPO: Interior-Point Policy Optimization under Constraints (Liu et al, 2020)\n\n### **Experiments**\nThe set of baselines misses many standard constrained RL methods. It is not explicitly shown whether representation contamination actually negatively affects baselines."}, "questions": {"value": "+ What algorithm are you actually using to train LGAN?\n+ The interpretability of your method seems to involve analyzing attention maps for individual actions and for all components of the state? How does this scale to environments with large, possibly non-symbolic state spaces and many actions?\n+ Can your method be seen like an instance of \"A Closer Look at Invalid Action Masking in Policy Gradient Algorithms\" (Shengyi Huang, Santiago Ontañón, 2021)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EyCGBE3ZTd", "forum": "aMgYP4lh0d", "replyto": "aMgYP4lh0d", "signatures": ["ICLR.cc/2026/Conference/Submission12657/Reviewer_1dEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12657/Reviewer_1dEt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762359532879, "cdate": 1762359532879, "tmdate": 1762923497263, "mdate": 1762923497263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to use a legality  mask to gate the query formation process. This ensures only legal actions are chosen. Specifically, the queries correspond to actions and keys corresponds to states, and the authors compute the attention scores, where Relu is used instead  of softmax."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method ensures that gradients with respect to the illegal actions are zeros and has strong empirical performance."}, "weaknesses": {"value": "The baselines are changing across experiments. In Table 1, MLP-DQN and MLP-DQN(masked) are used as baselines. However, in table 2, RESNET-DQN and RESNET-A3C  are used as baselines."}, "questions": {"value": "To make the baselines consistent, could the authors report the performance of RESNET-DQN and RESNET-A2C in table 1 and the performance of MLP-DQN and MLP-DQN(masked) in table 2?\nHow many games is the win rates calculated from (in tables 1-3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ybkJZf5KuZ", "forum": "aMgYP4lh0d", "replyto": "aMgYP4lh0d", "signatures": ["ICLR.cc/2026/Conference/Submission12657/Reviewer_6KDh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12657/Reviewer_6KDh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762408645219, "cdate": 1762408645219, "tmdate": 1762923497030, "mdate": 1762923497030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the problem of deep RL where certain actions are contextually restricted, i.e., illegal. The key idea of the work is to provide a structural bias in a transformer style policy where the attention mechanism can only consider legal actions. This is done by masking. Experiments shows that this empirically helps and out-performs post-hoc masking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clear and easy to read. Furthermore the method does seem to work."}, "weaknesses": {"value": "The result presented is fairly straightforward and I would refer to a \"folk-trick\". While nice to have written up, I think it and other similar attention masking tricks are well understood in the community. For example, the zero invariance and structural gradient isolation are the same trick required for batch training, e.g., adding dummy nodes for training graph neural networks.\n\nWhile a nice write up, I hesitate to recommend this for ICLR."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pNDrlF2Cj4", "forum": "aMgYP4lh0d", "replyto": "aMgYP4lh0d", "signatures": ["ICLR.cc/2026/Conference/Submission12657/Reviewer_dX58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12657/Reviewer_dX58"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762408659640, "cdate": 1762408659640, "tmdate": 1762923496768, "mdate": 1762923496768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}