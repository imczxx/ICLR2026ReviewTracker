{"id": "RErUA45pI9", "number": 6655, "cdate": 1757991454138, "mdate": 1759897902674, "content": {"title": "CLIP-FMoE: Scalable CLIP via Fused Mixture-of-Experts with Enforced Specialization", "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach for scaling deep learning models while maintaining computational efficiency. However, existing MoE adaptations for Contrastive Language-Image Pre-training (CLIP) models suffer from significant computational overhead during sequential training and degradation of zero-shot capabilities. To address these limitations, we propose CLIP-FMoE, a novel approach that integrates MoE architecture into CLIP fine-tuning. Our method uses Isolated Constrained Contrastive Learning, a pipeline that trains specialized experts on cluster-based data partitions to accelerate expert specialization. Additionally, we introduce a Fusion Gate mechanism to mitigate catastrophic forgetting of pre-trained knowledge. Extensive experiments across multiple benchmarks demonstrate that our approach achieves consistent improvements on downstream tasks while preserving zero-shot capabilities. Furthermore, our method demonstrates robust performance across varying context lengths, making it particularly suitable for diverse real-world applications.", "tldr": "The manuscript proposes a novel fine-tuning CLIP method using Mixture-of-Experts.", "keywords": ["Vision-Language Models", "Mixture of Experts", "Fine-tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7340773d673384210bbc2f276f950c9968f6239.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CLIP-FMoE, a mixture-of-experts scaling method for CLIP models. CLIP-FMoE combines an Isolated Constrained Contrastive Learning stage and a fused MoE training stage to preserve pretrained knowledge. Experiments on zero-shot classification, image-text retrieval, and long-caption benchmarks show modest improvements over existing baselines.\n\n**The assigned paper is outside my area of expertise. My reviews are based on my limited understanding.**"}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses an important challenge and is valuable for multimodal foundation model development.\n2. The method is evaluated on diverse benchmarks, including standard and fine-grained classification, retrieval tasks, and long-context understanding.\n3. The paper is clearly written and well-organized."}, "weaknesses": {"value": "1. It seems like novelty is incremental: Both ICCL and Fusion Gate largely reuse familiar concepts (e.g., semantic clustering of data, constrained batch sampling, and gating-based interpolation).\n2. The authors should provide stronger evidence for expert specialization. The current evaluation of expert behavior relies only on limited task-wise performance comparisons (Table 4) and does not convincingly show that semantic clustering leads to meaningful routing or complementary feature learning."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FsBGCTgbFs", "forum": "RErUA45pI9", "replyto": "RErUA45pI9", "signatures": ["ICLR.cc/2026/Conference/Submission6655/Reviewer_jDHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6655/Reviewer_jDHU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573231567, "cdate": 1761573231567, "tmdate": 1762918966683, "mdate": 1762918966683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CLIP-FMoE, a scalable MoE framework for CLIP to handle diverse visual-text data efficiently. The key idea is a two-stage clustering mechanism that partitions data by visual similarity, enabling each expert to specialize in a subset of the modality space. A lightweight Fusion Gate is introduced to adaptively aggregate expert outputs, balancing specialization and generalization. The experiments demonstrate improvements in zero-shot and retrieval benchmarks over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and the proposed method is presented clearly, making it easy to follow.\n2. Experimental results are consistent across multiple benchmarks, showing solid improvements over vanilla CLIP and other MoE-based baselines."}, "weaknesses": {"value": "1. The novelty of the fusion gate is somewhat incremental, as gating mechanisms for interpolating between network modules are a common way in deep learning. The paper could strengthen its contribution by better positioning this component with respect to prior art in model merging or dynamic network composition.\n2. The analysis of expert collaboration is limited. The expert specialization results in Table 4 suggest the unified model often matches the performance of the single best expert, rather than demonstrating strong synergistic gains from combining multiple experts.\n3. The sensitivity of balancing loss coefficient alpha in Eq. 5 has not been evaluated. The authors use a fixed value of 0.01, but an ablation study would be necessary to understand its impact and the robustness of the model to this choice.\n4. In the context of preventing catastrophic forgetting, continual learning literature offers several relevant strategies. Besides MoE-based approaches such as MoE-Adapters [1] and L2P [2], there also exist regularization-based approaches e.g. ZSCL [3], orthogonality-based approaches e.g. PGP [4] and feature compression approaches e.g. LADA [5]. These methods should be discussed in the related work section.\n\n\n\n[1] Yu, Jiazuo, et al. \"Boosting continual learning of vision-language models via mixture-of-experts adapters.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024.\n\n[2] Wang, Zifeng, et al. \"Learning to prompt for continual learning.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n[3] Zheng, Zangwei, et al. \"Preventing zero-shot transfer degradation in continual learning of vision-language models.\" *Proceedings of the IEEE/CVF international conference on computer vision*. 2023.\n\n[4] Qiao, Jingyang, et al. \"Prompt gradient projection for continual learning.\" *The Twelfth International Conference on Learning Representations*. 2024.\n\n[5] Luo, Mao-Lin, et al. \"LADA: Scalable Label-Specific CLIP Adapter for Continual Learning.\" *Forty-second International Conference on Machine Learning*."}, "questions": {"value": "1. Could the authors provide details on the sample distribution across the generated data clusters? Specifically, how balanced were the clusters?\n2. In Stage 2, only the router and fusion gate are trained. Have the authors experimented with also fine-tuning the expert weights with a small learning rate? It seems plausible that allowing for minor adjustments during this unification stage could further improve performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sg0uKRAYxO", "forum": "RErUA45pI9", "replyto": "RErUA45pI9", "signatures": ["ICLR.cc/2026/Conference/Submission6655/Reviewer_2spG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6655/Reviewer_2spG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660568808, "cdate": 1761660568808, "tmdate": 1762918966220, "mdate": 1762918966220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of existing MoE adaptations for CLIP (high sequential training overhead and degraded zero-shot capabilities) by proposing CLIP-FMoE, an approach integrating MoE into CLIP fine-tuning via two core designs: Isolated Constrained Contrastive Learning (ICCL), a two-stage pipeline that performs two-level semantic clustering to train specialized experts in parallel and reduce token imbalance in MoE, and a Fusion Gate mechanism to mitigate catastrophic forgetting of pre-trained knowledge ; extensive experiments on benchmarks (11 zero-shot classification datasets, standard/fine-grained/long-context retrieval tasks) show CLIP-FMoE preserves zero-shot capabilities (average accuracy drop <1% vs. original CLIP), outperforms baselines like Vanilla Fine-tuning and CLIP-MoE, and maintains efficiency (72.5% less training time than CLIP-MoE, 14% higher inference FLOPs) ; its main contributions include introducing CLIP-FMoE with ICCL and Fusion Gate, ensuring expert specialization, and demonstrating robust performance across context lengths and dataset sizes ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Motivation**: The paper designs a two-stage training pipeline (Isolated Constrained Contrastive Learning, ICCL) and a Fusion Gate mechanism to address key issues in CLIP scaling: it uses two-level semantic clustering (coarse-grained primary clusters + fine-grained sub-clusters) for parallel expert training in Stage 1, ensuring experts learn non-overlapping domain knowledge and avoiding the accumulated errors of sequential training in prior CLIP-MoE; the Fusion Gate dynamically balances pre-trained CLIP knowledge and expert-specific expertise in both training stages, effectively mitigating catastrophic forgetting.\n\n\n\n2. **Solid Experimental Design**:  The experiments cover diverse task scenarios: 11 zero-shot classification benchmarks (including fine-grained, scene, and general datasets), 6 retrieval tasks (standard MS COCO, fine-grained DOCCI, long-context Urban-1K), and long-context understanding (extending context length from 77 to 248 tokens). All methods use the same CLIP ViT-L/14 backbone and fine-tune only odd layers in the second half of the encoder, ensuring fair comparison detailed hyperparameters (e.g., batch size, learning rate) for different datasets (CC3M, ShareGPT4V) and extended experimental results (e.g., 17 classification benchmarks in the appendix) are provided, enhancing result reproducibility.\n\n 3. **High computational efficiency**: CLIP-FMoE achieves an excellent trade-off between performance and computational cost: compared with CLIP-MoE, it reduces total training time  through parallel expert training; its inference FLOPs only increase by 14%, while the peak trainable parameters are 80% fewer than Up-cycling, reducing GPU memory pressure. Additionally, it maintains robustness across scenarios—performing well in both resource-constrained environments (due to low memory usage) and diverse practical tasks (e.g., fine-grained product recognition, long-text image retrieval), expanding the application scope of CLIP-style models."}, "weaknesses": {"value": "1. **Incremental Contribution**: CLIP-FMoE’s core components are all combinations of mature existing technologies in the field: the MoE architecture , contrastive learning with InfoNCE loss , K-means semantic clustering, and the Fusion Gate —no breakthrough technical ideas  and insights are proposed. In terms of performance, its improvement is only incremental: in zero-shot classification tasks (Table 1), its average accuracy (68.65) is only 2.18 percentage points higher than the second-best baseline CLIP-MoE (66.47), and even 1.19 percentage points lower than the original CLIP (69.84); in retrieval tasks (Table 2), the average R@1 is only 0.39 percentage points higher than CLIP-MoE, with a tiny improvement margin that fails to open up new technical directions."}, "questions": {"value": "# Questions for the Authors  \nGiven that this paper presents an engineering-sound work on CLIP scaling via CLIP-FMoE, i have two questions:  \n1. What new insights do you believe this work provides to the field of CLIP scaling and MoE adaptation for vision-language models, which were not sufficiently addressed or recognized in prior studies?  \n2. The current framework is tailored for CLIP fine-tuning. Do you think the core design (e.g., ICCL’s two-level clustering, Fusion Gate) can be extended to train non-CLIP models? If so, what key adjustments might be needed; if not, what inherent limitations of the framework restrict such extension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kCtztNbV8W", "forum": "RErUA45pI9", "replyto": "RErUA45pI9", "signatures": ["ICLR.cc/2026/Conference/Submission6655/Reviewer_Wka5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6655/Reviewer_Wka5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825768685, "cdate": 1761825768685, "tmdate": 1762918965880, "mdate": 1762918965880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLIP-FMoE, a method for scaling Contrastive Language-Image Pre-training (CLIP) models using a Fused Mixture-of-Experts (MoE) architecture, with the aim to reduce computation cost when scaling. It involves two stages: Isolated Constrained Contrastive Learning (ICCL) where data is semantically clustered for separate experts training & specialization, and the integration of these experts into an MoE layer, by introducing the Fusion Gate mechanism to dynamically blend pre-trained knowledge with new, domain-specific expert knowledge. The method is evaluated on CC3M and achieves better performance consistently across tasks. Though, the method is not evaluated on billion-scale dataset training while the generalization of this method is expected to be further examined if more computational resource is available."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The idea to use CLIP model to split the data via clustering is interesting. \n- The performance is good and the result is convincing & promising. \n- The fusing mechanism is indeed needed and this paper provides a good methodology to prevent forgetting and enable efficient scaling. \n- Though not clearly stated in the method, but this paper consider shared and specialized parameters, which is good."}, "weaknesses": {"value": "- **Ablation on Clustering Strategy**: The impact of the specific clustering algorithm and hyperparameters (e.g., choice of N and M clusters) on final performance is not thoroughly explored. It is unclear how sensitive the method is to the quality and granularity of the semantic clusters.\n- **Single-modal clustering**: The paper uses image modality only for the clustering, but more discussion is needed. To fully discover the finegrained visual concept, the same image may need to be compared with different set of negative samples that focus on different aspects, thus it is unclear how the clustering over visual embeddings can help highlight the fine-grained visual elements. Also, as discussed in MoDE, the limited description in text is the primary cause of false negative. As such, more discussion is needed. \n- **Generalization**: The method is only evaluated on small-scale CLIP training, and the generalization over much larger-scale dataset is hard to determine. I completely understand that the computation resources is a primary constraint, but I would still keen to know how your method can generalize."}, "questions": {"value": "I am curious on how your approach generalize when trained on OpenAI CLIP ViT-Base. After all, the visual embedding quality of ViTs with different scale varies, which may hurt the performance.\n\nMeanwhile, I am happy to increase my score further if the authors can address all my questions & weakness part properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kgM1o9LUa5", "forum": "RErUA45pI9", "replyto": "RErUA45pI9", "signatures": ["ICLR.cc/2026/Conference/Submission6655/Reviewer_sfPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6655/Reviewer_sfPq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966668030, "cdate": 1761966668030, "tmdate": 1762918965524, "mdate": 1762918965524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}