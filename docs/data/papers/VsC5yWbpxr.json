{"id": "VsC5yWbpxr", "number": 10973, "cdate": 1758185904770, "mdate": 1759897617245, "content": {"title": "CAT: Post-Training Quantization Error Reduction via Cluster-based Affine Transformation", "abstract": "Post-Training Quantization (PTQ) reduces the memory footprint and computational overhead of deep neural networks by converting full-precision (FP) values into quantized and compressed data types.\nWhile PTQ is more cost-efficient than Quantization-Aware Training (QAT), it is highly susceptible to accuracy degradation under a low-bit quantization (LQ) regime (e.g., 2-bit and 4-bit).\nAffine transformation is a classical technique used to reduce the discrepancy between the information processed by a quantized model and that processed by its full-precision counterpart; however, we find that using plain affine transformation, which applies a uniform affine parameter set for all outputs, is ineffective in low-bit PTQ.\nTo address this, we propose Cluster-based Affine Transformation (CAT), an error reduction framework that applies cluster-specific affine transformation to align LQ and FP outputs.\nCAT directly refines quantized outputs with only a negligible number of additional parameters.\nExperiments on ImageNet-1K demonstrate that CAT consistently outperforms prior PTQ methods across diverse architectures and low-bit settings, achieving up to 53.18\\% Top-1 accuracy on W2A2 ResNet-18, and delivering improvements of more than 3\\% when combined with strong PTQ baselines.\nWe plan to release CAT’s code alongside the publication of this paper.", "tldr": "We develop a novel Error Reduction approach for Post-Training Quantization via Cluster-based Affine Transformation", "keywords": ["Post-Training Quantization", "Deep Learning", "Image Classification", "Convolutional Neural Networks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e61aaa018a4b9767fecfed25c863affc66bf6601.pdf", "supplementary_material": "/attachment/a646179119a3dc62bf38d8622011b7e5bb98cb8f.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes Cluster-based Affine Transformation (CAT), a framework to reduce PTQ errors in deep neural networks. Instead of applying a single global affine correction, CAT performs cluster-specific affine mappings between quantized and full-precision logits. Each cluster’s scale and bias parameters are estimated in closed form without gradient updates, using a small calibration set. Experiments on ImageNet-1K with diverse architectures show that CAT consistently improves accuracy—especially under extreme low-bit settings (e.g., W2A2)—while adding negligible parameter overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using cluster-specific affine corrections to align quantized and full-precision outputs is intuitive, lightweight, and easy to implement.\n2. CAT consistently improves Top-1 accuracy across multiple architectures (ResNet, MobileNetV2, RegNetX, MNasX2) and bit-widths, especially in 2-bit settings.\n3. The paper includes detailed ablation studies (α blending, cluster number, PCA dimension, sample size) demonstrating robustness and stability."}, "weaknesses": {"value": "1. The paper does not provide any analysis of the inference-time overhead introduced by CAT. Although PCA and cluster assignment are claimed to be lightweight, they may still incur noticeable latency. A detailed runtime and latency evaluation is necessary to substantiate the claim of negligible cost.\n2. While CAT consistently improves accuracy, the absolute improvements are relatively small (often below 1%) on several benchmarks. The practical significance of these gains, given the added complexity, should be discussed more thoroughly.\n3. Results on how CAT generalizes to ViTs and LLMs would make the work more convincing and broadly relevant.\n4. The paper lacks deeper discussion or comparison with other recent PTQ techniques. For example, OBQ, GPTQ, COMQ ...\n\n[1] Frantar, Elias, et al. \"Optimal brain compression: A framework for accurate post-training quantization and pruning.\" Advances in Neural Information Processing Systems 35 (2022): 4475-4488.\n\n[2] Frantar, Elias, et al. \"Gptq: Accurate post-training quantization for generative pre-trained transformers.\" arXiv preprint arXiv:2210.17323 (2022).\n\n[3] Zhang, Aozhong, et al. \"Comq: A backpropagation-free algorithm for post-training quantization.\" IEEE Access (2025)."}, "questions": {"value": "1. How does CAT affect inference latency and memory footprint?\n2. How are cluster IDs assigned for unseen inputs during inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h4ZXzLnP3N", "forum": "VsC5yWbpxr", "replyto": "VsC5yWbpxr", "signatures": ["ICLR.cc/2026/Conference/Submission10973/Reviewer_Tbsw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10973/Reviewer_Tbsw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274965403, "cdate": 1761274965403, "tmdate": 1762922165286, "mdate": 1762922165286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel method called Cluster-based Affine Transformation (CAT) to reduce the accuracy gap between full-precision (FP) models and their low-bit quantized (LQ) counterparts. The authors observe that traditional plain affine transformations, which apply a single uniform correction, are ineffective or even detrimental in aggressive low-bit (e.g., 2-bit or 4-bit) Post-Training Quantization (PTQ) settings"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well-motivated. It directly targets the failure of plain affine transformation in low-bit PTQ, a claim supported by the authors in Table 1.\n2. The authors test CAT on a diverse set of CNN architectures (ResNet-18/50, MobileNetV2, RegNet, MNasX2) and multiple low-bit settings (W4A4, W2A4, W4A2, W2A2).\n3. The paper includes a comprehensive set of ablation studies (Section 5) that validate the key hyperparameters introduced by CAT, including the blending factor ($\\alpha$), the number of clusters, the PCA dimension, and the calibration sample size."}, "weaknesses": {"value": "1. The paper discusses parameter overhead but provides no analysis of the computational (latency) cost introduced by the CAT-specific steps during inference (PCA projection, cluster assignment, and affine transformation).\n2. The paper is functionally incomplete. It makes at least nine distinct references to appendices for crucial details, including pseudocode, comprehensive results, additional ablations, and results for Vision Transformers (ViT). Without these, the paper is not fully verifiable.\n3. The citation style is disruptive, often using long, uncontextualized lists of references and creating confusing attributions.\n4. The main method section only vaguely suggests \"k-means\" , leaving the reader to infer from the limitations section  that it was the algorithm used. No justification for this choice over other clustering algorithms is provided.\n5. All experiments were run with only three seeds. This is a very small sample size to confidently claim statistical significance for the many marginal improvements reported."}, "questions": {"value": "1. Missing Appendices: The paper references Appendices A through I, which are not included. Could you please provide these appendices? They seem to contain essential information, including pseudocode, full experimental results, and (critically) the results for Vision Transformers (ViT).\n2. Inference Latency: The \"Limitations\" section analyzes parameter overhead  but omits computational cost. What is the total inference latency overhead (in ms or FLOPS) introduced by CAT, including the PCA projection, cluster assignment, and final affine transformation?\n3. Clustering Algorithm: The \"Method\" section does not specify the clustering algorithm, but Limitations implies K-Means was used. Could you confirm this and justify why K-Means was chosen over other algorithms.\n4. Can you report results of your experiments for with at least 10 randomly selected seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oK6o1Xnnc9", "forum": "VsC5yWbpxr", "replyto": "VsC5yWbpxr", "signatures": ["ICLR.cc/2026/Conference/Submission10973/Reviewer_WCQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10973/Reviewer_WCQU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852400106, "cdate": 1761852400106, "tmdate": 1762922164856, "mdate": 1762922164856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"CAT\" post-training quantization of neural networks using cluster-based linear transformations. The clusters are found using principal component analysis and then. A subset of image samples is used to find the CAT parameters for PCA projections, cluster assignments, and the transformation parameters. These transformations have to be applied at inference time to obtain network output."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and provides a good background on post-training quantization. The experimental results are convincing and show improvements over the SOTA. The proposed method works especially well for extremely low-bit-count models. It also contains multiple ablation studies, which provide a better understanding of what is happening."}, "weaknesses": {"value": "The authors should address the following issue in the paper:\n\n1. A comparison with BRECQ (Block Reconstruction for post-training quantization). The authors have referenced and discussed the paper, but have not included it in the comparison table. One of the results for 2/4 for ResNet-18 is better in the BRECQ paper.\n2. The method requires overhead in terms of space, as is listed as one of the weaknesses. What the paper does not mention is the inference time overhead. The PCA and affine transformations will require complex and specialized activations. The authors should compare the inference time of their method to competing methods. Overall, the idea of quantization is to decrease the energy and space utilization of these models. Decreasing the model size but adding additional parameters and creating expensive and complex activations does not lead to a fair comparison.\n3. The number of hyperparameters that are introduced and then have to be tuned does not, in my opinion, justify the small increase in accuracy. The work seems incremental, and it would be OK if there were significant accuracy gains, but that is not the case here."}, "questions": {"value": "1. Table comparing the runtime of their method and the SOTA, including BRECQ.\n2. How difficult is it to tune the hyperparameters?\n3. Can you use other clustering algorithms, and how do they compare in performance? K-means usually does not provide consistent results for me."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3umYl2TKcs", "forum": "VsC5yWbpxr", "replyto": "VsC5yWbpxr", "signatures": ["ICLR.cc/2026/Conference/Submission10973/Reviewer_Q6KF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10973/Reviewer_Q6KF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934818396, "cdate": 1761934818396, "tmdate": 1762922164371, "mdate": 1762922164371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CAT, a two-stage PTQ framework combining KL-based distillation to refine quantization parameters and cluster-specific affine transformations to correct residual errors. Experiments on vision models and tasks are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is novel to first cluster  and then use affine transformation to correct residual errors. \n2. Can be used to improve PTQ methods."}, "weaknesses": {"value": "1. The experiments are limited to relatively small vision models. Could the authors clarify why evaluations were not extended to other modalities (e.g., language or multimodal models)? Since the proposed method is general, it would be valuable to demonstrate its applicability beyond vision tasks.\n2. The performance gains appear quite marginal compared to existing quantization methods, particularly in Tables 2 and 3.\n3. The method involves fine-tuning quantization scales and zero-points via a distillation-like procedure, which may introduce nontrivial computational cost. An ablation study evaluating the impact of this stage (with vs. without distillation) would help assess its necessity.\n4. The recent work PEQA also fine-tunes scales for quantized models. A direct comparison or discussion of the differences would strengthen the paper."}, "questions": {"value": "In Equation (2), is $\\delta$ the scale factor and $z$ the zero-point? The symbol $z$ was previously used to denote logits in the definition of $p_{FP}(x)$, so reusing it here is confusing. It would be clearer to use distinct notation for these variables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H5ek0V1kI1", "forum": "VsC5yWbpxr", "replyto": "VsC5yWbpxr", "signatures": ["ICLR.cc/2026/Conference/Submission10973/Reviewer_sBgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10973/Reviewer_sBgh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952030113, "cdate": 1761952030113, "tmdate": 1762922163933, "mdate": 1762922163933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}