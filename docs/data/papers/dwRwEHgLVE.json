{"id": "dwRwEHgLVE", "number": 788, "cdate": 1756818159690, "mdate": 1759898241894, "content": {"title": "PQP: A one-shot collaborative method for post-quantization pruning of LLMs", "abstract": "Achieving large-scale model compression at a lower cost without significantly compromising performance remains an important challenge in the deployment of LLMs. To address this, we propose Post-Quantization Pruning (PQP), a framework that enables effective synergy between quantization and pruning. Specifically, we introduce a pruning metric tailored for post-quantization scenarios. In addition, we integrate the statistical properties of quantization error into the loss function for block-wise pruning, allowing the model to simultaneously eliminate weights that are unimportant or heavily affected by quantization noise. The PQP method integrates post-training quantization and one-shot pruning, reducing the compressed model's storage requirements to just 1/8 of the original size, with minimal impact on performance. The entire compression process is lightweight, easy to implement, and does not require fine-tuning. Experimental results on LLaMA and LLaMA-2 models demonstrate that PQP consistently outperforms existing methods in compressing models of various scales. Our code is available at \\url{https://anonymous.4open.science/r/PQP}.", "tldr": "A one-shot, high-ratio compression method for LLMs leveraging collaborative quantization and pruning.", "keywords": ["model compression", "one-shot pruning", "one-shot quantization", "large anguage models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be38634688ca02c802b8830097c5394304b2c8e4.pdf", "supplementary_material": "/attachment/d9655dadca211f1ccb76316915df2b8810e2dfa5.pdf"}, "replies": [{"content": {"summary": {"value": "The authors propose a novel method, PQP, that performs both quantization and pruning in one-shot without any re-training. The core idea behind the method is to minimize the layer-wise reconstruction error of the quantized and pruned model with respect to the pruning mask and sparsity rates. The authors propose an objective to minimize as well as solutions to tackle the non-differentiability and challenges of convergence. The authors benchmark their method across a variety of downstream tasks against pruning methods applied to quantized models showing that PQP is able to outperform prior pruning methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a unified framework to performing both quantization and pruning.\n2. The performance benchmarks of PQP seem to be very competitive and outperform approaches where pruning is merely applied naively after quantization. \n3. The method does not require re-training and can be done in one-shot — providing a low-cost alternative to methods that would require re-training."}, "weaknesses": {"value": "1. No intuition/reasoning is provided behind the PQP metric.\n2. Equation 13 needs more exposition and is currently unmotivated. Why is this a reasonable thing to be doing? \n3. The algorithm has **a lot** of hyperparameters that are not fully ablated in the paper. For example, there are a set of hyperparameters associated with optimizing the objective Equation 9 that are not fully addressed. The choice of D and how the {r_d} should be chosen/spaced are also not fully addressed either. This is particularly important since the spacing of {r_d} seems to affect Equation 13 and it’s unclear how things would change if {r_d} is varied. \n4. The method is only tested on dated models (LLaMA and LLaMA-2) and a single model family which potentially leads to questions as to whether the method generalizes to more recent models and different model families.\n5. PQP is formulated in a way such that pruning is done post-quantization. This has been shown to be suboptimal in [1] where the authors show (theoretically and empirically) that pruning should be done prior to quantizing. \n6. A lot of effort is put into the paper to simplify the objective (Equation 3) in order to get better convergence but there is nothing in the paper that suggests that the simplified objective (Equation 8) actually converges or if it is superior to the original formulation without the simplification. \n7. In the supplementary material (Table 10), it seems to show that increasing the training epochs of Equation 9 can actually lead to worse perplexity performance. This is counter-intuitive and raises into question as to whether Equation 9 is actually being minimized practically.\n8. The clarity of Figure 1 could be improved a it is currently unclear without reading the inline text."}, "questions": {"value": "1. Could the authors provide some intuition/motivation as to why the PQP metric is a reasonable metric to be using for the pruning metric? Equation 13 also warrants additional motivation. \n2. In the results for PQP, how many epochs were used to optimize the objective (Equation 9)?\n3. Could the authors include a plot that shows the objective (Equation 9) decreasing with training epochs?\n4. Did you ablate minimizing Equation 4 directly versus minimizing Equation 8?\n5. In Tables 2/3/4, are the pruning metrics that PQP is being compared to just being computed directly using the quantized model? Since PQP is made aware of the quantization-step, this comparison is a bit unfair, as the other pruning algorithms are operating directly on the quantized model and unaware of the quantization that was performed. Are there any methods that the authors can compare to that combine both pruning and quantization in one single method? (If no such method exists, this should be emphasized in the paper). \n6. There are various instances of where \\citet and \\citep are misused. \n7. Is the normality assumption on B necessary? Don't you simply need the first moment to be 0? Could the authors provide empirical evidence to justify that the expectation of B is zero?\n8. Wouldn't it make more sense to put the PQP Algorithm bubble in the main text as opposed to the appendix? \n9. The authors might want to encapsulate Section 3.3 into a theorem box accompanied with the proof. \n10. It might be better to clarify what W is in Table 1 (i.e. are the pruning scores being computed directly with the quantized models?) \n11. Are there perhaps any more recent pruning methods that the authors are not comparing their method to?\n\n[1] EFFECTIVE INTERPLAY BETWEEN SPARSITY AND QUANTIZATION: FROM THEORY TO PRACTICE"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z6Dv4u6BtI", "forum": "dwRwEHgLVE", "replyto": "dwRwEHgLVE", "signatures": ["ICLR.cc/2026/Conference/Submission788/Reviewer_6pDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission788/Reviewer_6pDF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761172990789, "cdate": 1761172990789, "tmdate": 1762915605008, "mdate": 1762915605008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this manuscript propose PQP (Post-Quantization Pruning), a one-shot, training-free method to collaboratively prune already-quantized LLMs. The core idea is to introduce a new pruning metric (PQP-Metric) specifically designed for quantized weights and to integrate the statistical properties of quantization error into a block-wise pruning loss. This allows the framework to remove weights that are either unimportant or heavily impacted by quantization noise, all without requiring fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1- The paper is well-written and the proposed PQP method is clearly explained and easy to follow.\n\n2- The one-shot, training-free nature of the framework is a significant strength, making it practical and computationally efficient for real-world deployment.\n\n3- The concept of a pruning metric (PQP-Metric) that is aware of the post-quantization state of the weights is a novel and well-motivated contribution.\n\n4- The reported perplexity (Table 2) and zero-shot accuracy (Table 3) results show consistent improvements over baseline one-shot pruning methods (Wanda, SparseGPT, etc.) applied on top of quantized models."}, "weaknesses": {"value": "1- The comparison results are missing a crucial benchmark, JSQ [1], which is one of the closest and most relevant works on joint one-shot sparsification and quantization. Without this comparison, it's difficult to evaluate the true state-of-the-art performance of PQP.\n\n2- The models used for evaluation (LLaMA and LLaMA 2) are known to be relatively robust to compression. Based on my experience, a stronger evaluation would include more recent and sensitive models, such as the LLaMA 3.x or Gemma 3 families, to truly test the method's robustness.\n\n3- The main results (Tables 2 and 3) focus on 50% unstructured sparsity, which is notoriously difficult to accelerate on GPUs. While 2:4 sparsity is briefly explored for the metric (Table 4), the paper lacks a comparison to state-of-the-art semi-structured sparsity methods like MaskLLM [2] or ProxSparse [3], which are designed for practical speedups.\n\n4- A more detailed analysis of the overhead is needed. While Section 4.3 discusses the low parameter overhead of learning the pruning rates, it does not provide a clear picture of the time overhead of this block-wise pruning process.\n\n---\n\n[1] Guo et al., Compressing Large Language Models by Joint Sparsification and Quantization, ICML 2024\n\n[2] Fang et al., MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models, NeurIPS 2024\n\n[3] Liu et al., PROXSPARSE: REGULARIZED LEARNING OF SEMI-STRUCTURED SPARSITY MASKS FOR PRETRAINED LLMS, ICML 2025"}, "questions": {"value": "1- How does PQP compare against JSQ [1] in a head-to-head comparison on the same models and compression settings?\n\n2- For semi-structured 2:4 sparsity, how does PQP (using its row-wise metric) compare against specialized methods like MaskLLM [2] and ProxSparse [3]?\n\n3- Is the PQP framework compatible with low-rank approximation (LoRA) methods? For example, could it be combined with a method like SLiM [4] for even greater compression or higher accuracy?\n\n4- Given the focus on 50% unstructured sparsity, what are the practical, measured inference speedups (if any) on a GPU? Or is the primary benefit of PQP at this sparsity level limited to storage reduction?\n\n---\n\n[1] Guo et al., Compressing Large Language Models by Joint Sparsification and Quantization, ICML 2024\n\n[2] Fang et al., MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models, NeurIPS 2024\n\n[3] Liu et al., PROXSPARSE: REGULARIZED LEARNING OF SEMI-STRUCTURED SPARSITY MASKS FOR PRETRAINED LLMS, ICML 2025\n\n[4] Mozaffari et al., SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression, ICML 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fAIiw7Tkre", "forum": "dwRwEHgLVE", "replyto": "dwRwEHgLVE", "signatures": ["ICLR.cc/2026/Conference/Submission788/Reviewer_qc9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission788/Reviewer_qc9r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511338681, "cdate": 1761511338681, "tmdate": 1762915604870, "mdate": 1762915604870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PQP (Post-Quantization Pruning), a one-shot pruning pipeline that first performs post-training quantization (PTQ) and then prunes the already-quantized weights. The key ideas are: a PQP-Metric that scores importance for quantized weights by multiplying their magnitude with a pruning metric and a simplified block-level reconstruction loss with learnable differentiable sparsity. Experiments show consistent perplexity and zero-shot improvements over previous methods, compressing the model size to one eighth of its original size (4-bit × 50% sparsity) while maintaining performance. The metric can be also transferred to 2:4 and 4:8 semi-structured sparsity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The article presents and defines a clear issue: it well clarifies the challenge of compound errors when naively combining PTQ and pruning, and addresses this problem by pruning metrics and explicitly considering quantified losses.\n2. The PQP-Metric combines quantized weights and pruning metric from the original weights, which is a clean way to inject pre-quantization importance while operating on quantized parameters. It avoids the complex computational load of gradients/Hessian inverses, and can be easily combined with existing methods or pipelines.\n3. Across LLaMA/LLaMA-2 families of different sizes, PQP consistently achieves the lowest perplexity at 50% sparsity after 4-bit quantization and improves zero-shot averages. Meanwhile, under semi-structured constraints, this method can still maintain leading performance."}, "weaknesses": {"value": "1. Equation (3) is derived from Eq. (1) by substituting $\\(\\hat{W_l} - B_l)$ for $\\hat{W_l}$, but since $B_l = \\hat W_l - W_l$, this collapses to masking $W_l$ and is **not equivalent** to Eq. (1). It should instead be $ \\arg\\min_{M_l}||X_l W_l - X_l\\big((W_l + B_l)\\odot M_l\\big)||_F^2$, which preserves the intended incorporation of quantization error. Building on this decomposition, the validity of the later analysis and proofs hinges on an objective that appears to be derived from a non-equivalent substitution.\n2. While performance are strong, the paper does not report latency or throughput in real scenarios, which are critical for claiming deployment benefits, especially since unstructured sparsity and 4-bit kernels can be hardware-limited. \n3. Baseline scope is skewed toward pruning-only metrics applied after GPTQ.  It is necessary to supplement the results based on different quantification methods (such as OPTQ, AWQ) to verify the robustness of PQP. Meanwhile, given the pitch is joint compression,  comparison to recent joint sparsification+quantization frameworks (For example, the JSQ[1] method mentioned in the article) would strengthen claims. \n\n\n[1] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong Liu. Compressing large language models by joint sparsification and quantization. In Forty-first International Conference on Machine Learning, 2024."}, "questions": {"value": "1. Results only use 4-bit GPTQ. How does PQP behave with other PTQ schemes (e.g., AWQ, OWQ, activation quantization, per-channel vs per-group)?\n2. For unstructured and 2:4/4:8 sparsity, what are the actual end-to-end throughput and latency gains?\n3. How many steps/epochs and how much GPU time are needed to learn the parameters compared to other pruning methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JSAKdmxNQz", "forum": "dwRwEHgLVE", "replyto": "dwRwEHgLVE", "signatures": ["ICLR.cc/2026/Conference/Submission788/Reviewer_Qx1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission788/Reviewer_Qx1R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536432519, "cdate": 1761536432519, "tmdate": 1762915604762, "mdate": 1762915604762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Post-Quantization Pruning (PQP), a Quantization + Pruning method that improves sparsification of quantized models. The authors propose a novel saliency score for pruning that accounts for quantization. The paper integrates the quantization reconstruction error into the loss function to learn the sparsity mask and shows improvements on perplexity and downstream benchmarks with Llama and Llama-2 as the base models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper shows improvements over relevant methods across perplexity and downstream benchmarks.\n* The proposed PQP metric is able to correct for the noise induced by quantization during saliency calculation."}, "weaknesses": {"value": "* The paper evaluates on Llama and Llama-2, both of which are far away from the frontier open-source models in quality. The authors should evaluate on more recent models such as Qwen2.5/Qwen3 or DeepSeek."}, "questions": {"value": "* Equation 4 can be replaced with learning a sparsity mask on the original weights with the PQP metric. What is the intuition behind framing it this way? Also, how does it compare with learning a sparsity mask on the original weights with the PQP metric.\n* Do you see a tradeoff between the precision and the sparsity? That is, for a target memory requirement, what is the most optimal precision and sparsity ratio.\n* How does PQP metric compare to PW(Quantized W)?\n* How does the algorithm here compare with using PW(Quantized W) following by minimizing Equation 4 with only quantized weights (and not with the original weights).\n* Are both weights and activations quantized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ke3TrN5ydm", "forum": "dwRwEHgLVE", "replyto": "dwRwEHgLVE", "signatures": ["ICLR.cc/2026/Conference/Submission788/Reviewer_xPjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission788/Reviewer_xPjA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971058855, "cdate": 1761971058855, "tmdate": 1762915604549, "mdate": 1762915604549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}