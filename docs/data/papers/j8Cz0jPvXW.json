{"id": "j8Cz0jPvXW", "number": 14056, "cdate": 1758227696027, "mdate": 1763636055533, "content": {"title": "Qonvolution: Towards Learning of High-Frequency Signals with Queried Convolution", "abstract": "Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier features have made great strides in improving performance, there remains scope of improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality. Our code and models will be made publicly available.", "tldr": "This paper improves learning of high frequency signals through convolving queries and the low frequency signals.", "keywords": ["High-Frequency Learning", "Novel View Synthesis", "Regression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14f9193805a8cac3fd491f264514c82e23f88598.pdf", "supplementary_material": "/attachment/5844f1693def20af052079627f5581e709c81f26.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Qonvolution / QNN (Queried Convolutional Neural Network), a convolutional module that jointly consumes (i) local neighborhoods of a low-frequency signal and (ii) per-pixel/per-sample query coordinates, and then predicts the missing high-frequency residual signal. Instead of fitting high-frequency content purely with coordinate MLPs or purely with CNNs over low-resolution images, QNN explicitly concatenates the encoded coordinates with the low-frequency approximation and applies convolution over local neighborhoods to regress the high-frequency component.\n\nThe authors evaluate QNN across four regimes: (1) 1D high-frequency regression, (2) 2D super-resolution, (3) 2D residual image regression, and (4) novel view synthesis (NVS). The paper also provides theoretical arguments that (a) adding neighborhood context and explicit query coordinates cannot worsen the optimal achievable risk compared to using either alone, and (b) high-frequency fidelity with pure Gaussian primitives may require exponentially many Gaussians, motivating a learned residual head instead of endlessly increasing primitive count."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Unified viewpoint across tasks.\nThe same basic idea (concatenate query coordinates and a low-frequency approximation, then convolve locally to predict high-frequency residuals) is tested on 1D regression, 2D SR, and 2D residual regression, not just NVS. This reduces the risk that the method is a one-off trick.\n\n- Lightweight and modular.\nQNN is a shallow conv stack (e.g., 4 conv layers, 3×3 kernels, ~64 channels) trained jointly with the baseline renderer. It does not require ray marching, volumetric integration, or massive MLP decoders per ray, which keeps training time much closer to 3DGS/MCMC and far below Zip-NeRF.\n\n- Some theoretical grounding.\nThe appendix offers two arguments:\n(i) more contextual features (neighborhood + coordinates) can only improve the achievable optimal predictor’s error;\n(ii) purely Gaussian splatting may require exponentially many primitives to drive MSE down, motivating a learned residual head instead of brute-force Gaussian proliferation."}, "weaknesses": {"value": "- Lack of qualitative results.\nThere are many tasks for this paper, but only a few qualitative comparisons are provided by the author. It is hard for me to fully evaluate the performance of QNN based on the current results. Especially in NVS task, providing rendered videos will largely enhance this work.\n- Effect size and statistical robustness.\nMany gains in PSNR/SSIM on SR and NVS are in the +0.1–0.3 dB / +0.01 SSIM range. This is meaningful but not dramatic, so variance matters. The current draft does not clearly communicate per-scene standard deviations or multiple random seed evaluations.\n- Novelty vs. existing coordinate-aware refinement nets.\nThe paper positions QNN as a new primitive. But conceptually, adding (x,y,...) coordinates as extra channels to a conv net and asking it to learn a residual correction on top of a coarse render is reminiscent of CoordConv-like ideas and of post-render refinement heads seen in prior SR / NeRF variants. The authors acknowledge that QNN reduces to known special cases (pure CNN if you drop queries, coordinate CNN if you drop some parts), which somewhat blurs how much is genuinely new versus systematized.\n- Baseline fairness/training protocol transparency.\nThe NVS table mixes “Reported,” “Reproduced,” and “+QNN” numbers. It’s unclear to what extent the baselines (3DGS, MCMC) were retrained under exactly the same hyperparameters and optimization schedule as the QNN-augmented version, or whether the baselines were tuned as hard as QNN. This matters because a 0.2–0.4 dB PSNR bump could come from better optimization rather than architectural superiority.\n- Definition of ‘low-frequency’ vs ‘high-frequency’.\nThe paper repeatedly relies on this split (e.g., “low-frequency splatted image from 3DGS” vs. “high-frequency residual predicted by QNN”), but the formalization is scattered across sections and tasks. A more consistent, quantitative definition (e.g., specific band-limited cutoff or residual construction) would make the paper easier to follow, and would help future researchers reproduce the same decomposition."}, "questions": {"value": "These are the major questions that will affect my ratings:\n- Qualitative results.\nCan you provide more qualitative results for each task? If there is a way to provide video during rebuttal, the results of NVS will be more convincing.\n- Variance / statistical significance.\nFor Tables 1–3 and the NVS table: please report per-scene or per-seed variance. Are the ~0.2–0.4 dB PSNR gains statistically consistent across scenes, or dominated by a few?\n- High-frequency decomposition.\nIn NVS you describe the QNN as predicting a high-frequency residual that gets added back to the splatted image. How exactly is that residual defined across datasets and tasks? Is it literally GT - 3DGS_render per pixel? If so, is there any regularization to keep QNN from hallucinating view-inconsistent detail across viewpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WWGmGKUfHz", "forum": "j8Cz0jPvXW", "replyto": "j8Cz0jPvXW", "signatures": ["ICLR.cc/2026/Conference/Submission14056/Reviewer_xHFh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14056/Reviewer_xHFh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563249145, "cdate": 1761563249145, "tmdate": 1762924541494, "mdate": 1762924541494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes queried convolutional neural networks (QNNs) as a signal representation analogous to implicit neural representations (INRs), but designed to improve ability to fit high frequency signals (ie mitigate spectral bias). The architecture is similar to a standard CNN, except that the QNN takes as input not only an image but also its encoded queries (pixel coordinates). Experiments apply QNNs to 1D and 2D regression, 2D super-resolution, and novel view synthesis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main idea of augmenting an INR with low-frequency image features, or augmenting a CNN with coordinate features, makes sense as a way to combine the strengths of CNNs and INRs. The discussion of related work is clear and fairly complete (with one exception noted as a question). The quantitative results in table 2 show large PSNR improvements by adding QNN. I also appreciate the inclusion of a theoretical result relating the expressive capacity of QNNs and CNNs. The novel view synthesis experiments use a large suite of datasets and baseline methods, and training times are reported. I also appreciate the honest mention of a key limitation (the requirement of neighborhood information) in the conclusions."}, "weaknesses": {"value": "Many of these weaknesses should be read as suggestions to improve the paper presentation. Some are weaknesses with respect to the motivation and experimental setup. Listed in rough order of appearance, not by importance.\n- Line 042 cites Rahimi & Recht, 2007, in the context of neural networks for processing 1D data. This strikes me as a bit misleading, since that paper does not involve neural networks.\n- Figure 1, line 87, and line 140 mention a combination of the proposed Queried CNN (QNN) with 3DGS, but since these two methods use completely different representations (and since QNN operates in 2D while 3DGS operates in 3D) it is not clear at these points in the paper how they would be combined.\n- Line 124 suggests that implicit representations of images are a key aspect of diffusion-model-based super-resolution. However, the two papers cited in this line do not involve diffusion models. It is true that both diffusion models and implicit representations can be used for super-resolution, but the current framing makes these approaches seem more intertwined than they are (or at least, than they are in the papers cited).\n- The sentence on line 129 is also a bit misleading. It makes it seem like the continuous nature of the NeRF representation is what enables its high fidelity. However, a few sentences later the paper notes (correctly) that 3DGS works just fine at the same task; since 3DGS uses a discrete representation, it is surely not the continuity of NeRF’s representation that explains its performance.\n- From section 4.2 and Theorem 1 it seems that part of the paper’s contribution is a theoretical investigation of the predictive power of QNNs compared to MLPs and CNNs. This should be mentioned as a contribution in the bullet point list at the end of the introduction, which as written left me surprised to see any theory in the paper. \n- Some more specific comments on Theorem 1: (i) I’m not sure why feature map 1 is described as an MLP. That description makes it sound like a standard INR, but the actual definition of feature map 1 is just some approximation of the true function, not including the coordinates as input. It would be preferable to include some common INR in the theorem statement, and if that is not feasible then I suggest renaming feature map 1 from MLP to just “approximation” or something along those lines, as there is not necessarily an MLP involved in f^low. (ii) Line 248 introduces the theorem saying that “we perfectly approximate the target function”--this strikes me as a bit of an overstatement. The theorem states that the minimum mean squared error achievable by a QNN is zero, but as QNN optimization is still nonconvex there is no guarantee that a QNN will actually achieve this minimum error. (iii) The theorem focuses on the case of 1D input and 1D output; that should be noted when the theorem is introduced, not just in the theorem statement.\n- Line 206-208 argues that QNN’s structure of retaining the spatial dimensions of the input and output is an advantage over typical implicit neural representations (INR), because it avoids the need for reshaping. This argument does not make much sense to me, because reshaping is a fairly efficient operation, and there are clear benefits to operating on individual coordinates because (i) it means an INR can be used in an inverse problem where each measurement involves only a portion of the signal, without evaluating the entire signal for each measurement update, and (ii) it means an INR can be used to represent a signal in arbitrary dimensions, with negligible architectural changes. Perhaps a more convincing argument for this property of QNNs is that the neural network need only be evaluated once for the entire signal, rather than once per coordinate. But it does come at some cost in terms of flexibility, since it’s not obvious how a QNN would work for a 3D signal.\n- The 2D super-resolution results in Table 1 show very marginal improvement over baselines. It’s not clear if the model sizes and training times were comparable between methods, so I can’t be confident that the small improvement is really due to the architectural change of using QNN.\n- I’m not sure I understand Figure 3. The “GT LP” signal is supposed to be a dashed line, but I don’t see a dashed line in the figure. I suspect it is because it is covered up by the predictions of the MLP, CNN, and QNN, which all seem to predict a fairly smooth function. If this is the case, it would seem that these methods are all basically learning the low-pass portion of the target, which doesn’t really support the case that QNN is mitigating spectral bias more than the other methods.\n- It is a bit confusing that the 2D super-resolution experiments train on a dataset of images, whereas all the other experiments are in the INR setting where the network is trained individually on each signal. The reasoning for including both types of experiments needs to be explained; currently even the fact that these experiments are of different types is not very obvious (I only noticed because of the description of the training data in section 5.2, and the footnote on page 2).\n- The wording for the dataset description around line 350 is not clear, specifically the term “residual image.” I think this is saying that the QNN takes a 3DGS splatted image (and its coordinates) as input, and the output is compared against a mip-NeRF rendered image. But the wording sounds like the target is a residual image? I am also confused more broadly by the setup of this experiment, which seems to treat 3DGS images as a low-frequency approximation of Mip-NeRF images…usually 3DGS produces more detailed images than Mip-NeRF, so this seems backwards to me. Also since the QNN operates on images, using it as a postprocessing step (which I think is what is proposed) could break the 3D consistency that both 3DGS and Mip-NeRF enforce.\n- As much as I appreciate the extensiveness of the experiments reflected in table 3 and table 4, and that the QNN was trained alongside the main radiance field to encourage 3D consistency (unlike in table 2), the quantitative results themselves show only marginal improvement (less than 0.5 db of PSNR) by adding QNN."}, "questions": {"value": "This is neither a strength, weakness, nor question, so I am listing it here. The main idea of the method bears some similarity to part of the patch-based diffusion method proposed in https://arxiv.org/abs/2406.02462. While the goal and architectures differ between that paper and this one, the idea of concatenating the image pixel coordinates to the image itself as input to a neural network, is used in that paper. It is sufficiently different as to not impinge on the novelty of this work, but I suggest mentioning it to acknowledge that idea.\n\nThere are a few questions embedded in the “weaknesses” section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uz7UaS4IJf", "forum": "j8Cz0jPvXW", "replyto": "j8Cz0jPvXW", "signatures": ["ICLR.cc/2026/Conference/Submission14056/Reviewer_Vfyj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14056/Reviewer_Vfyj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793000098, "cdate": 1761793000098, "tmdate": 1762924541062, "mdate": 1762924541062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal for Qonvolution"}, "comment": {"value": "We thank all the reviewers for their thorough and thoughtful feedback. We are happy that reviewers find QNN has:\n- Conceptually clean (R1, R2), simple (R2, R3), general (R1) and unified (R4) idea.\n- Thorough experiments (R1, R2) across multiple data domains (R1, R2, R3, R4).\n- Elegant theoretical justification (R2, R3, R4).\n- Strong gains for 2D regression (R3) and 3D NVS tasks (R2, R3).\n- Clear ablations (R1).\n- Lightweight and modular (R4), which does not need ray marching or volumetric integration (R4).\n- Good literature survey (R3).\n- Is well-written (R1).\n- Honestly mentioned limitation (R3).\n\nWe address the reviewers' insightful concerns in our individual responses. We first answer the questions in the Questions section followed by the ones listed in the Weaknesses section. We mark the changes we made during the rebuttal in blue."}}, "id": "jNAOvuOTzd", "forum": "j8Cz0jPvXW", "replyto": "j8Cz0jPvXW", "signatures": ["ICLR.cc/2026/Conference/Submission14056/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14056/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14056/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763636222397, "cdate": 1763636222397, "tmdate": 1763636222397, "mdate": 1763636222397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Qonvolution Neural Networks (QNNs) — a new way to learn high-frequency signals by combining low-frequency image features with coordinate queries through a convolution operation. This approach aims to overcome spectral bias in neural networks. The authors test QNNs across multiple domains: 1D regression, image super-resolution, residual image regression, and novel view synthesis (NVS), showing consistent improvements. Notably, when paired with 3D Gaussian Splatting, QNNs outperform Zip-NeRF in image quality while training much faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "QNN is a neat, intuitive idea that merges the spatial inductive bias of CNNs with the flexibility of coordinate-based models. The method is simple to implement but demonstrates strong gains, especially for challenging 3D view synthesis. The theoretical result provides a clear rationale for why adding neighborhood and query information helps. Experiments are broad and well executed, with clear tables and ablations that show real benefits from each design choice. The NVS results are particularly impressive, suggesting QNNs can close the quality gap with far heavier NeRF-based systems."}, "weaknesses": {"value": "While effective, the idea feels like an incremental step over CoordConv or query-conditioned CNNs. The theoretical justification is elegant but based on idealized assumptions. The gains in simpler 1D or 2D tasks are quite small, and the role of QNN in complex architectures like SR3 isn’t deeply analyzed. The method also depends on access to low-frequency signals, limiting generalization to cases without them. Lastly, there’s little discussion of computational overhead or scalability throughout the paper."}, "questions": {"value": "- How is QNN fundamentally different from CoordConv beyond combining with low-frequency inputs?\n\n- How sensitive are results to the quality of the low-frequency signal?\n\n- Does QNN actually recover more fine-grained details, or mainly improve smoothness and consistency in outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nn4QYfBIoZ", "forum": "j8Cz0jPvXW", "replyto": "j8Cz0jPvXW", "signatures": ["ICLR.cc/2026/Conference/Submission14056/Reviewer_cc5r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14056/Reviewer_cc5r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963228133, "cdate": 1761963228133, "tmdate": 1762924540691, "mdate": 1762924540691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"Queried Convolution\", a technique for learning high-frequency details from signals with spatial coordinates.\n\nThe idea is fairly simple: instead of directly applying a convolutional network to a signal/image, we can first concatenate the signal with encoded queries (eg, coordinates). The authors demonstrate that this is useful for 1D regression, for 2D super-resolution, and as a cleanup/refinement stage for 3D NVS tasks.\n\n*Overall rating:* I'm rating the paper as a weak reject. The execution seems good and the empirical insights will be valuable for the community, but given the weaknesses I'll described below I'm not yet ready to recommend the paper for publication. I'm more than happy to revisit this rating based on response from the authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written, with a conceptually clean and general idea. I found the experiments quite thorough, and convincing for the core message of the paper: CNNs benefit from coordinates concatenated to the inputs.\n\nI appreciated that the authors evaluated QNNs across very different data domains (1D, 2D, and 3D) and provided clear ablations isolating the impact of query inclusion, convolutional context, and encoding type. The NVS application in particular is cool: treating QNN as a geometry-aware refinement stage shows that the method can complement existing generative or reconstruction pipelines."}, "weaknesses": {"value": "As the authors acknowledge in their limitations section, the QNN method is restricted to signals with spatial locality. I don't feel this is a big issue though; the same applies to standard CNNs.\n\nIf the error difference is qualitatively perceptible, it would be nice to see analysis on how certain encodings fail, e.g., whether Fourier embeddings alone disrupt spatial smoothness or result in some kind of overfitting to high frequency signals.\n\nI’m slightly worried that the result that vanilla queries outperform Fourier encodings may be misleading, since there are many ways to implement sinusoidal/Fourier features. From my read, the authors try only one variation using random projection matrices with standard deviation 10. If convenient for the authors to run, it would be helpful to see results with:\n- Per-axis Fourier features (like in original NeRF)\n- Common exponential-style frequency bands (eg, `2**n`) used in original NeRF, Vaswani et al. transformer positional encodings, RoPE, etc. \n\nConcatenating queries with Fourier features appeared to be the best approach for the 1D experiments. Given the ordering of the paper, I found it slightly confusing that the 2D experiments then only evaluated vanilla queries and Fourier feature inputs independently, and that 3D experiments then only use vanilla queries.\n\nIf these experiments are difficult to run, it might be helpful to add some discussion on caveats to this result:\n> Changing encodings from vanilla to Fourier (Tancik et al., 2020) decreases performance on both val and train sets.\n\n\nA broader concern I have about the work is that some may find the conclusions of the work, that concatenating signals with coordinates before passing them to convolutional layers is beneficial, not too surprising. I don't think this necessarily detracts from the value of the work (the empirical results are still very useful), but this is in part because this feels like a practice that exists in the field. For example, it's already common to concatenate image inputs with per-pixel ray origins and directions for generative novel view synthesis:\n- https://arxiv.org/abs/2405.10314\n- https://arxiv.org/abs/2410.17242\n- https://arxiv.org/abs/2507.10496\n- https://arxiv.org/abs/2501.18804\n\nAre the subset of these works that use CNNs already using QNNs? Discussing connections to past work like these may strengthen the QNN paper."}, "questions": {"value": "Have you considered or compared other approaches for injecting query vectors into the network? For example: AdaLN or FiLM-style approaches might be interesting alternatives to concatenation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lU0X8PMNic", "forum": "j8Cz0jPvXW", "replyto": "j8Cz0jPvXW", "signatures": ["ICLR.cc/2026/Conference/Submission14056/Reviewer_aHPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14056/Reviewer_aHPi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14056/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055899784, "cdate": 1762055899784, "tmdate": 1762924539999, "mdate": 1762924539999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}