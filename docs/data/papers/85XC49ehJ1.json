{"id": "85XC49ehJ1", "number": 1901, "cdate": 1756962165660, "mdate": 1759898179793, "content": {"title": "Mixture-of-Gaussian Evidential Learning for Uncertainty-Aware Stereo Matching", "abstract": "Stereo matching remains a challenging task due to the presence of uncertainties in real-world data, particularly in textureless, occluded, or reflective regions. While existing methods incorporate uncertainty estimation into stereo matching for better performance, they typically assume a pixel-wise unimodal Gaussian distribution. However, the depth distributions in real-world scenarios are rarely unimodal, making the single-Gaussian assumption inadequate for modeling their heteroscedastic and multimodal characteristics. We address this limitation with a new evidential learning framework that models each pixel with a Gaussian mixture distribution. Each mixture component is regularized by an inverse-Gamma prior, and the network predicts pseudo-posterior mixture probabilities, enabling principled per-component uncertainty estimation. \nWe evaluate our method on stereo matching by training on the Scene Flow dataset and testing on KITTI 2015 and Middlebury 2014. Experimental results consistently show that our approach outperforms baseline methods and achieves new state-of-the-art performance on both in-domain and cross-domain benchmarks, demonstrating the robustness and effectiveness of the proposed framework.\nThe code will be publicly released upon completion of the review process.", "tldr": "We propose an evidential Gaussian-mixture framework for stereo matching that better captures multimodal depth uncertainties, achieving state-of-the-art performance across in-domain and cross-domain benchmarks.", "keywords": ["Stereo Matching"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea4a0b33531f952121ac5aab4dc519d702160d55.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an uncertainty-aware stereo matching framework. Here, each pixel's disparity is modeled with a mixture of Gaussians, and each component is governed by a Normal Inverse Gamma prior for uncertainty estimation. The network is trained in an evidential learning paradigm rather than sampling or iterative Bayesian updates. It predicts the parameters of this mixture model, including per-component uncertainty hyperparameters and pseudo-posterior responsibilities. The authors integrate their approach with the STTR stereo transformer architecture, showing improved disparity accuracy and calibrated uncertainty estimates compared to baseline methods. Extensive experiments on Scene Flow, KITTI 2015, and Middlebury 2014 show the proposed approach's performance compared to competing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The training approach is grounded in Expectation-Maximization (EM) formulation, but the authors avoid the complexity of iterative EM by adopting an amortized EM strategy. The network has dedicated output heads that directly predict the pseudo-posterior responsibilities for each mixture component, alongside the NIG hyperparameters per component.\n\n2. Empirically, the proposed model shows sharper disparity boundaries and lower error rates than the baseline, especially in challenging regions.\n\n3. This work extends evidential learning to a mixture, i.e., the approach could separate aleatoric uncertainty (expected data noise per component) and epistemic uncertainty (variance in the estimated mean across components) for each pixel. To this end, the paper provides formulae for computing per-component aleatoric variance and overall epistemic variance from the predicted NIG parameters."}, "weaknesses": {"value": "1. A critical limitation of the proposed method is the approach model assumption that all mixture components share the same predicted mean for a given pixel disparity. In the formulation, the network predicts a single $\\gamma_i$ per pixel (the mean of the latent true disparity), while the Gaussian mixture components provide different variance scales around that mean. This effectively means the mixture captures heteroscedastic uncertainty (varying noise levels) but not multiple distinct depth hypotheses for the same pixel. Kindly clarify.\n\n2. Apart from the mixture size, other design choices are not thoroughly ablated or justified. For example, the authors include an \"incorrect evidence\" regularization term (analogous to Amini et al. approach) with a hyperparameter $\\lambda$ to penalize predictions that have high confidence but large error. However, the paper does not detail how $\\lambda$ was chosen or how sensitive the results are to this value. \n\n3. The concept of pseudo-posterior responsibilities and how they implicitly combine prior and data is a bit hard to understand. By not explicitly predicting the prior mixture weights $\\pi_k$, the network's $r_{ik}$ output is expected to encode both the prior belief and the likelihood evidence for component $k$. Nevertheless, it is not obvious to the reader what prior is assumed for the mixture weights.\n\n4. Why are the mixture components segmented by depth (as observed in Figure 4)? The paper notes this empirical finding but doesn't delve into why the model chooses to align components with depth layers or edges. Is it because disparities at similar depths share similar uncertainty patterns (possibly due to similar local textures or occlusion states)? Such a discussion to understand the model's behavior is missing.\n\n5. The ideas presented in the paper share quite a similar motivation to that presented in Liu et al.'s icml 2024 paper on stereo risk and Liu et al.'s cvpr 2023 paper on multivariate Gaussian. Missing such a discussion with recent papers with a similar motivation. Kindly point out the difference between this work and theirs."}, "questions": {"value": "Q1. How are the mixture responsibilities ($r_{ik}$) handled in terms of prior assumptions? Since the network directly outputs posterior-like responsibilities without an explicit $\\pi_k$, did you effectively assume a uniform prior over mixture components? \n\nQ2. Can you clarify the interpretation of mixture weights as \"pseudo-posterior\" and how prior evidence is reflected? The network's output $r_{ik}$ combines prior and likelihood evidence – does this mean that when the model is unsure (e.g., in very ambiguous pixels), the responsibilities default to a roughly uniform distribution (as a prior would). In contrast, in confident cases, one $r_{ik}$ is near 1? Detailing an example of how $r_{ik}$ behaves in practice would help.\n\nQ3. How the current model behaves compared to the current Foundation model for zero-shot stereo matching, i.e., FoundationStereo cvpr 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8zl9JSufA0", "forum": "85XC49ehJ1", "replyto": "85XC49ehJ1", "signatures": ["ICLR.cc/2026/Conference/Submission1901/Reviewer_eeUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1901/Reviewer_eeUX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837632330, "cdate": 1761837632330, "tmdate": 1762915937723, "mdate": 1762915937723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Mixture-of-Gaussian Evidential Learning (MOG EL) framework for stereo matching with uncertainty. It models depth using a Gaussian mixture, regularized by an Inverse-Gamma prior. The network predicts parameters to handle complex noise in real-world scenarios. Tests on the Scene Flow, KITTI 2015, and Middlebury 2014 datasets demonstrate top performance in disparity accuracy and uncertainty, especially in challenging regions like boundaries and occlusions. The paper also includes robustness tests, such as component analysis and adversarial perturbation evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly motivated, making it easy to understand the author's approach and reasoning.\n2. The method achieves state-of-the-art (SOTA) performance on the Scene Flow dataset, with visual comparison results outperforming existing methods.\n3. The mathematical and theoretical analysis of the proposed method is thorough.\n4. The paper provides a comprehensive discussion of existing related work.\n5. The motivation and framework diagrams effectively showcase the author's innovations and improvements."}, "weaknesses": {"value": "1.The experiments are limited, with testing conducted only on the Scene Flow dataset, and only cross-domain evaluation on the Middlebury 2014 and KITTI 2015 datasets. It is recommended to include results from at least one mainstream online benchmark, such as Middlebury, KITTI, or ETH3D.\n2.The ablation study lacks direct comparisons with baseline models and the single-Gaussian method. It does not clearly demonstrate that the core innovation is the primary source of performance improvement.\n3.As seen in Figure 6(b), when the model's confidence is low, the proposed method shows larger errors than the EL framework method. This issue is not sufficiently analyzed.\n4.In Table 2, the optimal number of components for the Gaussian mixture reaches its best performance around 20, and the performance remains stable around 2 as the number increases. Is this phenomenon related to the dataset? A deeper analysis of this experimental result is missing."}, "questions": {"value": "Please answer my questions in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Su3phdaP7z", "forum": "85XC49ehJ1", "replyto": "85XC49ehJ1", "signatures": ["ICLR.cc/2026/Conference/Submission1901/Reviewer_KYM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1901/Reviewer_KYM6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874609360, "cdate": 1761874609360, "tmdate": 1762915937426, "mdate": 1762915937426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Mixture-of-Gaussians (MoG) evidential learning framework for uncertainty-aware stereo matching. Instead of the common single-Gaussian likelihood per pixel, each pixel’s disparity is modeled as a Gaussian mixture whose components have different variances to capture heterogeneous noise patterns (boundaries, occlusions, textureless regions). A Normal–Inverse-Gamma (NIG) prior is imposed on each component, and the network predicts component responsibilities and NIG parameters in a single forward pass, yielding estimates of both aleatoric and epistemic uncertainty. The method is plugged into an STTR-style stereo backbone and evaluated on Scene Flow (train) with testing on KITTI and Middlebury, reporting improved accuracy and better-calibrated uncertainty, including cross-domain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A mixture likelihood modeling matches depth ambiguity at edges, occlusions, and low-texture. The qualitative observation that uncertainty clusters around such regions is well-motivated for stereo matching pipelines. \n- Predicting NIG hyper-parameters per component provides closed-form uncertainty, which is practical for dense stereo."}, "weaknesses": {"value": "- Unclear justification for the inverse-Gamma prior: The paper adopts an inverse-Gamma prior for variance following standard evidential regression but does not theoretically motivate why this choice appears to be appropriate in the proposed hierarchical mixture setting. In a mixture model, other priors (e.g., log-normal or inverse-Wishart for covariance) could better capture multi-component variance behaviors. The inverse-Gamma assumption is used without proper justification.\n- Ambiguous role of hierarchical structure: The hierarchical distributions are introduced as an extension of evidential learning but their necessity is not clearly established. It is not evident how the hierarchy fundamentally improves inference beyond increasing model capacity.\n- Insufficient evaluation: Although the method shows clear improvements when applied to the STTR backbone, the experimental section lacks comparisons with other stereo baselines. Additional evaluations using diverse architectures (e.g., cost-volume-based, CNN, or transformer-based models) are strongly needed to demonstrate the general applicability of the approach."}, "questions": {"value": "Beyond the points raised in the Weaknesses, the following questions should also be addressed to further clarify the contribution.\n- Cross-domain robustness: Beyond KITTI and Middlebury, how does the method address synthetic-to-real nighttime or adverse weather stereo? Any failure analyses where component variances collapse or responsibilities become diffuse?\n- Training stability: Any observed mode collapse among mixture components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gSKJre4cI5", "forum": "85XC49ehJ1", "replyto": "85XC49ehJ1", "signatures": ["ICLR.cc/2026/Conference/Submission1901/Reviewer_q5jT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1901/Reviewer_q5jT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964931597, "cdate": 1761964931597, "tmdate": 1762915937239, "mdate": 1762915937239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new stereo matching pipeline that deals with the multi-modal depth distribution in real world. Specifically, the disparity of each pixel is modeled with Gaussian mixture distribution, which is learned with Evidential Learning. The performance on several synthetic and real-world benchmarks is impressive. However, I think more thorough literature review, comparison, and deep understanding of the results are required to improve the quality of this paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using mixture-of-Gaussian to represent the complex depth distribution in real world is reasonable and interesting. \n\n2. The performance on various synthetic and real-world benchmarks is impressive. Compared to the baseline STTR, the improvement is generally explicit (however, D1-3px on KITTI is worse than STTR). \n\n3. Ablation study on number of Gaussian mixture components shows the effectiveness of incorporating multi-modal distribution."}, "weaknesses": {"value": "1. Insufficient literature review. Multi-modal distribution is already explored in stereo matching. SMD-Nets [1*] is a stereo matching method that explored mixture densities to deal with disparity discontinuity at edges. It is very relevant to the problem that MOG is trying to solve. However, it is not discussed or compared. \n\n2. On the benchmarks, I suggest adding FoundationStereo [2*], the current state-of-the-art, for comparison. \n\n3. Currently, only visualization on synthetic Scene Flow is included. More visualization on real-world Middlebury and KITTI is required, e.g. visualization of Gaussian mixture as Figure 4. \n\n4. To give a better understanding of the distribution, I recommend adding plots (x axis: disparity; y axis: probability density) of the learned Gaussian mixture distribution for different regions, e.g. textureless area, occlusions, and reflective regions. \n\n[1*] Tosi et al. SMD-Nets: Stereo Mixture Density Networks. CVPR 2021.\n\n[2*] Wen et al. FoundationStereo: Zero-Shot Stereo Matching. CVPR 2025."}, "questions": {"value": "The authors integrate MOG framework with STTR. Could the authors try other stereo backbones and see if the performance is also improved? If so, it would be a strong point that MOG framework can be robustly used as a plug-in for different backbones."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "scb00qE1rq", "forum": "85XC49ehJ1", "replyto": "85XC49ehJ1", "signatures": ["ICLR.cc/2026/Conference/Submission1901/Reviewer_owh5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1901/Reviewer_owh5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989101094, "cdate": 1761989101094, "tmdate": 1762915936999, "mdate": 1762915936999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}