{"id": "M36IXztHLF", "number": 9689, "cdate": 1758134741252, "mdate": 1759897704063, "content": {"title": "SPLiT: Popularity-Bias-Aware Online Prompt Optimization for LLM-based Recommendation", "abstract": "Large Language Model (LLM)-based recommender systems often rely on preference summaries to condense a user’s interaction history and help the model better capture the user's interests. The quality of downstream recommendations depends heavily on how accurately the preference summaries align with true preferences. However, prior work has overlooked popularity bias in these summaries, which often over-represents popular items, and thus, recommendation quality degrades. Moreover, the inherent randomness of LLMs produces summaries with varying fidelity and bias. To address this, we propose an online learning approach that identifies the most accurate and least biased preference summary. We formulate the preference summary selection task as a Contextual Bayesian Optimization with Constrained Set problem and introduce the Semantic Popularity Lift-based Preference Summary selecTion (SPLiT) framework. \nSPLiT incorporates a Semantic Popularity Lift penalty that quantifies how much a summary amplifies popularity bias. The penalty discourages selecting high-bias summaries and guides the choice toward those that better reflect the user’s true preferences.\nSPLiT significantly improves recommendation performance by mitigating popularity bias, achieving 13.8\\% higher Normalized Discounted Cumulative Gain and 6.9\\% higher Hit Rate compared with the best baseline.\nThis highlights the importance of popularity bias-aware summary selection \nfor debiasing prompt optimization, advancing fairness and accuracy in LLM-based recommender systems.", "tldr": "We propose SPLiT, an online prompt optimization algorithm that mitigates popularity bias in preference summaries, improving recommendation performance in LLM-based recommender systems.", "keywords": ["Recommender Systems", "Large Language Models", "Prompt Optimization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e023232ea586cd0c20de43522dc5ea98fb799c9d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the problem of popularity bias in recommender systems based on large language models, focusing specifically on a previously overlooked source: the bias in popularity inherent in the preference summaries generated by LLMs. The authors observe that when using LLMs to summarize user interaction histories into natural language preference summaries, the generated summaries often overemphasize popular items or categories, leading to reduced downstream recommendation quality. To address this issue, the paper proposes the semantic popularity lift metric (SPL) to quantify the degree of popularity bias in preference summaries. This metric measures the bias amplification effect by comparing the category preference distribution of a summary with the category preference distribution of user history. Based on this metric, the authors formulate the preference summary selection problem as a contextual Bayesian optimization problem over a set of constraints and propose the SPLiT framework. The core idea of this framework is to generate multiple candidate preference summaries in each recommendation round and then select the optimal summary by combining a prediction reward and an SPL penalty, where the SPL penalty disincentivizes summaries that overrepresent popular categories. Experiments are conducted on the MovieLens-1M and Last.fm datasets, randomly sampling 100 users each time, using GPT-4o-mini as the summary generation and recommendation model. Results show that SPLiT achieves a 13.8% improvement in NDCG@1 and a 6.9% improvement in HR@3 on MovieLens-1M compared to the best baseline, with similar improvements achieved on Last.fm. The paper also demonstrates that SPLiT outperforms standard Bayesian optimization baselines such as GP-UCB and BNN-UCB in terms of cumulative regret, and selects summaries with lower SPL values, demonstrating that this approach can improve recommendation accuracy while mitigating popularity bias."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel problem identification. The paper identifies a source of the problem that had been overlooked in previous research: popularity bias exists during the generation of preference summaries, not just in the final recommendation results. This insightful observation expands our understanding of the sources of bias in LLM recommender systems.\n2. Targeted evaluation metrics are proposed. The SPL metric is specifically designed for natural language preference summaries, differing from traditional popularity metrics for recommendation lists. This metric quantifies popularity bias in summaries using a semantic similarity model, providing an interpretable bias measurement mechanism.\n3. Clear problem formulation. The paper formulates the preference summary selection problem as a contextual Bayesian optimization problem over a set of constraints. The system model and problem definition are clearly structured, making the theoretical framework of the method easier to understand.\n4. Detailed experimental documentation. The appendix provides extensive experimental details, a complete baseline description, prompt word templates, algorithm pseudocode, and supplementary analysis, facilitating understanding and replicability of the work."}, "weaknesses": {"value": "1. The introduction and related work section of the article take up nearly 4 pages. The author can slightly adjust the distribution of the content to highlight the core content.\n2. The experimental scale is severely insufficient. Each experiment sampled only 100 users, the candidate set consisted of only 5 items, and the user history had only 2 to 6 interactions. This scale is far from sufficient to verify the effectiveness and reliability of the method. Real-world recommendation scenarios typically involve thousands of users and dozens of candidate items. The current experimental setup does not reflect actual application scenarios.\n3. The validity of the core metrics is questionable. Figure 5 shows that the Pearson correlation coefficient between SPL and traditional popularity lift metrics is only 0.64, and the Spearman correlation coefficient is only 0.43. These correlations are insufficient to prove that SPL can adequately capture popularity deviations. Figure 8 shows that the relationship between SPL and NDCG exhibits significant fluctuations in certain regions and is non-monotonically decreasing, undermining the credibility of the metrics.\n4. Theoretical analysis is completely lacking. The paper formulates the problem as an optimization problem but does not provide any convergence proof, theoretical bounds on cumulative regret, or optimality guarantees. As an optimization method, this lack of theoretical support is a serious flaw, and its performance in different scenarios cannot be guaranteed.\n5. The rationale for the method design is insufficient. The paper adopts a strategy of generating multiple candidate summaries and then selecting them, but lacks sufficient justification for not directly generating low-bias summaries. The paper acknowledges that the self-refine strategy is computationally expensive and slow, requiring multiple LLM calls for each candidate. However, Appendix A.28 shows that the two-stage selection of summaries and lists does not outperform the selection of summaries alone, suggesting that there may be issues with the selection strategy itself.\n6. The baseline comparison is unfair. Methods such as WOK debias the recommendation LLM, while SPLiT debiases the preference summaries. These two methods operate at different stages, making a direct comparison inappropriate. A fair comparison should compare different selection or debiasing strategies under the same preference summaries. Ablation experiments using preference summaries without SPL penalties are lacking.\n7. The performance gains are limited and inconsistent. Table 2 shows only a 3.12% improvement on NDCG@5 and a 3.13% improvement on Last.fm. Given the complex implementation and high computational cost, the practical significance of these improvements is questionable. The magnitude of the improvements across different metrics varies significantly, with NDCG@1 improving by 13.8% and NDCG@5 by only 3.12%, lacking consistency. \n8. Unclear causal relationships. The paper fails to clearly demonstrate whether the popularity bias of summary preferences leads to poor recommendation performance, or whether other factors, such as summary quality and semantic accuracy, play a role. Table 3 only shows variability cases for three users, a sample size too small to draw reliable conclusions. Large-scale experiments with controlled variables are needed to verify the causal mechanism.\n9. Lack of hyperparameter and robustness analysis. The trade-off parameter λ is a key hyperparameter of the method, but the paper lacks sensitivity analysis and does not discuss how to choose an appropriate λ value for different datasets and scenarios. Furthermore, the experiments are conducted only using GPT-4o-mini, without testing the performance of other LLMs such as Claude or Llama. This leaves the method's robustness and generalization uncertain.\n10. Key definitions such as Eh and Epr are placed in Appendix A.11 rather than the main text, hindering readability. The paper acknowledges in Appendix A.26 that other types of biases may interact in complex ways and are not addressed by the framework. This is a significant limitation that is not fully discussed in the main text. Certain experimental details, such as the specific BNN architecture and training process, are not clearly described."}, "questions": {"value": "1. The introduction and related work section of the article take up nearly 4 pages. The author can slightly adjust the distribution of the content to highlight the core content.\n2. Currently, each round samples only 100 users, with a candidate set of only 5 and a history length of 2–6 items. Can this scale be expanded? Does SPLiT's stability and benefits remain valid when the candidate size and history length increase?\n3. How do the correlations between SPL and traditional PL (Pearson 0.64, Spearman 0.43) shown in Figure 5 (page 20) and the non-monotonic relationship between SPL and NDCG shown in Figure 8 (page 33) support SPL as a core bias metric?\n4. As an optimization method within the CBO-CS framework, can you provide assumptions under which SPLiT can achieve convergence or no-regret properties (e.g., requirements on noise, smoothness, or context distribution)? If not, can you clarify the difficulties and boundary conditions, and explain the impact of the inclusion of λ in the selection criterion on discriminability and the exploration/exploitation trade-off?\n5. The paper uses the strategy of \"generating multiple candidate summaries and then selecting them,\" but lacks justification for why it is not possible to directly generate low-bias summaries (e.g., by introducing a target SPL or control signal during generation). Could you provide a systematic comparison with the \"direct low-SPL generation\" approach and explain the specific reasons why the \"two-stage: summary + list selection\" approach in Appendix A.28 (page 32) is not superior to \"summary selection only\"?\n6. Regarding baseline fairness, could you provide additional ablation experiments? This would be more helpful in drawing conclusions. Comparing different selection/debiasing strategies (including WOK) under the same preference summary, and providing additional ablation results with λ = 0 (selection only, no SPL penalty) and removing the penalty while maintaining the same reward estimator/acquisition function, could help isolate the contribution of the SPL penalty itself.\n7. Could you provide complete cost information? Include the number of LLM calls per user, average tokens, inference time, and monetary cost, along with performance-cost curves. Also explain why NDCG@1 improves by approximately 13.8% on MovieLens, while NDCG@5 only improves by approximately 3.1%. Similar discrepancies are observed on Last.fm (Table 2, page 9 and Table 5, page 30). What is the significance of this \"better at the top, limited overall\" revenue model in real-world deployments?\n8. To clarify the causal relationship between \"whether summary popularity bias leads to performance degradation,\" additional ablation experiments would be more illuminating. For example, while minimizing the semantic quality and readability of the summary, only manipulate the SPL (for example, rewrite the same summary to preserve semantics to change the popularity weight) and observe the changes in recommendation quality. Furthermore, additional failure/counterexample analysis should be conducted on a larger scale, rather than just the small number of user cases in Table 3.\n9. Key hyperparameters and robustness are still unclear. Can the sensitivity of λ, the number of candidates n, and the structure and training configuration of the reward estimator M be clarified? Reproduce the experiment on different LLMs (such as Claude and Llama) and different STS models, comparing performance and SPL changes, to demonstrate that the method is critical to the generalization ability of the model and corpus transfer.\n10. To improve readability and reproducibility, it is recommended to move core definitions such as Eh and Epr from Appendix A.11 (pages 19–20) to the main text and provide intuitive explanations; complete the BNN structure and training details (prior/posterior approximation, input features, number of layers/width, optimizer, learning rate and number of training rounds, etc., see A.23 pages 28–29); at the same time, clarify the statistical caliber and online update range of the global popularity θ(γ), so that readers can easily reproduce it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v7JSn7EBV8", "forum": "M36IXztHLF", "replyto": "M36IXztHLF", "signatures": ["ICLR.cc/2026/Conference/Submission9689/Reviewer_taZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9689/Reviewer_taZE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394483513, "cdate": 1761394483513, "tmdate": 1762921203477, "mdate": 1762921203477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the *popularity bias* introduced in the preference summaries used by LLM-based recommender systems. To address this issue, the authors formulate a new problem termed **CBO-CS** and propose **SPLiT**, a framework designed to mitigate popularity bias in such systems. Specifically, SPLiT introduces a **Semantic Popularity Lift (SPL)** metric to quantify the popularity bias of summaries and selects LLM-generated summaries that better align with user interests while exhibiting lower bias, based on both SPL and accuracy rewards. Empirical results on two datasets, each containing hundreds of sampled users, demonstrate the effectiveness of SPLiT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**S1: Intuitiveness of SPL.** The proposed **Semantic Popularity Lift (SPL)** is an intuitive metric that quantifies the relative improvement in popularity bias between generated preference summaries and the original user contexts. Since popularity bias is a critical issue in recommender systems, SPL provides a clear and interpretable way to assess how well the model mitigates this bias.\n\n**S2: Reasonable Methods.** The design of SPLiT is reasonable and easy to follow. It employs rewards that account for both accuracy and popularity bias, and adopts a modular recommendation pipeline, as illustrated in Figures 1 and 2."}, "weaknesses": {"value": "**W1: Limited Experiment Scale.** The paper only samples 100 users from the MovieLens-1M dataset during evaluation. Some analysis experiments in the Appendix (e.g., A12 and A29) also sample only hundreds of users. Such a small user scale is insufficient to reliably assess recommendation performance. Experiments on the full dataset or with at least thousands of users would make the evaluation more convincing.\n\n**W2: Limited Dataset Diversity.** Only two datasets are used for evaluation, which restricts the generalizability of the results. Including one or more additional datasets would provide stronger evidence of the robustness and applicability of the proposed approach.\n\n**W3: Outdated Baselines.** The most recent baseline listed in Table 1 was published in 2016 [1]. Incorporating more recent methods, especially those based on large language models or recent recommendation frameworks, would yield a fairer and more meaningful comparison.\n\n**W4: Appendix Organization.** The Appendix is not well-structured and could benefit from clearer organization. For example, separating it into sections such as *Additional Experiments*, *Implementation Details*, and *Related Works* would improve readability and help readers locate specific information more easily.\n\n[1] Courbariaux M, Hubara I, Soudry D, et al. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1[J]. arXiv preprint arXiv:1602.02830, 2016."}, "questions": {"value": "**Q1: Implementation Details.** The paper does not specify the backbone model used to initialize the policy. Please clarify the exact backbone model to ensure reproducibility and transparency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4DMUxOb8I8", "forum": "M36IXztHLF", "replyto": "M36IXztHLF", "signatures": ["ICLR.cc/2026/Conference/Submission9689/Reviewer_pvoe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9689/Reviewer_pvoe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830625749, "cdate": 1761830625749, "tmdate": 1762921203189, "mdate": 1762921203189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily focuses on the disproportionate emphasis placed by LLM-based recommendation systems on popularity. This is a critical factor affecting the recommendation performance of LLM recommendation systems, presenting significant challenges. The authors conducted a detailed analysis of its underlying causes. Building upon prior research, it explores an aspect largely overlooked by most studies: summary LLM. The author innovatively proposed the SPLiT model and the CBO-CS optimization method. The SPL metric employed by the former serves as an extension addressing the limitation that the frequency-based evaluation metric PL, previously used for recommending LLMs, cannot be generalized to textual content. The latter creatively proposed an online resolution strategy that incorporates the evolving history of user interactions, making it more aligned with real-world scenarios. The author validated the model using two public datasets, achieving the best performance across all four selected metrics and demonstrating approximately a 10% improvement over previous work. The final conclusion demonstrated the feasibility of the plan and outlined prospects for future work. My overall impression is that the experiment is solid and rigorous, though I perceive some shortcomings in the details that could potentially be improved. The work tackles an extremely challenging problem, and the author demonstrates a thorough understanding of recent high-quality literature in the field."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "a. This paper addresses the issue of excessive emphasis on popular content in LLM-based recommendation systems and innovatively proposes a solution tailored for summary LLM. The overall conceptual innovation is sufficiently substantial. \n\nb. In the algorithm design for problem-solving, the use of SPL as an equivalent replacement for the previously studied PL demonstrates thorough and comprehensive proof, while simultaneously overcoming the challenge of measuring popularity across texts relative to individual words. \n\nc. The application of the CBS-CS method transforms past offline approaches into online methods. These represent significant innovations."}, "weaknesses": {"value": "a. The primary shortcomings of the article lie in the arrangement of its textual content structure and the failure to present certain experimental details in greater specificity. For example, the placement of Figure 1 appears before the detailed explanation of the recommendation system based on the summary LLM and recommendation LLM, which may cause some confusion in understanding the article. I suggest moving the figure to after the background section or before the paragraph introducing the summary LLM. \n\nb. Next is the perspective on mathematical notation. The symbols in section 4.3 are overly complex, and some lack explanation, making the overall presentation difficult to grasp. I believe the formula representation could be slightly simplified to avoid appearing cluttered."}, "questions": {"value": "a. My primary concerns lie in the experimental section, particularly regarding the evaluation of experimental conclusions. While the authors utilize some established evaluation metrics from prior work, as this introduces a novel model with significantly increased computational demands, I believe they should have analyzed the model's runtime. If the recommendation time is excessively long, its widespread adoption would be hindered. \n\nb. Furthermore, the authors mention the cold-start problem faced by early recommendation systems at the outset, yet their own designed algorithm also lacks historical data during its initial phase. I suspect it may also encounter cold-start and strong-get-stronger issues (where certain metrics are amplified from the outset). The authors provide no explanation for this. Even though the conclusions and prior experiments are comprehensive, I believe the paper should supplement its content with analysis addressing these concerns to make the work more thorough."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wVbbL7mRpP", "forum": "M36IXztHLF", "replyto": "M36IXztHLF", "signatures": ["ICLR.cc/2026/Conference/Submission9689/Reviewer_BBZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9689/Reviewer_BBZG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903509272, "cdate": 1761903509272, "tmdate": 1762921202941, "mdate": 1762921202941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Discusses prompt optimization in the context of a traditional (movie/music) recommendation system. The work assumes that a prompt will contain a user interaction history, and of course a candidate list. The only component we are optimizing is a summary of user preferences included in the prompt to provide additional information. The summary is produced from the user interaction history through a separate summary generation and selection process. Within this context, the paper proposes and evaluates SPLiT, a prompt selection method that minimizes popularity bias.\n\nThe paper is well written and readable, although I was sent to read the appendix for critical details more often than I would like. SPLiT is part of a larger system where most of the components are not novel, so it's hard to see the benefit in the context of the overall system. There is no ablation study. There are lots of baselines, but as far as I can see none of these baselines is Figure 2 with the summary component removed. As far as I can see the only LLM used in the experiments is GPT-4o-mini. For what they are worth, table 2 show a solid improvement over the baselines they used."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "LLM based recommendation is an important topic. Paper is readable, even if I have to go to the appendix too much. There's nothing super novel, but there are new elements. Certainly with additional experiments the paper may be publishable, even if its not making the strongest contribution. We just need clarity on what that contribution is."}, "weaknesses": {"value": "It's difficult to tease out what is really novel from the overall system. At the very least, we would need an ablation study. Remove the bottom half of the system in Figure 2 (and adjust the template) what happens?"}, "questions": {"value": "The key question is given in the \"weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "an6t00QbPa", "forum": "M36IXztHLF", "replyto": "M36IXztHLF", "signatures": ["ICLR.cc/2026/Conference/Submission9689/Reviewer_Sipr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9689/Reviewer_Sipr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9689/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762118700657, "cdate": 1762118700657, "tmdate": 1762921202658, "mdate": 1762921202658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}