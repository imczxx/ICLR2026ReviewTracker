{"id": "mMB3Y1ERqi", "number": 21498, "cdate": 1758318289951, "mdate": 1759896918893, "content": {"title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning", "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan–execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.", "tldr": "SimuAgent is a lightweight LLM agent trained with reflection-guided reinforcement learning that converts Simulink models into compact Python dictionaries, plans and executes modeling tasks, and outperforms GPT-4o on the new SimuBench benchmark.", "keywords": ["Large Language Models", "Simulink Modeling", "Reinforcement Learning", "Intelligent Agent", "Engineering Automation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a641b4919a86d41c9f3b16486e7a6a1e52d2bec4.pdf", "supplementary_material": "/attachment/6278296ed8b8f3ca8ff7412bccab1f8e908050d3.zip"}, "replies": [{"content": {"summary": {"value": "This work addresses the challenge of applying LLMs to Simulink code generation, which differs substantially from traditional coding tasks. Simulink adopts a hierarchical, graphical paradigm with complex block diagrams, signal routing, and strict topological constraints. These characteristics impose significant challenges for LLMs: requiring them to respect rigid graph structures, handle very long contexts, and suppress hallucinations to produce reliable and interpretable code.\nThe authors propose an RL-based approach to mitigate these challenges. In particular, ReGRPO leverages tool-invocation feedback to guide model learning, effectively improving code quality through iterative reinforcement. The proposed framework demonstrates competitive performance across multiple experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The model adopts a two-stage curriculum learning strategy to handle complex tasks, fostering higher-order capabilities such as planning, abstraction, and modular composition.\n2. The Abstract–Reconstruct mechanism alleviates data scarcity while ensuring the structural integrity and accuracy of the generated outputs.\n3. The ReGRPO component enhances model performance through tool-based reflection and reinforcement, enabling more consistent reasoning."}, "weaknesses": {"value": "1. While the experimental results are promising, the method appears limited in scalability. The Abstract–Reconstruct loop does not introduce new reward signals, meaning that improvements still rely heavily on the model’s inherent abilities. The authors require to include ablation studies showing how performance varies with different data scales.\n2. The ReGRPO mechanism may be susceptible to reward hacking. Without proper supervision of the reflection phase, the model could exploit shortcuts, e.g., performing unnecessary or repetitive reflections to maximize reward. The paper would benefit from a more detailed discussion or empirical analysis of this issue."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "we2ezGQm1m", "forum": "mMB3Y1ERqi", "replyto": "mMB3Y1ERqi", "signatures": ["ICLR.cc/2026/Conference/Submission21498/Reviewer_BYXp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21498/Reviewer_BYXp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913729098, "cdate": 1761913729098, "tmdate": 1762941806237, "mdate": 1762941806237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SimuAgent, an LLM-based agent to automate and assist with Simulink modeling and simulation tasks. The claimed contribution is a Python-dictionary representation for Simulink models, although that's nowhere to be found in the core text. Supposedly, it improves interpretability of Simulink models."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Interesting problem, certainly high industry impact"}, "weaknesses": {"value": "- Basically zero scientific novelty. This is an engineering project without many generalizable takeaways.\n- Presentation is inconsistent and unclear what the actual contribution is: toolbox, method, architecture, benchmark... All of these are claimed in the paper, but unclear which one is it. For some reason, it is claimed that a \"Python-based model representation,\" which is a dictionary, is a contribution. Certainly not for a top conference. It supposedly improves interpretability. This obviously cannot be true as visual modeling languages, such as Simulink's causal-block diagrams, have superior interpretability for humans; and serialization into a JSON file achieves the same affect as serializing into whatever other file format as there's no added semantic information.\n- No real evaluation. Without evaluating this on various classes of Simulink models, the utility of the approach remains questionable."}, "questions": {"value": "No questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UHjfHoMaRZ", "forum": "mMB3Y1ERqi", "replyto": "mMB3Y1ERqi", "signatures": ["ICLR.cc/2026/Conference/Submission21498/Reviewer_hZi9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21498/Reviewer_hZi9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926442257, "cdate": 1761926442257, "tmdate": 1762941805945, "mdate": 1762941805945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SimuAgent, an LLM-based agent for constructing, modifying, and querying Simulink models. The core contributions are (1) a compact, Python-dictionary representation of Simulink models that reduces token usage and enables fast in-process validation and debugging, (2) a two-stage staged training curriculum (execution --> planning) augmented with a self-supervised Abstract–Reconstruct data augmentation loop, and (3) Reflection-GRPO (ReGRPO) i.e., an extension of Group Relative Policy Optimization that injects automatic self-reflection traces to provide intermediate textual feedback for sparse-reward, long-horizon tasks. The authors also release SimuBench, a 5,300-task, multi-domain benchmark for LLM-based modeling (creation, editing, QA). Experiments with Qwen-2.5-7B show that SimuAgent (Stage1+Stage2 + ReGRPO) converges faster and attains the best overall accuracy on SimuBench (51.89% average), narrowly outperforming a GPT-4o XML/image baseline (50.45%). Ablations analyze the contribution of ReGRPO, curriculum stages, augmentation, group sizes, and reflection schedules; failure analysis pinpoints typical error modes (topology, block selection, parameter omission, premature termination, context limits)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of Reflection-GRPO with Simulink tool feedback is a notable contribution. The agent leverages intermediate reflection traces and programmatic validation signals (e.g., structural checks, execution feedback, block-level errors) to guide long-horizon updates. This mechanism improves sample efficiency, stabilizes training under sparse rewards, and provides a general recipe for scaling RLHF-style methods to complex tool-using domains beyond text-only reasoning.\n\n- The Python-dictionary representation, in-process validation testbed, and tool integration directly tackle the large number of tokens, slow MATLAB engine loops, and debugging friction. These are crucial for deployment in model-driven engineering and show a practical system design that addresses real engineering issues in designing such automation.\n\n- The authors provide many controlled ablations (stage curriculum, reflection schedules, group sizes, reward shaping, LoRA, model scale) and a failure taxonomy that identifies limitations.\n\n- The paper provides 5.3k multi-domain tasks (models + schematics + QA), filling a benchmarking gap for graphical model automation and enabling reproducible comparisons."}, "weaknesses": {"value": "- The Introduction section is very well-written and effectively motivates the need for an automation agent for Simulink. However, the proposed method and experimental sections lack critical implementation details and could be substantially improved through better organization. For instance, in the architecture description (Section 3), it would be far more informative if the pipeline stages were presented sequentially, explaining the order of operations and data flow, rather than only listing the tool’s individual features.\n\n- The tool processes a natural language description to create, modify, or query Simulink models. It does so by first prompting an LLM to produce a step-by-step plan, which is then translated into a Python dictionary representation. These dictionaries are subsequently converted into executable Simulink commands (e.g., adding blocks, setting parameters). However, it remains unclear how the semantic fidelity of the plan to the original NL description is ensured during inference. While the Python-based executor can validate syntax, it does not guarantee semantic alignment or correctness of the generated plan.\n\n- For the results in Table 2, particularly for the modification task, there is an inconsistency in input modalities. Competing SoTA models (e.g., GPT-4o) receive an NL prompt along with an image or XML input, whereas SimuAgent operates on a Python dictionary representation. For a fair comparison, all models should be evaluated under identical input formats and testing conditions, or at least the differences should be clearly justified and analyzed."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8NENYhrsn", "forum": "mMB3Y1ERqi", "replyto": "mMB3Y1ERqi", "signatures": ["ICLR.cc/2026/Conference/Submission21498/Reviewer_mz4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21498/Reviewer_mz4J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213526278, "cdate": 1762213526278, "tmdate": 1762941805673, "mdate": 1762941805673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SimuAgent for LLM based agent for modeling in Simulink. Key contributions are (1) using the python dictionary structure rather than XML or other token heavy schemes, (2) a two-stage curriculum with a a selfsupervised Abstract–Reconstruct loop and (3) an algorithm ReGRPO that deals with the sparse reward nature of the long horizon problem using self-generated textual reflection traces. It also intruces SimuBench, a large-scale dataset of tasks and show that a Qwen2.5-7B model trained with their pipeline outperforms other baselines (GRPO and GPT-4o). The paper also shows ablation studies to show effect of the two-stage training, reflection and VAE style abstract-reconstruct augmentation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper effectively frames the problem, shows how previous methods (XML) lead to a large number of tokens, and showcases Python-dictionary representation as a suitable choice\n- Reflection and retry is a simple mechanism to tackle the sparse reward issue of just having the output of 0/1 at the end of the episode.\n- The SimuBench dataset provides examples over various system-design domains. \n- The paper is well written, has done extensive experiments, with multiple ablations and transfer to other similar platforms, solidifying the contribution. The figures and plots add to understanding."}, "weaknesses": {"value": "- The algorithm is only compared with GRPO. How does the method compare to other baselines for LLM tool-use and RL?\n- Improvements on generic NLP benchmarks are small, code-based tasks show more gain, but SimuBench is the setting where reflection is most helpful.\n- More methodological clarifications on reward structure, prompt differences for image-based inputs are needed."}, "questions": {"value": "1. How are the different terms in the reward structure weighted?\n2. What minimal hardware is needed for inference/deployment? The manuscript only describes training GPUs and claims laptop-grade GPUs.\n3. How can we compare the multi-modal inputs/prompts used for the GPT-4o with the other models?\n4. Why does setting the algorithm to Always reflect hurt performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8g9t2GADUk", "forum": "mMB3Y1ERqi", "replyto": "mMB3Y1ERqi", "signatures": ["ICLR.cc/2026/Conference/Submission21498/Reviewer_Ehok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21498/Reviewer_Ehok"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227842263, "cdate": 1762227842263, "tmdate": 1762941805197, "mdate": 1762941805197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}