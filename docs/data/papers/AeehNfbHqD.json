{"id": "AeehNfbHqD", "number": 6948, "cdate": 1758003123872, "mdate": 1759897882247, "content": {"title": "Improving Human-AI Coordination through Online Adversarial Training and Generative Models", "abstract": "Being able to cooperate with diverse humans is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is a promising method that allows dynamic data generation and ensures that agents are robust. It creates a feedback loop where the agentâ€™s performance influences the generation of new adversarial data, which can be used immediately to train the agent. However, adversarial training is difficult to apply in a cooperative task; how can we train an adversarial cooperator?\nWe propose a novel strategy that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method \\textbf{GOAT}: \\textbf{G}enerative \\textbf{O}nline \\textbf{A}dversarial \\textbf{T}raining. In this framework, the GOAT dynamically searches the latent space of the generative model for coordination strategies where the learning policy---the Cooperator agent---underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by keeping the generative model frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.", "tldr": "", "keywords": ["multi agent", "adversarial training", "zero-shot coordination", "human-AI interaction", "cooperation", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6abf02cd2914e58a8eb926bc1027270974fdb483.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the problem of training a cooperative agent in a two player cooperative Markov game. It combines a generative procedure for sampling cooperative partner agents with an adversarial procedure for finding agents whose strategies challenge a learning agent to train an agent that is robust to diverse partner strategies while playing cooperatively and maximizing returns.\n\nThe generative procedure uses a VAE similar to previous work. The adversarial procedure works by finding embedding values that would lead to partner agents being sampled by the generator that maximize regret. This regret is calculated by calculating the difference between self-play scores and cross-play scores with the learning agent.\n\nEvaluation is done on a toy matrix game, a reaching environment, and two overcooked environments. Evaluation with human participants in overcooked show that the proposed technique, GOAT, outperforms evaluated baselines in terms of dealing with human strategies."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper combines ideas from previous works in a clear and fairly principled manner.\n* The idea is explained well. Figure 1 is a good overview of the technique. Section 3 is succinct but clear, and the figures referring to the experiments are also clear and fairly well explained. Some caveats here will be expounded on the weaknesses section\n* Human evaluations give the idea a lot of credence. The idea seems to be performing as advertised in the overcooked domain.\n* The human evaluation procedure is well designed.\n* Appreciate the error bars and their explanation in Figure 4.\n* As far as I can tell, all required relevant work seems to be cited."}, "weaknesses": {"value": "* Figures 5 and 6 are a little hard to follow. After reading the text in the experiments and intuiting the intention, I am able to guess what the authors are trying to show. Figure 6 can be understood well in this manner. But Figure 5 was still a little confusing due to the two different dots for GAMMA and GOAT in addition to the gradient colored dots for the episode numbers. If Figure 5 can be simplified it would get the point across much better.\n* Nit about how the value function is defined at the end of the first paragraph of Section 3: Value functions are generally conditioned on the state. Perhaps better to phrase this expression as expected returns?\n* Figure 3 was also a little difficult to look at. But I appreciate the summation across methods in the last row. Perhaps this row can be separated or highlighted so a reader can look at the summary first before going through the detailed comparison for each heuristic agent. The description would also benefit from mentioning that the maximum sum would be `11`.\n\n\nMinor: Line 83, the citation should be a `\\citep`"}, "questions": {"value": "* In the paragraph preceding equation 4, the paper claims that self-play is a valid proxy for optimal score without actually stating that this is a proxy. Would self play actually be optimal in all scenarios? I can imagine scenarios where the two agents need very different policies in order to act optimally. E.g. being when agents need to fulfill different roles. Perhaps this paragraph needs to be phrased more carefully."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P9WGDMjjgd", "forum": "AeehNfbHqD", "replyto": "AeehNfbHqD", "signatures": ["ICLR.cc/2026/Conference/Submission6948/Reviewer_HUTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6948/Reviewer_HUTe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409074795, "cdate": 1761409074795, "tmdate": 1762919176937, "mdate": 1762919176937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present GOAT, a method that generates adversarial agents to train a cooperator agent for Human-AI Coordination. The method first utilizes a GAMMA-style VAE to encode an agent population in latent space. Next, the authors propose to train an Adversary to transform the GAMMA latents to Adversarial embeddings, which is then used by the GAMMA decoder to simulate an adversarial agent. The Cooperator Agent and Adversary are jointly trained together in a minimax objective where the adversary tries to maximize the regret (SP to XP gap) between the adversary and Cooperator agent. The authors then evaluate GOAT on 3 cooperative environments, a matrix game, a cooperative reaching game and Overcooked-AI."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The authors provide a intuitive solution to a long-standing problem in the Human-AI/ZSC community, that of generating viable adversarial cooperative agents without sabotaging effects.\n- The paper is very well written and structured."}, "weaknesses": {"value": "- The authors cites two recent/concurrent related works [1] and [2] but did not compare GOAT with the two methods in the experimental sections. It will be interesting to see how GOAT compares to the two methods especially ROTATE as they also proposes a regret like objective to generate a curriculum of adversarial agents to train a cooperator agent.\n- GOAT is evaluated on fairly simple, fully observable grid-world environments which limits its generalizability to more complex environments such as those with partial observable states [3] or continuous actions [4]. \n\n[1] Wang, C., Rahman, A., Cui, J., Sung, Y., & Stone, P.  ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork. arXiv preprint arXiv:2505.23686.\n\n[2] Villin, V., Buening, T. K., & Dimitrakakis, C.  A Minimax Approach to Ad Hoc Teamwork. In Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (pp. 2105-2114).\n\n[3] Gessler, T., Dizdarevic, T., Calinescu, A., Ellis, B., Lupu, A., & Foerster, J. N. OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination. In The Thirteenth International Conference on Learning Representations.\n\n[4] Kang, X., Lee, S. W., Liu, H., Wang, Y., & Kuo, Y. L.  Moving Out: Physically-grounded Human-AI Collaboration. arXiv preprint arXiv:2507.18623."}, "questions": {"value": "- In Figure 5, the authors show that the adversarial latent vectors sampled by GOAT is significantly far away from the cluster of GAMMA latent vectors. As I understand, the GAMMA latent vectors can be interpreted as an interpolation between actual agent trajectories encoded in latent space during the VAE training process. Does this imply that GAMMA could extrapolate new agents out of the standard trained distribution?\n- Why is REINFORCE chosen over PPO to train the Adversary?\n- The authors state that the main limitation of GOAT is the reliance on a trained VAE model. Could the authors comment on what might the minimally viable population size/amount of data to train a viable VAE model for GOAT?\n- Relatedly, does the type of agents (CoMeDi vs MEP) matter when it comes to VAE training?  Does it affect the quality of latent vectors of the VAE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sNvQav7319", "forum": "AeehNfbHqD", "replyto": "AeehNfbHqD", "signatures": ["ICLR.cc/2026/Conference/Submission6948/Reviewer_hn3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6948/Reviewer_hn3Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728899855, "cdate": 1761728899855, "tmdate": 1762919176573, "mdate": 1762919176573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of learning cooperative agents that are robust to a diverse distribution of human and artificial agents. The models uses a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. The policy itself tries to minimize regret, this induces an adversarial setup, but the challenge is to not the adversary be adversarial _and_ cooperative - two conflicting objectives. If the adversary is not constrained to such policies, then it can become overly combative, resorting to sabotage instead of making the policy robust and foolproof. In contrast to zero-sum games like chess where self-play can improve the performance of the policy by exploiting weaknesses against itself, in cooperative games there is no such incentive for a policy. \nTo prevent cross-play performance from self-sabotaging, the model essentially samples adversarial players from a learned distribution of agents using a variational autoencoder. this limits the capability for self-sabotage since the agents are sampled from a predefined distribution. This makes the contribution of the work relevant to the community."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths**\n1. The VAE model and generative policy allows for sampling policies parametrically with a smooth latent space, preventing expensive zeroth-order methods to sample policies. This makes the optimization objective more tractable and inexpensive. This idea is also used in controllable image generation and editing where latent variables are optimized to achieve a test-time objective with a pretrained network. \n2. The formulation is pretty straightforward - given a cooperator policy $\\pi_C$, the adversarial policy chooses a partner policy $\\pi_P$ to minimize the value function with cross play of the cooperator and partner, while maximize the self-play performance of the partner policy - this is presumably to prevent selecting incompetent policies. This is also interpreted (accurately) as the regret of choosing a cooperative policy over itself in a cross-play scenario. Regret for bad partner policies will be lower (due to low first term in Equation (4)) than a good partner policy that does not work well with the cooperator policy. \n3. The experiment setup is satisfactory - including a cooperative matrix game allows easy visualization of one-step policies and analytically verifying optimal adversarial policies for a given cooperative policy, a 5x5 cooperative reaching game with more complexity but still tractable, and a hard game with a larger action space (Overcooked) with real-time dynamics on which performance is also shown with human cooperative players. \n4. Extensive analysis with related methods - GAMMA and MinMax is shown in RQ4 and RQ5. Figure 5 shows the adversarial objective drifting to new policies that significantly deviate from the normal distribution (in the projection space), showing that the model is indeed aiming to find harder partner policies."}, "weaknesses": {"value": "1. The robustness of the policy depends on the coverage / support of the autoencoder and its simulated policies. The paper uses fixed agent populations with training methods (Line 178).\n2. The paper is not self-sufficient in terms of mentioning how the distribution of partner policies are learned before the adversarial sampling is performed. \n3. Figures 5 and 6 are nice, but they also show that the policies deviate a lot from random normal as training progresses. Since the generative model works off a VAE, are the policies that have a latent so far from the normal distribution meaningful at all? Is a regularization apart from the self-play regret (as mentioned in Line 722) enough to constrain the space of sampled policies?\n4. Figures 5 and 6 also show that the coverage of partner policies is very concentrated and lacks diversity - could it lead to a possible oscillation of the cooperative policy if certain partner policies are not compatible with each other? \n\nMinor nits: \n1. Line 75: \"The regret objective proves effective because it challenges the learning agent with a curriculum of increasingly difficult, yet still feasible tasks.\" - Regret minimization by itself does not motivate a curriculum learning approach. This line could say something like \"it challenges the learning agent and expands the frontier / coverage of harder yet feasible state configurations\""}, "questions": {"value": "**Questions**\n\n1. Since the distribution of the partner policies is learned using a VAE, why cant the cooperative agent be trained to minimize regret over the entire distribution of partner policies (by sampling partner policies from the VAE) instead of using an adversarial method to sample partner policies? Is that going to be slower or faster than the proposed training in terms of environment interactions and training time? \n2. Line 376 - \"We hypothesize this is because the generative model, trained on cooperative trajectories, encodes a latent space rich in strategic variations.\" - the paper posits that the generative model encodes a rich space of strategic variations is what leads to fast convergence - this approach is also used in other prior work (some of which are mentioned in the paper). Does GOAT perform better than prior work leveraging generative policies due to its adversarial sampling? \n\nMinor questions:\n1. Line 45/74: Why is Adversary capitalized? \n2. uncommon notation for distribution, i.e. $\\Delta$ is used. This could be replaced with a better notation in my opinion\n3. Line 363: \"PBT methods rely on simulated populations of self-play partners, are often computationally expensive, and have poor coverage of actual human behaviors.\"  - why is cross play and adversarial training less expensive than self play? I thought the tradeoff was to spend more compute on adversarial training to have better mode coverage of adversarial policies. Figures 4b and 4e show that GOAT is indeed faster but is there any argument or justification as to why that is the case, especially because adversarial training can be slow and brittle."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1tnELdcUzJ", "forum": "AeehNfbHqD", "replyto": "AeehNfbHqD", "signatures": ["ICLR.cc/2026/Conference/Submission6948/Reviewer_Zoti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6948/Reviewer_Zoti"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6948/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124680525, "cdate": 1762124680525, "tmdate": 1762919176208, "mdate": 1762919176208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}