{"id": "I4meJN28Ol", "number": 9635, "cdate": 1758131505260, "mdate": 1759897707643, "content": {"title": "CellDuality: Unlocking Biological Reasoning in LLMs with Self-Supervised RLVR", "abstract": "\\begin{abstract}\nDeveloping generalist large language models (LLMs) capable of complex biological reasoning is a central challenge in computational biology. While existing LLMs excel at predictive tasks like cell type annotation and logically-constrained problems, enabling open-ended and mechanistic reasoning remains a challenge. A promising direction is Reinforcement Learning from Verifiable Rewards (RLVR), which has been shown to significantly enhance complex reasoning in general domains like mathematics and code synthesis. However, its application in biology is hindered, as most biological outcomes are non-verifiable. For example, verifying a generated gene sequence is usually infeasible. In this paper, we introduce CellDuality, a self-supervised framework that enables LLM agents for robust reasoning in single-cell biology. Our framework is built on the principle of complementary task duality, a self-verification process that leverages a bidirectional reasoning loop. First, the model performs a forward reasoning task by predicting a biological outcome (e.g., a cell's response to a drug). Then, in a complementary inverse task, it must reason backward from its own prediction to reconstruct the initial conditions (e.g., the original drug perturbation). The fidelity of this reconstruction serves as an intrinsic reward signal, creating a feedback loop that enforces logical and biological consistency. We use these intrinsic rewards to align the base LLM via reinforcement learning, without requiring ground-truth verification labels. We demonstrate that CellDuality achieves state-of-the-art performance and provides coherent biological explanations across a diverse suite of single-cell reasoning tasks. Critically, on the challenging out-of-distribution perturbation prediction benchmark, our self-supervised approach significantly outperforms the standard fine-tuning baseline and narrows the performance gap to a supervised RLVR baseline. Our work showcases a new path toward scalable training of biological foundation models.", "tldr": "We introduce a self-supervised RL framework that trains LLMs for biological reasoning by rewarding the consistency of a forward-inverse task loop.", "keywords": ["Reinforcement Learning", "Biological Reasoning", "Foundation Models", "Single-Cell Biology"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/128b83a0a7c2b71b9364c3c6463ad59776812f59.pdf", "supplementary_material": "/attachment/5410937e53032aa7d9a2dd71d710fd672151bcd3.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes CellDuality, a self-supervised RL-framework that enables large language models (LLMs) to perform open-ended biological reasoning in single-cell analysis. The key idea is to make sure each biological reasoning task such as drug response prediction is paired with an inverse task such as  recovering perturbation from response, forming a bidirectional reasoning loop. The consistency between the forward and inverse predictions is used as an intrinsic reward signal without labeling or annotation, enabling RL alignment without ground-truth verification labels. Experiments across four representative single-cell reasoning tasks demonstrate that CellDuality achieves competitive or slightly better performance compared to SFT baselines and narrows the gap to fully label-supervised RLVR models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed self-supervised RL training paradigm is training-efficient and agnostic to the choice of policy optimization algorithm, showing that consistent rewards can provide alignment signals without external supervision.\n- The Complementary Task Duality concept is clearly formulated and grounded in prior work (eg., DuPO), providing a natural extension to domains where canonical verifiable rewards are not available or expensive to acquire."}, "weaknesses": {"value": "- The self-consistency reward effectively measures semantic or reconstruction fidelity rather than true biological correctness. While this provides a stable and well-defined signal, it may not inject new knowledge or enable deeper reasoning as canonical verifiers do in math, code, or symbolic domains. In this sense, the biological reasoning learned may remain shallow.\n- Across multiple evaluation cases (Tab 2-6), CellDuality shows seemingly marginal gains over its SFT baseline and does not consistently outperform strong supervised or specialized models. This raises doubts about the real effectiveness of the proposed self-supervised alignment compared to simpler fine-tuning approaches and how is applied to other relevant domains in biology.\n- The methodological novelty appears limited relative to DuPO (She et al., 2025), with CellDuality serving primarily as an instantiation in the single-cell domain rather than a substantial conceptual advancement."}, "questions": {"value": "- How many samples or optimization steps are passed for a complete RL stage? Could the authors annotate such information in Figure 2 (a)(b)? \n- Regarding the differences between the proposed self-consistency reward and ground-truth reward, what if the model predict the wrong label (X <> Y) during rollout sampling? Should the model learn useful knowledge or will this cause the model being enforced to predict the wrong label? \n- What is the base model and performance for CellDuality in Table 2-5? Line 377 indicates this is LLama-3.2-3B. Could the authors add the performance of base model before SFT for better comparison in these tables? \n- Could the authors attach the exact text prompts for constructing the CellDuality in the appendix, as a common practice, to enhance the reproducibility?\n- Could you also show some failure modes when the model exhibits degenerated consistency (eg., mutually consistent but biologically incorrect predictions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rN9MI0ciSd", "forum": "I4meJN28Ol", "replyto": "I4meJN28Ol", "signatures": ["ICLR.cc/2026/Conference/Submission9635/Reviewer_nVH5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9635/Reviewer_nVH5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709917381, "cdate": 1761709917381, "tmdate": 1762921167002, "mdate": 1762921167002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CellDuality, a self-supervised reinforcement learning framework for training large language models to perform biological reasoning in single-cell analysis. The method leverages Complementary Task Duality, where forward (primal) and backward (dual) reasoning tasks generate intrinsic rewards that enable alignment without ground-truth supervision. Extensive benchmarking on classification and generative tasks is also present."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper presents a technically sound and well-motivated framework that adapts reinforcement learning with verifiable rewards to inherently non-verifiable biological problems.\n2) The principle of complementary task duality is conceptually elegant, providing a self-consistent mechanism for intrinsic reward generation without labeled data.\n3) The experimental results are extensive, covering multiple benchmark datasets, and the ablation studies convincingly isolate the contribution of the self-supervised RL stage."}, "weaknesses": {"value": "1) The study lacks an explicit evaluation of biological interpretability: while metrics demonstrate numerical improvements, there is no assessment of whether reasoning outputs capture biologically meaningful mechanisms.\n2) The framework’s scalability and computational cost are not detailed, especially regarding the RL alignment phase, which could be a practical limitation.\n3) The proposed approach is validated only on transcriptomic data; extending it to multimodal biological datasets would strengthen its generality claims."}, "questions": {"value": "I would ask the authors to address the weaknesses listed above:\n1) Please include biologically grounded evaluations, such as verifying whether the model’s reasoning outputs (for example, perturbation responses) are consistent with known pathways or regulatory relationships.\n2) Provide an analysis of computational cost and scalability, including training time and hardware requirements for the RL alignment stage.\n3) Please discuss how the framework could be extended to multimodal datasets (for instance, integrating ATAC-seq or proteomic data) to further validate the generality of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hvlx2jGeWo", "forum": "I4meJN28Ol", "replyto": "I4meJN28Ol", "signatures": ["ICLR.cc/2026/Conference/Submission9635/Reviewer_vKnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9635/Reviewer_vKnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906262998, "cdate": 1761906262998, "tmdate": 1762921166623, "mdate": 1762921166623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Summary:\nThis paper introduces CellDuality, a self-superivsed framework to enhance the biological reasoning ability of LLM. It tries to address the challenge that it’s hard to build reward functions for most biological outcomes as it is non-verifiable. The framework uses complementary task duality. The method has achieved SOTA performance without needing ground-truth verification labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pros:\n- The paper introduces CellDuality, a new self-supervised method to enhanve biological reasoning ability of LLMs\n- The method has achieved SOTA performance compared with baseline methods."}, "weaknesses": {"value": "Cons:\n- The “cell sentences” representation used in the method is a simplified representation discarding the actual expression value. There should be better way to encode single-cell transcripomics data.\n- The claim of unified framework may be a bit too broad, as it only includes 4 downstream tasks with limited settings. \n- The proposed method is not consistently outperform baselines methods plus the additional reinforcement learning only bring very marginal benefits."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JSwOcyJNOK", "forum": "I4meJN28Ol", "replyto": "I4meJN28Ol", "signatures": ["ICLR.cc/2026/Conference/Submission9635/Reviewer_Kuj3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9635/Reviewer_Kuj3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960370494, "cdate": 1761960370494, "tmdate": 1762921166362, "mdate": 1762921166362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}