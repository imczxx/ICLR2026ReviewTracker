{"id": "qdtdDX18GE", "number": 15545, "cdate": 1758252570414, "mdate": 1763137375637, "content": {"title": "RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical Image Segmentation with SAM 2", "abstract": "Segment Anything Model 2 (SAM 2) is a prompt-driven foundation model that extends SAM to both image and video domains, demonstrating superior zero-shot performance over its predecessor. While SAM 2 builds on SAM's success in medical image segmentation, it retains limitations such as binary mask outputs, lack of semantic label inference, and reliance on precise prompts for target object identification. Moreover, applying SAM and SAM 2 directly to medical image segmentation tasks often yields suboptimal results. In this paper, we investigate the upper performance limit of SAM 2 using custom fine-tuning adapters and ground-truth prompts, achieving a Dice Similarity Coefficient (DSC) of 92.30% on the BTCV dataset, surpassing the state-of-the-art nnUNet by 12%. To address prompt dependency, we explore multiple prompt generation strategies and introduce a UNet that autonomously predicts masks and bounding boxes, which are then used as input to SAM 2. Dual-stage refinements within SAM 2 further improve performance. Extensive experiments demonstrate that our method achieves state-of-the-art results on the AMOS2022 dataset, with a 1.4% Dice improvement over nnUNet, and outperforms nnUNet by 6.4% on the BTCV dataset.", "tldr": "", "keywords": ["SAM 2", "Automatic Prompt", "Refinement", "Medical Image Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0425662ff3635d4d57990b72b45e1d136415b628.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a medical image segmentation framework that uses SAM2 as a segment refiner. In this paper, SAM2 is employed to further refine the segmentation results of nn-UNet, but the performance improvement is not significant."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1.This paper provides detailed explanations in certain sections, particularly regarding the SAM2 framework's workflow and the ablation experiments.\n2.The research motivation of this paper is clear."}, "weaknesses": {"value": "1.The structure of the paper is unclear. The methods section mixes in a large portion of the ablation study, such as the Frame Selection Strategies and the Prompt Generation sections. The methods section should clearly and explicitly describe the methods used, while other experimental attempts should be  part of the ablation study in the experiments section.\n2.The overall method lacks detailed illustration. There should be a clear diagram illustrating how the proposed method processes 3D images. Figure 2 only shows the workflow for processing a single 2D image, while other components are described separately in different sections. For example, Figure 4 outlines the Frame Selection Strategy and Prompt Generation, but the method framework in Figure 4 (4f) still shows the workflow for processing a single 2D image. The separate illustrations create significant difficulties for readers in understanding the overall methodology of the paper.\n3.The training strategy is unclear. The paper lacks necessary training details of components such as UNet and DWConvAdapter.\n4.Most importantly, the performance improvement brought by the proposed method is not significant. As shown in Table 5, despite incorporating a considerable number of components and processes, the proposed method only improves performance by about 1% over the  nn-UNet used in Step 0."}, "questions": {"value": "When the amount of data is insufficient to train a robust baseline network (like the nnU-Net used in this paper), does the method proposed in this paper could achieve better refinement results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P18ICRRKf7", "forum": "qdtdDX18GE", "replyto": "qdtdDX18GE", "signatures": ["ICLR.cc/2026/Conference/Submission15545/Reviewer_BiLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15545/Reviewer_BiLJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902238904, "cdate": 1761902238904, "tmdate": 1762925822295, "mdate": 1762925822295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hVmOzT3Q54", "forum": "qdtdDX18GE", "replyto": "qdtdDX18GE", "signatures": ["ICLR.cc/2026/Conference/Submission15545/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15545/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763137374861, "cdate": 1763137374861, "tmdate": 1763137374861, "mdate": 1763137374861, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **RFMedSAM 2**, an automatic prompt refinement framework for **Segment Anything Model 2 (SAM 2)**, designed to enhance volumetric (3D and multi-slice) medical image segmentation performance. The method first explores the upper performance limit of SAM 2 through **custom fine-tuning adapters** and **ground-truth prompts**, achieving a Dice Similarity Coefficient (DSC) of **92.30%** on the BTCV dataset — a **12% improvement** over nnUNet. Then, an **Automatic Prompt Generation Module** is introduced, where a UNet autonomously predicts masks and bounding boxes as prompts for SAM 2. Within SAM 2, a **Dual-stage Refinement** mechanism iteratively improves the segmentation results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed automatic prompt generation and dual-stage refinement pipeline effectively reduce SAM’s dependence on manual prompting, moving toward fully automatic medical image segmentation.  \n2. The experimental validation is comprehensive, covering multimodal 3D medical datasets (CT/MRI), including BTCV and AMOS2022, and comparing against strong baselines such as nnUNet and MedSAM.  \n3. The method consistently surpasses current state-of-the-art models, achieving **+6.4% Dice improvement** on BTCV, demonstrating clear practical gains."}, "weaknesses": {"value": "1. The core pipeline mainly relies on combining and fine-tuning existing models (SAM 2 and UNet) with limited theoretical novelty.  \n2. Details regarding the Bayesian search and dual-stage refinement — including parameter settings, loss functions, and training strategies — are insufficient, affecting reproducibility.  \n3. The combination of automatic prompting and dual-stage refinement in SAM 2 may introduce significant inference-time overhead, yet the paper does not report runtime or memory cost analysis.\n4. The code is not open-sourced, which limits reproducibility and community verification."}, "questions": {"value": "1. Can the UNet-generated prompts generalize across imaging modalities, and how does the system handle low-contrast or complex anatomical regions?  \n2. Please clarify whether the training and evaluation settings for nnUNet and MedSAM are strictly consistent to ensure fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G70HbNwkwU", "forum": "qdtdDX18GE", "replyto": "qdtdDX18GE", "signatures": ["ICLR.cc/2026/Conference/Submission15545/Reviewer_BGKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15545/Reviewer_BGKL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927439852, "cdate": 1761927439852, "tmdate": 1762925821598, "mdate": 1762925821598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Segment Anything Model 2 (SAM 2) to the medical domain by introducing an automatic prompt generation and refinement framework. The authors first train a U-Net to generate initial masks and bounding boxes that serve as prompts for SAM 2. They further enhance SAM 2 with depth-wise convolutional adapters in the attention blocks, CNN-Adapters in the FPN module, and modifications to the frame selection and memory attention mechanisms. Evaluations on the BTCV and AMOS datasets demonstrate that the proposed method achieves leading performance, outperforming several strong automatic segmentation baselines as well as prompt-based SAM variants."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear motivation: it identifies SAM 2’s key limitations, including prompt dependency, binary mask outputs, and lack of semantic understanding, and systematically addresses them through architectural and training refinements.\n2. The paper demonstrates solid engineering, with well-designed modules and thorough ablation studies that clearly support the claimed improvements.\n3. The method achieves competitive performance, outperforming other SAM-based models on both BTCV and AMOS datasets when prompts are available."}, "weaknesses": {"value": "1. All evaluations are conducted only on abdominal CT datasets, limiting the generalizability of the findings.\n2. The overall framework heavily depends on the U-Net’s outputs for prompt generation, which makes it susceptible to error propagation—mistakes from the U-Net cannot be effectively corrected by the subsequent refinement stages.\n3. Following 1 and 2, since the U-Net already performs better than many competing methods, it is unclear how much the proposed system contributes beyond the U-Net itself. Additional evaluations on more challenging datasets where the U-Net performs poorly would strengthen the claims.\n4. The method’s performance is comparable to other automatic segmentation approaches despite its more complex architecture. A detailed analysis of computational cost and efficiency would help justify the added architectural complexity."}, "questions": {"value": "1. Could the authors evaluate the method on more diverse datasets, particularly on harder modalities or anatomical regions where the U-Net baseline performs poorly, to better demonstrate generalization and robustness?\n2. The novelty appears limited, as the improvements primarily stem from architectural and engineering refinements rather than fundamentally new algorithmic contributions. Could the authors clarify what conceptual advancements distinguish this work from prior SAM adaptation studies?\n3. The paper is dense with implementation and ablation details, which may obscure the main contributions. The authors could consider simplifying or streamlining the presentation, or expanding the work into a journal version where the technical depth would be more appropriate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GoXgoldOqP", "forum": "qdtdDX18GE", "replyto": "qdtdDX18GE", "signatures": ["ICLR.cc/2026/Conference/Submission15545/Reviewer_N4s2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15545/Reviewer_N4s2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934788230, "cdate": 1761934788230, "tmdate": 1762925820953, "mdate": 1762925820953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RFMedSAM 2, an extension of SAM 2 for medical image segmentation. The framework integrates an auxiliary UNet that automatically generates mask- and box-based prompts for SAM 2, enabling prompt-free segmentation. It further introduces depth-wise convolutional adapters and CNN-adapters to better capture spatial information while keeping the number of trainable parameters low. The model performs two refinement stages within SAM 2 to enhance accuracy. Experiments on AMOS22 and BTCV datasets show consistent gains over nnUNet and existing SAM-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a thorough empirical study of SAM 2’s behavior on medical images, including analysis of prompt dependency and temporal positioning.\n- Well-designed engineering pipeline combining UNet and SAM 2, with clear modularity and reasonable computational cost.\n- Extensive ablation studies support each design choice (adapters, frame selection, refinement, prompt generation).\n- Achieves strong performance across two standard medical datasets, surpassing established baselines like nnUNet, SAMed, and SAM3D.\n- Writing and figures are clear and comprehensive, making the method easy to follow and reproduce."}, "weaknesses": {"value": "- Conceptual novelty is limited; the framework mainly adapts and combines existing components (UNet, SAM 2, adapters) without new algorithmic insight.\n- The “automatic prompt refinement” is largely implemented via a separate UNet, which reduces the claimed autonomy of the system.\n- Improvements over strong baselines (e.g., nnUNet) are modest on larger datasets and may not reflect a fundamental advance.\n- Lack of theoretical or analytical understanding of why the multi-stage refinement contributes to performance.\n- The evaluation uses only CT datasets; generalization to MRI or other modalities remains untested."}, "questions": {"value": "- How sensitive is the system to errors from the UNet-generated prompts? Would the performance drop significantly if UNet predictions are noisy?\n- How does the method perform in real-time or low-resource clinical settings, given SAM 2’s high memory requirements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNa0WUIN02", "forum": "qdtdDX18GE", "replyto": "qdtdDX18GE", "signatures": ["ICLR.cc/2026/Conference/Submission15545/Reviewer_aGLU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15545/Reviewer_aGLU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962666601, "cdate": 1761962666601, "tmdate": 1762925820273, "mdate": 1762925820273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}