{"id": "veFs5UfYq9", "number": 744, "cdate": 1756816389069, "mdate": 1763607919824, "content": {"title": "KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models", "abstract": "Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose significant challenges for deployment in resource-constrained environments.\nVector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by constructing and leveraging a codebook—where weight vectors are mapped to the most similar discrete codewords within the codebook. \nHowever, its direct application to MoEs suffers from significant performance degradation caused by two critical obstacles:  (1) redundant representation among experts leads to VQ repeatedly quantizing similar representations for each expert, resulting in inefficient utilization of the limited codebook capacity; and\n(2) cumulative outputs bias, amplified by expert aggregation, leads to distributional shifts in the quantized outputs, resulting in degraded model accuracy.\nTo this end, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. \nKBVQ-MoE introduces two lightweight and offline techniques that introduce negligible runtime computational and memory overhead:\n(1) Input-driven redundancy elimination, where a Karhunen–Loève Transform (KLT) guided singular value decomposition (SVD) extracts and shares dominant weight components across experts. \n(2) Bias-corrected output stabilization, where vector quantization is applied to expert-specific (i.e., non-redundant) representations and the quantized outputs are corrected with channel-wise affine compensation.\nExperiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For instance, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring the potential of KBVQ-MoE for efficient deployment on edge devices and other resource-constrained platforms.", "tldr": "", "keywords": ["vector quantization", "llm", "Moe"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e2be9fd82c19fb193098fbadec293a8f8bf5725.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes KBVQ-MoE, a post-training vector quantization framework for MoE LLMs combining a KLT-guided SVD to remove cross-expert redundancy (IDRE) with a lightweight channel-wise affine correction (BCOS) to stabilize outputs after VQ. The key idea is to map expert weights into an input-coherent basis via KLT, extract dominant shared components with SVD and keep them in full precision, then quantize only the expert-specific (i.e., non-redundant) representations and correct distributional shifts by mean–variance matching. The authors claim the affine correction is “not a heuristic adjustment but an unbiased MMSE-optimal estimator.”"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is clearly motivated by two obstacles in MoE VQ, including redundant representation among experts and cumulative outputs bias.\n\n* Empirical result looks promising, particularly at low bits.\n\n* The method acts as an easy-to-use plugin that improves multiple VQ baselines and yields speedups."}, "weaknesses": {"value": "* Evaluation tasks are limited. The evaluated tasks are normally for pre-trained models, but some of the models are post-trained models (e.g. Qwen3). I would love to see some post-training benchmark (e.g. MMLU, AIME24, HumanEval) results on the Qwen3 model.\n\n* Some typos: (line 756) “A.5 ALL EXPRIMENTS” → “All Experiments.”; (line 81) \"uantization\" -> \"quantization\"; (line 105) \"non-redundan\" -> \"non-redundant\"\n\n* Some of the related works on MoE quantization can be further discussed or compared with, e.g. [1] [2] [3].\n\n(I'm more than happy to increase my score if my questions are adequately addressed)\n\n[1] Kim, Y.J., Fahim, R. and Awadalla, H.H., 2023. Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness. arXiv preprint arXiv:2310.02410.\n\n[2] Li, P., Jin, X., Tan, Z., Cheng, Y. and Chen, T., 2024. QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts. arXiv preprint arXiv:2406.08155.\n\n[3] Duanmu, H., Li, X., Yuan, Z., Zheng, S., Duan, J., Zhang, X. and Lin, D., 2025. MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design. arXiv preprint arXiv:2505.05799."}, "questions": {"value": "* How is `Qwen3-30B-A3B ` evaluated? Specifically, did you enable \"reasoning\" of this model? I would love to see the quantization results on a reasoning LLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vqosa63hLR", "forum": "veFs5UfYq9", "replyto": "veFs5UfYq9", "signatures": ["ICLR.cc/2026/Conference/Submission744/Reviewer_Dyjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission744/Reviewer_Dyjg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700724432, "cdate": 1761700724432, "tmdate": 1762915595457, "mdate": 1762915595457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel quantization method, KBVQ-MoE, designed for MoE-based large language models. The method first applies the KLT and SVD to extract dominant weight structures shared across experts, thereby reducing redundancy and improving codebook utilization. Building on this foundation, it employs vector quantization for expert-specific representations and further proposes a channel-wise affine compensation module to refine the mean and standard deviation of the quantized outputs. Extensive experiments demonstrate superior performance across multiple datasets and LLM architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Figures 2 and 3 provide highly illustrative examples that effectively support the paper’s claims on redundancy extraction and channel-wise bias correction.\n2. The overall motivation is solid and clearly articulated. The proposed method is both conceptually sound and straightforward, making it easy to understand and reproduce. Moreover, it successfully addresses two critical challenges outlined in the introduction.\n3. Table 5 convincingly demonstrates that the proposed IDRE and BCOS modules can be seamlessly integrated into other MoE quantization approaches, highlighting the method’s flexibility and potential for broad applicability."}, "weaknesses": {"value": "1. Some mathematical symbols should be revised (e.g., matrices should be boldfaced). Additionally, minor grammatical and formatting issues should be corrected, and certain technical details could be described more clearly.\n2. The contribution of each component within the IDRE and BCOS modules remains unclear due to the lack of quantitative results and in-depth analysis in the ablation study.\n3. The comparison with other MoE-based compression methods, such as EAC-MoE, D2-MoE, and SubMoE, which are illustrated in the related work, should be included for a more comprehensive comparison."}, "questions": {"value": "1. Is the IDRE technique applied exclusively to router experts or to all experts? The notation in Eqs. (1) and (3) suggests it concerns router experts, while the workflow in Eq. (2) seems to encompass all experts. Clarification on whether the symbol $n$ refers to different concepts across equations would be helpful.\n2. What is the rationale for maintaining the shared structure at full precision while quantizing only the expert-specific weights? Why not quantize the shared weights as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lZfNHMBBNq", "forum": "veFs5UfYq9", "replyto": "veFs5UfYq9", "signatures": ["ICLR.cc/2026/Conference/Submission744/Reviewer_v8EJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission744/Reviewer_v8EJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722768161, "cdate": 1761722768161, "tmdate": 1762915594884, "mdate": 1762915594884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KBVQ-MoE, a vector quantization framework designed specifically for compressing Mixture-of-Experts (MoE) language models. The method addresses two key challenges: (1) redundant representations across experts that waste codebook capacity, and (2) cumulative output bias from quantization errors that get amplified through expert aggregation. The approach combines Input-Driven Redundancy Elimination (IDRE), which uses KLT-guided SVD to extract and preserve shared components at full precision, with Bias-Corrected Output Stabilization (BCOS), which applies vector quantization only to expert-specific weights and corrects distributional shifts via channel-wise affine transformations. Experiments on models like Qwen and Mixtral show strong performance at 2-3 bits, with 3-bit Qwen1.5-MoE-A2.7B achieving near-FP16 accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a novel contribution by adapting vector quantization specifically for MoE architectures. The identification of expert redundancy and amplified quantization bias as key bottlenecks is insightful, and the KLT-guided SVD approach creatively aligns weight decomposition with input activation statistics.\n2. The technical approach is sound with theoretical justifications provided in the appendices. The experimental evaluation is comprehensive, covering multiple MoE models with thorough ablations. Results show consistent and meaningful improvements at ultra-low bit-widths (2-3 bits), where baseline methods struggle significantly. The modular design demonstrates compatibility with existing VQ methods, increasing practical impact."}, "weaknesses": {"value": "1. The paper mentions \"negligible\" computational overhead but provides limited quantitative analysis. How long does the KLT-SVD calibration take compared to standard VQ? What is the actual inference-time cost of the channel-wise bias correction operations? These practical considerations matter for deployment. Could you provide some results on this, like time cost of quantization method.\n2. The baseline methods, especially MoEQuant, show surprisingly poor performance at 2-bit in Table 1 (e.g., W2 of 583542 for Qwen1.5). More discussion on this failure would help.\n3. The choice of truncated rank k=n/128 appears empirically driven from Table 4 but lacks theoretical justification. While the ablation shows diminishing returns beyond this point, why this specific ratio is optimal across different models and tasks remains unclear. The paper would benefit from analysis connecting rank selection to properties of the expert weight matrices or input distributions.\n4. The evaluation relies on simple zero-shot reasoning tasks (ARC, HellaSwag, PIQA, etc.) that may not fully capture model capabilities. Including more challenging benchmarks like MMLU, MATH, and code generation tasks (MBPP, EvalPlus) would better demonstrate the method's effectiveness across diverse domains.\n5. Table 1 only shows 2-bit and 3-bit results. Adding 4-bit and 8-bit comparisons would provide a more comprehensive results, especially since 4-bit quantization is common in practice."}, "questions": {"value": "1. typo: Qwen1.5-Moe-A2.7B->Qwen1.5-MoE-A2.7B"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KcKVjHebNl", "forum": "veFs5UfYq9", "replyto": "veFs5UfYq9", "signatures": ["ICLR.cc/2026/Conference/Submission744/Reviewer_TstW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission744/Reviewer_TstW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793171625, "cdate": 1761793171625, "tmdate": 1762915594745, "mdate": 1762915594745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a vector quantization approach targeted for mixture of expert layer. The idea is to extract redundant representation across experts and keep it in higher precision while expert specific components are vector quantized. Within vector quantization of expert specific components, scaling and bias is applied to improve quantization. The paper is well written and easy to understand. The techniques proposed are intuitive and I appreciate the authors providing actual hardware speedup numbers. The paper is missing some recent non linear quantization baselines and comparison at iso-compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is easy to understand and well written.\n2. Technique proposed is intuitive."}, "weaknesses": {"value": "1. The paper is missing comparison with recent non linear quantization baselines : VPTQ (https://arxiv.org/abs/2409.17066), AQLM (https://arxiv.org/pdf/2401.06118), QUIP (https://arxiv.org/pdf/2307.13304), QUIP# (https://arxiv.org/pdf/2402.04396), SqueezeLLM (https://arxiv.org/pdf/2306.07629), GPTVQ (https://arxiv.org/pdf/2402.15319),  etc.\n2. Among the baselines presented, the compression achieved by various techniques is missing. \n3. Iso-compression results are missing.\n4. Evaluation on complex tasks is missing : math understanding, coding, reasoning, long context abilities, etc."}, "questions": {"value": "1. How does this approach compare with ResQ (https://openreview.net/pdf?id=4qIP1sXcR1)? Although ResQ does activation quantization as well and integer quantization of weights, it also uses eigen value decomposition to isolate high precision components. How does this approach compare with using IDRE proposed in this work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zcX0jBopBv", "forum": "veFs5UfYq9", "replyto": "veFs5UfYq9", "signatures": ["ICLR.cc/2026/Conference/Submission744/Reviewer_XsNC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission744/Reviewer_XsNC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947267521, "cdate": 1761947267521, "tmdate": 1762915594464, "mdate": 1762915594464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KBVQ-MoE, a vector quantization framework tailored for ultra-low-bit compression of Mixture-of-Experts (MoE) large language models (LLMs). The study is motivated by the substantial performance drop observed when conventional quantization methods are directly applied to MoE architectures. In particular, KBVQ-MoE mitigates expert redundancy and output bias through input-driven redundancy elimination and bias-corrected output stabilization mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and convincing, highlighting MoE-specific issues of expert redundancy and output bias.\n\n2. The proposed KBVQ-MoE is validated on several representative MoE architectures, demonstrating consistent improvements."}, "weaknesses": {"value": "1. While IDRE and BCOS are ablated individually, there is no fine-grained study of codebook size sensitivity.\n\n2. Lack more advanced or concurrent MoE-aware compression baselines (e.g., D2-MoE, SubMoE mentioned in related work).\n\n3. The evaluation of computational efficiency is insufficient. The paper only reports a simple “Decoder speed test” in Table 6, without providing detailed analysis of computational or memory overhead.\n\n4. The core motivation of the paper lies in the claim that redundancy elimination and bias correction help stabilize expert output distributions; however, the supporting evidence (e.g., Fig. 2–3) is insufficient. It would be more convincing if the authors compared other methods reported in Table 1 to quantitatively validate the claimed effect.\n\n5. The paper’s presentation lacks rigor in notation and consistency. For instance, the dimension oc in Step 2 is undefined, and the superscript in Equation (3) for the routing expert is unexplained. There are also typos (e.g., uantization → quantization) and inconsistent use of MoE/moe.\n\n6. The paper does not explicitly discuss the limitations of the proposed method."}, "questions": {"value": "Given that the calibration set contains only 256 samples from RedPajama, could the authors provide ablation results on calibration size to verify its representativeness for computing reliable KLT statistics and bias correction factors? Additionally, how do the authors ensure that no potential data leakage occurs during calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t810g73iMx", "forum": "veFs5UfYq9", "replyto": "veFs5UfYq9", "signatures": ["ICLR.cc/2026/Conference/Submission744/Reviewer_7GqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission744/Reviewer_7GqJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993672642, "cdate": 1761993672642, "tmdate": 1762915594281, "mdate": 1762915594281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}