{"id": "4NYdDrRPSc", "number": 10783, "cdate": 1758181845051, "mdate": 1759897628957, "content": {"title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation", "abstract": "Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning.", "tldr": "", "keywords": ["Mixture-of-World-Models", "Embodied Planning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23df204e445645681157bf6310723d6f2a6d120b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a mixture-of-world model that integrates pixel-level and latent-level world models. The pixel-level and latent-level features are concatenated and processed in a residual manner (L229). Experiments on the CALVIN benchmark demonstrate that the proposed world model enhances the performance of the diffusion policy, achieving SOTA results on CALVIN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of combining pixel-level and latent features is compelling.\n- The authors propose a simple yet effective strategy to fuse these two distinct types of features.\n- Experiments on the CALVIN benchmark demonstrate that the proposed world model helps the diffusion policy achieve SOTA performance."}, "weaknesses": {"value": "- The fusion of pixel-level and latent features appears to be heuristic, and while it proves effective, several questions remain. For instance, why is this particular fusion strategy sufficient for feature integration? Could it be extended to other types of latent representations or real-world images?\n\n- The paper lacks quantitative analysis of the MoWM. The authors only provide quantitative results for the robot policy. It would be valuable to understand the reasons behind the significant performance improvements achieved by using MoWM."}, "questions": {"value": "- What is the potential for applying MoWM in real-world environments? Would it still achieve performance improvements under such conditions?\n- In Figure 3 (top) and Figure 4, the decoded latent feature images show desks in different colors, while the bottom part of Figure 3 displays a consistent desk color. What explains this discrepancy?\n\nI would be willing to raise my score if the authors could address these concerns, particularly regarding the relationship between the policy performance improvement and MoWM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0qyGMtpztn", "forum": "4NYdDrRPSc", "replyto": "4NYdDrRPSc", "signatures": ["ICLR.cc/2026/Conference/Submission10783/Reviewer_9HQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10783/Reviewer_9HQb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663706353, "cdate": 1761663706353, "tmdate": 1762921998352, "mdate": 1762921998352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new mixture framework by fusing representations from hybrid world models, pixel world model and latent world model. The key idea is to use the motion awareness captured by the latent world model’s representations to modulate and enhance the features extracted by the pixel-space world model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The approach is simple and straightforward.\n* The problem that this paper would like to address is clear. Pixel space has redundant information for embodied tasks."}, "weaknesses": {"value": "* The evaluation environment is too simple. The paper only conducts the experiment on the CALVIN.\n* It seems the limitation of pixel world model the paper states, “pre-trained with pixel-level reconstruction objectives. Such objectives emphasize detailed pixel recovery, yet many robotic tasks do not require perfect reconstruction of all visual elements” is not the main problem in this paper? It’s very confusing to state it. In my understanding, the limitation for the pixel world model that the paper would like to address is that the pixel has redundant information that hinders action decoding.\n* Not ablation for other feature Modulation methods."}, "questions": {"value": "* What if the motion generated by the latent world model and the video generated by the pixel world model are not aligned, like they generate different trajectories for the robot arm?\n* In Figure 2, are the latent-WM encoder and pixel-WM encoder in the wrong place?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BV6zHIa85O", "forum": "4NYdDrRPSc", "replyto": "4NYdDrRPSc", "signatures": ["ICLR.cc/2026/Conference/Submission10783/Reviewer_azXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10783/Reviewer_azXP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760136682, "cdate": 1761760136682, "tmdate": 1762921996675, "mdate": 1762921996675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MoWM (Mixture-of-World-Models), a hybrid framework for embodied action planning that combines a latent-space world model (motion-aware, compact) with a pixel-space diffusion-based world model (fine-grained visual detail). The key idea is to use the latent model’s representations as a high-level prior to modulate features from the pixel model, thereby suppressing visual redundancies while preserving manipulation-critical details. The fused representation is then used in an end-to-end diffusion-based action decoder. Evaluated on the CALVIN benchmark, MoWM achieves state-of-the-art task success rates, especially on long-horizon tasks, and includes ablation studies validating the fusion design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: The “mixture-of-world-models” concept creatively bridges latent and pixel paradigms. While hybrid representations exist in other domains, their application to embodied planning via latent-to-pixel modulation is novel.\n2. Quality: Rigorous experiments across multiple baselines (IL, VLA, world models), strong ablations, and qualitative visualizations support claims.\n3. Clarity: Technical details (e.g., feature alignment, fusion via gating) are explained intuitively. Figures effectively communicate key ideas.\n4. Significance: Demonstrates that explicit fusion of complementary world models can outperform monolithic approaches, potentially influencing future architectures in robotics and video modeling."}, "weaknesses": {"value": "1. Limited Real-World Validation: All experiments are in simulation (CALVIN). While common, the absence of real-robot testing leaves open questions about robustness to sensor noise, actuation errors, or domain gaps.\n\n2. Fusion Mechanism Simplicity: The best-performing fusion is simple concatenation, which raises the question: Is the gain primarily from multi-source features, or from the specific modulation design? A more nuanced analysis (e.g., attention maps, feature importance) could strengthen the claim.\n\n3. Computational Cost: Training two large world models (SVD + ViT-g transformer) is resource-intensive. The paper does not discuss inference latency or scalability—important for real-time robotics."}, "questions": {"value": "1. Have you tested MoWM on real robots or sim-to-real transfer settings? If not, what are the main barriers?\n\n2. Why did cross-attention underperform concatenation? Was it due to optimization difficulty, or does it suggest that pixel features already contain sufficient spatial structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hQKJiMdNhH", "forum": "4NYdDrRPSc", "replyto": "4NYdDrRPSc", "signatures": ["ICLR.cc/2026/Conference/Submission10783/Reviewer_M6LR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10783/Reviewer_M6LR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997279689, "cdate": 1761997279689, "tmdate": 1762921992544, "mdate": 1762921992544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}