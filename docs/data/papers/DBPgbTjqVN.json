{"id": "DBPgbTjqVN", "number": 1710, "cdate": 1756910139148, "mdate": 1763090343395, "content": {"title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates", "abstract": "Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.", "tldr": "We propose LTGS, an efficient pipeline for updating the initial reconstruction from the collection of images that capture object-level changes in temporally-var environments.", "keywords": ["Gaussian splatting", "Dynamic scene", "Change detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6a460fd0101adb24d0f4d87bd5e106cc2dd50ba7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work tackle the task for updating the reconstructed 3DGS, where some of the objects are moved, added, or deleted, from few images captured in sparse time steps. To this end, this work try to separate the scene into dynamic objects and background part so different time step can be explained by blending the transformed dynamic objects and the background. The entire pipeline has several stages to reconstruct the dynamic object templates: dynamic objects segmentation, instance matching, tracking, and 3DGS optimization using the initial and the updated views. The results show significant improvement comparing to previous dynamic 3DGS methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed new setup is interesting and could be useful in practice. If we use 3DGS as map, users or robot navigators may want the ability to quickly updating the map from few observations."}, "weaknesses": {"value": "My main concerns is about its long pipeline and somewhat ad-hoc approach. Each stage in the pipeline is heuristic algorithm aided by several foundation models (e.g., SAM, DINOv2) for change detection, instance matching and tracking. These are all very important and fundamental task in CV and will be very nice if the proposed algorithm can improve them. However, these components don't seems will validated in this work. The results are showcased on eight scenes where five of them are newly collected by this work. As the proposed method are a sequence of heuristics algorithms, I'm not so convinced regarding the robustness. Besides, I think the hyperparameters and their effect need more analysis which will gives users hint on how to tune them in case some of the stages fail."}, "questions": {"value": "In case the background or objects are occluded or only partially observed at initial, how does the proposed method can recover or complete them? It seems that the proposed method does not have a way to add more Gaussians."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mkejBalzPZ", "forum": "DBPgbTjqVN", "replyto": "DBPgbTjqVN", "signatures": ["ICLR.cc/2026/Conference/Submission1710/Reviewer_3N9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1710/Reviewer_3N9D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655015554, "cdate": 1761655015554, "tmdate": 1762915863557, "mdate": 1762915863557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We respectfully request to withdraw our submission. After further internal review, we found that our manuscript require major updates. We thank the reviewers and AC for their time and consideration."}}, "id": "kxfdtsmmfV", "forum": "DBPgbTjqVN", "replyto": "DBPgbTjqVN", "signatures": ["ICLR.cc/2026/Conference/Submission1710/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1710/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763090342503, "cdate": 1763090342503, "tmdate": 1763090342503, "mdate": 1763090342503, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LTGS is designed to capture highly unconstrained, casually recorded variations in everyday scenes and to robustly model the long-term evolution of a scene. Experiments demonstrate that LTGS surpasses existing baselines in reconstruction quality while enabling fast and lightweight updates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a new task and dataset, achieving superior performance over existing methods.\n\n - Employs multiple strategies to enhance temporal and multi-view consistency across different observations of the same object."}, "weaknesses": {"value": "- Deformable objects are modeled as separate instances, leading to insufficient exploitation of temporal consistency across time.\n\n - Add a flowchart for Section 3.3 would help readers better understand the overall pipeline.\n\n - The reliance on multiple foundation models raises concerns about robustness."}, "questions": {"value": "- How does the method perform in more complex scenes, such as when people walk in the background or curtains move due to wind? The presented examples appear static at each moment, which greatly limits the applicability of the model to dynamic real-world scenarios.\n\n - What level of precision is required for instance matching? How do matching errors affect reconstruction quality? Similarly, how do segmentation errors from SAM impact the final results?\n\n - Providing long-term reconstructed videos would help readers more effectively assess the method’s performance.\n\n - Will the collected real-world dataset be released publicly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "44DglHoSLe", "forum": "DBPgbTjqVN", "replyto": "DBPgbTjqVN", "signatures": ["ICLR.cc/2026/Conference/Submission1710/Reviewer_PKSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1710/Reviewer_PKSH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791755807, "cdate": 1761791755807, "tmdate": 1762915863400, "mdate": 1762915863400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper studies long-term scene updating under sparse views. In particular, it asks how to update a 3D Gaussian scene efficiently when the scene changes, but only a few new images are available.\n2. Existing methods like 4DGS mainly target continuous/deformable motions and do not handle this setting well.\n3. The authors propose LTGS with three main components: (i) change detection, (ii) object template construction, and (iii) long-term Gaussian update.\n4. The method is evaluated on the CL-NeRF dataset and a real dataset collected by the authors, and achieves SOTA results. Table 3 gives ablations on object tracking, pose refinement, and background initialization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is novel and practical: real environments change, but current methods cannot update a scene from such sparse observations.\n2. The method is technically sound: it detects changes, reconstructs and stores object templates, and then updates the scene.\n3. The experiments are quite thorough, especially the ablations in Table 3."}, "weaknesses": {"value": "1. The method does not support non-rigid or articulated objects, such as people, hands, small animals, or clothes. The authors could consider integrating 4DGS-style deformation or a more general feed-forward reconstruction (e.g., VGGT-style) for local dynamic changes in the future.\n2. The results seem sensitive to illumination changes, and the object templates do not appear to transfer well across scenes.\n3. The motivation could be better justified: it is not fully clear what concrete real-world applications need this exact setting."}, "questions": {"value": "Please address the issues listed in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VUIH9cHl9g", "forum": "DBPgbTjqVN", "replyto": "DBPgbTjqVN", "signatures": ["ICLR.cc/2026/Conference/Submission1710/Reviewer_Q2eX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1710/Reviewer_Q2eX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915533336, "cdate": 1761915533336, "tmdate": 1762915863217, "mdate": 1762915863217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of reconstructing scenes that undergo object-level changes over time. In particular, it presents the first approach capable of adapting to object changes and reconstructing dynamic environments even when only a sparse set of images is available at each time step. The proposed pipeline maintains previously captured Gaussian representations while detecting, tracking, and re-localizing changed objects from newly observed images. Each object is stored as an object-level Gaussian template, which is then used to refine the Gaussian parameters through optimization. By separating the dynamic objects from the static background, the framework preserves geometric consistency and integrates new object information effectively, enabling long-term scene reconstruction from sparse view updates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly defines its research problem, thoroughly analyzes the limitations of existing methods, and effectively overcomes them by combining multiple complementary techniques.\n2. Unlike prior continual learning approaches based on Gaussian Splatting, the proposed framework robustly tracks object-level changes and maintains scene consistency even under sparse image updates.\n3. Extensive comparisons against baselines demonstrate significant improvements in both quantitative metrics and qualitative reconstruction quality.\n4. The introduction of object-level Gaussian templates as reusable structural priors enables efficient and stable long-term scene reconstruction, achieving strong generalization across multiple time steps."}, "weaknesses": {"value": "1. The proposed method designs a pipeline that reconstructs time-varying scenes using 3D Gaussians, even with a limited number of images per time step. However, apart from the Long-Term Gaussian Splats Optimization component, most parts of the framework are built upon existing techniques, and the technical contribution is not particularly prominent. In this sense, the method appears to be a well-engineered system design that effectively integrates established modules rather than introducing fundamentally new ideas. Nonetheless, this integrative approach offers practical value and clear problem-oriented insights, though the level of research novelty may be somewhat limited.\n2. The proposed method heavily relies on MASt3R for acquiring geometric information under sparse-view conditions. However, this dependence makes the overall pipeline sensitive to the performance and potential errors of MASt3R. It would strengthen the paper to include additional experiments or mechanisms demonstrating robustness to MASt3R’s estimation errors or uncertainties.\n\n<Minor weakness>\nIncluding a video visualization of the reconstructed 3D Gaussian templates would help clearly demonstrate the reliability of the object tracking and Gaussian update stages under dynamic scene changes."}, "questions": {"value": "Please refer to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V7mfFdQu18", "forum": "DBPgbTjqVN", "replyto": "DBPgbTjqVN", "signatures": ["ICLR.cc/2026/Conference/Submission1710/Reviewer_HRSR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1710/Reviewer_HRSR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952584347, "cdate": 1761952584347, "tmdate": 1762915863013, "mdate": 1762915863013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}