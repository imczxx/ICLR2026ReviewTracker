{"id": "H4RVXhicSj", "number": 15298, "cdate": 1758250017312, "mdate": 1759897315062, "content": {"title": "Semi-Supervised Contrastive Learning with Orthonormal Prototypes", "abstract": "Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.", "tldr": "", "keywords": ["Contrastive learning", "Semi-Supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3496983ac7de1a1d45b669fda3442c73081c487c.pdf", "supplementary_material": "/attachment/1547e61317f37f07e6a14060b04688bfc56f819e.zip"}, "replies": [{"content": {"summary": {"value": "This work has two main contributions. First, it identifies a mechanism of representation collape from repulsive force, instead of only gravitational force. Second, it identifies a solution simple solution which mitigates the problem: randomly generating fixed orthonormal protypes assingned to classes. This proves to be quite effective on numerous classfication and object detection tasks, including semi-supervised learning and learning with unbalanced labels."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The analysis is interesting, in particular showing that repulisive forces cause collapse and correspond to stationary points. \n* The results are fairly strong on semi-supervised tasks and imbalanced label tasks. \n* Findings about needing smaller batch sizes are also interesting."}, "weaknesses": {"value": "* Transfer learning and object detection results are much more marginal than semi-supervised and imbalanced label results. \n* CLOP doesn't seem to be useful self-supervised learning. \n* (Minor) introduces another hyper-parameter, increasing tuning cost, though it appears to be stable."}, "questions": {"value": "* Would it make sense to do some kind of smart assingment of prototypes at the beginning of training?\n* Is there some kind of extension that can be applied for self-supervised learning?\n* On a related note, this work seems related to the protytypes in SwAV?  [1] It could be good to discuss the similarities and differences of these works. \n\n[1] Caron, Mathilde, et al. \"Unsupervised learning of visual features by contrasting cluster assignments.\" Advances in neural information processing systems 33 (2020): 9912-9924."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jjm5fxQpHI", "forum": "H4RVXhicSj", "replyto": "H4RVXhicSj", "signatures": ["ICLR.cc/2026/Conference/Submission15298/Reviewer_fKbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15298/Reviewer_fKbf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875009564, "cdate": 1761875009564, "tmdate": 1762925594085, "mdate": 1762925594085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work presents a semi-supervised learning loss function called CLOP that initializes orthonormal vectors as many as the number of classes in order to draw the similarity of class specific embedding functions towards these vectors to enable better separation and mitigate collapse. This is added as a regularization term to the InfoNCE loss function. Empirical and visual analyses of the method against known methods are provided."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is well performant.\n\n2. The regularizer is modular and could serve as an addendum to other methods.\n\n3. Tackles a known issue of dimensional collapse."}, "weaknesses": {"value": "1. The proposed contribution is an incremental one that combines existing notions of orthonormalization with standard contrastive learning without introducing any new theoretical insight.\n\n2. Comparisons against known approaches [1] for this problem aren't conducted.\n\n3. Initialization of prototypes in an orthonormal manner may be misguided since several concepts or classes in datasets may be semantically very related.\n\n4. The theoretical contribution may be a restatement from known work [1].\n\n\n[1] Jing, Li, et al. \"Understanding dimensional collapse in contrastive self-supervised learning.\" arXiv preprint arXiv:2110.09348 (2021)."}, "questions": {"value": "1. Is the Lemma 1 a known result that all-equal or co-linear embeddings are stationary points? [1, 2]\n\n2. Could this work be a special case of the work in SWAV wherein the the unsupervised clustering and assignment mechanism is replaced with fixed, label-anchored prototypes? [3]\n\n\n[1] Jing, Li, et al. \"Understanding dimensional collapse in contrastive self-supervised learning.\" arXiv preprint arXiv:2110.09348 (2021).\n\n[2] Wang, Tongzhou, and Phillip Isola. \"Understanding contrastive representation learning through alignment and uniformity on the hypersphere.\" International conference on machine learning. PMLR, 2020.\n\n[3] Caron, Mathilde, et al. \"Unsupervised learning of visual features by contrasting cluster assignments.\" Advances in neural information processing systems 33 (2020): 9912-9924."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jiyiDE1LJK", "forum": "H4RVXhicSj", "replyto": "H4RVXhicSj", "signatures": ["ICLR.cc/2026/Conference/Submission15298/Reviewer_2Mig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15298/Reviewer_2Mig"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887007638, "cdate": 1761887007638, "tmdate": 1762925593710, "mdate": 1762925593710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLOP, a semi-supervised contrastive learning method that mitigates dimensional collapse by aligning embeddings with orthonormal class prototypes. The approach theoretically and empirically shows that conventional contrastive losses (e.g., InfoNCE) suffer from degenerate optima and that enforcing orthogonal subspaces preserves representational diversity. CLOP consistently outperforms prior methods like SupCon and SimMatch across CIFAR and ImageNet benchmarks, showing strong robustness to small batch sizes and high learning rates. The paper is clearly written, well-motivated, and experimentally solid, making it a strong accept candidate despite assuming fixed class structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tSolid Theory – Offers a clear theoretical analysis explaining why InfoNCE leads to dimensional collapse and how orthogonal prototypes can prevent it.\n2.\tNovel Loss Design – Proposes CLOP, a simple yet effective loss that enforces orthogonality among class embeddings to maintain diversity.\n3.\tStrong Empirical Results – Demonstrates consistent gains over baselines like SupCon and SimMatch on CIFAR and ImageNet.\n4.\tRobustness – Performs stably under large learning rates and small batch sizes, avoiding collapse seen in prior methods.\n5.\tPractical Implementation – Easy to integrate into existing contrastive frameworks with minimal computational overhead.\n6.\tClear Presentation – Well-written, logically structured, and easy to follow with theory and experiments well aligned."}, "weaknesses": {"value": "1.\tFixed Prototype Assumption – CLOP assumes a fixed number of well-separated classes and static orthonormal prototypes. How would the method adapt to open-set or hierarchical label scenarios where class structures evolve over time?\n2.\tLimited Scope of Evaluation – All experiments are in vision-based benchmarks (CIFAR, ImageNet). Can CLOP generalize to non-visual domains such as text, graphs, or multimodal tasks, if you don't have time, please disccuss its possibility?\n3.\tLack of Computational Analysis – The paper does not report the training overhead introduced by prototype orthogonalization or additional loss terms. How significant is the extra cost compared to standard contrastive methods?\n4.\tDependence on Label Quality – CLOP relies on a subset of labeled data to guide prototype alignment. How robust is it to noisy or inaccurate labels, and would incorrect prototype supervision lead to representation drift?"}, "questions": {"value": "Please check weaknesses, and try to argue them, I will definitely read your response, good luck!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dNsFsqKgAg", "forum": "H4RVXhicSj", "replyto": "H4RVXhicSj", "signatures": ["ICLR.cc/2026/Conference/Submission15298/Reviewer_1k4M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15298/Reviewer_1k4M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928356000, "cdate": 1761928356000, "tmdate": 1762925593218, "mdate": 1762925593218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Contrastive Learning With Orthonormal Prototypes (CLOP), which forms orthonormal prototypes to prevent dimensional collapse of the embeddings learned by semi-supervised loss functions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper focuses on an important research area of semi-supervised contrastive learning"}, "weaknesses": {"value": "* The authors did not follow the standard ICLR style\n\n* No theoretical results supporting the success of CLOP\n\n* It is unclear when/why CLOP works well"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bR5tpVdcSB", "forum": "H4RVXhicSj", "replyto": "H4RVXhicSj", "signatures": ["ICLR.cc/2026/Conference/Submission15298/Reviewer_8Akm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15298/Reviewer_8Akm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000656830, "cdate": 1762000656830, "tmdate": 1762925592727, "mdate": 1762925592727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}