{"id": "e9cV4xSjbR", "number": 11331, "cdate": 1758196469011, "mdate": 1759897587039, "content": {"title": "ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics", "abstract": "Molecular dynamics (MD) simulations underpin modern computational drug discovery, materials science, and biochemistry. Recent machine learning models provide high-fidelity MD predictions without the need for repeated quantum-mechanical force calculations, enabling significant speedups over conventional pipelines. Yet many such methods typically enforce strict equivariance and rely on sequential rollouts, thus limiting their flexibility and simulation efficiency. They are also commonly single-task, trained on individual molecules and fixed time frames, which restricts generalization to unseen compounds and extended timesteps. To address these issues, we propose Atomistic Transformer Operator for Molecules (ATOM), a pretrained transformer neural operator for multi-task molecular dynamics. ATOM adopts a quasi-equivariant design that does not require an explicit molecular graph and employs a temporal attention mechanism to enable accurate, parallel decoding of multiple future states. To support operator pretraining across chemicals and timescales, we curate TG80, a large, diverse, and numerically stable MD dataset with over 2.5 million femtoseconds of trajectories across 80 compounds. ATOM achieves state-of-the-art performance on established single-task benchmarks, such as MD17, RMD17, and MD22. After multi-task pretraining on TG80, ATOM shows exceptional zero-shot and robust generalization to unseen molecules across varying time horizons. We believe ATOM represents a significant step toward accurate, efficient, and transferable molecular dynamics models.", "tldr": "We present the first neural operator to our knowledge that generalizes to out-of-domain molecular dynamics trajectories and performs strongly on large molecules; we release TG80, a multitask MD dataset (>2.5M fs, 80 compounds) to support future work.", "keywords": ["Molecular dynamics", "neural operator", "transformer", "ai for science", "equivariant"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3379b78a121d24db9af4891753ce9b67f6a3a6dc.pdf", "supplementary_material": "/attachment/42a2020ab6f8750743a0b2b052dbdcfb0a07a105.zip"}, "replies": [{"content": {"summary": {"value": "This work presents the atomistic transformer operator for molecules (ATOM), which preserves quasi-equivariance and enables parallel decoding of molecule states across multiple timesteps. The model leverages E(3)-equivariant linear layers to produce symmetry-aware features, and proposes temporal RoPE (T-RoPE) to build the heterogeneous temporal attention mechanism. For both single-task experiments on MD17, MD22 and multitask experiments on the curated TG80 dataset, ATOM consistently outperforms the selected baselines, showing an out-of-distribution generalization across unseen molecules."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is written with a clear logical flow, facilitating reader comprehension. Each component of the model architecture is well justified and supported by comprehensive ablation studies.\n- The proposed T-RoPE method ingeniously enables attention to achieve translation equivariance along the temporal dimension, thereby supporting parallel prediction of future molecular states at various timesteps.\n- The newly introduced TG80 dataset represents a valuable extension over datasets such as MD17 and MD22 in terms of both scale and trajectory accuracy, contributing to the advancement of the field.\n- ATOM demonstrates out-of-distribution generalization on the TG80 dataset, which is crucial for practical applications."}, "weaknesses": {"value": "- The authors claim that ATOM enables parallel computation through the T-RoPE technique, thereby overcoming the limitation of sequential sampling. However, this claim lacks experimental evidence. For instance, it would be helpful to compare the inference time of ATOM with that of sequential generative models when generating trajectories of equal wall-clock duration.\n- Given the molecular state at time $t$, the model outputs future states at different time intervals based on the assumption that the trajectory is uniquely determined by the initial state. However, in realistic scenarios like protein–ligand interactions in aqueous solution, thermal fluctuations must be taken into account, which are typically modeled using Langevin dynamics. In such cases, the trajectory is not uniquely determined but rather represented as a probabilistic distribution. Therefore, the training and evaluation of ATOM are not fully aligned with real physical settings.\n- As the authors mentioned in the Limitations section, ATOM has not yet been validated on larger molecular systems. However, for small-molecule systems with fewer than 20 heavy atoms, MD simulations using empirical force fields are already highly efficient. The authors should therefore further clarify the advantages of ATOM over classical MD simulations based on empirical force fields."}, "questions": {"value": "1. Whether ATOM offers efficiency improvements over sequential generative models remains to be verified. The authors are encouraged to provide a comparison of inference time between ATOM and conventional sequential generation models when generating trajectories of equal wall-clock duration.\n2. Although ATOM is theoretically capable of predicting future states at different time intervals through T-RoPE, the maximum predictable time interval is pre-defined as $\\Delta T$. Therefore, when generating a long trajectory, using ATOM appears equivalent to employing a sequential generative model with a fixed time step of $\\Delta T$. Please provide a reasonable explanation for this or correct me if I understand wrong.\n3. In line 316, the experimental setup reports $\\Delta T=3000$. Is this value expressed relative to the frame spacing of trajectories in the training datasets? What wall-clock time does it correspond to?\n4. To my knowledge, for molecular systems with fewer than 20 heavy atoms, MD simulations using empirical force fields can already achieve high efficiency. Please clarify the advantages of ATOM over classical MD methods, preferably supported by additional experimental results.\n5. In Table 3, EGNO appears to exhibit a much smaller performance drop from in-distribution to out-of-distribution settings compared to ATOM. Please provide an explanation for this observation.\n6. ATOM predicts future states at different time intervals given the current state, which typically relies on the assumption that the trajectory is uniquely determined by the initial state. However, in broader application scenarios, biomolecules generally interact in aqueous solution, where thermal fluctuations must be considered. Thus, trajectories should be represented as distributions rather than being uniquely determined. ATOM appears not directly applicable to such settings. How do the authors address this limitation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sZ02l89JVt", "forum": "e9cV4xSjbR", "replyto": "e9cV4xSjbR", "signatures": ["ICLR.cc/2026/Conference/Submission11331/Reviewer_hg7v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11331/Reviewer_hg7v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760520712168, "cdate": 1760520712168, "tmdate": 1762922467683, "mdate": 1762922467683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ATOM, a pretrained neural operator designed to improve molecular dynamics (MD) modeling by incorporating approximate geometric equivariance. The method proposes a heterogeneous temporal attention mechanism along with a temporal RoPE (T-RoPE) adapation, and further enables large-scale pretraining and generalization across different MD tasks. Experimental results demonstrate that ATOM achieves competitive performance on multiple benchmarks compared to strong baselines such as EGNO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses a critical problem, that is, improving neural operators for molecular dynamics, and further enables pretraining on large-scale datasets, which is an important step toward generalizable molecular representation learning.\n\n* This paper constructs a new large-scale molecular dynamics dataset, TG80, which not only facilitates the pretraining of ATOM itself but also provides valuable resources for future research in this area.\n\n* The empirical results are generally strong and show that ATOM performs well across multiple benchmarks, indicating the effectiveness of the proposed approach."}, "weaknesses": {"value": "* **Unclear Architecture Design.** The paper repeatedly employs the term **quasi-equivariant** without providing a clear mathematical definition. Moreover, while the ablation studies compare various architectural variants, the main text and appendix lack a detailed, mathematically grounded description of the original model design. The absence of released code further limits reproducibility. The authors should clearly define quasi-equivariance, explain its advantages, and elaborate on how it is concretely maintained in the model.\n\n* **Limited Discussion of Related Works.** The problem studied in the paper can also be framed as a **Time-Coarsened Dynamics** problem, yet many relevant works in this area are not sufficiently discussed [A, B, C]. The authors are encouraged to analyze the connections and differences between ATOM and prior works, including distinctions in problem formulation, modeling strategy, and evaluation metrics.\n\n[A] Klein, Leon, et al. \"Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics.\" Advances in Neural Information Processing Systems 36 (2023): 52863-52883.\n[B] Hsu, Tim, et al. \"Score dynamics: Scaling molecular dynamics with picoseconds time steps via conditional diffusion model.\" Journal of Chemical Theory and Computation 20.6 (2024): 2335-2348.\n[C] Yu, Ziyang, Wenbing Huang, and Yang Liu. \"UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "*  In the single-task setting, what is the key difference between ATOM and the main baseline EGNO? Can ATOM degenerate into EGNO if some critical components are removed? If so, such a description would clarify the key improvements and help highlight the method’s contribution.\n\n* Does multi-task training improve the performance of each individual task? The paper does not include an ablation study that isolates the effect of multi-task learning itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6dd4vq0K8K", "forum": "e9cV4xSjbR", "replyto": "e9cV4xSjbR", "signatures": ["ICLR.cc/2026/Conference/Submission11331/Reviewer_L5rw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11331/Reviewer_L5rw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867022712, "cdate": 1761867022712, "tmdate": 1762922467381, "mdate": 1762922467381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ATOM, a neural operator for molecular dynamics that introduces a quasi-equivariant architecture for better flexibility and simulation efficiency. Molecular systems are modeled as fully connected point clouds. It employs a equivariant lifting layer to embed atomic positions and velocities into a symmetry-aware latent space and a temporal attention mechanism to enable parallel decoding of multiple future states. ATOM demonstrates better performance than state-of-the-art approaches on standard MD benchmarks and shows that it can transfer to unseen molecules better than prior approaches. The authors also present TG80, a large-scale benchmark dataset with simulation data for a diverse set of molecules."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* This is an important and timely problem\n\n* For a multitask set up, the authors demonstrate better performance that state-of-art baselines\n\n* Parallel decoding can be more efficient\n\n* An MD dataset, TG80, is produced, which intended to be chemically diverse and numerically stable to help in multitask pretraining and benchmarking tasks"}, "weaknesses": {"value": "* The choice and implications of modeling atoms as a point cloud is not discussed \n\n* As a non-expert, I found few insightful discussions or learnings in the paper. It would be good if the authors discussed:\n(1) the explicit novelty of each part of their architecture in comparison to prior works; (2) the importance of generality as opposed to specialized models; (3) the insights behind structure of the attention-based mechanism.\n\n* It is not clear what \"heterogeneous attention\" means and its importance.\n\n* There is no evaluation of the efficiency benefits of the proposed mechanism \n\n* Figure 2 does not add much value to the paper"}, "questions": {"value": "Please address comments in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zox96iTtD2", "forum": "e9cV4xSjbR", "replyto": "e9cV4xSjbR", "signatures": ["ICLR.cc/2026/Conference/Submission11331/Reviewer_h8zv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11331/Reviewer_h8zv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11331/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112876517, "cdate": 1762112876517, "tmdate": 1762922467031, "mdate": 1762922467031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}