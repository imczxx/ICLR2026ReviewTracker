{"id": "yBJZw5DBzU", "number": 17369, "cdate": 1758275129662, "mdate": 1763017400848, "content": {"title": "Reinforced Adaptive Routing for Mixture-of-expert Models", "abstract": "With the rapid development of large language models (LLMs), the mixture-of-experts (MoE) architecture attracts increasing attention due to its advantages in scaling capacity and enhancing performance. However, MoE requires activating multiple experts during training and inference, which introduces substantial computational and memory overhead. This makes acceleration essential in resource-constrained or latency-sensitive settings. Existing adaptive expert selection approaches often rely on heuristics or single-source supervision, lacking a unified formulation that simultaneously captures accuracy, balanced utilization, and efficiency. To address this, we propose a reinforcement learning–based adaptive routing approach that integrates a policy network into the standard MoE framework and optimizes expert selection end-to-end with a multi-objective reward. Experiments on benchmark datasets demonstrate that our approach substantially improves training efficiency while maintaining accuracy and promoting more balanced expert utilization.", "tldr": "", "keywords": ["Mixture-of-experts", "Large Language Model", "Adaptive Routing", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/1f7aee75d6c7230669eca4fb80e138c2790fdb96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a RL policy network into the training of MoE models, aiming to learn dynamic expert counts selected for each token at each layer. The authors claim that this approach improves efficiency, but I contend there are many factual mistakes."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The writing and figures are clear and well-organized."}, "weaknesses": {"value": "1. The motivation lacks validity. Lines 13–15 state, “MoE requires activating multiple experts during training and inference, which introduces substantial computational and memory overhead.” However, it is important to note that MoE is explicitly designed to enable sparse activation compared to dense models with equivalent “total” parameter counts. The authors incorrectly frame MoE’s efficiency by comparing it to dense models with the same single MLP size. Deploying a sparsely activated model is typically more efficient. This suggests that the authors have a misunderstanding of MoE’s core design intent.\n\n2. Based on this misunderstanding that MoE is inefficient, the authors propose a policy network to dynamically predict the number of activated experts per layer and per token, claiming improved efficiency due to lower activation ratios. However, this approach overlooks critical practical considerations. For instance, models are typically deployed on devices sized to handle their maximum computational and memory demands. If activation ratios vary dynamically across layers, layers with lower ratios will underutilize resources (whether memory- or compute-bound), leading to waste.\n\n    The inefficiency issues of the proposed method might be exacerbated when activated expert counts are determined per token. In some cases, layer runtime is bottlenecked by the token requiring the most expert computations, while tokens assigned fewer experts finish early, creating pipeline bubbles. Additionally, deploying this approach with expert parallelism would introduce greater complexity in kernel optimization and communication engineering.\n\n    It is recommended that the authors report training/inference runtime and throughput metrics. They should also explicitly clarify whether their implementation employs efficient MoE techniques—such as GEMM optimizations or expert parallelism—or relies on naive PyTorch for-loops. This distinction is critical: while naive loop-based implementations might easily yield apparent efficiency gains when reducing expert activation, real-world deployments leveraging GEMM and parallel computing often produce contradictory results.\n\n3. The authors' claim of evaluating \"pre-training\" effectiveness using a mere 101M tokens is inconsistent with established practices in LLM pre-training and lacks validity. A 101M token dataset is not just \"small\" but orders of magnitude smaller than standard pre-training corpora. This \"toy experiment\" cannot mimic the long-term knowledge accumulation, generalization, and convergence dynamics of real pre-training.\n\n4. The experimental improvements are insignificant. I do not think this method brings solid improvements in terms of downstream performance.\n\n5. I might have missed it, but I did not find any discussion of the overhead introduced by the RL policy network. I do not know whether this additional module makes training slower than standard end-to-end training.\n\n6. Figure 2a shows that between steps 15K and 40K, the training loss even increases slightly. Figure 2c shows that expert selection is not balanced at all: the most selected expert handles 20% of tokens, while the least selected processes only 6%. In practice, this leads to a pipeline bubble equivalent to the computation time for 14% of total tokens. How can this be considered efficient or balanced? Please provide quantitative metrics such as load balance loss [1] or balance entropy [2] to evaluate the model’s load balance.\n\n[1] Eq.4, https://arxiv.org/pdf/2101.03961\n[2] Eq.7, https://proceedings.mlr.press/v267/lv25b.html"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OXDHjbRDig", "forum": "yBJZw5DBzU", "replyto": "yBJZw5DBzU", "signatures": ["ICLR.cc/2026/Conference/Submission17369/Reviewer_7Q5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17369/Reviewer_7Q5c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553169854, "cdate": 1761553169854, "tmdate": 1762927281984, "mdate": 1762927281984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "gewhGhjzOI", "forum": "yBJZw5DBzU", "replyto": "yBJZw5DBzU", "signatures": ["ICLR.cc/2026/Conference/Submission17369/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17369/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763017399393, "cdate": 1763017399393, "tmdate": 1763017399393, "mdate": 1763017399393, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a reinforcement learning–based adaptive routing method for MoE)models. A policy network is introduced to dynamically predict the number of experts to activate per token at each layer. The routing strategy is trained via DPO, using a multi-objective reward that jointly considers accuracy, expert utilization balance, and computational efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and presents its method in a structured and understandable manner. The experimental setup includes multiple model backbones and benchmark tasks."}, "weaknesses": {"value": "* **Significant Overlap with Prior Work**: The method proposed in this paper shows substantial similarity to a previously work at ICLR 2025 [1], particularly in terms of both architectural design and training strategy. In particular, the policy network used to predict the number of experts strongly resembles the allocator module in [1], and the formulation of the multi-objective reward also overlaps significantly. The main distinction appears to be the adoption of DPO in this work, as opposed to PPO in [1]. Given the considerable design overlap, I suggest the authors to explicitly acknowledge these similarities and clearly articulate how their method differs from, or improves upon, the prior work.\n\n* **Evaluation Setup and Reported Performance**: The reported evaluation results raise questions regarding the interpretability of the proposed method. For instance, in the Table 4 (Row 5), when initializing from Qwen3-0.6B, the upcycled MoE trained with more data achieves 39.1 on MMLU and 29.6 on GSM8K, while the original Qwen3-0.6B [2] reports much higher scores (e.g., 52.81 on MMLU and 59.59 on GSM8K). The substantial performance gap casts doubt on the training effectiveness and model competitiveness. The authors are encouraged to provide more details about evaluation settings.\n\n* **Lack of Empirical Evidence for Adaptive Routing**: The paper claims to support adaptive routing by dynamically adjusting the number of experts per token based on input complexity. However, the empirical evidence provided does not fully demonstrate this property. To substantiate the claim of adaptivity, the authors could provide (i) analyses of expert activation patterns across different tokens, (ii) performance breakdowns under different target sparsity levels (e.g., activation ratio κ).\n\n[1] Yue T, Guo L, Cheng J, et al. Ada-k routing: Boosting the efficiency of moe-based llms[C]//The Thirteenth International Conference on Learning Representations. 2024.\n\n[2] Yang A, Li A, Yang B, et al. Qwen3 technical report[J]. arXiv preprint arXiv:2505.09388, 2025."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vMdnCD6gnb", "forum": "yBJZw5DBzU", "replyto": "yBJZw5DBzU", "signatures": ["ICLR.cc/2026/Conference/Submission17369/Reviewer_Z2eT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17369/Reviewer_Z2eT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985207164, "cdate": 1761985207164, "tmdate": 1762927281422, "mdate": 1762927281422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an adaptive routing method for MoE models. A policy network is introduced to allocate the number of experts for each token. It proposes a multi-objective reward function, optimized by DPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is conceptually intuitive and presented in a way that is accessible and easy to follow, making the methodology readily understandable."}, "weaknesses": {"value": "1.The paper omits comparisons with some relevant baselines that share similar motivations. For example:\nMixture-of-Depth\t[1], which achieves adaptive per-token computation by conditionally skipping layers.\nAdaMoE[2], which enables dynamic expert selection through the use of null experts.\nIncluding these baselines would strengthen the empirical analysis, especially given their focus on compute-adaptive architectures.\n\n2.The paper does not report the distribution of expert activations across layers. If the computation is imbalanced layer-wise, pipeline parallelism becomes significantly more difficult to implement efficiently, affecting both training throughput and inference latency due to load imbalance and increased synchronization overhead.\n\n3.Figure 2(c) is difficult to interpret and does not clearly demonstrate any discernible patterns. If the authors aim to support their claims regarding learned adaptive routing strategies, a more fine-grained and interpretable visualization is necessary.\n\n[1] Raposo D, Ritter S, Richards B, et al. Mixture-of-depths: Dynamically allocating compute in transformer-based language models[J]. arXiv preprint arXiv:2404.02258, 2024.\n[2] Zeng Z, Miao Y, Gao H, et al. Adamoe: Token-adaptive routing with null experts for mixture-of-experts language models[J]. arXiv preprint arXiv:2406.13233, 2024."}, "questions": {"value": "Please refer to the above Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8HXIEerOAY", "forum": "yBJZw5DBzU", "replyto": "yBJZw5DBzU", "signatures": ["ICLR.cc/2026/Conference/Submission17369/Reviewer_jEjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17369/Reviewer_jEjD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995280173, "cdate": 1761995280173, "tmdate": 1762927280956, "mdate": 1762927280956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a reinforcement learning-based adaptive routing method for MoE models, aiming to improve computational efficiency while maintaining model performance. The approach incorporates a policy network to dynamically adjust the number of experts activated per token and layer, and optimizes the routing strategy using a multi-objective reward function via DPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-structured and clearly written, with a coherent presentation of the motivation, method, and experimental design. The proposed method is evaluated across multiple model architectures and benchmark tasks, which strengthens the validity of the results."}, "weaknesses": {"value": "1.Dynamic adjustment of the number of activated experts to enhance efficiency appears to have been recently and effectively explored by various works [1,2,3]. Given the high similarity in both the technical goal and the application scenario, this paper fails to clearly delineate its core adavantage and differentiation from this previous work through discussions or experiments. \n\n2.Providing experiment data on the average latency/throughput could help make the inference cost improvement more significant. Besides, the paper does not demonstrate the relationship between the reduction in FLOPs and the activation ratio.\n\n[1] Team M L C, Li B, Lei B, et al. Longcat-flash technical report[J]. arXiv preprint arXiv:2509.01322, 2025.\n\n[2] Zeng Z, Miao Y, Gao H, et al. Adamoe: Token-adaptive routing with null experts for mixture-of-experts language models[J]. arXiv preprint arXiv:2406.13233, 2024.\n\n[3] Yue T, Guo L, Cheng J, et al. Ada-k routing: Boosting the efficiency of moe-based llms[C]//The Thirteenth International Conference on Learning Representations. 2024."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yL1jpUicOh", "forum": "yBJZw5DBzU", "replyto": "yBJZw5DBzU", "signatures": ["ICLR.cc/2026/Conference/Submission17369/Reviewer_Ys1f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17369/Reviewer_Ys1f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014823317, "cdate": 1762014823317, "tmdate": 1762927280518, "mdate": 1762927280518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}