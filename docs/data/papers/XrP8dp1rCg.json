{"id": "XrP8dp1rCg", "number": 11555, "cdate": 1758201602034, "mdate": 1759897568118, "content": {"title": "From Assistant to Independent Developer — Are GPTs Ready for Software Development?", "abstract": "Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. \nUnlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch.\n\nTo address this gap, we propose \\tool, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with \\textbf{implementing the functionality into an Android app from scratch}. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct \\tool, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, \\tool incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8\\% functionally correct applications, highlighting fundamental limitations in current models' ability to handle complex, multi-component software engineering challenges.", "tldr": "We show that while LLMs can generate code snippets well, they fail dramatically at real-world Android app development due to inability to handle complex and architectural reasoning.", "keywords": ["software dvelopment", "app development", "coding agent", "LLM", "code model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f09be2f1a494403d214f73520ad0bbb018e1a764.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a benchmark called AppForge. The goal is to evaluate LLM's capability, beyond generating isolated snippets or functions. The argument is that existing benchmarks do not test system-level reasoning and integration required to build a complete application. To this end, AppForge builds on Android application development as the benchmark domain, and it is based on the top 200 highest-scoring apps across different categories. Evaluations compare how 12 LLMs perform on AppForge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reading this paper brings back the fond memory of developing Android apps.\n\n1. I can see how app development can support the claim of going beyond generating isolated code snippets and functions. The idea of using Android apps as the benchmark domain seems reasonable.\n\n2. While I haven't personally used a lot of LLM benchmarks, the explanation on how AppForge is different from others is clear to me.\n\n3. I also like that AppForge includes a diverse range of apps. And, evaluations (over 12 LLMs) suggest that AppForge can differentiate model capabilities better than existing code generation benchmarks."}, "weaknesses": {"value": "I have been thinking whether I would use AppForge myself, and the following aspects are not clear to me:\n\n1. What LLM capabilities is AppForge really testing? I understand that the end-goal is generating fully-working Andorid apps, but app development is simply a scenario, not a capability. From the introduction, the capability seems to be system-level reasoning, but what is system-level reasoning? As a system researcher, I find that system-level reasoning can be an overloaded term: reasoning about system behavior, properties, states, and so on?\n\n2. Is the scoring system just pass-or-fail? Developing a fully-working app is non-trivial, and it involves a lot of planning and implementation steps. Is it possible that minor mistakes along the way result in app failure?\n\n3. AppForge is based on top 200 highest-scoring apps across many categories. I am wondering whether solution leaking would be a problem. In other words, are there open-sourced version of these 200 apps, whose source codes are present in the LLM training data? After all, these are highest-scoring apps, and it is likely that people have released open-sourced version?\n\n4. One minor question. Software development is typically imposed with budget constraints. How does AppForge ensure that the better-performing LLMs are not using excessively more budgets than others?"}, "questions": {"value": "Please refer to the \"Weaknesses\" section, for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jFB96oZKaJ", "forum": "XrP8dp1rCg", "replyto": "XrP8dp1rCg", "signatures": ["ICLR.cc/2026/Conference/Submission11555/Reviewer_WwYL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11555/Reviewer_WwYL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377181090, "cdate": 1761377181090, "tmdate": 1762922644129, "mdate": 1762922644129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents APPFORGE, a novel benchmark designed to evaluate the end-to-end software development capabilities of large language models (LLMs) in the context of Android app development. Unlike existing benchmarks that primarily assess function-level or patch-level code generation, APPFORGE targets real-world software engineering tasks, including system-level reasoning, component integration, UI design, state management, and exception handling. The benchmark includes 101 tasks automatically constructed and validated from real-world open-source Android apps, and features a fully automated evaluation pipeline involving compilation, test execution, and fuzzing. The authors evaluate 12 state-of-the-art LLMs (including GPT-5, Claude-Opus, and others), revealing that current models perform poorly on this benchmark—with a maximum success rate of only 18.8%—thus highlighting the significant gap between current capabilities and the demands of real-world software development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The first to comprehensively evaluate LLMs in full Android application development from scratch, addressing a crucial gap in current code evaluation benchmarks.\n2. Tasks are derived from real-world, actively maintained F-Droid apps, ensuring diversity and authenticity.\n3. The benchmark combines automatic task construction and evaluation with expert validation, ensuring high-quality and reproducible results.\n4. The paper provides a breakdown of LLM failure cases, behavioral patterns (e.g., avoidance strategies), and complexity trends."}, "weaknesses": {"value": "While the paper identifies that some models (e.g., GPT-4.1) evade development by deleting faulty code instead of fixing it, it does not offer a detailed exploration of this behavior. There is no case analysis or inspection of prompts provided to the model during iterations. The paper does not clarify whether this is due to training data bias, a form of “lazy” behavior by the model, or a failure in prompt engineering. This limits the explanatory depth of an otherwise critical observation.\n\nThe title poses a broad question—“Are GPTs Ready for Software Development?”—but the empirical scope is restricted to Android app development. While Android development is non-trivial, its characteristics (e.g., strict lifecycle constraints, XML UI design, framework conventions) do not generalize to other domains such as backend systems, web applications, or embedded systems. The gap between the title’s generality and the study’s actual scope should be addressed."}, "questions": {"value": "1. In Table 1, are the results under “with Compilation Error Feedback” obtained from a single iteration or multiple rounds of refinement? For instance, Qwen3-Coder reports 85.15% compilation success, but this seems inconsistent with the values in Figure 5. In Figure 5, on the line for Qwen3-Coder Compile, there seem to be no node values corresponding to 85.15%. Please clarify.\n2. Could the authors provide the prompt used for iterative generation based on the previous compilation error output?\n3. For the observed “avoidance behavior”, could the authors provide concrete cases and detailed analysis? Specifically, is this phenomenon a result of training data bias, a form of lazy model strategy, or prompt design shortcomings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vskdtJJvbb", "forum": "XrP8dp1rCg", "replyto": "XrP8dp1rCg", "signatures": ["ICLR.cc/2026/Conference/Submission11555/Reviewer_TWQk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11555/Reviewer_TWQk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405683036, "cdate": 1761405683036, "tmdate": 1762922643486, "mdate": 1762922643486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces APPFORGE, a new end-to-end benchmark for evaluating LLMs on real-world software development tasks. APPFORGE requires models to develop full Android applications from scratch, given only natural-language specifications, unlike existing code benchmarks, are focus on a single task. Under human expert verification, each task contains a detailed functional description, natural-language test cases, and implementation constraints for comprehensive information to resolve the problem. Finally, they evaluate 12 SOTA models and two code agents to show that currently, LLMs are still far from handling such an end-to-end development task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem formulation. APPFORGE proposes code evaluation by shifting from “code generation” to “full software engineering.” It evaluates end-to-end development — design, implementation, integration, and runtime — not just isolated snippets.\n\n2. Complete and reproducible evaluation pipeline. The benchmark provides a fully automated, Dockerized workflow: natural-language task → JSON output → Gradle build → emulator test → fuzz evaluation. Both contain static and dynamic evaluation in multiple dimensions.\n\n3. Realistic and validated data construction. Tasks are grounded in real F-Droid apps, with automatic conversion by LLM and human-in-loop verification to guarantee the quality of the tasks. Also, the tasks contain many aspects and kinds of sub-tasks that are highly related to realistic development problems."}, "weaknesses": {"value": "1. The JSON format output is not flexible enough for evaluation. It is understandable that this work is mainly focused on evaluating LLM rather than agents, and such an evaluation method can avoid a lot of environmental problems. But for the SOTA models, agentic tool call and planning are important capabilities, which should be useful and need to be evaluated. Maybe can provide more evaluation for agentic workflows with different models.\n\n2. Currently, generating code based on JSON format may lead to the model needing to output a large amount of code at once when the task is complex. Long outputs may cause a decrease in the quality of the generated code. However, it is uncertain whether this potential problem can be solved by using an agent, COT, or other multiple polling methods. Does the author have any experiments or ideas on that? And it might be good to show the time-consuming and cost for agent evaluation."}, "questions": {"value": "1. It might be good to cite some secure code generation work as a shared related interest for LLM coding.\n\n[1] Seccodeplt: A unified platform for evaluating the security of code genai\n\n[2] SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code\n\n[3] CodeLMSec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models\n\n2. It might be good to cite more code agent works as a shared related solution for LLM coding.\n\n[4] OpenHands: An Open Platform for AI Software Developers as Generalist Agents\n\n[5] PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts on Formal Verification\n\n[6] Agentless: Demystifying llm-based software engineering agents\n\n3. For future work, it might be exciting to extend to other ecosystems, like iOS, web or desktop. Because different scenarios might involve different programming languages and unique challenges for LLM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L0oBuhKMS0", "forum": "XrP8dp1rCg", "replyto": "XrP8dp1rCg", "signatures": ["ICLR.cc/2026/Conference/Submission11555/Reviewer_Y9mg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11555/Reviewer_Y9mg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982242455, "cdate": 1761982242455, "tmdate": 1762922643105, "mdate": 1762922643105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a benchmark for tackling end-to-end app development. This provides a basis for assessing the abilities of models to be independent developers of complex applications and not just individual aspects of software engineering cycle. \n\nThe benchmark is a collection of 101 Android applications sourced from F-droid, the open source Andriod repository. An interesting question in building this is that of how does one obtain the specifications for the task. The paper makes use of application traces from the original apps and then uses LLMs to synthesize test cases from which LLMs generate app description.  The evaluations consist of test cases thus generated. The entire set of tasks were manually verified and validated by software developers. \n\nThe evaluations demonstrate that the tasks are challenging for current state-of-the-art models and coding agents."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark targets end-to-end application development. Relative to existing benchmarks this presents a new investigation, one that is also likely impactful. \n\n- The benchmark creation process is sound. The apps are sourced from real applications and thus represent real needs and the levels of complexity expected in real apps. \n\n- The evaluations test a wide range of models and some coding agents. The substantially low performance numbers indicate the difficulty of this task and provide a useful empirical point on the capabilities of LLMs for end-to-end software engineering. The analysis are generally sound and provide detailed insights into the failure modes and in some cases posits reasons for why. These provide useful insights for future work on this dataset."}, "weaknesses": {"value": "The generation process of the target task descriptions and the test cases are well motivated. I also see that the task descriptions and the test cases have been validated. However, I see some areas of improvement here.  \n- First,  it is not clear why this would correspond to how users would want to provide descriptions of apps. It would help to motivate how the authors envision such a system being used and how app development specifications are produced. \n\n- Second, the Android Developer Validation section can be improved in terms of details. Currently, it outlines the kinds of checks being done on the task descriptions and the test cases. However, it leaves out some details on what happened when developers found errors, inconsistencies, or completeness problems. Did they modify the descriptions manually? Did multiple developers iterate over each others validation process? Was there any specific validation that checked whether the test cases were clearly aligned with the task descriptions?\n\n- In practice, apps are rarely developed with full functionality in one go, and specifications themselves are often refined in the process of development. Is it reasonable to expect models to develop the entire functionality in one attempt? The process outlined allows for iterative development of functionality in the sense that failures are allowed to be corrected. I wonder if it will be useful to test, if feasible, incrementally add functionalities to the app. At the very least some acknowledgement of how software engineering for apps operates in practice and what differences is being posed in the current setting would be helpful."}, "questions": {"value": "See questions in weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sKfGNhAKYE", "forum": "XrP8dp1rCg", "replyto": "XrP8dp1rCg", "signatures": ["ICLR.cc/2026/Conference/Submission11555/Reviewer_Fhv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11555/Reviewer_Fhv9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125362626, "cdate": 1762125362626, "tmdate": 1762922642754, "mdate": 1762922642754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}