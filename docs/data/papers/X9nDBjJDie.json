{"id": "X9nDBjJDie", "number": 17787, "cdate": 1758280494905, "mdate": 1759897153926, "content": {"title": "Molecular Generation through Reasoning with Large Language Models", "abstract": "Molecule generation is significant for its potential in scientific discovery and practical applications, e.g., accelerating drug discovery by directly generating candidate molecules. Recent attempts often frame this task as a \\textit{translation} problem from molecular caption to structural representation, such as SMILES. This paper first examines the feasibility of modeling the task as a reasoning process with large language models (LLMs), generating higher-quality molecules through structural decomposition and recombination within Chain-of-Thought (CoT). We then introduce a workflow for curating accurate CoT data, incorporating both machine and expert verification. Lastly, we demonstrate that with a limited dataset of $4{,}213$ high-quality samples, namely \\textbf{MolCoT4K}, we elicit strong reasoning capabilities for molecule generation in open-source LLMs such as Qwen2.5-7B, achieving state-of-the-art exact match accuracy over strong open-source baselines (e.g., MolT5 and LlaSMol) as well as advanced commercial LLMs like GPT-4o. Moreover, the resulting model, \\textbf{MolGeneration}, attains a Pass@16 exact match accuracy of 48.46\\%, highlighting its strong potential for real-world experimental applications when supported by a feasible external verifier or chemistry experts. Our analysis shows that the correctness of the CoT path is crucial, while reasoning ability primarily enhances accuracy in fine-grained molecule generation. The dataset, model, and training codebase will be released to the community.", "tldr": "We construct chain-of-thought (CoT) data to train reasoning LLMs for molecular generation.", "keywords": ["Large Language Models (LLMs)", "Molecule Generation", "Supervised Fine-Tuning", "Reinforcement Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d679806665df10d834219342b978dc807449f19a.pdf", "supplementary_material": "/attachment/6fb275f767060294f2b22743154bec6eece188c9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MolGeneration, utilizing the reasoning process of LLMsfor molecular generation. To this end, the authors propose MolCoT4K, a dataset with 4,213 high-quality examples distilled from GPT-4o. The results show that MolGeneration achieves a Pass@16 exact match accuracy of 48.46%."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. It is definitely meaningful to apply reasoning in LLMs for molecular generation.\n2. It is also appreciated that the authors commit to open-source their models and datasets.\n3. The superior performance demonstrates the effectiveness of the porposed method on their dataset."}, "weaknesses": {"value": "1. The distilled dataset is rather limited, with only 4,213 entires, which can not demonstrate the real effectiveness generalizability of the model. When the training size of reasoning dataset scales up, the performance should be worse than SFT.\n2. The novelty is not enough. The entire framework seems an application of the DeepSeek R1 framework in molecular generation.\n3. The authors claim that they are the \"first to examinesthe feasibility of modeling the task as a reasoning process with large language models (LLMs)\". However, recent works like ether-0 [1] and MolReasoner [2] are earlier and seem more competitive.\n4. This work misses a lot of related works. The authors should make careful investigations.\n5. Poor baselines. All the selected models are kind of out-of-the-date or simply general LLMs. More recent models should be discussed.\n\n## References\n[1] Narayanan, S. M., Braza, J. D., Griffiths, R. R., Bou, A., Wellawatte, G., Ramos, M. C., ... & White, A. D. (2025). Training a Scientific Reasoning Model for Chemistry. arXiv preprint arXiv:2506.17238.\n[2] Zhao, G., Li, S., Lu, Z., Cheng, Z., Lin, H., Wu, L., ... & Gao, Z. (2025). Molreasoner: Toward effective and interpretable reasoning for molecular llms. arXiv preprint arXiv:2508.02066"}, "questions": {"value": "Please see in Weaknesses.\n\nI am happy to change my mind if the authors could resolve my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2QLUJLDw2", "forum": "X9nDBjJDie", "replyto": "X9nDBjJDie", "signatures": ["ICLR.cc/2026/Conference/Submission17787/Reviewer_YrgL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17787/Reviewer_YrgL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757127574, "cdate": 1761757127574, "tmdate": 1762927629660, "mdate": 1762927629660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce MolCoT-4K, a high-quality dataset of 4,213 Chain-of-Thought (CoT) samples that were first generated by an LLM and then manually verified and refined by chemical experts. They also use this small but potent dataset to fine-tune an open-source model to achieve good results on the MolGeneration dataset."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The curation of the MolCoT-4K dataset is crucial to the community.\n2. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. For the trajectory selected in the data curation, what if there are multiple trajectories that seem correct? Is there a way to automatically select the best trajectory? Or does it require a human expert to manually select one?\n2. The paper's evaluation on the MolCoT-4K dataset is missing crucial information. It omits the training dynamics for both the SFT and RL models. The paper does not show any learning curves, such as validation accuracy over training steps. Without this information, it is hard to confirm if the Chain-of-Thought (CoT) model's superior performance is genuinely due to the CoT method. It's just as possible that its win is due to overfitting or simply that the baseline (non-CoT) model was undertrained, making the comparison unfair.\n3. The paper's experimental setup on SMolInstruct is a key weakness because it fails to demonstrate generalization. It only trains and tests the model on splits of the SMolInstruct. Standard practice in reasoning research requires training on the proposed dataset and then evaluating the model on multiple, different external benchmarks to prove that it has learned a generalizable skill, which was not done here."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xMnXPgb9is", "forum": "X9nDBjJDie", "replyto": "X9nDBjJDie", "signatures": ["ICLR.cc/2026/Conference/Submission17787/Reviewer_JCtU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17787/Reviewer_JCtU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869983025, "cdate": 1761869983025, "tmdate": 1762927628839, "mdate": 1762927628839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “Molecule Generation Through Reasoning with Large Language Models” presents a new approach to molecular design by reframing molecule generation as a reasoning problem rather than a simple translation task. Traditional systems translate molecular descriptions directly into representations such as SMILES but often generate inaccurate or chemically implausible molecules and depend on large annotated datasets. In contrast, this work introduces a reasoning-based paradigm in which large language models (LLMs) explicitly decompose a molecule into substructures or functional groups and then recombine them step by step to form the final structure.\n\nTo support this approach, the authors construct MolCoT-4K, the first high-quality Chain-of-Thought (CoT) dataset for molecule generation. The dataset contains 4,213 samples created through a combination of machine generation and expert curation, ensuring chemically valid reasoning paths. Building on this dataset, they develop a two-stage training method that first applies supervised fine-tuning to teach models to produce reasoning traces and valid molecular structures, followed by reinforcement learning with customized molecular rewards to strengthen reasoning and output correctness.\n\nUsing this method, the authors train MolGeneration, an open-source molecular LLM based on Qwen2.5-7B, and demonstrate that it achieves state-of-the-art performance on the challenging SMolInstruct benchmark. The model attains an exact-match accuracy of 44.97%, surpassing both specialized models such as MolT5 and commercial LLMs like GPT-4 and Gemini-2.5. When multiple samples are allowed with expert verification (Pass@16), the model reaches 48.46% accuracy, showing strong potential for practical applications in drug discovery.\n\nThe study finds that the correctness of the reasoning trace is crucial for performance, and that reasoning primarily enhances fine-grained molecular accuracy, whereas traditional translation methods mostly improve coarse structural similarity. Overall, the paper demonstrates that explicit reasoning with LLMs can lead to more accurate, interpretable, and efficient molecule generation, establishing a promising direction for AI-driven chemical and pharmaceutical research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Conceptual Motivation\nThe paper proposes a well-articulated and timely idea—treating molecular generation as a reasoning task rather than a direct translation problem. This conceptual shift highlights the potential benefits of interpretability and structured decision-making in chemical design, offering a refreshing perspective compared to purely data-driven approaches.\n\n2. Introduction of the MolCoT-4K Dataset\nDespite its limited scale, the MolCoT-4K dataset represents a meaningful step toward integrating explicit reasoning into molecule generation. The dataset’s combination of GPT-generated reasoning traces and expert verification demonstrates an initial attempt to bring chemical reasoning structure into LLM-based modeling, bridging natural language reasoning with chemistry.\n\n3. Methodological Integration and Experimentation\nThe study effectively integrates multiple techniques—supervised fine-tuning, reinforcement learning, and reasoning-based prompting—into a cohesive experimental pipeline. The approach is well-executed from an engineering standpoint, showing technical competence and thoughtful design in aligning reasoning objectives with molecule validity constraints.\n\n4. Promising Empirical Results and Clarity of Presentation\nThe proposed model achieves notable performance gains over prior baselines on the SMolInstruct benchmark, especially in exact-match accuracy. The experiments are clearly presented, and the results successfully demonstrate that structured reasoning can improve fine-grained molecular correctness. The writing is also clear and organized, making the paper accessible to both NLP and chemistry audiences."}, "weaknesses": {"value": "1. Limited Innovation\nAlthough the paper introduces the idea of reasoning-based molecular generation, it mainly transfers the Chain-of-Thought (CoT) paradigm from natural language processing to chemistry without offering any substantial methodological or chemical reasoning innovation. The approach relies largely on fine-tuning and prompt design rather than introducing genuinely new modeling techniques or theoretical insights.\n\n2. Insufficient Dataset Scale and Quality\nThe proposed MolCoT-4K dataset contains only about 4,000 samples, which is far too small to support robust LLM training for chemistry. Despite claims of expert verification, the dataset lacks thorough chemical diversity analysis and clear validation of the reasoning chains. Many of the reasoning traces appear linguistically structured rather than chemically meaningful, raising doubts about their scientific soundness.\n\n3. Narrow and Shallow Evaluation\nThe evaluation focuses almost exclusively on text-to-SMILES translation accuracy, neglecting real-world chemical or pharmaceutical relevance. There is no assessment of molecular novelty, synthesis feasibility, or biological activity, leaving unclear whether the generated molecules have any genuine utility in drug discovery or chemical design.\n\n4. Poor Reproducibility and Weak Reasoning Analysis\nThe paper provides insufficient methodological details—such as model architecture, hyperparameters, and reinforcement learning reward functions—making the approach difficult to reproduce. Moreover, the analysis of reasoning quality is superficial, lacking quantitative evaluation or in-depth chemical interpretation. The examples shown do not convincingly demonstrate true chemical reasoning capability."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yDCjuOrCqS", "forum": "X9nDBjJDie", "replyto": "X9nDBjJDie", "signatures": ["ICLR.cc/2026/Conference/Submission17787/Reviewer_aAQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17787/Reviewer_aAQ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177699137, "cdate": 1762177699137, "tmdate": 1762927628285, "mdate": 1762927628285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}