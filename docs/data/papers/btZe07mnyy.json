{"id": "btZe07mnyy", "number": 14383, "cdate": 1758234262744, "mdate": 1759897373746, "content": {"title": "Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage  Visual Grounding", "abstract": "Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications. However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -- for example, selecting which trajectory better describes an event in the image. In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding. We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace generation), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios. Extensive experiments with over ten state-of-the-art VLMs reveal several interesting findings. For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with trajectory planning.", "tldr": "", "keywords": ["vision language model", "visual grounding", "benchmark"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60fa7c0b8e3e782c4c4dc75e5044b4f75ca15f56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Point-It-Out (PIO), a benchmark that evaluates embodied reasoning (ER) of VLMs via direct visual grounding rather than indirect multiple-choice or language-only formats. PIO defines a three-stage, hierarchical protocol: S1 (referred-object localization), S2 (task-driven pointing/ action affordance), and S3 (visual trace prediction / coarse 2D trajectories). The benchmark includes 600+ human-annotated datapoints that span indoor/household, kitchen, driving, and robotic manipulation settings and reports results for more than 10 models. Results show that models with explicit grounding supervision (MoLMO, Qwen-VL, RoboRefer) outperform general VLMs on S1/S2, while generalist reasoning models (Gemini-2.5-Pro, GPT-o3) perform better on the motion-oriented S3 task. The authors also propose a normalized IoU to fairly compare point vs. box predictions in S1/S2, and assess S3 performance through human ratings and a prompted LLM evaluator."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The core idea of evaluating embodied reasoning via explicit, precise visual grounding (points, bounding boxes, trajectories) as a direct output is a clear advancement over indirect, language-only, or multiple-choice evaluations.\n2. The three-stage hierarchy is well-justified, capturing the natural progression of complexity from basic localization to affordance reasoning to temporal planning.\n3. The extensive evaluation of 10+ VLMs and the derived insights regarding the trade-off between specialization (grounding-fine-tuned models excel at S1/S2) and generalization/planning (generalist models excel at S3) are valuable.\n4. The paper is well-written, making contributions easy to understand. The figures are effective at conveying the core ideas and results."}, "weaknesses": {"value": "1. While diverse, the size of the benchmark is relatively small compared to other recent embodied benchmarks (Table 1, RoboRefIt 10k, EmbSpatial-Bench 3.6k). Given the split across 4 scenarios and multiple subclasses, this small size may limit the statistical robustness of the findings across all fine-grained categories. A discussion on the challenges and potential plans for scaling the benchmark would be welcome.\n2. The absence of GT trajectories is understandable, but the combination of human and LLM raters warrants inter-rater reliability reports.\n3. The evaluation of visual traces in S3 relies on human and GPT-4o ratings. While justified by the multimodal nature of trajectories, the lack of an objective ground-truth path or a more rigorous metric is a limitation. Expanding on the limitation could strengthen the paper."}, "questions": {"value": "1. Could more detail be provided on the human evaluation process for the S3 task? How many annotators rated each trajectory, and what was the inter-annotator agreement? Was the performance of the GPT-4o evaluator validated against the human scores to ensure its reliability?\n2. Do higher S3 scores correlate with success rates in a lightweight simulator or a small real-robot eval? A correlation study could strengthen the causal claim.\n3. The paper notes that S1/S2 proficiency doesn't translate to S3 success for models like MoLMO and Qwen. Could this also be related to their model architecture, or training data attribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wDvEaenafV", "forum": "btZe07mnyy", "replyto": "btZe07mnyy", "signatures": ["ICLR.cc/2026/Conference/Submission14383/Reviewer_dRu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14383/Reviewer_dRu3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837701612, "cdate": 1761837701612, "tmdate": 1762924797892, "mdate": 1762924797892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Point-It-Out (PIO), a benchmark for evaluating the embodied reasoning abilities of vision-language models (VLMs) through precise visual grounding instead of multiple-choice tasks. Covering four domains—household, kitchen, driving, and robot manipulation—PIO defines a three-stage hierarchy: (S1) referred-object localization, (S2) task-driven grounding involving affordance or contact reasoning, and (S3) visual trace prediction for temporal reasoning. With over 600 human-annotated samples using polygon-based masks and fine-grained subclasses, the authors benchmark more than ten VLMs (e.g., GPT-4o, Claude-3.7, Gemini-2.5, MoLMO-7B, Qwen2.5-VL, RoboRefer) and find that models trained with explicit grounding supervision outperform general-purpose ones on spatial reasoning (S1/S2), while generalist models show advantages in temporal planning (S3)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper goes beyond MCQ-style or language-only benchmarks by enforcing pixel-level output (points, boxes, trajectories) that directly connects perception and action, addressing an underexplored gap in embodied reasoning evaluation.\n\nThe three-stage structure (S1–S3) is conceptually clear and mirrors increasing task complexity—from perception to reasoning to action planning—providing diagnostic insights into different reasoning stages.\n\nThe authors test a wide range of both closed and open VLMs, highlighting clear trends and limitations, and propose practical guidelines for model users."}, "weaknesses": {"value": "The contributions are somewhat limited. Although the PIO benchmark addresses a gap in evaluating embodied reasoning through visual grounding, the concept itself has been well explored in prior works. The paper lacks deeper methodological innovations or proposals to improve model performance beyond dataset design.\n\nThe experimental section does not include a single quantitative table, making the results difficult to follow and reducing the readability and clarity of the analysis.\n\nIn Stage S3, GPT-o4-mini is used as the automatic judge for trajectory evaluation, but its rationality and reliability should be further justified through detailed case studies. This design choice may also introduce bias that favors GPT-series models."}, "questions": {"value": "How scalable is the benchmark—could it be extended to 3D or video-based embodied data?\n\nHow consistent are human annotations across annotators, and were inter-rater reliability metrics computed?\n\nFor S3, how reliable is the GPT-o4-mini auto-scoring compared to human ratings—was any correlation analysis performed?\n\nHave the authors examined whether S1/S2 performance correlates with actual robot execution success (e.g., using RT-1 or AgiBot replay)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8VpvJtECWR", "forum": "btZe07mnyy", "replyto": "btZe07mnyy", "signatures": ["ICLR.cc/2026/Conference/Submission14383/Reviewer_Gmue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14383/Reviewer_Gmue"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916038554, "cdate": 1761916038554, "tmdate": 1762924797512, "mdate": 1762924797512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an evaluation-only benchmark designed to test whether vision–language models (VLMs) can explicitly ground their reasoning in visual space. Unlike text-based embodied reasoning tasks, PIO focuses purely on spatial outputs rather than verbal answers, assessing models’ ability to predict bounding boxes, contact points, and 2D trajectories across three hierarchical stages: object localisation, task-driven grounding, and visual trace prediction.\n\nThe benchmark comprises roughly 600 human-annotated samples drawn from real-world robotic, driving, and indoor datasets, covering diverse embodied scenarios. Over ten existing VLMs—including general models such as GPT-4o, Gemini, and Claude, and grounding-specialized models such as Qwen2.5-VL and RoboRefer—are evaluated in a zero-shot setting using unified, structured prompts.\n\nResults show that grounding-trained models perform better in localisation and affordance prediction, while general large models do relatively better in trajectory planning. No model succeeds consistently across all three stages, which the authors interpret as evidence that current VLMs remain spatially ungrounded and lack integrated embodied reasoning. PIO thus serves as a diagnostic benchmark that quantifies the gap between perception, grounding, and action reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shifts embodied reasoning assessment from text-based QA to explicit visual grounding, offering a new angle on how well VLMs can connect perception with spatial understanding.\n\n2. The three-stage hierarchy (referred object localization, task-level grounding, trajectory prediction) conceptually makes sense and reflects a realistic progression from perception to action, making the benchmark easy to interpret and reproducible.\n\n3. PIO integrates data from multiple embodied domains: household, driving, and robotics, and evaluates a wide range of both general and specialized VLMs, providing a comprehensive empirical comparison.\n\n4. The authors introduce a unified interface for structured outputs (boxes, points, trajectories), ensuring consistent evaluation across models with very different architectures and capabilities."}, "weaknesses": {"value": "1. The benchmark evaluates all models as if they were trained to output pixel-level coordinates, masks, or trajectories, even though most general VLMs (e.g., GPT-4o, Gemini) were never optimized for such structured outputs. This conflates grounding ability with format familiarity, making cross-model comparisons unfair and potentially misleading.\n\n2. PIO equates “embodied reasoning” with the ability to generate spatial outputs. A model that conceptually understands the task or internally attends to the correct region but cannot produce explicit coordinates is penalized as incorrect, reducing reasoning to mere coordinate prediction.\n\n3. The benchmark never analyzes whether accurate boxes or trajectories correspond to correct semantic understanding or successful task completion, leaving unverified the claim that grounding precision reflects reasoning quality.\n\n4. Since all models are evaluated zero-shot with a fixed prompting template, poor performance may stem from interface incompatibility rather than genuine reasoning failure. Consequently, the findings expose prompting and modality gaps more than fundamental reasoning limitations."}, "questions": {"value": "1. Given that many general-purpose VLMs were never trained to output coordinates, masks, or trajectories, how do the authors ensure that PIO evaluates reasoning ability rather than task-format familiarity?\n\n2. The benchmark equates embodied reasoning with spatial output accuracy. Can the authors justify this definition and explain why linguistic or internal-attention-based reasoning should be considered invalid or insufficient?\n\n3. Have the authors analyzed whether models that produce accurate boxes or trajectories also demonstrate higher semantic or task-level correctness? Without such correlation, how is grounding quality linked to reasoning ability?\n\n4. Do the authors believe that differences in output structure or prompt compliance might explain much of the performance gap between general and grounding-trained models? Would minimal adaptation or instruction-tuning change these results?\n\n5. If a model correctly answers a task conceptually but fails to output precise coordinates, should this truly count as a reasoning failure? How might the benchmark be extended to capture implicit reasoning without requiring explicit coordinate prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hekYKMFDbz", "forum": "btZe07mnyy", "replyto": "btZe07mnyy", "signatures": ["ICLR.cc/2026/Conference/Submission14383/Reviewer_Vqkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14383/Reviewer_Vqkf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922832505, "cdate": 1761922832505, "tmdate": 1762924797051, "mdate": 1762924797051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new benchmark for evaluating the visual grounding ability of vision-language models (VLMs) in embodied AI tasks. The benchmark contains approximately 600 QA pairs, categorized into three types: referred-object localization, task-driven pointing, and visual trace prediction across diverse scenarios. It further evaluates both open-source and closed-source MLLMs, offering valuable insights derived from the comparative results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This work focuses on visual grounding in images, providing an interpretable way to compare the visual understanding capabilities of different VLMs.\n\nIt designs three task types that consider the use of VLMs as an interface for low-level policy learning."}, "weaknesses": {"value": "The proposed tasks including referred-object localization, task-driven pointing, and visual trace prediction have already been explored in prior work, making the task-level novelty of the benchmark relatively limited.\n\nThe overall work is engineering-oriented, focusing primarily on data curation rather than introducing new methods to enhance MLLM performance, which reduces the novelty and conceptual contribution of the paper.\n\nThe tasks span household rooms, kitchen environments, driving scenes, and robotic manipulation scenarios, which results in some domain overlap. Moreover, the motivation for including driving scenes is unclear and appears loosely connected to embodied reasoning."}, "questions": {"value": "Would it be possible to conduct a systematic evaluation of embodied agents across multiple dimensions  including referred-object localization, task-driven pointing, and visual trace prediction by combining existing benchmarks instead of creating a new one?\n\nIt would be helpful to briefly discuss the cost and feasibility of performing benchmarking experiments on PIO.\n\nCould you clarify the motivation for dividing the scenarios into household rooms, kitchen environments, driving scenes, and robotic manipulation settings?\n\nHow many different types of robots, object categories, and object properties (e.g., color, material) are included in this benchmark? Providing such statistical details would strengthen the clarity and completeness of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cQaXqUDYp9", "forum": "btZe07mnyy", "replyto": "btZe07mnyy", "signatures": ["ICLR.cc/2026/Conference/Submission14383/Reviewer_XKnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14383/Reviewer_XKnN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14383/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939004496, "cdate": 1761939004496, "tmdate": 1762924796573, "mdate": 1762924796573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}