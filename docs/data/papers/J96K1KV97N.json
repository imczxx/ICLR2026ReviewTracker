{"id": "J96K1KV97N", "number": 8490, "cdate": 1758086836800, "mdate": 1759897781121, "content": {"title": "Revisiting Distribution Reconstruction via Sample Adaptability: Diffusion-Guided Data-Free Quantization", "abstract": "Data-Free Quantization (DFQ), which seeks to align a low-bit quantized network Q with its full-precision counterpart P without access to original training data, has attracted growing attention. The core idea is to synthesize reconstructed samples that approximate the underlying distribution of real data. However, we observe that reconstructed samples in existing arts are adaptive only to Q with specific bit widths, rather than all bit widths, especially ultra-low bit widths. This raises a key challenge: how to rectify distributional information during sample reconstruction to produce desirable samples that generalize across varied quantization levels. In this paper, we revisit distribution reconstruction via sample adaptability, revealing that 1) the desirable samples enjoy benefits of rectifying distribution reconstruction via adaptability information; beyond that, 2) the forward diffusion process is an optimal noise strategy to rectify reconstructed samples for obtaining desirable samples; and propose a novel Diffusion-Guided Data-Free Quantization approach, dubbed DiffDFQ. Unlike conventional direct optimization, we rectify distribution reconstruction via noise diffusion in a progressive approximation manner. Technically, we decompose DFQ into three stages: sample synthesis to obtain reconstructed samples; upon that, sample diffusion to progressively infuse the noise via forward diffusion process, yielding desirable samples for varied Q; and network calibration to calibrate Q with progressive selection strategy over a series of diffused samples. Our DiffDFQ enjoys the appealing insights: 1) the diffused samples exhibit effective balance between distribution reconstruction and sample adaptability to facilitate varied Q, especially ultra-low bit widths, e.g., 2 bits and 3 bits; notably, 2) unlike the generator-reliance arts requiring up to 1.2M synthetic samples, DiffDFQ synthesizes merely 5.12K (1K) samples to earn performance gain over ImageNet for classification. Our empirical studies verify the merits of DiffDFQ over state-of-the-arts for classification across varied bits to Q. Our code is available in the supplementary material package.", "tldr": "", "keywords": ["Data-free quantization; distribution reconstruction; sample adaptability; diffusion models;"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f0827945fb33176a6cb34535286e35214f92958.pdf", "supplementary_material": "/attachment/b9b7bf794c78da0d22e2f83217f29e0d5de2bab6.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to improve data-free quantization by generating better synthetic samples. Specifically, the paper proposes adding Gaussian noise to the synthetic samples, following the noise scheduler from diffusion models. DiffDFQ first shows that diffusion-like Gaussian noise is an optimal noise strategy for approximating per-sample gradients from the quantization gap between the full-precision model and the quantized model. Then, the paper empirically validates that different levels of noise are needed for different bitwidths. Based on these observations, the paper proposes using synthetic samples with adaptive Gaussian noise for data-free quantization. Experimental results on CIFAR and ImageNet benchmarks show that the proposed method achieves accuracy gains, especially in low-bit QAT settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The paper provides a theoretical basis for adding Gaussian noise to approximate gradients between a full-precision model and its quantized variant.\n* The proposed method consistently shows better quantization accuracy, empirically validating it as a superior training strategy.\n* The paper offers a detailed background section, which helps readers understand the proposed method."}, "weaknesses": {"value": "* The composition of the paper is awkward.\n\nThe paper presents a 3-page introduction section that includes detailed background, motivational experimental results, problem formulation, observations, and even anticipated reader questions about the method. Furthermore, the second section after the introduction is the method section, which is combined with the preliminaries. The experiment section also includes details on quantization and related work. This composition not only results in lengthy sections but also makes it difficult for readers to follow. Therefore, this reviewer suggests separating the sections and reducing the amount of detail in the introduction.\n\n* The theoretical basis is questionable.\n\nWhile this reviewer acknowledges that providing a theoretical basis is a strength of the paper, the conclusion is not directly aligned with the proposed method. Specifically, the conclusion of Proposition 2 should be that using Gaussian noise is optimal, but the paper instead claims it as a forward diffusion process. Although the forward diffusion process also uses Gaussian noise, the detailed proof in Appendix A.3 concerns only Gaussian noise, not the forward diffusion process, which includes a noise schedule. Moreover, in the proof, the forward diffusion process cannot be applied to Equation 18, since the forward diffusion process represents an interpolation between the sample and the noise, not simply the addition of Gaussian noise (i.e., $E[\\Delta M]$ is not zero because x differs). Therefore, the proof demonstrates that the proposition applies only to additive Gaussian noise, not to the forward diffusion process.\n\n* The term adaptability should be further clarified.\n\nThe paper states that the reconstructed samples are usually adaptive to specific bitwidths rather than all bitwidths. However, the basis for this claim is the lower accuracy of 3-bit quantization compared to 4-bit quantization. This reviewer believes that the lower accuracy of 3-bit quantization is natural and primarily caused by harsher quantization errors, not by sample quality. Moreover, the explanation of the adaptive strategy is too shallow, making the method difficult to reproduce."}, "questions": {"value": "Please refer to the weakness section.\nAdditionally, here are minor questions and suggestions.\n* The citation format of the paper could be improved. This reviewer suggests using parentheses in citations.\n* Figure 2 (b): Aadaptability -> Adaptability\n* How do easy samples, which induce a small gap between the full-precision model and its quantized variant, achieve better accuracy?\n* There are no complete experimental results on the CIFAR dataset, while the ablation study and visual analysis are based on CIFAR. This reviewer believes that including ablation and analysis results on the ImageNet dataset could strengthen the paper, as the main experimental results are derived from ImageNet."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VUf0HxnAp3", "forum": "J96K1KV97N", "replyto": "J96K1KV97N", "signatures": ["ICLR.cc/2026/Conference/Submission8490/Reviewer_r6c5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8490/Reviewer_r6c5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811448978, "cdate": 1761811448978, "tmdate": 1762920364772, "mdate": 1762920364772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In data-free quantization, which is a research field that quantizes target neural networks without real data, synthetic dataset are generated to replace the original dataset and preserve the performance of target models.\nGenerally, a synthetic dataset is optimized for the target bit setting.\nThe paper contends that synthesized samples are adaptive to the target bit setting only, and using them to quantize a model to a different bit setting leads to performance degradation.\n\nTo resolve this problem, the paper adopts a diffusion forward process that is used to train diffusion models that synthesize realistic images or videos.\nThe paper proposes to construct additional synthetic datasets by adding noise perturbation to the original synthetic dataset for quantization.\nAlso, the paper proposes progressive sample selection that uses fewer noise samples step by step while quantization.\n\nBy integrating the proposal with prior works, the paper achieves performance improvement on various models and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper defines the problem situation of quantization well that people have not much thought of before.\n\n- It is an original approach that using diffusion process to quantization.\n\n- Compared to various existing studies, the paper shows the performance improvement effect of the proposed method."}, "weaknesses": {"value": "- Even though readers can infer the meanings, the authors should have defined the key terms, such as sample adaptability and distribution reconstruction, with their own words.\n\n- The connection between propositions 1 and 2 is weak. In Proposition 1, the direction of the perturbation should be the same as that of M's gradient. However, perturbations in the diffusion process usually follow a Gaussian distribution. It is difficult to understand the relationship between the argument of proposition 2 and the direction of perturbation by the given content alone.\n\n- The paper executed experiments of the proposed method based on other works. To show the effectiveness of the proposal clearly, it would be better to execute ablation studies without prior works such as IntraQ, HAST, and Genie.\n\n- The paper tackles that the synthetic dataset is adaptive to quantization bit, thus using this dataset to quantize models with other bit settings can harm their performance. However, it seems that there isn't any intervention of a quantization model or quantization bit settings while generating a synthetic dataset generation based on reconstruction.\nIt runs counter to the argument.\n\n- Typo: In the section 3.3.1, there are consecutive commas like ', ,'."}, "questions": {"value": "- What if a target model is quantized with the whole dataset, including diffused samples of every step?\n\n- According to the paper's argument, the performance of a quantized model to 4 bit would be degraded if it is quantized with a synthetic dataset generated under 3 bit setting.\nTo strengthen the paper's argument of the problem, can the authors execute other experiments about quantizing the model to 4 bit with 3 bit dataset, and compare it with the opposite setting?\n\n- In the experiments of Section 3.5, the paper contends that the quantization complexity of HAST + DiffDFQ is lower than that of HAST itself.\nDoes it mean that the experimental settings for them are different? Or does HAST + DiffDFQ stop early before the experiment ends?\nAccording to the experimental setting, the number of epochs is 400, and in this case, the complexity of both methods is the same.\nIf the intention is that HAST + DiffDFQ converges faster than HAST itself, the reviewer think that it is not a complexity problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qVOudDJuJ2", "forum": "J96K1KV97N", "replyto": "J96K1KV97N", "signatures": ["ICLR.cc/2026/Conference/Submission8490/Reviewer_JsvL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8490/Reviewer_JsvL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900248821, "cdate": 1761900248821, "tmdate": 1762920364208, "mdate": 1762920364208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new data-free quantization (DFQ) method called DiffDFQ. The core idea of this paper is to solve two problems in existing methods: (1) the data synthesized by the existing methods is only applicable to a special bit width. Once it is used for the ultra-low bit width (such as 3-bit), the performance will decline sharply; (2) some methods that try to solve the adaptability problem will damage the quality of synthetic data in order to meet the gap between FP model P and quantized model Q. This paper points out that P and Q should not pursue \"zero gap\". On the contrary, there is an ideal gap M*. Then, this paper proves that the forward diffusion process (gradually adding noise) is the optimal noise strategy approaching M*. An three-stage algorithm is proposed: (1) sample synthesis: it first synthesize a batch of well reconstructed samples x_0; (2) sample diffusion: perform forward diffusion (plus noise) on x_0 to obtain a \"calibration set\", which contains a series of samples from x_t (noisy) to x_0 (clean); (3) progressive calibration: wen training Q, it first use the sample with high noise (such as x_t) for training, and then gradually transition to the one with low noise (such as x_0)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive experimental results.\n\n2. The theoretical proof is logical."}, "weaknesses": {"value": "1. The paper proves that diffusion is an efficient disturbance strategy, but it does not (and cannot) prove that the final approximation of this disturbance is the theoretical M*. \n\n2. This method seems specified to solve the 3-bit case. On 4-bit and 5-bit, the benefits of this complex diffusion and progressive calibration can be almost ignored by experimental errors. This has reduced the universality claimed in the paper.\n\n3. Many concepts in this paper make me very confused. I have done my best to read this paper, but I still can not find out the meaning of reconstructed samples and distribution reconstruction. Also, what do you mean when you say ‘AdaDFQ achieves the enhanced adaptability across varied bit widths, but compromises the fidelity of original reconstructed information for Q∗’? How do you support this argument?\n\n4. This paper seems to generate x_0 at first and then add noise to it. Is that so? This paper also claims that this method shows a generalization across bit-widths. How did you embody this configuration in your experiment? \n\nI do believe this paper need to be polished greatly to make it easy to read. The current version is not suitable for the reader following. I tend to reject this paper, not only for it vague writing but also for some technical flaws."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Hc5SxPE8M", "forum": "J96K1KV97N", "replyto": "J96K1KV97N", "signatures": ["ICLR.cc/2026/Conference/Submission8490/Reviewer_DYmb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8490/Reviewer_DYmb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157390493, "cdate": 1762157390493, "tmdate": 1762920363840, "mdate": 1762920363840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key challenge in Data-Free Quantization (DFQ) , where synthetic data used to calibrate a quantized network (Q) often fails to generalize across different bit widths, particularly for ultra-low bit (e.g., 2 or 3-bit) models. The authors observe that existing synthetic samples lack \"sample adaptability,\" and methods that optimize for adaptability tend to compromise distribution reconstruction fidelity. They propose Diffusion-Guided Data-Free Quantization (DiffDFQ), a three-stage approach. First, it uses an existing method to synthesize reconstructed samples. Second, it applies a forward diffusion process (progressively adding noise) to these samples to create a \"calibration set\" of diffused samples at various noise levels. Third, it calibrates the quantized network using a \"progressive sample selection strategy\" that utilizes this diffused sample set. The authors claim this diffusion process rectifies the samples, striking an effective balance between reconstruction and adaptability, thus yielding \"desirable samples\" that improve performance for Q, especially at ultra-low bit widths."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provide significant performance improvement on 3-bits.\n2. The paper includes several useful ablation studies that validate the contributions of its core components"}, "weaknesses": {"value": "1. The writting of this paper is poor. It takes me long time to understand the writting even I am a expert in quantzization. The writing should be re-organized.\n2. This paper show negligible performance improvement on 4-bits, but only achieve benefit on 3-bit. However, I doubt the practical usage of 3-bit data free quantization in real scnarios. In my mind, 3-bit quantization suffers from significant performance degeneration and can not be directly use. Therefore, 3-bit model has to rely on QAT on real samples.\n3. The claim of improved training efficiency is based on a single cherry-picked data point, while other results show the benefit is marginal and not universal. The paper boasts that DiffDFQ reduces convergence time by \"at most 35.5%\" , citing the 4-bit case (18.96h vs 12.23h). However, the same table shows that for the 5-bit case, the time-saving is a mere 11.2% (16.83h vs 14.94h). Similar to the accuracy gains, this suggests the claimed efficiency benefit is not general but is instead another best-case result from a specific configuration, while the gains diminish at other bit-widths.\n4. The \"Progressive Calibration\" stage is just standard curriculum learning, but the paper fails to acknowledge this or position its work within that field."}, "questions": {"value": "Please refer weakness for detials."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JzNvhz5Vgs", "forum": "J96K1KV97N", "replyto": "J96K1KV97N", "signatures": ["ICLR.cc/2026/Conference/Submission8490/Reviewer_2WQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8490/Reviewer_2WQw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762159456391, "cdate": 1762159456391, "tmdate": 1762920363421, "mdate": 1762920363421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}