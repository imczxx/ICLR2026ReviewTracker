{"id": "J26acVpqAu", "number": 4004, "cdate": 1757583131909, "mdate": 1759898058635, "content": {"title": "Gaussian Process Attention with Kernel Modeling", "abstract": "Transformers dominate NLP, yet their core component, self-attention, remains a heuristic, lacking a robust theoretical foundation. This paper reinterprets self-attention combined with rotary positional embeddings (RoPE) as an instance of Nadaraya-Watson (NW) kernel regression, unlocking a novel framework for enhancing attention through kernel modeling. \nWe introduce Gaussian Process Attention (GPA), a novel technique, which leverages kernel theory to augment RoPE with a bank of decaying periodic functions.  We demonstrate the result better captures the fine-grain characteristics of mutual information between tokens as a function of the distance between them. \nTested on a GPT model using character-based tokenization and trained on a corpus of 14-million-characters, GPA outperforms baseline RoPE, reducing mean cross-entropy loss.  More importantly, with only a nominal increase in the number of model parameters, GPA reduces computation time by 75\\% for an equivalent level of mean cross entropy loss. Moreover, we show that our GPA model opens a new avenue for studying mechanistic interpretability, revealing  structures, such as paragraph lengths, and identifying redundant attention heads for model pruning. Our work bridges the application of kernel methods and the study of Transformers, providing a theoretical lens for analyzing self-attention while also delivering practical, scalable gains in performance and interpretability.", "tldr": "Gaussian Process Attention reframes self‑attention as kernel regression, augmenting RoPE with learned decaying periodic kernels to improve efficiency, accuracy, and interpretability in Transformers.", "keywords": ["Transformers", "Kernel Methods", "Gaussian Processes", "Interpretability", "Language Modeling"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54795dfd708714551fb66eb2c6fe8932d420ecd5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Thanks for the submission. The paper considers position encodings based on modulating the attention logits between tokens i and n by a function of $|i-n|$, with this function parameterised as a sum of products of learnable ‘kernel banks’ (Eqs 3 and 4) – essentially, an extra relative position encoding (RPE). Compared to if RoPE is used on its own, this extra encoding improves performance in a small GPT model. The authors interpret the ‘bump’ in the learned RPE function as corresponding to mean paragraph length."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Training a small GPT model is a nice (and fairly ambitious!) experiment. Improving position encodings is an interesting and important research direction. Interpret learned RPE patterns and relating them to the underlying text structure makes good sense."}, "weaknesses": {"value": "1. ‘Gaussian process attention’ is a misnomer. The paper is unrelated to GPs – stochastic processes for which all finite collections of random variables are jointly normally distributed. The authors use ‘GP’ because GPs use kernels and they parameterise their new position encoding using kernels (line 129). To me, this doesn’t make much sense, and is a little misleading. There's nothing probabilistic here. \n2. $G(x_n, x_i)$ is a learnable function of $n - i$, parameterised using functions $P_k$ and $D_k$. As Eq 6 makes clear, this is incorporated as a relative position encoding (though I don’t think the authors ever explicitly describe it as such). RPEs are not novel. \nIf the contribution of the paper is actually a different parameterisation of RPEs, I think the text and experiments should be reworked to reflect this. \n3. I also think the authors should compare to more sensible baselines, e.g. different existing RPE parameterisations. It’s not at all surprising that RoPE + RPE beats RoPE on its own – really, we want to know whether this RPE is better than previous RPEs.\n4. Minor points:  issues with citation formatting – if the citation is not to be read as part of the flow of the text, I think it ought to be in brackets.\n\nIn light of missing baselines and a misleading framing (GPs aren't involved and this is really just and RPE), I don’t think this paper should be accepted to ICLR."}, "questions": {"value": "1. Have I missed the point here, or is paper indeed about a (possibly novel) RPE parameterisation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "97KaEvvFQA", "forum": "J26acVpqAu", "replyto": "J26acVpqAu", "signatures": ["ICLR.cc/2026/Conference/Submission4004/Reviewer_hepz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4004/Reviewer_hepz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760710978281, "cdate": 1760710978281, "tmdate": 1762917132786, "mdate": 1762917132786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper models attention as an instance of kernel density regression, which provides a framework to further analyze and interpret transformers. Built on this foundation, the paper proposes Gaussian Process Attention, a type of kernel attention combined with rotationary positional embedding (RoPE). Language modelling experiments on a text dataset show marginally better result than RoPE."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper provide some interesting empirical analyses on the influence of the text data distribution on the performance of the proposed method."}, "weaknesses": {"value": "- Representing self-attention as nonparametric kernel density regression is not a novel idea. Several works in the literature have pointed out this interpretation of self-attention [1]. \n- The paper aims to interpret self-attention as kernel regression, however it lacks entirely the relevant discussion on transformer interpretability [1, 5] and Gaussian Process representation of attention [2, 3, 4].\n- The paper suffers from sub-standard writing quality. The introduction is particularly poorly written with few convincing arguments to motivate the method.\n- The experiments in the paper do not include relevant baselines such as kernel attention [5], Gaussian Process attention [2,3,4], making it difficult to evaluate the effectiveness of the proposed method.\n- The experiments are very limited to just one text dataset. It is crucial to include more datasets in diverse settings to show the efficacy of the proposed method.\n- I do not see the reason why the authors refer to this model as a Gaussian Process attention since it does not compute any uncertainty or covariance and the resulting attention (GPA in the paper) is essentially the mean function of a Gaussian Process.\n\n[1] FourierFormer: Transformer Meets Generalized Fourier Integral Theorem\n[2] Revisiting Kernel Attention with Correlated Gaussian Process Representation\n[3] Calibrating Transformers via Sparse Gaussian Processes\n[4] Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes\n[5] Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3jjr41l2Ge", "forum": "J26acVpqAu", "replyto": "J26acVpqAu", "signatures": ["ICLR.cc/2026/Conference/Submission4004/Reviewer_Go2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4004/Reviewer_Go2L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944352581, "cdate": 1761944352581, "tmdate": 1762917132565, "mdate": 1762917132565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work assesses the self attention in transformers as a form of kernel regression, and presents a novel augmentation called Gaussian Process Attention.  Results demonstrate improved accuracy and efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The formulation of rope based attention in the vein of Nadaraya-Watson kernel regression is an interesting and potentially highly impactful research direction. \n\nThe claimed 75% reduction in compute time is substantial, showing the potential to significantly improve convergence rates if this result can be generalised."}, "weaknesses": {"value": "As emphasised in the Conclusion, the experiments are limited to a small corpus on a small model. A necessary first step, but the gulf between this character-level tokenisation approach and a modern LLM is so great that the claims outlined in the abstract feel very overstated.\n\nThe work is lacking in ablation studies. While it is interesting to speculate on the paragraph structure's impact on the kernel, what would be more compelling of course is to perform more experiments, e.g. retrain on a corpus with a different paragraph style and verify whether the bump moves as expected.\n\nThe introduction is incomplete and does not adequately cover the existing literature in this area - there have been very many attempts to connect attention and kernel methods, including NW regression. eg see \"Elliptical Attention\" (Nielsen et al., 2024); Correlated GP Transformer (Bui et al., 2024)."}, "questions": {"value": "What other kernel designs have you considered beyond the decaying periodic?\n\nHow do you see your work in relation to other recent works exploring the connection between kernels and attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kyfydc0MKN", "forum": "J26acVpqAu", "replyto": "J26acVpqAu", "signatures": ["ICLR.cc/2026/Conference/Submission4004/Reviewer_TBx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4004/Reviewer_TBx3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990193071, "cdate": 1761990193071, "tmdate": 1762917132350, "mdate": 1762917132350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reinterprets self-attention with Rotary Positional Embeddings (RoPe) as a form of Nadaraya-Watson kernel regression. Based on this view, the authors propose Gaussian Process Attention (GPA), which augments RoPe with a learned bank of decaying periodic kernels designed to more flexibly model how mutual information between tokens varies with positional distance.\nThe kernel bank consists of K decaying periodic kernels, each parameterized by an amplitude, decay scale, and wavelength, enabling the model to learn richer patterns of long-range and periodic dependencies than RoPe alone. GPA modifies the attention score by additively combining the RoPE-modulated dot-product score with this kernel mixture. The model is evaluated on a small character-level GPT, for the task of next token prediction. The authors report: ~2% improvement in perplexity over RoPe alone and ~75% reduction in compute required to reach equivalent validation loss, with only 0.03% increase in parameter count. Additionally, the structure of the learned kernels reveals interpretable patterns (e.g. a peak at lag ≈70 corresponding to average paragraph length), and some heads effectively collapse, suggesting potential for attention-head pruning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides an intuitive reinterpretation of RoPe based attention as a form of Nadaraya–Watson kernel regression.\n- The proposed GP Attn. only introduces around 4k additional parameters per headcmaking the method practical, lightweight, and easy to integrate into existing transformer architectures.\n- The combined RoPe + kernel bank model reaches the same cross-entropy performance as the baseline with ~75% fewer training steps, demonstrating meaningful improvements in training compute efficiency.\n- Analysis of the learned kernel shapes reveals alignment with structural properties of the data, e.g. the bump in the kernel shape at lag 70 corresponding to avg. new line separation length."}, "weaknesses": {"value": "- While the bump at lag ~70 is interesting, I feel, the interpretability claims are anecdotal rather than systematically developed. Additional analysis would be needed to establish whether GPA consistently yields more interpretable heads or whether such patterns are dependent on the data.\n- Not sure about the evaluation framework and character level tokenisation task that the authors present. There may be a reason that the authors pick this style rather than the traditional tokenisation in current GPT style next token prediction models. Would the results generalise to the traditional subword tokenisation setting?\n- For a rigorous experimental suite there should be some mention of why there are no comparisons to other kernlised attention approaches or positional alternatives."}, "questions": {"value": "- The reported ~75% reduction in compute arises from faster convergence, not from a reduction in FLOPs or memory per step?\n- It is difficult to assess whether GPA is competitive relative to existing efficient attention methods...without any comparisons to baselines.\n- The kernel bank uses K=64 decaying periodic kernels per attention head, but many of the learned kernels appear qualitatively similar (Figure 4). Could the authors provide an ablation over K? to understand if the performance benefits require a large kernel bank?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7DX61J47BD", "forum": "J26acVpqAu", "replyto": "J26acVpqAu", "signatures": ["ICLR.cc/2026/Conference/Submission4004/Reviewer_r6Wh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4004/Reviewer_r6Wh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762370889271, "cdate": 1762370889271, "tmdate": 1762917132164, "mdate": 1762917132164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}