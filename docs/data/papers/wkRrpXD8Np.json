{"id": "wkRrpXD8Np", "number": 13833, "cdate": 1758223382691, "mdate": 1759897409656, "content": {"title": "Can Large Reasoning Models Self-Train?", "abstract": "Recent successes of reinforcement learning (RL) in training large reasoning models motivate the question of whether  self-training — the process where a model learns from its own judgments — can be sustained within RL. In this work, we study this question using majority voting as a simple self-feedback mechanism. On a comprehensive set of experiments on both synthetic and real reasoning tasks, we find that even this basic approach improves not only the model's reasoning performance, but also its capability of generating better quality feedback for the next RL iteration, driving further model improvement. Yet our analysis also reveals a critical limitation of such a self-training paradigm --- prolonged RL with self-reward leads to reward hacking where models learn to maximize training (pseudo-)reward,\nresulting in sudden performance collapse. Together, these results highlight feedback design as the central challenge and call for future research on mechanisms to enable prolonged self-improvement.", "tldr": "We examine the promises and pitfalls of self-improvement with LLMs on mathematical reasoning domains", "keywords": ["LLM", "reasoning", "self-improvement"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4bcad1fcbcaea07605af77f4985af1fb3e1a5071.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores whether models can improve themselves without ground-truth rewards. The authors use Self-Rewarded Training, which uses majority voting over the model’s own sampled outputs as a pseudo-reward signal. They perform experiments on both synthetic and mathematical reasoning tasks.  However, prolonged training leads to reward hacking and model collapse where the model outputs consistent but incorrect answers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper conducts training on both synthetic and mathematical reasoning tasks, and covers as many RL algorithms, model types, and datasets as possible to ensure comprehensive evaluation.\n\n2. The paper provides thorough descriptions of experimental details, offering strong reproducibility."}, "weaknesses": {"value": "1. The paper defines self-improvement with a model “judging the correctness of its own outputs,” whereas the standard usage typically means training on the model’s own outputs. Actually, several cited works (e.g., STAR, REST-EM, RFT) rely on verifiable ground-truth rewards rather than purely self-judged signals. This distinction affects claims of novelty.\n\n2. The title asks “Can large reasoning models self-train?” but the answer is effectively No. especially on harder datasets where base accuracy is low and long-horizon training is required (the setting we usually care about). Given prior expectations about reward hacking without reliable verification, this negative result is unsurprising, which weakens the paper’s novelty. \n\n\n3. Experimental selection and presentation need stronger justification. For example, \n\n* Figure 3 shows small gaps between SRT and RL with ground truth (Lines 290–292), but Appendix C (e.g., Figure 9 and related plots) reveals much larger gaps on other train/test pairs. The paper should explain why those particular main-text curves were chosen.\n\n* Inconsistent evaluation temperatures. Lines 285–288 justify greedy decoding for Llama because the model card uses it, yet Qwen-2.5-Math is evaluated at temperature = 1.0 despite common recommendations for greedy. \n\n\n[1] Zelikman, Eric, et al. \"Star: Bootstrapping reasoning with reasoning.\" Advances in Neural Information Processing Systems 35 (2022): \n15476-15488.\n\n[2] Singh, Avi, et al. \"Beyond human data: Scaling self-training for problem-solving with language models.\" arXiv preprint arXiv:2312.06585 (2023).\n\n[3] Zhang, Yifan, et al. \"Cumulative reasoning with large language models.\" arXiv preprint arXiv:2308.04371 (2023)."}, "questions": {"value": "1. SRT’s effectiveness appears strongly dependent on the initial accuracy of the base model. If the model starts near zero performance, majority voting will not yield meaningful pseudo-labels, while RL with ground-truth rewards can still learn effectively. Could the authors provide a more systematic criterion (e.g., minimum initial majority@k or pass@1 threshold) for when SRT is expected to succeed?\n\n2. The motivation states that many domains lack ground-truth solutions, yet all experiments are conducted in environments with ground-truth verifiers (math, synthetic logic). Can the authors evaluate SRT on truly open-ended tasks (e.g., writing) to demonstrate practical benefits when verifiers are unavailable?\n\n3. Lines 052–074 mention the arXiv version of this submission, which implicitly compromises anonymity during double-blind review. I leave this for the AC to judge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m6nCQi7QCI", "forum": "wkRrpXD8Np", "replyto": "wkRrpXD8Np", "signatures": ["ICLR.cc/2026/Conference/Submission13833/Reviewer_RbnJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13833/Reviewer_RbnJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578951369, "cdate": 1761578951369, "tmdate": 1762924356504, "mdate": 1762924356504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to conduct research on RL without any ground-truth supervision and uses self-feedback. The authors choose one possible signal \"majority vote\" as a case study and use this as a reward function. The authors find that this reward signal will improve the base model on some key reasoning metrics such as maj@k and avg@k at the beginning, but the authors also find that with prolonged training using this reward signal, the model finally outputs the same template final answer, which maximizes training reward but leads to complete collapse on test datasets, which can be called a kind of \"reward hacking\". The authors spend a large amount of space testing the effectiveness of SRT on different models and tasks, but there is less analysis of the reward hacking that occurs during the prolonged training process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors tested many models including Llama3.1-8b, Qwen2.5-Math-7B, Qwen3-14B, and Deepseek-Math-7B, and also conducted tests on many tasks such as Reasoning GYM and Math reasoning. Moreover, the authors honestly reported at the end that long-term training leads to model collapse."}, "weaknesses": {"value": "1. This paper gives me a strong sense of disconnect. The front section spends a large amount of space introducing Self-Reward Training (SRT) (i.e., using majority voting as a supervision signal), but we can see that in most scenarios, this method is far inferior to directly training on Ground Truth Labels, as shown in Figure 3 and Figure 4. Furthermore, we know from the later text that this training method leads to a collapse phenomenon where the model's accuracy rapidly declines in the later stages. This gives one the feeling that the value of this method is not very significant.\n2. From Line 237-Line 240, we can see that before conducting SRT training, the authors also used ground truth labels to perform a cold start on the base model, which further calls into question the value of SRT.\n3. From Appendix C, we can also see that the SRT method even begins to collapse from the first two to three hundred steps (Figure 9 and Figure 12), which also raises doubts about SRT's generalizability across different methods such as DAPO.\n4. The biggest problem with SRT, as the authors stated, is that long-term training leads to model collapse, but the authors only spent a small amount of space displaying the phenomenon, without providing in-depth experimental or theoretical analysis, simply summarizing it as \"reward hacking\". However, I believe that an in-depth analysis of this phenomenon, or even proposing certain solutions, should be the focus of work related to self-training."}, "questions": {"value": "Please see in Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PTpabEufQI", "forum": "wkRrpXD8Np", "replyto": "wkRrpXD8Np", "signatures": ["ICLR.cc/2026/Conference/Submission13833/Reviewer_nK26"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13833/Reviewer_nK26"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745231779, "cdate": 1761745231779, "tmdate": 1762924356108, "mdate": 1762924356108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes self-rewarded training (SRT), which does not use explicit verifiable rewards for training the reasoning model. By adopting the majority vote on self-generated answers as a reward function, the model do RL using its own reward function. In several experiments on reasoning datasets, the performance of the model trained by SRT has competitive results compared to the RL on ground truth rewards. However, as the authors said, the majority vote on self-generated data has significant drawbacks: answer collapse. Since generating identical answers on multiple reasoning trajectories can also get positive reward, the model eventually lose the ability on reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The notion of the self rewarded training using self-generated data is quite novel. In previous methods, the most common way to do RL without verifiable reward is using teacher model as a verifier or the source of the knowledge distillation. However, in this paper, the proposed method only uses a single model that can both generate the answers and verify them  by majority voting."}, "weaknesses": {"value": "1. I think this paper also should propose an alternative way beyond the majority vote. It is straightforward that majority vote has shortcut answer that is generating identical answers through different trajectories. Therefore, to show the validity of SRT, the authors should give proper examples on self-verification which do not have shortcut solutions.\n\n2. The authors do not explain the main reason of the phenomenon that the performance of using SRT is quite competitive to its counterpart (i.e. RL with ground truth reward) in concrete way. It would be better to show the analysis on using SRT in reasoning models. Why the model trained with SRT beats the base model? And how can we determine the RL training step before collapsing?"}, "questions": {"value": "Already mentioned in the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3zywPJQQ1Q", "forum": "wkRrpXD8Np", "replyto": "wkRrpXD8Np", "signatures": ["ICLR.cc/2026/Conference/Submission13833/Reviewer_f2W8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13833/Reviewer_f2W8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107739728, "cdate": 1762107739728, "tmdate": 1762924355709, "mdate": 1762924355709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}