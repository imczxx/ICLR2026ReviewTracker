{"id": "tIQZ7pVN6S", "number": 12359, "cdate": 1758207271499, "mdate": 1759897514791, "content": {"title": "Generalizable Heuristic Generation Through LLMs with Meta-Optimization", "abstract": "Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) heuristic-optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective heuristic-optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse heuristic-optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC heuristic-optimizer. These constructed heuristic-optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.", "tldr": "", "keywords": ["Combinatorial Optimization", "Large Language Models", "Heuristic Generation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0179e9987f0afb78b9f36ccd405c1b1ac000b044.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Meta-Optimization of Heuristics (MoH), a new framework that uses large language models (LLMs) to automatically discover effective heuristic optimizers for combinatorial optimization problems (COPs). Unlike prior work that relies on fixed evolutionary computation (EC) strategies and single-task training, MoH operates at the optimizer level. It employs an outer loop for meta-optimization (optimizer design) and an inner loop for heuristic design. The framework uses multi-task training to improve generalization and supports diverse optimizer discovery through (self-)invocation mechanisms within LLMs. Extensive experiments—including TSP, online/offline Bin Packing, and CVRP—demonstrate that MoH outperforms traditional, neural, and recent LLM-based heuristic generation methods in both performance and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The shift from fixed heuristic-optimizers to directly searching for optimizers automates the design of entire optimization frameworks, not just heuristics. This represents a conceptual advance over state-of-the-art LLM-based automatic algorithm design.\n- MoH is benchmarked across multiple tasks and demonstrates state-of-the-art or competitive optimality gaps, especially when generalizing to larger instance sizes.\n- Cost analysis—covering LLM requests, tokens, and wall time—helps contextualize practical applicability."}, "weaknesses": {"value": "- Despite noteworthy generalization to larger problem instances and other COPs, the empirical evaluation focuses on classical, well-studied benchmarks (TSP, BPP, CVRP). The approach hasn't been applied to real-world, domain-constrained, or industrial COPs, which may limit its immediate practical impact. It would be interesting to see how it performs on real-world optimization problems that LLMs are unfamiliar with. Author comments on this aspect would be valuable.\n- The distinction between meta-optimizer and heuristic-optimizer is confusing, especially since they turn out to be the same in the end.\n- The comparaison to other baselines seems not fair, see questions for details. I would like to hear rebuttal from the authors before making the final decision."}, "questions": {"value": "- During inference, you run MoH for 10 iterations and select the best-performing heuristic based on the final results. Is this strategy also applied to other baselines?\n- In Table 8, you distinguish between MoH-Train and MoH-Inference, but Table 9 only mentions MoH. Could you clarify this difference?\n- For Tables 8 and 9, testing on large instances increases MoH inference time but not the other methods, correct? The same concern applies to token consumption. I suggest clarifying the entire procedure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yw8blBTMwE", "forum": "tIQZ7pVN6S", "replyto": "tIQZ7pVN6S", "signatures": ["ICLR.cc/2026/Conference/Submission12359/Reviewer_7Ges"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12359/Reviewer_7Ges"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823041898, "cdate": 1761823041898, "tmdate": 1762923272225, "mdate": 1762923272225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Meta-Optimization of Heuristics (MoH), a framework that uses large language models (LLMs) to generate effective, interpretable heuristics for combinatorial optimization problems (COPs). Unlike traditional methods, which rely on predefined evolutionary computation (EC) heuristics or single-task training, MoH uses meta-learning to automate the design of meta-optimizers, enabling broader heuristic exploration and better generalization to new problems. The authors demonstrate MoH’s superiority over existing LLM-based methods on classic COPs like the Traveling Salesman Problem (TSP) and Bin Packing Problem (BPP)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MoH introduces the idea of meta-optimization within LLM-based heuristics for combinatorial optimization, addressing key limitations of existing methods like the lack of diversity in heuristic exploration and challenges in generalization.\n2. Extensive experiments demonstrate that MoH outperforms both traditional and LLM-based heuristic methods across various settings, showing its ability to tackle problems like TSP and BPP effectively."}, "weaknesses": {"value": "1. While the authors claim that MoH does not incur significant computational overhead, the introduction of a meta-optimization layer adds complexity, which may increase the time and resources required, especially for large problems.\n2. Though MoH performs well on classical COPs, its scalability to more complex or non-classical optimization problems (e.g., real-world applications) has not been thoroughly tested.\n3. While multi-task learning is a strength, it could also lead to overfitting on the training tasks if not managed properly. The paper doesn't provide a clear strategy for mitigating such risks.\n4. While MoH increases the exploration space, there is no detailed analysis of how efficiently it can explore very large or complex search spaces in comparison to simpler heuristics or other optimization techniques."}, "questions": {"value": "1. Please list up and carefully describe any questions and suggestions for the authors. Think of the things where a response from the author can change your opinion, clarify a confusion or address a limitation. This is important for a productive rebuttal and discussion phase with the authors.\n2. While MoH performs well on classical COPs, its scalability to more complex, real-world optimization problems (e.g., dynamic environments, non-classical COPs) has not been thoroughly tested. Can you provide any insights into how MoH might adapt to these problems? Have you considered testing MoH on real-world benchmarks or dynamic problem settings?\n3. Multi-task learning is a strength of MoH, but it could also lead to overfitting, especially when tasks are not sufficiently diverse or are too similar. The paper does not clarify how overfitting is mitigated during training. Could you elaborate on the strategies used to ensure that the framework generalizes well across tasks? Did you apply any regularization techniques, cross-validation, or other safeguards to address this risk?\n4. While MoH expands the heuristic search space, how does it perform when compared to simpler heuristic methods or other optimization techniques, especially in terms of efficiency? Given the large search space, how does MoH ensure it doesn’t waste resources on ineffective explorations? Could you provide a more detailed analysis of MoH’s efficiency in exploring vast search spaces, especially for large and complex problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OQnhDMgP6t", "forum": "tIQZ7pVN6S", "replyto": "tIQZ7pVN6S", "signatures": ["ICLR.cc/2026/Conference/Submission12359/Reviewer_9cf3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12359/Reviewer_9cf3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915579193, "cdate": 1761915579193, "tmdate": 1762923271889, "mdate": 1762923271889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Meta-Optimization of Heuristics (MoH), a novel framework that uses Large Language Models (LLMs) to generate generalizable heuristics for Combinatorial Optimization Problems (COPs). Unlike prior methods that optimize heuristics directly, MoH operates at the optimizer level. It aims to discover a highly effective \"heuristic-optimizer\" by meta-learning. This process involves optimizing a meta-prompt that guides an LLM to sample and refine heuristics. The meta-optimizer is trained to maximize a utility function across a diverse set of tasks (e.g., TSP instances of varying sizes), with the goal of producing heuristics that generalize well. The authors evaluate MoH on the Traveling Salesperson Problem (TSP), demonstrating improved performance over several baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea of optimizing the optimizer (via a meta-prompt) rather than just the heuristics themselves is a novel and interesting approach to leveraging LLMs in the optimization domain.\n- The paper correctly identifies generalizability as a key weakness in existing heuristic generation methods and explicitly designs its utility function to reward performance across different task distributions (i.e., problem sizes)."}, "weaknesses": {"value": "- While the method is described with complex terminology, its core mechanism appears to be a sophisticated form of meta-prompt optimization. The \"meta-optimizer\" is, in essence, a highly-tuned prompt that guides the LLM to sample effective heuristics. This idea, while implemented well, feels intuitive and perhaps more incremental than a fundamental breakthrough, which may limit the paper's conceptual contribution.\n- The paper's \"generalizability\" claim is weak and potentially misleading. Firstly, the framework does not generalize across problem domains; it requires training a new, specialized \"meta-optimizer\" for each problem class (TSP, BPP, CVRP). Secondly, even within a single problem, the generalization is limited to varying instance sizes from the same data distribution. There is no evidence that the optimizer generalizes to new instances drawn from a different distribution (e.g., from uniformly distributed TSPs to clustered TSPs).\n- The experimental comparison to baselines appears to be unfair. MoH's computational cost includes both a training phase (1,000 heuristic evaluations) and a separate, additional \"inference stage\" to generate the final heuristics. In contrast, baseline methods like EoH are presented as more \"online\" and may not have this distinct (and costly) inference phase. For a fair comparison, the baselines should be allocated a total computational budget equal to the sum of MoH's training and inference costs. This is particularly concerning given that the performance gains reported in Table 1 are incremental."}, "questions": {"value": "- If the experiment is run multiple times, will it produce a \"meta-optimizer\" with similar performance, or are the results highly variant? This is a critical point for assessing the method's reliability.\n- Why was the utility function weighted by the size of the task? Figure 1 seems to suggest that performance suffers when emphasizing larger instances. What is the performance of the MoH framework when using a uniform weight for all task sizes? What is the performance if the baselines use the weighted utility?\n- What is the performance of the final heuristics obtained at the end of the training phase? This would help clarify the exact performance gain and cost attributed to the separate inference stage.\n- Could you clarify the practical difference between the heuristic generation strategy used in the MoH inference stage and the strategy used by the baseline EoH?\n- What specific heuristic was tested on the TSPLIB benchmark? Was it the single best heuristic from Table 1? If so, from which problem size distribution was this heuristic generated?\n- Why are the results for the ReEvo baseline missing in the TSP-GLS case in Table 1?\n- The paper requires careful proofreading to correct several typos (e.g., \"generats\" in line 74, \"hsmaller\" in line 928)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9TIyFW7Dhm", "forum": "tIQZ7pVN6S", "replyto": "tIQZ7pVN6S", "signatures": ["ICLR.cc/2026/Conference/Submission12359/Reviewer_9XE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12359/Reviewer_9XE1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975070885, "cdate": 1761975070885, "tmdate": 1762923271483, "mdate": 1762923271483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MoH (Meta-Optimization of Heuristics), a two-level LLM-driven framework that searches not just for task heuristics, but for the heuristic-optimizers that generate them. An outer loop uses a meta-optimizer to invoke an LLM and produce candidate heuristic-optimizers, while an inner loop applies each candidate to evolve concrete heuristics for downstream COP tasks. Selection uses utility on validation tasks, and training is multi-task to encourage cross-size generalization. Experiments on TSP and online BPP report state-of-the-art gaps, especially on larger, unseen sizes. Further ablations suggest benefits from maintaining populations and using natural-language idea descriptions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Ablations show benefits from the proposed idea and examine different LLM backends and population sizes.\n\nThe paper provides concrete examples/analysis indicating discovered strategies can resemble or hybridize classic metaheuristics.\n\nThe paper shows strong empirical results and cross-size generalization on TSP and Online BPP. The proposed MoH often outperforms baselines.\n\nMulti-task training and controlled evaluation budgets are thoughtfully designed to encourage generalization."}, "weaknesses": {"value": "Improvements over strong baselines can be modest in some settings. Further discussion should be included.\n\nAccording to experimental setups, main tables emphasize best-of-three runs, which can overstate gains versus mean/variance reporting.\n\nThere might exist sensitivity to LLM choice and prompts. The robustness under model drift is uncertain."}, "questions": {"value": "Which two main loops make up the MoH framework?\n\nDoes the paper claim cross-size generalization on TSP?\n\nDoes the method rely on a population of candidates during search?\n\nWhat is the difference between heuristic-optimizers and meta-optimizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "14zufM0Hh9", "forum": "tIQZ7pVN6S", "replyto": "tIQZ7pVN6S", "signatures": ["ICLR.cc/2026/Conference/Submission12359/Reviewer_LJhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12359/Reviewer_LJhB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991759740, "cdate": 1761991759740, "tmdate": 1762923271031, "mdate": 1762923271031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}