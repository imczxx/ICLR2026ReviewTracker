{"id": "uOt3QNXYy2", "number": 24113, "cdate": 1758352873137, "mdate": 1759896781077, "content": {"title": "NVE-Adaptor: Novel View Editing Adaptor for Unseen View Consistent 3D Editing", "abstract": "3D editing aims to transform a given 3D structure according to the user's intent. Multi-view consistent 3D editing has been proposed to ensure consistent editing effects across different views of a 3D model, resulting in high-quality 3D structures. However, such consistency is only observed from viewpoints near the trained reference images, while renderings from other viewpoints (i.e., unseen view) often appear inconsistent and blurry. This is because current 3D editing systems are heavily optimized only for the given reference viewpoints. In real-world scenarios, it is also challenging for human to manually identify all regular viewpoints that ensure consistent 3D quality. To this end, we propose Novel View Editing Adapter (NVE-Adaptor), which enables 3D editing systems to maintain consistent quality even in unseen views. NVE-Adaptor supplements the limited reference views by sensibly exploring novel view in 3D space with rendered images from those views. These images are then refined using diffusion-based editing and used as additional supervision to improve view consistency. Our concept is simple, model-agnostic, and broadly applicable to multi-view 3D editing systems. We demonstrate its effectiveness on two 3D scene benchmarks (Mip-NeRF 360, Instruct-NeRF2NeRF) as well as on real-world data. The code will be made publicly available.", "tldr": "This paper proposes Novel View Editing Adaptor, which enables 3D editing systems to maintain consistent editing effects even in unseen views.", "keywords": ["Multi-view Consistent 3D Editing", "Text-based 3D Editing", "Gaussian Splatting", "Diffusion Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fafba5d9229048c5f1b5661efc9fe2a8e5097288.pdf", "supplementary_material": "/attachment/42821bcfa334d7f924b88e0d4617585dd8fac647.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel view editing adaptor (NVE-Adaptor) for the 3D editing task. The main idea is to explore more novel views in 3D space and use Diffusion model to edit those novel views. Then the novel views and original reference views are combined to optimise the 3D scenes such as NeRF or 3D Gaussian Splatting. \n\nThe main contritution is the 3D Gaussian Probabilistic View Sampling (3DG-PVS) module, which explore novel views in a more effective way for 3D editing. \n\nExperiments are conducted on Instruct-NeRF2NeRF (IN2N) and Mip-NeRF 360 datasets. The proposed method NVE-Adaptor is added to several 3D editing methods, and improves the performance w.r.t. several metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n- Illustration of vulnerability of unseen view rendering results of current 3D editing systems. This paper explores this phenomenen from two perspectives: angle ($\\phi>25$) and distance ($r<0.5$). The visualization and continous set of angle/distance analysis show the severe degration of editing effect on novel views far from reference views. \n- The Novel View Editing-Adaptor (NVE-Adaptor), at its core, it proposes a 3D GAUSSIAN PROBABILISTIC VIEW SAMPLING strategy for more effective sampling of new views for 3D editing. \n- After sampling more views and utilizing them in the Consistent Diffusion Model, the proposed method can be combined with various method, and improve the performance."}, "weaknesses": {"value": "## Weaknesses\n- Even the vulnerability of unseen view is comprehensively explored and visualized, this behaviour is a well-know common sense [R1-R3]. Simply mentioning this phenomenen is not a strong contribution. \n- Overall, the proposed method is very simple, selecting more views for optimising the 3D scene (shown in Figure 2). Trivially modifying Eq. 4 to include novel views. And the newly added novel views go the same procedure as previous ones: Diffusion Model editing. Only difference is adding a new prompt \"high quality\", which is qutie simple. \n- 3D Gaussian Probabilistic View Sampling (3DG-PVS) can be a minor contribution, which samples more views by iteratively selecting the inverse probability. But no other comparison strategies except for the default 'Regular' strategy is compared. \n- The performance is improved after combined with several methods. Howevere, this requries much more views to edit the 3D scene, which is an unfair comparison. In general, more views for optimisation should have better performance. Even the Regular strategy might have better performance when combined with the baseline methods. \n- Are there other novel view selection/sampling methods? This should be surveyed and discussed in the related work and in the experiments. \n- How is Human performance evaluated? Why the number differs so much e.g., 0.22 VS. 0.78 while the other metrics distinctions are not that significant?\n- In Table 2, T*=40 is better than T*=30, wrongly highlighted the number. \n\n[R1] Remondino, F., Karami, A., Yan, Z., Mazzacca, G., Rigon, S., & Qin, R. (2023). A critical analysis of NeRF-based 3D reconstruction. Remote Sensing, 15(14), 3585.\n[R2] Zhang, J., Zhang, Y., Fu, H., Zhou, X., Cai, B., Huang, J., ... & Tang, X. (2022). Ray priors through reprojection: Improving neural radiance fields for novel view extrapolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18376-18386). \n[R3] Hull, M., Yang, H., Mehta, P., Phute, M., Cho, A., Wang, H., ... & Chau, P. (2025). 3D Gaussian Splat Vulnerabilities. arXiv preprint arXiv:2506.00280."}, "questions": {"value": "Are there other novel view selection/sampling methods? This should be surveyed and discussed in the related work and in the experiments.\n\nHow is Human performance evaluated? Why the number differs so much e.g., 0.22 VS. 0.78 while the other metrics distinctions are not that significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SQ3Mkhox7v", "forum": "uOt3QNXYy2", "replyto": "uOt3QNXYy2", "signatures": ["ICLR.cc/2026/Conference/Submission24113/Reviewer_CjWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24113/Reviewer_CjWi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468886117, "cdate": 1761468886117, "tmdate": 1762942943884, "mdate": 1762942943884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper brought up an interesting challenge of limited view coverage in current 3D editing, where the edited 3D model exhibits visual artifacts in the views that are distant from the edited reference images. To address this issue, the authors propose a novel view sampling strategy to expand the reference views. Subsequently, the newly sampled views are forwarded into multi-view diffusion model along with original reference views for editing. Experiments show the expansion achieves significant improvement on the unseen view’s visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper discusses on a novel and practical view coverage issue in current 3D editing task. Starting from this challenge, the author propose a novel view sampling strategy that can be plug-and-played by current 3D editing methods.\n\n2.Both qualitative and quantitative experiments in this paper are comprehensive and reasonably designed. The presented visual quality of the rendered video results are promising.\n\n3.The paper is clearly presented and easy to follow"}, "weaknesses": {"value": "My concerning mainly lies on the setting. In my understanding, the main contribution of this paper is the view sampling issue. In another world, after optimally selecting the novel views, the rest processing is feed-forwarding the views to current 3D editing models. Current experiments seem to focus more on the comparison between the results with view expansion and that without view expansion to emphasis the effect of view expansion. In my opinion, the idea of \nusing view expansion itself is not sufficiently novel, as it is a common sense in reconstruction. Instead, I would like to see the improvement of proposed view sampling strategy corresponding to the baseline random sampling or uniform sampling, presenting how and why the proposed sampling strategy outperforms a vanilla strategy. I would temporarily give a borderline reject rating and raise my score upon this contribution is well illustrated."}, "questions": {"value": "See weaknesses. I would suggest the authors respond to the concern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iIs1qV9mzU", "forum": "uOt3QNXYy2", "replyto": "uOt3QNXYy2", "signatures": ["ICLR.cc/2026/Conference/Submission24113/Reviewer_hNRr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24113/Reviewer_hNRr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921368914, "cdate": 1761921368914, "tmdate": 1762942943578, "mdate": 1762942943578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the unseen-view inconsistency problem in multi-view 3D editing: edits look good near trained reference views but degrade at novel viewpoints. It proposes NVE-Adaptor, which (i) samples novel viewpoints via a probabilistic strategy (3DG-PVS), (ii) renders those views, (iii) enhances them with diffusion-based multi-image editing (with prompts like “high quality”), and (iv) uses the edited renders as extra supervision during 3D editing, improving view consistency on Mip-NeRF 360 and IN2N, plus a real-world set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed NVE-Adaptor only augments supervision with edited novel-view images and can attach to various NeRF / 3DGS edit pipelines.\n\n- Across several baselines (IN2N, GaussCtrl, VcEdit, DGE), the adaptor improves FID/SSIM and CLIP-based consistency on both seen and unseen views."}, "weaknesses": {"value": "- The method improves novel-view renders by editing them with a text-to-image model (multi-image consistency + prompts like “high quality”) before using them as supervision. This can hallucinate textures/geometry, drifting from the true scene and biasing the 3D optimization. Therefore, it would be good to quantify drift with image-space metrics to ground-truth photos where available (you already capture real images + COLMAP extrinsics for the unseen set) and report an edit-intensity control or mask-based constraints to limit semantic changes.\n\n- Tables report average scores, but variances / CIs are missing. Several gains (e.g., CLIP/SSIM deltas) are modest. Add std/CI over seeds (you mention 10 or 30 seeds in places) to support significance claims, especially for HumanFID and directional CLIP where variance can be high.\n\n- 3DG-PVS uses isotropic Σ with σ=0.4 and a cube-based outlier-removal heuristic. The performance sensitivity to these hyper-parameters is unclear. It would be good to add a comprehensive hyperparameter analysis.\n\n- The authors provided per-scene training time overheads (e.g., IN2N: 56→68 min; DGE: 15→18 min), but not inference-time cost or total GPU hours for the full training + novel-view editing pipeline. Please add wall-clock and GPU memory profiles for typical scenes, and break down where time is spent (rendering vs diffusion editing vs 3D optimization). This would help better understand the compute-quality trade-offs.\n\n- Table 8 shows that when reference views are very sparse (N≤40), gains shrink or vanish; at N=20 performance can even degrade (Base vs Base+NVE nearly equal or worse). Please discuss failure modes (diffusion editor invents content due to under-constrained views) and consider confidence-based filtering to discard low-reliability edited novel views in those regimes."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eDzckEDDTX", "forum": "uOt3QNXYy2", "replyto": "uOt3QNXYy2", "signatures": ["ICLR.cc/2026/Conference/Submission24113/Reviewer_syaU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24113/Reviewer_syaU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943933674, "cdate": 1761943933674, "tmdate": 1762942943289, "mdate": 1762942943289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes NVE-Adaptor, a plug-and-play module that improves multi-view consistent 3D editing, especially for unseen viewpoints that were not part of the original reference views.\n\n- The method explores novel camera viewpoints, renders images from those new views, refines them with diffusion-based editing, and uses them as extra supervision to improve consistency in 3D editing results.\n\n- This paper works in a model-agnostic manner, complementing existing 3D editing pipelines (e.g., NeRF / Gaussian Splatting–based systems), and shows consistent improvements on seen and unseen views across multiple benchmarks and real-world scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper clearly identifies a critical limitation of existing multi-view 3D editing systems, degradation and inconsistency when rendering from unseen viewpoints, and provides quantitative sensitivity analysis to motivate the problem.\n\n- This paper proposes a simple, model-agnostic adaptor that can be plugged into existing 3D editing pipelines without architectural changes, making it widely applicable to NeRF-based and Gaussian Splatting-based systems.\n\n- This paper provides strong empirical evidence across multiple datasets, demonstrating consistent quality improvements in both seen and unseen viewpoints, including real-world data, validating the practicality and robustness of the method."}, "weaknesses": {"value": "- In demo.mp4 around the timestamp 00:58, most of the input 3D characteristics disappear during the editing process. This phenomenon appears in multiple examples, yet it is not addressed in the limitations section, which is disappointing.\n\n- The illustration in Figure 3 describing the functionality of the Novel View Editing-Adaptor (NVE-Adaptor) could be made more intuitive. Instead of using red and blue dots, the figure could be revised so that readers can clearly distinguish reference views and novel views without referring back to a glossary.\n\n- Including the code snippet (source_code.py in supplementary) in the supplementary material is appreciated, but the implementation details are too brief. To ensure reproducibility, more detailed explanations should be provided, and relevant code components should be explicitly linked or referenced in the appendix.\n\n- For experimental settings such as Table 8, where specific parameters are used, it would be helpful to explain why those parameter choices were made. Providing this rationale would improve the clarity and interpretability of the paper."}, "questions": {"value": "Mentioned in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T2aVZ3Apc9", "forum": "uOt3QNXYy2", "replyto": "uOt3QNXYy2", "signatures": ["ICLR.cc/2026/Conference/Submission24113/Reviewer_NoU9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24113/Reviewer_NoU9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964381515, "cdate": 1761964381515, "tmdate": 1762942943084, "mdate": 1762942943084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}