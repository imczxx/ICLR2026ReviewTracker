{"id": "PO6OodFuGw", "number": 2554, "cdate": 1757144714459, "mdate": 1759898141201, "content": {"title": "BERNOULLI FLOW MODELS", "abstract": "Diffusion-based generative modeling for data with Bernoulli distributions has broad potential applications, but it relies on carefully designed forward processes. Recently, flow matching-based methods have addressed this issue. However, when these methods are naively applied to the Bernoulli distribution, their dependence on predicting the instantaneous velocity field during sampling can introduce invalid Bernoulli parameters, leading to model collapse. To address this challenge, we introduce **Bernoulli Flow Models (BFM)**, a novel generative framework that fuses flow matching with vanilla binary diffusion. BFM ensures valid Bernoulli parameters throughout the sampling process by deriving a one-step forward transition kernel and a closed-form, normalized posterior based on the pre-defined flow-matching probability path in the Bernoulli parameter space. As a result, BFM simplifies the training process of current binary diffusion models and can be easily integrated into existing  architectures with minimal modification. We empirically validate the generative performance of BFM on high-dimensional binary manifolds, including Ising model simulations, both unconditional and conditional image generation. Experiments show that our model achieves comparable performance to both continuous and discrete space generative models.", "tldr": "", "keywords": ["Binary Diffusion", "Flow Matching", "Bernoulli Distribution", "Generative Models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0379b21237ff3ba6034544bf42bd1b2ceac2fef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Binary Flow Matching (BFM), a new training loss for Binary Diffusion Models that draws inspiration from Flow Matching (FM). Specifically, BFM modifies the noise scheduling strategy of Binary Latent Diffusion Models (BLM) [1] to follow a linear interpolation between the data and noise distributions, analogous to flow matching in continuous diffusion frameworks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Extensive Experimental Evaluation**  \n   The authors conduct a comprehensive set of experiments on high-dimensional datasets.  \n\n2. **Novel Integration of Flow Matching Concepts**  \n   Incorporating the principles of flow matching into binary diffusion models is an novel idea that extends the applicability of flow-based training to discrete domains.  \n\n3. **Clear and Accessible Writing**  \n   The paper is generally well-structured and easy to follow."}, "weaknesses": {"value": "1. **Unclear Motivation for Flow Matching Noise Scheduler**  \n   The rationale for replacing the standard BLM noise scheduling [1] with a flow-matching-inspired one is not well articulated. It remains unclear why the new schedule is expected to improve over the established BLM formulation.  \n\n2. **Underwhelming Empirical Performance**  \n   The main results indicate that BFM underperforms relative to BLM, raising doubts about its practical utility. Further discussion is needed to clarify potential advantages (e.g., interpretability, stability, or low-NFE behavior) that could justify the approach.  \n\n3. **Lack of Direct Comparisons with BLM**  \n   Given that BFM is a close variant of BLM, a side-by-side comparison under identical conditions (same architecture, datasets, and training budget) would be highly informative for evaluating the claimed improvements."}, "questions": {"value": "1. **Generalization Beyond Gaussian Transitions**  \n   Flow matching enables transformations between arbitrary distributions, not limited to Gaussian-based processes as in diffusion models. Does BFM similarly generalize BLM to handle arbitrary unknown initial distributions, potentially enabling tasks such as image-to-image translation?  \n\n2. **Clarify Technical Motivation**  \n   The motivation for BFM remains vague, as the proposed formulation still relies on transition kernels, and the primary difference lies in the variable ordering rather than a fundamental theoretical innovation. A deeper discussion of what this change accomplishes in practice would strengthen the paper.  \n\n3. **Low-NFE Performance Analysis**  \n   Flow matching methods are known to perform well in low NFE (number of function evaluations) regimes. It would be valuable to investigate whether BFM exhibits similar behavior compared to BLM, potentially highlighting cases where it provides computational benefits despite lower overall accuracy.\n\n---\n\n## References  \n[1] **Binary Latent Diffusion Models.**  \n[2] **Flow Matching for Generative Modeling.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4uSGoW4MzC", "forum": "PO6OodFuGw", "replyto": "PO6OodFuGw", "signatures": ["ICLR.cc/2026/Conference/Submission2554/Reviewer_eGqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2554/Reviewer_eGqD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811826714, "cdate": 1761811826714, "tmdate": 1762916280376, "mdate": 1762916280376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Bernoulli Flow Models (BFM), a binary generative framework that defines a probability path in Bernoulli space and derives a closed-form forward kernel and posterior. This avoids invalid Bernoulli parameters when applying standard flow matching to binary data. Overall, BFM is a principled, practical alternative to heuristic binary diffusion and discrete score-based / flow-based methods, with clear theory and solid results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. BFM unifies flow matching with diffusion in the discrete domain,and guarantees all intermediate parameters valid.It is stable on binary systems,suggesting good robustness and cross-domain transferability.\n\n2. A major strength is the derivation of an analytical one-step transition and posterior for the Bernoulli diffusion process.\n\n3. Empirically, BFM shows competitive results across diverse tasks, often with advantages in efficiency.And it’s engineering friendly,both training and sampling are operationally lightweight."}, "weaknesses": {"value": "1. The claimed advantages on Ising-like systems are under-substantiated: evaluation is confined to a single small 2D Ising setting with limited metrics and baselines. Moreover, the paper does not clearly demonstrate superiority on binary data.\n\n2. While BFM performs well, it does not decisively outperform some specialized prior models in terms of raw image fidelity on certain benchmarks.For instance, on high-resolution image synthesis (LSUN/FFHQ in Table 2), BLD (Wang et al., 2023) still achieves significantly better FID scores (e.g., 5.85 vs BFM’s 10.87 on FFHQ).\n\n3. Scope Limited to Binary Data: By design, BFM focuses on Bernoulli distributions. This specialization is logical, but it means the method is not directly applicable to non-binary categorical data (except by binarizing them).\n\n4. The paper lacks a systematic ablation and sensitivity analysis of the probability-path design , temperature, and step count.\n\nZe Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, pp. 22576–22585, 2023."}, "questions": {"value": "1.\tBlackout Diffusion (Santos et al., 2023) achieves an extremely low FID (0.02) on binarized MNIST. Could the authors clarify what might account for this near-perfect score?\n\n2.\tIn the high-resolution image synthesis experiments, the Precision/Recall gap between BFM and BLD (Wang et al., 2023) appears small and dataset-dependent.What factors do the authors think drive these differences? Are there inherent limitations of the linear probability path that affect sample fidelity? Would increasing the step count reduce the FID gap?\n\nJavier E Santos, Zachary R Fox, Nicholas Lubbers, and Yen Ting Lin. Blackout diffusion: generative \ndiffusion models in discrete-state spaces. In International Conference on Machine Learning, pp. \n9034–9059. PMLR, 2023.\n\nZe Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, pp. 22576–22585, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X287chxYQ3", "forum": "PO6OodFuGw", "replyto": "PO6OodFuGw", "signatures": ["ICLR.cc/2026/Conference/Submission2554/Reviewer_b63H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2554/Reviewer_b63H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922599518, "cdate": 1761922599518, "tmdate": 1762916280175, "mdate": 1762916280175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to learn discrete flows on binary data. My understanding is the training procedure is roughly the following: given a batch of binary data, flip some of the bits according to a scheduler, and use a neural net to predict the data given the noise. This is similar to BERT style training (as most things are), but instead of masking random elements you flip them.\n\nAuthors experiment on image generation by directly modelling the pixel bits, which is an interesting application.\n\nI did not check the math."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The method is simple to understand and has visually pleasing images.\n- Given how image generation metrics are largely overfitted by the community, and don't serve much as a signal when fighting for decimal points, I believe the authors have a nice method regardless of FID and other results."}, "weaknesses": {"value": "- The method is akin to discrete flow matching and \"discrete diffusion\", but the authors don't review discrete flow matching: instead they review regular continuous FM. Am I missing something?\n\n- Some discussion around inference time speed would be nice. For example, image quality as a function of inference steps, throughput on whatever GPU they have at their disposal, etc.\n\n- Suggestion for future work: multimodal data."}, "questions": {"value": "1. I couldn't tell if this \"Heuristic Bernoulli Diffusion Models\" is prior work or if the authors are proposing it in this paper.\n\n2. Can the authors please give an example and clarify what this means: \"However, when these methods are naively applied to the Bernoulli distribution, their dependence on predicting the instantaneous velocity field during sampling can introduce invalid Bernoulli parameters, leading to model collapse.\"\n\n3. I believe the probability path is similar to the uniform one present in https://arxiv.org/abs/2412.03487, but on binary data (I'm not advocating you cite this paper, just calling your attention to it). Can the authors comment on this?\n\n4. My understanding is the training procedure is roughly the following: given a batch of binary data, flip some of the bits according to a scheduler, and use a neural net to predict the data given the noise. This is similar to BERT style training (as most things are), but instead of masking random elements you flip them. Is this correct?\n\n5. Could the authors add some discussion around speed, as mentioned in the box above? I'm willing to raise my score if this is properly addressed (the rest is less important imo). Thanks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12xreV71Lv", "forum": "PO6OodFuGw", "replyto": "PO6OodFuGw", "signatures": ["ICLR.cc/2026/Conference/Submission2554/Reviewer_xTnq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2554/Reviewer_xTnq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925744637, "cdate": 1761925744637, "tmdate": 1762916279705, "mdate": 1762916279705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}