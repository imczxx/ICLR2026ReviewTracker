{"id": "f5LVpApGOw", "number": 20387, "cdate": 1758305394182, "mdate": 1759896980492, "content": {"title": "Excision Score: Evaluating Edits with Surgical Precision", "abstract": "Many tasks revolve around editing a document, whether code or text. \nWe formulate the revision similarity problem to unify a wide range of machine learning evaluation problems whose goal is to assess a revision to an existing document.\nThis definition rests on the observation that revisions usually change only a small portion of an existing document, so the existing document and its immediate revisions share a majority of their content.\n\nWe formulate five adequacy criteria for revision similarity measures, designed to align them with human judgement.\nWe show that popular pairwise measures, like BLEU, fail to meet these criteria, because their scores are dominated by the shared content. \nThey report high similarity between two revisions when humans would assess them as quite different. \nThis is a fundamental flaw we address.\n\nWe propose a novel static measure, Excision Score (ES), which computes longest common subsequence (LCS) to remove content shared by an existing document with the ground truth and predicted revisions, before comparing only the remaining divergent regions.\nThis is analogous to a surgeon creating a sterile field to focus on the work area. \nWe use approximation to speed the standard cubic LCS computation to quadratic.\nIn code-editing evaluation, where static measures are often used as a cheap proxy for test-execution-based scores, we demonstrate that ES surpasses existing measures. \nWhen aligned with test execution on HumanEvalFix, ES improves over its nearest competitor, SARI, by 12\\% Pearson correlation and by >21\\% over standard measures like BLEU.\nThe key adequacy criterion is invariance to increasing shared context; when we perturb HumanEvalFix with increased shared context, ES' improvement over SARI increases to 20\\% and >30\\% over standard measures.\nES also handles other corner cases that other measures do not, such as correctly aligning moved code blocks, and appropriately rewarding matching insertions or deletions.", "tldr": "Existing text similarity measures are ill-suited for evaluating edits; we introduce a new measure, Excision Score, which does better.", "keywords": ["Revision Similarity", "Excision Score", "Code Editing", "SARI", "BLEU"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00a882b8747c5d8fe2a834118b1c4059771bc729.pdf", "supplementary_material": "/attachment/e4ff8d82b6b616b106dc6bb9ac8c341bc981b904.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a metric for evaluating revisions to existing documents. The basic idea is to use LCS to calculate both the deviation of the generated revision to the target document and the deviation of the ground truth revision to the target document, and compare how the two deviations differ from each other. The empirical evidence indicates that the metric proposed in this paper has better correlation with the results of dynamic execution for two code editing tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of using LCS to establish a metric for revision tasks is novel.\n2. The empirical results can confirm the effectiveness of the proposed metric."}, "weaknesses": {"value": "1. Since the correlation is based on comparing each metric with actual execution, it seems that actual execution is a perfect metric. Thus, the usefulness of the proposed metric is mainly due to its efficiency or circumstances where execution is infeasible.\n2. The impact of exact matches is not considered in the evaluation. Since the metric of Exact Match can already identify generated revisions that are identical to ground truth revisions, it should be more interesting to show the effectiveness of each metric on only cases where generated revisions literally differ from ground truth revisions.\n3. NLP revision tasks may significantly differ from code revision tasks. Therefore, I feel that it may be an over claim that the proposed metric is also effective for NLP revision tasks. Or some empirical evidence should be provided."}, "questions": {"value": "1. If we do not consider the difference in efficiency, what are thhe circumstances where ES is clearly superior to actual execution?\n2. If we remove the cases where the generated revisions are identical to the ground truth revisions, how would the correlation results become?\n3. Have you experimented on revision tasks in the domain of NLP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KnfNdmjjg8", "forum": "f5LVpApGOw", "replyto": "f5LVpApGOw", "signatures": ["ICLR.cc/2026/Conference/Submission20387/Reviewer_9vsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20387/Reviewer_9vsX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703349750, "cdate": 1761703349750, "tmdate": 1762933836006, "mdate": 1762933836006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Excision Score, a new static metric designed for the revision similarity problem. This problem involves evaluating generated document edits against a ground-truth revision, given the original source. The authors argue that popular metrics like BLEU are ill-suited for this, as their scores get inflated by the large amount of unchanged text shared between the original and revised versions. ES solves this by first removing the shared content (identified via a 3-way Longest Common Subsequence) from the original, the reference, and the prediction. It then applies the SARI metric only to the remaining parts where the text actually differs, allowing it to focus on the quality of the edit itself. In experiments on code-editing datasets, ES shows a significantly higher correlation with test execution pass/fail rates than existing metrics like SARI and BLEU, especially on edit-heavy scenarios. The paper further demonstrates ES's robustness by showing its performance is unaffected by adding large amounts of shared, irrelevant context, a scenario where other metrics degrade."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- A well-motivated problem: The paper clearly defines the task of revision similarity and highlights a fundamental flaw in using standard metrics like BLEU for it. As AI-driven editing becomes more common, a reliable metric for this is highly important.\n\n- An intuitive and well-motivated solution: The core idea of removing the shared context to focus on the edited regions is simple, elegant, and directly addresses the stated problem. The authors also justify their design by showing how ES avoids the specific pitfalls of related metrics like SARI and DiffBLEU.\n\n- Strong results: The evaluation shows ES has clear advantages. On the HumanEvalFix dataset, ES-Token's correlation with test pass/fail rates was significantly better all baseline methods. A stress test that adds a large shared prefix to all inputs confirmed the metric's robustness over other baseline methods."}, "weaknesses": {"value": "- Limited Semantic Understanding: The metric is still fundamentally lexical. Since the final step uses SARI, the evaluation relies on matching n-grams within the edited regions. The paper itself acknowledges this limitation, stating that ES only \"partially satisfies\" its own criterion for semantic equivalence (Property 5) . While this approach is shown to handle simple cases like misplaced insertions, it would likely fail to reward more complex, semantically-equivalent-but-lexically-different code (e.g., refactoring a for loop to a list comprehension) if the n-grams don't align."}, "questions": {"value": "The \"LLM-as-a-judge\" paradigm is a popular evaluation approach nowadays. Have you considered comparing your approach with an LLM-based baseline (e.g., using a small Qwen model to score the edit's correctness) to see which metric achieves higher agreement with human judgment or results? I am curious if there's a model performance threshold above which an LLM assessment becomes more accurate or correlative than static, n-gram-based metrics like ES."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g6PvrJqvX4", "forum": "f5LVpApGOw", "replyto": "f5LVpApGOw", "signatures": ["ICLR.cc/2026/Conference/Submission20387/Reviewer_C6eL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20387/Reviewer_C6eL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798504615, "cdate": 1761798504615, "tmdate": 1762933835664, "mdate": 1762933835664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a static similarity measure (ES) to compare a proposed solution (S) that is the revision of original document (O) with a reference solution (S'). This specific measure aims to satisfy four properties: reward matches, penalize mismatches, being invariant to shared context (anything unchanged should be ignored), and being invariant to O changing, and aims to reward semantically equivalent mismatches.\n\nThe measure consists of applying another measure (SARI) after removing the longest common subsequence.\n\nThe evaluation compares other measures (like SARI) to ES by analyzing their correlation to execution match."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Evaluating the correctness of code is an important problem, and it is indeed not always possible to execute code.\n- The paper analyses highlights limitations of existing token-based metrics, and focusing on the *changed* parts of code is a sensible observation."}, "weaknesses": {"value": "- The paper makes very strong claims about execution not being a suitable measure for revision tasks, yet considers approximation of execution as the only metric in its evaluation. Extensive benchmarks like SWE-Bench can take a long time to evaluate, but a significant portion of this time is the time to obtain a solution, not just the time to evaluate the solution. I would argue that under-approximating program behavior using execution is better than not even evaluating proper syntactic correctness: it seems better to evaluate whether a program yields *a* correct solution than not yield a compilable or executable solution at all. The two benchmarks used for evaluation actually do facilitate easy execution, further decreasing the motivation for this paper.\n- The only property that the proposed solution solves with respect to previous solutions is invariance to shared context. Besides being an incremental improvement, it solves the wrong problem for code: the semantic correctness property.\n- A common approach to measure similarity between code is using tree edit distances [1] on abstract syntax trees. Whereas not addressing semantic correctness, it does require code to be syntactically correct. Tree edit distances are completely ignored.\n- Semantic scores (like CodeBERTScore [2]) are completely ignored in the evaluation.\n\n[1] http://tree-edit-distance.dbresearch.uni-salzburg.at/\n[2] Zhou, S., Alon, U., Agarwal, S., & Neubig, G. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. In The 2023 Conference on Empirical Methods in Natural Language Processing."}, "questions": {"value": "Did you encounter any real-world uses for this evaluation metric that is not solved by existing measures, like tree edit distances or semantic similarities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GUrLqa67ER", "forum": "f5LVpApGOw", "replyto": "f5LVpApGOw", "signatures": ["ICLR.cc/2026/Conference/Submission20387/Reviewer_dBTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20387/Reviewer_dBTr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859362276, "cdate": 1761859362276, "tmdate": 1762933835259, "mdate": 1762933835259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new metric called Excision Score designed to evaluate the quality of edits in documents, such as code or text. he authors show that traditional similarity metrics like BLEU and ROUGE are flawed for documents with large common contexts. Excision Score is designed to capture better correlation with human judgement for such a case. The claim is support with empirical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "In experiments on code-editing datasets, the paper demonstrates that Excision Score correlates significantly better with actual test execution results (pass/fail) than existing metrics, especially when a large amount of shared context is present."}, "weaknesses": {"value": "While the work is valuable, the contribution may lack the substantiality and novelty expected for a full paper at ICLR."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EqBtR5aNEo", "forum": "f5LVpApGOw", "replyto": "f5LVpApGOw", "signatures": ["ICLR.cc/2026/Conference/Submission20387/Reviewer_SvX1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20387/Reviewer_SvX1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937180327, "cdate": 1761937180327, "tmdate": 1762933834753, "mdate": 1762933834753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}