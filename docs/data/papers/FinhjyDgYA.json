{"id": "FinhjyDgYA", "number": 23725, "cdate": 1758347624147, "mdate": 1763641911774, "content": {"title": "Hyper-SET: Designing Transformers via Hyperspherical Energy Minimization", "abstract": "Transformer-based models have achieved remarkable success, but their core components, Transformer layers, are largely heuristics-driven and engineered from the bottom up, calling for a prototypical model with high interpretability and practical competence. To this end, we conceptualize a principled, top-down approach grounded in energy-based interpretation. Specifically, we formalize token dynamics as a joint maximum likelihood estimation on the hypersphere, featuring two properties: semantic alignment in the high-dimensional space and distributional uniformity in the low-dimensional space. By quantifying them with extended Hopfield energy functions, we instantiate this idea as a constrained energy minimization problem, which enables designs of symmetric attention and feedforward modules with RMS normalization. We further present *Hyper-Spherical Energy Transformer* (Hyper-SET), a recurrent-depth alternative to vanilla Transformers naturally emerging from iterative energy optimization on the hypersphere. With shared parameters across layers, Hyper-SET can scale to arbitrary depth with fewer parameters. Theoretically grounded and compact, it achieves competitive or superior performance across diverse tasks, including Sudoku solving, image classification, and masked image modeling. We also design novel variations under the proposed general principle, such as linear attention and gated feedforward layer, and showcase its scalability with depth-wise LoRA. Our results highlight Hyper-SET as a step toward interpretable and principled Transformer design.", "tldr": "We present Hyper-SET, an energy-principled Transformer derived from minimizing quantified token dynamics interpretations on the hypersphere, achieving strong performance with interpretability and scalability", "keywords": ["Transformer", "Hopfield Energy", "Principled Model Design"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e0794c4d986157c5bdadffbacbb570d06467ee1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose Hyper-SET architecture, a recurrent-depth transformer-like sequence-to-sequence architecture. It is constructed as a discretization of a constrained gradient flow on a modified Hopfield energy function restricted to the product of spheres. This network is a single layer iterated an arbitrary number of times, with one layers' worth of parameters being shared across every application of the layer. The layer itself is a novel construction obtained via gradient descent-like steps on different parts of the aforementioned Hopfield-like energy. The authors demonstrate via experimental results that under this highly parameter-constrained setup, the model performs competitively against other architectures formed in the same way (i.e., single layer iterated arbitrarily many times) with different architectures for the layer, e.g., a standard transformer module, across a diverse set of problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The mathematical derivation seems complete, correct, and interesting: while cooking up some objective whose optimization unrolling/discretization reproduces a transformer-like architecture has been done before [1, 2, 3], only a few previous works such as [3] attempt to give a holistic interpretation to the objective or optimization procedure.\n- The new block is novel and shows a genuine attempt at developing a prescriptive theory to improve the current practice.\n- The paper is written well; the connection to previous work and situation in the literature is clear; and the current approach is well-motivated and rigorously exposited.\n\n[1]: Ramsauer, Hubert, et al. \"Hopfield networks is all you need.\" arXiv preprint arXiv:2008.02217 (2020).\n\n[2]: Yang, Yongyi, and David P. Wipf. \"Transformers from an optimization perspective.\" Advances in Neural Information Processing Systems 35 (2022): 36958-36971.\n\n[3]: Yu, Yaodong, et al. \"White-box transformers via sparse rate reduction.\" Advances in Neural Information Processing Systems 36 (2023): 9422-9457."}, "weaknesses": {"value": "Some meaningful weaknesses:\n- By the authors' own admission, the proposed network \"suits better [sic] under a resource-constrained setting as its inherent structural biases could limit its scaling on large datasets.\" This seems like a downside compared to the regular Transformer whose benefits over other architectures _only_ manifest at scale. With this in mind, maybe comparisons to some simpler RNNs/convnets may be meaningful?\n- Comparisons against other single-layer recurrent-depth baselines require some more explanation and elaboration (beyond Appendix D). See \"Questions\" Q1 below.\n- The single-layer recurrent-depth setting is interesting but impractical. A well-trained ViT with 12 (non-tied) layers can easily get > 90 on CIFAR 10 for instance (cf Table 1 where the high score is 90, whereas even the original ViT paper [1] boasts 99% in Table 6). Almost all practical use cases do not use this weight sharing. With this in mind, the empirical results currently do not demonstrate the practical efficiency of such networks. See \"Questions\" Q2 below.\n- Relatedly, the baseline deep networks in Figure 4 seem under-tuned, e.g., the baseline 12-layer ViT is said to get 66% on CIFAR100 in Table 4 but [1] Table 6 shows that a 12 layer ViT can get > 90% on CIFAR100. See \"Questions\" Q2 below.\n\nSome nits:\n- It does not really make sense to write two separate ODEs to show that you optimize each part of the features (e.g. (7) and (10)), which would suggest a parallel-structure network. Instead it is more mathematically sound to have a combined ODE with both RHS terms summed together, and obtain the network architecture via operator splitting (or some similar technique).\n- Table 2 empirical performances in the first two columns are close and seem within-noise, requiring 5 significant figures on the benchmark to distinguish the models; within this context, bolding is done slightly incorrectly, e.g., only bolding Hyper-SET numbers in the case of a tie (0.417 in the second column). On the other hand, the text referencing Table 2 is slightly unfair to Hyper-SET since it can emphasize parity in the first two columns instead of saying it lags in all metrics.\n\n[1]: Dosovitskiy, Alexey. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020)."}, "questions": {"value": "Q1: Can you elaborate a little on the surprising performance of the baselines in the experiments? Namely,\n- Q1.1: For Figure 3a, do you have intuition why otherwise reasonably-successful architectures completely fail this task and get a 0?\n- Q1.2: For Table 2, do you have intuition why the energy transformer performance drops by a large amount on IN1K?\n\nQ2: Can you train some non-weight-shared networks with more parameters, e.g., using a largely validated pipeline such as the original ViT pipeline [1], to simultaneously establish fair comparisons w.r.t. deep network baselines, and show in some more detail how the proposed network can be made practically performant (in either the single-layer recurrent-depth or deep settings)? \n\n[1]: Dosovitskiy, Alexey. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8jYcDT8E2C", "forum": "FinhjyDgYA", "replyto": "FinhjyDgYA", "signatures": ["ICLR.cc/2026/Conference/Submission23725/Reviewer_fUR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23725/Reviewer_fUR7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761417634054, "cdate": 1761417634054, "tmdate": 1762942781786, "mdate": 1762942781786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a principled, \"top-down\" derivation of components of the Transformer architecture from an energy-based learning perspective. The authors posit that effective token representations should simultaneously achieve two goals: semantic alignment (clustering of similar concepts) and distributional uniformity (spreading representations out to avoid collapse). They formulate these two objectives as a single, combined hyperspherical energy function. The paper then demonstrates that an iterative optimization process to minimize this energy function naturally yields the core components of a Transformer block: symmetric self-attention emerges from the uniformity objective, a symmetric feedforward network from the alignment objective, and RMSNorm from the hyperspherical constraint. The resulting model, HYPER-SET, is a recurrent, parameter-shared architecture. The authors evaluate HYPER-SET on tasks like Sudoku solving, image classification, and masked image modeling, comparing its performance and parameter efficiency to standard Transformers."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well-written. The argument is presented with great clarity, smoothly taking readers from a conceptualization to mathematical derivations and finally to empirical validation.\nThe core premise of the work is very interesting. It is not the traditional heuristic-driven design of modern architectures; the \"first principles\" approach is compelling. The conceptualization of token dynamics as a balance between \"semantic alignment\" and \"distributional uniformity\" is intuitive and provides a compelling theoretical foundation for the architecture.\nThe mathematical derivations that connect these two principles to the standard Transformer components are elegant. Showing how attention, FFNs, and RMSNorm emerge as natural solutions to a single constrained energy minimization problem is the central theoretical strength of this work.\nThe empirical evaluation is extensive and respectable. The authors are transparent about the model's performance, showcasing its strengths in parameter efficiency and extrapolation on tasks like Sudoku, while also honestly reporting its limitations on larger-scale datasets. This thoroughness provides a complete picture of the model's utility and grounds the compelling theoretical framework in practical results."}, "weaknesses": {"value": "The primary drawback, which the authors acknowledge through their experiments, is the model's difficulty to scale to larger datasets (like ImageNet-1K) and more complex generative tasks. On these, it lags behind standard Transformer baselines. While the authors are not expected to conduct a full-scale SOTA-level experiment, and their attempt to scale by stacking two distinct layers is a valuable inclusion, the paper would be significantly strengthened by a more in-depth discussion on how this scaling limitation could be overcome. The paper falls short in hypothesizing why this limitation exists. Is the hard parameter-sharing of the recurrent model creating an optimization bottleneck that a deep stack of independent layers avoids? The work would be more complete if it drew parallels to existing literature on deep vs. recurrent models or the challenges of weight-sharing in large-scale settings. A discussion on potential paths forward would provide a more comprehensive outlook. Without this, the proposed model, while theoretically compelling, feels disconnected from the path toward practical, large-scale application."}, "questions": {"value": "For Sudoku extrapolation (Figure 3b), the authors attribute the gains to the proposed energy minimization dynamics. However, these improvements could maybe come from implicit regularization introduced by the symmetric parameter sharing and hyperspherical constraints, instead of the iterative energy optimization itself? Did the authors conduct ablations to disentangle the effects of the energy-based formulation from those of architectural regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yvOWT3mdS7", "forum": "FinhjyDgYA", "replyto": "FinhjyDgYA", "signatures": ["ICLR.cc/2026/Conference/Submission23725/Reviewer_Dhku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23725/Reviewer_Dhku"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943671090, "cdate": 1761943671090, "tmdate": 1762942781406, "mdate": 1762942781406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduces HYPER-SET, a principled that reframes the Transformer as an iterative energy minimizer on a hypersphere. This hyperspherical energy is designed to balance two key goals: semantic alignment with important features and distributional uniformity to prevent representational collapse. By deriving the math for this optimization, core components like symmetric attention, feedforward layers, and RMSNorm emerge naturally rather than hand-designed. The final recurrent-depth model is parameter-efficient and demonstrates competitive performance on diverse tasks, including Sudoku solving, image classification, and masked image modeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary strength of this work is its \"white-box\" design. The authors start from a unified principle (hyperspherical energy minimization) and derive the architectural components (Bi-Softmax attention, FFN, RMSNorm) as the mathematical solution.\n\n2. Figure 5 demonstrates that the designed energy function decreases during the forward pass. Figure 6 shows that the effective rank and average angle of tokens increase, empirically confirming that the \"distributional uniformity\" objective is being met. This link between theory and empirical dynamics is therefore good.\n\n3. HYPER-SET outperforms other models in the \"white-box\" design class (CRATE and Energy Transformer).\n\n4. Table 3 provides an ablation that validates the design choices. It shows that the components derived from the theory outperform more common alternatives."}, "weaknesses": {"value": "W1. The modelâ€™s good results on iterative reasoning tasks (like Sudoku) do not generalize to general-domain tasks. On standard benchmarks like ImageNet-1K and masked image modeling, the vanilla Transformer baseline remains superior.\n\nW2. The recurrent-depth design is a major practical drawback. As confirmed by the authors' runtime analysis in Table 15, the 1-layer, 12-iteration HYPER-SET model is significantly slower than a standard Transformer, despite having fewer parameters. This high latency severely limits its practicality."}, "questions": {"value": "Q1. The model is slower than a standard Transformer (Table 15) and underperforms on general-domain tasks (Weakness 1). Given these latency and performance gaps, how do the authors justify the practical value of its parameter efficiency?\n\nQ2. Does the training memory cost scale linearly with the number of iterations to store activations for backpropagation?\n\nQ3. The paper's most significant performance gap appears on the Sudoku task, where both CRATE and Energy Transformer completely fail on this task. What is the authors' hypothesis for why these other \"white-box\" models fail catastrophically on this reasoning task, while HYPER-SET succeeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3fBdXQ29ET", "forum": "FinhjyDgYA", "replyto": "FinhjyDgYA", "signatures": ["ICLR.cc/2026/Conference/Submission23725/Reviewer_jreb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23725/Reviewer_jreb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987827130, "cdate": 1761987827130, "tmdate": 1762942780991, "mdate": 1762942780991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new transformer component, based on the principle of optimizing a hyperspherical energy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper seeks to formulate model architecture design through the principle of energy minimization, which is an interesting and ambitious attempt.\n- The experiments span several domains, showing the effectiveness of the proposed method.\n- This paper is well presented and easy to follow."}, "weaknesses": {"value": "- It is unclear if the proposed \"energy\" is actually minimized. I don't see any empirical verification or any theoretical proof regarding this.\n- Also, there is no justification for whether the energy decrease actually correlates with performance.\n- The resulting architecture appears extremely similar to a standard Transformer with RMSNorm or other normalization layers. It is unclear what the \"energy\" formulation contributes beyond rephrasing existing operations in geometric language."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6WDT3wFwI7", "forum": "FinhjyDgYA", "replyto": "FinhjyDgYA", "signatures": ["ICLR.cc/2026/Conference/Submission23725/Reviewer_PVNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23725/Reviewer_PVNu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23725/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233043499, "cdate": 1762233043499, "tmdate": 1762942780672, "mdate": 1762942780672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}