{"id": "04HwYGgp2w", "number": 3835, "cdate": 1757544654492, "mdate": 1759898067519, "content": {"title": "ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning", "abstract": "The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a ``look-think-predict\" paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality—achieving an improvement of 10% over scalar-based reward models.", "tldr": "", "keywords": ["Image reward model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab62de115d368d82b0351f14bb9466e9bbe97c92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper,the authors introduces ImageDoctor, a unified,multi-aspect evaluation framework for Text-to Image(T2I) models. Unlike previous methods that provide a single scalar, ImageDoctor assesses image quality across four dimensions: plausibility, semantic alignment, aesthetics, and overall quality.ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. The model is built on a multi-modal large language models(MLLMs) and adopts a “look-think-predict” paradigm. Training involves a two-phase process: cold start and reinforcement finetuning with Group Relative Policy Optimization(GRPO) using tailored rewards. Furthermore, the paper proposes DenseFlow-GRPO, which utilizes ImageDoctor’s dense, pixel-level heatmaps as a dense reward signal. Experiments demonstrates that ImageDoctor achieves strong alignment with human preference across multiple datasets. Furthermore,when used as a reward model for preference tuning, ImageDoctor achieves an improvement of 10% over scalar-based reward models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.ImageDoctor breaks through the limitation of a single score by evaluating images from four meaningful dimensions(plausibility, semantic alignment, aesthetics, and overall quality).\n2.The ablation study demonstrates that the “look-think-predict” paradigm significantly improves the model’s overall performance.\n3.ImageDoctor is further introduced DenseFlow-GRPO, which leads to superior generation quality, especially in refining local details.\n4.As a reward model, ImageDoctor improves the quality of T2I model generation by 10% compared to scalar based reward models.\n5.ImageDoctor's heatmaps and inference chain can directly locate defect areas, filling the gap of lack of interpretability and spatial localization in T2I evaluation."}, "weaknesses": {"value": "1.ImageDoctor is trained mainly on RichHF-18K,which is limited in size and flaw type. These may affect its generalization ability to state-of-the-art T2I model generated images.\n2.ImageDoctor involves a MLLM, a heatmap decoder, a two-phase training pipeline.Complex processes are not replicated and promoted on a large scale."}, "questions": {"value": "1.When there are multiple overlapping flaws in the image, the bounding boxes generated in the \"look\" stage may have \"overlapping coverage\". Will this cause the \"think\" stage to be unable to distinguish between different types of defects?\n2.Could the authors provide a detailed explanation through which refining the bounding boxes via reward in the RFT phase leads to more accurate artifact heatmaps and higher score prediction accuracy? Does better \"look\" translate to better \"think\"?\n3.The MLLM output involves reasoning chains and scores. How critical is the rich textual reasoning generated by the MLLM to the final quality of the heatmaps, beyond just the task token?\n4.The paper chooses Qwen2.5-VL as the backbone of MLLM and does not mention the comparison with other mainstream multi-modal models such as Gemini 2.5 Flash and GPT-4o. What is the core reason for choosing Qwen2.5-VL? If replaced with Gemini 2.5 Flash, will ImageDoctor have performance differences in score consistency (PLCC/SRCC) and heatmap quality (MSE/CC)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yxpL47YNqW", "forum": "04HwYGgp2w", "replyto": "04HwYGgp2w", "signatures": ["ICLR.cc/2026/Conference/Submission3835/Reviewer_bUcB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3835/Reviewer_bUcB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716099696, "cdate": 1761716099696, "tmdate": 1762917057419, "mdate": 1762917057419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel VLM-based evaluation framework for text-to-image generation, named ImageDoctor. ImageDoctor not only provides multi-dimensional scoring capabilities, such as aesthetics and text-image alignment, but also offers pixel-level localization of flawed regions, enabling it to actively identify areas of misalignment and visual implausibility. Notably, the latter capability introduces a fresh perspective for reward modeling in text-to-image generation. Combined with the authors' proposed DenseFlow-GRPO method, which leverages pixel-level supervision signals for reinforcement learning, the framework effectively enhances the performance of image generation models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive Evaluation Framework: It introduces ImageDoctor, a novel VLM-based framework that provides not only multi-dimensional scoring (e.g., on aesthetics and text-image alignment) but also pixel-level localization of flawed areas, offering a more thorough analysis than mere scoring systems.\n\n- Innovative RL Paradigm for Image Generation: The pixel-level supervision signal from ImageDoctor paves the way for a novel reward model, which, combined with the proposed DenseFlow-GRPO reinforcement learning method, effectively enhances the performance of image generation models."}, "weaknesses": {"value": "- The model's ability to localize flaws is trained on the RF-18K dataset. While effective, this dataset's relatively small scale may constrain the full potential of the Vision-Language Model (VLM), potentially limiting its generalizability and precision in identifying a wider variety of visual anomalies.\n\n- The evaluation is based on GenAI-Bench and TIFA. To more rigorously demonstrate ImageDoctor's advanced capabilities, particularly in pinpointing violations of physical laws and common sense, future work could benefit from testing on newer, more challenging benchmarks such as WISE[1] and ABP[2].\n\n[1] WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation\n\n[2] Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation"}, "questions": {"value": "1. Given that ImageDoctor's localization capability is trained on the RF-18K dataset, to what extent can it generalize to detect a diverse range of physically implausible or commonsense-violating anomalies?\n\n2. The authors have successfully engineered a VLM capable of comprehensive image \"diagnosis\", including flaw localization. Have the authors considered applying this diagnostic capability directly within a unified understanding and generative model? We posit that integrating such a critical feedback mechanism internally could potentially offer more direct and significant improvements to the model's own generative capabilities, perhaps even surpassing the gains achieved by the current approach of using an external reward model for reinforcement learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t3QGxQNWMn", "forum": "04HwYGgp2w", "replyto": "04HwYGgp2w", "signatures": ["ICLR.cc/2026/Conference/Submission3835/Reviewer_z1tc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3835/Reviewer_z1tc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721750375, "cdate": 1761721750375, "tmdate": 1762917057213, "mdate": 1762917057213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ImageDoctor, a unified framework for Text-to-Image (T2I) evaluation that simultaneously outputs multi-aspect scores and spatially grounded heatmaps, offering richer and more interpretable feedback than traditional single-scalar assessments. The paper also introduces DenseFlow-GRPO, a method for T2I model fine-tuning, with experimental results demonstrating the value of pixel-level feedback in improving evaluation accuracy and eliminating local artifacts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  ImageDoctor mitigates the lack of fine-grained diagnostic signals in existing T2I evaluation metrics. Its multi-aspect scores and heatmaps greatly enhance the practicality and interpretability of the feedback.\n2.  The proposed DenseFlow-GRPO successfully leverages the dense rewards from ImageDoctor, achieving T2I model optimization superior to that of traditional sparse rewards (Flow-GRPO), and is particularly adept at handling local artifacts.\n3.  The experimental results demonstrate a clear advantage performance."}, "weaknesses": {"value": "1.  Does the reward model exhibit \"reward hacking\" during optimization with DenseFlow-GRPO? How does the paper address this issue, and do the reported experimental results come from trials where reward hacking was successfully mitigated?\n2.  Were the same seeds used in the visualization examples for DenseFlow-GRPO and Flow-GRPO to ensure a fair comparison? In the visualization example, a tennis racket still contains artifacts; is this limitation due to the reward model not being strong enough, or is it an inherent issue with the base diffusion model? If it is a problem with the base model, can the effectiveness of the proposed reward model and DenseFlow-GRPO in addressing artifacts be validated using a more powerful base model?\n3.  Compared to existing lightweight scalar reward models, what is the specific overhead of ImageDoctor, in terms of inference speed and resource consumption? Please provide a quantitative comparison.\n4.  Can you add a discussion about more recent reward models? For example, works like 'Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment' and 'HPSv3: Towards Wide-Spectrum Human Preference Score'."}, "questions": {"value": "Answer the questions in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ldm65H1lzA", "forum": "04HwYGgp2w", "replyto": "04HwYGgp2w", "signatures": ["ICLR.cc/2026/Conference/Submission3835/Reviewer_yq1W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3835/Reviewer_yq1W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886210370, "cdate": 1761886210370, "tmdate": 1762917057007, "mdate": 1762917057007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ImageDoctor, a unified and interpretable evaluation framework for text-to-image generation. ImageDoctor provides multi-dimensional feedback and introduces pixel-level diagnostic heatmaps for grounded and fine-grained evaluation. The model adopts a \"look-think-predict\" paradigm. Experimental results show that ImageDoctor achieves state-of-the-art correlation with human judgments and improves text-to-image generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of heatmap-guided dense rewards for T2I reinforcement learning is  innovative.\n\nExtensive experiments across multiple datasets demonstrate consistent improvement in human alignment metrics.\n\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "ImageDoctor’s reasoning quality may depend heavily on the backbone MLLM (Qwen2.5-VL). The paper lacks analysis on robustness across different model sizes or architectures.\n\n\nThe “look-think-predict” reasoning chain increases inference time compared to scalar evaluators. A discussion of trade-offs between interpretability and efficiency would strengthen the work.\n\n\nHow sensitive is ImageDoctor’s performance to the choice of MLLM backbone? Would smaller or different architectures (e.g., LLaVA, BLIP2) yield similar trends?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w94QtyzdrS", "forum": "04HwYGgp2w", "replyto": "04HwYGgp2w", "signatures": ["ICLR.cc/2026/Conference/Submission3835/Reviewer_LnNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3835/Reviewer_LnNs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979739324, "cdate": 1761979739324, "tmdate": 1762917055866, "mdate": 1762917055866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}