{"id": "YlU6SELb6C", "number": 15571, "cdate": 1758252754620, "mdate": 1763735103545, "content": {"title": "Feasible Policy Optimization for Safe Reinforcement Learning", "abstract": "Policy gradient methods serve as a cornerstone of reinforcement learning (RL), yet their extension to safe RL, where policies must strictly satisfy safety constraints, remains challenging. While existing methods enforce constraints in every policy update, we demonstrate that this is unnecessarily conservative. Instead, each update only needs to progressively expand the feasible region while improving the value function. Our proposed algorithm, namely feasible policy optimization (FPO), simultaneously achieves both objectives by solving a region-wise policy optimization problem. Specifically, FPO maximizes the value function inside the feasible region and minimizes the feasibility function outside it. We prove that these two sub-problems share a common optimal solution, which is obtained based on a tight bound we derive on the constraint decay function. Extensive experiments on the Safety-Gymnasium benchmark show that FPO achieves excellent constraint satisfaction while maintaining competitive task performance, striking a favorable balance between safety and return compared to state-of-the-art safe RL algorithms.", "tldr": "Proposes a region-wise policy optimization method that monotonically improves safety and performance.", "keywords": ["Safe reinforcement learning", "feasible policy iteration", "region-wise policy optimization", "constraint decay function"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d96c3f6efa75c4701ed0a99372e6b1d85f525b06.pdf", "supplementary_material": "/attachment/0bfb7e9c5d4c91c50496fe63577a221c15a19bf7.zip"}, "replies": [{"content": {"summary": {"value": "The paper talks about an efficient approach where, instead of enforcing the constraint satisfaction explicitly after every policy update, it talks about maximizing the Value function in the present feasible domain and then progressively expanding the feasible domain.\nIn particular, the paper solves the problem in two ways. At first, they solve \n$\\max_{\\pi} \\mathbb{E}_{x \\sim d_{init}}\\left[ \\mathbb{I}\\left[{F^{\\pi_{k}} \\leq 0}\\right]V^{\\pi}(x)\\right]$\ns.t. $ \\max_{\\pi} \\mathbb{E}_{x \\sim d_{init}}\\left[ \\mathbb{I}\\left[{F^{\\pi_{k}} \\leq 0}\\right]F^{\\pi}(x)\\right] \\leq 0$\n\nSimultaneously, they aim to expand the feasible range by solving the following optimization problem.\n$\\max_{\\pi} \\mathbb{E}_{x \\sim d_{init}}\\left[ \\mathbb{I}\\left[{F^{\\pi_{k}} > 0}\\right]F^{\\pi}(x)\\right]$\ns.t. $ \\max_{\\pi} \\mathbb{E}_{x \\sim d_{init}}\\left[ \\mathbb{I}\\left[{F^{\\pi_{k}} \\leq 0}\\right]F^{\\pi}(x)\\right] \\leq 0$\n\nThey show that the two problems have a common optimization solution and further finds the feasibility bounds of their solution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The Constraint enforcement at every step is shown to be overly conservative and not necessary\n2. Talks about a Feasible Policy Optimization algorithm which aims to get the safe policy at every instant\n3. At each step it can guarantee policy improvement almost surely.\n4. Evaluated on some standard Safe RL benchmarks."}, "weaknesses": {"value": "1. The writing was little hard to follow with a lot of minor gaps in equations for example, In equation 11 $k$,  should come in the subscript for the equations. $d^{\\pi_{k}}$ and not $d^\\pi k$ and  if $A^{\\pi_{k}}(x,u)$ was introduced\n\n2. The reachability set is little too stringent. I get that it is required for the proofs but it created confusions too. For example. In Theorem 1 the paper claims that if $\\pi_{k+1}$ gets a value in $$ \\mathcal{R}^{\\pi_{in}(\\mathcal{X}_{init} \\cap X^{\\pi_{k}}) $$, $\\pi_{k+1}$ would be replaced by $\\pi_{in}$. This need not be true always. This is because $\\mathcal{R}$ is a reability state. And according to definition of $\\mathcal{R}$ introduced in the paper, it might be possible that $\\pi_{k+1}$ and $\\pi_{k}$ can have some overlapping reachability sets but it does not necessarily means that it will switch to some other policy. \n\n3. Over usage of $c$, used as cost as well as $\\mathbb{I}\\left[ h(x) > 0\\right]$ for indicator constraint violation. Is the constraint violation same as cost or not. These small details made it little hard to follow,"}, "questions": {"value": "1. The reachability set is little too stringent. I get that it is required for the proofs but it created confusions too. For example. In Theorem 1 the paper claims that if $\\pi_{k+1}$ gets a value in $$ \\mathcal{R}^{\\pi_{in}(\\mathcal{X}_{init} \\cap X^{\\pi_{k}}) $$, $\\pi_{k+1}$ would be replaced by $\\pi_{in}$. This need not be true always. This is because $\\mathcal{R}$ is a reability state. And according to definition of $\\mathcal{R}$ introduced in the paper, it might be possible that $\\pi_{k+1}$ and $\\pi_{k}$ can have some overlapping reachability sets but it does not necessarily means that it will switch to some other policy. It would be helpful if this is explicitly clarified.\n\n2. In Figure 1, we observe that the return and cost of FPO are the best, which is great. I am just curious to know that only FPO has the lowest uncertainty range in terms of cost. But its uncertainty range in terms of return is a little higher. So, if we run for long enough as you have for your experiments, why is the uncertainty range for other algorithms, such as PPO-Lag and FOCOPS, so vast? Because for PPO-Lag, there is a possibility that it can go less than 0.08 in cost while giving better value return. For example, if you check the Point Button plot, PPO-lag has supremely outperformed FPO.\n\n3. Also curious how the given FPO algorithm would perform compared to CRPO"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0I35DT8As2", "forum": "YlU6SELb6C", "replyto": "YlU6SELb6C", "signatures": ["ICLR.cc/2026/Conference/Submission15571/Reviewer_6eAt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15571/Reviewer_6eAt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761513226094, "cdate": 1761513226094, "tmdate": 1762925844565, "mdate": 1762925844565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new policy optimization scheme for safe reinforcement learning based on limiting trajectory-wise constraint violations rather than expected constraint violations. The authors develop a dynamic programming method to estimate policy feasibility and combine it with PPO, achieving a scalable RL algorithm that maintains high rewards while avoiding unsafe states."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Novel safety framework introducing the constraint decay function $ F $  \n- Focus on trajectory-wise feasibility rather than expected feasibility (even though $ F $ is estimated in expectation, it measures violations more directly)  \n- Clear writing and presentation  \n- Excellent Figure 1 (should be standard for safe RL)"}, "weaknesses": {"value": "- More recent baselines would strengthen the empirical comparison (beyond Ji et al., 2023)  \n- Curiosity: isnâ€™t $ F $ typically underestimated due to truncated trajectories? Could this affect feasibility estimation?"}, "questions": {"value": "- See weak points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oSjCv83RI0", "forum": "YlU6SELb6C", "replyto": "YlU6SELb6C", "signatures": ["ICLR.cc/2026/Conference/Submission15571/Reviewer_KU9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15571/Reviewer_KU9E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936521029, "cdate": 1761936521029, "tmdate": 1762925844175, "mdate": 1762925844175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose feasible policy optimization to avoid over-conservatism in safe RL problem. Specifically, it progressively increases feasible region when updating the value function by maximizing the value function in feasible region and minimizing the feasibility function outside. The experiments on safety gymnasium benchmark show that it achieves better balance between reward and cost constraint."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of this paper, i.e., update feasibility function and value function, is clearly presented.\n- The topic of this paper is to the interest of safe RL community."}, "weaknesses": {"value": "From the experiment (e.g, in fig.2), it is hard to say FPO is superior to baselines.\n- First, the baselines in some tasks are not well set up. For example, PointCircle task is obviously much easier than PointButton and CarGoal. However, the experiments show that PPO-Lag, RCPO, FOCOPS cannot learn a relatively safe policy but they can perform relatively well on PointButton or CarGoal. Actually, I believe the authors did not even try to set a reasonable lagrangian coefficient for them on PointCircle because they perform just like unconstrained RL. As a reference, PPO-Lag can learn well on PointCircle in other library [1].\n- Second, the authors claim they follow the original hyperparameter in Omnisafe. However, Omnisafe tuned the hyperparameter for safety constraint $=25$ (and the results show that the baselines can indeed make cost smaller than 25) while this paper uses cost limit $=0$. \n- Consider the performances of three hardest tasks (PointButton, CarGoal, CarPush), FPO performs similarly to PPO-Lag and RCPO: FPO has lower reward and a little lower cost.\n- The velocity tasks are MUCH easier than navigation tasks in terms of reward-cost trade-off [2]. Meanwhile, the baselines on swimmervelocity is absolutely not well tuned. The ppo-lag and FOCOPS perform well in [1]."}, "questions": {"value": "- The authors use $F^\\pi(x) \\leq 0$ to denote the feasibility region but also use $F=E_\\tau[\\gamma^{N(\\tau)}]$ which is $\\geq 0$. So what's the physical meaning of $F^\\pi(x) < 0$?\n- The compared baselines (e.g., Lagrangian-based) can learn different policies w.r.t different constraint limits. How does FPO adjust to different safety preference?\n\n[1] https://fsrl.readthedocs.io/en/latest/tutorials/benchmark.html\n\n[2] https://safety-gymnasium.readthedocs.io/en/latest/environments/safe_velocity.html#costs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ewIilLJxUf", "forum": "YlU6SELb6C", "replyto": "YlU6SELb6C", "signatures": ["ICLR.cc/2026/Conference/Submission15571/Reviewer_92qC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15571/Reviewer_92qC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979533105, "cdate": 1761979533105, "tmdate": 1762925843697, "mdate": 1762925843697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}