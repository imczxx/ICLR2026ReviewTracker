{"id": "9PWyFDedjx", "number": 19129, "cdate": 1758293755568, "mdate": 1763337663091, "content": {"title": "Adaptive Policy Backbone via Shared Network", "abstract": "Reinforcement learning (RL) has achieved impressive results across domains, yet learning an optimal policy typically requires extensive interaction data, limiting practical deployment. A common remedy is to leverage priors—such as pre-collected datasets or reference policies—but their utility degrades under task mismatch between training and deployment. While prior work has sought to address this mismatch, it has largely been restricted to in-distribution settings. To address this challenge, we propose $\\textbf{A}$daptive $\\textbf{P}$olicy $\\textbf{B}$ackbone (APB), a meta-transfer RL method that inserts lightweight linear layers before and after a shared backbone, thereby enabling parameter-efficient fine-tuning (PEFT) while preserving prior knowledge during adaptation. Our results show that APB improves sample efficiency over standard RL and adapts to out-of-distribution (OOD) tasks where existing meta-RL baselines typically fail.", "tldr": "A novel method for sample efficiently policy adaptation to out-of-distribution tasks", "keywords": ["Reinforcement learning", "Meta reinforcement learning", "Transfer meta reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/80e44c9a6ae3ec01f7df6cc4848159d1219ccc91.pdf", "supplementary_material": "/attachment/b40673cefa3276c6679ef63eb65876e7a4324bb4.zip"}, "replies": [{"content": {"summary": {"value": "To solve the out-of-distribution adaptation problem, the authors have proposed a new meta learning approach. In the proposed approach, a novel policy structure is designed called adaptive policy backbone (APB). With this policy structure, a parameter-efficient finetuning can be achieved. The proposed approach is evaluated in the mujoco domain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed approach is not only empirically evaluated, but also well theoretically grounded.\n\n2.\tThe paper is well written, so the importance of OOD adaptation is adequately conveyed."}, "weaknesses": {"value": "1.\tIt is unclear how different the structure in Figure 1 is from the common multi-head policy structure, which is widely employed in the multi-task learning setting. This is related to the novelty of the proposed approach. \n\n2.\tAs APB is nearly the same as the multi-head policy network, how can it achieve OOD adaptation? Is it related to the freezing learning scheme in the meta-testing phase? \n\n3.\tThe experiments have shown that the proposed approach can work in certain OOD settings. However, these settings have not demonstrated significant difference between meta-training and meta-testing. Can the proposed approach adapt to more difference between meta-training and meta-testing? For example, the simulated robots have different morphologies."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YQkFEbUmOX", "forum": "9PWyFDedjx", "replyto": "9PWyFDedjx", "signatures": ["ICLR.cc/2026/Conference/Submission19129/Reviewer_QFW1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19129/Reviewer_QFW1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558919986, "cdate": 1761558919986, "tmdate": 1762931149221, "mdate": 1762931149221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "jB2FpKcvtX", "forum": "9PWyFDedjx", "replyto": "9PWyFDedjx", "signatures": ["ICLR.cc/2026/Conference/Submission19129/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19129/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763337659383, "cdate": 1763337659383, "tmdate": 1763337659383, "mdate": 1763337659383, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Adaptive Policy Backbone (APB), a meta-transfer RL method designed to improve sample efficiency and adaptation to out-of-distribution (OOD) tasks. APB introduces a shared policy backbone with lightweight linear pre- and post-layers, enabling parameter-efficient fine-tuning while preserving prior knowledge. Theoretical analyses support the claim that updating only these linear layers can adapt to new tasks, and empirical results on MuJoCo benchmarks demonstrate improved OOD adaptation in terms of mean return compared with existing meta-RL baselines.\n\nHowever, the theoretical analysis is overly restrictive, and the empirical performance gains are not particularly strong."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The insertion of lightweight linear layers before and after a shared backbone offers a clean and parameter-efficient approach to meta-transfer RL, bridging ideas from PEFT and meta-learning. The algorithm design is intuitive and easy to implement.\n\nThe authors did comprehensive experiments on six MuJoCo environments cover both reward and dynamics shifts."}, "weaknesses": {"value": "1. The core theorem relies on isomorphic MDPs (state permutation assumption), which rarely hold in practice. The theoretical link between this case and realistic OOD adaptation remains somewhat heuristic.\n2. Although APB reduces the number of trainable parameters, the reported gains in sample efficiency over standard RL are modest, suggesting limited practical advantage. Moreover, APB seems to induce higher variance than standard RL and baseline meta RL algorithms."}, "questions": {"value": "1. Meta-RL typically assumes differences in transition dynamics (and possibly rewards) across environments. This work focuses only on reward shifts in both the problem formulation and theoretical analysis. Could the authors justify this simplification or discuss its implications for broader generalization?\n2. The experimental improvements are relatively weak. The gains over standard RL are modest, and APB is not consistently better than other meta-RL baselines. Since PEFT methods primarily aim for computational efficiency, it would strengthen the paper if the authors could provide quantitative evidence of reduced computational cost or training time compared to existing baselines.s is overly restrictive, and the empirical performance gains are not particularly strong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QH8fJslPD8", "forum": "9PWyFDedjx", "replyto": "9PWyFDedjx", "signatures": ["ICLR.cc/2026/Conference/Submission19129/Reviewer_LC2o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19129/Reviewer_LC2o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878008840, "cdate": 1761878008840, "tmdate": 1762931148587, "mdate": 1762931148587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose Adaptive Policy Backbone, a method that does Meta RL adaptation by only tuning two task-specific linear layers (on the inputs, and on the outputs)."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The proposed method is very straightforward, and the paper is written extremely clearly."}, "weaknesses": {"value": "- “Across most tasks, the proposed method achieves (marginally) better performance than a standard RL algorithm, exhibiting faster convergence and/or higher asymptotic average return for the same number of interactions. The results are demonstrated in Figure 4” Figure 4 does not show better performance for APB than for the standard RL algorithm, neither with faster convergence nor asymptotic average return, across most tasks. Asymptotically, only (d) shows statistically significant differences between the methods. With respect to faster convergence, only (a). Please correct me if I am missing something, but this does not seem like an honest reading of the numbers being reported.\n- The algorithm formulation is incomplete. Critic and actor losses are undefined.\n- The core idea of the project is extremely limited. For some given pre-trained policy, there is only a limited set of new tasks for which two additional linear layers would suffice for adaptation. The toy example for theoretical study is just not useful.\n- From the abstract: “Our results show that APB improves sample efficiency over standard RL”. From the limitations statement: “it does not yield a significant improvement in sample efficiency;”."}, "questions": {"value": "“Furthermore, it is well established that fine-tuning only a subset of parameters can significantly improve sample efficiency and reduce training costs” Can you please add citations for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qtE3bxAjJs", "forum": "9PWyFDedjx", "replyto": "9PWyFDedjx", "signatures": ["ICLR.cc/2026/Conference/Submission19129/Reviewer_Z8P4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19129/Reviewer_Z8P4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976872221, "cdate": 1761976872221, "tmdate": 1762931147889, "mdate": 1762931147889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}