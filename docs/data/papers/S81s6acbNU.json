{"id": "S81s6acbNU", "number": 16661, "cdate": 1758267382417, "mdate": 1763629690596, "content": {"title": "TRACE: Adaptive Curtailment of Reasoning in Retrieval-Augmented Generation via Trajectory Reflection", "abstract": "Large Reasoning Language Models (LRLMs) excel at complex reasoning tasks by generating multi-step chains of thought. However, their autoregressive nature can lead to overthinking, a tendency to generate overly verbose reasoning that inflates computational costs and can even degrade accuracy. Advanced methods mitigate overthinking by monitoring the model's internal confidence and terminating the process once a high-confidence answer is found. This strategy is effective when models reason using their parametric knowledge, but it faces significant challenges and risks failure in Retrieval-Augmented Generation (RAG) scenarios where external knowledge is introduced. We have conducted an in-depth analysis of this issue and reveal that the reasoning process in RAG universally follows a distinct, two-stage Exploratory-Synthesizing pattern. Unlike scenarios that rely solely on parametric knowledge where confidence gradually accumulates, the initial exploration phase in this pattern involving external documents exhibits premature confidence, where models become highly certain after inspecting only partial evidence. This early confidence surge misleads conventional termination methods, causing them to halt the process prematurely and produce incorrect answers. To address this, we propose Trajectory Reflection with Adaptive Curtailment and Exit (TRACE), a training-free framework that employs a cascading check at each reasoning step. First, it monitors the stability of the model's predictive beliefs to ensure sufficient knowledge exploration. Subsequently, it assesses task completion by confirming high confidence in a synthesized final answer. Extensive experiments demonstrate that TRACE reduces token generation by 22\\% to 54\\% while achieving comparable or superior accuracy to standard Chain-of-Thought prompting.", "tldr": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Chain-of-Thought", "overthinking"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46f4a33e040e9ad4d26d27713b6b30511bb78f5c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript investigates how to decrease the thought length of large reasoning models regarding for the RAG setting. This manuscript first demonstrated the failure of existing methods and provided the reasons. Then, this manuscript proposes a new method with new metrics to address the problems in RAG settings. The experiments on various models and various benchmarks showed the effectiveness of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Propose an effective and convenient methods to address the overthinking problems of reasoning models.\n2. The writing is good and the presentation is clear, which make this manuscript is easy to understand.\n3. The results show that the proposed methods is competitive with baselines."}, "weaknesses": {"value": "1. I have some concerns about the experiment setting. First, Qwen 3 8B and Qwen3 14B are not reasoning models. Second, the selected RAG tasks only require about 1k outputs, which cannot represent the long outputs of reasoning models[1].\n\n2. Regarding the results, providing the latency of the reasoning thought can directly demonstrate the speed-up ratio. I am curious about the comparison with [1].\n\n\n[1] R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"}, "questions": {"value": "1. What's the motivation for compressing the reasoning thoughts of LRM over the RAG setting? What's the redundancy？\n\n2. Is this method compatible with the situation without RAG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CEiY03urDq", "forum": "S81s6acbNU", "replyto": "S81s6acbNU", "signatures": ["ICLR.cc/2026/Conference/Submission16661/Reviewer_gtP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16661/Reviewer_gtP5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655660746, "cdate": 1761655660746, "tmdate": 1762926720921, "mdate": 1762926720921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an efficiency challenge in Large Reasoning Language Models (LRLMs) operating within RAG frameworks. The authors identify a fundamental limitation of existing confidence-based early-exit methods (e.g., DEER) in RAG settings: they are misled by premature confidence, where models exhibit high certainty after processing only partial evidence, leading to incorrect answers and degraded accuracy. Through empirical analysis, the authors characterize an Exploratory-Synthesizing pattern in RAG reasoning, contrasting it with the convergent deduction pattern of non-RAG scenarios. To mitigate this, they propose TRACE, a training-free framework that employs a cascading, dual-check mechanism. At each reasoning step, TRACE first assesses the stability of the model's predictive belief state to ensure sufficient knowledge exploration and then verifies high confidence in a synthesized final answer before termination. Extensive experiments demonstrate that TRACE significantly reduces token generation while maintaining accuracy compared to standard Chain-of-Thought prompting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The motivation of the paper is clear. The paper reveals that RAG introduces a unique reasoning dynamic that systematically undermines existing efficiency methods. The identification and quantitative validation of the \"premature confidence\" phenomenon contributes to the area and the idea of \"Exploratory-Synthesizing\" is reasonable.\n\n(2) The proposed TRACE framework is solid. Its training-free nature ensures broad applicability without the cost of additional fine-tuning. The dual-check mechanism is well-justified.\n\n(3) The experimental design is thorough, employing multiple models and diverse benchmarks to establish generalizability.  The ablation study is effective."}, "weaknesses": {"value": "(1)  The framework relies on two key thresholds, θ_{exp} and θ_{inf} , which are tuned per dataset. While a grid search protocol is described, the paper does not deeply explore the sensitivity of the results to these values. The generalizability of these optimal thresholds across different model families or task domains should be discussed.\n\n(2) Only some selected QA datasets are used for evaluation. There are a large number of datasets available for RAG evaluation, and a wider range of evaluation should be conducted. Please check https://github.com/RUC-NLPIR/FlashRAG for more datasets available."}, "questions": {"value": "(1) Could the authors provide more insight into the failure modes of the belief stability check? Are there scenarios where the belief state stabilizes on an incorrect hypothesis, and how might TRACE be extended to detect and correct such cases?\n\n(2) Given that the belief state  is derived from a single token, how does the method perform with answers that are phrases or entities whose first token might be ambiguous or uninformative? Has the team experimented with multi-token probing strategies?\n\n(3) How will TRACE be integrated into more advanced RAG paradigms, such as those involving iterative retrieval or self-critique (e.g., Self-RAG)? Could the trajectory reflection module be used to trigger not just termination, but also a retrieval of additional documents if exploration is deemed insufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jOlH25xL2o", "forum": "S81s6acbNU", "replyto": "S81s6acbNU", "signatures": ["ICLR.cc/2026/Conference/Submission16661/Reviewer_mt9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16661/Reviewer_mt9Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978708943, "cdate": 1761978708943, "tmdate": 1762926720215, "mdate": 1762926720215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TRACE, a training-free framework designed to prevent premature early exit during reasoning in RAG framework.\nThe method aims to adaptively curtail reasoning by comparing belief-state distributions across reasoning steps, measured through KL divergence and combining this signal with an answer-confidence check. The authors argue that this dual criterion helps distinguish between two reasoning phases: Exploration and Synthesis, mitigating the issue of premature termination caused by overconfident early retrieval. Experiments on several QA datasets (NQ, TriviaQA, SQuAD, HotpotQA) using Qwen and DeepSeek models suggest that TRACE reduces reasoning length (22 ~ 54% token savings) without accuracy loss relative to Chain-of-Thought (CoT) prompting or DEER-style early exit."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. $\\textbf{Clear motivation and problem framing.}$\nThe paper identifies an important limitation in RAG applied with CoT prompting: confidence-based early exit methods often fail when external documents cause inflated confidence before synthesis is complete. This diagnosis is intuitively appealing and practically relevant.\n2. $\\textbf{Training-free and easily deployable method.}$\nTRACE can be applied to any LLM pipeline without retraining or model modification. Its compatibility with existing RAG setups makes it attractive from an engineering perspective.\n3. $\\textbf{Empirical efficiency improvements.}$\nThe reported token reduction (22 ~ 54%) and runtime gains are impressive, especially given that accuracy is maintained on most benchmarks."}, "weaknesses": {"value": "1. $\\textbf{Unconvincing causal link between distributional stability and early-exit mitigation.}$\nThe central claim that comparing belief distributions over reasoning steps can prevent premature exits is not sufficiently supported.\nAs shown in Figure 1, the “with-document” setting maintains high confidence from the start, yet it is unclear how distributional comparison (e.g., small KL divergence) can reliably distinguish premature local certainty from true synthesis completion.\nStabilized distributions might simply indicate consistent but incomplete retrieval reasoning rather than successful synthesis. The paper provides no direct analysis linking KL stability to epistemic completeness or correctness.\n2. $\\textbf{Limited experimental depth and analysis.}$z\nDespite mentioning “extensive experiments,” the evaluation primarily includes only two result tables (Tables 2–3).\nThere is no detailed analysis of the relationship between distributional dynamics and performance, nor ablation on threshold sensitivity or task type. Visualization or correlation studies between KL divergence and correctness would be essential to substantiate the paper’s theoretical motivation.\n3. $\\textbf{Hyperparameter and baseline configuration unclear.}$\nThe paper briefly mentions empirical choices of thresholds ($\\theta_\\text{exp}, \\theta_\\text{inf}$) in Appendix A.1 but does not describe details of performance and length trend when the hypereparameters were different.\nSimilarly, baseline methods such as DEER or Self-RAG are presented without clarification of whether comparable hyperparameter tuning was performed. This omission makes the fairness of the comparison somewhat uncertain.\n4. $\\textbf{No analysis on the number of retrieved documents.}$\nSection 4.1 notes that the experiments use the top-5 retrieved passages, but there is no study on how performance or early-exit stability changes with different document counts (e.g., top-3 vs. top-10). Given that premature confidence is tied to retrieval context size, such an ablation is crucial."}, "questions": {"value": "1. In Figure 1, confidence remains high from the first to the last step in the “with document” setting. How can TRACE detect such premature confidence if belief distributions are already stable early?\n2. How sensitive is TRACE to the thresholds ($\\theta_\\text{exp}, \\theta_\\text{inf}$)? Were there any specific pattern between the thresholds and performance or generation length?\n3. Were similar tuning efforts (e.g., grid search or threshold sweeping) performed for baseline methods such as DEER or Dynasor-CoT?\n4. The paper mentions using top-5 retrieved passages in RAG. How does performance vary when the number of retrieved documents changes (e.g., top-3 vs. top-10)? Is the method robust to such variations compared to baselines?\n5. Have the authors verified whether KL stability correlates with semantic completeness, such as evidence coverage or synthesis correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sGjW7PcXrn", "forum": "S81s6acbNU", "replyto": "S81s6acbNU", "signatures": ["ICLR.cc/2026/Conference/Submission16661/Reviewer_fBbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16661/Reviewer_fBbi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996745227, "cdate": 1761996745227, "tmdate": 1762926719733, "mdate": 1762926719733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a crucial problem of overthinking in large reasoning models under RAG scenarios. Most of the existing frameworks are based on the assumption that reasoning depends only on the model's internal, parametric knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1. This paper addresses an unaddressed problem of overthinking in LLMs under RAG conditions.\n2. TRACE is a novel method that addresses the overthinking under RAG."}, "weaknesses": {"value": "Weaknesses:\n1. Mosty Qwen family is considered for the experiments."}, "questions": {"value": "1. Is the same phenomenon seen for other models such as Llama or open-source models?\n2. Is there a scaling law?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hkxg72denY", "forum": "S81s6acbNU", "replyto": "S81s6acbNU", "signatures": ["ICLR.cc/2026/Conference/Submission16661/Reviewer_nASG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16661/Reviewer_nASG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762437893857, "cdate": 1762437893857, "tmdate": 1762926718914, "mdate": 1762926718914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}