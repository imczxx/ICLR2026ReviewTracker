{"id": "iPiEllQhsn", "number": 18750, "cdate": 1758290627408, "mdate": 1759897083347, "content": {"title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework", "abstract": "Current learning-based subject customization approaches, predominantly relying on U-Net architectures, suffer from limited generalization ability and compromised image quality. Meanwhile, optimization-based methods require subject-specific fine-tuning, which inevitably degrades textual controllability. To address these challenges, we propose InstantCharacter—a scalable framework for character customization built upon a foundation diffusion transformer. InstantCharacter demonstrates three fundamental advantages: first, it achieves open-domain personalization across diverse character appearances, poses, and styles while maintaining high-fidelity results. Second, we introduce a scalable dual-adapter architecture with stacked transformer encoders, which effectively processes open-domain character features and seamlessly interacts with the latent space of modern diffusion transformers. Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples. The dataset is systematically organized into paired (multi-view character) and unpaired (text-image combinations) subsets. Our dual-adapter structure addresses the challenge of generating multi-character images by enhancing subject consistency through the image adapter and improving layout control of multiple subjects through the text adapter. Qualitative experiments demonstrate the advanced capabilities of InstantCharacter in generating high-fidelity, text-controllable, and character-consistent images, setting a new benchmark for character-driven image generation.", "tldr": "", "keywords": ["Personalized Character Generation; Diffusion Transformer"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/327ac42e4d455cc1c34d6769ed194bc2b92d83a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors of the paper propose a solution for customized text-to-image generation. \nThey leverage adapter training on custom data and leave the diffusion network untouched.\nThere are multiple adapters for reference image and text processing, which come from preselected image and text feature extraction models. Only adapters get trained, and everything else remains untouched. They propose 3 stages of the training process, which have different purposes. The authors claim that their method can generate multiple characters as well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Constructing a custom dataset with 10 million examples for character customization\n2. Adapter-based text-to-image customization approach, which is flexible and avoids touching the diffusion model or causing knowledge loss."}, "weaknesses": {"value": "1. Poor results on almost all quantitative comparisons. The superiority of the proposed model is not justified with quantitative comparisons. The results are mostly worse than other competitive solutions. This also questions the fairness of the example selection of qualitative comparisons.\n    \n   Specifically, in the \"Quantitative Results\" section, the claim about UNO performance is noticeably poor. To justify UNO's better performance in Tab 1 and Tab 2, the authors refer to a qualitative comparison. For a fair comparison, the claim and the justification should be in the same field: either both should be compared in qualitative results or in quantitative results. \n\n2. Limited scientific novelty. Even though the authors have done an extensive job, there is nothing unique or new.\n\n3. The paper is poorly written.\n\n    a) In expression 1, there is F and F^{Q}, but in the description, F is explained as a concatenation result of F^{siglip}, F^{dino}; meanwhile, F is the output of the attention.\n\n    b) (small note) In expressions 2 and 3, H is noted as the hidden features of DiT. However, in expression 4, H is for text embeddings.\n\n    c) In section 3.1.1 (208-211), the authors mention that the output features of the image encoder, F^{siglip}_{l} and F^{Dino}_{r}, go through separate encoders (multiple encoders) for further processing. What encoders are they? It can't be the image adapter, cause the authors mention separate and multiple encoders. There are no details about them."}, "questions": {"value": "1. Would you give more details about \"learnable queries\" you mentioned in multiple places (e.g., 236)?\n2. In the ablation study, how are reference image features used when the Transformer Encoder is removed?\n3. In Figure 2, how are the hidden features of DiT described as image and textual features, in 2 separate groups? Aren't the Text Adapter outputs and textual embeddings supposed to have an attention before being injected into DiT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q7VdwY0Q1S", "forum": "iPiEllQhsn", "replyto": "iPiEllQhsn", "signatures": ["ICLR.cc/2026/Conference/Submission18750/Reviewer_8u1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18750/Reviewer_8u1R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793436682, "cdate": 1760793436682, "tmdate": 1762928468753, "mdate": 1762928468753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a scalable framework, called InstantCharacter, for character customization built upon a DiT-based diffusion model: Flux. InstantCharacter consists of three key components: a scalable dual-adapter architecture that parses character features and interacts with DiTs latent space, a progressive three-stage training strategy that separates training for character consistency, text editability, and visual fidelity, and a new pipeline for constructing training data pairs for multi-character customization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The images generated by the proposed method for character customization are plausible and impressive.\n2. This paper is well-written and well-organized.\n3. This paper provides a versatile 10-million-level character dataset, which contains paired (multi-view character) and unpaired (text-image combinations) subsets.\n4. Extensive experiments are conducted to evaluate the performance of the proposed method."}, "weaknesses": {"value": "1. What are the objective loss functions used in the three training stages of this paper? The first stage involves the reconstruction of the input image, which is presumably achieved using standard diffusion loss. However, both the second and third stages involve transformations of the original image; what loss functions are used in these stages?\n\n2. As shown in Tables 1 and 2, the quantitative results of the proposed method do not seem very satisfactory, as it fails to demonstrate a clear advantage over existing baseline methods.\n\n3. The ablation studies conducted in this paper are not sufficient, and the effects of many important components have not been evaluated. For example, what would be the difference in performance with and without the text adapter? What would be the difference in performance with and without the dual-stream feature fusion strategy (Section 3.1.1)?\n\n4. What is the time efficiency of different methods? Some evaluations regarding this should be conducted."}, "questions": {"value": "Please see **Weaknesses**.\n\nOthers:\n\nDoes the proposed method in this paper support 3-character(or more) personalization? Currently, many existing methods do not limit the number of concepts when performing multi-concept personalization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2PiFCuK4C2", "forum": "iPiEllQhsn", "replyto": "iPiEllQhsn", "signatures": ["ICLR.cc/2026/Conference/Submission18750/Reviewer_UcZj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18750/Reviewer_UcZj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537021511, "cdate": 1761537021511, "tmdate": 1762928468138, "mdate": 1762928468138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InstantCharacter, a novel and scalable framework for character customization built upon a foundation Diffusion Transformer (DiT). This work effectively addresses the critical gap left by previous U-Net and optimization-based methods, which suffered from limited generalization and compromised textual controllability. The core technical contributions include a scalable dual-adapter architecture for injecting character-specific features and enhancing multi-subject layout control , complemented by an effective three-stage progressive training strategy. Furthermore, the authors construct a large-scale (10-million-level) character dataset for training the framework. Experimental results demonstrate superior performance in generating high-fidelity, text-controllable, and character-consistent images across diverse appearances and styles."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The work is the first to develop a DiT-based framework specifically optimized for character customization, introducing a novel dual-adapter design (Image Adapter and Text Adapter) that seamlessly interacts with the DiT's latent space to maintain high-fidelity results.\n* The proposed three-stage progressive training strategy is highly effective in accommodating the heterogeneous 10M dataset, successfully decoupling the training for character consistency, textual controllability, and image fidelity.\n* The comparative experiments demonstrate the method's superior capabilities in consistently preserving character identity and high facial fidelity while maintaining precise text controllability, showing excellent potential for real-world applications.\n* Introducing the new Character350 evaluation benchmark."}, "weaknesses": {"value": "The paper repeatedly emphasizes the “scalability” of its framework, yet only briefly mentions its Transformer-based adapter design. \nHowever, the paper lacks a rigorous technical argument or experimental evidence to convincingly justify why this DiT-based dual-adapter approach holds a tangible advantage over U-Net-based adapters or other micro-tuning techniques specifically when scaling to significantly larger DiT models. This central claim requires more thorough substantiation"}, "questions": {"value": "*  The 10-million-level dataset is a fundamental component of the model's success. It is highly recommended that the authors provide a more detailed and comprehensive explanation in the main text or supplementary material regarding the dataset's construction, cleaning, and filtering standards, as this information is crucial for reproducibility and understanding the model's performance.\n* Please unify the formatting of the first column across all tables in the paper (e.g., consistently bold or consistently non-bold) for improved visual consistency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iDwUxQQ3Tm", "forum": "iPiEllQhsn", "replyto": "iPiEllQhsn", "signatures": ["ICLR.cc/2026/Conference/Submission18750/Reviewer_yUPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18750/Reviewer_yUPG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911117049, "cdate": 1761911117049, "tmdate": 1762928467324, "mdate": 1762928467324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"InstantCharacter,\" a framework for character customization built on the FLUX.1 diffusion transformer (DiT) backbone. It introduces a dual-adapter architecture to address the limitations of prior U-Net and tuning-based methods. An Image Adapter injects multi-level character features (from SigLIP and DINOv2) into the DiT's image tokens to ensure character consistency. Concurrently, a Text Adapter injects character features into the text tokens, which is claimed to improve layout control, especially for multi-character scenarios. The method is trained on a massive 10-million sample dataset using a progressive three-stage strategy to balance consistency, controllability, and image fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant Engineering Effort: The authors demonstrate a substantial engineering effort, including the curation of a massive 10M-sample dataset, the implementation of a complex multi-stage/multi-resolution training pipeline, and a novel synthetic data-generation loop for multi-character images.\n\n1. Thoughtful Architecture for DiT: The design of the Image Adapter, which uses stacked transformers to process multi-level features (low-level, region-level, and semantic) from multiple encoders (SigLIP + DINOv2), is a thoughtful approach to capturing a robust character representation suitable for the DiT.\n\n1. Multi-Character Handling: The explicit inclusion of a Text Adapter to manage multi-character generation is a valuable design choice. This design choice directly addresses a common failure point in personalization models."}, "weaknesses": {"value": "1. Outdated Premise and Inaccurate Framing: The paper's primary motivation, posing itself as a superior alternative to U-Net-based adapters, is largely outdated. The SOTA research frontier has decisively shifted to DiT-based methods for some time. This inaccurate framing extends to its claims of being the \"first DiT-based framework\", which is factually contradicted by the paper's own citations and comparisons to other concurrent DiT-based methods .\n\n2. Incremental Contribution: Viewed in the correct context (as one of many DiT-adapter methods), the methodological novelty is limited. The dual-adapter approach (Image + Text adapters) is a logical, but not highly innovative, recombination of existing concepts (e.g., IP-Adapter's cross-attention injection, PhotoMaker's fused embeddings) applied to a DiT.\n\n3. Critically Incomplete Baseline Comparisons: The comparisons do not reflect the true SOTA and are missing the actual competitors.\n\n- It omits the dominant U-Net/SDXL-based SOTA methods that set the community benchmark, namely InstantID and PhotoMaker. A SOTA claim is impossible without comparing to them.\n\n- More importantly, it fails to compare against or even acknowledge other advanced DiT-native personalization methods (e.g., FLUX-Kontext), which represent the true state-of-the-art for this backbone. This makes the paper's performance unevaluated against its true peers.\n\n4. Incomplete and Ambiguous Ablation Study: The paper's core contribution is its \"dual-adapter\" architecture, but the ablation study fails to scientifically validate this specific design choice.\n\n- The Text Adapter's contribution is unproven. The paper claims the Text Adapter is crucial for multi-character layout and separation . To prove this, an ablation study w/o Text Adapter should have been run. \n\n5. Unsatisfactory Qualitative Results & Model Bias: Despite claims of high fidelity, the qualitative results in the appendix (Figure S9) reveal significant failures in ID consistency. The model shows a strong stylistic bias. For example, when given 2D cartoon characters (e.g., columns 1 and 3) and the prompt \"a {character} wearing sunglasses, rain\", the generated outputs are rendered as 3D realistic characters, and the clothing is noticeably changed. This demonstrates a failure to preserve the core style and details of the reference character, undermining the paper's central claims.\n\n6. Dependence on Proprietary Data: The method's performance is inextricably linked to a massive, 10-million-sample proprietary dataset. This makes the results non-reproducible and makes it impossible to disentangle the contribution of the architecture from the contribution of the data."}, "questions": {"value": "Please refer to Weakness section. I will consider raise my score if all my concerns are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1ttTYjVZvl", "forum": "iPiEllQhsn", "replyto": "iPiEllQhsn", "signatures": ["ICLR.cc/2026/Conference/Submission18750/Reviewer_3n5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18750/Reviewer_3n5x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997747783, "cdate": 1761997747783, "tmdate": 1762928466483, "mdate": 1762928466483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}