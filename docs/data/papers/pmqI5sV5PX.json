{"id": "pmqI5sV5PX", "number": 23351, "cdate": 1758342476240, "mdate": 1759896819488, "content": {"title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models", "abstract": "Knowledge editing offers an efficient way to update model knowledge without full retraining, but prior work has concentrated almost exclusively on textual or visual modalities. We introduce SAKE, the first benchmark specifically designed for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory attributes, capturing knowledge types that go beyond conventional textual and visual domains. We benchmark seven editing methods on two LALMs along four dimensions: reliability, generality, audio/text locality, and portability. Results highlight challenges such as preserving intra-attribute knowledge unrelated to the edit, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates. SAKE provides a principled framework to study how knowledge editing extends to the auditory modalities, opening new directions for maintaining and adapting LALMs in more diverse real-world scenarios.", "tldr": "The first benchmark comprehensively evaluates the knowledge editing of auditory attribute knowledge in large audio-language models.", "keywords": ["Large audio-language models", "auditory attribute", "knowledge editing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e094cced73190a504fd7eb23c1607770043912b5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SAKE, the first benchmark specifically designed to evaluate knowledge editing in Large Audio-Language Models (LALMs). The authors posit that editing auditory attribute knowledge (e.g., speaker emotion, animal sounds) presents unique challenges not found in text or vision, as these attributes are abstract, high-level perceptual concepts rather than discrete facts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Pioneering Problem Definition:** This is the first work to formally define and systematically benchmark the problem of editing auditory attribute knowledge in LALMs, moving beyond traditional text/vision fact editing.\n* **Rigorous Benchmark Framework:** The SAKE benchmark is methodologically strong. Its design, particularly the granular \"Audio Locality Type 2\" (intra-attribute) and \"Portability\" (reasoning propagation) metrics, astutely captures the unique, abstract challenges of the auditory modality.\n* **Significant and Actionable Findings:** The paper delivers clear, impactful results. The discovery of \"intra-attribute entanglement\" (e.g., editing \"dog\" breaks \"cat\") and the documented failure of IKE methods for LALMs are crucial findings that highlight specific, unsolved challenges for the field.\n* **High-Quality Presentation:** The paper is exceptionally clear. Figure 1, in particular, serves as an excellent visual abstract that effectively communicates the benchmark's complex design and evaluation dimensions."}, "weaknesses": {"value": "* **Mismatch in Attribute Scope:** The paper's motivation hinges on editing \"abstract and continuous\" auditory concepts, yet the benchmark's attributes (e.g., animal sound, language) are predominantly evaluated as discrete classification labels. This under-delivers on the core premise, as the challenges of editing truly continuous attributes (like pitch or prosody) remain unexplored.\n* **Superficial Analysis of IKE Failure:** The paper reports the stark failure of In-Context Editing (IKE) but attributes it to a generic \"limited in-context learning ability.\" This analysis is shallow; it fails to investigate the specific failure mechanism, such as whether the LALM struggles to process in-context audio or fails to apply textual instructions to its auditory processing.\n* **Limited Model Diversity:** Key findings, such as \"intra-attribute entanglement,\" are derived from only two LALMs. While acknowledged as a limitation, this narrow scope makes it difficult to ascertain if these significant challenges are fundamental to LALMs or are artifacts of the specific architectures tested."}, "questions": {"value": "1.  **IKE Failure Mechanism:** Given the failure of In-Context Editing, is this due to the LALM's inability to process in-context *audio examples*, or a more general failure to apply *textual instructions* to its auditory processing? A test using only text-based instructions could isolate the precise point of failure.\n2.  **Intra-Attribute Entanglement Mechanism:** What is the hypothesized cause for the severe \"Audio Locality Type 2\" failure (e.g., editing \"dog\" breaks \"cat\")? Is it (a) **acoustic entanglement**, where representations are similar in the audio encoder, or (b) **semantic entanglement**, where the LLM backbone co-locates these concepts?\n3.  **Scope of Portability Failure:** The portability test (animal sound $\\rightarrow$ diet) fails. How wide is this reasoning disconnect? For the \"frog\" $\\rightarrow$ \"dog\" edit, do other related concepts like \"habitat\" (pond $\\rightarrow$ house) or \"classification\" (amphibian $\\rightarrow$ mammal) also fail to update, or is the failure isolated to the tested attribute?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WZQm2l7tro", "forum": "pmqI5sV5PX", "replyto": "pmqI5sV5PX", "signatures": ["ICLR.cc/2026/Conference/Submission23351/Reviewer_c6Zy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23351/Reviewer_c6Zy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967417054, "cdate": 1761967417054, "tmdate": 1762942622956, "mdate": 1762942622956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAKE, the first benchmark designed to evaluate knowledge editing in Large Audio-Language Models. SAKE targets auditory attribute knowledge, such as speaker gender, emotion, spoken language, and animal sounds.\nThe benchmark evaluates seven editing methods across four dimensions: Reliability, Generality, Locality, Portability\nExperiments were conducted on two strong LALMs: DeSTA2.5-Audio and Qwen2-Audio.\nThe results show that while existing editing methods can successfully change specific auditory knowledge, they struggle to generalize, maintain unrelated knowledge, and support multiple sequential edits.\nThe paper concludes that new methods are needed to handle abstract, perceptual auditory knowledge more robustly"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- First benchmark focused on auditory attribute knowledge editing, extending a well-studied concept from text and vision into the audio domain.\n- Significant for maintaining and updating multimodal model knowledge efficiently.\n- Results are well-analyzed, identifying causes of poor generality and locality in existing methods."}, "weaknesses": {"value": "- Only 2 LALMs are evaluated. \n- Only 4 attributes are covered. Can other auditory attributes like environmental sound types, etc. be considered for a stronger benchmark?"}, "questions": {"value": "- Are the paraphrased text human-checked for semantic consistency?\n- Can the authors show evaluation on recent LALMs like Audio Flamingo 2, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hXWLFMLSzS", "forum": "pmqI5sV5PX", "replyto": "pmqI5sV5PX", "signatures": ["ICLR.cc/2026/Conference/Submission23351/Reviewer_gqjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23351/Reviewer_gqjC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979228695, "cdate": 1761979228695, "tmdate": 1762942622443, "mdate": 1762942622443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAKE, the first comprehensive evaluation benchmark specifically designed for auditory attribute knowledge editing in Large Audio-Language Models. \nThe SAKE benchmark evaluates editing capabilities across four core auditory attributes (speaker gender, emotion, language, and animal sounds) along four key dimensions: reliability, generality, locality, and portability. The authors conducted experiments on two leading LALMs (DeSTA2.5-Audio and Qwen2-Audio), evaluating seven common editing methods, including fine-tuning, Knowledge Editor, MEND, and In-Context Knowledge Editing in both single and sequential editing settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is very interesting and well-motivated.\n2.  The SAKE benchmark is designed with four critical dimensions: reliability, generality, locality, and portability. The benchmark has a potential impact on the following studies."}, "weaknesses": {"value": "1. The evaluated scope of knowledge editing methods is limited. While seven editing methods were evaluated, some SOTA editing methods can be considered, such as WISE, AlphaEdit, UltraEdit.\n2. The paper primarily focuses on evaluating the effects of editing (what works/doesn't work, and where). However, there's a relative lack of deeper mechanistic explanations for why certain methods are effective or ineffective in the auditory modality. For instance, which parameters or layers are most crucial for auditory attribute knowledge editing? What changes occur in the internal representations of the model? How is knowledge of different auditory attributes encoded and interlinked within the model? Incorporating interpretability analyses (e.g., feature attribution, probing tasks) to delve into the internal workings of knowledge editing on LALMs would provide more profound insights.\n3. The considered auditory attributes are limited to 4. This may be due to the speech modality itself, but a more solid analysis is needed.\n4. The edited performance can be related to the audio generation performance of the original model (without any editing). More baselines can be useful for discussion.\n5. The editing task is limited to audio understanding, while audio generation can be a  much more significant scenario."}, "questions": {"value": "Please see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "71C5ZuUhCr", "forum": "pmqI5sV5PX", "replyto": "pmqI5sV5PX", "signatures": ["ICLR.cc/2026/Conference/Submission23351/Reviewer_vcdK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23351/Reviewer_vcdK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985756646, "cdate": 1761985756646, "tmdate": 1762942622150, "mdate": 1762942622150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAKE, a benchmark for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). It targets four attributes, including speaker gender, speaker emotion, spoken language, and animal sounds, and evaluates seven editing methods on two LALMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is the first systematic benchmark for auditory attribute editing.\n2. It tests two competitive LALMs, multiple attributes, single vs. sequential editing, and a comparative suite of seven methods."}, "weaknesses": {"value": "1. Though the contribution is positioned primarily as a benchmark, the sample of edits provided is quite narrow and under specified. For example, when editing an attribute (e.g., changing “sad” → “angry”), the paper does not clearly define whether the edit is restricted to a specific sound instance or intended to generalize across all instances of the “sad” attribute. Without this scope clarity, it is difficult to interpret whether the model is simply mapping the one training instance or truly generalizing the attribute change.\n\n2. In the locality evaluation, the benchmark requires that unrelated knowledge (i.e., non-edited items) remains unaffected. However, some of the design choices undermine this. For instance, in Figure 4 the locality sample has the answer “sad,” which exactly matches the original (pre-edited) attribute. This raises a concern: if the “locality” example uses the same attribute value as the edited item, then a correct response could simply reflect propagation of the edit rather than demonstrating true preservation of unrelated knowledge. The benchmark therefore may not reliably distinguish between genuine locality preservation and unintentional overlap with the edited attribute."}, "questions": {"value": "Please refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Us3FOd4pD0", "forum": "pmqI5sV5PX", "replyto": "pmqI5sV5PX", "signatures": ["ICLR.cc/2026/Conference/Submission23351/Reviewer_J6YR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23351/Reviewer_J6YR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033538839, "cdate": 1762033538839, "tmdate": 1762942620556, "mdate": 1762942620556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}