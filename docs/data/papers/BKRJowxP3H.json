{"id": "BKRJowxP3H", "number": 1640, "cdate": 1756900416364, "mdate": 1759898197149, "content": {"title": "VeriWeb: Verifiable Long-Chain Web Benchmark for Agentic Information-Seeking", "abstract": "Recent advances have showcased the extraordinary capabilities of Large Language Model (LLM) agents in tackling web-based information-seeking tasks. However, existing efforts mainly focus on single-fact retrieval and rely on outcome-only verification, thereby limiting their scalability in realistic knowledge-intensive scenarios that involve long-horizon web tasks requiring large-scale retrieval and synthesis of information from diverse sources. In this work, we introduce VeriWeb, a novel verifiable long-chain web benchmark designed to facilitate the evaluation and development of web agents within realistic web environments. Our benchmark emphasizes two critical dimensions: (1) long-chain complexity, encompassing both breadth- and depth-oriented search tasks to assess how effectively web agents ensure comprehensive information coverage and consistent context tracking in multi-hop reasoning; and (2) subtask-level verifiability, where tasks are decomposed into a sequence of interdependent verifiable subtasks. This structure enables diverse exploration strategies within each subtask, while ensuring that each subtask-level answer remains unchanged and verifiable. The benchmark consists of 302 tasks across five real-world domains, each with a complete trajectory demonstration, annotated by human experts. Extensive experiments on VeriWeb using various agents powered by different foundation models reveal significant performance gaps in handling long-horizon web tasks, highlighting the need for more powerful agentic information-seeking capabilities", "tldr": "", "keywords": ["LLM Agents", "Datasets and Benchmarks", "Web Search"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d849877cd58c5ce8b35696cd53c4546a2956fb0d.pdf", "supplementary_material": "/attachment/6e207d764d84448443a052b69d56a0a48c5f3a61.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark for web agents, primarily focusing on information retrieval / deep research tasks. The authors claim that their benchmark is novel and introduces features not previously seen in other benchmarks. \n1. Long, multi-question information retrieval - Each sample spans multiple questions that test both breadth and depth of reasoning, requiring agents to perform multi-hop information retrieval while maintaining contextual coherence across web pages.\n2. Intermediate step verifiability - A large task broken into several, intermediate subtasks, each of which has verifiable, fixed ground truth answers.\n\nThe authors provide detailed analysis of benchmark’s data statistics and discuss the evaluation metrics used to prove the uniqueness and complexity of their benchmark. The authors use these metrics to evaluate several agentic frameworks and frontier models on their benchmark, showing generally low scores across metrics like Success Rate (SR) and Completion Rate (CR). They also provide a comparative analysis of how models and agent paradigms perform across different domains and across breadth vs depth-oriented tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed benchmark introduces long-horizon, multi-question challenge to deep research agents with verifiable subtasks, where the multi-question consistency feature is an important extension to existing benchmarks that largely focus on single-fact tasks or subjective report generation.\n\n2. Empirical result shows that the benchmark is difficult for  most advanced agentic systems and frontiner models, which provides a a potential new direction the research community can make measurable progress towards.\n\n3. The experiments and analysis are relative extensive, and covers a good representation of agent paradigms and frontier models to make sure that the observations of benchmark difficulty is not overfitting particular agent designs or models."}, "weaknesses": {"value": "1. The main evaluation metrics this paper use are based on LLM as a judge, but there is no study of how well the LLM judge matches human assessment of answer quality on this dataset. This is especially important since this dataset claims to expose reliable subtask-level assessment of agent quality. If the evaluation itself is not stable enough, it puts into question all of the evaluation results presented in this work. One potential alternative, for instance, taken by the GAIA benchmark, is to specify the output format with task instructions and use stricter evaluator functions like string match.\n\n2. One of the main contributions of this benchmark concerns subtask-level verification, which is important and useful for complex information retrieval tasks that is oft-neglected in many benchmarks. However, the paper fails to discuss this contribution in relation to previous work in the literature. For one, HotpotQA (which is cited by this paper) introduced supporting fact evaluation for reasoning, which is a proxy to checking the necessary reasoning steps are reached to arrive at the final answer. As a more recent example, the Agent Company [1] uses subtask rewards to evaluate agents on long-horizon agent tasks.\n\n3. Some claims in the paper are unsubstantiated. For instance, L360 states \"Search engine agents, constrained to passive retrieval, typically achieve the lowest success rates.\" But the agents that achieved the lowest success rates in Table 2 are Browser-Use Agents and Multi-agent Systems.\n\n[1] TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks. (https://arxiv.org/pdf/2412.14161)"}, "questions": {"value": "1. Why does the paper use Browser actions as the main metric for step / action efficiency? Just because this is how humans do it with a browser doesn't mean it's the most effective (as evidenced by the results) or efficient way for agents to do it.\n\n2. Some examples in Figure 4 have SR that's 0 < SR < 1, does this mean that SR is not a binary 0/1 metric, or is this averaged over several runs? What does it mean if a task is 50% successful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEnEmcePRI", "forum": "BKRJowxP3H", "replyto": "BKRJowxP3H", "signatures": ["ICLR.cc/2026/Conference/Submission1640/Reviewer_tn1J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1640/Reviewer_tn1J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943210019, "cdate": 1761943210019, "tmdate": 1762915844783, "mdate": 1762915844783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a benchmark that tests web-based language agents on complex, multi-hop information-seeking tasks. Each task comes from a human browsing session, where annotators solved real web problems and recorded the steps needed to find and verify facts. The dataset breaks these trajectories into verifiable subtasks. VeriWeb scores agents using three measures: Success Rate for full-task completion, Completion Rate for subtask accuracy, and Action Efficiency for how effectively they use the browser. The authors test several large language models and find that agents often locate some facts correctly but struggle to plan, search deeply, and keep results consistent across steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Tasks are long-chain and information-dense, combining multi-hop retrieval and synthesis with subtask-level verifiability.\n- The benchmark introduces several evaluation metrics, including task success rate, completion rate, and action efficiency.\n- Human-annotated trajectories provide empirically grounded task structures."}, "weaknesses": {"value": "- The benchmark’s subtask-level verifiability requires each sub-answer to be fixed and unambiguous, but real-world web tasks often involve context-dependent or time-sensitive information. This design choice may therefore underrepresent the uncertainty present in realistic settings.\n- The absence of a human performance baseline makes it hard to interpret how well current agents perform relative to human proficiency on the same tasks.\n- Evaluation only uses gpt-4o as the judge, with no analysis on potential LLM judge bias or comparison against human-annotated labels."}, "questions": {"value": "- Would it be possible to run a quantitative breakdown of failure modes across agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mmAHArFBAi", "forum": "BKRJowxP3H", "replyto": "BKRJowxP3H", "signatures": ["ICLR.cc/2026/Conference/Submission1640/Reviewer_Vcuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1640/Reviewer_Vcuj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979233396, "cdate": 1761979233396, "tmdate": 1762915838786, "mdate": 1762915838786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VeriWeb, a new benchmark for evaluating long-horizon, web-based information-seeking agents. VeriWeb focuses on long-chain complexity (requiring multi-hop reasoning and synthesis across diverse sources) and subtask-level verifiability (allowing fine-grained evaluation of intermediate steps). The dataset comprises 302 human-annotated tasks across five domains, each decomposed into verifiable subtasks. Experiments with multiple agents powered by different foundation models show low success rates, highlighting the difficulty of realistic web reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a novel benchmark emphasizing both long-chain reasoning and verifiable subtasks.\n\n- The dataset is diverse and human-annotated, covering five realistic domains.\n\n- The experimental evaluation is comprehensive, testing multiple agent paradigms and models.\n\n- The paper provides insightful analyses of action efficiency and task difficulty, helping identify weaknesses in current web agents."}, "weaknesses": {"value": "- Unclear which LLM generated task instructions and subtasks.\n\n- The reasonableness of subtask decomposition is not independently validated.\n\n- Details of the human demonstration process (e.g., annotator number, quality checks, or fairness) are limited.\n\n- Many tasks involve hundreds of steps, but efficiency guarantees or annotation consistency are not analyzed.\n\n- The LLM-as-a-Judge metric may not align with human evaluation; human verification would strengthen credibility.\n\n- Single-run experiments due to API costs limit statistical reliability.\n\n- No human baseline is reported to contextualize task difficulty.\n\n- Error analysis could be broader and more quantitative."}, "questions": {"value": "- Which model was used to generate and decompose the tasks?\n\n- How is the quality or coherence of subtask decomposition verified?\n\n- What measures ensure fairness and accuracy in human demonstrations?\n\n- How do the authors justify using LLM-as-a-Judge without human correlation studies?\n\n- Could results be re-evaluated with multiple runs perhaps with open-source models to report variance?\n\n- Is there a plan to report human performance per difficulty level?\n\n- Can the authors share the API cost estimates and efficiency trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IUuyB50VN9", "forum": "BKRJowxP3H", "replyto": "BKRJowxP3H", "signatures": ["ICLR.cc/2026/Conference/Submission1640/Reviewer_Q1hS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1640/Reviewer_Q1hS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991349814, "cdate": 1761991349814, "tmdate": 1762915838669, "mdate": 1762915838669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VeriWeb, a novel verifiable long-chain benchmark, intended to address critical limitations in existing web agent evaluation—specifically the overreliance on single-fact retrieval and outcome-only validation. VeriWeb comprises 302 human-annotated tasks across five real-world domains. The benchmark mandates long-chain complexity (requiring large-scale retrieval, multi-hop reasoning, and information synthesis) and incorporates subtask-level verifiability. Experiments confirm the benchmark’s difficulty, showing that state-of-the-art LLM-powered agents achieve consistently poor performance on these complex, long-horizon tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary contribution is the development of a benchmark that rigorously enforces two previously neglected dimensions: long-chain complexity (integrating breadth- and depth-oriented search) and subtask-level verifiability. This fine-grained decomposition is essential, providing an informative supervision signal and allowing for error localization, which outcome-only evaluation protocols fail to capture. The dataset, curated through a costly human-annotation process across diverse real-world domains, effectively serves its purpose by revealing significant performance gaps and underscoring current agent limitations in synthesis and complex retrieval."}, "weaknesses": {"value": "This is a paper proposing a new web agent benchmark. However, a new benchmark must clearly state the problem it solves and rigorously demonstrate why this problem and the evaluation method are important, with the analysis of failure cases being able to guide the direction of field development.\nThe problem this paper addresses is relatively clear, and the proposal of the dataset and its construction method also have value. However, there is no particularly detailed justification for why evaluation should be conducted through subtasks (and other agent evaluation papers have proposed similar evaluation methods and metrics). Furthermore, the analysis of failure cases lacks sufficient depth and does not offer unique insights.\nIf this paper merely defines a new benchmark data generation process (with missing details on the data synthesis process) and conducts a certain evaluation of existing model capabilities, then it has a contribution but is not an ICLR-level paper."}, "questions": {"value": "1. Details of Data Definition:\n- Automated Filtering: Batches of generated instructions first undergo automated filtering.\n- Multi-Round Model Validation: This is followed by a second, more rigorous validation stage involving multiple model evaluations.\n- Final Retention: Only tasks that pass all validation steps are retained as the final instructions.\nWhat are the specific details? What insights are gained?\n2. What are your next steps or ideas for addressing the failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CsDY2SwjTw", "forum": "BKRJowxP3H", "replyto": "BKRJowxP3H", "signatures": ["ICLR.cc/2026/Conference/Submission1640/Reviewer_76ET"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1640/Reviewer_76ET"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998153108, "cdate": 1761998153108, "tmdate": 1762915838530, "mdate": 1762915838530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}