{"id": "DztuuQCn13", "number": 1277, "cdate": 1756868748396, "mdate": 1759898217472, "content": {"title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved the performance of various tasks, but continue to suffer from visual hallucinations, a critical issue where generated responses contradict visual evidence. While Direct Preference Optimization (DPO) is widely used for alignment, its application to MLLMs often fails to capture fine-grained semantic differences and encourages shortcut learning. To address these challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard curriculum built upon our Semantic Curriculum Preference Pairs dataset, which provides fine-grained semantic contrasts sorted by difficulty. This curriculum is trained with a dynamic reference model and a novel symmetric, bidirectional objective to facilitate simultaneous learning from both textual and visual preferences. To our knowledge, SCPO is the first framework to unify semantics, symmetry, and curriculum for MLLMs alignment, effectively mitigating visual hallucinations. Extensive experiments on LLaVA models across various scales and versions validate that SCPO demonstrates superior performance compared to baseline models on multiple hallucination benchmarks, reducing the hallucination rate by up to 62.9%. Moreover, evaluations on generalized benchmarks show that SCPO improves factuality while preserving general capabilities, with its performance remaining stable across general vision-language benchmarks.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Visual Hallucination", "Preference Optimization", "Cross-modal Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed97ba390fa0e471fb9b2ea4360115facb6c3d4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address visual hallucinations in MLLMs, this paper introduces SCPO, a framework that unifies curriculum learning, fine-grained semantics, and a symmetric loss for model alignment. Its key contributions are a difficulty-ranked dataset of semantic preference pairs and a novel cirriculum training objective. The method is validated by dense experiments that SCPO can reduce hallucination rates compromising general performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this is a solid piece of work with the following strengths:\n\n1. The proposed easy-to-hard curriculum learning strategy is well-designed and highly targeted. It effectively addresses the model's difficulty in effectively learning from challenging samples that are too distant from its current policy model. The three-stage learning approach is logically sound and tackles a significant practical issue in DPO. Although the methodology is relatively straightforward (simply dividing the learning into three stages), in my view, this constitutes the most important contribution of this paper.\n\n2. The experimental section is quite comprehensive. It not only demonstrates the progressive improvement in mitigating model hallucinations across stages but also includes sufficient ablation studies. These experiments effectively validate the advantages and rationale behind the multi-stage learning approach."}, "weaknesses": {"value": "1. A key limitation is the lack of innovation in the SCPO loss. It seems to be a composite of existing techniques rather than offering a novel theoretical or methodological contribution to loss function design in hallucination mitigation.\n2. The \"easy-to-hard\" issue discussed by the authors is a very interesting topic. However, since difficulty is relative, introducing this issue inevitably raises the question of offline versus online evaluation. Although the authors employ various methods and strategies to categorize samples into easy, middle, and hard levels, determining these difficulty levels online—i.e., from the perspective of the model being optimized—would likely further enhance performance. Moreover, as the model’s capabilities improve after each of the three optimization stages, the criteria for difficulty should also evolve accordingly."}, "questions": {"value": "1. The evaluation on general capability benchmarks is not sufficiently comprehensive. Beyond LLaVA-Wild and MMBench-CN, results on additional general benchmarks are required to conclusively demonstrate that the method does not harm the model's general capabilities.\n\n2. The results in Table 6 and Figure 4 are somewhat puzzling. For instance, why does the hallucination rate increase (e.g., the \"chair\" metric performing worse than the original model) simply by changing the optimization order? The authors need to provide a deeper analysis and explanation for these counter-intuitive findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7yVgNjrPrM", "forum": "DztuuQCn13", "replyto": "DztuuQCn13", "signatures": ["ICLR.cc/2026/Conference/Submission1277/Reviewer_dk1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1277/Reviewer_dk1t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566853580, "cdate": 1761566853580, "tmdate": 1762915726283, "mdate": 1762915726283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of visual hallucination in MLLMs, where model outputs contradict the visual evidence. The authors propose Semantic Curriculum Preference Optimization (SCPO), a new framework for multimodal alignment that has three novelty:\n1) A Semantic Curriculum Preference Pairs (SCPP) dataset, providing fine-grained preference pairs with quantified difficulty scores.\n2) A symmetric, bidirectional optimization objective combining complementary textual and visual preference losses with a cross-modal symmetry loss  to ensure consistent grounding.\n3) A curriculum-based dynamic training strategy, where the model is aligned progressively from easy to hard examples, updating the reference model at each stage to avoid optimization instability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper addresses an important and concrete weakness of MLLMs, visual hallucination, and lays out the limitations of existing DPO-style alignment approaches clearly.\n2) The use of both symmetric optimization and curriculum learning feels natural and well motivated. The method avoids overfitting to either the language or vision side, and the easy-to-hard schedule with dynamic reference updates is well thought out.\n3) The experiments are thorough. The proposed method consistently improves hallucination-related metrics on all tested models. \n4) Good ablations and analysis. The ablations on each component clearly show what matters. \n5) The paper is clear, the equations are easy to follow, and the overall structure makes sense."}, "weaknesses": {"value": "1) The building blocks are not very new on their own. Semantic difficulty scoring, symmetric alignment, and curriculum learning have each been used in prior work. The novelty lies in how they are put together rather than in a single new idea.\n2) Limited validation of the dataset. It is unclear whether “hard” samples are genuinely harder for humans or just confusing for the base model.\n3) Narrow experimental scope. All experiments use the LLaVA family. It would be more convincing to show that SCPO also helps models with different architectures or training pipelines."}, "questions": {"value": "I can't access the files in the anonymous github repo, other than the readme file."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1JoaX6ODXj", "forum": "DztuuQCn13", "replyto": "DztuuQCn13", "signatures": ["ICLR.cc/2026/Conference/Submission1277/Reviewer_Cq3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1277/Reviewer_Cq3F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715962462, "cdate": 1761715962462, "tmdate": 1762915726135, "mdate": 1762915726135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SCPO, a preference-optimization framework for multimodal LLMs that couples (i) a semantics-aware curriculum built from a new Semantic Curriculum Preference Pairs (SCPP) dataset, (ii) a symmetric, bidirectional objective that aligns both visual and textual preferences, and (iii) iterative training with a dynamic reference model to stabilize DPO-style optimization across easy to hard stages. Experiments on LLaVA families show consistent hallucination reductions on datasets including AMBER, MMHal, and AMBER-Discriminative; the abstract claims “up to 62.9%” hallucination-rate reduction, with general capabilities preserved."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Good presentation**. The writing and presentation of contents are clear and easy to follow.\n- **Well-motivated objective:** The symmetric bidirectional loss directly combats shortcut learning by enforcing both positive and negative cross-modal constraints; the formulation is clearly presented. \n- **Stability via dynamic reference:** The staged π_ref reset neatly addresses off-policy saturation/vanishing-gradient issues in later stages and admits a cumulative-reward view.  \n- **Comprehensive evaluation suite** with both generative and discriminative checks and head-to-head against specialized anti-hallucination baselines."}, "weaknesses": {"value": "1. **Design and Details of Difficulty score.** What is supporting the design of the difficulty scores (H, sCLIP, dOT), i.e., why these factors can measure the difficulty for comprehending the images? Besides, please provide more details on the score calculation process. It's not enough for just \"using CLIP and DINOv2\".\n2. **No related work?** This paper **seems not have the section for related work**, which I believe is a big problem. Please add this section and discuss the related works in detail.\n3. **Clarification and comparison with related work**. There is a very related and similar work with this paper - SymMPO[1], which also constructs symmetric image-text pairs for mitigating MLLM hallucinations. However, there is no citation or comparison on the preference optimization design, nor incorporated in experimental comparison as baseline. So please discuss and consider the novelty issue of the design CCO/CSO in this paper and provide experiments.\n4. **New benchmarks in Experiments.** There still lack some newest benchmarks such as HallusionBench[2] and MMStar[3] in the main experiments, which will help identify the effectiveness in both generative and discriminative settings.\n\nReferences:\n\n[1] Liu et al., “Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization” In NeurIPS 2025.\n\n[2] Guan et al., \"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models\" In CVPR 2024.\n\n[3] Chen et al., \"Are we on the right way for evaluating large vision-language models?\" In NeurIPS 2024."}, "questions": {"value": "1. **λ scheduling.** Is λ in SCPO fixed or scheduled across curriculum stages (e.g., increase CSO weight on harder data)? Any evidence of over-regularization when λ is large? \n2. **Compute/throughput.** Please report training time, GPUs, batch sizes, and hyperparameters related.\n3. **Comparability across evaluation protocols.** MMHal uses GPT-4-based assessment, which can be sensitive to prompt/aggregation choices; please document exact configs to ensure reproducibility. \n4. **“Up to 62.9%” claim.** Please anchor this to a specific model/benchmark/metric cell in the main tables (not only the abstract) and report absolute baselines alongside relative deltas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MZJC7UzRP1", "forum": "DztuuQCn13", "replyto": "DztuuQCn13", "signatures": ["ICLR.cc/2026/Conference/Submission1277/Reviewer_r7er"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1277/Reviewer_r7er"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952661716, "cdate": 1761952661716, "tmdate": 1762915726030, "mdate": 1762915726030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}