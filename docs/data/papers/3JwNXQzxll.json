{"id": "3JwNXQzxll", "number": 18012, "cdate": 1758282910246, "mdate": 1759897139482, "content": {"title": "Advancing Spatiotemporal Representations in Spiking Neural Networks via Parametric Invertible Transformation", "abstract": "Spiking Neural Networks (SNNs) are regarded as energy-efficient neural architectures due to their event-driven, spike-based computation paradigm. However, existing SNNs suffer from two fundamental limitations: (1) the constrained representational space imposed by binary spike firing mechanisms, which restricts the network's capacity to encode complex spatiotemporal patterns, and (2) the ineffective design of surrogate gradient functions that leads to gradient mismatch issues and suboptimal learning dynamics. To address these challenges, we propose the Parametric Invertible Transformation (PIT), which operates in a conjugate manner with neuronal dynamics to achieve adaptive modulation and augmented spike representations simultaneously. Second, we design an auxiliary gradient correction term to mitigate the gradient mismatch issue and oscillation phenomena during training. Moreover, we introduce a theoretical framework for analyzing the spatiotemporal representation space of SNNs. Extensive experiments on both static and neuromorphic datasets demonstrate state-of-the-art performance with our proposed method. This approach lays the theoretical foundation for expanding the spatiotemporal representations of SNNs, offering a viable pathway for developing low-latency and high-performance neuromorphic processing systems in resource-constrained environments.", "tldr": "We propose a parametric invertible transformation to enhance the spatiotemporal representations of spiking neural networks.", "keywords": ["Spiking Neural Networks", "Spatiotemporal Representations", "Neuromorphic Computing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/980d60fd361feeb948f607f93bc42158772c29cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper builds on I-LIF–style neurons and instantiates firing as uniform integer quantization of membrane potential with a learnable per-channel step. Concretely, it applies a conjugate scaling scheme ($A^{-1}$ before rounding and $A$ after) so that $s_t^l=A_t^l,\\mathrm{clip}(\\lfloor (A_t^l)^{-1}u_t^l\\rceil,0,D)$ and $v_t^l=u_t^l-s_t^l$, alongside the standard charging equation. The scale $A_t^l$ is parameterized per channel, initialized by a channel-wise $3\\sigma$ rule, and trained end-to-end with a rectified surrogate-gradient (STE-like) update. For $T=1$ and small $D$ (e.g., $D=4$), the neuron reduces to a one-shot multi-level activation quantizer. Inference remains event-driven by folding $A$ into downstream weights. Experiments train the SNN directly rather than via ANN→SNN conversion, and the paper positions the approach as an engineering refinement that reconciles SNN charge–discharge–reset semantics with practical quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a coherent synthesis: framing firing as uniform integer quantization wrapped by conjugate scaling, with per-channel learnable steps and a pragmatic $3\\sigma$ initializer, plus a rectified surrogate gradient near quantization boundaries. In terms of quality, the components are principled and practical—easy to implement in standard toolchains, trainable end-to-end, and compatible with event-driven deployment."}, "weaknesses": {"value": "1. With temporal dynamics effectively disabled (e.g., $T=1$ so no carryover via $v_t$), Eqs. (9–11) collapse to a static per-layer mapping\n$a^{(l)} = Q_{A^{(l)},D}\\big(W^{(l)} a^{(l-1)}\\big)$,\nwhere $Q_{A,D}(z)=A\\,\\mathrm{clip}(\\mathrm{round}(z/A),0,D)$.\nThis is a per-channel uniform activation quantizer with a learnable step, i.e., a QAT-style layer; the model no longer exhibits SNN temporal dynamics. When $T\\times D = 1\\times 4$, this is exactly the one-shot 5-level quantizer $Q_{A,4}(z)=A,\\mathrm{clip}(\\mathrm{round}(z/A),0,4)$; with $T=1$ the temporal state is negligible, so the layer behaves like a QAT activation rather than a classical LIF gate. The paper does not clearly acknowledge this equivalence and risks overstating novelty relative to standard QAT (per-channel scale learning with STE).\n\n2. Missing baselines and fairness. For $T=1$ and $D=2/4$ in the experiments, the correct baseline is an ANN with uniform QAT using the same level count and per-channel scale learning (same rounding rule, same folding of scales). These head-to-head comparisons are missing, so it is unclear whether the gains come from SNN dynamics or simply from quantization.\n\n3. Temporal dynamics conflation. Although the scheme preserves charge–discharge–soft-reset, firing is no longer a spike-time thresholding mechanism but a multi-level amplitude code. Claims attributed to SNN dynamics should be isolated by sweeping $T$ and $\\lambda$. With $T=1$, Eq. (9) contributes little temporal effect; with $T>1$, residual carryover arises from $v_t = u_t - s_t$. Accuracy results should be reported."}, "questions": {"value": "1. Explicit equivalence: acknowledge that Eq. (10) is a uniform quantizer and provide the mapping $s_t^l = Q_{A_t^l,D}(u_t^l)$ with $Q_{A,D}(z)=A,\\mathrm{clip}(\\mathrm{round}(z/A),0,D)$. Clarify what is fundamentally new beyond scale learning plus STE.\n\n2. QAT baselines: add ANN+QAT baselines with identical level counts (e.g., 5 levels for $D=4$), per-channel scale learning, and the same rounding and folding; compare accuracy.\n\n3. Temporal isolation: sweep $T$ and report accuracy.\n\n4. Energy evidence and fair accounting. Since this reduces to quantization (e.g., $T\\times D=1\\times 2/4$), report experiments against an ANN+QAT baseline with the same levels/scales and matched energy budgets.\n\n---\n\nShen et al., “Are Conventional SNNs Really Efficient? A Perspective from Network Quantization,” CVPR 2024; https://arxiv.org/pdf/2311.10802  ￼"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NSl7HupifS", "forum": "3JwNXQzxll", "replyto": "3JwNXQzxll", "signatures": ["ICLR.cc/2026/Conference/Submission18012/Reviewer_KE4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18012/Reviewer_KE4i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761040150690, "cdate": 1761040150690, "tmdate": 1762927804619, "mdate": 1762927804619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Parametric Invertible Transformation (PIT) and provides a theoretical analysis of the spatiotemporal representation space and capacity of Spiking Neural Networks (SNNs). Experimental results demonstrate improved accuracy on several datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is well written and clearly organized.\n\n+ The proposed PIT effectively enhances information representation by transforming both membrane potentials and spikes in spiking neurons.\n\n+ The introduction of an auxiliary gradient correction term further facilitates SNN optimization.\n\n+ The paper provides comprehensive empirical comparisons with state-of-the-art models."}, "weaknesses": {"value": "- While energy efficiency is presented as the main motivation of this work, the validation in this aspect appears rather limited. Simply calculating the computational energy is not sufficient; memory access cost should also be considered. A more detailed analysis or discussion on this point is recommended.\n\n- Compared with E-SpikeFormer, the proposed method achieves only a 0.7% improvement under the same model size. It would be helpful to include a more in-depth comparison with E-SpikeFormer to better demonstrate the advantages and strengths of your approach."}, "questions": {"value": "See in Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bhKRfhmSzu", "forum": "3JwNXQzxll", "replyto": "3JwNXQzxll", "signatures": ["ICLR.cc/2026/Conference/Submission18012/Reviewer_rRf7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18012/Reviewer_rRf7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761380807980, "cdate": 1761380807980, "tmdate": 1762927804159, "mdate": 1762927804159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a innovative solution to the long-standing problem of limited representational power and training instability in Spiking Neural Networks (SNNs). Its core contribution is the Parametric Invertible Transformation (PIT), a conjugate transform applied before and after the neuron's firing process. The elegant design allows the model to learn with rich, ANN-like continuous representations during training, while a reparameterization trick recovers the efficient, event-driven spike-based computation at inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The novelty of the PIT concept. Instead of incrementally improving existing SNN components, it fundamentally rethinks how information flows and transforms within a neuron.\n2. Empirical support. The experimental results provide evidence for the method's effectiveness and generality with SOTA results on a wide range of standard datasets."}, "weaknesses": {"value": "1. Redefining the SNN Paradigm: By introducing continuous-valued transformations during training, the PIT method blurs the lines between traditional SNNs and ANNs with specialized quantization. This raises the question of whether we are still strictly in the SNN domain or creating a new class of SNN-inspired, highly efficient hybrid networks. The paper could benefit from a more explicit discussion of its philosophical positioning within the broader neural network landscape.\n2. Unaddressed Trade-off between Temporal Dynamics and Bit-Depth. The paper relies on the T×D metric to claim comparable latency with prior work, assuming that trading temporal steps (T) for higher bit-depth (D) is an equitable exchange. This assumption is could be a weakness as it overlooks a critical trade-off. By decomposing a D-bit spike into D sequential sub-steps within a single logical timestep (as detailed in Appendix A), the model's architecture fundamentally shifts from a truly sequential processor to a static one with enhanced precision. A model with high T and low D (e.g., T=4, D=1) can integrate information across distinct and potentially distant time steps, making it suitable for tasks with genuine temporal dependencies. In contrast, the authors' preferred configuration of low T and high D (e.g., T=1, D=4) largely sacrifices this temporal processing capability in favor of higher representational richness within an isolated moment. The paper fails to acknowledge or discuss this fundamental architectural trade-off. This limits the claimed generality of the method, as its effectiveness on truly temporal tasks (like video analysis or complex time-series forecasting) compared to traditional high-T SNNs is not established and remains questionable. Furthermore, the equivalence of latency under the T×D metric is an idealization that may not hold on real hardware, where micro-looping overhead for high D could introduce different latency characteristics compared to processing distinct high-T steps."}, "questions": {"value": "1. On the Fairness of the T×D Comparison: The paper uses T×D as a measure of equivalent inference latency, comparing their method (e.g., T=1, D=4) with prior works (e.g., T=4, D=1 or T=2, D=2).While T×D can be a proxy for total operations in some cases, different combinations of T and D can have different implications for actual hardware latency, memory access patterns, and the model's dynamic properties. For instance, a larger T implies longer sequential dependencies, while a larger D might require more complex computation within a single step. Could the authors discuss this trade-off in more detail? What are the limitations of this comparison metric?\n\n2. On the Sensitivity of the Rectified Surrogate Gradient: The rectified surrogate gradient proposed in Eq. (13) is an interesting design that aims to reduce training oscillations by penalizing inputs near the rounding boundary. The computation introduces a hyperparameter λ, which is set to 0.01 in Appendix B. How sensitive is the model's performance to the choice of λ? A sensitivity analysis on the value of λ would be valuable for demonstrating the robustness and practical utility of this gradient correction term.\n\n3. On the Distinction from Quantized Neural Networks (QNNs): In the low-latency regime of T=1 and D>1, the proposed model strongly resembles a Quantized Neural Network, where the integer spike value acts as a quantized activation and the neuron's dynamics effectively become a stateful quantization function. Could the authors elaborate on the fundamental distinctions between their approach (specifically in the T=1 case) and mainstream QNN research? While the inspiration from neuronal dynamics is clear, what are the key methodological or performance advantages of framing this problem within the SNN paradigm, as opposed to approaching it from a pure network quantization perspective? This clarification would help position the work more precisely."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0tL1saRAV", "forum": "3JwNXQzxll", "replyto": "3JwNXQzxll", "signatures": ["ICLR.cc/2026/Conference/Submission18012/Reviewer_p6Qe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18012/Reviewer_p6Qe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732231361, "cdate": 1761732231361, "tmdate": 1762927803809, "mdate": 1762927803809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Parametric Invertible Transformation (PIT), a linear transform integrated conjugately around the firing/quantization step of spiking neurons. The motivation is to enhance the representational capabilities of SNNs. A rectified surrogate gradient is introduced that penalizes proximity to rounding decision boundaries to mitigate oscillations. A theoretical section defines “representation space/capacity” and shows that PIT can expand the space by offering more degrees of freedom while preserving spike-driven inference. Experiments on static and neuromorphic datasets demonstrate that the method improves the performance across ResNet-like SNNs and spike-driven transformers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Good empirical performance. The reported multi-dataset and multi-architecture improvements demonstrate substantial gains under various scenarios.\n2. Ablation results in Appendix show the importance of each component.\n3. Theoretical energy consumption analysis show that PIT yields lower energy through firing-rate reduction."}, "weaknesses": {"value": "1. Expressivity claim is overstated. PIT is largely a reparameterization: the linear transformation is a per-channel rescaling of weights which does not change the function class. It is akin to learnable pre-quantization scaling or BN-like affine parameters rather than a fundamentally more expressive operator. Gains plausibly stem from better quantization alignment and training dynamics, not representational power. This aligns with the paper’s own capacity formula that is identical to the non-PIT case. It largely undercuts the title/abstract’s emphasis on “advancing spatiotemporal representations”.\n2. Time-varying linear transformation is not friendly for neuromorphic deployment. The transformation $A_t$ varies over time, meaning that per-t weight variants at inference is required. Standard neuromorphic hardware does not support such kind of operations, and it prevents the model from extending to more time steps.\n3. Positioning vs. prior works. PIT’s diagonal scaling and distribution-aware initialization feel close to learnable pre-quantization scaling (similar to Real Spike’s reparameterization). The difference is primarily the conjugate placement and the specific initialization method. A clearer differentiation for contributions is required."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rtnLHrSjnT", "forum": "3JwNXQzxll", "replyto": "3JwNXQzxll", "signatures": ["ICLR.cc/2026/Conference/Submission18012/Reviewer_CsiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18012/Reviewer_CsiP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935331424, "cdate": 1761935331424, "tmdate": 1762927803366, "mdate": 1762927803366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}