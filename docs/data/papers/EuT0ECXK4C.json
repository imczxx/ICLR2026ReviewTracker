{"id": "EuT0ECXK4C", "number": 11540, "cdate": 1758201156504, "mdate": 1759897568946, "content": {"title": "Automatic Robustness Stress Testing of LLMs  as Mathematical Problem Solvers", "abstract": "Large language models (LLMs) have achieved distinguished performance on various reasoning-intensive tasks. However, LLMs might still face the challenges of robustness issues and fail unexpectedly in some simple reasoning tasks. Previous works evaluate the LLM robustness with hand-crafted templates or a limited set of perturbation rules, indicating potential data contamination in pre-training or fine-tuning datasets. In this work, inspired by stress testing in software engineering, we propose a novel framework, Automatic Robustness Checker (AR-Checker), to generate mathematical problem variants that maintain the semantic meanings of the original one but might fail the LLMs. The AR-Checker framework generates mathematical problem variants through multi-round parallel streams of LLM-based rewriting and verification. Our framework can generate benchmark variants dynamically for each LLM, thus minimizing the risk of data contamination. \nExperiments on GSM8K and MATH-500 demonstrate the strong performance of AR-Checker on mathematical tasks. We also evaluate AR-Checker on benchmarks beyond mathematics, including MMLU, MMLU-Pro, and CommonsenseQA, where it also achieves strong performance, further proving the effectiveness of AR-Checker.", "tldr": "", "keywords": ["LLM", "robustness", "LLM weaknesses", "math reasoning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b726f58e34d719640d1fad5f3b1062089371f399.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AR-CHECKER, a framework to generate mathematical problem variants that maintain the semantic meanings of the original one but might fail the LLMs in solving it. The approach uses 3 LLM components: a rewriter, verifier, and target LLM. The framework uses multi-round parallel streams as well to explore the space of challenging variants. Empirically, GSM8K and MATH-500 demonstrate significant drops in performance when applying AR-CHECKER."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a novel framework for modifying traditional stress testing methods. The paper designed a novel method in changing the semantic meanings of the original one and shows that models fail after modifying the semantic meaning. The analysis is thorough, backing up the method. In addition, the method shows that accuracy decreases abruptly when applying AR-CHECKER, and is generalizable towards other domains beyond mathematical reasoning. This could potentially show data contamination and insufficient stress testing in benchmarks."}, "weaknesses": {"value": "1. I am slightly unconvinced with the manual annotation in table 3. While table 3 shows 98% precision, this is based on only 150 samples. More qualitative analysis should be done towards the accuracy, as the model is well known to generate errors in rephrasing. It would be great to discuss the F1 and recall with more detailed analysis (and with samples beyond 150). Given that the method is using GSM8K, it would be great to use verifiers such as Python to detect the correctness of the entire samples.\n\n2. How would this differ from GSM-Symbolic, where their core contribution is to convert GSM8K by changing the variables, rephrasing, and others? I don't quite understand how GSM-Symbolic's methodology showing contamination within GSM8K relates to how applying AR-CHECKER into GSM-Symbolic helps.\n\n3. What are the failure examples of the model generating variants? For example, when does AR-CHECKER fail to find challenging variants? The paper shows some models have lower TFR (e.g., 33.33% for Qwen2.5-7B-Instruct on GSM8K) but doesn't deeply analyze why.\n\n4. The paper reports point estimates without error bars or measures of variance, with temperature=1. Statistical analysis should be provided in this case.\n\n5. Multiple-choice questions such as those in MMLU should show the examples further. Do the answer choices change as well or just the question?"}, "questions": {"value": "1. Could you provide more information on the false negative rate of the verifier? What happens when the verifier incorrectly judges that meaning has changed?\n\n2. While multiple-choice options are applied, how does the robustness vary based on [1]?\n\n3. It would be great to clarify the core contribution of GSM-Symbolic (since they focused on changing the questions as well).\n\n4. Would it be possible to verify more samples for accuracy compared to the 100/150 samples of manual annotation?\n\n5. Are there examples showing how the rewrites evolve during the multi-round iteration process?\n\n6. AR-CHECKER is only applied to traditional question answering for evaluation, such as CoT. Does method such as Faithful CoT [2], CoMAT [3], or ToT [4] others mitigate the decrease in errors?\n\n[1] Gupta, Vipul, et al. \"Changing answer order can decrease mmlu accuracy.\" arXiv preprint arXiv:2406.19470 (2024).\n\n[2] Lyu, Qing, et al. \"Faithful chain-of-thought reasoning.\" The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023). 2023.\n\n[3] Leang, Joshua Ong Jun, Aryo Pradipta Gema, and Shay B. Cohen. \"Comat: Chain of mathematically annotated thought improves mathematical reasoning.\" arXiv preprint arXiv:2410.10336 (2024).\n\n[4] Yao, Shunyu, et al. \"Tree of thoughts: Deliberate problem solving with large language models.\" Advances in neural information processing systems 36 (2023): 11809-11822."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ksWX6rcrFX", "forum": "EuT0ECXK4C", "replyto": "EuT0ECXK4C", "signatures": ["ICLR.cc/2026/Conference/Submission11540/Reviewer_eNkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11540/Reviewer_eNkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761311069428, "cdate": 1761311069428, "tmdate": 1762922633964, "mdate": 1762922633964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose to evalaute the LLM robustness regarding mathmatical problems. To implement the evaluation, the authors design a framework consists of a rewriter (to paraphrase the math problem), a verifier (to checker wheter the initial core meaning is preserved), and a target LLM. Extensive experiments on various benchmarks reveal the vulnerability of existing LLMs on solving math problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-writting and easy to follow.\n- The paper systematically analyze the robustness of LLMs towards math problems.\n- The proposed framework can serve as a benchmark to conduct automatic robustness stress testing on LLMs. It is also a useful tool to measure the degree of training data contamination in existing pre-trained LLMs."}, "weaknesses": {"value": "- The contribution in this paper is somewhat trivial, as the LLM's vulnerability of math problems has been revealed by various existing studies [1,2]. Based on this observation, this paper only provides a simple and intuitive automatic evaluation framework. It seems directly to use PAIR jailbreak [3] into the MATH evaluation. \n\n\n- Lack in-depth analysis and useful inspiration. The presented experimental findings are not surprising and this work fails to provide any suggestion or experiments to mitigate this issue.\n\n\n[1] Abedin, Z.U., Qamar, S., Flek, L. and Karimi, A. Arithmattack: Evaluating robustness of llms to noisy context in math problem solving. In ACL Workshop, 2025.\\\n[2] Li, Q., Cui, L., Zhao, X., Kong, L. and Bi, W. Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. In ACL, 2024.\\\n[3] Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G.J. and Wong, E. Jailbreaking black box large language models in twenty queries. In IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)."}, "questions": {"value": "Listed in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lrnuj12PbE", "forum": "EuT0ECXK4C", "replyto": "EuT0ECXK4C", "signatures": ["ICLR.cc/2026/Conference/Submission11540/Reviewer_jDpR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11540/Reviewer_jDpR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920841710, "cdate": 1761920841710, "tmdate": 1762922633545, "mdate": 1762922633545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces `AR-Checker`, a novel framework for automatically evaluating the robustness of LLMs in mathematics and other disciplines. They propose the framework to address the limitations of static benchmarks which may suffer from data contamination using a dynamic, multi-stage approach inspired by software engineering stress tests.\n\nThe `AR-Checker` framework utilizes a rewriter LLM to generate diverse, semantically-equivalent variants of mathematical problems through multi-round parallel streams. A verifier LLM then assesses these variants to ensure the core meaning is preserved while confirming that the target LLM fails to produce the correct answer. \n\nThrough experiments conducted on the GSM8K and MATH500 benchmarks, the authors clain that `AR-Checker` effectively uncovers robustness issues across a range of LLMs, causing a significant drop in accuracy compared to the original datasets (sometimes going beyond 50%). The work also performs various analyses on the weaknesses and transferability of failure modes, while also verifying the performance of each component, justifying their design choices."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The core idea of automatically generating adversarial mathematical rephrasings, alongside rigorous verification is potentially valuable to automate detecting the presence of contamination.\n\n2. Each component is well-ablated, showing that the choice of both verifier and rewriter are sensible for the given setting.\n\n3. This work performs interesting analysis into the weaknesses, and transferability of failures between different targets, showing that some errors can be common between models."}, "weaknesses": {"value": "1. **Reproducibility (IMPORTANT)**: The authors release neither the data nor the code, and do not have a reproducibility statement included in the paper. As the primary contribution of this paper is a method of evaluation, I find it critical that the authors 1) release all generated datasets for each of the models, 2) release the code to replicate `AR-Checker`.\n\n 2. **Evaluation methodology (CRITICAL)**: The primary flaw in this paper is how the authors evaluate their results against the baseline benchmarks. In particular, the authors retry sampling until the target LLMs fail, and only consider a passing grade if the LLMs solve **all** instances. This is statistically **significantly different** to the standard GSM8K/MATH500 evaluation. In particular, if the model has a probability of $p=0.95$ to output the correct solution, if you use the full budget of $M=NK=15$, assuming no failures due to equivalence issues, the model has a probability of $1-p^{15}=54%$ chance to fail. This significant difference can overinflate the success of `AR-Checker` and must be thoroughly verified. This is likely the primary driving factor behind the results in Figure 3, unless the authors are able to isolate the effect to showcase otherwise. I outline experiments the authors **need to address** for me to consider raising my score in **Q1**. \n\n 3. **Data contamination**: This work features the GSM8K and MATH500 as its primary sources. These datasets contain relatively simple problems, which have been already shown to be significantly exposed to test-set contamination, making it unclear how much of the pipeline is affected due to lack of robustness and how much it is affected by resolving explicit contamination. \n\n 4. **Model hyperparameters**: \n    - The authors do not, but should, include the other relevant hyperparameters for running the experiments, especially the `max_tokens`.\n    - The authors sample each model with a temperature of 0. This stifles potential exploration of the solution space, and is **detrimental to reasoning models**. Given the authors used DeepSeek-R1-Distill-LLama-8B, this was specifically included in the instructions for the model:\n    ```Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.```\n    Given this, the effects can become significantly more prevalent when evaluating the model can only pass the benchmark if it succeeds in **all** instances. It might also explain why the authors underreport the results of DeepSeek-R1-Distill-LLama-8B on MATH500 [1].\n\n 5. **Model selection**: All models the authors consider in their paper are significantly outdated. In particular, Qwen 2.5 models are superseeded by Qwen3, GPT-4o by GPT-4.1 and GPT-5, DeepSeek-R1-distill-LLama-8B by DeepSeek-R1-0528-Qwen3-8B. Their evaluation features only a single, outdated reasoning model, evaluated with temperature 0 (which is a significant issue, as elaborated in **W4**). While the authors claim to evaluate against stronger targets in **Section 5**, they nevertheless feature only outdated non-reasoning models, which are 1) known to feature similar scalability issues as smaller, weaker ones and 2) are far worse than much smaller reasoning models.\n\n6. **Comparison to prior work**: Similar to point 2, the authors evaluate against GSM Plus and GSM-Symbolic, but measure the case where the model fails on all instances for `AR-Checker`, while for the other benchmarks, they seem to measure failure on a single instance.\n\n7. **Novelty**: While the paper's pipeline is sensibly designed and well-ablated, the core contribution is not particularly novel, compared to works like GSM Symbolic, with the exception that this work can be more easily automatable."}, "questions": {"value": "1. To address the severe concern in **W2** (and similarly all other experiments, comparing to existing work), the authors should run both the following experiments:\n    - Run the equivalent sample on the vanilla benchmark (related work benchmark), the same number of times as they prompted the `AR-Checker`, and record a success only if the target model succeeds **in all instances**.\n    - Take the final dataset and rerun the target model on them multiple times (preferrably at least 4 per sample). This is not a complete solution to eliminate the sample bias, but it helps reflect whether the identified error happened because of a fundamental weakness for the sample, or because of randomly stumbling in one instance.\n\n 2. Similarly, can you run the results from **Section 4.3** again but with rerunning the final generated problem, to account for the sampling bias?\n\n 3. The authors should consider adding [1] in their Related Work discussion, as it is a very relevant prior work, that shows the presence of data contamination of GSM8K through novel problems mirroring the original dataset.\n\n 4. Can the authors run their pipeline on a combination of the AIME 2024, 2025, and HMMT Feb 2025 competitions (potentially with other competitions featuring in MathArena [2] if they need a bigger sampling size), with stronger reasoning models being featured? This would demonstrate whether AR-Checker scales to significantly more capable models and datasets, and how big of a factor data contamination is.\n\n 5. Can the authors add the problems for each target, as well as the code for `AR-Checker` in an anonymous repository, or the supplementary material?\n\n 6. Does the choice of rewriter exhibit a certain bias towards itself as a target, making rephrasing easier/harder?\n\n 7. Given all the models used in the paper are relatively weak, it would be interesting to see if significantly better/worse rewriters are better/worse at exploiting the weaknesses of the targets.\n\n 8. In Figure 4, do the models exhibit the same error, or do they just fail due to the phrasing/difficulty of the problem?\n\n 9. What principles are generally most effective in inducing errors? \n\n 10. Please address all further concerns from the **Weaknesses** section.\n\n\n\n\n## Current recommendation\n\nI am assigning this paper a score of **2: Reject**. While the core contribution is sensibly argumented, and the ablation studies and analysis ideas are commendable, the paper seems to suffer from three main flaws - outdated datasets and models, potential sampling bias, and lack of reproducibility. I will only consider raising my score if the majority of these issues are addressed in the rebuttal.\n\n### References\n\n[1] Guo, Daya, et al. \"Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\" arXiv preprint arXiv:2501.12948 (2025).\n\n[2] Zhang, Hugh, et al. \"A careful examination of large language model performance on grade school arithmetic.\" Advances in Neural Information Processing Systems 37 (2024): 46819-46836.\n\n[3] Balunović, Mislav, et al. \"Matharena: Evaluating llms on uncontaminated math competitions.\" arXiv preprint arXiv:2505.23281 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ElYSAQUyVJ", "forum": "EuT0ECXK4C", "replyto": "EuT0ECXK4C", "signatures": ["ICLR.cc/2026/Conference/Submission11540/Reviewer_LRqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11540/Reviewer_LRqW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931140579, "cdate": 1761931140579, "tmdate": 1762922633125, "mdate": 1762922633125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AR-CHECKER, a novel automatic robustness evaluation framework for large language models (LLMs) framed as a stress-testing problem. Inspired by stress testing in software engineering, AR-CHECKER employs three interacting LLMs—a rewriter, verifier, and target model—to automatically generate semantically-preserving yet adversarial variants of mathematical problems. The system runs multiple parallel rewriting streams and multi-round iterative refinement, seeking variants that keep the original problem’s meaning while causing the target model to fail.\n\nExperiments on GSM8K and MATH-500 show that the generated adversarial variants significantly reduce accuracy, revealing substantial robustness gaps. Additional experiments on MMLU, MMLU-Pro, and CommonsenseQA further demonstrate the framework’s generality beyond math reasoning. Ablations show how the number of streams and iterations, rewriter/verifier model choices, and presence/absence of explicit rewriting principles affect test failure rates. Weakness analyses highlight common LLM failure modes such as breakdowns in multi-step reasoning and sensitivity to irrelevant context"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Comparisons show consistent and significant improvements in discovering robustness failures over prior benchmarks (e.g., +15–45% TFR gains over GSM-Plus and GSM-Symbolic).\n\n* Addresses a crucial gap in current LLM evaluation: robustness to semantically-equivalent reformulations.\n\n* Provides a scalable, automatic alternative to hand-curated robustness datasets, potentially enabling continuous evaluation pipelines for commercial and research models."}, "weaknesses": {"value": "* While the verifier achieves 98% precision, there is no quantitative estimate of false negatives (missed failures) or semantic drift beyond human-annotated subsets. The framework could over- or under-estimate robustness depending on verifier reliability.\n\n* The entire evaluation loop depends on LLM judgments (rewriter + verifier). Without human calibration across multiple datasets, evaluation biases from the chosen verifier model (Qwen2.5-32B) may persist.\n\n* The experiments on GSM8K and MATH-500 are convincing, but these datasets contain relatively short problems. It remains unclear whether AR-CHECKER scales to long, multi-step proofs or competition-level math (e.g., MATH-A or Putnam-Axiom)."}, "questions": {"value": "* Why 5 × 3 iterations is optimal or how rewriting diversity correlates with semantic fidelity?\n\n* Could AR-CHECKER be extended to code or logic reasoning tasks where exact functional equivalence can be verified automatically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uR9Ux77a4r", "forum": "EuT0ECXK4C", "replyto": "EuT0ECXK4C", "signatures": ["ICLR.cc/2026/Conference/Submission11540/Reviewer_eHR1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11540/Reviewer_eHR1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975571525, "cdate": 1761975571525, "tmdate": 1762922632758, "mdate": 1762922632758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel framework: Automatic Robustness Checker (ARCHECKER), inspired by stress testing in software engineering,  to generate mathematical problem variants that maintain the semantic meanings of the original one but might fail the LLMs. The framework can generate benchmark variants dynamically for each LLM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "i) Important direction and novel framework. Measuring and improving reliability is an important area of LLM research that this work caters to. Introduces a novel framework to generate variants, dynamically, that test robustness of individual LLMs, primarily in the domain of reasoning.      \nii) C.4 (appendix), highlights failure modes of different models.     \niii) Framework generalizes to MMLU, MMLU-pro, CommensenseQA ((for LLama and Gemma based SLMs).    \niv) As shown in Table 4, AR-CHECKER can more effectively identify robustness issues of LLMs than GSM Symbolic and GSM-Plus (for LLama and Gemma SLMs)."}, "weaknesses": {"value": "i) limited contribution. Prior work, GSM-Symbolic [1], has already shown on how adding perturbations to the questions can expose unreliability in LLM's performance. While the framework of variant generation is novel, the insights seems similar to what we have already learned from prior works.     \nii) Doesn't address relevant math/science benchmarks. Due to saturation of scores on GSM8K, MATH and MMLU, tougher/competition-level benchmarks like AIME, HMMT and GPQA have become more relevant for testing reasoning. This would also naturally require authors to test more diverse set of reasoning models.     \niii) limited evaluation of different math reasoning techniques in LLMs. The authors only address Chain-of-thought (language based) reasoning and do not evaluate program generation based techniques (listed in question section) for math reasoning in LLMs.     \n\nReference     \n[1] http://arxiv.org/pdf/2410.05229"}, "questions": {"value": "i) Can this method scale to currently relevant and more tougher math olympiad like benchmarks such as AIME, HMMT? Olympiad level problems perhaps would be difficult to perturb, hence showing progress in that direction could be beneficial for math reasoning domain.    \nii) Further analysis on reasoning models: How would reasoning models like DeepSeek-R1-Distill-Qwen-7B or DeepSeek-R1-Distill-Qwen-14B, built on top qwen base model and not llama, perform?  Given Qwen2.5-7B-Instruct has higher Racc than LLaMA-3-8B-Instruct, it would be interesting to get this data point. Testing more (latest iterations eg Qwen3) reasoning model, could potentially generate newer inights.    \niii) What about robustness of closed source models (perhaps the mini or the lite variants)?    \niv) Current methods only evaluates robustness of Chain-of-thought (language based) reasoning. Code generation methods such as POT [1], ToRA [2], SBSC [3] have actively been used to solve mathematical problems. How robust are these models when using these prompting techniques ?    \nv) Why not explore a reasoning model as a rewriter and verifier?   \n\nReference:    \n[1] http://arxiv.org/pdf/2410.05229   \n[2] https://arxiv.org/abs/2309.17452    \n[3] http://arxiv.org/pdf/2410.05229"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YQngBwdpMy", "forum": "EuT0ECXK4C", "replyto": "EuT0ECXK4C", "signatures": ["ICLR.cc/2026/Conference/Submission11540/Reviewer_aNea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11540/Reviewer_aNea"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11540/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070844511, "cdate": 1762070844511, "tmdate": 1762922632280, "mdate": 1762922632280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}