{"id": "lv9YuSccD1", "number": 16499, "cdate": 1758265200469, "mdate": 1759897237085, "content": {"title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments", "abstract": "Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content---an aspect crucial for effective judgment detection.\nInspired by this, we introduce \\textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \\textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.", "tldr": "", "keywords": ["Large Language Models", "Judgment Detection", "LLM-as-a-judge"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66277d55b409faf5129ce90274afb1abe62c8b09.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to J-Detector detect LLM-generated paper reviews. They use Qwen3-8B to extract LLM-enhanced features and spacy to extract linguistic features. These features are used to train LGBM, RandomForest and XGB to classify contents. Results show that the proposed method is effective, substantially surpassing baseline by a large margin. Ablations also validate the necessity of each feature used."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed task is well motivated and is of great importance.\n\n2. The proposed method seems effective and outperforms SLM/LLM-based method by a large margin. Experiment settings are mostly reasonable."}, "weaknesses": {"value": "1. Contribution 1 is suspiciously overclaimed since there are other judgement detection methods already. While I understand that the proposed methods leverage both scores and content to achieve a more effective detection process, the judgement detection task itself is not new.\n\n2. There seems to be a misalignment between the task setting and real practice. The proposed task is to classify a group of reviews G into 0 or 1. But in practice, a paper submission often receives a mix of human-generated and AI-generated reviews.\n\n 3. The motivation for choosing features introduced in the paper should be elaborated (i.e. the 7 bullet points in Feature Augmentation paragraph of sec5). Are they exhausted and comprehensive? Are there better features to be considered? Besides, these features generated by Qwen-3-8B are not at all verified.\n\n4. I think the term “i.i.d.” in line 136 is misused. I.I.D means drawing independently identically distributed samples from the same distribution, but since you have different content in a batch, the input to the model should be different. Therefore, my understanding is $review_i$  \\~ $P_{Model}(Y|X_i)$, which is not i.i.d. If you believe this is not a misuse, could you express the process in formula?\n5. For group-level aggregation in sec5, each sample is treated independently. Is it possible to gather cross-sample features first and train a classifier on top of it? Intuitively this should give more information for classification because all samples in the same groups must share some features in common.\n\n\n6. In Implementation Details of section 6.1, the authors should point out what are the subsections in appB. For instance, the Qwen-3-8B prompts for generating features should be referred right after where it’s mentioned. Also, training and testing splits are not specified which can be confusing.\n\n7. In appB.2, it’s better to summarize the datasets used in a table, with features listed (e.g. input, output, what aspects are annotated). Also, appB.3 is empty.\n\n8. There lacks a case study for the proposed method. The case study can also show readers what inputs/outputs are like, which are absent in the current version as well.\n\n\n9. While using ML classifiers is efficient, the efficiency of prompting Qwen3-8B for feature extraction is not discussed."}, "questions": {"value": "1. Missing citations: \n  - line 153: two models\n  - line 155: four datasets\n  - sec6.1: datasets and models. Also, why are models introduced in the dataset paragraph?\n\n\n\n2. Fig2, two subfigures have different orders of models. It’s better to align. Also, there’s an overlap of information. It’s better to compare 4 settings (single/multi + with/without cand)  for each model and integrate results into one plot. \n\n3. The setting of section4 is a bit unclear.\n    1) while the authors refer the detailed settings to sec6.1, it’s better to explain the settings in sec4 so that readers do not have to jump between sections.\n    2) How are candidates fed into the two SLMs, given that the context length is 512 for RoBERTa and 4096 for LongFormer? Are they fed in the same way as in J-Detector?\n\n4. The format of sec5 is a bit off. Indentation can be used to better indicate the structure and improve readability.\n\n5. How is the generalizability of J-Detector? This can be a bonus for the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YgqYVyzvyZ", "forum": "lv9YuSccD1", "replyto": "lv9YuSccD1", "signatures": ["ICLR.cc/2026/Conference/Submission16499/Reviewer_Q5X3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16499/Reviewer_Q5X3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695259808, "cdate": 1761695259808, "tmdate": 1762926593789, "mdate": 1762926593789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Frames the new task of judgment detection that distinguishes human vs. LLM judges using scores and candidate content, formalized for instance and group settings across pointwise, pairwise, and listwise judgments.\n- Introduces J-Detector, a lightweight and interpretable classifier that fuses judgment-intrinsic and judgment-candidate interaction features, with ablations and bias diagnostics.\n- Provides a benchmark spanning multiple datasets and judge models, and analyzes detectability factors such as group size, dimensionality, and rating scale; includes a peer-review application."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated problem with realistic scenarios where text is missing or minimal.\n- Interpretable feature design surfaces biases such as length, complexity, and confidence.\n- Comprehensive evaluation and JD-Bench with diverse judge models and tasks.\n- Practical peer-review co-detection application demonstrates utility beyond pure detection metrics."}, "weaknesses": {"value": "- Writing quality and organization are uneven, with inconsistent terminology and placement of key details.\n- Technical novelty is limited relative to prior feature-based detection approaches.\n- Missing analyses on OOD transfer, adversarially crafted judgments, and computational cost of LLM-enhanced features.\n- Potential overfitting to dataset-specific correlations and privacy tradeoffs from using candidate content are underexplored."}, "questions": {"value": "- How does performance hold up against judges explicitly prompted to mimic human score distributions or under post-processed scores designed to evade detection?\n- What is cross-domain transfer when training on one dataset and evaluating on a new domain and rubric without finetuning?\n- Why use simple summation for group-level aggregation rather than a learnable aggregator that weights confidence or consistency?\n- What is the computational overhead of LLM-enhanced features and how do cheaper models affect accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Un31eEshfT", "forum": "lv9YuSccD1", "replyto": "lv9YuSccD1", "signatures": ["ICLR.cc/2026/Conference/Submission16499/Reviewer_j9Dq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16499/Reviewer_j9Dq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896318044, "cdate": 1761896318044, "tmdate": 1762926593346, "mdate": 1762926593346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical gap in the \"LLM-as-a-Judge\" paradigm by formally defining the \"Judgment Detection\" task—distinguishing between human-generated and LLM-generated evaluation scores (e.g., academic review scores, content quality ratings) using only two inputs: the \"candidate content\" (e.g., a paper, a response) and the \"evaluation score\" (no text feedback required)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is the first work to formalize \"Judgment Detection\"—a task that addresses a practical pain point (unreliable LLM evaluations in batch scenarios) and fills a gap left by existing detection methods.\n2. JD-Bench provides a unified benchmark for future judgment detection research, reducing duplication of effort. The work also offers actionable insights (e.g., factors affecting detectability: group size, evaluation dimensions) that guide subsequent studies."}, "weaknesses": {"value": "1. The paper asserts it is the \"first to propose judgment detection\", but this is essentially a scenario migration of the long-existing need to \"distinguish machine vs. human ratings\". Fields like automated essay scoring (AES) and fake rating detection in recommendation systems have long explored \"distinguishing machines from humans based on the correlation between ratings and content\", which the paper fails to adequately acknowledge. Additionally, the task relies on the strong assumption of \"no textual feedback\", which is rare in real scenarios (e.g., academic reviews or product evaluations usually include fragmented text). This setting seems more \"innovation-for-innovation’s-sake\" than a solution to real pain points.\n2. No innovativeness in factor analysis and applications: Conclusions like \"better detectability with larger group sizes or more rating dimensions\" are common sense in statistical learning, requiring no complex experiments to verify. The integration of \"J-Detector + text detector\" in academic review scenarios is merely basic result concatenation (without weighted fusion or conflict resolution), a approach already adopted in similar studies.\n3. The baselines only include SLM-based (RoBERTa/Longformer) and LLM-based methods, deliberately avoiding strong relevant methods from AES or recommendation systems—exaggerating J-Detector’s advantages. The LLM-generated ratings in the JD-Bench dataset rely on fixed prompts, which may introduce \"prompt-induced bias\" (rather than inherent patterns of LLM judgments), undermining the credibility of detection results.\n4. The authors mention that LLM evaluations can be manipulated via malicious prompts (e.g., hidden text) but do not test J-Detector’s performance on adversarially modified candidate content (e.g., content designed to mimic human-like evaluation triggers). This limits understanding of J-Detector’s resilience in high-stakes scenarios."}, "questions": {"value": "1. Would J-Detector maintain its performance when evaluated on specialized domain LLMs (e.g., CodeLlama for code evaluation, BioGPT for medical content)? \n2. How does J-Detector perform on adversarially manipulated candidate content (e.g., content with hidden white text to trick LLM evaluators into high scores)? Could adding adversarial training (using such content) further improve its robustness?\n3. The paper uses light-weight classifiers (RandomForest, XGB) for efficiency—have you tested larger models on J-Detector’s feature set?\nFor the JD-Bench dataset, what was the inter-annotator agreement (e.g., Cohen’s kappa) for human-generated scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jwzGRWqvoH", "forum": "lv9YuSccD1", "replyto": "lv9YuSccD1", "signatures": ["ICLR.cc/2026/Conference/Submission16499/Reviewer_3DRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16499/Reviewer_3DRS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900830164, "cdate": 1761900830164, "tmdate": 1762926592954, "mdate": 1762926592954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of detecting whether judgments are AI-generated by proposing a simple yet effective approach. Their method integrates multiple types of features, including given judgment scores, LLM-enhanced features, and linguistic features, and employs several traditional machine learning models (LGBM, RandomForest, and XGB). The approach outperforms existing SLM-based and LLM-based models across four datasets. The authors further provide ablation studies and analyses to verify the effectiveness of their method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe task of determining whether an evaluation is human- or AI-generated is crucial for research credibility, particularly in peer-review settings. This is a timely and important topic.\n2.\tThe proposed approach is straightforward and practical, and the experimental results on the JD-bench dataset (composed of multiple existing sources) demonstrate its effectiveness.\n3.\tThe paper includes extensive analyses that confirm the consistency of their findings with prior conclusions."}, "weaknesses": {"value": "I do not question the effectiveness of the proposed model itself. My main concerns lie in practical applicability, clarity of methodological description, and interpretation of results.\n\n1.\tAlthough identifying whether a review comment is AI-generated is valuable in research, it remains difficult to apply this approach in real scenarios. In practice, the key challenge is distinguishing between AI-assisted (e.g., AI-polished) and AI-generated reviews. Even if a system flags a review as AI-generated, reviewers can still claim it was merely AI-edited, a practice currently accepted by the community.\n2.\tThe methodological details are somewhat unclear. How exactly are the features from the different sources integrated into the input of these traditional ML models? What LLM was used, and how were its features extracted? In natural-language form or as embeddings? Figure 4 shows ablations on each feature type, but the pipeline remains vague.\n3.\tThe experimental results are not well-explained. In Table 3, the datasets appear to have different formats. Were all datasets trained jointly in a single model, or was a separate model trained for each dataset? For example, SLM performs very well on H2 and NIPS but poorly on H3 and ANTIQUE (The proposed method also has a slight degradation). The authors attribute this to SLMs relying on inter-dimension patterns and failing to link judgments with candidates when such distributional cues are absent. But it is the difference between datasets instead of the methods. Also, were the baseline SLMs trained within 4 datasets? \n4.\tAnother concern is that the paper emphasizes the importance of “candidate content” for distinguishing human vs. AI reviews, yet there seems to be little modeling effort or analysis explicitly targeting this aspect (unless the LLM-enhanced features are intended to serve that role). I think one promising direction is to check whether the generated content is fair, correct, or consistent with the paper content, whether it is generated by the AI or a human.\n5.\tRegarding Figure 7, the authors suggest that a lower detectability score indicates a more human-like output. But since prompt wording also affects the human-likeness of generated text, does this imply that prompt design can directly reduce detectability?\n6.\tSection 7.2 presents an interesting experiment, but it is insufficiently explained. As written, it seems merely to extend the binary classification problem into a multi-class setting.\n7.\tThe formatting (citations, figure/table styles, underlines, etc.) has been adjusted. While not critical, it may indicate that other template-level modifications were made."}, "questions": {"value": "See the weakness above. If the authors can provide convincing explanations for the issues raised, I am open to revising my evaluation toward a positive score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4zK1gKomSB", "forum": "lv9YuSccD1", "replyto": "lv9YuSccD1", "signatures": ["ICLR.cc/2026/Conference/Submission16499/Reviewer_Z8K9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16499/Reviewer_Z8K9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985697870, "cdate": 1761985697870, "tmdate": 1762926592563, "mdate": 1762926592563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}