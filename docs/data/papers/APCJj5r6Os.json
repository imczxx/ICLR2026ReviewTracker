{"id": "APCJj5r6Os", "number": 22123, "cdate": 1758326410832, "mdate": 1763754322567, "content": {"title": "A Computationally Efficient Case-Control Sampling Framework for G-Formula with Longitudinal Data", "abstract": "Estimating the causal effect of time-varying treatments on survival outcomes in large observational studies is computationally demanding, particularly when outcomes are rare. The iterative conditional expectation (ICE) estimator within the g-formula framework is effective but becomes computationally burdensome when bootstrapping is used for variance estimation. Additionally, the rarity of outcomes at each time point can create extreme class imbalance, leading to instability and convergence issues in logistic regression and related models. To address these challenges, we propose a novel case-control enhanced g-formula approach, which integrates case-control sampling with ICE estimation. This approach significantly reduces computational burden while maintaining consistency and improving estimation stability. By strategically selecting informative subsets of data and applying appropriate reweighting, the approach mitigates class imbalance, substantially reduces computational cost, and preserves consistency and asymptotic efficiency. We evaluate the method through simulations and validate it using a large-scale EHR cohort study on social and behavioral determinants of health (SBDH) and suicide risk, demonstrating its effectiveness for modeling rare outcomes in longitudinal data.", "tldr": "We propose a case-control enhanced g-formula approach to efficiently estimate causal effects of time-varying treatments on rare survival outcomes.", "keywords": ["Causal inference", "time-varying treatment", "survival analysis", "rare outcomes", "case-control sampling"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/658ad4179f79223532a975e8f217d916171d109d.pdf", "supplementary_material": "/attachment/ee525227f38ff44fd084a12a64be34ef68b1b1d9.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a subsampling method for increasing the computational efficiency of G-computation for longitudinal causal inference. \nThe method relies on subsampling and upweighting the control group at different time steps to reduce the effective sample size while still enabling unbiased estimation. \nThe method is evaluated using both synthetic and real-world longitudinal data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method increases computational efficiency and retains unbiasedness of the G-formula estimator\n\n- The method is applicable in various application domains such as medicine"}, "weaknesses": {"value": "- I suspect that subsampling comes **at the cost of increasing estimation variance**, which the authors did not discuss in their paper. \nSubsampling implies that we essentially decrease our sample size. I believe it is crucial to analyze this and provide guidelines on how to choose.\n\n- The novelty is limited. Both G-computation and subsampling are **well-established methods** which the authors combine in their work.\n\n- I do not think that Eq. (1) and (2) are the correct identification formulas under censoring. For censoring, identification usually either follows via hazard functions or inverse censoring weights.\n\n- The empirical improvements are mostly in the 5-10 second range and do not seem practically relevant.\n\n- The writing and general presentation of the paper could be improved."}, "questions": {"value": "- Can the authors double check the correctness of the G-formula under censoring? \n\n- How does the proposed method perform in more computationally demanding settings (e.g., when using ML models instead of logistic regressions)? The empirical improvements are mostly in the 5-10 second range and do not seem practically relevant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vfkZkfluFj", "forum": "APCJj5r6Os", "replyto": "APCJj5r6Os", "signatures": ["ICLR.cc/2026/Conference/Submission22123/Reviewer_TBcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22123/Reviewer_TBcK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760526393498, "cdate": 1760526393498, "tmdate": 1762942074202, "mdate": 1762942074202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "U7kE67cZzb", "forum": "APCJj5r6Os", "replyto": "APCJj5r6Os", "signatures": ["ICLR.cc/2026/Conference/Submission22123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22123/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754321936, "cdate": 1763754321936, "tmdate": 1763754321936, "mdate": 1763754321936, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the computational burden of using bootstrapping for variance estimation with iterative conditional expectation, particularly in the context of rare outcomes, this paper proposes a case-control enhanced g-formula.\n\nThe proposed method has two key components. The first is the g-formula, which is commonly used in longitudinal settings to alleviate weight instability in causal inference. The second component is case-control sub-sampling. This strategy is analogous to up-sampling (or retaining) the \"case\" subjects and matching them with a sufficiently large set of \"control\" subjects with weighting. This enables more efficient parameter estimation while significantly reducing computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I appreciate the connection the authors make between the sub-sampling strategy and the weighted estimating equation, which allows for the straightforward incorporation of censoring in survival outcomes. The resulting estimator is shown to be consistent under reasonable assumptions.\n\nThe numerical experiments align with theoretical expectations:\n\n* The case-control enhanced g-formula is shown to be less time-consuming, particularly when a complex ensemble model like Super Learner is used (as shown in Table A2).\n* The estimator produced by the case-control sampling method yields results that are similar to those from the complete-data scenario. It is also noted that higher sampling ratios lead to estimated standard errors that more closely approximate the complete-data scenario.\n\nIn addition, it is promising to see that the case-control estimator achieves results comparable to the complete-data approach while requiring significantly less computation time."}, "weaknesses": {"value": "* Since the case-control sub-sampling strategy may also address the weight instability by drawing informative subsets of the control data, why combining g-formula with the case-control sampling strategy, instead of MSM or SNM with the case-control sampling? Since the g-formula requires the correct model specification of all sub-models, it seems quite restrictive compared to MSM/SNM which offers the double robustness (I.e., correct either treatment or outcome model) and semi-parametric efficiency.\n\n* Based on the box plots of risks in Figure 1, the case-control estimator seems to have similar empirical standard errors as the complete-data scenario. As the estimated standard errors are larger for smaller sampling ratios, I would expect some over-coverage issue in these cases, which might affect the power of the estimator. I suspect the case-control sampling strategy can be used in conjunction with any other estimating equation (besides the g-formula). Did author consider some more efficient and doubly robust estimation equations as candidates?"}, "questions": {"value": "* What is the coverage of the estimator across the 100 simulated datasets? The results only show the estimated standard error from bootstrapping.\n\n* What models were used for the real-data application? My guess would be a SuperLearner, but the running time of approximately 25 secs reported in Table 4 seems very fast for such a model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8kVHHy7uFw", "forum": "APCJj5r6Os", "replyto": "APCJj5r6Os", "signatures": ["ICLR.cc/2026/Conference/Submission22123/Reviewer_NEeR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22123/Reviewer_NEeR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848822913, "cdate": 1761848822913, "tmdate": 1762942073680, "mdate": 1762942073680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of estimating a time-varying treatment effect on survival data with rare events. The desired quantity can be estimated using the \"g-formula\" via two existing methods, ICE and NICE. They identify two problems with these existing methods. Namely, when the methods are fit using a large dataset, the computational cost is high, especially since there are no closed-form expressions for the estimator variance this variance is generally approximated via bootstrap sampling, further exacerbating the computational cost. Second, when events are rare, there is a large class imbalance between failure/non-failure at each time point, which can lead to slow convergence for model fitting. They propose a subsampling/importance reweighting approach to address both of these issues. The subsampling to a smaller dataset reduces the computational cost, while the importance reweighting means that a comparable number of failure/non-failure datapoints can be sampled without introducing bias. They confirm that their approach gives similar output to the original method fit to the whole dataset on both real and synthetic data."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The subsampling approach introduced by the authors does in fact obtain a large speedup over fitting to the whole dataset. Based on their experiments, it seems that the output of their subsampled method is consistent with the full dataset method."}, "weaknesses": {"value": "I found the paper difficult to follow. Some formulas are not clearly stated, or quantities not explicitly defined. For instance, one can infer the definition of $f\\_{\\bar{L}\\_j, \\bar{A}\\_{j-1}, C\\_j, Y\\_j}(\\bar{l}\\_j, \\bar{a}^g\\_{j-1},0,0)$ from context, but especially given the intricacy of the formulas it would help to state it explicitly. Another instance which impedes understanding of the paper/results is equation (2). I am not actually sure how to fill in the ... in the formula. Perhaps these quantities are clear to a reader who is familiar with the g-formula, but it was not obvious to me.\n\nAdding to this difficulty is the fact that some critical pieces of the paper are relegated to appendices. For example, the definition of the main comparison algorithm (Algorithm A1) is in an appendix, though its description is referred to extensively. For the main runtime reduction results on the synthetic data, one must compare the results of the proposed algorithm (presented in seconds) in Table 2 with the results of the baseline algorithm (presented in minutes) in appendix Table A2.\n\nThe example starting on line 161 is also presented in a confusing manner. To begin with, it is not stated why we want to estimate the expected value of a covariate X, nor what the relationship is between X and the quantitites L, C, A, and Y introduced in the problem setup. The presentation of the weighting scheme could also use improvement, as it consists of several dense paragraphs with equations interspersed both inline and in standalone equations. More importantly, the final goal of the choice of weights, or even the point of the section, is not clear with respect to the main goal of the paper, i.e., to estimate the time-varying treatment effect. It is only at the *end* of the section that it is explained that the weighting scheme is just debiasing an expected gradient calculation used to fit the models at each time step (and the explanation of this also uses quite non-standard notation).\n\nTable 3 presents the bootstap means and standard error of the ICE and NICE estimates, but it is not clear how to interpret these results. Since we have the ground truth for the synthetic data, it would make more sense to present the error of each method. Or, if the main point is just that the proposed subsampling approach is a good approximation of the more computationally intensive complete-data method, then the error of these estimators to the full-data estimate should be provided.\n\nIn addition to the general problems with clarity, the novelty of the contribution itself is limited. After digging past the notation, the proposed method boils down to a standard class-imbalanced importance reweighting technique to subsample a large dataset without introducing bias to gradient calculations. The computational speedups obtained are purely from the smaller dataset size, and the ICE and NICE methods can be applied directly (with the standard importance weights) on the smaller dataset.\n \nTypos:\n1. The appendix starts with the filler text \"You may include additional sections here.\""}, "questions": {"value": "Does the consistency assumption mean that two individuals with the same treatment history necessarily have the same covariate sequence?  If so, doesn't this make the covariates $L_j$ redundant, as they can be recovered from the treatment sequence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IERFkg05Iy", "forum": "APCJj5r6Os", "replyto": "APCJj5r6Os", "signatures": ["ICLR.cc/2026/Conference/Submission22123/Reviewer_xhkF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22123/Reviewer_xhkF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950596268, "cdate": 1761950596268, "tmdate": 1762942073304, "mdate": 1762942073304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}