{"id": "oTh8kgWCUO", "number": 23564, "cdate": 1758345542671, "mdate": 1759896807562, "content": {"title": "How do Large Language Models Learn New Domain Knowledge?", "abstract": "Despite the success of pretraining, it's not clear how large language models (LLMs) learn complex domain knowledge. We design controlled experiments to measure how they learn a new domain in the setting of continued pretraining, building upon a base level of general knowledge and intelligence. We probe the target domain knowledge at two levels: factual and compositional. First, we show that while repeated exposure improves learning with diminishing returns, multiple views of the underlying knowledge (e.g. textbooks, blogs, and stack exchange questions) yield stronger learning and generalization. We postulate that multiple views, which formulate and communicate the same knowledge differently,  construct a *scaffolding* that is key to learning in LLMs, and we suggest that this scaffolding naturally occurs in pretraining corpora. Second, we examine how learning dynamics differ with model size, post-training, and the design of data replay. Third, we find that LLMs can possess only a partial understanding of the prior knowledge required for a domain, and bridging these gaps markedly increases learning.", "tldr": "", "keywords": ["large language models", "continued pre-training", "domain knowledge"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/269b8e98ad620afcee1b2446fc78212f2ca9f85b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study how the model acquires new knowledge in the continual pretraining process. They construct factual and compositional probes to evaluate model learning dynamics. The compositional probes is an interesting angle but the construction of the probe doesn't seems rigorous enough to support the claims in paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Understanding how model learns compositional knowledge during continued pretraining is interesting and important.\n\n* The study covers a wide ranges of model sizes.\n\n* the ablation study and variants of training corpus are comprehensive."}, "weaknesses": {"value": "* Claims about 49 paraphrasing reduce performance “excessive linguistic diversity may hinder factual recall” (Line 299-300), how does the author ensure the linguistic diversity? \n\n* The diversity of the probe is not established, so the claim “This demonstrates a genuine generalization of knowledge rather than surface-level pattern matching” (Line 322-323) is not very well supported. A trivial explanation: The probes might all look similar so the trends look similar.\n\n* Line 301 “paraphrasing enables robust knowledge acquisition over extended training”; how do the author measure “robust acquisition”?\n\n* ``Scaffolding'' is not defined but seems important for argument of the work (i.e. appear and highlighted in abstract)\n\n* Unclear arguments about data replay: Replay is meant to mitigate catastrophic forgetting, but learning target domain has nothing to do with catastrophic forgetting. I think the experiment results are expected. I am not sure why would training data from pretraining corpus would help learning new knowledge that doesn’t exist in pretraining corpus. I am lost about what the author want to prove."}, "questions": {"value": "* Line 187 does \"2,298 probes\" include both factual and compositional probes?\n\n* Is it correct that the compositional probe requires original + related sentence to answer?  How do the author make sure information “Related sentence” is not contained in original sentence? \n\n* What exactly does one batch of “new knowledge” contain? Does it contain all original sentence and related sentences for a \"new knowledge\"? For paragraphs, prior knowledge and auxiliary views, are each of the augmentation applied to both original sentence and related sentence?\n\n* Figure 3: why 1B with Priori knowledge has a sudden drop in mean log prob?\n\n* Unclear why post-training is required: If the evaluation is only log prob or generate from probe prefix, why is post-training needed for the experiments? Figure 6,7,8\n\n* Figure 10, it seems surprising that paraphrase only which has very little tokens counts (< 0.05K tokens?) are super effective. Does author have some explanation for this?\n\n* Could the author discuss the difference between this work and prior work [1, 2] on learning dynamics?\n\n[1] Pretrained Language Model Embryology: The Birth of ALBERT\n\n[2] Probing Across Time: What Does RoBERTa Know and When?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VMCWierOIw", "forum": "oTh8kgWCUO", "replyto": "oTh8kgWCUO", "signatures": ["ICLR.cc/2026/Conference/Submission23564/Reviewer_v3Kv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23564/Reviewer_v3Kv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631223329, "cdate": 1761631223329, "tmdate": 1762942713558, "mdate": 1762942713558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates **how LLMs acquire new domain knowledge via continued pretraining (CPT)** through a series of controlled experiments. Six recent AI papers are treated as novel “domains,” and the authors construct both **factual** and **compositional** probe sets to measure learning. They compare three training strategies: (1) source-only documents, (2) paraphrasing, and (3) **auxiliary views** (textbook, blog, and StackExchange–style materials).  \nKey findings include:  \n- Learning improves with repetition but saturates after ~100 exposures.  \n- Paraphrasing mitigates overfitting and enhances generalization.  \n- **Auxiliary views act as pedagogical scaffolding**, substantially improving compositional understanding.  \n- **Prior-knowledge pretraining** on prerequisite concepts boosts downstream domain learning.  \n- **Data replay** from the pretraining corpus consistently harms new knowledge acquisition.  \nExperiments are conducted on OLMo2 models (1B/7B/13B), with detailed ablations and insightful discussion on the “scaffolding hypothesis” for LLM learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-controlled methodology** with dual-level probes and systematic construction/validation for factual and compositional learning.  \n2. **Clear, interpretable insights** on paraphrasing, auxiliary views, and prior knowledge that could guide future CPT practice.  \n3. **Meaningful negative result:** excessive data replay hinders domain learning — a practically important observation.  \n4. **Thoughtful discussion** connecting findings to human learning theories (e.g., scaffolding, double descent), offering conceptual depth."}, "weaknesses": {"value": "1. **Limited scope and scale:** Only six CS papers and models up to 13B; unclear if findings generalize to other domains or frontier-scale models.  \n2. **Synthetic auxiliary data:** Auxiliary “textbook/blog/Q&A” materials are LLM-generated, which might introduce lexical overlap or bias with probes despite stratified analysis.  \n3. **Missing baselines:** The paper does not compare against retrieval-augmented or instruction/RL-based knowledge injection methods, leaving uncertainty about whether scaffolding remains advantageous under other paradigms."}, "questions": {"value": "1. Which components of the auxiliary bundle (textbook, blog, Q&A) contribute most to compositional gains — and do these effects persist with **human-written** materials?  \n2. Could alternative **replay curricula** (e.g., staged or post-cycle replay) mitigate forgetting without degrading target-domain learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kvoW8TCrcv", "forum": "oTh8kgWCUO", "replyto": "oTh8kgWCUO", "signatures": ["ICLR.cc/2026/Conference/Submission23564/Reviewer_99D6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23564/Reviewer_99D6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832478967, "cdate": 1761832478967, "tmdate": 1762942713096, "mdate": 1762942713096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Questions: \"\nRQ 1: How is domain knowledge on varying levels, factual and compositional, acquired?\nRQ 2: When learning new knowledge, does the gap in the LLM’s prior knowledge matter?\nRQ 3: How does data replay, post-training, chunking, and other factors affect the learning of the new knowledge? \"\n\nOur corpus consists of six papers from the field of artificial intelligence\n\nClaims:  \"\n(1) Acquisition of complex knowledge requires significant repetition, saturating after approximately 100 exposures in our study. \n(2) Diverse, auxiliary views dramatically improve the learning of both factual and compositional knowledge in a way that paraphrasing does not. \n(3) Bridging knowledge gaps by first training the LLM on prerequisite concepts significantly improves learning. \n(4) Conversely, increasing the amount of data replay from the original pretraining corpus monotonically harms the acquisition of new knowledge. We also ablate several aspects of our training setup to provide pracitcal suggestions for continued pretraining. \""}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This paper is very interesting in its fundamental question. What is the impact of so called auxiliary views - views of the information that are intended to increase scaffolding and understanding. This is a good and important question."}, "weaknesses": {"value": "LLMs largely lack the ability to synthesize complex, novel information from primary sources alone. - this claim is not supportable for LLMs in general. Finding that current LLMs don't do something is very different from the general class of methods lacking the ability. \n\nWhen you cite a paper in the related work section, don't like 5 different papers. \"Since GPT-3, the overarching narrative of LLMs\nhas been scale, regarding both model size and data volume (Brown et al., 2020; Kaplan et al., 2020;\nHoffmann et al., 2022; Carlini et al.; Kandpal et al., 2023; Tirumala et al., 2022).\" This is not helpful and is not convincing. If they are all important, explain clearly what you intend to support with each. \n\nAnother example: \"Continued pretraining has proven to non-trivial(Wang et al., 2021; Janget al., 2021; Hu et al., 2023; Ovadia et al., 2024; Hoffbauer et al., 2024; Jiang et al., 2024).\" There is no purpose discernible from the paper for these citations.\n\nGrammar and writing matter when disseminating research - this is not ready for review: \"Given the recency of the focus on continued pretrainig, the exist body body is young and investigative in nature (Yıldız et al., 2024; Ou et al., 2025)\" \n\nThe formal statement of the problem has an error: \"What are the properties of a good training corpus K that most effectively enable the acquisition of knowledge Kfor fθ?\" The corpus is C_k\n\n\"In addition to probes, we generate paraphrases, prior knowledge, and auxiliary views for each paper in the dataset, utilizing GPT-4.1 for paraphrasing and GPT-5-mini for the rest. While these alternate texts may seem to confer an implicit advantage by distilling the model’s knowledge into the training corpus, this is intentional for the auxiliary views. We treat the LLM as a proxy for domain experts who produce materials such as textbooks and blogs which enters the pretraining data. Inspired by prior works  (Gunasekar et al., 2023; Allen-Zhu & Li, 2024; Jiang et al., 2024), we focus on textbooks, Stack Exchange–style Q&A, and blogs as auxiliary views.\" - GPT 5 is not a domain expert in the topic of a research paper on which it has not been trained. So, the experiments can't actually evaluate the influence of auxiliary views as defined as follow on explanations by domain experts ie. the illustrated transformer as a decompression of \"Attention is all you need\".  \n\nmonotic is not the word which is intended in figure 1. The legend in figure 1 doesn't match the actual signals in the graph. It's not clear what is being conveyed. It is very bad practice to put graphs on the same axis with different values on that axis. It makes the results incomparable. \n\nWhile the central question of the paper is good, the presentation in the paper makes it difficult to judge the voracity of the evidence. The presentation of the paper is not ready for publication."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VHmXOfHRmZ", "forum": "oTh8kgWCUO", "replyto": "oTh8kgWCUO", "signatures": ["ICLR.cc/2026/Conference/Submission23564/Reviewer_ccbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23564/Reviewer_ccbK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923080789, "cdate": 1761923080789, "tmdate": 1762942712796, "mdate": 1762942712796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work analyzes the knowledge acquisition dynamics of LLMs through the controlled setup of continued pretraining. The analysis reveals that the increase of log probability induced by repeated exposure leads to saturation, while providing the model with multiple views of given knowledge improves its learning and generalization. Based on the extensive analysis, the paper conjectures that LLMs largely lack the ability to infer novel compositions of primary knowledge sources without the aid of auxiliary views."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(S1) The experiments are appropriately designed to deal with each research question.\n\n(S2) The results reveal several interesting phenomena, in particular the double-descent-like behavior and the effect of model size on the benefit of auxiliary views.\n\n(S3) The paper is well-written with clean and precise language."}, "weaknesses": {"value": "(W1) While providing various insights, (as mentioned in the discussion) the experiment relies on a single domain of research paper understanding, and it is unclear whether the same result can be applied to other specific domain knowledge, for example, structured documents that require the expertise in medical or legal domain.\n\n(W2) The mechanistic understanding of the observed behaviors is limited, making the claims on the ‘structure’ stay hypothetical (for example, “auxiliary views help the model build a more structured knowledge representation”, in L345). It would be great to see additional analysis of what makes it different from the model trained with/without auxiliary views, in terms of the information encoded in representations or gradients. In addition, studies on synthetic data reveals that grokking often accompanies certain structures of representation (e.g., [1]). Will such “grokking-like” behavior observed in this study related to some induced structure inside the parameters?\n\n(W3) It has been actively discussed in recent studies that domain adaptation often leads to catastrophic forgetting. While the current analysis reveals many potential advantages of domain knowledge acquisition under certain conditions, it might be at the cost of forgetting unrelated knowledge that should be maintained. Could you share your thoughts on the forgetting dynamics under domain adaptation, or additional experiments to rule out that providing auxiliary views does not aggravate catastrophic forgetting?\n\n\n[1] https://arxiv.org/abs/2405.15071"}, "questions": {"value": "Please refer to the points in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DkQJNHXzwA", "forum": "oTh8kgWCUO", "replyto": "oTh8kgWCUO", "signatures": ["ICLR.cc/2026/Conference/Submission23564/Reviewer_PDMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23564/Reviewer_PDMR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981466618, "cdate": 1761981466618, "tmdate": 1762942712569, "mdate": 1762942712569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}