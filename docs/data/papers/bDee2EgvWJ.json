{"id": "bDee2EgvWJ", "number": 17993, "cdate": 1758282724454, "mdate": 1759897140354, "content": {"title": "Approximate Message Passing for Bayesian Neural Networks", "abstract": "Bayesian methods for learning predictive models have the ability to consider both sources of uncertainty (i.e., data and model uncertainty) within a single framework and thereby provide a powerful tool for decision-making. Bayesian neural networks (BNNs) hold great potential for  training data efficiency due to full uncertainty quantification, making them promising candidates for more data-efficient AI in data-constrained settings such as reinforcement learning in the physical world.  However, current computational approaches for learning BNNs often face limitations such as overconfidence, sensitivity to hyperparameters, and posterior collapse, highlighting the need for alternative computational approaches. In this paper, we introduce a novel method that leverages approximate message passing (MP) of a full factorized neural network model using mixed approximations to overcome these problems while maintaining data efficiency. Our framework supports convolutional neural networks while addressing the issue of double-counting training data, which has been a key source of overconfidence in prior work. We demonstrate the data-efficiency of our method on multiple benchmark datasets in comparison to state-of-the-art methods for learning neural networks.", "tldr": "Our framework is the first to support convolutional neural networks for Bayesian learning.", "keywords": ["Bayesian Neural Networks", "Message Passing", "Uncertainty Quantification", "Bayesian Inference"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30137e2fd00fa2562b3e47bbe9e2fbc855d7ac27.pdf", "supplementary_material": "/attachment/2dadbcf720dae01b10f3d03a7863d5981c385dc9.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel Bayesian neural network (BNN) training framework based on **approximate message passing (MP)** over a factor graph representation of the predictive posterior. The key claim is that by modeling the network’s joint posterior via a factor graph with scalar latent variables and applying Gaussian message approximations, one can train BNNs (even convolutional nets) while accurately propagating uncertainty. Notably, the authors emphasize that their algorithm explicitly **avoids “double-counting”** of data as a known issue in prior MP-based BNN methods, which they argue has caused overconfident predictions in earlier approaches. The method is evaluated on several benchmarks. Empirically, the MP method achieves accuracy comparable to strong baselines while often exhibiting better calibration. The paper positions its contributions as: (1) a new MP framework with closed-form Gaussian message updates for all factors; (2) a practical implementation supporting CNNs and avoiding data double-counting; and (3) empirical evidence that this method is competitive with state-of-the-art approaches while improving uncertainty estimates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\t**Novel MP framework:** Introduces a principled factor-graph/BP approach for BNNs, deriving all necessary Gaussian message equations. This is a first step in applying belief propagation to large neural networks.\n\n2.\t**CNN support and data-debiasing:** Unlike earlier MP/EP methods, this framework handles convolutional layers and explicitly avoids “double-counting” of data during batch updates. This addresses a known cause of overconfident predictions in previous work.\n\n3.\t**Strong uncertainty calibration:** Empirically, the method yields excellent predictive uncertainty. For instance, on MNIST with limited data, MP achieved low expected calibration error (∼0.02) vs very high error for SGD. On CIFAR, MP’s ECE was the lowest among baselines. Synthetic tests show that credible intervals track true coverage well (correlation ~0.9). These demonstrate the method’s efficacy in capturing epistemic uncertainty.\n\n4.\t**Broad experimentation:** The authors evaluated on a variety of settings: tabular UCI regression, synthetic functions, MNIST and CIFAR image tasks. They compare to strong baselines (AdamW, IVON, deep ensembles) across multiple metrics (accuracy, NLL, Brier, OOD-AUC). This thoroughness lends credibility to their claims.\n\n5.\t**Clear writing of results:** The paper clearly reports results and highlights key findings (e.g. calibration advantage) in the text accompanying each table. The contributions and limitations are transparently listed."}, "weaknesses": {"value": "1.\t**Heaviness of approximations:** The core MP algorithm uses Gaussian approximations for all messages without theoretical guarantees. Such approximations in highly nonlinear, loopy graphs may introduce bias. The paper does not analyze the accuracy or failure modes of these approximations, leaving uncertainty about when MP might break down.\n\n2.\t**Scalability concerns:** The method is computationally expensive. As acknowledged by the authors, training is up to two orders of magnitude slower and more memory-intensive than standard SGD/AdamW. Each training example carries a full set of messages, and the Julia implementation uses double-precision for safety, making it impractical for very large models. \n\n3.\t**Limited experimental breadth:** The neural architecture used is relatively small (890K parameters, plain 6-layer CNN). Modern benchmarks (ResNet, Transformers) with residuals or normalization are not evaluated. It is unclear how the method handles practical architectures (batch-norm, skip-connections). This restricts claims of general applicability.\n\n4.\t**Mixed empirical performance:** Although calibration is good, the MP method’s predictive accuracy and NLL are inferior to top alternatives. For example, on CIFAR-10 Deep Ensembles achieve 81.9% accuracy vs 77.3% for MP. The paper notes that differences in architecture (lack of normalization/residuals) may explain this, but it underscores that MP does not yet match state-of-art predictive performance in high-dimensional tasks.\n\n5.\t**Reproducibility details:** The paper defers many architectural and hyperparameter details to the Appendix. Important choices (e.g. noise priors, initialization variances) are only briefly mentioned. The lack of multiple runs or error bars also raises questions about robustness."}, "questions": {"value": "1.\t**Double-counting fix:** Can you clarify the theoretical justification for the batch-update rule (dividing out old aggregate messages) that prevents double-counting? Does this exactly recover Bayesian posterior updates, or is it a heuristic? How sensitive is training to this scheme?\n\n2.\t**Scalability to modern nets:** Have you tried incorporating residual connections or normalization (e.g. BatchNorm)? You mention a possible factor for division, but has this been validated? Can the MP algorithm scale to architectures like ResNets or Transformers?\n\n3.\t**Multiple passes or convergence:** How many MP iterations (sweeps through the factor graph) are performed per batch? Is the algorithm guaranteed to converge, or do you observe oscillations? Have you considered damping or other techniques to stabilize message updates?\n\n4.\t**Baselines and hyperparameters:** How were AdamW and IVON tuned? Are the baselines given the best possible schedules and hyperparameters? For CIFAR, why only 25 epochs? Would longer training change the comparisons?\n\n5.\t**Prediction procedure:** For classification, you mention an “argmax factor” in Appendix F. Does this mean you approximate the softmax? How are predictive probabilities computed at test time? Clarify how classification uncertainty is obtained from the Gaussian-approximated network outputs.\n\n6.\t**Posterior quality diagnostics:** Beyond ECE and credible intervals, did you assess other uncertainty metrics (e.g. negative log-likelihood on OOD, Brier decomposition)? Did you examine any divergences or posterior samples to verify that the approximate posterior makes sense (e.g. covariance structure)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical issues were identified."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0cEUdTioi6", "forum": "bDee2EgvWJ", "replyto": "bDee2EgvWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17993/Reviewer_kFYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17993/Reviewer_kFYd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780448967, "cdate": 1761780448967, "tmdate": 1762927788250, "mdate": 1762927788250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel approximate message-passing framework for Bayesian Neural Network inference that scales to architectures such as CNNs and MLPs. Unlike earlier approaches such as Expectation Backpropagation (EBP) and Probabilistic Backpropagation (PBP), the proposed method explicitly avoids double-counting of training data and prevents posterior collapse. The framework models BNNs as factor graphs and derives Gaussian message approximations for all relevant factors. The framework is tested on MNIST, CIFAR-10, and  UCI regression benchmarks, comparing to baselines like SGD, IVON, and Deep Ensembles."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The work attempts to address a core flaw in prior message-passing BNNs (double-counting), which may lead to posterior collapsing\n\n2. The work extends message passing to CNNs, while earlier works were limited to MLPs.\n\n3. The framework formulates BNN inference as belief propagation in a factor graph with analytically tractable Gaussian message approximations, which is computationally efficient."}, "weaknesses": {"value": "1. I unfortunately find that the empirical validation is insufficient to fully support the paper’s claims.\n    * The authors claim their message-passing framework alleviates overconfidence and posterior collapse compared to prior approaches such as EBP and PBP. However, these methods are not included as experimental baselines, making it difficult to verify those claims empirically.\n    * The choice and coverage of baselines are inconsistent across experiments. For example, Figure 3a omits R-SGD, Figure 3b shows Deep Ensemble results only, and Figures 3c, 3d exclude AM-MP and R-SGD. It is also unclear which architecture (MLP or LeNet-5) Figure 3 corresponds to.\n    * In Table 4, the use of RMSE alone is insufficient to assess calibration quality or uncertainty estimation. Complementary metrics such as coverage or NLL would strengthen the evidence.\n\n2. There might be potential fragility and computational cost due to numerous numerical stabilizations. The framework relies on several ad-hoc stability guardrails (Section 4), including special handling for activations like LeakyReLU, re-normalization of small constants, and periodic recomputation of weight marginals. While these measures improve numerical stability, they may introduce additional computational overhead and raise concerns about the robustness and generalizability of the approach across architectures or datasets.\n\n 3. Minor on the writing side. There are a lot of acronyms that are used without being introduced or introduced later, such as IVON, AM-MP"}, "questions": {"value": "1. How are regression-based MPs trained on classification tasks?\n2. Can the \"double-counting\" issue of EBP and PBP be overcome by using the tempered posterior [1]\n\n\n\n[1] Ng, Kenyon, et al. \"Temperature Optimization for Bayesian Deep Learning.\" arXiv preprint arXiv:2410.05757 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K4bWc3a5SD", "forum": "bDee2EgvWJ", "replyto": "bDee2EgvWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17993/Reviewer_JWMe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17993/Reviewer_JWMe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858053727, "cdate": 1761858053727, "tmdate": 1762927787844, "mdate": 1762927787844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a message-passing framework for Bayesian Neural Networks (BNNs) that reformulates training as inference on a factor graph using a diagonal-Gaussian approximation family. It addresses the double-counting problem in existing variational inference methods by introducing a batch-wise “divide-then-multiply” update scheme and a layer-level implementation. The authors implement the method in Julia with GPU support, demonstrating its scalability to MLPs and CNNs and improved calibration over AdamW, IVON, and Deep Ensembles on small-scale benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a message-passing framework for Bayesian neural networks that provides a principled way to model uncertainty while mitigating issues such as data double-counting and overconfidence observed in earlier Bayesian inference methods.\n\n2. The framework is mathematically coherent, deriving closed-form Gaussian message updates for common factor types and supporting deterministic training without reliance on sampling-based approximations.\n\n3. Experiments demonstrate that the proposed approach produces well-calibrated uncertainty estimates and competitive predictive performance across several benchmarks."}, "weaknesses": {"value": "1. Expanding one convolutional kernel into tens of thousands of scalar $\\delta$ factors substantially increases the size of the factor graph so that memory usage is tied to the count of scalar edges, not to the parameter count. This raises practical scalability problems for deep or wide CNNs.\n\n2. Each ReLU layer is handled by moment-matched Gaussian messages. But since the exact moments are computed under a Gaussian assumption that ignores the truncated tail, the resulting estimates may introduce small systematic biases. These biases could accumulate over multiple layers, potentially affecting calibration accuracy in deeper models.\n\n3. The batch-wise “divide-then-multiply” update operates in precision space by dividing the current marginal by the old batch message. When the old message carries low precision, this division amplifies rounding errors and often produces negative precisions that must be clamped to preserve numerical validity. These corrective heuristics reveal intrinsic instability in the update rule, which could undermine convergence and consistency across training iterations."}, "questions": {"value": "1. Have you tested any sparse or tensor-factorised representations that keep a single factor per convolutional kernel element instead of one $\\delta$ -node per scalar multiply–add, and if so what reduction in peak GPU memory did you observe without changing the converged marginal means?\n\n2. Have you checked the skewness or other higher-order moments of the ReLU messages to verify that the Gaussian approximation remains adequate across depth?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9HSGlca4P", "forum": "bDee2EgvWJ", "replyto": "bDee2EgvWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17993/Reviewer_bdZB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17993/Reviewer_bdZB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870459154, "cdate": 1761870459154, "tmdate": 1762927787386, "mdate": 1762927787386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper trains BNNs using “approximate message passing on factor graphs”: it combines Gaussian messages in batches to avoid duplicate counting, scales to CNNs, and improves calibration and OoD performance, but training and memory costs remain high"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Laplace/EP-style Gaussian messages makes stable training and avoid posterior collapse\n2. Batch message aggregation mitigates double-counting and better calibration than AdamW/ensembles"}, "weaknesses": {"value": "1. Computational and memory overhead is high, as Gaussian messages are maintained per sample, resulting in scalability that falls short of standard training pipelines like SGD/Adam\n2. Insufficient large-scale validation, lacking systematic comparisons and ablation studies on larger datasets/modern benchmarks\n3. The processing of residuals, normalization, and other modern layers remains unclear, and there is insufficient evidence of generalization beyond simple CNNs\n4. Loopy BP lacks convergence guarantees. Although the authors acknowledge that guarantees only hold under specific conditions (such as Simon's condition) and are difficult to verify, still remains some concerns"}, "questions": {"value": "The computational power/graphics memory cost of the method is excessively high, its scalability evidence is insufficient, and its effectiveness and stability in medium-to-large-scale, modern networks (residual/normalized) and larger datasets remain poorly substantiated by robust empirical evidence"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mOVxU0IGjF", "forum": "bDee2EgvWJ", "replyto": "bDee2EgvWJ", "signatures": ["ICLR.cc/2026/Conference/Submission17993/Reviewer_mpkr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17993/Reviewer_mpkr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953307775, "cdate": 1761953307775, "tmdate": 1762927786877, "mdate": 1762927786877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}