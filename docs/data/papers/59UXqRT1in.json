{"id": "59UXqRT1in", "number": 23994, "cdate": 1758351434700, "mdate": 1759896787400, "content": {"title": "AGRI-Check: Fact-Checking Agricultural Misinformation", "abstract": "Agricultural misinformation spanning fertilizers, pesticides, crop diseases, government policies, and weather forecasts poses serious risks to farmer decision-making and food security. Despite advances in automated fact-checking for politics and healthcare, agriculture remains largely overlooked. To address this gap, we introduce AGRI-Check, a benchmark dataset and framework for agricultural misinformation detection. Each entry in AGRI-Check is structured by category and includes the original claim, a misinformation variant, and the correct authoritative information from trusted sources such as ICAR, FAO, IMD, and official government documents. This design supports both misinformation detection and correction. We further conduct a comparative study of conventional large language models (LLMs) and quantum-enhanced LLMs for claim verification, integrating evidence retrieval, classification, and justification generation. Results show that while classical LLMs perform strongly in textual reasoning, quantum-based models offer efficiency gains and improved robustness for ambiguous cases. AGRI-Check establishes the first domain-specific benchmark in agriculture and advances quantum NLP applications in fact-checking.", "tldr": "AGRI-Check is the first agriculture-focused fact-checking dataset, benchmarking classical and quantum LLMs for detecting and correcting misinformation on crops, fertilizers, policies, and weather.", "keywords": ["Agricultural Misinformation", "Fact-Checking", "Benchmark Dataset", "Claim Verification", "Evidence Retrieval", "Large Language Models (LLMs)", "Quantum-enhanced LLMs", "Natural Language Processing (NLP)", "Justification Generation", "Domain-specific Misinformation Detection"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e712945c5d329763f1a2a149e5dd0ba46fc11852.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "AGRI-Check targets agricultural misinformation—spanning fertilizers, pesticides, crop diseases, policy, and weather—that threatens farmer decisions and food security. They present a benchmark and framework where each entry includes a claim, a misinformation variant, and the authoritative correction from sources such as ICAR, FAO, IMD, and official government documents, enabling both detection and correction. They also compare conventional LLMs with quantum-enhanced LLMs for claim verification, integrating evidence retrieval, classification, and justification; classical models excel at textual reasoning, while quantum models improve efficiency and robustness in ambiguous cases. AGRICheck establishes the first domain-specific benchmark for agriculture and advances quantum NLP in fact-checking."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. It is the first fact-checking dataset proposed in the agricultural domain.\n\n2. The authors also proposed a framework that achieved high performance on the constructed dataset."}, "weaknesses": {"value": "1. A dataset of such low difficulty was created that even a TF-IDF logistic regression model achieved 96.2% accuracy. Since this dataset has already been effectively solved, it’s unclear what significance it can have for the AI community.\n\n2. There is a lack of detail about the data construction process. It’s unclear how the evidence was composed, and although it seems that the claims were crawled from websites, there is no analysis of the characteristics of claims extracted from each site.\n\n3. The explanation of the research motivation in the Introduction is far too limited.\n\n4. There is insufficient analysis of the dataset itself. Sections 6.2 and 6.3 seem more appropriate for the Appendix; it would be better to use this space instead to include additional dataset analysis."}, "questions": {"value": "It would be great if you could provide responses to the points written under Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AV6zTV1CKj", "forum": "59UXqRT1in", "replyto": "59UXqRT1in", "signatures": ["ICLR.cc/2026/Conference/Submission23994/Reviewer_yfVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23994/Reviewer_yfVZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772729875, "cdate": 1760772729875, "tmdate": 1762942890014, "mdate": 1762942890014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AGRI-Check, a benchmark dataset and fact-checking framework for detecting agricultural misinformation.\nEach entry contains a real claim, a misinformation variant, and the corresponding verified information from authoritative sources such as ICAR, FAO, IMD, and official government documents.\nUsing this benchmark, the authors compare classical LLMs with quantum-enhanced LLMs for claim verification.\nExperimental results show that while classical LLMs perform well in textual reasoning, the quantum-enhanced ones offer efficiency gains and improved robustness when handling ambiguous or uncertain claims.\nOverall, AGRI-Check provides the first domain-specific benchmark for agricultural misinformation and explores a novel hybrid quantum–classical pipeline for fact-checking."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The AGRI-Check benchmark fills an important gap by providing the first structured dataset for agricultural misinformation detection.\n- The proposed fact-checking pipeline includes several stages: parsing, evidence retrieval, numeric validation, and justification generation, along with evaluation metrics.\n\n- The system demonstrates practical potential for real-world deployment in agricultural contexts."}, "weaknesses": {"value": "- The presentation quality is poor: missing background context, inconsistent writing, and unclear explanations of the pipeline.\n\n- Although the practical value of this paper is good, the depth of analysis is limited. The paper does not sufficiently explain why quantum-enhanced LLMs are necessary or how they specifically improve reasoning and robustness."}, "questions": {"value": "I believe this paper can be significantly improved by providing adequate background information and clearer justifications for the proposed AGRI-Check system, including how it differs from existing fact-checking frameworks.\n\nConduct a deeper comparative analysis between conventional LLMs and quantum-enhanced LLMs within the context of agricultural misinformation detection — especially explaining why quantum methods help, and under what specific conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KoouQDSZr4", "forum": "59UXqRT1in", "replyto": "59UXqRT1in", "signatures": ["ICLR.cc/2026/Conference/Submission23994/Reviewer_ACom"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23994/Reviewer_ACom"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841611914, "cdate": 1761841611914, "tmdate": 1762942889638, "mdate": 1762942889638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AGRI-Check, a new benchmark dataset for the novel, domain-specific task of agricultural misinformation, and uses it to compare conventional LLMs against quantum-enhanced LLMs. The study evaluates these models on a verification pipeline (retrieval, classification, and justification), finding that while classical LLMs perform strongly, the quantum-based approaches demonstrate potential advantages in efficiency and robustness. However, **I would recommend a Major Revision to the authors.** The current manuscript lacks critical details across the Introduction, Related Work, Method, and Experiments sections. The authors must thoroughly revise the paper to provide a self-contained, detailed, and replicable explanation of their motivation, data collection pipeline, and proposed system."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors tackled a novel, domain-specific fact-checking problem: agricultural disinformation verification in both English and a low-resource language, Hinglish."}, "weaknesses": {"value": "1. Introduction and Motivation: The introduction is overly simplistic and lacks crucial context.\n+ The motivation for this work must be strengthened. While the lack of benchmarks is stated, authors need to clarify why agricultural disinformation is a severe problem (e.g., impact on farming practices, food security, public health) to fully justify the effort.\n+ The outline of the proposed method (L38-L40) is too vague. For instance, regarding the evidence in L38, please specify: What is the nature and source of this evidence?\n\n2. Related Work: This section is not self-contained and fails to adequately highlight the novelty.\n+ Table 1 (Fact-Checking Datasets) must be explained and accompanied by a comparative analysis against the proposed benchmark. Citing survey papers is not an acceptable substitute for this analysis.\n+ The inclusion of multimodal and structured data in Section 2.1 is inconsistent with the subsection's title and should be moved to a more relevant, independent section.\n+ The manuscript must provide detailed evidence of the superiority of QNLP models in fact-checking for readers who are not specialists in those models.\n\n3. Data Collection and Annotation: The entire content of Section 3 is scattered and incomplete, making it impossible to replicate the dataset creation process.\n+ Authoritative Evidence: Clearly define the selection process and rationale for using sources like ICAR, FAO, IMD, government bodies, and peer-reviewed literature. What are the statistics of the authoritative references corresponding to the claims?\n+ Domain and Language: Justify the selection of the eight domains. Explain the choice of Hinglish over standard Hindi.\n+ Labeling Process: Provide references and a clear explanation for the selection of the four-point veracity scale.\n+ Data Pipeline: The description of the \"methodical, multi-phase process\" is insufficient. A detailed explanation of the data collection pipeline is necessary, as Section 3.3 does not adequately convey this.\n+ Citations: Please clarify the necessity and context for citing Wadden et al., Chen et al., Hanselowski et al. (2019), and Aly et al. (2021) in Section 3.4.\n\n4. The Proposed AGRI-Check System: This section is critically missing the algorithmic and methodological details required for proper evaluation.\n+ What is the specific algorithm used for semantic retrieval?\n+ How was the geographic and temporal filtering implemented?\n+ Detail the source authority scoring mechanism.\n+ Describe the process of evidence integration. Does this process utilize an LLM?\n+ How is the evidence quality measured?\n+ What kind of numeracy analysis was conducted?\n+ Fully elaborate on the mechanisms and methodology behind \"Constrained Generation,\" \"Faithfulness Validation,\" and \"Multi-Modal Support.\"\n\n5. Experiments: The strong performance of the simple TF-IDF based logistic regression is a serious concern. Given this baseline's simplicity and effectiveness, authors must clearly state the advantage and necessity of using the proposed AGRI-Check system. Moreover, the high accuracy of a simple baseline suggests a potential lack of data difficulty or quality. Please provide further analysis (e.g., complexity metrics, inter-annotator agreement) to demonstrate that the benchmark is sufficiently challenging.\n\n6. Formatting Issues\n+ L41: A white space is missing between \"textual reasoning\" and \"Peral.\"\n+ Ensure consistent white spaces in the \"Introduction-Contributions\" paragraph."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iEXGH0PN44", "forum": "59UXqRT1in", "replyto": "59UXqRT1in", "signatures": ["ICLR.cc/2026/Conference/Submission23994/Reviewer_7rd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23994/Reviewer_7rd6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916040906, "cdate": 1761916040906, "tmdate": 1762942888823, "mdate": 1762942888823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AGRI-Check, a new benchmark dataset and framework for fact-checking agricultural misinformation. The authors point out that agriculture is a high-impact, yet overlooked, area for automated fact-checking. The main contributions are threefold: 1) The AGRI-Check dataset, which includes code-mixed (Hinglish) data and is based on authoritative, domain-specific sources ; 2) A modular fact-checking pipeline designed to handle the domain's unique challenges, such as numeric and geo-specific claims; 3) A comparative study of classical LLMs versus quantum-enhanced LLMs for this task, with the latter showing robustness advantages in ambiguous cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper's greatest strength is its focus on the agricultural domain. The authors make a compelling case that misinformation in this area poses serious risks to food security and farmer decision-making, creating the first benchmark for this domain. \n2.\tThe dataset is well-designed, including Hinglish claims, and its inclusion of heterogeneous sources and semi-structured data is a key strength, addressing known gaps in NLP. \n3.\tThe proposed AGRI-Check pipeline is a robust modular architecture. This design is well-suited for high-stakes, domain-specific fact-checking. Commendably, it includes specialized components like 'Numeric validation' and 'Evidence graph construction'."}, "weaknesses": {"value": "1.\tClaims about quantum-enhanced LLMs lack empirical evidence, which is the paper's most serious weakness. The authors claim 'quantum-based models offer efficiency gains and improved robustness for ambiguous cases' and 'superior uncertainty estimates'. However, these claims are not supported by rigorous evidence. \n1.1 The authors compare their hybrid quantum-LLM to a single classical pipeline. Any claim of improved 'robustness' must be benchmarked against strong and mature classical robustness methods, for example, Ensemble Learning. An ensemble of classical models often also improves robustness and uncertainty calibration. Without this comparison, it is impossible to know if the 'quantum' module provides any benefit beyond what a classical ensemble could achieve. \n1.2 The authors do not provide the most critical ablation study: replacing the quantum module with a parameter-matched classical layer and re-evaluating. Otherwise, the observed 'gains' cannot be attributed to any quantum principle; they may simply stem from adding more parameters or a specific hybrid architecture, which has nothing to do with quantum computation itself. \n1.3 The paper claims 'efficiency gains', but the reproducibility checklist indicates the quantum model adds inference overhead ('+0.8s quantum'). And 'improved robustness' and 'better-calibrated conclusions' were not quantitatively measured using any standard robustness metrics or calibration scores. \n2.\tThe paper reports a 'perfect agreement rate of 59.68%' and an overall agreement of 82.66%. This moderate level of perfect agreement is a critical finding, not just a quality metric. A moderate IAA often indicates inherent ambiguity in the task itself. \n3.\tThe authors miss a key opportunity to analyze why the experts disagreed in 17.34% of cases. Are these disagreements concentrated in specific categories, for example, 'Government Policies' or 'Weather Forecasts'? Table 2 implies these categories have lower agreement. A qualitative analysis of these 'ambiguous cases' would be extremely valuable"}, "questions": {"value": "1.\tCan you provide a direct comparison against a strong classical robustness baseline, such as an ensemble of classical pipeline models? \n2.\tCan you provide an ablation study where the quantum module is replaced by a parameter-matched classical neural network to isolate the specific contribution of the quantum-based computation?\n3.\tHow were 'robustness' and 'better-calibrated conclusions' quantitatively measured? Can you provide results from standardized adversarial perturbation tests or a formal calibration analysis?\n4.\tIAA Analysis: The 59.68% perfect agreement rate indicates significant domain ambiguity. Can you provide a qualitative or quantitative analysis of the claims where your two experts disagreed? Do these 'disagreement' cases correlate with specific categories or claim types?\n5.\tWhat is the performance delta of your framework on the English-only claims versus the Hinglish claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qvRtHLGXP5", "forum": "59UXqRT1in", "replyto": "59UXqRT1in", "signatures": ["ICLR.cc/2026/Conference/Submission23994/Reviewer_Wfoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23994/Reviewer_Wfoh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23994/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088274581, "cdate": 1762088274581, "tmdate": 1762942888569, "mdate": 1762942888569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}