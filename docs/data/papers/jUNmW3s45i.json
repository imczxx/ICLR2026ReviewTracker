{"id": "jUNmW3s45i", "number": 12174, "cdate": 1758206156578, "mdate": 1762964066620, "content": {"title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance", "abstract": "Video generation models based on diffusion transformers have recently attracted widespread attention for their excellent generation quality.\nDespite recent progress, their computational expense remains the principal bottleneck. In particular, attention alone accounts for more than 80\\% of the overall latency, and the synthesis of only 8 seconds 720p video takes tens of minutes, which severely restricts practical applicability and scalability.\nTo address this, we propose **DraftAttention**, a training-free framework for the acceleration of video diffusion transformers with dynamic sparse attention on GPUs.\nThe key idea is to compute the low-resolution draft attention based on the downsampled low-resolution query and key with minor computational overhead. The draft attention exposes redundancy both spatially within each feature map and temporally across frames, thus identifying the most important areas in the attention map.\nThe resulting low-resolution sparse mask then guides full-resolution sparse attention computations.\nTo align region-level sparsity with token-level computations, we further propose a deterministic reordering of tokens such that entries in each region become contiguous in memory, ensuring hardware-friendly execution of sparse attention.\nOur theoretical analysis demonstrates that the low-resolution draft attention closely approximates the full attention, providing reliable guidance for constructing accurate sparse attention.\nExperimental results show that our method outperforms existing sparse attention approaches in video generation quality and achieves up to 2x end-to-end speedup on GPUs.", "tldr": "", "keywords": ["Video Generation", "Efficient Video Generation", "Sparse Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/89651ed6f0637a5752cf7b2022469825e9a54ebf.pdf", "supplementary_material": "/attachment/84c3b89e822ef44f572ffd8586c4576ba6ccf454.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes DraftAttention, a training-free acceleration method for video diffusion transformers. It computes a low-resolution (pooled) draft attention over downsampled Q/K to select top-r region-to-region interactions, then applies a corresponding structured block-sparse mask at full resolution. A deterministic locality-preserving permutation (patch grouping) makes the masked pattern hardware-friendly. The authors give upper bounds on approximation error (pooling → draft vs. dense; sparsification vs. dense), and report up to ~2× E2E speedups at similar quality to dense or to a strong static baseline, with small overhead for reordering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Simple and effective idea. Compatible with FP8.\n\n2. Hardware-aware: the permutation bridges region-level masks to fixed-size block kernels.\n\n3. Some theory with Frobenius-norm bounds.\n\n4. Consistent quality at meaningful speedups; overhead appears small."}, "weaknesses": {"value": "1. Missing highly relevant prior work.\n\na. Low-res (mean-pooled) attention as a coarse proxy. MoBA and Quest (Query-aware sparsity) are, to my knowledge, the earliest to explicitly use mean-pooling as coarse scoring in LLM (what this paper's authors refer to as  “low-resolution attention”); SpargeAttention and VSA later adopt analogous ideas in image/video. \n\nb. Locality-preserving permutations. Sliding Tile Attention first proposes locality-preserving permutations for efficient block-sparse attention in video; SpargeAttention concurrently uses a Hilbert-curve variant. This paper's reordering techniques appears functionally identical. \n\n**I find the lack of citation to highly relevant prior works in this paper deeply concerning**. This paper should (i) credit those works, (ii) make the novelty claim precise and (iii) clarify differences.\n\nLu, Enze et al. MoBA: Mixture of Block Attention for Long-Context LLMs. arXiv preprint arXiv:2502.13189 (2025). \n\nTang, Jiaming et al. QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference. In ICML 2024.\n\nZhang, Peiyuan et al. STA: Fast Video Generation with Sliding Tile Attention. In ICML 2025. \n\nZhang, Jintao et al. SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference. In ICML 2025. \n\nZhang, Peiyuan et al. VSA: Faster Video Diffusion with Trainable Sparse Attention. In Neurips 2025.\n\n\n2.  It is a bit tricky to say the the attention sparsity is 90% because the first 25% timestep if kept dense attention. I would presume it is only 90% sparse for the later 75% timestep? It is a known fact the full denoising step (50) for video diffusio model is highly redundant. I wonder: a. how the model will perform if use use less dense attention step. b. What if we just reduce the total timestep from 50 to 25, and let the first 10 steps to be full attention. Will the human preference make a huge difference?"}, "questions": {"value": "1. The method fixes 2D pooling 8×16. Why not try 3D pooling to exploit temporal redundancy as well？\n\n2. Implementation is less efficient on H100. I also wonder if the full attention baseline on H100 use FlashAttention2 or FlashAttention3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iMTLfH1pqW", "forum": "jUNmW3s45i", "replyto": "jUNmW3s45i", "signatures": ["ICLR.cc/2026/Conference/Submission12174/Reviewer_FEKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12174/Reviewer_FEKX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760663412614, "cdate": 1760663412614, "tmdate": 1762923125797, "mdate": 1762923125797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "WAX1juygfv", "forum": "jUNmW3s45i", "replyto": "jUNmW3s45i", "signatures": ["ICLR.cc/2026/Conference/Submission12174/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12174/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762964065763, "cdate": 1762964065763, "tmdate": 1762964065763, "mdate": 1762964065763, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DraftAttention, a training-free framework to speed up video diffusion transformers while keeping good generation quality. It first computes a lightweight low-resolution \"draft attention\" (via downsampling query and key) to identify important regions and create a sparse mask. It also reorders tokens to make each region’s tokens contiguous. Experiments show it achieves good speedup on GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents DraftAttention, a training-free, plug-and-play sparse attention framework that can accelerate video diffusion transformers while maintaining generation quality.\n\nThe method is simple, generalizable, and practical for integration into existing models without retraining."}, "weaknesses": {"value": "**Limited novelty**  \n  The paper mainly proposes two techniques, but both with limited novelty:  \n  **(a)** Mean pooling compression to obtain a low-resolution attention map for guiding sparse attention has appeared in SpargeAttn, SeerAttention, etc.  \n  **(b)** Token reordering to exploit token sparsity and improve hardware efficiency has also been studied in SpargeAttn and SVG2.\n    Moreover, the proposed token permutation is simple (it can be implemented with one line of code in Eniops) and mainly serves to meet kernel memory contiguity requirements. It should not be presented as a key innovation.\n\n**Incomplete experimental evaluation**  \n  **(a)** No comparison of attention kernel speed with baseline kernels.  \n  **(b)** No analysis of the relationship between sparsity ratio and kernel speed, which is crucial to assess kernel optimization.  \n  **(c)** No end-to-end model speed comparison with baselines.  \n  **(d)** The set of baselines is too limited: only *SVG* is included. Additional comparisons with *SVG2*, *SpargeAttn*, and *RadialAttn*, etc. are needed.  \n  **(e)** The kernel is implemented based on Block Sparse Attention from *FlashAttention 2*, which does not align with *FlashAttention 3* (the standard on H100 GPUs), weakening the paper’s practical contribution.  \n  **(f)** There is no kernel-level speed or overhead analysis, including the cost of token permutation and draft computation."}, "questions": {"value": "What advantages does DraftAttention offer over SpargeAttn, which also employs mean pooling and token permutation, along with additional improvements such as sparse online softmax and selectively mean pooling? Since SpargeAttn is a relatively early but closely related work, why is there no direct comparison with it?\n  \nIt would be better to discuss sparsity across different attention heads and layers, and to explain how to determine the sparsity ratio *r*. While there is a theoretical analysis of the error, there is no empirical verification of the error."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RSgKE1YcuI", "forum": "jUNmW3s45i", "replyto": "jUNmW3s45i", "signatures": ["ICLR.cc/2026/Conference/Submission12174/Reviewer_baso"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12174/Reviewer_baso"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758978699, "cdate": 1761758978699, "tmdate": 1762923125107, "mdate": 1762923125107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DraftAttention, a training-free method to speed up video diffusion transformers. It computes a low-resolution attention map to identify important regions and uses this to guide sparse attention at full resolution. Combined with a token reordering step for GPU efficiency, the approach achieves up to 2x faster generation with minimal quality loss compared to dense attention and outperforms prior sparse attention methods under the same compute budget."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is well written; it is easy to read and understand.\n2) The method is training-free and plug-and-play, it works with existing state-of-the-art video diffusion transformers.\n3) Good empirical results: the approach achieves up to 2× speedup on A100/H100 GPUs while preserving video quality.\n4) The method is orthogonal to other acceleration techniques; it can be combined with quantization or distillation for further efficiency gains."}, "weaknesses": {"value": "1) The experiments mainly compare against Sparse VideoGen (SVG). Other sparse attention methods like AdaSpa are discussed but not included, which weakens claims of state-of-the-art performance. \n2) The paper relies entirely on automated metrics (PSNR, SSIM, LPIPS, VBench) to assess perceptual quality. These metrics often fail to capture nuanced aspects of realism and user preference. A human study or preference test would significantly strengthen the quality claims.\n3) There are no comparisons to other transformer acceleration techniques, like token merging, token pruning, feature caching across timesteps, and so on."}, "questions": {"value": "1) Could you please provide VBench total score for entries in Table 1?\n2) Currently, kernel for draft attention is only spatial. Have you also tried or considered 3D kernels (e.g., 4x8x16)? Would this method perform well in this scenario?\n3) I’m curious whether the proposed method is compatible with step-distilled models such as DMD2, which can generate high-quality videos in just 1–4 sampling steps. If it only works with the original, non-distilled models, its practical utility becomes limited, since the method is not applied during the first 25% of diffusion steps, already more computationally expensive than the entire budget of these distilled models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FLXg29uEHM", "forum": "jUNmW3s45i", "replyto": "jUNmW3s45i", "signatures": ["ICLR.cc/2026/Conference/Submission12174/Reviewer_PbAn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12174/Reviewer_PbAn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862493726, "cdate": 1761862493726, "tmdate": 1762923124515, "mdate": 1762923124515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DraftAttention, a training-free acceleration framework for video diffusion transformers that introduces dynamic sparse attention guided by a low-resolution draft attention map. The approach computes attention at a downsampled resolution to identify spatially and temporally redundant regions, and then applies a block sparse mask to full-resolution attention computations. The method achieves up to 2× speedup in video synthesis with minimal loss in generation quality, outperforming existing sparse attention baselines according to the reported results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation is clear and well organized, with thorough motivation, method description, and experimental validation.\n\n- The computational bottlenecks in attention of video diffusion transformers are a pressing issue, and the proposed approach targets a key challenge for scaling video generation systems."}, "weaknesses": {"value": "- The core idea—using pooling-based  approximations to guide sparse block attention—is conceptually similar to prior work (e.g., MInference for LLMs and SpargeAttention for video diffusion). This overlap weakens the novelty claim.\n\n- The experimental evaluation omits direct comparisons with several relevant sparse or spatially adaptive attention methods such as SpargeAttention, SlidingTileAttention, RadialAttention, and XAttention, which are necessary to situate the method’s performance within the broader literature."}, "questions": {"value": "- How does DraftAttention differ fundamentally from existing pooling-based block-sparse acceleration techniques (e.g., MInference)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yv8sj9nyKq", "forum": "jUNmW3s45i", "replyto": "jUNmW3s45i", "signatures": ["ICLR.cc/2026/Conference/Submission12174/Reviewer_CeWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12174/Reviewer_CeWm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943731141, "cdate": 1761943731141, "tmdate": 1762923123992, "mdate": 1762923123992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}