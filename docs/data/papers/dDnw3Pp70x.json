{"id": "dDnw3Pp70x", "number": 11899, "cdate": 1758204534484, "mdate": 1759897547853, "content": {"title": "TIPO: Text to Image with Text Pre-sampling for Prompt Optimization", "abstract": "TIPO (Text-to-Image Prompt Optimization) introduces an efficient approach for automatic prompt refinement in text-to-image (T2I) generation. Starting from simple user prompts, TIPO leverages a lightweight pre-trained model to expand these prompts into richer, detailed versions. Conceptually, TIPO samples refined prompts from a targeted sub-distribution within the broader semantic space, preserving the original intent while significantly improving visual quality, coherence, and detail. Unlike resource-intensive methods based on large language models (LLMs) or reinforcement learning (RL), TIPO provides computational efficiency and scalability, opening new possibilities for effective, automated prompt engineering in T2I tasks.\n\nWe provide visual results, human preference report to investigate TIPO's effectiveness. Experimental evaluations on benchmark datasets demonstrate substantial improvements in aesthetic quality, significant reduction of visual artifacts, and enhanced alignment with target distributions along with significant human preference proficiency. These results highlight the importance of targeted prompt engineering in text-to-image tasks and indicate broader opportunities for automated prompt refinement.", "tldr": "", "keywords": ["prompt optimization", "prompt engineering", "text-to-image"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a438ee13fface9689a0f8caccbff7568789044f8.pdf", "supplementary_material": "/attachment/3f00be52a0d428273bc34517046f7d0d160a9932.zip"}, "replies": [{"content": {"summary": {"value": "TIPO (Text-to-Image Prompt Optimization) is an efficient framework designed for the automatic refinement of simple user inputs into rich, detailed prompts for text-to-image generation. The core intuition is to align the optimized prompt with the text distribution used in text-to-image model training. Compare with using expensive LLM rewriting or reinforcement learning approach, TIPO trains a lightweight multi-task language model, employing a pre-sampling mechanism to expand the original user prompt. This multi-task training includes pretext tasks for handling both tag-based and natural language inputs. Experiments demonstrate that TIPO achieves superior image quality, stronger text alignment and a 62.8% human preference win rate against strong baselines, while also providing up to a 29.4% runtime efficiency improvemen."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core intuition is that optimal prompts must align with the large-scale text distributions used to train T2I models, which can reduce the mismatch between the training and inference. It is novel and straightforward. Moreover, the method is designed to be universal, leveraging its large-scale curated corpus of over 30 million text descriptions to ensure compatibility across various T2I models.\n\n2. The method employs a lightweight language model to expand the use prompts. This computational efficiency is a major practical advantage over resource-intensive LLM-based or RL-based methods.\n\n3. The experimental results are solid. It is conducted on both in-domain and out-of-domain tasks, which shows that the method is general."}, "weaknesses": {"value": "1. The limitation of this work is that it needs the large-scale open-sourced text-to-image training data to make sure that prompt expanding is robust. But some current models are trained use higher-quality close-sourced data, often containing some synthetic data. It is impossible  to get the reasonable data distribution of these models, making the proposed method ineffective.\n\n2. The framework does not consider any feedback from the generated images, which means that it cannot adaptively improve the prompts according to user feedback or model feedback."}, "questions": {"value": "What sampling method do you use, top-p or greedy decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SsYRf5U8M9", "forum": "dDnw3Pp70x", "replyto": "dDnw3Pp70x", "signatures": ["ICLR.cc/2026/Conference/Submission11899/Reviewer_TZN3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11899/Reviewer_TZN3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714749301, "cdate": 1761714749301, "tmdate": 1762922911083, "mdate": 1762922911083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed TIPO is a prompt optimization method for text-to-image (T2I) models that leverages a lightweight pre-trained model to automatically expand users' simple inputs into detailed and semantically rich prompts. The core idea is to transform user input prompts into prompts that align with the text distribution of T2I model training data, thereby significantly improving visual quality, coherence, and detail while preserving the original intent. The experiments report that prompt refinement can be performed in a lightweight, fast, and general manner across various evaluation metrics such as FDD and Aesthetic Score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The implementation approach, which uses a LLaMA-based 200M parameter model for pre-training and does not depend on large-scale LLMs or reinforcement learning, is attractive from a practical standpoint. Additionally, it does not involve fine-tuning of T2I models.\n\n- For tag-based prompts, TIPO achieves the best FDD (0.2282), and for natural language-based (short) and truncated long prompts, it demonstrates the best or second-best performance in Aesthetic Score and AI Corrupt Score with stable performance.\n\n- Beyond Stable Diffusion family models, performance improvements are confirmed on diverse T2I models including FLUX.1-dev, Omnigen2, Lumina-2, and HiDream-I1. The fact that TIPO is effective even for models with undisclosed training data demonstrates the high generalizability and robustness of the proposed method.\n\n- While conventional methods indirectly optimize prompts using reward models, the finding that better prompts can be created simply by directly aligning with the data distribution learned by T2I models, as in the proposed method, provides an interesting insight."}, "weaknesses": {"value": "- The verification of \"distribution-aligned\" is indirect. While claiming to \"align with T2I training distributions,\" the paper does not directly measure the distance of text distributions themselves (e.g., KL divergence of embedding distributions or perplexity differences). The alignment is primarily inferred from image-side metrics (such as FDD) and human comparisons, lacking direct measurement of text distribution.\n\n- In OOD settings, while diversity (Vendi Score) improves significantly, there is an issue that GPT-generated prompts are accurate but lack diversity. Although TIPO optimization adds additional details that harmonize with the original themes and significantly improves the diversity of generated images, the relative positioning of aesthetic scores and corruption rates compared to some baselines should be explicitly stated as a limitation of generalizability.\n\n- While the paper presents inference time cost comparisons (Table 4 showing up to 29.4% speedup), and Table 6 shows training settings for each TIPO model (GPU types, training time, datasets), there is insufficient direct quantitative comparison with RL-based methods such as Promptist and PAE regarding training costs. Explicit comparison is desired for fair evaluation.\n\n- The adopted metrics are primarily image-centric, such as Aesthetic Score, AI Corrupt Score, Vendi Score, and FDD, and do not include automatic metrics that measure text-image alignment such as CLIPScore or GenEval. There is insufficient quantitative evidence to support the claimed \"stronger text alignment.\""}, "questions": {"value": "1. Can you add metrics that directly measure text distribution alignment (e.g., embedding distance from training data or perplexity)?\n\n2. While diversity improves in OOD environments, do you have any causal analysis or improvement strategies regarding the relative performance differences with other baselines?\n\n3. Can you provide a quantitative comparison of training computational costs and resource requirements (GPU memory, training time, etc.) with RL-based methods such as Promptist and PAE?\n\n4. Can you add evaluation results using automatic metrics that directly measure text-image alignment, such as CLIPScore or GenEval?# Review of \"TIPO: Text to Image with Text Presampling for Prompt Optimization\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oy9firTMet", "forum": "dDnw3Pp70x", "replyto": "dDnw3Pp70x", "signatures": ["ICLR.cc/2026/Conference/Submission11899/Reviewer_LKuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11899/Reviewer_LKuZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892519662, "cdate": 1761892519662, "tmdate": 1762922910597, "mdate": 1762922910597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TIPO (Text-to-Image Prompt Optimization) proposes an efficient approach to automatic prompt enhancement for T2I models. Instead of fully rewriting user inputs with a large LLM, TIPO uses a lightweight, multi-task pretrained language model to perform text presampling before image generation: it retains the original user prompt and appends a structured, detail-rich continuation that better matches the text distribution T2I models are actually trained on."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s key strength is conceptual clarity: it treats “bringing the prompt back to the T2I training text distribution” as the central objective. To achieve this, it adopts a lightweight model that does prefix-preserving, suffix-expansion prompt optimization, which (i) avoids the off-topic drift often seen in full LLM rewrites, and (ii) is cheaper, faster, and more deployable than RL-based prompt optimization. The experimental section is also solid: it covers in-domain and out-of-domain settings, multiple T2I backbones, and evaluates with FDD, Aesthetic Score, AI Corrupt Score, Vendi, plus human preference, so the claimed gains are reasonably well supported."}, "weaknesses": {"value": "The method is still distribution-dependent: if the user prompt is very niche, domain-specific, or stylistically unusual, the expanded prompt may not be reliable. Once the prompt is made more detailed, the generation space narrows, and Vendi indeed drops in some settings. For closed-source or stylistically distant T2I models, the “distribution-aligned” expansion can sometimes misfire, leading to slight aesthetic regressions. Finally, the current TIPO is a generic optimizer — it doesn’t condition on user profile, project domain, or target style, so it cannot precisely do “expand this, but keep my style.”"}, "questions": {"value": "- Since a single TIPO model is used for multiple T2I backbones (rather than training one optimizer per T2I model), does this create subtle distribution-mismatch issues for models whose caption/style format differs from the main training corpus?\n- Does the method exhibit a scaling law? In other words, if we move from the 200M model to larger variants, do we see monotonic gains in fidelity, aesthetic score, and OOD robustness — and where is the efficiency/quality turning point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OsmAdL0FVD", "forum": "dDnw3Pp70x", "replyto": "dDnw3Pp70x", "signatures": ["ICLR.cc/2026/Conference/Submission11899/Reviewer_TMB7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11899/Reviewer_TMB7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922037110, "cdate": 1761922037110, "tmdate": 1762922909970, "mdate": 1762922909970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a new method for prompt rewriting for text-to-image generation. They design a specific format to structurally combine natural language and tag-based prompts and implement a pre-sampling algorithm that progressively refines arbitrary, coarse user input into organized, fine-grained prompts by training LLMs. They evaluate their models against prior baselines as well as SOTA LLMs on a large variety of text-to-image models as well as native multimodal models using standardized metrics and human preference survey."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has very comprehensive experiments, especially the range of text-to-image models and native multimodal models that they test with. And the fact that it is effective even for models with self-refinement capabilities also shows good practical applicability of their method.\n2. ELO rating is an interesting and creative way to showcase human evaluation comparison results among multiple models. I don’t think this is a standard metric for image generation evaluation yet, but I think it should become one.\n3. The description of their training recipe is also very clear and easy to follow, making the method also very adoptable.\n4. The inference speed is very fast.\n5. The models are trained with relatively small GPUs in a relatively short amount of time, which makes the method resource efficient."}, "weaknesses": {"value": "1. The improvement on benchmarks that TIPO brings is not very consistent across the board and is sometimes very marginal.\n2. Mild writing suggestions: Section 3 does not really provide much information and is not very well connected to the rest of the paper. Given how packed the remainder of the paper is, I would suggest either shortening that section or removing it entirely and just explaining the notations when using them later on.\n3. Minor: there are some inconsistencies in citation styles in the first paragraph of the introduction."}, "questions": {"value": "From the qualitative examples, it seems like the diversity of the images should be improved after rewriting. However, this is not reflected with the Vendi scores. Do the authors have an explanation of why this may be the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HVjM1877Bm", "forum": "dDnw3Pp70x", "replyto": "dDnw3Pp70x", "signatures": ["ICLR.cc/2026/Conference/Submission11899/Reviewer_fBC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11899/Reviewer_fBC3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975413427, "cdate": 1761975413427, "tmdate": 1762922909442, "mdate": 1762922909442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}