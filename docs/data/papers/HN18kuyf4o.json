{"id": "HN18kuyf4o", "number": 5155, "cdate": 1757856945286, "mdate": 1759897991495, "content": {"title": "Distilling Knowledge from Caption-Guided Replay for VLM-based Continual Learning", "abstract": "Continual learning with vision-language models is challenged by catastrophic forgetting, where the acquisition of new knowledge compromises previously learned information. Generative replay synthesizes past samples to mitigate forgetting, while avoiding the data-privacy risks and heavy storage overhead of directly replaying historical data. However, existing methods often rely on simple class-level prompts, such as class-name with templates, resulting in synthetic images that poorly capture the semantics of original images. To address this, we propose a \\textit{caption-guided replay paradigm} that stores instance-level captions generated by a Multi-modal LLM as memory and reconstructs past images using a LoRA-adapted text-to-image model. This approach enables high-fidelity and instance-aware synthetic replay while remaining efficient in storage. In addition to improving replay fidelity, we observe the phenomenon of \\textit{feature drift} in continual learning, which refers to pervasive shifts in intermediate representations during sequential training and is only partially addressed by logit distillation. To address this, we introduce a distribution-based distillation method that aligns feature distributions at multiple intermediate layers, effectively suppressing feature drift and enhancing model stability. Extensive experiments under various settings demonstrate that our proposed method consistently outperforms state-of-the-art approaches.", "tldr": "We present a novel caption-guided replay paradigm and distribution distillation for continual learning, which effectively mitigates forgetting and consistently surpasses previous state-of-the-art approaches.", "keywords": ["continual learning", "synthetic data", "knowledge distillation", "visual-language model"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ee13335b37a6a31b4725141294ca2d9a2c6baaa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a caption-guided replay paradigm that stores instance-level captions generated by a multi-modal large language model as memory and reconstructs past images using a LoRA-adapted text-to-image model, achieving high-fidelity and instance-aware synthetic replay with efficient storage. This approach effectively addresses the data privacy concerns and heavy storage demands associated with replay-based methods that rely on real historical samples. Additionally, this paper identifies the issue of feature drift in continual learning—shifts in intermediate representations during sequential training that are only partially alleviated by conventional logit distillation. To tackle this, the proposed method introduces a distribution-based distillation strategy that aligns feature distributions across multiple intermediate layers, thereby suppressing feature drift and enhancing model stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper introduces a novel caption-guided replay paradigm that not only addresses the storage burden and data privacy issues inherent in traditional replay-based methods relying on real historical data, but also overcomes the limitations of conventional generative replay approaches. Existing generative replay methods, which typically use simple prompts constructed from class names and CLIP-style templates, fail to capture instance-aware semantics and thus often produce synthetic images with a noticeable fidelity gap compared to original task data. In contrast, this method generates detailed, instance-level captions for each image, enabling the text-to-image (T2I) model to synthesize images that are semantically and visually closer to the originals. \n2.\tTask-specific adaptation is achieved by fine-tuning the text-to-image model with task-specific LoRA modules, ensuring that the visual characteristics of each task are well preserved during replay.\n3.\tThe proposed caption-guided replay paradigm is compatible with existing replay-based frameworks and can be seamlessly integrated to enhance replay fidelity and efficiency."}, "weaknesses": {"value": "1.\tAlthough the proposed method leverages a Multi-modal Large Language Model (MLLM) to generate captions for each image, enabling the text-to-image (T2I) model to produce high-fidelity, instance-aware samples, the overall generation process is computationally expensive. It involves three stages: (1) the MLLM generates instance-level captions, (2) the T2I model undergoes LoRA fine-tuning using these captions, and (3) the T2I model synthesizes replay samples. All three steps are highly time-consuming. While Table 8 reports the training time of the method, the reported duration (Real images: 403 ms; Instance-level Caption: 404 ms) is almost identical to that of real-image replay, suggesting that the additional computational overhead of these three processes may not have been included in the reported training time.\n2.\tUsing MLLMs to produce instance-level descriptions and T2I models to synthesize images can introduce domain-specific challenges. For fine-grained categories or cross-domain scenarios such as medical scenarios, these foundation models may lack the necessary domain knowledge, leading to noisy or invalid captions and images that fail to capture meaningful semantics.\n3.\tIn terms of novelty, the proposed multi-layer distribution-based distillation strategy lacks originality, as a similar concept has already been introduced in PODNet [1], which also employs MSE loss for multi-layer distribution distillation in continual learning. The authors should clearly explain the differences between their approach and PODNet to justify their contribution.\n4.\tThe paper also lacks a clear description of the preliminary. Continual learning encompasses various settings, including class-incremental, task-incremental, domain-incremental, and online learning. From the context, it appears that this work adopts a cross-dataset continual learning setup, but this should be explicitly stated in the methodology section.\n5.\tWhile the proposed approach performs well in cross-dataset continual learning, the domain gap between datasets is substantial. It would be valuable to investigate whether the method can be applied to intra-dataset class-incremental learning, particularly on fine-grained datasets such as CUB [2] or Cars [3], and how does it perform compared to existing baselines?  This is especially relevant since the method relies on a T2I model—whether such a model can generate fine-grained images that truly capture class-specific attributes remains an open question.\n6.\tThe comparison with state-of-the-art methods also raises concerns. For instance, MoE-Adapter [4] is a non-replay-based method that does not use generated old samples, which makes the comparison potentially unfair. The authors should clearly indicate in the results table whether each compared method employs replay, and if so, whether it uses real or synthetic samples.\n7.\tThe organization of the paper could be improved. The Introduction section should avoid including excessive experimental results and instead focus on motivation and preliminary observations directly related to the research question.\n8.\tThe manuscript contains several typographical and formatting errors (e.g., ”***”), which should be carefully corrected throughout the paper.\n[1] Douillard A, Cord M, Ollion C, et al. Podnet: Pooled outputs distillation for small-tasks incremental learning[C]. ECCV. 2020.\n[2] Wah C, Branson S, Welinder P, et al. The caltech-ucsd birds-200-2011 dataset[J]. 2011.\n[3] Krause J, Stark M, Deng J, et al. 3d object representations for fine-grained categorization[C]. ICCV. 2013.\n[4] Yu J, Zhuge Y, Zhang L, et al. Boosting continual learning of vision-language models via mixture-of-experts adapters[C]. CVPR. 2024."}, "questions": {"value": "1.\tThe proposed method involves three computationally intensive stages: MLLM caption generation, LoRA fine-tuning, and image synthesis. However, the training time reported in Table 8 is almost identical to that of real-image replay. Could you please clarify exactly which processes are included in this reported time? Specifically, does it encompass the full overhead of the MLLM inference and the T2I model fine-tuning steps? If not, please provide a more comprehensive breakdown of the computational cost.\n2.\tThe method relies on general-purpose foundation models (MLLM and T2I). How does it handle domain-specific scenarios, such as medical imaging or fine-grained categories, where these models may lack the necessary knowledge? What evidence or experiments can you provide to show that the generated captions and images in such domains are semantically meaningful and not noisy?\n3.\tThe multi-layer distribution-based distillation strategy bears a strong resemblance to the approach used in PODNet, which also employs MSE loss for multi-layer distribution distillation. Could you explicitly delineate the key differences between your distillation strategy and that of PODNet? Please justify the novelty of your contribution in this specific aspect.\n4.\tThe paper does not explicitly define the continual learning setting it adopts (e.g., class-incremental, task-incremental, domain-incremental). Based on the experiments, it seems to be a form of domain-incremental or cross-dataset continual learning. Please clearly state and formally define the exact continual learning setting used in the methodology section.\n5.\tWhile the method is evaluated on cross-dataset scenarios with large domain gaps, its performance on standard class-incremental learning within a single fine-grained dataset (e.g., CUB, Cars) remains unclear. Can you demonstrate the method's effectiveness in such a setting? Given the reliance on a T2I model, can it reliably generate high-fidelity images that preserve the subtle, class-specific attributes necessary for fine-grained classification?\n6.\tThe comparisons include methods like MoE-Adapter, which is a non-replay-based approach. To ensure a fair and transparent comparison, please clearly indicate in the results tables (e.g., with a footnote or a separate column) which compared methods use a replay strategy and, if so, whether they use real or synthetic samples for replay.\n7.\tThe Introduction section currently includes extensive experimental results. It is recommended to streamline this section by focusing on the motivation, background, and the specific research gaps this work aims to address. Detailed experimental analysis should be reserved for the later sections to improve the paper's narrative flow.\n8.\tThe manuscript contains several typographical and formatting errors (e.g., ”***”). Please perform a thorough proofreading of the entire document to correct these issues and improve the overall presentation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ys0gGeFVIr", "forum": "HN18kuyf4o", "replyto": "HN18kuyf4o", "signatures": ["ICLR.cc/2026/Conference/Submission5155/Reviewer_DtAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5155/Reviewer_DtAe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761129074503, "cdate": 1761129074503, "tmdate": 1762917913222, "mdate": 1762917913222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a caption-guided generative replay framework for continual learning with vision-language models. Instead of relying on coarse class-level prompts, it stores instance-level captions generated by a multimodal LLM and reconstructs past samples using a LoRA-adapted text-to-image model, enabling high-fidelity, instance-aware replay with minimal storage. Furthermore, it identifies and addresses feature drift—representation shifts during sequential learning—through a distribution-based distillation strategy that aligns intermediate feature distributions. Together, these techniques enhance both replay quality and model stability in continual learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The experiments in this paper are extensive, analyzing the proposed method from different perspectives.\n2. This paper features a clear structure and method diagram, and is easy to understand"}, "weaknesses": {"value": "1. Training the T2I model and ingesting a multimodal LLM will bring additional training costs. Is it necessary to ingest two large-scale models to train the CLIP model?\n2. Authors emphasize in the abstract and introduction that the simple prompts constructed by using class name and CLIP templates in existing methods have limitations, but they still use class name in the comparison in Figure 1. This cannot well provide support for the claimed motivation.\n3. The compared methods, such as GIFT and LoRA-Loopd, both use SD1.5 as the T2I model. Why does this paper adopt SD3-Medium? This may cause a discussion on comparison fairness. And from Table 4(b), SD3-Medium does not bring obvious performance boosts.\n4. The ablation study provided by the authors do not showcase the actual baseline, which may confuse readers about the results. Row 3 of Table 3, when using class name and template, already outperforms all the comparison methods. Is this due to the use of a stronger baseline model?\n5. What is the core innovation of DistKD? Distilling the hidden states at the middle layers of the model is a common technique."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sHQeoyQVWa", "forum": "HN18kuyf4o", "replyto": "HN18kuyf4o", "signatures": ["ICLR.cc/2026/Conference/Submission5155/Reviewer_KkVw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5155/Reviewer_KkVw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571998170, "cdate": 1761571998170, "tmdate": 1762917912960, "mdate": 1762917912960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CaRD to address catastrophic forgetting in VLMs for continual learning. The method has two key components: a caption-guided replay paradigm that reconstructs high-fidelity past images using instance-level captions and a LoRA-adapted T2I model, and a multi-layer distribution distillation method to mitigate the feature drift. Experiments show the method outperforms existing approaches on the MTIL and X-TAIL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and the proposed method is presented clearly.\n2. The proposed caption-guided replay paradigm is a well-motivated and intuitive innovation and the visualization of feature drift in Fig. 1b is insightful.\n3. The experimental evaluation is thorough including MTIL and X-TAIL with full and few-shot settings."}, "weaknesses": {"value": "1. The method introduces significant computational overhead, which raises questions about its practical feasibility. For each new task, it requires running a large VLM to generate captions for all training samples and separately fine-tuning a T2I model to obtain task-specific LoRA weights.\n2. The novelty of the distribution distillation component is limited. While successfully applied to VLM continual learning, the core idea of matching the mean and covariance of feature maps is a well-explored concept in knowledge distillation by Lv et al [1].\n3. The related work section has some omissions. The discussion on replay could be strengthened by including methods based on a series of prototype augmentation methods [2] [3]. The method LADA, which is used as a key comparison baseline in Tab.10 and 11, is not introduced in the related work section.\n4. The paper's presentation could be strengthened by reorganizing the main experimental results. The authors might consider moving the X-TAIL full-shot results into the main paper while retaining the MTIL full-shot results. The MTIL 5-shot setting, while valuable, could be moved to the appendix to make space for the more challenging task-agnostic X-TAIL benchmark.\n\n\nReference:\n\n[1] Lv, Jiaming, Haoyuan Yang, and Peihua Li. \"Wasserstein distance rivals kullback-leibler divergence for knowledge distillation.\" *Advances in Neural Information Processing Systems* 37 (2024).\n\n[2] Zhu, Fei, et al. \"Prototype augmentation and self-supervision for incremental learning.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2021.\n\n[3] Asadi, Nader, et al. \"Prototype-sample relation distillation: towards replay-free continual learning.\" *International conference on machine learning*. PMLR, 2023.\n\n[4] Luo, Mao-Lin, et al. \"LADA: Scalable Label-Specific CLIP Adapter for Continual Learning.\" *Forty-second International Conference on Machine Learning*. PMLR, 2025."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "juYHKohIwv", "forum": "HN18kuyf4o", "replyto": "HN18kuyf4o", "signatures": ["ICLR.cc/2026/Conference/Submission5155/Reviewer_LRVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5155/Reviewer_LRVX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898344534, "cdate": 1761898344534, "tmdate": 1762917912667, "mdate": 1762917912667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce CaRD, a methodology designed to mitigate catastrophic forgetting in Continual Learning using Vision-Language Models (VLMs). CaRD is characterized by the following components:\n\n - Generative Replay: the method leverages QWen2.5-VL to generate captions for each instance, which are stored in a replay buffer. A Text-to-Image (T2I) model -- specifically, Stable Diffusion 3 (medium) -- is then trained to synthesize high-quality images for the current task. For each task, the T2I model is fine-tuned with a distinct set of LoRA modules, which are preserved across tasks. During subsequent training, these task-specific LoRA adapters can be reloaded individually to generate accurate synthetic samples representing all previous tasks.\n\n- Knowledge Distillation, which employs the model from the most recent task as the teacher. The distillation loss combines, in a layer-wise fashion, the Wasserstein distance between feature statistics (means and covariances) and the squared norm of the differences between raw feature representations.\n\nExperiments are carried out on standard VLM continual learning benchmarks, such as MTIL, under both full-shot and 5-shot configurations, and evaluated across two different task orderings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Experiments have been conducted on a variety of scenarios, and additional metrics are provided in the Appendix, validating the proposed approach. CaRD outperforms all competitors in standard benchmarks for CL with VLMs.\n- The paper is well written and easy to follow.\n- The ablation studies justify many of the authors' choices for their pipeline."}, "weaknesses": {"value": "1. Total training time: in Table 8, the authors report that CaRD does not introduce a substantial computational overhead. However, it appears that the reported training time only accounts for a single forward and backward pass, hence reflecting only the minor overhead caused by the additional per-layer loss terms. It is unlikely that the table includes the time required to fine-tune the diffusion model. A more comprehensive comparison of CaRD’s total training time against that of other methods would be valuable to fairly assess its overall efficiency and trade-offs.\n\n2. Model sizes: CaRD relies on two large auxiliary models -- a Text-to-Image (T2I) model, Stable Diffusion 3-medium (≈2B parameters), and a Multimodal LLM (MLLM) captioner, QWen2.5-VL (≈7B parameters) -- totaling roughly 9B parameters external to the trained model. In contrast, the main VLM used for continual learning is CLIP with a ViT-B/16 vision backbone, which contains around 150M parameters. This means that the auxiliary \"helper\" models have roughly 60 times more parameters than the model being trained, giving CaRD an unfair advantage over competing methods that do not employ such massive external models."}, "questions": {"value": "See \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p5sshxnuBC", "forum": "HN18kuyf4o", "replyto": "HN18kuyf4o", "signatures": ["ICLR.cc/2026/Conference/Submission5155/Reviewer_J4Rf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5155/Reviewer_J4Rf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955192958, "cdate": 1761955192958, "tmdate": 1762917912397, "mdate": 1762917912397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}