{"id": "Er7Jkzu3ox", "number": 15407, "cdate": 1758251030997, "mdate": 1763299615193, "content": {"title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "abstract": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge across multiple modalities, play a pivotal role by complementing the implicit knowledge of Multimodal Large Language Models (MLLMs) and enabling more grounded reasoning via Retrieval Augmented Generation (RAG).\nHowever, existing MMKGs are generally limited in scope: they are often constructed by augmenting pre-existing knowledge graphs, which restricts their knowledge, resulting in outdated or incomplete knowledge coverage, and they often support only a narrow range of modalities, such as text and visual information.\nThese limitations restrict applicability to multimodal tasks, particularly as recent MLLMs adopt richer modalities like video and audio.\nTherefore, we propose the Visual-Audio-Text Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive multimodal knowledge graph that covers visual, audio, and text information, where each triplet is linked to multimodal data and enriched with detailed descriptions of concepts.\nSpecifically, our construction pipeline ensures cross-modal knowledge alignment between multimodal data and fine-grained semantics through a series of stringent filtering and alignment steps, enabling the automatic generation of MMKGs from any multimodal dataset.\nWe further introduce a novel multimodal RAG framework that retrieves detailed concept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical value in unifying and leveraging multimodal knowledge.", "tldr": "We propose VAT-KG, the first knowledge-intensive and concept-centric multimodal knowledge graph integrating visual, audio, and text data, enhancing multimodal tasks by providing detailed, modality-correlated descriptions.", "keywords": ["knowledge graph", "multimodal", "rag"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/61494a8e7c5fdaee37625c09606b8d726a069dde.pdf", "supplementary_material": "/attachment/7ae5f3160c3d7b744c708a1efbb71d3b13177b7e.zip"}, "replies": [{"content": {"summary": {"value": "This work aims to construct a multimodal knowledge graph that (i) contains text, image, video, and audio modalities, and (ii) is aligned with text descriptions. This allows VAT-KG to reach new knowledge coverage over previous MMKGs. The authors also provide a RAG baseline that utilizes the new features introduced by VAT-KG, which demonstrates superiority experimentally."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. VAT-KG has achieved new height of knowledge coverage.\n2. The construction pipeline is carefully designed, with very rigorous filtering rules to ensure high quality of the final outcome.\n3. The experiments have shown consistent improvement."}, "weaknesses": {"value": "1. (Major) It seems that the entire construction pipeline focuses on refining the text modality, while the data of other modalities remain the same as what the initial corpus contain. Do the authors plan to have additional multimodal data crawled from the web?\n2. (Minor) While this MMKG includes both image and video modalities, the paper uses \"visual\" to denote both of them early on. This might cause some confusion, making readers think there are only three modalities.\n3. (Minor) There are two keywords in the title: knowledge intensive and RAG. In the introduction section, the first keyword is illustrated in detail, but I feel there needs to be more discussion on the second keyword. Specifically, how is VAT-KG specially designed for the RAG purpose?\n4. (Minor) It appears that the authors are not using \\citet and \\citep correctly for in-text and parenthesis citations."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jbLwFQf6GR", "forum": "Er7Jkzu3ox", "replyto": "Er7Jkzu3ox", "signatures": ["ICLR.cc/2026/Conference/Submission15407/Reviewer_Bjtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15407/Reviewer_Bjtk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761098926517, "cdate": 1761098926517, "tmdate": 1762925684741, "mdate": 1762925684741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "MR6xKgYzlr", "forum": "Er7Jkzu3ox", "replyto": "Er7Jkzu3ox", "signatures": ["ICLR.cc/2026/Conference/Submission15407/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15407/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763299614486, "cdate": 1763299614486, "tmdate": 1763299614486, "mdate": 1763299614486, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a concept-centric visual-audio-text trimodal knowledge graph (VAT-KG). A four-stage construction process is designed, comprising multimodal alignment filtering, knowledge-intensive recaptioning, multimodal triple grounding, and cross-modal description alignment. The authors employ VAT-KG as the external knowledge base for video, audio, and audio-video understanding tasks. By incorporating modality-independent retrieval and retrieval verification, the knowledge from VAT-KG is injected into baseline models such as Video-LLaMA2 and Qwen2.5-Omni, leading to improved performance for both."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for building a joint visual-audio-text trimodal knowledge graph is meaningful and offers promising prospects for multimodal understanding tasks.\n2. By incorporating knowledge from the constructed VAT-KG, the baseline models Video-LLaMA2 and Qwen2.5-Omni achieve modest performance improvements on video, audio, and audio-video understanding tasks."}, "weaknesses": {"value": "1. First, from the results in Table 2, the performance improvement of the proposed method on VCG and AVQA is marginal. In fact, I believe that datasets such as AudioCaps-QA, VCGPT, and AVQA, which rely more on common sense than knowledge, are not well suited for knowledge graph applications. The authors should focus more on knowledge-related video and audio understanding datasets to better leverage the power of knowledge graphs. In addition, a comprehensive comparison with existing state-of-the-art open-source and closed-source MLLMs should be provided, rather than showing improvements only over the chosen baselines.\n2. During the construction of the knowledge graph, how are entities and concepts distinguished? In Figure 2, terms like “bridge” and “helicopter” appear to be entities as well.\n3. In L197, why are explanatory videos with background music removed? What is the underlying insight behind this design?\n4. In L257, since the goal is to build a knowledge base for video retrieval, why are embeddings extracted from video frames rather than from segments? Segment-level representations may capture temporal semantics more effectively.\n5. In L303, the phrase “the number of multimodal data per concept” is ambiguous, what exactly does “number of multimodal data” refer to in this context?\n6. When performing retrieval checks, is it appropriate to match the query embedding with the text embedding of the triple? The prompt \"Describe in detail...\" for description tasks, as shown in Figure 6, does not seem to effectively match concepts in the knowledge graph."}, "questions": {"value": "The reference format used in this paper seems unusual. For example, \"retrieval Alberts et al. (2021)\" in L37 is often written as \"retrieval (Alberts et al., 2021).\"\nMy concerns with this paper are detailed in the weaknesses section, and I would be happy to discuss them with the authors to further consider my final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTk8hAWjzd", "forum": "Er7Jkzu3ox", "replyto": "Er7Jkzu3ox", "signatures": ["ICLR.cc/2026/Conference/Submission15407/Reviewer_ha9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15407/Reviewer_ha9f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540939766, "cdate": 1761540939766, "tmdate": 1762925684277, "mdate": 1762925684277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VAT-KG, a knowledge-intensive, concept-centric multimodal knowledge graph that includes visual, audio, and textual modalities, specifically designed to enable RAG for MLLMs. The authors detail a rigorous four-stage pipeline for VAT-KG construction, emphasizing stringent cross-modal alignment and concept-level knowledge enrichment. The work further proposes a multimodal RAG framework that retrieves semantically aligned, detailed concept descriptions for queries from arbitrary modalities. Extensive experiments on multiple QA benchmarks demonstrate that VAT-KG consistently boosts MLLMs' performance over previous multimodal and text-only knowledge graphs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors clearly identify a central limitation of existing MMKGs that they provide shallow or entity-centric structures and narrow modality coverage, which hinders their utility for retrieval-augmented generation with state-of-the-art MLLMs. VAT-KG’s ambition to unify video, image, audio, and text into a richly described, concept-level knowledge graph fills a visible gap and aligns with unmet needs in the community.\n2. The paper introduces a comprehensive and well-justified pipeline that goes beyond simple data aggregation, while generating a dataset for MMKG."}, "weaknesses": {"value": "1. I am very concerned about the fairness of the comparison in this article. Since the knowledge contained in VAT-KG is not universal but case-specific. As shown in the sample in Figure 2, the knowledge of an airplane flying over only appears in this picture, while existing MMKG methods are usually targeted at general knowledge retrieval scenarios. I think this comparison is not fair.\n2. There have been many works on MMKG-related datasets in recent years. The characteristics of the VT-KG dataset are limited, which restricts the contribution of the paper.\n3.  While the ablation on retrieval strategies in Figure 12 is insightful, the empirical analysis remains heavily oriented towards the effectiveness of integrating VAT-KG into generic QA. There is less attention paid to dissecting which aspects of the visual, audio, or video modalities most impact performance in challenging or ambiguous scenarios, or how VAT-KG’s design specifically delivers benefits for multi-modality vs. strong single-modality queries."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P1iXNHBdpR", "forum": "Er7Jkzu3ox", "replyto": "Er7Jkzu3ox", "signatures": ["ICLR.cc/2026/Conference/Submission15407/Reviewer_fjZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15407/Reviewer_fjZJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706143826, "cdate": 1761706143826, "tmdate": 1762925683713, "mdate": 1762925683713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VAT-KG, a large-scale, concept-centric multimodal knowledge graph that text, image, audio, and video modalities. The paper proposes a four-step automated pipeline to construct VAT-KG from Video-Audio-Text Corpora, Encyclopedia and LLM, and demonstrate its use in a multimodal RAG system. Experiments on AQA, VQA, and AVQA tasks show consistent performance improvements over existing knowledge graphs such as Wikidata5M, VTKG, and M2ConceptBase.\n\nVAT-KG is the first MMKG covering all four major perceptual modalities with concept-level descriptions. It provides a useful dataset for future research on multimodal reasoning and retrieval. However, the paper lacks a clear explanation of the specific problem that VAT-KG is designed to solve. It focuses on expanding modality coverage rather than addressing a well-defined knowledge-intensive reasoning challenge. The proposed multimodal RAG system is essentially a traditional RAG pipeline employing existing encoders (e.g., CLIP, CLAP) with VAT-KG as a new data source. No new retriever, alignment, or reasoning mechanism is introduced. In the experiments, the reported performance gains may result from diverse reasons (e.g. dataset bias or contextual enrichment) rather than genuine reasoning improvements. The paper does not analyze the underlying causes of these gains.\n\nThe dataset is an valuable contribution. However, the author aims to elevate it into a work that boasts great innovation in both data and methodology. This mismatch has resulted in a very awkward and convoluted writing logic in the paper.\n\nOverall, this paper represents a good dataset and system contribution but is limited in methodological innovation, making it more suitable for data-centric or knowledge graph venues than for modeling-focused conferences."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces VAT-KG. It is the first concept-centric multimodal knowledge graph that simultaneously covers text, image, audio, and video modalities, providing a potentially influential data resource for future multimodal reasoning research involving video and audio modalities.\n\n2. The four-stage construction pipeline for building VAT-KG is clearly described and reproducible, demonstrating strong system integration and engineering quality. It comprises multimodal alignment filtering, knowledge-intensive recaptioning, triplet extraction, and cross-modal description alignment.\n\n3. VAT-KG, together with the proposed RAG framework, shows significant performance improvements on audio, video, and audio-visual QA tasks, indicating the practical utility of VAT-KG’s concept-level descriptive knowledge in perception-driven scenarios."}, "weaknesses": {"value": "Seen in Questions."}, "questions": {"value": "1. VAT-KG\n\n1.1 Motivation and Novelty of VAT-KG\n\nThe contribution of VAT-KG lies in engineering integration rather than in a cross-modal semantic breakthrough. Modality diversity serves more as a narrative hook than as a justified research necessity. The paper does not clearly explain why constructing such a knowledge graph covering 4 modalities is scientifically or methodologically important.\n\nAs shown in Figure 1, the contrast among (a) non-concept-centric, (b) concept-centric with limited modalities, and (c) the proposed VAT-KG only shows that no one has done this combination before, which alone does not constitute a research challenge.\n\nThe true challenge should stem from non-trivial difficulties encountered when extending concept-centric MMKGs beyond image–text pairs, such as semantic granularity mismatches across modalities, one-to-many or many-to-one mapping conflicts, or other issues introduced by incorporating new modalities.\nIf the motivation were reframed around the requirements of specific knowledge-intensive downstream tasks and the alignment challenges introduced by the mere engineering integration of modalities, rather than simple modality expansion, then VAT-KG would present itself as a genuinely challenging and meaningful contribution.\n\n1.2 Why are relationships in VAT-KG based on co-occurrence rather than knowledge-intensive concept-level semantics?\n\nThe example triplet in Figure 2 (\"helicopter flies over bridge\") reflects co-occurrence rather than knowledge-intensive concept-level semantics. The visual and audio modalities simply co-occur, without showing clear reasoning or interaction between them. Thus, this example underplays VAT-KG’s claimed conceptual strength.\n\nA more convincing example would show how different modalities work together. For instance, how sounds match events or how visual and audio cues help reasoning. Otherwise, the proposed VAT-KG is only a dataset / knowledge base, not a graph.\n\n2. The proposed RAG Framework\n\nIn this paper, the proposez multimodal RAG framework simply applies existing multimodal encoders to retrieve concept descriptions from VAT-KG and feeds these descriptions into an MLLM for generation. Thus, the so-called framework is more as a standard RAG pipeline operating on a new multimodal knowledge source, rather than as a newly improved RAG method. It introduces no new architectural design or reasoning component: no new retriever structure, no new cross-modal encoder, no new fusion or interaction mechanism, and no new retrieval scoring or reranking strategy.\n\nA innovative multimodal RAG need to address new challenges specific to VAT-KG, such as modality alignment issues, granularity mismatches, or other cross-modal inconsistencies introduced by additional modalities.\n\n3. Soundness of Experimental Results\n\nThe claims about VAT-KG’s experimental performance improvements are overstated and require clearer justification. The experiments validate VAT-KG’s utility as a multimodal retrieval source but do not establish its necessity for addressing knowledge-intensive multimodal reasoning or mitigating hallucination (mentioned in line 472).\n\n3.1 Overstated: Downstream Task \n\nThe reported gains mainly appear on perception-oriented tasks (AQA, VQA, AVQA), such as “What instrument is playing?” or “What sound occurs?”, where the improvement may be from the closer modality match between VAT-KG and the benchmarks. VAT-KG’s closer modalities match to the benchmarks. The authors should clarify whether the performance gains come from complementary multimodal information or simply from modality alignment. If it is the latter, a multimodal knowledge base would suffice, rather than a knowledge graph, and therefore relation extraction is not needed.\n\n3.2 Unfair Comparison between VAT-KG and other MMKGs\n\nThe comparison against Wikidata5M, VTKG, and M2ConceptBase is inherently biased, as these baselines lack audio/video modalities, giving VAT-KG a built-in advantage in video/audio tasks.\nThus, the experiments effectively measure modality matching rather than knowledge effectiveness.\n\n3.3 Missing Convincing Explanation of Improvements\n\nThe logic in this paper is circular: you build a new KG covering video and audio along with descriptions ${\\rightarrow}$ use it in RAG  ${\\rightarrow}$  outperform other KGs without video or audio  ${\\rightarrow}$  therefore it works. The logic seems to be incorrect. \nThis paper doesn't explain why other MMKGs perform poorly or why VAT-KG performs better except the only expaned modalities resouces.\nTo strengthen the argument, the authors should identify whether the gains are from inclusion of extra modalities or better knowledge structure and alignment of VAT-KG.\n\n4. Writing Issues\n\n4.1 Overclaim\n\nThe claim about the generality of its construction pipeline is overstated.\n\nFor example, Lines 93–95:\n\n> \"We introduce an effective pipeline for constructing multimodal knowledge graphs from arbitrary multimodal corpora…\"\n\nThe proposed pipeline is only validated on video, audio, and text modalities, without evidence of generalizability to other types such as sensor data, 3D information, or gesture-based modalities.\n\nTherefore, the pipeline should be described as a modality-specific integration system, not a universal multimodal construction framework.\n\n4.2 Logical Inconsistency\n\nLine 15-19:\n\n> \"However, existing MMKGs are generally limited in scope: they are often constructed by augmenting pre-existing knowledge graphs, which restricts their knowledge, resulting in outdated or incomplete knowledge coverage, and they often support only a narrow range of modalities, such as text and visual information.\"\n\nThe discussion of prior MMKG limitations conflats two distinct issues: \n\n1\\) Outdated knowledge caused by a lack of timely updates and maintenance.\n\n2\\) Limited modality coverage because existing KGs only support text–image data.\n\nThese are independent problems with different causes and implications, yet the paper presents them as a single causal chain.\nExtending to new modalities does not inherently address the problem of outdated or incomplete knowledge.\nThis paper should clarify which limitation VAT-KG primarily aims to resolve.\n\n5. Suggestions for Revision\n\n5.1 Motivation Should be Clear\n\nThe paper’s current motivation is not clear: Existing MMKGs lack modalities ${\\rightarrow}$  We build a more complete MMKG ${\\rightarrow}$  It improves downstream tasks involving these modalities. This coverage-driven narrative makes the work appear as a dataset extension rather than a solution to a concrete knowledge-intensive problem. \n\nA clearer motivation would show when perception alone fails and world knowledge is needed. VAT-KG should then be presented as a structured bridge to close this gap.\n\n5.2 Reframing Based on Figure 6 (a very good example)\n\nThe insight of the paper lies not in its multimodal coverage but in the knowledge-guided perception phenomenon illustrated in Figure 6. \nThis example shows that MLLMs often misinterpret perceptual inputs due to the lack of video conceptual knowledge, while VAT-KG provides structured knowledge that corrects these perceptual errors (e.g., identifying a \"synthesizer\" rather than a \"keyboard\").\n\nReframing the paper around this problem would transform VAT-KG from a data-engineering contribution into a substantive study of knowledge-intensive multimodal reasoning. A data-engineering contribution is more suited for venues emphasizing knowledge graph construction, data systems, or multimodal data management (e.g., ICDE, CIKM) ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oJYSRgPBtd", "forum": "Er7Jkzu3ox", "replyto": "Er7Jkzu3ox", "signatures": ["ICLR.cc/2026/Conference/Submission15407/Reviewer_G1kN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15407/Reviewer_G1kN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990072908, "cdate": 1761990072908, "tmdate": 1762925683288, "mdate": 1762925683288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}