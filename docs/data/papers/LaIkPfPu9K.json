{"id": "LaIkPfPu9K", "number": 6720, "cdate": 1757993436376, "mdate": 1763611323561, "content": {"title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention", "abstract": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named **Dolphin**. For visual feature extraction, we develop **DP‑LipCoder**, a dual‑path lightweight video encoder that transforms lip‑motion into discrete audio‑aligned semantic tokens. For audio separation, we construct a lightweight encoder–decoder separator, in which each layer incorporates a global–local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50\\% fewer parameters, more than 2.4$\\times$ reduction in MACs, and over 6$\\times$ faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at https://dolphin-avss.github.io/Dolphin.", "tldr": "", "keywords": ["Audio-video speech separation", "vector quantization", "lightweight network", "discrete semantic units"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9184f76dfabc879b10e69a48444b9ca247847a48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Dolphin, a compact and efficient audio-visual speech separation framework that integrates visual motion cues with attention-based processing to isolate target voices. It employs DP-LipCoder, a lightweight dual-path video encoder that learns discrete visual representations aligned with speech, and a single-iteration separator with global–local attention for efficient feature modeling. Through its optimized design, Dolphin achieves superior separation quality with over 50% fewer parameters, 2.4× lower computational cost, and 6× faster inference, making it well-suited for real-time and edge deployment."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "For the AVSS task, we designed a highly efficient model that outperforms existing approaches in key metrics such as SI-SNRi and SDRi, while achieving a 50% reduction in parameters and 2.4× lower computational cost, effectively balancing the trade-off between performance and efficiency.\n\nAlso, throughout the paper, reasonable architectural designs such as the single-pass GLA mechanism and the DP-LipCoder, which quantizes lip movements into discrete tokens, were employed to reduce computational cost while maintaining high performance. The effectiveness of these architectural innovations is well supported by the experimental results."}, "weaknesses": {"value": "There is a lack of ablation studies for individual components. In particular, an additional experiment comparing the performance without using VQ, which plays a crucial role in this work, would be valuable.\n\nMoreover, the paper lacks sufficient analysis of the individual contributions of global and local attention within the GLA blocks, as well as comparisons with other audio-visual fusion strategies\n\nIt is also unclear whether Dolphin can maintain its performance without a pretrained teacher model such as AV-HuBERT.\n\n* Minor weaknesses\n    * Line 234: The link for Figure 3 should include the letter “F.”\n    * For better experimental transparency, it would be helpful to indicate which results in Table 3 were taken from the original papers."}, "questions": {"value": "* It is unclear whether the encoders in the Semantic Path and Reconstruction Path shown in Figure 2 are trained separately or if there is parameter sharing between them. A more detailed explanation would be helpful.\n\n* For the results reported in Table 2, it is not specified whether the performance was measured after retraining the model with DP-LipCoder or simply by replacing the visual encoder without additional training. Clarification on this point is needed.\n\n* Including a discussion on the limitations of this work would make the paper more comprehensive and could provide valuable guidance for future research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P2hFzh7Urg", "forum": "LaIkPfPu9K", "replyto": "LaIkPfPu9K", "signatures": ["ICLR.cc/2026/Conference/Submission6720/Reviewer_mwcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6720/Reviewer_mwcV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828380426, "cdate": 1761828380426, "tmdate": 1762919010480, "mdate": 1762919010480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an audio-visual model for speech separation. The main contribution is a model which achieves SOTA results and is also lightweight. Results on multiple datasets and an ablation study are presented."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The presentation of an audio-visual speech seperation model which achieves SOTA performance and is also lightweights is a useful contribution.\n- Results on multple datasets and detailed ablation study."}, "weaknesses": {"value": "- The title is a bit misleading. The model recovers the speech of the main speaker only, and not all speech signals at the same time. So speech enhancement might have been a better term for this task (and it's more consistent with the literature).\n- The paper does not compare with some recent SOTA models, e..g. \"LA-VocE: Low-SNR audio-visual speech enhancement using neural vocoders\n- It's not clear what results (in terms of number of background speakers) are presented in Section 5. Given that the appendix includes results with multiple background speakers, then these results probably include 1 background speaker. Also, why not testing the model's performance on different types of background noise and combination of background noise+multiple speakers.\n- No supplementary material is provided, so it's hard to judge how good the denoised samples are. The numbers look good, but without listening to examples it's hard to judge the quality. Some examples are provided on the provided link, but they are too few. There are only 6 examples where the model is compared to other models. Would also be good if some details about the type of noise in each example are added."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IOmYB64DwX", "forum": "LaIkPfPu9K", "replyto": "LaIkPfPu9K", "signatures": ["ICLR.cc/2026/Conference/Submission6720/Reviewer_1MMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6720/Reviewer_1MMg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011230000, "cdate": 1762011230000, "tmdate": 1762919009575, "mdate": 1762919009575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles audio-visual speech separation (AVSS). The core problem it targets is that current SOTA methods either achieve good separation quality but rely on a heavy visual backbone plus multiple iterations of the separator, or they are lightweight but require repeated iterations, which hurts real-time performance and deployability. The authors propose an overall framework called Dolphin, with two main components: a lightweight dual-path video encoder DP-LipCoder and a single-iteration encoder–decoder separator with Global–Local Attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It makes explicit a long-standing but very real issue in AVSS that the visual encoder is too heavy, and then deliberately designs a dual-path + VQ + distillation structure on the visual side. This is a more thoughtful route than simply plugging in an off-the-shelf lipreading backbone or adding a small autoencoder. It jointly considers both reconstructive and semantic aspects, rather than just trimming the backbone.\n\nThe separator also states the efficiency goal of replacing multiple iterations with a single pass quite clearly, and then compensates for it with global and local components."}, "weaknesses": {"value": "The paper clearly states that compressing lip-reading backbones causes semantic loss, and that purely reconstruction-oriented lightweight encoders only capture shallow, pixel-level cues. It then proposes a dual-path design with VQ-based discrete semantics plus distillation to address this. However, the mechanism of why the dual-path + discrete tokens specifically preserve the task-relevant semantics under heavy compression is mostly justified empirically (Table 1), rather than analytically. A more explicit explanation or diagnostic would strengthen the claim."}, "questions": {"value": "The proposed GLA-based separator is presented as a single-pass design for efficiency. However, since the block itself is a generic feature transform, it seems technically feasible to unroll it for multiple passes (as commonly done in audio-visual separation) to trade extra compute for further gains. I would like to see how the model performs under multiple iterations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yyBMBqT9HX", "forum": "LaIkPfPu9K", "replyto": "LaIkPfPu9K", "signatures": ["ICLR.cc/2026/Conference/Submission6720/Reviewer_Pm3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6720/Reviewer_Pm3b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176327563, "cdate": 1762176327563, "tmdate": 1762919009160, "mdate": 1762919009160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a more computationally efficient AV speech separation system called Dolphin. As part of this system, the author's also present a lightweight visual encoder that maps video frames to semantic, well-aligned-to-audio discrete tokens. Together they yield state-of-the-art quality across a variety of corpora and metrics while being fairly computationally efficient."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates an approach for building a state-of-the-art speech separation system. They modify TDANet, moving from iterative, progressive separation to single separation step for efficiency with joint local and global attention for each layer for a large win on efficiency. They also note the need for improving throughput in the vision encoder, which encodes lip motion frames into discrete semantic tokens, by developing a fast, high quality model called DP-LipCoder. Third, for separation they use an efficient, mixed global/local attention encoder/decoder. These innovations yield a system that surpasses many recent AV speech separation systems.\n\nFurther, the system is efficient showing less memory use, fewer MACs, faster GPU inference than many competing systems. \n\nThe paper will provide a github repo that will allow others to fully reproduce results as much as possible."}, "weaknesses": {"value": "In some ways, this paper is an incremental approach to AVSS, not providing many new insights, however strong the results are. Critics may wonder if human evaluations/side-by-sides would agree that the Dolphin system is indeed superior to the other approaches in terms of quality improvement and fewer artifacts. Lastly, the tasks all appear to be artificially created speaker overlap and equation (1) is a large simplication--with the Lombard effect the noise causes non-linear changes to speech production."}, "questions": {"value": "How well does the Dolphin system do with real overlapping speakers?\nHave conducted any human listening tests / comparative evals against other systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NK0VFUTyQ7", "forum": "LaIkPfPu9K", "replyto": "LaIkPfPu9K", "signatures": ["ICLR.cc/2026/Conference/Submission6720/Reviewer_Qs5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6720/Reviewer_Qs5W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6720/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762749171091, "cdate": 1762749171091, "tmdate": 1762919008717, "mdate": 1762919008717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}