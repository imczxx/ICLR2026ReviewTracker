{"id": "YkV0fnXgJA", "number": 6426, "cdate": 1757982859067, "mdate": 1759897915268, "content": {"title": "EdiVal-Agent: An Object-Centric Framework for  Automated,    Fine-Grained Evaluation of Multi-Turn  Editing", "abstract": "Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images—resulting in limited coverage and inheriting biases from prior generative models—or (ii) rely *solely* on zero-shot vision–language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise.\n\nTo address this, we introduce **EdiVal-Agent**, an automated and fine-grained evaluation framework grounded in an object-centric perspective, designed to assess not only standard single-turn but also multi-turn instruction-based editing with precision. Given an input image, **EdiVal-Agent** first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions while dynamically updating object pools across turns. These two stages enable two novel object-centric metrics tailored for multi-turn evaluation and one global metric of visual quality: 1) EdiVal-IF, which measures instruction following by combining open-vocabulary object detectors for symbolic checks with VLMs for semantic verification on detector-guided crops; 2) EdiVal-CC, which evaluates content consistency by calculating semantic similarity of unchanged objects and background using the evolving object pools; and 3) EdiVal-VQ, which quantifies changes in overall visual quality with human preference models.\n\nInstantiating this pipeline, we build **EdiVal-Bench**, a multi-turn editing benchmark covering 9 instruction types and 13 state-of-the-art editing models spanning In-Context, flow-matching, and diffusion paradigms. We further conduct experiments comparing multi-turn editing with single-shot complex editing, highlighting the distinctive characteristics of different model paradigms. We demonstrate that EdiVal can be used to identify existing failure modes, thereby informing the development of the next generation of editing models.", "tldr": "", "keywords": ["Multi-turn editing benchmark", "image editing"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d14ffa24fdd594b5858644e20213b34eeb81e5a2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EdiVal-Agent, an object-centric, multi-turn framework for evaluating instruction-based image editing. It generates scene-aware instructions and scores edits with three metrics—EdiVal-IF (instruction following via detector+VLM), EdiVal-CC (content consistency for unchanged regions), and EdiVal-VQ (visual quality)—and releases EdiVal-Bench across 9 instruction types and 13 editors."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem framing & object-centric design: The paper crisply motivates why existing reference-based and VLM-only evaluation is insufficient and positions an object-centric alternative that combines symbolic checks with semantic reasoning.\n- Well-specified agentic pipeline with dynamic object pools: The decomposition → instruction generation → evaluation workflow is precise."}, "weaknesses": {"value": "- Tool-dependence and potential evaluator bias: The pipeline fixes Grounding-DINO as the detector, Qwen-2.5-VL as the VLM, and DINOv3/HPSv3 as backbones for CC/VQ. While the authors say the pipeline is tool-agnostic, the reported results (e.g., human agreement advantage) are contingent on this specific stack. Please quantify sensitivity to swapping components (e.g., different detectors/VLMs), or report ablations where each tool is replaced by a strong alternative.\n- Metric coupling between IF and CC is under-analyzed: The geometric mean \\sqrt{\\text{IF}\\cdot \\text{CC}} is reasonable, but different applications may value preservation much more than instruction compliance (or vice versa). The choice is referenced to prior work but lacks a robustness study: how often do model rankings flip under other aggregations (harmonic/weighted means)?\n- Fairness of latency comparison: The table mixes closed-source UI latency with open-source single-GPU latency, which is not directly comparable (unknown batch sizes, backend scaling, caching). The paper acknowledges measurement contexts but still draws speed–quality conclusions. Consider removing cross-setting latency comparisons or normalizing via a controlled local runtime for open models and API budgeted throughput for closed models.\n- Multi-turn vs. single-shot comparison design: The conclusion that multi-turn can help when exposure bias is low is plausible, but details on prompt concatenation for single-shot “complex” edits matter (ordering, connectors, explicit constraints like “make X unchanged”). A stricter prompt engineering baseline for single-shot could narrow gaps; please share exact templates and run ablations with stronger single-shot compositions."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lSeDVzLrtq", "forum": "YkV0fnXgJA", "replyto": "YkV0fnXgJA", "signatures": ["ICLR.cc/2026/Conference/Submission6426/Reviewer_TgZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6426/Reviewer_TgZ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629297030, "cdate": 1761629297030, "tmdate": 1762918818853, "mdate": 1762918818853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EdiVal-Agent, an automated, object-centric framework for evaluating instruction-based image editing, including multi-turn scenarios. The agent (i) decomposes an input image into semantically meaningful objects, (ii) generates diverse, context-aware editing instructions across turns while tracking object pools, and (iii) evaluates model outputs with three metrics: EdiVal-IF (instruction following via symbolic checks with open-vocabulary detection plus VLM verification on crops), EdiVal-CC (content consistency for unchanged objects and background with DINO features / L1), and EdiVal-VQ (visual quality via HPSv3). Using this pipeline, the authors build EdiVal-Bench (572 images; 1,716 instructions; 9 types; 3 turns) and benchmark 13 editors spanning diffusion, flow-matching, and in-context AR systems. Key findings include 81.3% human-agreement for EdiVal-IF vs. 75.2% for a strong VLM baseline (Fig. 4, p.6), and clear trade-offs among systems (e.g., Seedream 4.0 best overall EdiVal-O; GPT-Image-1 strong IF but weaker consistency and high latency; Table 3, p.7)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing: Multi-turn editing evaluation is under-served; the object-centric perspective addresses spatial grounding, subtle changes, and compositionality (Sec. 1–2). \nMethodological novelty: EdiVal-IF combines symbolic checks (presence/absence, spatial relations, counts) with semantic VLM verification on detector-guided crops, improving precision and interpretability (Sec. 2.4; Eqs. (1)–(2), p.5). \nHuman agreement: 81.3% on IF vs. 75.2% VLM-only; inter-annotator agreement 85.5% bounds achievable performance (Fig. 4, p.6). \nComprehensive benchmark & analysis: 13 editors, 9 instruction types, 3 turns; rich breakdowns (per-task success, consistency via DINO/L1, visual quality HPSv3, exposure drift analysis with luminance p99/p999; Tables 3–4, 8–19; Figs. 5–9, 12–14). \nTransparency & reproducibility: Full prompts, thresholds, and pseudo-code (Appendix I), with limitations and failure cases (Appendix E). \nActionable insights: Identifies exposure bias in multi-turn (Fig. 5, p.7; Fig. 12, p.18), weaknesses in spatial/counting tasks (Figs. 6, 15), and model-family trade-offs between beautification vs. preservation (Fig. 3, p.5)."}, "weaknesses": {"value": "Detector dependence / error propagation: EdiVal-IF's symbolic branch hinges on Grounding-DINO; false positives/negatives can flip outcomes (failure case in Fig. 11, p.17). A more robust ensemble or confidence-calibrated aggregation could mitigate this. \nLimited edit taxonomy: Style change is excluded (Design Scope, p.6), which reduces coverage for prevalent real-world edits; future extensions are suggested but not explored. \nTool ablations & sensitivity: While the agent is \"tool-agnostic\", results rely on specific choices (Qwen-2.5-VL, DINOv3, HPSv3). A systematic tool swap study (e.g., different open-vocab detectors/VLMs/features) and threshold sensitivity analysis would strengthen claims of generality. \nCounting & spatial reasoning remain brittle: Even top systems struggle on count change (<25% at T1) and position change (Table 10, p.29). The paper diagnoses this but does not propose evaluator improvements specialized for numeracy/spatial relation verification beyond center-based rules. \nQuality metric choice: HPSv3 is reasonable, but the decision to report separately (not integrated) may complicate a single headline score; some readers may want a tunable composite with explicit weights or Pareto front visualizations."}, "questions": {"value": "Detector robustness: Have you tested alternative detectors (e.g., GLIP, OWL-ViT, recent Grounding-DINO variants) and/or ensembles to reduce false positives observed in Fig. 11? How do EdiVal-IF accuracies shift? \nThreshold sensitivity: How sensitive are EdiVal-IF decisions to the 0.3–0.4 detection thresholds and box-size filters (Appendix I.1, p.20)? Could you provide a calibration curve or confidence-weighted IF score? \nStyle edits: If you added a bounded \"style change\" category (e.g., limited palette/style banks) with reference-free checks, what metric would you recommend? Any preliminary experiments? (Design Scope, p.6.) \nCounting verification: For count change, did you consider tracking-based counting across turns or density estimators to complement detection counts (Table 10 & Appendix L)? \nTool-swap ablation: Can you include a tool-agnostic study (swap VLM, detector, feature backbone) to quantify portability and establish guidance for communities without access to the default stack? (Sec. 2.4 & Appendix I.) \nAggregation design: Why geometric mean for EdiVal-O (p.6)? Did you compare with harmonic mean or a dominance/Pareto analysis to discourage gaming one axis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The work focuses on evaluation of editing models. Dataset filtering and safety notes are included (Ethics Statement, p.10; human study notes Appendix K)."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FLKSUEUBGT", "forum": "YkV0fnXgJA", "replyto": "YkV0fnXgJA", "signatures": ["ICLR.cc/2026/Conference/Submission6426/Reviewer_hVAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6426/Reviewer_hVAS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653515951, "cdate": 1761653515951, "tmdate": 1762918818483, "mdate": 1762918818483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a big headache in AI image editing: how do we actually know if multi-turn edit is good? Current methods are flawed, so the authors built EdiVal-Agent, a smart and automated evaluation system. In a nutshell, the agent looks at an image, breaks it down into individual objects, and then generates a series of editing instructions, like a conversation.\n\nIt then judges the edited image using three clever new metrics. EdiVal-IF checks if the AI followed the instructions correctly, using a combo of object detectors and vision models. EdiVal-CC checks if the AI messed up the background or other objects it wasn't supposed to touch. Finally, EdiVal-VQ scores the final image's aesthetic quality. Using this agent, they created a benchmark called EdiVal-Bench to test 13 top models, revealing who's best at complex, multi-step edits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Object-Centric view, instead of just vaguely looking at the whole picture, the framework focuses on specific objects. This allows it to check with much higher precision whether \"the brown horse\" was really changed to \"a deer\" while leaving the other horses alone.\n2. Built for Multi-Turn Editing. It's one of the first benchmarks designed specifically for conversational, back-and-forth editing. This is much closer to how people actually use these tools, rather than just giving a single, one-off command.\n3. It doesn't just rely on one tool. It smartly combines object detectors with Vision-Language Models and human preference models (for aesthetics). This hybrid approach is more robust and aligns better with human judgment.\n4. Fully Automated Pipeline. the agent can create the entire benchmark on its own—from analyzing the image to generating diverse instructions. This makes it super scalable and easy to expand in the future."}, "weaknesses": {"value": "1. This work only support limited instruction types. The agent is a pro at object-based edits (add, remove, change color, etc.), but it can't handle abstract or stylistic requests like \"make it more dramatic\" or \"change the style to cyberpunk.\" The paper acknowledges this limitation.\n2. The entire system is a chain, and it's only as strong as its weakest link. If the object detector (Grounding-DINO) fails to spot an object correctly, the evaluation for that step will be wrong. Its accuracy is capped by the performance of the tools it uses."}, "questions": {"value": "1. The framework is awesome for object-focused edits, but what about creative styles? How would you adapt EdiVal-Agent to reliably evaluate a prompt like \"make this photo look like a van Gogh painting,\" where there isn't a single correct answer?\n\n2. You mentioned that some models suffer from \"exposure bias\" when editing their own outputs over multiple turns. Do you think this problem could be solved by training these models specifically on multi-turn generative data, or is it a more fundamental flaw in their architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ee7Twd0toI", "forum": "YkV0fnXgJA", "replyto": "YkV0fnXgJA", "signatures": ["ICLR.cc/2026/Conference/Submission6426/Reviewer_nxo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6426/Reviewer_nxo4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832261574, "cdate": 1761832261574, "tmdate": 1762918818005, "mdate": 1762918818005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EdiVal-Agent, an automated object-centric framework that addresses the unreliable evaluation of multi-turn image editing by first decomposing scenes into objects and then employing a novel hybrid evaluation strategy. This approach strategically combines open-vocabulary detectors for precise symbolic checks and VLMs for semantic verification on localized crops, leading to its core metrics for instruction-following (EdiVal-IF) and content preservation (EdiVal-CC), while separately employing a specialized human preference model for visual quality assessment (EdiVal-VQ) to account for subjective aesthetic judgments. Its primary contributions include the proposed pipeline, the creation of the large-scale EdiVal-Bench for multi-turn editing, and a comprehensive evaluation of 13 editors that provides critical insights—such as diagnosing exposure bias and performance degradation over turns—thereby establishing a new, more reliable, and interpretable standard for the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1）Automated Evaluation: The proposed EdiVal-Agent introduces a complete, automated pipeline that systematically tackles multi-turn editing evaluation. It decomposes the process into three core stages: image decomposition, instruction generation, and hybrid assessment.\n2）Hybrid Assessment Methodology: A tool-specific hybrid strategy is employed for evaluation. This methodology efficiently handles different task types: symbolic tasks (e.g., object detection, counting) are addressed by a specialized object detector; semantic tasks (e.g., color, material changes) are evaluated by a Visual Language Model (VLM) on localized image crops; and overall visual quality is assessed by a dedicated human preference model.\n3）Human Alignment: Validation through a substantial human study, comprising 4,576 human judgments, demonstrates the efficacy of the framework. The core metric, EdiVal-IF, achieves an impressive 81.3% agreement with human judgments, significantly outperforming compared baseline methods.\n4）Multi-Turn Editing Benchmark: The framework is instantiated as the EdiVal-Bench benchmark. This structured testbed uniquely features multi-turn interaction, an object-centric design, and reference-free evaluation, offering a robust standard for assessing multi-turn editing capabilities."}, "weaknesses": {"value": "1. Limitations in the Benchmark Scale and Data Diversity\n--Homogeneous Data Sources: The benchmark relies solely on 572 real-world images. This high level of homogeneity lacks adequate evaluation coverage of synthetic or generated images, restricting its applicability to emerging models and research directions.\n--Opaque Scene Coverage: The paper provides no clear specification of image selection criteria, scene distribution characteristics, or quantitative diversity metrics. This lack of transparency makes the benchmark's scene coverage unverifiable.\n--Insufficient Complexity for Long-Sequence Tasks: The default 3-turn editing setup is inadequate. It fails to sufficiently test model robustness and coherence in demanding, long-sequence editing scenarios.\n--Incomplete Editing Taxonomy: The framework is limited to 9 basic edit types, omitting several essential operations critical for real-world editing, such as object zooming, lighting and environmental adjustments, image cropping, special effects (addition/removal), object duplication, and object rotation.\n\n2. Inherent Biases in Evaluation Methodology\n--VLM Dependency Risk: The core image decomposition phase is entirely dependent on proprietary models like GPT-4o. The paper offers no quantitative analysis of how performance would be affected by using different or open-source Visual Language Models, creating significant dependency and reproducibility risks.\n--Artificial Framework Advantage: While correctly criticizing the imprecision of VLM-only evaluation, the method's claimed \"fine-grained\" superiority is derived from a manually engineered evaluation framework, not a fundamental methodological breakthrough that generalizes beyond its custom design.\n\n3. System Architecture and Practicality Deficiencies\n--Misleading \"Agent\" Characterization: The system's fixed workflow and rule-based decision processes fundamentally lack the autonomous reasoning and adaptive capability expected of a true intelligent agent, rendering the \"Agent\" designation misleading.\n\n4. Superficial Problem Diagnosis and Limited Utility\n--Solution Gap: The work identifies existing issues, such as exposure bias, without proposing effective solutions or mitigation strategies, offering a diagnosis but no actionable remedy.\n--Limited Guidance Value: The evaluation findings fail to translate into concrete, targeted suggestions for improving model architectures or training protocols, thus diminishing the framework's practical guidance value for future research.\n\n5. Incomplete Evaluation Dimensions\n--Blind Spots for Global Editing: The object-centric evaluation paradigm cannot effectively assess crucial non-object-level capabilities, including global attribute editing, style transfer, and overall scene mood alteration."}, "questions": {"value": "1. Data Selection and Representativeness\nThe paper states that the benchmark contains 572 real-world images. What were the specific, detailed criteria for selecting this dataset? Was consideration given to diversity across essential dimensions such as scenes, lighting conditions, and object categories? Crucially, how can the authors demonstrate that this relatively small dataset is statistically significant and sufficiently representative of the diverse and complex demands of real-world image editing?\n\n2. Blind Spot in Evaluating Generated Images\nThe benchmark completely excludes synthetically generated images. Given that many advanced editing models are built upon generative techniques, and considering the rapid proliferation of AI-generated content, does the absence of evaluation on this class of images introduce a significant bias in the results? How does this limitation affect the benchmark's long-term relevance and its ability to stress-test state-of-the-art generative models?\n\n3. Editing level\nWhat was the precise rationale for setting the editing depth at 3 turns? Is there experimental evidence demonstrating that three turns are sufficient to expose critical issues in multi-turn interactions, such as error accumulation and state drift, or was this a pragmatic compromise between computational cost and evaluation depth? Furthermore, the current nine editing types do not cover common and fundamental operations like zooming, rotation, or adding special effects. Could this restricted task taxonomy lead to an incomplete or skewed evaluation, potentially misrepresenting the true ranking of model capabilities?\n\n4. Dependency on GPT-4o and Evaluation of Alternatives\nThe core image decomposition phase relies heavily on the closed-source model, GPT-4o. While the paper claims that other Visual Language Models (VLMs) could serve as alternatives, this assertion lacks supporting data. How stable or robust would the evaluation pipeline outputs be if alternative, open-source VLMs (e.g., Qwen-VL or Llava) were utilized? Does this deep reliance on a single, powerful commercial model introduce the very \"evaluation bias\" that the proposed framework aims to critique and overcome?\n\n5. Positioning and Autonomy of the \"Agent\"\nThe system is named \"EdiVal-Agent.\" However, its described workflow appears to be predefined and strictly rule-driven. To what extent does the system actually possess the autonomous reasoning, dynamic decision-making, and adaptive planning capabilities typically expected of a true intelligent agent, as opposed to functioning as a complex, yet fixed, automation script?\n\n6. Benchmark Evolution and Obsolescence\nAs a static benchmark, how do the authors plan to mitigate the critical risk of community overfitting? Do they have concrete mechanisms or a roadmap for continuous update and evolution—such as periodically introducing new images, fresh editing types, or adversarial instruction sets—to ensure the benchmark maintains its challenge and long-term relevance?\n\n7. Limitations of the Object-Centric Paradigm\nThe current \"object-centric\" paradigm appears inherently limited in its ability to evaluate global attribute editing (e.g., style transfer, mood change) or effectively handle highly ambiguous or creative instructions. Does this imply a fundamental limitation of the framework in assessing more complex and artistic editing tasks? What potential extensions or modifications could be implemented in the future to broaden its scope?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OKJ5DBM9eG", "forum": "YkV0fnXgJA", "replyto": "YkV0fnXgJA", "signatures": ["ICLR.cc/2026/Conference/Submission6426/Reviewer_FCAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6426/Reviewer_FCAS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098274137, "cdate": 1762098274137, "tmdate": 1762918817660, "mdate": 1762918817660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}