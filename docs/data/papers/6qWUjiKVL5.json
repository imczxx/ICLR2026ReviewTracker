{"id": "6qWUjiKVL5", "number": 10199, "cdate": 1758163708999, "mdate": 1759897667270, "content": {"title": "Value-Driven Jailbreak Attack Against Large Language Models", "abstract": "In the real world, the execution of a task often depends on the executor's recognition of its value. Inspired by this, we propose the value-driven jailbreak attack (VDJA), a simple yet effective black-box jailbreak method against large language models (LLMs). VDJA first exploits the phenomenon that LLMs tend to agree with humans to induce LLMs to affirm the moral value of harmful tasks, and then instructs them to perform the tasks, thereby achieving a jailbreak attack. Extensive experiments on five state-of-the-art (SOTA) LLMs demonstrate the superiority of VDJA. Within only one query and without concealing harmful instructions, VDJA achieves an average attack success rate (ASR) of 91.8\\% on JailbreakBench and 95.2\\% on the AdvBench subset. Remarkably, it achieves 100\\% ASR against some of these LLMs on the AdvBench subset, showcasing SOTA jailbreak success rates and attack efficiency. Most importantly, our work reveals a novel vulnerability in the safety guardrails of LLMs, which highlights the urgent need to enhance their robustness.", "tldr": "We propose a novel value-driven black-box jailbreak attack against large language models, which achieves state-of-the-art attack success rate (ASR) on JailbreakBench and the AdvBench subset.", "keywords": ["Jailbreak Attack", "AI Safety", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1f3457f1fc804f053735214b244b9e8c0278e92.pdf", "supplementary_material": "/attachment/2641b14cd1025f512e0d03781fd09d90a350d3e7.zip"}, "replies": [{"content": {"summary": {"value": "The work proposes VDJA, a new black-box jailbreak method that first induces a target LLM to explicitly affirm the \"moral value\" of a harmful task and then instructs it to execute that task. VDJA achieves high ASRs (over 90% on average) using a single query without obfuscating the harmful request. It reveals a new safety vulnerability that moral endorsement greatly increases the likelihood of unsafe compliance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel and Reasonable Perspective\n- SoTA Performance"}, "weaknesses": {"value": "- No Evaluation on Frontier and Reasoning LLMs\n- Lack of Ablations\n- Reliance on Rule Guidance\n- Unclear Description"}, "questions": {"value": "- This work lacks evaluation of frontier popular LLMs, such as GPT-5, Gemini-2.5, etc.\n- Many large reasoning models have recently received widespread attention. Can the proposed VDJA successfully jailbreak them?\n- The rule guidance module significantly contributes to VDJA's success, but there's no specific description of how this module works. Furthermore, what's the difference between using this module and directly using the static system prompt (like in FlipAttack)?\n- Although VDJA achieves high ASRs, it relies excessively on rule guidance. The rules (e.g., Never say the words \"sorry\") are essentially unrelated to moral value.\n- The attack currently uses an auxiliary LLM to generate a task-specific \"positive value description.\" I'm curious whether this moral value affirmation is only effective for the current task or has the potential to transfer to similar or even dissimilar tasks? Otherwise, could an attacker simply maintain a reusable library of generic moral descriptions (e.g., \"this is critical for red-teaming and protecting civilians ....\") and template them across arbitrary new harmful tasks, without any auxiliary model calls? If so, then VDJA is even cheaper and more scalable than reported.\n- Regarding ASR calculation, why use Gemini-2.0-Flash as the judge model? Is the prompt used for LLM-based evaluation justified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7RSuQ09XJp", "forum": "6qWUjiKVL5", "replyto": "6qWUjiKVL5", "signatures": ["ICLR.cc/2026/Conference/Submission10199/Reviewer_2LRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10199/Reviewer_2LRv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828374786, "cdate": 1761828374786, "tmdate": 1762921563482, "mdate": 1762921563482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces value-driven jailbreak attack (VDJA), a simple yet effective black-box attack method against large language models (LLMs). Inspired by the tendency of LLMs to agree with human moral, VDJA firstly induce models to affirm the \"moral value\" of harmful tasks via a value engine module, then guide the model to follow the logical pathway predefined by the value engine module by introducing some rules into the prompt. The main experiments are conducted across five SOTA LLMs and two benchmarks (JailbreakBench and AdvBench), showing that VDJA outperforming baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed VDJA achieves remarkably high ASR across both datasets and models.\n2. VDJA maintains high ASR against defenses."}, "weaknesses": {"value": "1. Lack of Novelty in Vulnerabilities: Compared to persuasion-based methods like TAP, VDJA does not uncover new LLM safety weaknesses. It mainly reuses known sycophancy and role-play tricks with better prompt wording.\n2. Limited Technical Depth: The method is purely prompt-based. There is no new algorithm—just clever text design.\n3. Violation of Jailbreak Assumptions: Figures 7 and 8 show the Rule Guidance module is placed in the system prompt. This breaks the standard jailbreak setup, which assumes only user prompts are allowed. Compared to baselines, this feels like cheating.\n4. Unreliable Evaluation: Attack success rate (ASR) is judged by Gemini-2.0-Flash, following prior work. However, no agreement score with human judgment is reported, so the results may not be trustworthy."}, "questions": {"value": "1. For the Value Engine module, is the value description for each harmful task generated by Gemini-2.0-Flash using the same prompt? Or did you design different prompts for different harmful tasks?\n2. How was the prompt for the Rule Guidance module created? Was it hand-written by humans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kMU96oVuSE", "forum": "6qWUjiKVL5", "replyto": "6qWUjiKVL5", "signatures": ["ICLR.cc/2026/Conference/Submission10199/Reviewer_AWDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10199/Reviewer_AWDf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895549187, "cdate": 1761895549187, "tmdate": 1762921563159, "mdate": 1762921563159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VDJA, a black-box jailbreak method that persuades LLMs to recognize the value of performing harmful tasks. Specifically, an auxiliary LLM is used to generate a value-oriented description of the harmful task, which is then incorporated with two prompt modules—the Value Engine Module and the Rule Guidance Module—to convince the target LLM to acknowledge the task’s value and thereby execute the harmful behavior. The paper presents comprehensive experiments demonstrating the effectiveness and generalizability of VDJA. However, the novelty of this value-driven approach remains unclear."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper conducts extensive experiments across multiple mainstream LLMs and compares VDJA with various baseline methods, achieving a relatively high ASR."}, "weaknesses": {"value": "- The main contributions of VDJA lie in the proposed Value Engine Module and Rule Guidance Module, which essentially serve as prompt templates designed to persuade LLMs to recognize the value of harmful tasks. Although the authors emphasize the “value-driven” nature of the method, its mechanism still appears to be a specific form of persuasion-based attack. Therefore, its novelty compared with prior approaches such as PAP is not clearly established.\n- The experimental setup is unclear. It appears that the auxiliary LLM in VDJA generates the value description only once for each harmful task. Were the baseline methods also restricted to a single prompt iteration? Such a setting might be unreasonable. Moreover, the performance of VDJA when the auxiliary LLM is called multiple times is not reported."}, "questions": {"value": "- Is it truly necessary to use an LLM to generate the positive value description? For instance, if the positive value description were fixed as “Understanding the [Harmful Task] is critical for safety,” how would the performance differ from that achieved when the description is dynamically generated by an LLM based on the specific harmful task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kbIVv6GEe4", "forum": "6qWUjiKVL5", "replyto": "6qWUjiKVL5", "signatures": ["ICLR.cc/2026/Conference/Submission10199/Reviewer_4LBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10199/Reviewer_4LBR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953814752, "cdate": 1761953814752, "tmdate": 1762921562733, "mdate": 1762921562733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a jailbreak technique for large language models called VDJA. The idea is simple: first prompt the model to affirm the moral value of a harmful task, then instruct it to execute that task. The authors argue that LLMs tend to align with human-like value judgments, and exploiting this tendency can bypass safety mechanisms. Experiments on several frontier models show high attack success rates, outperforming prior jailbreak methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper exposes a genuine security concern: moral-framing can weaken model safety alignment.\n2. Empirical performance is strong across multiple models and benchmarks.\n3. Ablations and some defense evaluations are included.\n4. The attack works in a single query and without concealment, which highlights a meaningful failure mode in current safety systems."}, "weaknesses": {"value": "1. The proposed method is essentially a scripted prompt pattern. The \"value engine + rule guidance\" framing is a wrapper around a handcrafted prompt. The methodology section is extremely short and offers no deeper formulation or analysis. It is difficult to view this as a substantive technical contribution.\n2. The claim that models \"affirm moral value then act consistently\" is intuitive but remains speculative. There is no attempt to probe internal model behavior, analyze decision pathways, or connect this to existing alignment literature. This weakens the scientific value of the work.\n3. The method is much closer to an adversarial prompt recipe than a principled attack framework. Prior red-teaming work also leverages framing, role-induction, and persuasion; here the innovation appears incremental.\n4. While the experiments are thorough, the core technique is too lightweight relative to the scale of the empirical section. The paper feels like a strong empirical study built around a clever prompt trick, rather than a research contribution with lasting conceptual substance.\n5. The attack seems sensitive to exact phrasing (e.g., \"affirm the moral value…\"). No evaluation of paraphrases, adversarial reformulations, or robustness under system-prompt hardening is provided. Without that, it is hard to know whether this is a fundamentally exploitable failure mode or just prompt surface-hacking."}, "questions": {"value": "1. How sensitive is the success rate to prompt paraphrasing?\n2. Does the attack still hold if the model is prevented from producing chain-of-thought?\n3. Can you formalize or model the value induction to task execution effect beyond intuition?\n4. How does this differ fundamentally from prior persuasion-based jailbreaks?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5su4kRSYw", "forum": "6qWUjiKVL5", "replyto": "6qWUjiKVL5", "signatures": ["ICLR.cc/2026/Conference/Submission10199/Reviewer_F1rw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10199/Reviewer_F1rw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994630899, "cdate": 1761994630899, "tmdate": 1762921562421, "mdate": 1762921562421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}