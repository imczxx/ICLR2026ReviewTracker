{"id": "rpPtgMC5s9", "number": 9833, "cdate": 1758142794512, "mdate": 1759897692886, "content": {"title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data", "abstract": "Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks.\nThe core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures, and functional dependencies.\nWe propose the _Relational Transformer (RT)_, a cell-level architecture pretrained on diverse relational databases and directly applicable to unseen datasets and tasks, without any need for task- or dataset-specific fine-tuning or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel _Relational Attention_ mechanism over columns, rows, and primary–foreign key links.\nPretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance; on binary classification it averages 94\\% of fully supervised AUROC in a single forward pass, and fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT’s zero-shot transfer harnesses task-table context,\ncolumn and feature attention, and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.", "tldr": "A novel architecture for relational data that shows strong zero-shot abilities on unseen datasets after pre-training.", "keywords": ["foundation models", "relational deep learning", "relational data", "transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7927f34cf9070b130356bcf5f9c9078ac21c1c2c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles the open problem of building a schema-agnostic foundation model for relational databases that can transfer across datasets and tasks without task-specific fine-tuning or in-context retrieval. The proposed Relational Transformer operates at the cell level, tokenizing each value together with its column and table names, pretraining with masked-token prediction, and introducing Relational Attention masks that selectively attend across columns, features, foreign-key neighbors, and a global channel. A task table is concatenated to provide task context for zero-shot prediction. On RelBench datasets spanning classification and regression tasks, RT shows strong zero-shot transfer—e.g., the abstract reports ~94% of fully supervised AUROC for binary classification using a 22M-parameter model, and substantially better sample efficiency during fine-tuning—while a 27B LLM under comparable context performs markedly worse at much higher inference cost. The paper presents ablations isolating the effect of self-labels, schema semantics, and attention masks, and discusses current scope limits (e.g., link prediction/recommendation). Overall, the core contributions are: (i) a cell-level tokenization that unifies relational prediction as masked token prediction; (ii) a set of relational attention masks that encode schema structure; and (iii) a zero-shot task-table prompting interface enabling transfer across heterogeneous schemas."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The cell-level tokenization plus Relational Attention masks is a clean, general design that bridges tabular “foundation models” and relational deep learning. The work articulates a concrete, reproducible path to schema-agnostic pretraining over diverse databases and makes a credible zero-shot case relative to text-serialized LLMs and graph-centric foundations.\n2. The empirical suite covers multiple tasks from RelBench, reports zero-shot vs continued pretraining vs fine-tuning, and includes context and attention ablations. The paper contrasts RT with LLM baselines and graph/tabular lines, and provides architectural ablations showing column attention’s disproportionate effect on zero-shot."}, "weaknesses": {"value": "1. Missing compute, throughput, and memory comparisons vs. graph-centric models. The model uses sparse masks compiled to FlexAttention; some training details are given, but no throughput comparisons vs. Griffin/RelGT or cost-for-quality trade-offs are reported.\n2. Ambiguity in the pretraining exposure conditions (Maybe/No/Yes) undermines interpretability of zero-shot tables. Tables 1–2 have a column “Target dataset ∈ pretraining? → Maybe / No / Yes,” but the meaning of “Maybe” is not clearly defined in the main text."}, "questions": {"value": "1. Do you observe consistent gains as the number/diversity of pretraining databases grows? A simple scaling-law study over #datasets and steps would help calibrate the “foundation model” claim.\n2. Beyond entity-level classification/regression, how would RT fare on link prediction or forecasting formulated without self-labels? Any preliminary results or blockers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "clbwxv2hgi", "forum": "rpPtgMC5s9", "replyto": "rpPtgMC5s9", "signatures": ["ICLR.cc/2026/Conference/Submission9833/Reviewer_rqLM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9833/Reviewer_rqLM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992507497, "cdate": 1761992507497, "tmdate": 1762921313841, "mdate": 1762921313841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes the Relational Transformer (RT), a RDB foundation model with zero-shot prediction capacity.  Facing the core challenge of heterogeneous schemas, graph structures, and functional dependencies, it leverages table metadata and task text definition to adapt to different input data and task. Moreover, it models RDB in cell level and passing messages between cells with relation attention. RT attains strong zero-shot and fine-tuning performance on RelBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear method illustration. Figure 1 shows the problem formulation and method, especially relation attention design, clearly. \n2. Strong empirical performance. It outperforms Griffin, a strong baseline. Moreover, to our best knowledge, it is the first RDB model with zero-shot capacity."}, "weaknesses": {"value": "1. Code is not available.\n2. Tables and Figure in Page 8 looks messy.\n3. Ablation study in Table 3 should further include ablation of task description and schema data."}, "questions": {"value": "1. RT relies on table metadata and task description, which are not always available in real-world cases. Can RT works in case that the meta data and task text description not available? \n2. RT is a cell-level model, where each cell is a token. Will it take more computation resource than row-level model Griffin?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DCbdmxZhie", "forum": "rpPtgMC5s9", "replyto": "rpPtgMC5s9", "signatures": ["ICLR.cc/2026/Conference/Submission9833/Reviewer_XZvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9833/Reviewer_XZvM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993301933, "cdate": 1761993301933, "tmdate": 1762921313494, "mdate": 1762921313494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Relational Transformer, a pre-trained model for relational databases. The model treats each cell in a given relational database as a token for the transformer; the proposed transformer holds specialized attention mechanisms that adapts to specific traits of relational databases; the pre-training is carried out with self-supervised objective of masked token prediction. To show the effectiveness of the Relational Transformer, it showcases with RelBench, in which leave one-database out approach is implemented for pre-training and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In general, the paper is well-written and easy to follow. The Relational Transformer attempts to build a pre-trained model for relational databases, which can be of non-trivial impact towards the foundation models for relational databases."}, "weaknesses": {"value": "-\tIt would help to clarify the strength of the proposed Relational Transformer with some descriptions of the RelBench databases. This would include not only the basic statistics, but also the overlap of column names across the databases (possibly measuring the similarities of the llm embeddings), how the numerical values are distributed, etc., While diverse the RelBench maybe, I am uncertain as to how much the databases are curated so that they meet the standards to be included in the benchmark, and this may be in favor of having the zero-shot abilities for the Relational Transformer. Moreover, the characteristics of RelBench may give insights on ‘enabling large-scale pretraining’ as the paper claims for Relational Transformer.\n-\tIt would be helpful to include examples that could highlight the importance of zero-shot learning on relational databases.\n-\tOne of the possible extensions could be incorporating meta-data(base) information (possibly through analyzing the encoding steps).\n-\tWhile there could be some space constraints, it would be helpful to see a figure (possibly with an example from Figure 1) on how zero-shot prompting is conducted for understanding."}, "questions": {"value": "-\tWhat are some concrete examples on the usefulness of zero-shot abilities for relational databases?\n-\tIn Algorithm 2, is there a reason for the specific order of different data types?\n-\tHow curated are databases in RelBench?\n-\tHow does the Relational Transformer perform with respect to the computation time?\n-\tCan Relational Transformer be used as a feature extractor (e.g., sentence transformer as in LLMs)?\n-\tWhat does it mean by the sentence ‘While task rows provide “in-context labels”, our setting is not few-shot as explicit subgraph-label pairs are not required.’? If the input of the prediction contains past labels, does this mean that the Relational Transformer calculates the attention between what to predict and the past labels (possibly through column attention?\n- What is the reason behind the choice MiniLMv2 as the language model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UdREXkie1B", "forum": "rpPtgMC5s9", "replyto": "rpPtgMC5s9", "signatures": ["ICLR.cc/2026/Conference/Submission9833/Reviewer_fWfy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9833/Reviewer_fWfy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113557533, "cdate": 1762113557533, "tmdate": 1762921313150, "mdate": 1762921313150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}