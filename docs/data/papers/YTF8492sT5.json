{"id": "YTF8492sT5", "number": 20200, "cdate": 1758303618596, "mdate": 1759896990744, "content": {"title": "Controlled disagreement improves generalization in decentralized training", "abstract": "Decentralized training is often regarded as inferior to centralized training because the consensus errors between workers are thought to undermine convergence and generalization, even with homogeneous data distributions. This work challenges this view by introducing decentralized SGD with Adaptive Consensus (DSGD-AC), which intentionally preserves non-vanishing consensus errors through a time-dependent scaling mechanism. We prove that these errors are not random noise but systematically align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Across image classification and machine translation benchmarks, DSGD-AC consistently surpasses both standard DSGD and centralized SGD in test accuracy and solution flatness. Together, these results establish consensus errors as a useful implicit regularizer and open a new perspective on the design of decentralized learning algorithms.", "tldr": "", "keywords": ["decentralized learning", "distributed optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd541a9893f661a958fae6aaea18128c1f88d025.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discovers and proves that the consensus errors in decentralized training align with the dominant Hessian subspace, acting as structured perturbations that guide optimization toward flatter minima. Building upon this insight, the authors propose DSGD-AC to maintain non-vanishing consensus errors throughout training. Experimental results show that this method achieves significantly improved generalization performance across multiple tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper provides an insightful interpretation of consensus errors in decentralized learning. Instead of considering these errors as harmful noise, the authors theoretically reveal their natural alignment with the dominant Hessian subspace and show that they implicitly serve the role of SAM-like perturbations.  \n2.\tExperimental results show that DSGD-AC surpasses centralized SGD on multiple benchmarks."}, "weaknesses": {"value": "1.\tAll experiments are conducted on the one-peer ring topology. Since consensus error behavior and spectral properties can vary substantially across different communication graphs, experiment results on other topologies (e.g., exponential graphs, fully-connected, random expander) would provide stronger evidence for the robustness of DSGD-AC.  \n2.\tWhile Proposition 2 establishes a relationship between the step size $\\alpha^{(t)}$, the Hessian spectrum, and the communication matrix $W$, this theoretical connection is not reflected in the adaptive factor $\\gamma^{(t)}$ used in the algorithm. As a result, the paper does not yet exploit the network topology in its adaptive consensus mechanism.  \n3.\tThe design of the algorithm, especially the adaptive consensus factor $\\gamma^{(t)}$ lacks the conceptual motivation and novelty.  \n4.\tThe paper lacks theoretical generalization bounds for DSGD-AC. The existing propositions mainly demonstrate that DSGD-AC maintains non-vanishing consensus errors, rather than providing guarantees on generalization performance."}, "questions": {"value": "1.\tOn Page 13, Line 682, $V_k$ is treated as a time-invariant quantity, despite the fact that $\\tilde{z}_k^{(t)}$ depends on the iteration $t$. It could be helpful if you could explain this.  \n2.\tIn the DAdam-AC experiments, the paper suggests that a more tailored adaptive consensus mechanism could further improve performance when combined with Adam. It would be better if you could elaborate on possible design directions.  \n3.\tProposition 2 provides a stability condition on the step size $\\alpha^{(t)}$. It would be helpful if you could clarify how this condition is guaranteed to hold in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5LPZpCeLXM", "forum": "YTF8492sT5", "replyto": "YTF8492sT5", "signatures": ["ICLR.cc/2026/Conference/Submission20200/Reviewer_vujS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20200/Reviewer_vujS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724206462, "cdate": 1761724206462, "tmdate": 1762933702518, "mdate": 1762933702518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's primary contribution is a novel perspective on the consensus step in decentralized SGD. The authors interpret the gossip-based parameter exchange as an implicit optimization on a quadratic consensus loss. Building on this, they argue that the *weight* of this implicit loss, which in standard DSGD is coupled with the inverse of the learning rate, should be controlled independently.\n\nThey introduce Decentralized SGD with Adaptive Consensus (DSGD-AC), which adds a time-dependent scaling factor $\\gamma^{(t)}$ to the consensus term. The main goal of this modification is to prevent the consensus errors from vanishing as the learning rate decays, thereby preserving what prior work has identified as a \"free sharpness regularization\" inherent in the decentralized method. The authors provide theoretical support, arguing that these controlled, non-vanishing errors are not random noise but are structurally aligned with the dominant Hessian subspace, guiding the optimization toward flatter minima."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The core observation is novel. Building on the work of Zhu et al. (2023), the idea that the implicit quadratic loss from the gossip phase should have its own adaptive scaling factor (decoupled from the main learning rate) is an insightful contribution.\n* The paper provides a theoretical justification for the algorithm's design. Proposition 1 formally demonstrates that DSGD-AC, with the proposed scaling, can maintain non-vanishing consensus errors, which is the mechanism intended to preserve the free sharpness regularization."}, "weaknesses": {"value": "* The paper's primary weakness lies in its experimental validation. The experiments are restricted to a very specific and limited setting: 8 nodes in a one-peer ring topology. This setup does not provide sufficient evidence for the method's effectiveness or scalability in more general or larger-scale scenarios (e.g., more nodes, different graph topologies).\n* The performance improvement on the machine translation task is very small. In Table 2, the BLEU score for DAdam-AC is only marginally better than that of the standard Adam baseline."}, "questions": {"value": "1.  Given that all experiments are on an 8-node ring, it is unclear how DSGD-AC would perform in other settings. Could the authors provide results or at least discuss the expected performance on larger systems (e.g., 16 or 32 nodes) or with different communication topologies (e.g., a torus or a more sparse random graph)?\n2.  The proof for Proposition 1 relies on the assumption of a \"quasi-stationary regime where the expectations and variances of the columns of $\\tilde{G}^{(t)}$ remain constant.\" Could the authors elaborate on this assumption? Specifically, why is it a reasonable assumption in this context, how strong is it, and under what conditions during training might it fail to hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mgDNkwdTrZ", "forum": "YTF8492sT5", "replyto": "YTF8492sT5", "signatures": ["ICLR.cc/2026/Conference/Submission20200/Reviewer_yV2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20200/Reviewer_yV2E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886314044, "cdate": 1761886314044, "tmdate": 1762933702084, "mdate": 1762933702084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first empirically shows a positive correlation between consensus error and generalisability, which suggests that maintaining non-diminishing consensus error can boost generalisability. Based on this motivation, the authors design a new algorithm that has non-diminishing consensus error. They further theoretically show that this consensus error \"align with dominant subspace of Hessian,\" which was shown linked to good generalisability. The paper also conducted experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is interesting and well motivated, though it is a bit simple. The theory gives insightful message for designing algorithms though (still) a bit simple."}, "weaknesses": {"value": "My first concern is that the theory has limited novelty and merit given an existing paper [Zhu et al 2023]. The previous paper has calculated the consensus error and proved the asymptotic equivalence between D-SGD and SAM. The new part in this paper is injecting a lambda to make the consensus error non-diminishing.\n\nFurther, the experimental results are not strong enough. The authors did not compare different topologies and hyperparameter alpha, two key tunable factors in this paper. The performance improvement is very marginal."}, "questions": {"value": "It would be great if the authors can clarify the two weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lVHwb7mtSo", "forum": "YTF8492sT5", "replyto": "YTF8492sT5", "signatures": ["ICLR.cc/2026/Conference/Submission20200/Reviewer_6ahb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20200/Reviewer_6ahb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930549757, "cdate": 1761930549757, "tmdate": 1762933701769, "mdate": 1762933701769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel algorithm called Decentralized SGD with Adaptive Consensus (DSGD-AC). The key idea is to introduce an adaptive scalar before the consensus term, which weakens the consensus process and intentionally maintains non-vanishing consensus errors. These errors are proven to be aligned with the dominant Hessian subspace. Empirically, the authors observe that, compared to DSGD and SGD, the proposed algorithm achieves higher test accuracy and converges to flatter minima on the CIFAR-10/CIFAR-100 image classification and WMT14 machine translation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow. The algorithm presented in Algorithm 1 is precise. The connection between the consensus error and the Hessian subspace is conceptually interesting."}, "weaknesses": {"value": "1. **Overly strong assumptions**  The theoretical analysis relies on the assumption of *data homogeneity*â€”that all agents share identical local objectives $ f_i = f_j $.  \n   This assumption weakens the theoretical contribution.\n\n2. **Lack of theoretical analysis for the adaptive scalar**   The adaptive scaling factor is taken as\n   $\\gamma^{(t)} = [\\alpha^{(t)}/\\alpha_{\\max}]^p$.\n  The theoretical analysis in this paper is not strong enough. The paper contains two propositions in the main section. Proposition 1 states that DSGD-AC's consensus error does not eventually drop to zero, unlike other DSGD algorithms. Proposition 2 states that the consensus error lies in the subspace spanned by the top eigenvectors of $H$. However, these two propositions do not convincingly demonstrate the necessity of using the adaptive scalar $\\gamma^{(t)}$ in the main algorithm (Algorithm~1). I leave my detailed concerns about the propositions in the question section.\n\n3. **Lack of significant experimental results** \n   The experimental results are not sufficient. The authors conduct experiments only on small-scale problems, namely CIFAR-10 and WMT14. The results on WMT14 do not verify stronger performance of the adaptive consensus algorithm compared to ordinary centralized and decentralized methods. Some of my concerns about the experiments are discussed in the question section.\n\n4. **Writing and notations** Some notations are confusing. In line 240, the authors use $L = U \\Lambda U^\\top$, while in line 310 they use the same symbol $H = U \\Lambda U^\\top$. Different symbols should be used for these different matrix decompositions.  \nIn line 315, $\\lambda_{\\min}^W$ is used without definition.  \nIn line 231, it should be $\\Delta^{(t)} = X^{(t)}(I - \\frac{1}{n}11^\\top)$."}, "questions": {"value": "1. In Proposition 2, the authors mention that by projecting the consensus error onto the $k$-th eigenvector $u_k$ (corresponding to the eigenvalue $\\lambda_k$), the projected consensus error is linearly stable under the stability condition\n$$\n\\alpha^{(t)} < \\frac{2 + (\\lambda_{\\min}^{W} - 1)\\gamma^{(t)}}{\\lambda_k}.\n$$\nHere, $\\lambda^{W}$ denotes the eigenvalue of the consensus matrix $W$. How does this proposition explain the advantage of DSGD-AC over ordinary DSGD? In ordinary DSGD, we have $\\gamma^{(t)} \\equiv 1$. \n\n2. Suppose we set $W = I$, meaning that $n$ agents independently optimize the same objective function $F$ without communication. In this case, we have $\\lambda_{\\min}^{W} = 1$, and the right-hand side of the inequality becomes even larger. This suggests that separate training without any information exchange would be theoretically more stable than DSGD-AC. How should this be interpreted?\n\n3. In Table 1, does ``SGD'' refer to centralized SGD? If so, why does DSGD achieve better testing accuracy and lower testing loss than SGD?\n\n4. In Table 2, the result does not appear strong enough: DAdam-AC does not perform better than DAdam. Could the authors elaborate on this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9xlrUv8Bhs", "forum": "YTF8492sT5", "replyto": "YTF8492sT5", "signatures": ["ICLR.cc/2026/Conference/Submission20200/Reviewer_jJrt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20200/Reviewer_jJrt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20200/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061436373, "cdate": 1762061436373, "tmdate": 1762933701359, "mdate": 1762933701359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}