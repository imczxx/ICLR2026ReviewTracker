{"id": "E22I6z7qFy", "number": 11024, "cdate": 1758187155150, "mdate": 1763705083251, "content": {"title": "Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation", "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios.\nTo address these limitations,\nwe propose \\textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts.\nTo enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm.\nExtensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.", "tldr": "", "keywords": ["Graph Domain Adaptation", "Graph Neural Networks", "Characteristic Function"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/451af0b0381788175da498b72040c86e89603a20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ADAlign, an adaptive distribution alignment framework for graph domain adaptation (GDA). The authors propose to address the challenge of complex, multi-faceted distributional shifts between source and target graphs by leveraging neural characteristic functions (CF) in the spectral domain. The key contribution is the Neural Spectral Discrepancy (NSD), which uses characteristic functions to capture feature-structure dependencies and employs a learnable frequency sampler to adaptively identify discriminative spectral components through minimax optimization. The framework is evaluated on 10 datasets across 16 transfer tasks, demonstrating superior performance compared to state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear Problem Formulation**: The paper effectively motivates the need for adaptive alignment by showing how dominant discrepancy sources vary across transfer scenarios (Figure 1), making a compelling case against fixed heuristic alignment strategies.\n\n2. **Theoretical Grounding**: The use of characteristic functions is theoretically well-motivated, with proper mathematical formulation including convergence and uniqueness theorems. The PAC-Bayesian analysis in Section 4.5 provides theoretical justification for the approach.\n\n3. **Comprehensive Experiments**: The experimental evaluation is extensive, covering 16 transfer scenarios across diverse graph domains (citation networks, airports, social networks) with consistent improvements over 15 baseline methods."}, "weaknesses": {"value": "1. The paper fails to clearly distinguish its characteristic function-based approach from existing spectral alignment methods in GDA. Most critically, the relationship to SA-GDA (Pang et al., 2023), which also performs spectral augmentation for graph domain adaptation, is not discussed. SA-GDA already demonstrated that spectral domain alignment can effectively handle distribution shifts in graphs by augmenting the graph Laplacian spectrum. \n\n2. The paper emphasizes the amplitude-phase decomposition (Equation 7) but doesn't convincingly demonstrate why this complex-valued representation is necessary.\n\n3. The adaptive frequency sampler (Section 4.3) appears to be a standard importance sampling approach: The normal scale mixture formulation (Equation 10) is not well justified - why this specific parametrization? Minimax optimization can lead to instability, though the authors claim stability without showing the training dynamics of the frequency sampler itself—missing comparisons with simple fixed-frequency grids or other sampling strategies beyond the limited ablation in Section 5.4.\n\n4.  The baselines don't include recent spectral methods like SA-GDA (Pang et al., 2023) or other Fourier-based domain adaptation approaches. The significance tests (Appendix E.2) compare against only three baselines, not the full set. The visualization in Figure 10 is not particularly convincing - the claimed \"sharp boundaries\" in ADAlign's embeddings are not clearly superior to baselines."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8eRhcKcqDW", "forum": "E22I6z7qFy", "replyto": "E22I6z7qFy", "signatures": ["ICLR.cc/2026/Conference/Submission11024/Reviewer_TwjK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11024/Reviewer_TwjK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542850434, "cdate": 1761542850434, "tmdate": 1762922202741, "mdate": 1762922202741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ADAlign, a framework for graph domain adaptation (GDA) that addresses complex distributional shifts between source and target graphs. It proposes the neural spectral discrepancy (NSD), which measures cross-graph gaps in the spectral domain using neural characteristic functions, and adaptively samples informative frequencies via a minimax optimization. The method is theoretically supported by PAC-Bayesian analysis and achieves state-of-the-art performance across 16 transfer scenarios with lower computational and memory costs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Figure 1 clearly illustrates the limitations of existing approaches, providing strong motivation for introducing adaptive alignment.\n2. The introduction of NSD based on neural characteristic functions is theoretically grounded. By performing adaptive alignment in the spectral domain, the method provides a more flexible framework.\n3. The experiments are comprehensive, covering 13 strong baselines across 16 transfer scenarios, with ablation studies and parameter sensitivity analyses supporting the method’s validity.\n4. ADAlign achieves significant improvements in computational efficiency, being faster and using less memory than baselines."}, "weaknesses": {"value": "1. Although the paper explains that low-frequency captures large-scale structures and high-frequency captures fine-grained structures, it does not clearly clarify what specific graph structural characteristics the learned frequency distributions reflect.\n2. There is no analysis or experiment showing how replacing the GNN backbone affects the NSD-based alignment performance.\n3. While the paper provides a complexity analysis, the paper lacks experiments on the scalability of ADAlign to very large graphs or how its performance changes with graph size."}, "questions": {"value": "Look at the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RFd6M2NPQV", "forum": "E22I6z7qFy", "replyto": "E22I6z7qFy", "signatures": ["ICLR.cc/2026/Conference/Submission11024/Reviewer_HQfr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11024/Reviewer_HQfr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835090919, "cdate": 1761835090919, "tmdate": 1762922202303, "mdate": 1762922202303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ADAlign, an adaptive distribution alignment framework for Graph Domain Adaptation to address complex distributional shifts between source and target graphs. It introduces Neural Spectral Discrepancy (NSD), a parametric distance leveraging neural characteristic functions in the spectral domain to capture multi-level feature-structure dependencies. Equipped with a learnable frequency sampler and minimax optimization, ADAlign automatically identifies and aligns discrepancies without manual criteria."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Eliminates manual specification of alignment criteria by dynamically prioritizing relevant spectral components for each transfer scenario.\n2. NSD integrates amplitude and phase differences for a comprehensive view of cross-graph shifts.\n3. Performs consistently well across domains."}, "weaknesses": {"value": "1. Regarding the acceleration aspect, I understand the paper adopts the spectral method. However, this approach does not have low computational complexity. What methods are used for acceleration or approximation in the paper? It would be better if the authors could provide a proof of the complexity.\n2. The perspective proposed in the paper is good, but I am not very familiar with the statement in the introduction: \"these approaches often rely on heuristic strategies that first manually design graph filters to extract relevant features\" and feel that it is not the mainstream method in Graph Domain Adaptation. Proposing a model based on this point does not seem to be an appealing argument. Besides, it would also be helpful if the paper provides some descriptions of this pipeline to assist readers' understanding.\n3. The KL divergence in Figure 1 is used to measure the difference between features. Can this be regarded as assuming that the dataset satisfies Ps(Y|X) = Pt(Y|X)? Otherwise, merely exploring the difference between Ps(X) and Pt(X) seems irrelevant to transfer learning.\n4. There is a lack of experiments on large datasets. All datasets used here are too small, such as Brazil."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yn6j4aR2FE", "forum": "E22I6z7qFy", "replyto": "E22I6z7qFy", "signatures": ["ICLR.cc/2026/Conference/Submission11024/Reviewer_2AFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11024/Reviewer_2AFp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990247775, "cdate": 1761990247775, "tmdate": 1762922201776, "mdate": 1762922201776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced the ADAlign framework, a novel approach to handling composite\ndistribution shifts in GDA. By leveraging characteristic function in the complex Fourier domain, ADAlign dynamically identifies and aligns discriminative spectral components through minimax optimization, eliminating the need for manual reweighting or heuristic metrics. Extensive experiments across 16 benchmarks show that ADAlign consistently outperforms state-of-the-art methods, while also improving training stability and reducing computational overhead. These advantages make ADAlign a highly effective and practical solution for real-world graph transfer learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel methodology: This paper introduces NSD, a novel parametric distance\nfor graphs that leverages neural characteristic function in the spectral domain to capture multi-level feature-structure dependencies, providing a unified approach to quantifying distributional shifts.\n2. Effective adaptive mechanism: This paper proposes an adaptive framework that automatically identifies and aligns the most relevant sources of discrepancy in each transfer scenario, enabling flexible and task-aware adaptation to dynamic graph shifts.\n3. Exceptional Empirical Performance and Significant Computational Efficiency: Extensive experiments in this paper show that ADAlign outperforms state-of-the-art baselines, significantly reducing memory consumption and training time."}, "weaknesses": {"value": "1. The paper's core philosophy is adaptivity, yet the amplitude weight k, which balances amplitude and phase, is a hand-tuned hyperparameter. The analysis in Figure 6 shows an optimal range, but it's possible that it is also scenario-dependent. A fixed k seems slightly at odds with the \"adaptive\" goal.\n2. It would be helpful if the paper could further justify modeling the sampler as a normal scale mixture, and clarify whether simpler parameterizations such as a single Gaussian, a fixed finite mixture would already work in practice.\n3. The paper adopts the GNN encoder  F (Liu et al., 2024a) to map each graph into node-level embeddings,  but it remains unclear how sensitive ADAlign is to the choice of backbone such as GCN and GAT.\n4. There is a minor inconsistency in the wording. For example, the main text states ‘We adopt the GNN encoder F (Liu et al., 2024a),’ whereas Figure 2 describes the components as ‘each processed by a GCN."}, "questions": {"value": "1. Regarding Weakness 1: Did the authors consider making the amplitude weight k adaptive?\n2. If you replace the GNN backbone with GCN or GAT, will the advantages of your model be offset?\n3. Could the authors visualize the learned frequency sampling distributions across different datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JkTEYZOSbN", "forum": "E22I6z7qFy", "replyto": "E22I6z7qFy", "signatures": ["ICLR.cc/2026/Conference/Submission11024/Reviewer_A6vr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11024/Reviewer_A6vr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000473627, "cdate": 1762000473627, "tmdate": 1762922201403, "mdate": 1762922201403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Response to Reviewers"}, "comment": {"value": "Firstly, sincerely thank all the reviewers for their efforts in reviewing our paper and providing constructive suggestions. We are greatly encouraged that the reviewers find that\n\n* Motivation: Reviewers (*A6vr, HQfr, TwjK*) agree that the paper clearly identifies why fixed alignment heuristics fail across scenarios, with compelling evidence showing scenario-dependent discrepancy sources. This provides a **strong and well-motivated foundation** for adaptive alignment.\n* Methodology: Reviewers (*A6vr, 2AFp*) highlight the **novelty and strength** of the proposed NSD-based adaptive alignment framework.\n* Theoretical Soundness: Reviewers (*HQfr, TwjK*) recognize that the method is **theoretically well-grounded**, supported by convergence and uniqueness properties of characteristic functions and further strengthened by PAC-Bayesian generalization analysis.\n* Experiment: **All Reviewers** uniformly praise the experimental is **comprehensive**. covering 13 strong baselines and 16 transfer scenarios across diverse domains.\n\nSecondly, as for the concerns and suggestions raised by each reviewer, we have done our best to address them thoroughly and have provided detailed responses to each of them. In the following, we summarize our responses to the main concerns raised by the reviewers.\n\n* Reviewer A6vr：We have **conducted experiments** swapping GCN and GAT backbones, and **clarified the sampler** by incorporating a simplified Gaussian baseline.\n* Reviewer 2AFp: We have **conducted new experiments** on **larger** datasets, to demonstrate improved efficiency and broader applicability.\n* Reviewer HQfr: We have **performed backbone-swap experiments** to assess sensitivity to GCN/GAT and **included scalability tests** on larger graphs.\n* Reviewer TwjK: We have **added experiments**, including justification and ablations for the amplitude–phase formulation and **normal-scale-mixture sampler** (with fixed-frequency and alternative sampling baselines), and **expanded comparisons** to include recent spectral methods with improved visualizations.\n\nFinally, in the revised manuscript, we have comprehensively addressed each concern, incorporating additional experiments and detailed discussions. Major changes and updates are highlighted in blue for your convenience."}}, "id": "79jSlqbUYL", "forum": "E22I6z7qFy", "replyto": "E22I6z7qFy", "signatures": ["ICLR.cc/2026/Conference/Submission11024/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11024/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission11024/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704979181, "cdate": 1763704979181, "tmdate": 1763704979181, "mdate": 1763704979181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}