{"id": "0zdlTzGoou", "number": 11294, "cdate": 1758195602459, "mdate": 1759897595737, "content": {"title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning", "abstract": "We reframe and operationalise adversarial inverse reinforcement learning (IRL) to large language model reasoning, learning a dense, token-level reward model for process supervision directly from expert demonstrations rather than imitating style via supervised fine-tuning. The learned reasoning reward serves two complementary roles: (i) it provides step-level feedback to optimise a reasoning policy during training; and (ii) it functions at inference as a critic to rerank sampled traces under fixed compute budgets. We demonstrate that our approach prioritises correctness over surface form, yielding scores that correlate with eventual answer validity and enabling interpretable localisation of errors within a trace. Empirically, on GSM8K with Llama3 and Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a learning signal to elicit reasoning, and (ii) predictive performance is improved from reward-guided reranking (notably for Llama-based policies). By unifying training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, this work suggests reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.", "tldr": "We learn a dense token-level reasoning reward via IRL from expert traces that both trains LLMs and reranks samples at inference, boosting accuracy and offering interpretable, tokenwise error localisation.", "keywords": ["inverse reinforcement learning", "adversarial learning", "large language models", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a68cb5e9ffa8b53c300e6accce745f0ab2efc527.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes and applies adversarial inverse reinforcement learning to the reasoning tasks of large language models. It learns a token-level dense reasoning reward model from expert demonstrations, instead of imitating styles through supervised fine-tuning. This reward model serves two complementary roles: it provides step-level feedback to optimize the reasoning policy during training, and acts as a critic to rerank sampled traces under a fixed computational budget during inference. Moreover, it prioritizes correctness over surface form, can correlate with the validity of the final answer, and locate errors within the traces. Experiments on the GSM8K dataset using Llama3 and Qwen2.5 as backbone models show that the dense reasoning reward can be used as a learning signal to elicit reasoning capabilities, and reward-guided reranking can improve predictive performance (especially for Llama-series models). Meanwhile, this work unifies training signals, inference selection, and token-level diagnostics"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The token-level dense reward model learned based on adversarial inverse reinforcement learning (IRL) in this paper breaks through the limitation of separating \"training signals\" and \"inference auxiliary tools\" in traditional methods. It can not only provide step-level training feedback for LLM reasoning policies (solving the \"black-box training\" problem that only relies on outcome correctness) but also act as a reranker during the inference stage to screen high-quality traces under a fixed computing budget (without the need for additional training of a dedicated reranking model)"}, "weaknesses": {"value": "1. The paper adopts the \"discriminator-policy\" adversarial optimization framework (maxφminθ E [expert reward - policy reward]), but fails to address the inherent **signal non-stationarity** of adversarial training. After the discriminator parameter φ is updated, the reward objective for policy optimization changes immediately, which may lead to oscillations in policy training (e.g., \"evaluation reward fluctuations\" in the training curve). The paper only describes this phenomenon but does not analyze its causes \n2. It does not test performance on other reasoning datasets (such as MATH, AIME, and other reasoning tasks like code reasoning), making it impossible to verify whether the reward model is only suitable for arithmetic reasoning or can be transferred to other harder reasoning tasks \n3. It does not test the model’s performance when \"the proportion of expert demonstrations is reduced\" (e.g., only 50% or 30% of expert traces are provided, and the rest are replaced with noisy traces) or \"expert demonstrations contain errors\". This makes it impossible to prove the effectiveness of the method in \"low-quality demonstration\" scenarios, which are actually more common in practical applications"}, "questions": {"value": "1. Why is GSM8K chosen as the only verification setting? \n2. If expert demonstrations contain 10%-20% of incorrect steps (e.g., intermediate calculation errors), will the reward model in the paper learn the wrong signals? To what extent will the model’s performance decline in this case?\n3. What are the specific impacts of \"signal non-stationarity\" (reward objective changes caused by discriminator updates) on policy training in adversarial training? Is there any theoretical analysis or experimental evidence to prove the convergence of this adversarial framework? If convergence exists, what are the convergence conditions? \n4. After removing the \"adversarial framework\" (training the reward model only with expert traces), \"token-level reward\" (replacing it with step-level reward), and \"reward normalization\" respectively, how will the model’s pass@k and error localization ability change? \n5. Does the method face an on-policy issue—namely, can a reward model trained on one reasoning model generalize to other reasoning models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oOo5YqmhVk", "forum": "0zdlTzGoou", "replyto": "0zdlTzGoou", "signatures": ["ICLR.cc/2026/Conference/Submission11294/Reviewer_RjAM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11294/Reviewer_RjAM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965286584, "cdate": 1761965286584, "tmdate": 1762922442369, "mdate": 1762922442369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting research question and proposes a framework based on adversarial inverse reinforcement learning (IRL), where a discriminator learns to distinguish expert reasoning trajectories from model-generated ones, and its logits are used as dense rewards for each token. \nThese rewards are then used both for training and inference-time reranking of reasoning candidates. Experiments on GSM8K with Llama3 and Qwen2.5 show that the learned reward correlates with correctness, improves reranking, and offers interpretable error localization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and underexplored direction, i.e., learning process-level reasoning rewards. \nThe method is clearly motivated, well-formulated, and conceptually elegant.\nIt unifies training, evaluation, and interpretability under a single learned dense reward. \nThe use of IRL in reasoning is novel, bridging reinforcement learning theory with LLM reasoning supervision. \nThe evaluation framework is also systematic and covers both performance and interpretability aspects."}, "weaknesses": {"value": "There are also several important concerns that need to be addressed. \nWhile the paper shows performance improvements from using the learned reward for reranking, it does not include comparisons against other SOTA mathematical reasoning baselines. This makes it unclear whether the observed gains truly come from the proposed inverse-RL reward or if they could be achieved by existing reasoning-specific techniques such as verifier-based reranking or process reward models.\n\nThe experiments are limited to GSM8K, which restricts the generality of the conclusions. Given that the central claim is about learning a reward that captures general reasoning process quality, evaluating on at least one non-mathematical or multi-domain dataset would make the evidence much more convincing. The lack of such breadth is a major limitation, even though the authors acknowledge this point in their Limitations.\n\nThe training setup involves alternating updates between the policy and the discriminator, yet the paper provides little analysis of training stability, variance, or convergence behavior. Important hyperparameters are not ablated, and the robustness of the discriminator to noisy or imperfect demonstrations remains untested."}, "questions": {"value": "Please refer to the concerns in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7THDlUJOO5", "forum": "0zdlTzGoou", "replyto": "0zdlTzGoou", "signatures": ["ICLR.cc/2026/Conference/Submission11294/Reviewer_HGz5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11294/Reviewer_HGz5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998252830, "cdate": 1761998252830, "tmdate": 1762922441937, "mdate": 1762922441937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach to enhancing multi-step reasoning in Large Language Models by learning a dense, token-level reward model via adversarial Inverse Reinforcement Learning. Instead of imitating expert reasoning traces through supervised fine-tuning, the method infers a reward function that evaluates the quality of intermediate reasoning steps. The learned reward serves a dual purpose: as a training signal to optimize a reasoning policy, and as an inference-time critic for reranking sampled traces under a fixed compute budget. The authors demonstrate that this reward correlates with correctness rather than stylistic conformity and provides interpretable, token-level diagnostics. Empirical evaluation is primarily conducted on the GSM8K mathematical reasoning benchmark using Llama and Qwen model families."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work creatively reformulates adversarial IRL for LLM reasoning, moving beyond pure imitation learning (SFT) and outcome-based reinforcement learning (e.g., GRPO). The idea of learning a dense, process-oriented reward from demonstrations is a notable contribution.\nThe paper is generally well-written, with a clear problem formulation, method description, and experimental setup. Figures are effective in illustrating key points.\nThe concept of a unified, reusable process-level critic for training, inference, and diagnosis is promising and could influence how reasoning is optimized in LLMs."}, "weaknesses": {"value": "the primary empirical validation is confined to a single domain—mathematical reasoning on GSM8K. While GSM8K is a standard benchmark, the claims of the method's general potential for \"multi-step reasoning\" would be significantly strengthened by evaluation on a more diverse set of reasoning tasks (e.g., logical deduction, commonsense reasoning, code generation). The current evidence is insufficient to conclude that the approach generalizes beyond arithmetic problems."}, "questions": {"value": "Given that the evaluation is primarily on GSM8K, what evidence or ablation studies can the authors provide to support the claim that the learned reasoning reward captures general reasoning principles rather than features specific to mathematical word problems? Would the method be effective on a fundamentally different reasoning task, such as logical reasoning (e.g., on PrOntoQA or ProofWriter) or multi-hop QA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eGjXpVdE5I", "forum": "0zdlTzGoou", "replyto": "0zdlTzGoou", "signatures": ["ICLR.cc/2026/Conference/Submission11294/Reviewer_fXdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11294/Reviewer_fXdx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051537366, "cdate": 1762051537366, "tmdate": 1762922441317, "mdate": 1762922441317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes reframing large language model reasoning as an inverse reinforcement learning problem, where a dense, token-level reward model is learned from expert demonstrations to provide process supervision. The reward model serves dual purposes: as a training signal for optimizing a reasoning policy and as a critic for reranking sampled traces at inference time. Contributions include demonstrating that this approach prioritizes correctness over stylistic imitation, improves predictive performance via reranking, and enables interpretable error localization, evaluated on GSM8K with Llama3 and Qwen2.5 models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper creatively adapts adversarial IRL to LLM reasoning, shifting from traditional supervised fine-tuning to learning dense rewards from demonstrations, which could inspire new ways to handle multi-step tasks without explicit reward engineering.\n\n2. The presentation is strong, with well-illustrated figures (e.g., Figure 1 overview) and tables (e.g., Table 1 comparison) that effectively communicate the approach and desiderata.\n\n3. Ignoring implementation challenges, using IRL to construct dense rewards offers a promising idea for unifying training and inference in reasoning tasks, potentially reusable across domains like math and logic."}, "weaknesses": {"value": "1. Limited Novelty: The core approach appears as a straightforward application of AIRL (Fu et al., 2018), with foundational elements like GAN-style training or PRM + GRPO being well-established and widely used; it lacks significant extensions to differentiate from prior IRL adaptations in LLMs.\n\n2. Insufficient Experiments: Evaluations are confined to GSM8K, a dataset that has lost much of its challenge for modern models, raising doubts about generalization to harder benchmarks like MATH or code generation; additional datasets would strengthen claims.\nTraining Instability: While IRL is conceptually appealing, joint training of discriminator and policy often struggles with convergence due to shifting distributions, exacerbating learning difficulty—as the authors acknowledge in limitations; more robust stabilization techniques could help.\n\n3. Readiness for Publication: The paper contains several typos and inconsistencies (e.g., line 98: \"y1\" should be \"y2\"; line 340: \"Amswer\" should be \"Answer\"), suggesting it needs polishing before submission.\n\n4. Weak Inference Comparisons: For test-time scaling, comparisons are limited to random ranking; lacking baselines like outcome reward models (ORMs), process reward models (PRMs), or heuristic methods (e.g., selecting shortest responses) makes it hard to assess the method's superiority."}, "questions": {"value": "For more complex scenarios, such as problems that a reasoning model might solve through reflection, where intermediate errors could be acceptable if the final answer is correct, is this dense reward still necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bi5zIO2sZS", "forum": "0zdlTzGoou", "replyto": "0zdlTzGoou", "signatures": ["ICLR.cc/2026/Conference/Submission11294/Reviewer_APqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11294/Reviewer_APqg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244281478, "cdate": 1762244281478, "tmdate": 1762922440854, "mdate": 1762922440854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}