{"id": "Ei0wYQ2zIG", "number": 24106, "cdate": 1758352775956, "mdate": 1759896781406, "content": {"title": "Breaking Information Impedance in Deep Spiking Neural Networks via Multi-Stage Foundation-Model Distillation", "abstract": "Brain-inspired spiking neural networks (SNNs) hold great promise for low-power, event-driven computation. Yet, their performance is fundamentally constrained by information impedance induced by spiking activations and spike-based propagation, a challenge that becomes more severe in deeper architectures and under limited time-steps. In this work, we conduct an information-theoretic analysis to reveal that such information impedance constitutes a key bottleneck to the learning of deep SNNs. To address it, we propose a multi-stage knowledge distillation (KD) method that leverages a high-capacity teacher model (DINOv2) to enhance the information extraction and transmission capabilities of SNNs. By decomposing a deep high-impedance path into low-impedance stages, our method effectively mitigates the representational bottlenecks caused by spike quantization. Extensive experiments demonstrate that our method substantially boost the learning of deep residual SNNs, e.g., on ImageNet-1K with ResNet-101, our method achieves 77.14\\% top-1 accuracy, which surpasses the prior SOTA by 2.93\\%. The gains are particularly significant for fully-spiking SNNs and deeper models. Importantly, while vanilla KD has been shown sufficient for ANNs on large-scale datasets, we show that for SNNs it is far from sufficient, and overcoming the information impedance is essential to fully unlock the potential of SNN distillation.", "tldr": "", "keywords": ["Spiking Neural Networks", "Information Impedance", "Knowledge Distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/414a99a277fb1d47b7e1997987c962cf5c74b5cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper attributes the difficulty of training deep SNNs to \"information impedance\" and proposes a multi-stage distillation framework to solve it. Using a foundation model DINOv2 to inject knowledge at multiple network stages, the method enhances information flow and achieves a new state-of-the-art accuracy on ImageNet with ResNet architectures, marking an advance for the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Principled and insightful problem analysis. The paper's primary strength is its rigorous, information-theoretic framing of the core difficulty in training deep SNNs. The concept of \"information impedance\" is intuitive and powerfully supported by both theoretical propositions and compelling empirical evidence. This analysis moves the field beyond heuristic solutions and provides a clear, fundamental reason for why deep SNNs are hard to train, which is a significant contribution in itself.\n2. Well-motivated and effective method: The proposed multi-stage distillation framework is a direct and logical solution to the identified problem of information impedance and its effectiveness is demonstrated through experiments.\n3. Well-written, clearly structured, and easy to follow."}, "weaknesses": {"value": "1. About novelty of methodological components: While the application and justification are highly novel, the core components of the method—multi-stage supervision (i.e., deep supervision [1,2]) and knowledge distillation with auxiliary branches [3,4] —are established techniques in the broader deep learning literature. The paper would be slightly strengthened by explicitly acknowledging this and framing its contribution as the novel synthesis and principled application of these techniques to solve a specific, fundamental problem in SNNs.\n2. Contextualization of the Information-Theoretic Perspective: While the analysis from an information-theoretic viewpoint is a key strength, the paper could better contextualize this contribution by expanding its review of related literature. The concept of \"information impedance\" is closely related to the Information Bottleneck principle, which has been used to analyze deep ANNs [5,6,7]. Discussing more of this prior work would help situate the paper's theoretical framing within a broader context and more clearly highlight its specific insights for SNNs.\n\n\n\n\n[1] Deeply-supervised nets. Artificial intelligence and statistics. PMLR, 2015.\n\n[2] Be your own teacher: Improve the performance of convolutional neural networks via self distillation. ICCV, 2019.\n\n[3] Distilling knowledge via knowledge review. CVPR, 2021.\n\n[4] Decoupling dark knowledge via block-wise logit distillation for feature-level alignment. TAI, 2024.\n\n[5] On the information bottleneck theory of deep learning, 2019.\n\n[6] Revisiting Locally Supervised Learning: an Alternative to End-to-end Training. ICLR, 2021.\n\n[7] Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply. 2023."}, "questions": {"value": "1. About the training overhead and trade-offs. The proposed method introduces auxiliary ANN adapters and reconstruction modules, which presumably add to the computational overhead during training. I am curious about the increase in training time (e.g., wall-clock time per epoch) and memory consumption compared to the baseline direct training and vanilla KD setups. A discussion on the trade-off between this increased training cost and the significant performance gains would be valuable, as it would help researchers assess the practical costs of implementing the approach.\n\n2. On the choice of teacher model. The choice of DINOv2 as a powerful, pre-trained foundation model is a key aspect of your framework. Many prior SNN-KD works, however, use an ANN counterpart with an identical architecture as the teacher (e.g., an ANN ResNet-34 teaching a spiking ResNet-34). Could you comment on the decision to exclusively use a foundation model? How do you expect the results might differ if a standard, trained-from-scratch ANN counterpart were used as the teacher? This would help to disentangle the benefits of the multi-stage distillation architecture itself from the benefits derived from the rich, pre-existing knowledge of the foundation model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ICfshDGYHS", "forum": "Ei0wYQ2zIG", "replyto": "Ei0wYQ2zIG", "signatures": ["ICLR.cc/2026/Conference/Submission24106/Reviewer_DrHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24106/Reviewer_DrHd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643757687, "cdate": 1761643757687, "tmdate": 1762942940813, "mdate": 1762942940813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel multi-stage knowledge distillation framework (MSKD-SNN) to mitigate the information impedance that limits the training of deep spiking neural networks (SNNs). The authors present a theoretical analysis based on information theory, derive entropy bottlenecks across layers, and validate their claim empirically. They introduce a multi-stage distillation pipeline leveraging a foundation model teacher (DINOv2) and auxiliary ANN adapters, showing significant gains on ImageNet-1K (77.14% Top-1 with MS-ResNet-101)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors clearly identify information impedance as a bottleneck in deep SNNs, where representational capacity diminishes due to spiking quantization and sparse activations. It provides an insightful information-theoretic formulation and two propositions quantifying representational entropy capacity and layer-wise bottlenecks. The theoretical framing is well articulated and provides a fresh analytical lens for SNN limitations."}, "weaknesses": {"value": "1. While the paper introduces “information impedance” as a concept, it largely reformulates known ideas about information bottlenecks in quantized or discrete neural systems. The propositions (Sec. 4.1–4.2) rely on standard mutual information reasoning without new derivations beyond entropy bounds.\n2. The proposed multi-stage KD resembles hierarchical distillation or intermediate feature matching, previously explored in ANN and hybrid SNN settings (e.g., TKS, EnOF, FRTD). The distinction mainly lies in terminology (“information impedance”) rather than methodological novelty.\n3. The use of DINOv2 as teacher makes the approach less generalizable to domains where large pretrained teachers are unavailable. This dependency somewhat contradicts the neuromorphic efficiency motivation, since DINOv2-based supervision is computationally heavy.\n4. Although the paper claims to improve “training efficiency,” no quantitative measurement of energy, latency, or FLOPs is provided. The auxiliary ANN adapters and multiple distillation heads add nontrivial computation overhead, which is not analyzed.\n5. The empirical evaluation uses a reconstruction proxy, but details such as reconstruction network architecture, regularization, or sampling variance are omitted. The reliability of the “information flow” estimation remains uncertain.\n6. Results are restricted to conventional GPUs. No neuromorphic hardware tests (e.g., Loihi, Tianjic) are conducted, despite claims about energy-efficient learning. It remains unclear whether the auxiliary ANN modules are implementable in spike-based hardware.\n7. The term “breaking information impedance” suggests a paradigm shift, but the actual improvement is incremental. The language throughout could be moderated to match the scale of contribution."}, "questions": {"value": "1. How sensitive is the proposed framework to the number and depth of auxiliary adapters?\n2. Can the distillation heads be pruned after training to reduce inference cost?\n3. Does the reconstruction module affect spike sparsity or latency during inference?\n4. How would the method perform if the teacher is another SNN rather than an ANN?\n5. Can “information impedance” be quantitatively estimated during training to adaptively guide loss weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sonuMTMHT3", "forum": "Ei0wYQ2zIG", "replyto": "Ei0wYQ2zIG", "signatures": ["ICLR.cc/2026/Conference/Submission24106/Reviewer_L8b7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24106/Reviewer_L8b7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830137387, "cdate": 1761830137387, "tmdate": 1762942940574, "mdate": 1762942940574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel multi-stage knowledge distillation framework for deep spiking neural networks, supported by an information-theoretic analysis that identifies information impedance as a key bottleneck. The proposed approach leverages supervision from a large teacher model (DINOv2) across multiple intermediate stages to mitigate information loss in deep SNNs. Extensive experiments on ImageNet and its variants demonstrate significant performance improvements over prior methods. The work is well-motivated and clearly written, combining theoretical insight and practical implementation. However, some important aspects—such as energy efficiency, feature alignment, and theoretical validation of the mitigation process—require further clarification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work provides an information-theoretic analysis of SNNs, which identifies increased information impedance as a bottleneck in deep spiking models.\n\n2. This work proposes a multi-stage distillation framework. The multi-stage supervision from the teacher is novel and well-motivated, effectively decomposing a high-impedance path into multiple low-impedance stages.\n\n3. This work evaluates models of different sizes (Spike-ResNet-34/50/101 and their variants), which shows the effectiveness of the multi-stage supervision from the teacher model.\n\n4. The manuscript is clearly written, provides a valuable theoretical and experimental analysis of deep SNNs, and uses a simple modeling approach to improve accuracy."}, "weaknesses": {"value": "1. Energy efficiency and computational cost are not analyzed (only firing rate is reported), although energy savings are a core motivation for using SNNs.\n\n2. The feature alignment between the ANN block and the SNN student is not clearly explained. It is unclear how the spike features are aligned with continuous teacher features.\n\n3. Knowledge distillation using DINOv2 can increase training cost significantly, while the goal of SNNs is to reduce computational cost.\n\n4. Although the theoretical motivation behind multi-stage supervision is shown, its implications could have been further analyzed with information theory (e.g., choice of the number of layers for each stage of supervision)."}, "questions": {"value": "1. Authors are suggested to report computational complexity and energy consumption in comparison with existing works.\n\n2. How are the spiking features aligned with the ANN blocks during distillation? Since the features are of different types (spiking and continuous, respectively), the alignment strategy should be mentioned.\n\n3. Apart from the datasets used, the authors may test the method on event-based or neuromorphic datasets (e.g., Spiking Heidelberg Digits and Spiking Speech Command datasets).\n\n4. Authors may provide an ablation study on the number of supervision stages for a fixed-size model.\n\n5. The teacher model size can be varied as an ablation to show the effect of using large or small teacher models.\n\n6. The robustness results (Table 2) have not been compared with previous works; such a comparison may be included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FxQ3LUhdlJ", "forum": "Ei0wYQ2zIG", "replyto": "Ei0wYQ2zIG", "signatures": ["ICLR.cc/2026/Conference/Submission24106/Reviewer_WDs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24106/Reviewer_WDs9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888155652, "cdate": 1761888155652, "tmdate": 1762942940307, "mdate": 1762942940307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies an **information impedance** bottleneck in deep SNNs: spike quantization & spike-only propagation compress mutual information as depth grows. It formalizes the target and derives layer-wise capacity/bottleneck results under spike-count coding. Building on this, the authors propose multi-stage knowledge distillation (MSKD) from a foundation teacher (DINOv2), adding intermediate heads via lightweight ANN adapters and an input reconstruction loss. On ImageNet-1K, they report strong results for deep spiking ResNets (e.g., MS-ResNet-101 with T=4 reaches 77.14%), with especially large gains for fully-spiking Spike-ResNet-50/101."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper offers a perspective that the training difficulty of deep SNNs to an information bottleneck/impedance and theoretically characterizes layer-wise capacity and its depth scaling, aligning analysis with observed phenomena.\n\n2) Multi-stage distillation decomposes a high-impedance learning paths into low-impedance stages, injecting KL objectives at multiple depths and adding reconstruction to preserve input information; the design matches the motivation.\n2)  On ImageNet-1K, MS-ResNet-101 (T=4) reaches 77.14% and surpasses prior SOTA; gains are larger for fully-spiking Spike-ResNet-50/101.\n2) The paper compares against vanilla KD, removes the reconstruction term, varies depth/architectures, and reports robustness and teacher-size effects."}, "weaknesses": {"value": "1) **Originality of multi-stage distillation.** While I appreciate the theoretical treatment of information impedance, the multi-stage distillation design appears relatively conventional. Elements such as intermediate heads, local KL terms, and auxiliary reconstruction have prior art in both ANN and certain SNN KD settings. The novelty seems to lie more in the systematic “information impedance” perspective and its integration tailored to deep SNNs. I suggest clarifying how your approach differs from **deep supervision, layer-wise distillation, and prior SNN KD methods**.\n2) **Depth-aware objective design.** Given the claim that **information impedance grows with depth**, Eqs. (8–10) could go further by explicitly making the supervision depth-aware. Concretely, (i) increase KL/reconstruction weights with layer depth, (ii) place denser intermediate heads toward later layers, and/or (iii) use a depth-dependent temperature to sharpen targets where the bottleneck is strongest.\n3) **Training cost & fully-spike consistency.** The method depends on ANN adapters and a large DINOv2 teacher. Although intermediate heads are removed during inference, please quantify the additional training overhead—specifically in terms of compute (FLOPs or GPU-hours), memory usage, energy consumption.\n4) **Missing SEW-ResNet-50/101 results under the proposed distillation.** While Table 1 includes prior distillation baselines for SEW-ResNet-34, it does not report SEW-ResNet-34/50/101 results under the authors’ proposed distillation, and this omission is not explained."}, "questions": {"value": "Please refer to the four points included in **Weaknesses** for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KjgmUqga0Z", "forum": "Ei0wYQ2zIG", "replyto": "Ei0wYQ2zIG", "signatures": ["ICLR.cc/2026/Conference/Submission24106/Reviewer_U5GF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24106/Reviewer_U5GF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890481496, "cdate": 1761890481496, "tmdate": 1762942939891, "mdate": 1762942939891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}