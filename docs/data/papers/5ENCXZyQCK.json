{"id": "5ENCXZyQCK", "number": 15318, "cdate": 1758250219992, "mdate": 1763648528590, "content": {"title": "Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "abstract": "Large Language Models (LLMs) typically reason through explicit, step-by-step natural-language traces. Humans, however, also rely on non-linguistic, unconscious processes, such as the inspirations that emerge during the incubation period. In this work, we introduce LatentSeek, a novel framework designed to enhance the reasoning capabilities of LLMs through Test-Time Instance-level Policy Gradient within the model’s latent space—thus complementing explicit natural-language steps. LatentSeek employs policy gradient optimization to iteratively refine latent representations, guided solely by a self-generated reward signal. This allows the model to adapt its reasoning trajectory dynamically on a per-instance basis. Empirical evaluations across diverse benchmarks, GSM8K, MATH-500, and AIME2024 as well as  multiple LLM families (e.g., LLaMA, Qwen) demonstrate that LatentSeek outperforms established baselines, including Chain-of-Thought (CoT), Best-of-N (BoN) and training-based methods. Further analysis indicates that LatentSeek is computationally efficient, typically converging within a few optimization iterations for average-level problems. Moreover, the model's performance improves as the number of latent update iterations increases, highlighting the benefits of exploring within the latent space. These findings highlight LatentSeek as a lightweight and effective paradigm for improving the reasoning capabilities of LLMs without changing their parameters.", "tldr": "We propose to enhance LLM reasoning ability via test-time instance-level policy gradient in latent space.", "keywords": ["reasoning", "test-time", "instance-level", "policy gradient", "latent space", "latent reasoning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2dc02c73897b5e22440290b5e3fcb0f66acb96df.pdf", "supplementary_material": "/attachment/d6f6ac969beaa98e1f189994e67bbc5a3acab774.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LATENTSEEK, a framework that enhances LLM reasoning by performing test-time policy gradient optimization in the latent space rather than over explicit tokens. Instead of fine-tuning model parameters or relying on handcrafted prompts, LATENTSEEK performs lightweight, instance-level adaptation: it iteratively updates latent representations (pre-decoding states) guided by a self-generated reward."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of latent-space optimization without parameter updates is noval and bridges reinforcement-based adaptation with test-time inference.\n\n2. Improving the quality of CoT paths by optimizing the latent space seems interesting, especially since the authors mention that the number of iterations is usually less than 3. This provides insight into test-time scaling.\n\n3. The experimental results are good."}, "weaknesses": {"value": "1. While the authors highlight that latent updates yield incoherent tokens, they do not analyze what these updates actually do in embedding space (e.g., directionality, similarity drift, or semantic shifts).\n\n2. The self-generated reward signal (Eq. 8) is vaguely defined and may not correlate with correctness. No ablation isolates its contribution or evaluates instability across tasks.\n\n3. Although the paper references “Appendices C–D for theoretical justification,” the main text does not clarify why the independence assumption between latent representations (Eq. 7) holds or how gradient estimation avoids collapse.\n\n4. Since the reward takes only binary values (0/1), it lacks continuous gradient information and therefore cannot provide fine-grained directional guidance.\nAs a result, the optimization may rely more on stochastic noise than on the objective reflected by the reward itself. Moreover, if the model’s self-evaluation were sufficiently accurate, one could simply adopt that evaluation as the final decision; if it is inaccurate, then its validity as an optimization signal becomes questionable. This reveals an inherent inconsistency: the model’s ability to assess its output after generation does not necessarily translate into an effective signal for guiding latent optimization before generation.\n\n5. The model optimizes only the first ρT (e.g., 20%) of the latent variables, with the number of iterations limited to ≤3. Such mild perturbation may be functionally equivalent to a single stochastic exploration step, where improvements arise from increased generative diversity rather than genuine gradient guidance. It is recommended to include control experiments with random perturbation (Random Perturbation / Reverse Reward) to rule out this factor.\n\n6. The proposed method first initializes the reasoning process with an explicit CoT trajectory and then performs optimization in the latent space. This design appears to rely heavily on the quality of the initial CoT trajectory. If the initial reasoning path deviates from the correct direction, performing only about three iterations of updates on merely 20% of the latent representations may be insufficient to substantially correct the reasoning trajectory.\n\n7. In most reasoning sequences, errors tend to occur in the middle or later stages rather than at the beginning. Therefore, optimizing only the early latent tokens may not effectively address the actual sources of error. Overall, the method seems to perform slight adjustments or perturbations to the initial reasoning direction. I remain skeptical that its effectiveness arises mainly from random perturbations to the initial direction, rather than from genuine reinforcement learning with stable and informative reward signals—especially given that the reward employed in the paper appears highly noisy."}, "questions": {"value": "I prefer the style and insight of the paper, but there are some issues that lead me to have some reservations. I lean more towards borderline (but there is no option for a score of 5). If the authors can address some of my concerns, I would be willing to raise my rating to acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SlOfnWbJKc", "forum": "5ENCXZyQCK", "replyto": "5ENCXZyQCK", "signatures": ["ICLR.cc/2026/Conference/Submission15318/Reviewer_EsPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15318/Reviewer_EsPa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207946226, "cdate": 1761207946226, "tmdate": 1762925615738, "mdate": 1762925615738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LatentSeek, a framework aimed at enhancing the reasoning capabilities of large language models (LLMs) by leveraging Test-Time Instance-level Policy Gradient optimization in the latent space. Unlike standard approaches that rely solely on explicit, step-by-step natural-language reasoning traces, LatentSeek iteratively refines the model’s latent representations based on a self-generated reward signal, allowing dynamic, per-instance adaptation of the reasoning trajectory. Empirical evaluations on multiple benchmarks (GSM8K, MATH-500, AIME2024) and across various LLM families (e.g., LLaMA, Qwen) show that LatentSeek outperforms established baselines, including Chain-of-Thought (CoT), Best-of-N (BoN), and training-based methods. Further analyses indicate that LatentSeek is computationally efficient, typically converging within a few iterations, and that performance improves with more latent update iterations. Overall, the study presents LatentSeek as a lightweight and effective paradigm for improving LLM reasoning without modifying model parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is highly novel, and its motivation is very clearly articulated.\n2. The experiments in this paper are comprehensive, covering various large language models and multiple benchmarks. The analyses are thorough and convincingly demonstrate the effectiveness of the proposed method.\n3. The methodology section is very clearly written, and the structured, formal presentation further clarifies how the method is applied."}, "weaknesses": {"value": "1. This method is a instance-level policy gradient method. Each input instance requires separate iterative optimization of its latent representations, which reduces inference efficiency, especially for large-scale or complex tasks.\n2. The updates occur in the latent space, making the internal changes of the model difficult to interpret. As a result, users or researchers may find it challenging to analyze or debug the model's reasoning process, reducing its practical usability."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k16VSAbPjL", "forum": "5ENCXZyQCK", "replyto": "5ENCXZyQCK", "signatures": ["ICLR.cc/2026/Conference/Submission15318/Reviewer_G8rH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15318/Reviewer_G8rH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811245172, "cdate": 1761811245172, "tmdate": 1762925615055, "mdate": 1762925615055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework, LATENTSEEK, designed to enhance the reasoning capabilities of large language models (LLMs) by applying instance-level policy gradient optimization in the latent space during inference. The motivation behind this approach is that, similar to human cognition, the optimal reasoning path for LLMs may not be limited to explicit natural language expressions but could instead leverage implicit, non-linguistic processes. The core idea of LATENTSEEK is to fix the model parameters during inference and iteratively optimize the latent representations of the input sequence to maximize a self-generated reward signal. This method bypasses the expensive cost of model fine-tuning. Experimental results across several reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, as well as across different model families, show that LATENTSEEK significantly outperforms methods such as CoT, Best-of-N, and training-driven approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.The approach demonstrates significant innovation by employing policy gradient methods (e.g., REINFORCE) for instance-level optimization of continuous latent space representations during test-time inference to guide LLM reasoning.\n2.The experimental results are highly promising, consistently achieving SOTA performance across multiple challenging mathematical reasoning datasets. \n3.The framework exhibits high computational efficiency, typically converging within a few optimization iterations for tasks of moderate difficulty."}, "weaknesses": {"value": "1.The reliability of the current self-generated reward function is questionable when faced with \"incoherent\" intermediate reasoning steps in the model's output. If the reward function is not sufficiently accurate, it may guide the latent space optimization towards local optima or incorrect reasoning paths.\n2.In the derivation of the policy gradient, the authors assume that the latent representations $z$ are independent of each other. Is this assumption appropriate for a Transformer-based sequence model?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mYqKFevXgR", "forum": "5ENCXZyQCK", "replyto": "5ENCXZyQCK", "signatures": ["ICLR.cc/2026/Conference/Submission15318/Reviewer_UKFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15318/Reviewer_UKFa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894383855, "cdate": 1761894383855, "tmdate": 1762925614315, "mdate": 1762925614315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LATENTSEEK, a training-free, test-time instance-level policy-gradient method that optimizes latent representations to improve reasoning, instead of optimizing prompts or model parameters. It initializes latents from a CoT decode, then iteratively updates a prefix of latents via REINFORCE using a self-generated reward, re-decoding after each update. Experiments on GSM8K, MATH-500, and AIME2024 with backbone models report consistent gains over CoT/BoN and several RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written and structured, followed by the motivation and explanation.\n2. The paper gives a compact REINFORCE formulation for test-time latent updates with token-factorization and per-latent gradients.\n3. Experiments show average gains over several baselines and robust improvements on AIME2024."}, "weaknesses": {"value": "1. The algorithm recursively computes the latent representations $z$ and decodes them back to $x$, which is more time-consuming compared to traditional latent-space methods that encode the text into latent representations only once. I suggest that the authors report the time or computational cost.\n2. In Line 150, Equations (6) and (7), the authors assume independence among latent representations. However, transformer hidden states are typically highly correlated across time steps.\n3. The hyperparameter $\\rho$ appears to be crucial to model performance. It would be helpful for the authors to clarify how it is selected and whether any hyperparameter tuning was conducted."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sdS93Qu7D6", "forum": "5ENCXZyQCK", "replyto": "5ENCXZyQCK", "signatures": ["ICLR.cc/2026/Conference/Submission15318/Reviewer_7PJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15318/Reviewer_7PJX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15318/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232498293, "cdate": 1762232498293, "tmdate": 1762925613824, "mdate": 1762925613824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}