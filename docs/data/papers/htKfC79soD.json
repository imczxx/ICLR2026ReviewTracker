{"id": "htKfC79soD", "number": 4413, "cdate": 1757676054289, "mdate": 1759898033795, "content": {"title": "Joint Geometry–Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion", "abstract": "Reconstructing both geometry and appearance of a digital human from a single image remains highly challenging. Existing approaches typically decouple geometry and appearance, employing separate models for each, which limits their ability to reconstruct digital humans in a unified manner.\nIn this paper, we propose JGA-LBD, which formulates human reconstruction as a bridge diffusion task in a unified latent space, yielding a joint latent representation that encodes both geometry and appearance.  We address the challenge of human reconstruction from heterogeneous conditions, i.e., depth maps and SMPL models estimated from RGB images. Directly combining heterogeneous modalities introduces substantial training difficulties, to overcome this, we unify all conditions into 3D Gaussian representation and compress them into a unified latent space using a sparse variantional autoencoder. All diffusion learning is then conducted within this unified latent space, which markedly reduces optimization complexity. Our setting strikingly lends itself to bridge diffusion: the depth map can be regarded as a partial observation of the target latent code, enabling the model to focus solely on inferring the missing components. Finally, a decoding module reconstructs geometry and renders novel-view images from the latent representation. Experiments demonstrate that JGA-LBD outperforms state-of-the-art methods in both geometry and appearance, and generates plausible results on in-the-wild images.", "tldr": "", "keywords": ["3D human reconstruction", "geometry", "diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d1ed4649bef3c455db7686e0408e4b0ba89ceaf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to unify heterogeneous modalities, such as depth maps and SMPL models, into 3D Gaussian representations. These modalities are jointly modeled in a unified latent space via a Sparse Variational Autoencoder (Sparse VAE), and a diffusion model is subsequently employed to generate complete latent codes.\n\n- Datasets: The model is trained on Thuman2.1. Evaluation is conducted on the 2K2K (25 scans) and CustomHuman (40 scans) benchmarks. Furthermore, the model's generalization capabilities are validated on in-the-wild images.\n\n- Metrics: Evaluation metrics include PSNR, SSIM, and LPIPS for texture quality, alongside Chamfer Distance (CD) and normal error for geometric accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The reconstruction of 3D Gaussian Splatting (3DGS) or mesh representations from a single image is a critical problem with numerous downstream applications.\n\n2. The proposed method facilitates end-to-end training, bypassing multi-stage synthesis pipelines (e.g., multi-view image generation followed by geometric reconstruction). This design potentially enhances inference efficiency compared to conventional multi-step approaches. The paper presents a framework that jointly leverages depth priors (from DepthAnything), diffusion priors, and SMPL priors."}, "weaknesses": {"value": "1. The integration of geometric priors into the diffusion process to guide generation is a relatively established technique, as seen in works like DiffSplat and Hunyuan3D. Furthermore, leveraging an SMPL prior as a structural constraint has been demonstrated in several existing methods (e.g., SiFU, SiTH), which calls the novelty of this specific contribution into question.\n\n2. The paper provides insufficient visualization results, making it difficult to assess the 3D consistency of the proposed method. Based on prior work in human avatar generation, a minimum of `15-20 video results` are typically required to substantiate generalization claims. Comparative video analysis against other methods is also strongly recommended. The presented qualitative results are unconvincing. Specifically, Fig. 2 and Fig. 3 exhibit significant loss of detail in facial regions and clothing. Moreover, the model performs poorly in challenging scenarios, such as the loose-fitting dress depicted in the third row. This apparent qualitative failure contrasts sharply with the reported quantitative metrics (e.g., `PSNR 29.91`, `SSIM 0.943`), which are surprisingly high, approaching the performance ceiling typically associated with multi-view reconstruction.\n\n3. A fundamental representational trade-off exists: mesh-based methods typically excel in geometric fidelity (e.g., sharp contours, fewer artifacts/floaters), whereas 3DGS representations generally offer superior texture modeling (e.g., facial details, complex clothing). Achieving state-of-the-art (SOTA) performance in both geometry and texture simultaneously within a single representation is non-trivial. It is recommended that the authors align their evaluation benchmark with established standards in existing literature.\n\n4. The paper omits comparisons with several critical and highly relevant baselines in human avatar generation, notably HumanSplat [1], Human3Diffusion [2], and PSHuman [3]. Empirically, these human-centric models demonstrate superior texture fidelity and overall quality compared to general object generation models, making them essential points of comparison.\n\n5. Given that the method jointly utilizes multiple priors (`DepthAnything V2`, a `diffusion prior`, and an `SMPL prior`), a comprehensive ablation study is required. This study must meticulously dissect the contribution of each component to quantify its relative importance and impact on the final performance.\n\n6. `Critical implementation details are omitted`. For example, while citing relevant works [e.g., 4, 5], the authors do not specify the exact architecture of the diffusion model employed as a prior, its parameter count, or the computational runtime.\n\n\nIf the authors can address the concerns, I am willing to raise my score.\n\n\n [1] Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models, NeurIPS 2024\n\n[2] HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors. NeurIPS 2024\n\n [3] PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing, CVPR 2025 \n\n[4] Zhou et al. Denoising diffusion bridge models. ICLR, 2024b \n\n[5] xiang et al. Structured 3d latents for scalable and versatile 3d generation, CVPR 2025"}, "questions": {"value": "- What is the exact architecture of the diffusion model employed?\n\n-  In sparse VAE's total objective, $ L_{Occ} $ (occupancy loss) and $\\lambda_7$ are undefined, with no calculation/role explained . \n\n- What is the parameter count of this model?\n\n- In bridge diffusion SDE, $g(t)$ (diffusion coeff) and $f(x_t,t|\\mathcal{S}_L)$ (drift term) lack definitions.\n\n- What is the associated computational runtime (e.g., inference latency)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xus6Jgsk94", "forum": "htKfC79soD", "replyto": "htKfC79soD", "signatures": ["ICLR.cc/2026/Conference/Submission4413/Reviewer_gV1m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4413/Reviewer_gV1m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586895569, "cdate": 1761586895569, "tmdate": 1762917351401, "mdate": 1762917351401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JGA-LBD to address the challenge of reconstructing both human geometry and appearance from a single RGB image. The core contribution is to formulate this as a \"bridge diffusion\" task within a \"unified latent space,\" which jointly encodes both geometry and appearance. The method takes depth and SMPL priors, unifies them into a 3D Gaussian (3DGS) representation, and then compresses this into a compact latent space. A bridge diffusion model is then used to reconstruct the complete 3D human from this latent code. Experimental results demonstrate that this approach yields improvement over 3DGS-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The model handles a case involving a child, which is typically a difficult body shape to reconstruct.\n2. Although the method is based on an SMPL prior, the final visual results appear to overcome some of SMPL's common limitations and achieve a good-quality outcome."}, "weaknesses": {"value": "### **Major**\n\n1. **Methodological Clarity:** The core methodology is not explained with sufficient clarity and is difficult to follow. Several key components are either ambiguous or inadequately described.\n    - The illustration in Fig. 1 is confusing. The \"Depth\" and \"SMPL\" inputs are depicted with color, which is misleading. Furthermore, visualizing these inputs in a 3DGS style implies a conversion has already occurred.\n    - **Input Conversion:** The paper fails to explain the process of converting Depth and SMPL data into the 3DGS-style representations shown. If this involves intermediate steps (e.g., depth back-projection to a colored point cloud, or SMPL vertex color gathering), these intermediate modalities are incorrectly labeled. For clarity, I strongly suggest renaming them to reflect their true nature, such as \"depth-derived 3D points\" or \"SMPL-derived mesh.\"\n    - **3DGS and Voxel Grids:** The association between the 3DGS representation and voxel grids (L158) is not explained. The authors should clarify if a voxelization process is used and detail its role in the pipeline.\n2. **VAE Training Process:** The training procedure for the compression VAE is a critical, unexplained step. L215 states that \"ground truth 3DGS\" is used for training, but the cited datasets (Thuman2.1, 2k2k) do not natively provide data in this format; L309 mentions that 1600 scans were used to \"prepare\" these attributes, but the preparation process itself is not detailed. This omission makes the method difficult to reproduce and its foundation unclear.\n3. **Missing Ablation Studies:** The design choice of integrating both depth and SMPL priors is not empirically justified.\n    - Given that SMPL models inherently contain 3D shape and (implicit) depth information, the necessity of an additional 2D depth map modality is not obvious.\n    - The paper would be significantly strengthened by an ablation study that evaluates the performance of the method using only SMPL priors, without depth (and using only depth priors with SMPL). Both quantitative and qualitative results should be provided for this comparison.\n4. **Insufficient Experimental Comparison:** The experimental validation is lacking. The paper only compares against other 3DGS-based methods. There is a severe lack of comparison against mainstream methods that generate textured meshes (e.g., ECON, SiTH, GTA, PSHuman). Without this context, the claimed superiority of the method is unsubstantiated. (Please refer to the qualitative comparisons in the paper of IDOL.)\n5. **Notation and Formalism:** The paper's notation is imprecise. Key variable dimensions (e.g., the latent dimension's shape, whether 1D or 3D) are not specified. Distinct notations should be introduced for the initial inputs (Depth, SMPL) versus their processed derivatives (e.g., depth-derived points, SMPL-derived mesh) and their corresponding latent representations.\n\n### **Minor**\n\n1. **Missing Related Work**: The paper omits a large and relevant category of optimization-based (SDS) methods, which often have better generalization, including:\n    - TeCH: Text-guided Reconstruction of Lifelike Clothed Humans\n    - Human-SGD: Single-Image 3D Human Digitization with Shape-Guided Diffusion\n    - GeneMAN: Generalizable Single-Image 3D Human Reconstruction from Multi-Source Human Data\n2. **Minor Grammatical Error**: L223 contains a grammar mistake (\"enables generate\" instead of \"enables generating\")."}, "questions": {"value": "1. Why does a method based on an SMPL prior (which can introduces pose errors) ultimately achieve such good results? Is this improvement come from depth regularization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q2slbDwXjJ", "forum": "htKfC79soD", "replyto": "htKfC79soD", "signatures": ["ICLR.cc/2026/Conference/Submission4413/Reviewer_dZF7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4413/Reviewer_dZF7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826566871, "cdate": 1761826566871, "tmdate": 1762917351062, "mdate": 1762917351062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes JGA-LBD, a method that reconstructs both 3D geometry and appearance of humans from single RGB images using bridge diffusion in a unified latent space. The key innovation is compressing heterogeneous modalities (depth maps and SMPL models) into 3D Gaussian representations, then learning to generate complete human models via bridge diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This jointly models smpl, geometry and appearance in a single latent space, ensuring better consistency.\n\n2.Outperforms state-of-the-art on both 2K2K and CustomHuman benchmarks across all metrics (geometry: CD, P2S, Normal; appearance: PSNR, SSIM, LPIPS)"}, "weaknesses": {"value": "1.Sparse convolutions with Minkowski Engine and Bridge diffusion are standard established techniques. The main contribution is primarily engineering/combination rather than fundamental innovation.\n\n2. VAE reconstructed results seems blurry, how this vae is compared with other formulations such as Trellis vae?\n\n3.The in-the-wild examples show relatively standard poses. Challenging cases aren't thoroughly evaluated.\n\n4.Looth clothes cases are not well presented.\n\n5.The \"unified latent space\" claim is somewhat oversold - depth and SMPL are converted to 3DGS format but remain distinct modalities.\n\n6.Missing references: TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. In 3DV 2024 \nMagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement. In Arxiv 2024\nGeneralizable Human Gaussians from Single-View Image. In ICLR 2025 \nHumangif: Single-view human diffusion with generative prior. In Arxiv 2025 \nWonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction. In Arxiv 2025"}, "questions": {"value": "How does the method perform when upstream depth/SMPL estimation fails? The paper doesn't analyze failure modes or provide robustness analysis when Depth Anything V2 or PIXIE produce incorrect predictions.\n\nWhat is the actual computational cost? Training time, inference time, and memory requirements are not reported. With 200k iterations for VAE and 100k for diffusion on 4× A6000 GPUs, this seems computationally expensive.\n\nWhy not learn depth and SMPL prediction end-to-end? The modular pipeline with frozen pre-trained models may be suboptimal compared to joint training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6fb80jgZSH", "forum": "htKfC79soD", "replyto": "htKfC79soD", "signatures": ["ICLR.cc/2026/Conference/Submission4413/Reviewer_NQHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4413/Reviewer_NQHJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987589026, "cdate": 1761987589026, "tmdate": 1762917350709, "mdate": 1762917350709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}