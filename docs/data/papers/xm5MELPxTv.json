{"id": "xm5MELPxTv", "number": 9419, "cdate": 1758121860643, "mdate": 1763663475454, "content": {"title": "Transformers through the lens of support-preserving maps between measures", "abstract": "Transformers are deep architectures that define ``in-context maps'' which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for a vision transformer). Previous work has studied the ability of these architectures to handle an arbitrarily large number of context tokens. To mathematically, uniformly analyze their expressivity, previous work considered the case that the mappings are conditioned on a context represented by a probability distribution which becomes discrete for a finite number of tokens. Modeling neural networks as maps on probability measures has multiple applications, such as studying Wasserstein regularity, proving generalization bounds and doing a mean-field limit analysis of the dynamics of interacting particles as they go through the network. In this work, we study the question what kind of maps between measures are transformers. We fully characterize the properties of maps between measures that enable these to be represented in terms of in-context maps via a push forward. On the one hand, these include transformers; on the other hand, transformers universally approximate representations with any continuous in-context map. These properties are preserving the cardinality of support and that the regular part of their Fr\\'{e}chet derivative is uniformly continuous. Moreover, we show that the solution map of the Vlasov equation, which is of nonlocal transport type, for interacting particle systems in the mean-field regime for the Cauchy problem satisfies the conditions on the one hand and, hence, can be approximated by a transformer; on the other hand, we prove that the measure-theoretic self-attention has the properties that ensure that the infinite depth, mean-field measure-theoretic transformer can be identified with a Vlasov flow.", "tldr": "We answer the question what kind of maps between measures are essentially transformers.", "keywords": ["Transformers", "Characterization", "Maps between measures", "Universal approximation", "Wasserstein topology"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9114cd93f2fc93483b5c4ab6f065088094d0f0cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To analyze the expressivity of transformers with arbitrarily large context, previous work suggested to view the input context as a probability distribution, and extend transformers accordingly to operate on probability distributions. This paper studies mathematically which maps between measures can be realized by a transformer."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Possibly, some deep math which I did not understand, to a large extent  because IMHO the writing is poor"}, "weaknesses": {"value": "* The author's motivation for addressing this problem is not clear. What do the results teach us about learning theory or expressivity of transformers? Why is this inquiry interesting, given prior work on transformer universality?\n* There are lots of abstract mathematical definitions without an explanation of how they relate to the standard formulation of transformers. For example, why are we interested in maps f between measures, and why are we interested in them being generated by 'in context maps'?\n* The main result provides a universality guarantee under some condition which is uninuitive. It is not clear how reasonable it is."}, "questions": {"value": "I do not think the paper can be accepted in its present form. If the authors wish to despute this claim, they can try to address the weaknesses mentioned above.\n\nBelow are some minor writing suggestions **which should not be addressed in the rebuttal**\n* Line 40:\" Leverage contexts of arbitrary length\" sounds too strong a statement\n* Line 41: \"the previous work\". The word \"the\" should be omitted.\n* Line 50 starting from \"the size\". This sentence  could be rewritten for clarity.\n* Line 105 and elsewhere. I would write \"certain\" without the parenthesis.\n* I was confused by Definition 2. In the definition you mention a \"in context map\", but you did not define this previously. My understanding is that any $G$ of this form is considered an in context map. Correct? It would be good to clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qCpkmDaVsN", "forum": "xm5MELPxTv", "replyto": "xm5MELPxTv", "signatures": ["ICLR.cc/2026/Conference/Submission9419/Reviewer_kfm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9419/Reviewer_kfm9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579759003, "cdate": 1761579759003, "tmdate": 1762921022509, "mdate": 1762921022509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper fully characterizes mappings between spaces of positive measures given by the pushforward of a continuous in-context mapping: $\\mu \\mapsto G(\\mu) \\mu$. This characterization depends on the regularity of a generalization of the Frechet derivative of the map. As a consequence of prior work that studies the ability of transformers to approximate continuous in-context mappings, the paper deduces that any map satisfying this regularity condition can be approximated by a transformer. As a representative example, the paper demonstrates that the solution map to the Vlasov equation satisfies this regularity condition, and hence can be represented by a transformer. Conversely, the paper also shows that mean-field transformers can also be identified with Vlasov equations in some sense."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The interpretation of transformers are measure-to-measure mappings has become important in understanding their ability to solve scientific problems (such as sampling and PDEs), and it is therefore natural to ask which such mappings can be realized by a transformer. This paper takes a first cut at this problem and establishes a necessary and sufficient criterion. I feel that the work has potential for impact, as it gives a recipe for proving approximation results for transformers beyond the Vlasov equation.\n\nIn addition, the paper is well-written, and the authors do a nice job of conveying intuition despite the technical nature of the results."}, "weaknesses": {"value": "The main weakness is that the approximation results are purely qualitative and do not provide bounds on the size of the transformer needed to approximate a given map. The authors acknowledge this limitation in the paper.\n\nAdditionally, the regularity condition is fairly technical, and thus it is not clear to me how difficult it is to verify it beyond the Vlasov equation, or how abundant are the mappings which satisfy it."}, "questions": {"value": "If I understand correctly, the support-preserving condition is not satisfied by autoregressive/causally-masked transformers, where the prediction of the 'i'-th token depends only on the first 'i'. tokens. I know that this is not the focus of the paper, but it may be worth to comment on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8lTooBmyVm", "forum": "xm5MELPxTv", "replyto": "xm5MELPxTv", "signatures": ["ICLR.cc/2026/Conference/Submission9419/Reviewer_uV5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9419/Reviewer_uV5C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759824518, "cdate": 1761759824518, "tmdate": 1762921022173, "mdate": 1762921022173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "# Context \nFor context, my original recommendation was to desk reject the submission due to breaching author anonymity, due to the reasons provided below in my original review. After the review deadline, the AC told me that they had consulted with the senior AC and PCs, who disagreed with me regarding author anonymity. I maintain my original stance, but the AC has asked me to write an updated review after the review deadline, due the SAC/PC position. I have done so (on the weekend at short notice), as detailed below, but have kept my unedited original review for posterity. I have done my best to write my updated review independently of, and unbiased by, my original reading of the work. I strongly urge the authors to remove the first person \"we\" in the abstract in the next update of their work.\n\n======================================\n\n# Original Review, Nov 1\n\nRecommending desk reject as the authors appear to have breached anonymity in my reading. \n\nNamely, the authors write on line 13 of the abstract: \"In previous work, **we** studied the ability of these architectures to handle an arbitrarily large number of context tokens\", and on line 50 write \"The size of this context might be very long, possibly arbitrarily long, which has been addressed in Furuya et al. (2024) that concerns the transformers as universal in-context learners.\"\n\nAs per the author guide, https://iclr.cc/Conferences/2026/AuthorGuide, \"Any paper where author identity is revealed in either the main text or the supplementary material will be desk rejected. Note that related arxiv papers by the same authors do not break anonymity; if cited, these should be cited in third person.\". The use of \"we\" in line 13 is first person.\n\n===========================\n\n# Updated review, Nov 8\nThis submission studies \"transformers through the lens of support-preserving maps between measures\". Namely, the authors build on previous work, Furuya et al, 2024 (F24), that establishes the universality of transformers within the family of continuous in-context mappings (definition 2). This work builds on F24 by characterising the nature of \"support-preserving maps\" (definition 1) that transformers can approximate, by identifying a certain smoothness condition (theorem 1, B2) that is essential for such in-context maps support-preserving maps to be in-context maps. This essentially covers the first 8 pages of the submission in my reading. The last page of the main paper is a corollary of theorem 1, that shows that the solution of the Vlasov equation satisfies the introduced conditions.\n\nI am giving the submission a score of 2, for reasons detailed below, but would give a 3 if there was the option."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper appears to be correct and demonstrates a condition to characterise the family of in-context maps within the family of support-preserving maps.\n- There is a significant amount of mathematical maturity that has gone into this paper (see also weaknesses)."}, "weaknesses": {"value": "Points 1-3 are all related and combine for my main criticism of the submission.\n1. **Technical novelty** The paper is heavily inspired by F24, and does not appear to be a significant technical advancement upon F24. I am not an expert here, but as I understand it, the technical contribution of this paper is to identify conditions (in theorem 1) that separate/equate the family of in-context maps from the family of support-preserving maps. Then this paper leans on F24 in Corollary 1, which has already established the universality of transformers within the family of in-context maps, to show the universality of transformers for support-preserving maps that satisfy conditions B1/B2 as a corollary. Based on this reading, it appears like F24 provided a significant result, and that this submission is a more minor addition on top of F24.\n2. **Technical relevance to ICLR community** Relatedly, if on the other hand the results of Theorem 1 (providing the conditions for equivalence between \"in-context maps\" and \"support-preserving maps\") are significant technically, it doesn't feel like an ML conference like ICLR is the best venue to appreciate the importance of the specific smoothness conditions (B2) that separate two families of maps between measures. It would be helpful if the authors could better motivate what this distinction means, i.e. what does it mean to satisfy B2 or not, and which bigger picture intuition can someone working on transformers take away from this?\n3. **Motivation** Also relatedly, the authors do not appear to do a good job of motivating the significance/importance of their work, in my reading. For example, with \"support-preserving maps\" the authors write they are \"natural\" to study transformers on line 170 and provide some links, but the context should be provided on line 170 and, in any case, the links provided do not make the motivation sufficiently clear to this reviewer. Given the importance of \"support-preserving maps\" for the technical contribution of this submission (aforementioned in point 1), this motivation needs to be stronger. Indeed, \"support-preserving maps\" appears to be a term coined in this submission, as opposed to a standard term in the literature. To play devil's advocate, what does the knowledge that certain support-preserving maps are in-context maps add for a transformers/deep learning researcher, on top of the knowledge that transformers are universal within the family of in-context maps (from F24)? Likewise, what is the importance of the finding that Vlasov equation solutions satisfy certain conditions in Section 4.2 and can be approximated by a transformer? The context for this is somewhat lacking (I hadn't heard of Vlasov equations before and I imagine most of the ICLR community would not have either).\n4. **Lack of accessibility** As mentioned, the submission requires quite heavy mathematical maturity to parse. This by itself is not a weakness, but I think the way the paper is written has several drawbacks for enabling a reader to understand it. One drawback is that the authors do not provide sufficient context of the mathematical objects studied in the paper, for the reader: for example the lacking motivation for \"support-preserving maps\" or Vlasov flows, as mentioned above. But in addition, the reader is expected to know a lot of notation/nomenclature that is often not found in ICLR papers, e.g. zero-homogeneous extensions or uniformly continuous or even basic measure theory (borel sets, sigma algebras etc), for which it would be helpful to have definitions or references provided. There are also a few typos/omissions which appear to be oversights that compromise the reader accessibility. For example, $\\cal{M}$ missing $\\cal{M}^{+}$ on line 143, or $\\cal{P}_{fin}$ is not defined on line 173 (this is bad as it's not clear that $f$ on line 180 is well defined as a result), or on line 192 what is $i$ (should it be $\\forall i$?)\n5. **Quantitative results** The authors mention the lack of quantitative results but it would for sure improve the broader relevance of the paper to the ICLR community to know how the universality is affect by e.g. changing the number of layers or the number of heads? Likewise, what is special about transformers to enable universality as opposed to other sequence models, e.g. state space models, which are known to perform worse on in context learning (though I guess this is outside the scope of the paper at present)."}, "questions": {"value": "- Why is the background material on basic transformers introduced in line 188, in the middle of an unrelated section that is describing Wasserstein metrics/distances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "author anonymity (this is from my original review, and I cannot delete in the updated review)."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBywDc2xbu", "forum": "xm5MELPxTv", "replyto": "xm5MELPxTv", "signatures": ["ICLR.cc/2026/Conference/Submission9419/Reviewer_gZyY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9419/Reviewer_gZyY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013767071, "cdate": 1762013767071, "tmdate": 1762921021868, "mdate": 1762921021868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}