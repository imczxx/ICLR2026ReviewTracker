{"id": "u1ChGXQEWa", "number": 2798, "cdate": 1757253475746, "mdate": 1763665444612, "content": {"title": "CL-Gen: An Inference-Time Iterative Optimization Framework for Reference-Consistent Image Generation Based on Closed-Loop Control", "abstract": "Controllable image generation technology enables precise content synthesis based on user-provided reference conditions, garnering significant research attention. However, existing methods still face significant challenges in maintaining reference consistency, as they lack the observation and error correction for the reference consistency of generated images. Inspired by the concept of closed-loop systems in control theory, we propose a framework that enhances reference consistency through an iterative optimization scheme during inference time. It takes the image generation model as the control plant, observes and feeds back the actual generation state, and then adjusts the input of the generation model through a modified PID (Proportianl Integral Derivative) controller to enhance reference consistency. This framework supports a variety of controllable generation methods and different types of control conditions. Moreover, it is easy to implement, requiring only the addition of a few lines of code without the need for extra training. We validate the application of this framework in three key tasks: identity-preserving portrait generation, pose-controlled generation, and depth-controlled generation. For identity-preserving portrait generation, our method improves facial similarity by 12.07\\%. For pose-controlled and depth-controlled generation, errors are reduced by 32.64\\% and 33.49\\%, respectively. This work not only provides a solution for reference-consistent image generation but also offers a new perspective: controllable image generation can be conceptualized as a control problem, wherein control theory is amenable to application for performance optimization. Our code will be released upon publication.", "tldr": "This paper proposes a closed-loop framework to iteratively optimize reference consistency in generated images at inference time.", "keywords": ["Image generation", "Closed loop control", "Iterative optimization", "Reference consistency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f685706bfdd7b4348411b54abbfbc9b9c4094b7c.pdf", "supplementary_material": "/attachment/6650dc30dc427106f4e97c2828e0b73a203173fb.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the task of controllable image generation with a focus on improving reference consistency between generated images and user-provided conditions. The authors propose a closed-loop, iterative optimization framework inspired by control theory, using a PID controller to dynamically adjust the generation model's inputs based on feedback during inference. The method is compatible with various controllable generation approaches and can be easily implemented without additional training. Experimental results demonstrate significant improvements in identity preservation, pose control, and depth control, highlighting the method's effectiveness and generalizability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem with clearly articulated motivation.\n- Experimental results demonstrate that the proposed method outperforms existing approaches."}, "weaknesses": {"value": "- The paper appears to be somewhat over-packaged. Its core idea—a simple iterative feedback mechanism introduced into conditional image generation—is straightforward, yet the authors emphasize the concept of closed-loop systems from control theory, which seems unnecessary. The use of closed-loop terminology primarily serves to label different stages of the generation process, without establishing a substantive connection between the two.\n- The introduction to closed-loop theory is insufficiently detailed. Most of the target readers are likely unfamiliar with this theory, yet the explanation is overly brief and may cause confusion. For example, it is unclear what u and v represent in Equation 1, what their subscripts denote, what the sampling period is, why it is needed, and what the meaning of Equation 1 is.\n- In Section 4.1, the authors claim that their approach consists of five core components, but \"reference\" is not explained immediately afterwards. In subsequent text, \"reference\" appears to be the conditional input; however, it is unclear if this should be considered a separate component of the method.\n- Equation 3 is presented as an improvement over Equation 1, but the rationale behind this modification is not clearly explained.\n- Although the authors claim their method requires only a few lines of code to implement, the paper does not provide any concrete code examples, despite the inclusion of pseudocode in Appendix B. Intuitively, it seems unlikely that the proposed approach can be implemented by modifying just a few lines of code.\n- The paper lacks any analysis or comparison of computational efficiency."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AfijXAGs8x", "forum": "u1ChGXQEWa", "replyto": "u1ChGXQEWa", "signatures": ["ICLR.cc/2026/Conference/Submission2798/Reviewer_QLbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2798/Reviewer_QLbv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483644368, "cdate": 1761483644368, "tmdate": 1762916382378, "mdate": 1762916382378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response Summary and Paper Revision"}, "comment": {"value": "Dear Reviewers,\n\nWe sincerely thank you for their valuable feedback, which has significantly improved the quality and rigor of our paper. Based on your suggestions, we have revised and updated the paper, highlighting changes and additions in red within the revised PDF. Below, we summarize the key revisions made:\n\n1. Clarify Research Motivation\nBased on the comments from Reviewer ezgE, we have revised the Introduction section to clarify the research motivation and core focus of this paper. Our motivation can be summarized as follow.\nMany applications require strict consistency between generated results and reference conditions. Inspired by control theory—where closed-loop systems use continuous feedback to drive the stable convergence of outputs to target values—this paper aims to formulate controllable image generation as a control problem. It achieves this by constructing a closed-loop system to force the features of generated images to converge toward conditional inputs, thereby enhancing reference consistency.\n2. Supplement the Preliminary section and clarify the Method section\nFollowing the suggestions from Reviewers ezgE, 3epi, and QLbv, we have added an introduction to the basic concept of closed-loop control in the Preliminary section, refined the description in the Method section regarding how closed-loop control integrates with the generative model, and further explained the rationale for revising Equation 1 to Equation 3.\n3. Addition of SOTA comparison\nFollowing the suggestions from Reviewers XvRU and 3epi, we have added a comparison with PuLID. Quantitative evaluation results are included in Table 1, and qualitative comparisons of mainstream methods (Figure A-6), which cover PuLID, have been added in the Appendix.\n4. Addition of computational efficiency analysis\nFollowing the suggestions from Reviewers ezgE and QLbv, we have added an analysis of computational efficiency in Section 5.5. Compared with the baseline methods, the peak memory usage and CPU utilization do not increase after adding closed-loop optimization. After loading the model and data, the total inference time is approximately the single inference time multiplied by the number of iterations.\n5. Addition of coefficient stability analysis\nFollowing the suggestion from Reviewer XvRU, we have added a coefficient stability analysis in Section C of the Appendix. The results indicate that the three coefficients, $K_p$, $K_i$, and $K_d$, are effective within a wide range.\n6. Addition of non-human results\nFollowing the suggestion from Reviewer XvRU, we have conducted tests on non-human samples in the deep controlled generation task, with the visualization results presented in Figure A-5 of the Appendix.\n7. Supplementary materials\nIn identity-preserving portrait generation, due to the limitations of the paper’s presentation format, some optimizations are reflected in facial details, making qualitative differences less prominent in intuitive visual perception. We provide images before and after optimization in the supplementary materials; switching between these images in an image viewer allows for a more intuitive perception of the changes. Additionally, to address Reviewer QLbv’s concern that our method can be implemented with only a few lines of code, we have included the source code before and after modifications in the supplementary materials.\n\nOnce again, we deeply appreciate the time and effort you have dedicated to improving this paper. Your insights have made the paper stronger and more comprehensive. \n\nBest wishes,\nTeam of 2798"}}, "id": "s2eD8qQGVs", "forum": "u1ChGXQEWa", "replyto": "u1ChGXQEWa", "signatures": ["ICLR.cc/2026/Conference/Submission2798/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2798/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2798/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763665657548, "cdate": 1763665657548, "tmdate": 1763666388176, "mdate": 1763666388176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an interesting algorithm based on the idea of closed loop control for reference-consistent image generation. The proposedl algorithm is well motivated. It contains five components: reference, encoder, controller, controlled plant, and sensor. Based on the proposed algorithm, it obtains reasonable results on three different applications like ID-preserving portrait generation, Pose-controlled generation, and Depth-controlled generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "*  Using the idea of PID control algorithm for image generation is novel. The closed loop system can well improve the generation consistency. \n\n* The proposed algorithm can be applied to three different tasks with reasonable experimental results like ID-preserving portrait generation, Pose-controlled generation, and Depth-controlled generation.\n\n* The reproduce of the paper should be easy as the paper provide sufficient implementation details in the paper."}, "weaknesses": {"value": "* The experiments should involve more state-of-the-art algorithms for comparison. For example,  in the ID-preserving portrait generation experiments, it should include the comparisons with [R1] and [R2]. \n\n[R1] PuLID: Pure and Lightning ID Customization via Contrastive Alignment\nhttps://arxiv.org/pdf/2404.16022\n\n[R2] InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity\nhttps://arxiv.org/pdf/2503.16418\n\n* For the ID similarity evaluation, why not use the metric based on face recognition feature like arcface, which may be a more robust metric for ID preserving evaluation. The current definition of \"facial similarity\" seems to be a bit weird. \n\n* In the PID control algorithm, how to ensure the math equations corresponding the target value? For example, how to validate the value of e_k corresponding to the physical value of the error of the model outuput against the reference?\n\n* As the algorithm involves multiple rounds of computation, the comparison with the baseline may not be fair. \n\n* One minor suggestion: there should provide a reference for the PID when dicussing it in Section 3."}, "questions": {"value": "Please mainly address the questions in the weakness section. More specifically, I would have more concerns on the limited experimental evaluations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3FjcaPN1SU", "forum": "u1ChGXQEWa", "replyto": "u1ChGXQEWa", "signatures": ["ICLR.cc/2026/Conference/Submission2798/Reviewer_3epi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2798/Reviewer_3epi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746768036, "cdate": 1761746768036, "tmdate": 1762916382142, "mdate": 1762916382142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CL-GEN, a reference-guided image generation framework based on inference-time optimization. The core contribution of this work is modeling reference consistent image generation by drawing an analogy between control systems and generative processes. Based on a PID-like optimization target, CL-GEN can achieve image generation results more faithful to the reference image. Experimental results on several datasets and various tasks demonstrate the effectiveness of CL-GEN."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Although the review criteria call for comments on originality, quality, clarity, and significance, I see substantive strengths only in originality; therefore, this section focuses solely on that aspect. To the best of my knowledge, framing reference-consistent image generation through the lens of control theory is novel and could motivate new inference-time optimization algorithms that offer improved output quality and finer controllability; it may also inspire researchers in related fields."}, "weaknesses": {"value": "1. The motivation is unclear. At the beginning of the Introduction section, the authors review the task of image generation and ID-preserving image generation, and then point out the central issue of existing studies: failing to guarantee reference consistency, as well as lacking theoretical foundations. Then, the authors immediately introduce the analogy between control systems and generative processes. How does it relate to the core issue just identified? What motivates you to propose such modeling to solve the problem pointed out beforehand?\n\n2. How does the proposed PID-like optimization framework work with the diffusion-based generation model? The tutorial in the preliminary section is insufficient, and the introduction in the method is also unclear.\n\n3. From my perspective, the experimental results fail to demonstrate the universal advantage of CL-GEN. Based on the qualitative results shown in Figure 2, 3, and 5, honestly, I can hardly identify the advantage brought by the proposed method. This is also reflected by the quantitative values shown in these figures. \n\n4. From Table 1, it can be clearly seen that incorporating CL-GEN can only bring subtle improvement (1e-2 ~1e-3), and sometimes even worse results. How do you explain it?\n\n5. Since inference-time optimization will inevitably introduce extra computation cost, an in-depth analysis on the computational complexity is necessary."}, "questions": {"value": "From my perspective, there is much room for further improvement before this manuscript can reach the threshold of being accepted by ICLR. Please refer to the 'Weakness' section for potential further improvement directions, and I do not think my evaluation and rating of this study will further change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NFwFk5zYte", "forum": "u1ChGXQEWa", "replyto": "u1ChGXQEWa", "signatures": ["ICLR.cc/2026/Conference/Submission2798/Reviewer_ezgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2798/Reviewer_ezgE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830554465, "cdate": 1761830554465, "tmdate": 1762916381520, "mdate": 1762916381520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel control-theoretic perspective on controllable image generation by formulating it as a closed-loop feedback system. Through a P(ose) I(d) D(epth)-based iterative optimization during inference, CL-GEN improves reference consistency without retraining. The framework is simple, generalizable, and empirically effective across ID, pose, and depth control tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n- First attempt to apply closed-loop control (PID feedback) to image generation at inference time\n- Integrates control theory with diffusion-based generative modeling, offering a new theoretical lens\n- No need to additional training"}, "weaknesses": {"value": "Weaknesses:\n- No analysis of stability, or control gain (K_p, K_i, K_d) sensitivity is provided\n- Insufficient performance comparison with SOTA methods qualitatively and quantitatively\n- No computational cost analysis"}, "questions": {"value": "- Why didn't you compare it with various SOTA methods?\n- In Table 1, I think that there are no significant differences across them except for facial similarity. But, the area occupied by the face in the image is not that large.\n- Are there any results from generating other objects (not human) or landscapes? Will it still work like ControlNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gjSjZ1Rrtz", "forum": "u1ChGXQEWa", "replyto": "u1ChGXQEWa", "signatures": ["ICLR.cc/2026/Conference/Submission2798/Reviewer_XvRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2798/Reviewer_XvRU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950266867, "cdate": 1761950266867, "tmdate": 1762916381091, "mdate": 1762916381091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}