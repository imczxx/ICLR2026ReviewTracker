{"id": "6YMFsGFabM", "number": 2506, "cdate": 1757127925421, "mdate": 1759898144232, "content": {"title": "Agentic Confidence Calibration", "abstract": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of \\emph{Agentic Confidence Calibration} and propose \\textbf{Holistic Trajectory Calibration (\\htcnospace)}, a novel diagnostic framework that extracts rich process-level features—ranging from macro dynamics to micro stability—across an agent’s entire trajectory. Powered by a simple, interpretable model, \\htc consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, \\htc delivers three essential advances: it provides \\emph{interpretability} by revealing the signals behind failure, enables \\emph{transferability} by applying across domains without retraining, and achieves \\emph{generalization} through a \\emph{General Agent Calibrator} (\\gacnospace) that {achieves the best calibration (lowest ECE)} on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration—laying the foundation for diagnosing and enhancing the reliability of AI agents.", "tldr": "", "keywords": ["Uncertainty Estimation", "Confidence Calibration", "AI Agent"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5db895fbb3e117591dc38ff4eccc938bc6a107fe.pdf", "supplementary_material": "/attachment/0bd8e97fb4546f8cc58329ea2a3c1ea4a6acec08.zip"}, "replies": [{"content": {"summary": {"value": "The authors formalize the problem of Agent Confidence Calibration, which requires coming up with confidence estimates for agents' actions based on their entire trajectory (rather than just their final output or verbalized confidence).\n\nThe paper proposes \"Holistic Trajectory Calibration\", a broad method for improving calibration in AI agents. The authors develop 48 features and frame the problem in a supervised learning setting. They show that their method outperforms existing methods (such as verbalized confidence, last-step confidence, average confidence across steps).\n\nAgent reliability is an important problem. Calibration is, further, and important sub-component of reliability. The paper makes a solid contribution to the problem formulation. I have some questions and concerns about the clarity of the paper when it comes to their solution (HTC), but I hope they can be addressed during rebuttals."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Experimental setup is rigorous, comparisons are conducted on a wide variety of benchmarks, and the baselines mentioned in the paper are well thought out.\n- Problem formulation is sound. The paper adequately motivates the setup, and justifies the problem's importance by tying it to real-world challenges faced during the deployment of AI agents.\n- Results show promise as a general-purpose method for estimating and implementing calibration."}, "weaknesses": {"value": "- The writing could be improved. For example, The structure of the results section is a bit all of the place: it includes the experimental results, then a discussion of the results, and then goes back to the results, making it really confusing for the reader.\n- There are many areas where concepts are introduced but never re-used again. For example, I was confused about where the learning-based baselines are compared to HTC? I couldn't see these results either in the main paper or the appendix (I double-checked by searching for the \"LSTM\" keyword and it doesn't appear in any figure or table)\n- The choice of features is not well motivated. Did the authors consider any features other than the 48 they mention using eventually? If yes, are there ablations for those results as well?\n- If not, it seems like there is scope for adding even more features to keep increasing the performance of the model by continuing to add more features/we're not yet at the point of diminishing returns? \n- Similarly, could the authors justify why the set of features they considered is comprehensive? I think there could be entire categories of features that are currently being ignored in the formulation and HTC setup.\n- What is the architecture of the GAC? Is it basically just HTC trained on data pooled from all of the benchmarks? If yes, why coin a separate term for it?"}, "questions": {"value": "I would be happy to increase my score if the authors answer the following questions/update the paper in response to these:\n\n1. Could you clarify where learning-based baselines are reported, and what the results are? Why aren't they compared in Table 1?\n2. Could you improve the structure of the paper, for example, by disentangling the results and the discussion of those results in Section 3?\n3. Could you clarify how you selected the 48 features, whether you think they are comprehensive, and if not, what other categories of features future work should look into?\n4. The authors disclose the use of LLMs for polishing the writing. However, there are many areas where this goes a touch too far. For example, coining the term \"cognitive mismatch\" for distribution shifts seems like a step too far. (I like this guide for editing AI text: https://www.sh-reya.com/blog/ai-writing/)\n5. Could you clarify the architecture of the GAC, whether it is the same as HTC, and if yes, why coin a new term for it? (This might also be a result of overreliance on LLMs)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sZpvKCVyd4", "forum": "6YMFsGFabM", "replyto": "6YMFsGFabM", "signatures": ["ICLR.cc/2026/Conference/Submission2506/Reviewer_gUNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2506/Reviewer_gUNF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921031846, "cdate": 1761921031846, "tmdate": 1762916259567, "mdate": 1762916259567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Developed a new calibration method for agentic systems, focusing on their processes"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The problem formulation of trajectory-level cabliration (ie. HTC) and design principles are well defined and executed."}, "weaknesses": {"value": "A very limited exploration of other possible agentic framework besides the smolagents and CodeAct.\nUnfortunately, the experiment sections seem a bit laundry list of what this framework is capable of, but not provide clear strengths and validation of the framework as calibration methods with respect to non-process based methods."}, "questions": {"value": "I don't get the point of comparing relative performance of ECE/BS/AUROC scores among different calibration methods. Having HTC with lower ECE doesn't mean that the proposed calibration measure is better aligned with true calibration effect. I thought HTC was developed to provide more accurate, fine-grained calibration measurement, but results only support which one is better. Please give me a bit more context of goals of your experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nkg3YKQ7fX", "forum": "6YMFsGFabM", "replyto": "6YMFsGFabM", "signatures": ["ICLR.cc/2026/Conference/Submission2506/Reviewer_FFZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2506/Reviewer_FFZT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940281928, "cdate": 1761940281928, "tmdate": 1762916259124, "mdate": 1762916259124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces and formalizes a new problem: confidence calibration for agentic systems, which aims to produce a calibrated estimate of the success likelihood of an agent’s trajectory by diagnosing its entire execution process rather than only the final output. The authors therefore proposes Holistic Trajectory Calibration, a feature-based framework that turns token log-prob traces across steps into a compact feature vector of trajectory-level diagnostics (e.g. early-step entropy, confidence gradients, stability dynamics), which are mapped to calibrated confidence estimates. Tested on eight benchmarks across factual, domain-specific QA and agentic planning and tool-use tasks and evaluated on a diverse set of open- and closed source models, the framework shows gains in both calibration and discrimination over multiple inference and training based baselines. Based on insights that uncertainty signals share underlying patterns, the authors also propose a pretrained calibrator that shows potential on  challenging out-of-domain tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Calibration for agentic systems with multi-step, interactive trajectories is a timely and important topic, and this paper presents a great initial effort to address the gap.\n* The proposed framework, which decomposes different uncertainty signals in the agent trajectories and learns a calibration function to map features to a calibrated confidence score, is simple, lightweight, interpretable, and novel. The pretrained, general-purpose agent calibrator achieves good generalization on challenging, unseen tasks. \n* Evaluations are quite comprehensive in terms of dataset and model selection. It covers both traditional knowledge-intensive QA tasks for calibration, and more recent and challenging tasks requiring complex reasoning, planning, and tool-use. Multiple models of varying sizes and families, as well as generalization on different agent frameworks, are evaluated.\n* The paper also presents comprehensive ablation studies, such as feature categories and cross-domain transfer. \n* The detailed analyses present some interesting insights for agent tasks (e.g. positional features are the strongest signals of failure; different uncertainty patterns and cognitive mismatch impact cross-domain transfer). \n* I find the paper well-presented, with good visualization and a detailed appendix. The motivation for why we need a unique framework for agent calibration instead of applying traditional methods is well established."}, "weaknesses": {"value": "* While the task and model selection is quite comprehensive for the evaluation, baselines are largely simple verbalized or token-logprob-based methods. A few related methods (that are cited but not compared) could be included as potentially stronger baselines [1,2].\n* The method requires log-prob access, which is not available for black-box models.\n\n---\n\n[1]. UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making\n\n[2]. SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"}, "questions": {"value": "* Why use a 48-dimensional trajectory feature space? Is there a motivation / rationale for choosing this number of dimensions?\n* Did you verify the reliability (e.g. human validation) of the LLM judge (Gemini-2.5-Pro) given the long, noisy, and complex context of agentic tasks?\n* Continuing the discussion for future aspects - regarding self-evolving agents, what benefits and risks do you foresee by having full control of such a calibrator for self-evolving? And regarding RL, how would you turn the HTC-calibrated success probability into a learning signal for agentic RL without opening it up to reward hacking (e.g. gamed by consistently low but calibrated confidence with degrading task accuracy)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Spz8DlTlLY", "forum": "6YMFsGFabM", "replyto": "6YMFsGFabM", "signatures": ["ICLR.cc/2026/Conference/Submission2506/Reviewer_CvNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2506/Reviewer_CvNP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990906413, "cdate": 1761990906413, "tmdate": 1762916258947, "mdate": 1762916258947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}