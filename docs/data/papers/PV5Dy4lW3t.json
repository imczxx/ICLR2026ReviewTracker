{"id": "PV5Dy4lW3t", "number": 21370, "cdate": 1758316799562, "mdate": 1759896925644, "content": {"title": "TSLM: Tree-Structured Language Modeling for Divergent Thinking", "abstract": "Current language models generate solutions through sequential reasoning, limiting their ability to systematically explore multiple solution paths. We introduce Tree-Structured Language Modeling (TSLM), which teaches language models to generate complete search trees within a single generation process using special tokens to encode branching structure. TSLM serializes tree exploration into linear sequences, enabling standard transformer training on tree-structured reasoning traces that capture both successful and failed solution attempts. Across structured planning (Game of 24, Gridworld) and open-ended reasoning tasks (ProntoQA, GSM8K), TSLM achieves superior performance: 100\\% accuracy on Game of 24 vs. 17\\% for sequential baselines, and robust extrapolation to 20×20 grids (76.5\\%) compared to Tree-of-Thought's collapse (26\\%). Remarkably, TSLM demonstrates 14× parameter efficiency, with a 0.5B model (68\\% scaling performance) outperforming 7B sequential baselines (19-26\\%). TSLM also exhibits emergent capabilities including unsolvable problem detection and rapid adaptation with minimal training data. These results challenge the assumption that reinforcement learning is necessary for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.", "tldr": "", "keywords": ["language models", "reasoning", "planning", "Supervised Learning", "Inference-time scaling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07154ecb4226dddfd23a0cd96a9937c924795844.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Tree-Structured Language Modeling (TSLM), a simple but distinctive way to let a standard transformer natively generate and traverse a search tree within a single sequence. The core idea is to serialize a tree into a linear token stream using special markers, so that teacher traces include both successful and failed branches. Training is standard LM loss on these serialized traces; inference reconstructs the tree and explores it until a solution or exhaustion. The paper reports results across Game of 24, Textualized Gridworld, ProntoQA, and GSM8K. On structured tasks, TSLM is very strong, shows rapid adaptation with few samples on ProntoQA, and claims parameter efficiency. For open-ended math (GSM8K), TSLM is competitive. Overall, TSLM is positioned as a purely supervised alternative to RL-style “reasoning” methods that require external search or test-time orchestration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The token-based tree serialization for end-to-end supervised training of internal exploration is a neat, low-friction idea. It avoids extra modules (verifiers or value functions) and RL, and while LM-based search is well-studied, representing divergent branches natively within a single generation is an original framing with distinct trade-offs.\n\nS2: Strong performance on Game of 24 and Gridworld demonstrates the method’s core promise. The BFS vs. DFS analysis in Appendix G provides insight into how traversal strategies interact with learned preferences.\n\nS3. No architectural changes are required; the approach works with standard SFT and any base LM (e.g., Llama-3, Qwen). Training and inference procedures are clear, with many worked examples provided, supporting reproducibility."}, "weaknesses": {"value": "W1: ToT is configured minimally, and comparisons omit Self-Consistency and verifier-based selection, which are strong baselines on GSM8K, as well as natural tree-search comparators like TS-LLM, LATS, and RAP. Without these, it is difficult to judge when TSLM’s internalized tree search provides meaningful benefits.\n\n\nW2: TSLM trains over all tree nodes O(N⋅L)), and inference produces long serialized trees. Claims of efficiency are not quantitatively compared against ToT’s extra model calls or SC/self-consistency multi-sample decoding.\n\nW3: The tasks are well-suited to structured search-tree reasoning but do not represent open-ended domains such as large-scale software engineering, real-world system design, multi-agent planning, or long-horizon code generation. Claims that strong performance on structured tasks will generalize to these domains remain untested and speculative."}, "questions": {"value": "Q1. What were the exact ToT settings (beam width, evaluation heuristic, depth/budget) per task? Did you evaluate Self-Consistency (sample-and-vote) or GSM8K verifier selection? If so, how do they compare under equal token budgets?\n\nQ2: The reported scaling performance (“68% for 0.5B vs 19–26% for 7B baselines”) requires clarification: What is the exact metric and task (Gridworld 20×20 only?), and are these single-seed results? Please provide confidence intervals.\n\nQ3: The paper provides a qualitative example. Across a large set, how often does TSLM correctly abstain on unsolvable Game of 24 instances?\n\nQ4: How well does TSLM handle branches in open-ended problem domains, where the branching structure is unclear or multiple valid solution paths exist without a single “correct” branch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HJYEfiahyA", "forum": "PV5Dy4lW3t", "replyto": "PV5Dy4lW3t", "signatures": ["ICLR.cc/2026/Conference/Submission21370/Reviewer_Aewk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21370/Reviewer_Aewk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631479986, "cdate": 1761631479986, "tmdate": 1762941727887, "mdate": 1762941727887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TSLM, a training approach that teaches an LLM to internally construct and traverse a tree of reasoning within a single generation, rather than relying on external multi-pass search at inference time. The authors position TSLM as more efficient and easier to train and deploy than external search pipelines, and as a simpler alternative to RL approaches such as GRPO for improving multi-step reasoning and planning. Empirically the paper claims that TSLM improves extrapolation on structured reasoning tasks and helps smaller models approach the performance of larger ones, while also benefiting large models.\n\nThe paper situates itself against external search methods, and also cites multi-path generation like self-consistency and recent long-reasoning models. The narrative is consistent with the goal of moving reasoning structure into the model rather than the inference loop. However, there is a relevant gap regarding diversity-driven reasoning frameworks that target breadth before convergence and explicitly reduce redundant reflections with task-agnostic memory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "TSLM aims at deployment simplicity. Eliminating external orchestration and multiple model calls is attractive for production where latency and engineering complexity matter. The authors take care to compare conceptually with ToT and with RL, explaining why a supervised approach can be more stable and economical. The stated effect that TSLM helps smaller models close the gap with larger ones is promising for cost-sensitive settings. \n\nThe conceptual boundary is easy to communicate. External search explores a tree outside the model, TSLM learns to write the tree inside the generation. That is a clean story, and it could generalize to code or games where branching structure is natural."}, "weaknesses": {"value": "Related work coverage is incomplete on diversity-first inference and redundancy reduction with memory, which is a very proximate thread to the authors’ efficiency and breadth claims. The current submission neither cites nor contrasts with these works (e.g., Lingam et al., ICLR 2025), which could mislead readers about the frontier on exploration efficiency. This needs correction. \n\nEvidence granularity is thin in the visible draft. The claims about improved extrapolation, small-to-large bridging, and efficiency would be more convincing with matched-budget comparisons against ToT, self-consistency, and diversity-driven methods, for example controlling node expansions or total tokens, plus wall-clock latency. The draft does not show whether TSLM outperforms a well tuned inference-time breadth strategy at the same compute."}, "questions": {"value": "(1) Under a fixed budget of model calls and total tokens, how does TSLM compare to ToT or self-consistency, and to a diversity-driven method with a task-agnostic memory bank such as DoT? \n\n(2) What happens if you linearize the tree to a CoT trace with the same token budget, or if you randomize branching order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The work is a modeling and training method, uses standard benchmarks, and does not involve sensitive data or human subjects."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o6TFf2fZEo", "forum": "PV5Dy4lW3t", "replyto": "PV5Dy4lW3t", "signatures": ["ICLR.cc/2026/Conference/Submission21370/Reviewer_BGCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21370/Reviewer_BGCK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761681757825, "cdate": 1761681757825, "tmdate": 1762941727571, "mdate": 1762941727571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs can generate sequential solutions but are unable to systematically explore multiple possibilities. The paper introduce a framework to serialize tree exploration into linear sequences for training to enable tree-structured reasoning. The method achieves better performance than baselines on most tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally clearly written.\n\nThe proposed tree-structured language modeling paradigm is interesting."}, "weaknesses": {"value": "The method is helpful on tasks where systematic exploration is needed, but I think more open-ended and real-world tasks are more important. As indicated in Table 1, the proposed method has no benefit in these scenarios. The bolded number in the GSM8k row should be the ToT one, by the way.\n\nThe experiment detailed configuration for the ToT baseline is not mentioned. Since the proposed method requires much higher inference-time compute, it is unclear how much the advantage will diminish\tif you let all methods use the same budget (say, the total tokens)."}, "questions": {"value": "Does the training data for the TSLM method include the 5 tasks that are also used for evaluation?\n\nCan you also try letting baseline methods train on the same training set, by extracting correct path from the tree. Basically, TSLM receives additional training signal and eliminating that factor will make the experiment results more solid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vCnEvjOm14", "forum": "PV5Dy4lW3t", "replyto": "PV5Dy4lW3t", "signatures": ["ICLR.cc/2026/Conference/Submission21370/Reviewer_fgNv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21370/Reviewer_fgNv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984063101, "cdate": 1761984063101, "tmdate": 1762941727118, "mdate": 1762941727118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Tree-Structured Language Modeling (TSLM), enabling language models to perform tree-structured reasoning natively within a single forward generation process.\nThe thought branching structures are shown into the token sequence through special markers.\nSerialized tree traces that capture both successful and failed reasoning paths are used to train standard transformers.\nExperiments are conducted on structured planning tasks and open-ended reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Tree structured reasoning is reflected into token sequences and only one model call is required. This is different from multiple independent model calls used by Tree-of-Thought.\n2. The token-based serialization approach is practical for achieving tree-structured generation using LLMs.\n3. The experiments span both structured and open-ended reasoning tasks."}, "weaknesses": {"value": "1. The time cost comparison with methods such as Tree-of-Thought is not presented.\n2. As shown in Table 1, the performance of TSLM is not as good as ToT in GSM8K.\n3. The token-based tree serialization relies on search trees gotten from other reasoning methods. As such, the performance is also limited by the other reasoning methods.\n4. There are some more advanced reasoning methods such as Graph-of-Thought and Everything of Thoughts."}, "questions": {"value": "What is the time cost of the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b0AlnmyloG", "forum": "PV5Dy4lW3t", "replyto": "PV5Dy4lW3t", "signatures": ["ICLR.cc/2026/Conference/Submission21370/Reviewer_cbMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21370/Reviewer_cbMw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006091374, "cdate": 1762006091374, "tmdate": 1762941726618, "mdate": 1762941726618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}