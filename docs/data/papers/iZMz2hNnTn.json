{"id": "iZMz2hNnTn", "number": 14352, "cdate": 1758233391664, "mdate": 1759897375589, "content": {"title": "Unveiling the Hidden Structure: Tight Bounds for Matrix Multiplication Approximation via Convex Optimization", "abstract": "Matrix multiplication lies at the heart of machine learning, yet standard approaches to approximate the multiplication often ignore the interactions that truly governs error. In this work, we introduce a structure-aware upper bound on the optimal achievable approximation using only linear combination of $k$ column-multiplication of the matrices. Our bounds, formulated via convex optimization over an interaction matrix, reveal the hidden challenges and opportunities in matrix multiplication. Through comprehensive numerical experiments, we demonstrate that our bounds not only outperform existing alternatives but also shed new light on the inherent complexity of structured matrix products. This framework paves the way for the development of structure-exploiting algorithms and principled performance guarantees in large-scale machine learning.", "tldr": "This paper introduces a novel, computable, structure-aware upper bound that more accurately estimates the true optimal k-term matrix approximation error, especially when complex data interactions make standard bounds uninformative.", "keywords": ["Matrix Multiplication Approximation", "Sparse Approximation", "Randomized Algorithms", "Error Bounds", "Upper Bounds", "Sparsity Constrained Quadratic Programming", "Gram Matrix Methods", "Convex Optimization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc0f8f7355261d7d8b34860b9b47edecc8b1de66.pdf", "supplementary_material": "/attachment/3b64244bd1f6ffa7b896db49f5730e95e81b2327.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new complexity measure for approximate matrix multiplication in the Frobenius norm. Let $G=(A^\\top A)\\circ (B^\\top B)$, the measure $\\rho_G=\\frac{{\\rm Tr}(G)}{{\\bf 1}^T G {\\bf 1}}$. Moreover, it studies the algorithmic question of finding an optimal weight vector $x\\in K$ with sparsity at most $k$ such that $\\\\|AB^\\top-AXB^\\top \\\\|_F$ is minimized, where $X$ is the diagonal matrix of vector $x$. Authors formulate it as a quadratic program with $\\ell_0$ constraint, which is NP-hard to solve. To overcome it, they propose to solve a convex quadratic program whose optimal value gives an upper bound on the optimal value of the sparse quadratic program. Several other simpler analytical upper bounds for particular convex sets $K$ are also studied. Finally, they empirically verify the gap between several popular AMM algorithms, including orthogonal matching pursuit, optimal sampling, and several oblivious subspace embedding algorithms and the optimal bound, upper bound of it, and several proxies, showing that OMP is much closer to the optimal than other approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed complexity measure is a more robust and unified estimate of the product $AB^\\top$, as it goes beyond estimate that only depends on norms, e.g., $\\\\|A\\\\|_F \\\\|B\\\\|_F$, and it captures how outer products $a_ib_i^\\top$ interact with each other. \n\n* The corresponding algorithmic problem, i.e., finding an optimal $k$-sparse weight vector also fits quite naturally, as various sampling methods have been studied before with various guarantees. This offers new perspective on studying traditional sampling algorithms such as leverage score sampling and row norm based sampling.\n\n* The study of the sparse convex program and its relaxations are quite thorough, a tractable convex quadratic relaxation is proposed and analyzed, and when the constraint sets are structured, better and simpler analytical bounds are studied. These bounds have the advantage that one can very quickly estimate them without solving the convex quadratic relaxation.\n\n* Experiments performed, which in my opinion, are quite insightful, as they show the fundamental limits of sketching and sampling based methods when $\\rho_G$ is large. In addition, they show that the most input-dependent algorithm OMP unsurprisingly gives the best error, albeit it is the slowest algorithm."}, "weaknesses": {"value": "* The theoretical analysis in this paper merely provides an upper bound on the optimal **value** of the sparse convex quadratic program, a much more interesting and practical question is how to efficiently compute an approximately optimal **solution**. For example, is there a rounding scheme for the convex quadratic program that serves as an upper bound on the value, that could also lead to a $k$-sparse approximate weighting? While the analysis on the optimal value is quite insightful in benchmarking different AMM algorithms, it falls short on converting the framework into a useful algorithm.\n\n* The convex program studied in this paper has a significant gap to oblivious sketching based algorithms, as these algorithms require possibly to linear combine all the columns, meaning that one cannot simply write it as $\\sum_i x_i a_ib_i^\\top$, I believe the formulation studied in this paper applies only to sampling algorithms. While one could argue that the performance of these sketching algorithms could still be compared, this should be clarified.\n\n* The implication and applicability of the optimal AMM should be further elaborated. In many modern day ML applications, in particular deep learning settings, matrices $A$ and $B$ are changing constantly, prior AMM tools such as sampling and sketching are much faster to compute than OMP, thus even though the error guarantees are weaker, they are at least tractable and can be integrated into modern DL pipelines, much less can be said for OMP. Authors should try to find some ML settings that computing an optimal sparse weight for AMM is particularly useful, and one could justify the expensive overhead to do so."}, "questions": {"value": "Typos:\n\n* Line 38, Frobenius nomrs -> Frobenius norms.\n\n* Line 466, what is $\\odot$? Is this the Hadamard product? If so, the notation established before should be $\\circ$.\n\nQuestions:\n\n* Is it possible to convert the solution of the convex quadratic program into a feasible solution to the original problem via a rounding scheme?\n\n* What are some ML applications that one could justify to compute an optimal or nearly-optimal sparse AMM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xl37m8VsKh", "forum": "iZMz2hNnTn", "replyto": "iZMz2hNnTn", "signatures": ["ICLR.cc/2026/Conference/Submission14352/Reviewer_hsCA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14352/Reviewer_hsCA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760577873312, "cdate": 1760577873312, "tmdate": 1762924776698, "mdate": 1762924776698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a new approach for approximations to matrix multiplication using convex optimization over an interaction matrix. The approach informally works by relaxing an NP-hard sparse optimization problem to a set of convex quadratic programs parameterized by interaction matrices that interpolates between the full interaction matrix and a diagonal matrix. The authors complement their approach by providing extensive experimental validation of their theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Although the paper is dense, the paper is structured well and generally well-written. The paper is also well contextualized with a discussion on related work. \n- The theoretical results seem to to be well justified, although I did not check all the proofs in detail.\n- The authors additionally provide specific case studies, including those that are do not fit the assumptions of the theory, e.g. binary matrices."}, "weaknesses": {"value": "- It seems like the greedy OMP strategy always outperforms the proposed method.\n- The runtime scaling seems potentially high, as we need to run k instances of quadratic programming to compute the solution. Can the authors provide runtime analysis of the method (big O)?\n- The guarantees are for just Frobenius norm, which is mentioned as a limitation. It would be nice to see whether this method can also give guarantees for spectral norm (either empirically), or rule this out.\n- Data is run on synthetic classes of random matrices. A real-world dataset would be useful."}, "questions": {"value": "- Please give context for Table 1 in the main text, along with additional discussion.\n- Can you use a different script for matrices and sets, e.g. sets are script instead of capital like matrices? Currently, the notation is a little confusing.\n- I think it would be more helpful to replace the last column in Table 2 with the runtime of each method. \n\nTypos:\n- line 38: nomr -> norm\n- line 752: equation repeated twice"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vbB9CkLicK", "forum": "iZMz2hNnTn", "replyto": "iZMz2hNnTn", "signatures": ["ICLR.cc/2026/Conference/Submission14352/Reviewer_JgEH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14352/Reviewer_JgEH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013519499, "cdate": 1761013519499, "tmdate": 1762924775961, "mdate": 1762924775961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies algorithms for approximating matrix products. The main result is Theorem 4.1 which gives a $O(n^3)$ time algorithm to select at most a size $k$ subset of the outer products of $M = AB^T$ which approximates $M$ up to additive error $\\|M\\|_F^2$ plus a mysterious term that is not so well defined."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper draws a nice analogy between relating $AB^T$ to selecting how to weight the outer products in the sum $AB^T = \\sum_i a_i b_i^T$, leading to a very intuitive algorithm design."}, "weaknesses": {"value": "Unfortunately I don't see a compelling use case of their particular sampling scheme (but note approximating a matrix product is indeed a fundamental problem).\n\nThe reason is that there really are no advantages of this method when we look at the axis of say error guarantees, speed, easiness to compute, and further downstream applications. Let me break this down:\n\n- Error guarantees: The error guarantee of their main theorem already has the $\\|M\\|_F^2$ term (plus an additional term), where $M$ is the matrix product $AB^T$ we wish to approximate. Note that approximating $AB^T$ by the all zeros matrix would already give this guarantee so this error term is not very meaningful theoretically. The nice thing about existing sampling/sketching based approximate matrix multiplication guarantees is that they *decrease* in error as one samples more rows/columns. This is not true here since there is already a huge $\\|M\\|_F^2$ term.\n\n- Speed: Again, traditional sampling approaches, eg sampling by row norms, can be instantiated in $O(n^2)$ time for $n \\times n$ matrices (we just need to compute row norms!), making them linear time which is extremely desirable in practice. The approach of their main theorem, Theorem 4.1, relies on solving a very complex optimization problem which they say takes $O(n^3)$ time. In that case, one can simply just find the SVD (which is even faster and can be done in matrix multiplication time).\n\n- Easiness to compute: Again, the existing row sampling methods are trivial to implement whereas the proposed method requires solving a very complicated optimization problem.\n\n- Further downstream applications: The advantages of existing sampling/sketching methods is that they can easily be instantiated in other settings such as streaming, which is an advantage the proposed method does not have.\n\nGiven these considerations, I really do not see any advantage of the proposed method."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XIaMB55qlA", "forum": "iZMz2hNnTn", "replyto": "iZMz2hNnTn", "signatures": ["ICLR.cc/2026/Conference/Submission14352/Reviewer_ccZV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14352/Reviewer_ccZV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935131157, "cdate": 1761935131157, "tmdate": 1762924775539, "mdate": 1762924775539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a tight theoretical benchmark for the best achievable error in $k$-term Approximate Matrix Multiplication (AMM), shifting focus from worst-case analysis to instance-specific, structure-aware guarantees.\n\n1.  The approximation error is formalized by defining the Interaction Matrix ($G$). This matrix is used to recast the problem as a Sparsity Constrained Quadratic Program, where the error equation is:\n    ($||E||_{F}^{2}=(1-x)^{\\top}G(1-x)$)\n2.  Since the SCQP is NP-hard, the paper derives a tight, computable upper bound ($u_k^\\*$) on the optimal error ($v_k^\\*$). This is done by solving a series of tractable convex Quadratic Programs, which are relaxations of the hard SCQP.\n3.  The Structure Ratio ($\\rho_G$) is introduced to quantify problem difficulty (high $\\rho_G$ means more term cancellation). Experiments show that standard AMM bounds fail catastrophically when $\\rho_G$ is high, while the new bounds remain accurate.\n4.  The bounds are shown to be orders of magnitude tighter than existing theoretical alternatives and are demonstrated to be achievable by greedy algorithms like Orthogonal Matching Pursuit (OMP). A simple, analytical Scaled Identity Bound is also provided as a practical, tight proxy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Below are the key strengths of the paper.\n\n* (s1) The paper contributes by reframing the Approximate Matrix Multiplication error problem. The introduction of the Interaction Matrix ($G$) to explicitly model the cancellation and reinforcement structure within the product sum, and its link to a Sparsity Constrained Quadratic Program, is an innovation. \n* (s2) The central theoretical claim, deriving a tight, computable upper bound ($u_k^*$) by using tractable convex relaxations (Auxiliary QPs) of the NP-hard SCQP is logically sound. The work successfully establishes a verifiable theoretical ceiling for AMM performance.\n* (s3) The experiments show that prior bounds (from sampling/sketching) are often orders of magnitude looser than the actual achievable error, especially in \"hard\" instances defined by a high Structure Ratio ($\\rho_G$). \n* (s4) The inclusion of the Scaled Identity Bound (Proposition 4.3) is a strength. This analytical proxy is shown to maintain high tightness while being significantly faster to compute than the main QP bound."}, "weaknesses": {"value": "Main drawbacks concern computational complexity, limited practical utility, and an unresolved gap between theory and efficient algorithmic implementation.\n\n* (w1) The framework's core requirement is calculating the Interaction Matrix ($G$), which takes $O(n^2(m+p))$ time. This is already too slow for large matrices where AMM is crucial. Furthermore, the main theoretical result, the Auxiliary QP Bound, demands solving $k$ convex Quadratic Programs, adding substantial overhead. This high cost restricts the bounds to offline analysis only, precluding their use in real-time or large-scale optimization.\n\n* (w2) The paper successfully proves that a structure-aware algorithm (like Orthogonal Matching Pursuit, OMP) can nearly achieve the derived tight bounds. However, OMP is slow, requiring iterative least-squares solutions. The paper identifies the fundamental theoretical ceiling but fails to provide a fast, scalable algorithm capable of efficiently reaching this boundary, leaving a disconnect between the theory and practical acceleration.\n\n* (w3)  The bounds strictly apply only to approximations that are a linear combination of $k$ selected outer product terms. This framework does not directly cover or offer optimization guidance for many popular, fast sketching techniques (like Gaussian projection or CountSketch). While the bounds critique the performance of these methods, they do not constructively inform how to improve them within their own distinct mathematical frameworks.\n\n* (w4)  The tightness of the computationally efficient Scaled Identity Bound (Proposition 4.3), which is derived from a highly restrictive analytical constraint ($y=\\gamma 1$), undermines the practical necessity of the main theoretical result (Theorem 4.1), which requires solving $k$ complex QPs. This raises a question about whether the added computational expense of the main theorem is justified by the marginal increase in tightness over its simple proxy.\n\n* (w5) Although the small-scale experiments suggest the bounds are tight, the theoretical contribution would be strengthened by a rigorous proof or construction that demonstrates the existence of matrix pairs $(A, B)$ where no $k$-sparse approximation can perform better than the derived bound for a general $\\rho_G$. This missing element prevents the bounds from being certified as truly unimprovable."}, "questions": {"value": "Below are the key questions that need to be addressed:\n\n* (q1) Given that calculating the Interaction Matrix ($G$) is $O(n^2(m+p))$, which is often slower than the matrix multiplication being approximated, can the authors propose a fast, randomized approximation of the bound? Specifically, can a bound on $v_k^*$ be derived based on a sampled or projected version of $G$ (e.g., $G \\approx G_{sampled}$) that is computed in $O(n \\cdot poly(k))$ time? A positive result here would transform the benchmark from a purely theoretical tool into an actionable diagnostic tool.\n\n* (q2) Can the authors theoretically analyze or provide empirical evidence for the existence of a fast, scalable structure-aware algorithm (running in time significantly faster than OMP and exact multiplication) that provably achieves an error within a small constant factor of the derived $u_k^*$ bound? Identifying a path to an efficient, near-optimal algorithm is necessary to bridge the gap between the theoretical ceiling and practical application.\n\n* (q3) The paper provides both the complex Auxiliary QP Bound (Theorem 4.1) and the much simpler analytical Scaled Identity Bound (Proposition 4.3), which is shown to be nearly as tight. Can the authors demonstrate a specific class of matrices (or a particular range of the structure ratio $\\rho_G$) where the full minimisation over $s \\in \\{1, \\dots, k\\}$ in Theorem 4.1 yields a demonstrably better, non-trivial result than the simpler Scaled Identity Bound?\n\n* (q4) The current bounds apply strictly to linear combinations of $k$ columns selected from the original $n$ columns. How do the theoretical insights and the structure ratio $\\rho_G$ relate to or bound the error of popular, non-linear, randomised sketching algorithms (e.g., CountSketch or Subsampled Randomised Hadamard Transform, SRHT)?\n\n* (q5) The paper relies on numerical evidence (the $n=30$ exhaustive search) to claim the bounds are tight. To achieve stronger theoretical completeness, the authors should include a constructive proof that demonstrates the existence of a matrix pair $(A, B)$ for which the optimal error $v_k^\\*$ is equal to or arbitrarily close to the derived upper bound $u_k^*$. This adversarial construction is standard practice for proving the theoretical tightness of a new bound. Can a similar result be shown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QgRaf5udbU", "forum": "iZMz2hNnTn", "replyto": "iZMz2hNnTn", "signatures": ["ICLR.cc/2026/Conference/Submission14352/Reviewer_NJTi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14352/Reviewer_NJTi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971941584, "cdate": 1761971941584, "tmdate": 1762924774925, "mdate": 1762924774925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the fundamental limit of approximating a matrix product $A B^{\\top}=\\sum_{j=1}^n a_j b_j^{\\top}$ using only $k$ of its rank- 1 terms. It shows that the optimal $k$-term approximation error can be formulated as a sparsity-constrained quadratic program in a weight vector $x$, where the error equals $(1-x)^{\\top} G(1-x)$ and $G= \\left(A^{\\top} A\\right) \\circ\\left(B^{\\top} B\\right)$ ($\\circ$ is the Hadamard product) captures interactions and cancellations among terms. Since this SCQP is NP-hard, the authors develop a computable, instance-specific upper bound by introducing a family of auxiliary convex quadratic programs based on an interpolated matrix $G(s)$. This yields a structure-aware upper bound $u_k^{{ }^*}(K, G)$ that closely tracks the true optimum and is significantly tighter than classical AMM bounds. They further derive closed-form analytical bounds that expose the role of the structure ratio $\\rho_G= \\operatorname{Tr}(G) /\\left(1^{\\top} G 1\\right)$, showing that large $\\rho_G$ implies intrinsic difficulty due to strong cancellation. Experiments show that the proposed bounds are much tighter than traditional norm-based guarantees, especially when interactions are strong."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel structural diagnostic for approximate matrix multiplication (AMM) through the definition of the structure ratio $\\rho_G=\\operatorname{Tr}(G) /\\left(1^{\\top} G 1\\right)$, which quantifies the degree of cancellation among rank-1 components in $A B^{\\top}$. This idea provides a new and interpretable way to characterize the intrinsic hardness of sparse or sampled approximations, bridging a gap between geometric structure and achievable approximation accuracy. The formulation of the auxiliary convex quadratic program (QP) and its closed-form Scaled-Id surrogate further represents a conceptually elegant framework that connects theoretical limits, practical computability, and structural properties of the data.\n\n2. The mathematical development is rigorous and well-structured, with clear derivations linking the structural ratio $\\rho_G$, the auxiliary QP, and the resulting bounds. The introduction of a hierarchy of relaxations: from the exact NP-hard sparse selection to a convex QP and then to the Scaled-Id analytic form-offers a well-motivated trade-off between theoretical tightness and computational efficiency.\n\n3. The proposed diagnostic provides a new lens to understand why standard sketching and sampling approaches may fail in highly structured or cancellation-dominated settings, emphasizing structural hardness rather than algorithmic deficiency."}, "weaknesses": {"value": "1. A key concept introduced in the paper is the structure ratio $\\rho_G=\\operatorname{Tr}(G) /\\left(1^{\\top} G 1\\right)$, which provides valuable insight into the intrinsic difficulty of achieving a sparse $k$-term approximation of $A B^{\\top}$. While theoretically informative, computing $\\rho_G$ exactly requires forming the interaction matrix $G=\\left(A^{\\top} A\\right) \\circ\\left(B^{\\top} B\\right)$, which involves constructing two dense Gram matrices and their Hadamard product. This computation incurs $O\\left(n^2\\right)$ time and memory, making it costly when the number of rank-1 components $n$ is large _i.e._, the regime for which understanding approximation hardness is most relevant. As a result, $\\rho_G$ is effective only for theoretical diagnosis (the authors also acknowledged this) when sparse approximation is fundamentally limited by cancellation effects. Its direct evaluation may not scale to large-scale matrix multiplication problems without additional approximation strategies or structure-exploiting techniques.\n\n2. While Figure 1 shows that classical AMM methods can be far from the theoretical optimum, this is demonstrated only for very small matrices ( $n=30$ ), a setting in which sketching-based AMM is not intended to excel and likely amplifies the observed performance gap. In the the larger-scale studies in Figure 2 (e.g., $n=5000$ or $n=8000$), this gap is much smaller as the situation is much more favorable for RandNLA-type algorithms although it focuses predominantly on low- or moderate-cancellation regimes ( $\\rho_G \\approx 1$). For large (fixed) $\\rho_G$, the sparsity vs. accuracy plot for is missing. \n\n3. All empirical evaluations are conducted on synthetic datasets generated under controlled structural assumptions (e.g., induced cancellation, repeated columns, nonlinear transforms). While this is appropriate for isolating the effect of $\\rho_G$, the paper does not assess the tightness or practical utility of the proposed bounds on real-world matrices. Moreover, the study does not examine common machine learning settings where AMM is routinely used (e.g., regression, kernel methods, low-rank approximation, neural network layers). As a result, it remains unclear whether the diagnostic insights and bounds carry over to realistic data distributions and practical AMM use cases."}, "questions": {"value": "1. Line 361: Fig. 5 $\\rightarrow$ Fig. 1\n\n2. The Scaled-Id bound relies on a single-parameter feasible point $y=\\gamma 1$. While analytically convenient, this seems restrictive. Why is this direction theoretically justified as representative of the QP's minimizer, and could alternative feasible vectors yield tighter or more interpretable bounds?\n\n3. Can you give some motivating real-world examples where such structures ($\\rho_G\\gg 1$) naturally arise?\n\n4. I would like to better understand the practical interpretation of the proposed diagnostic. Suppose we are given data matrices $A$ and $B$, and we can compute $\\rho_G$ efficiently. If the computed value of $\\rho_G$ is large, does this imply that RandNLA-type sketching or sampling methods are likely to fail, and that one should instead rely on sparse-recovery-style algorithms such as OMP (the fast variants of it) for downstream computations? In other words, is the intended use of $\\rho_G$ as a decision criterion for choosing between random projection-based and structure-aware approximation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3WOZlqt8nE", "forum": "iZMz2hNnTn", "replyto": "iZMz2hNnTn", "signatures": ["ICLR.cc/2026/Conference/Submission14352/Reviewer_cc8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14352/Reviewer_cc8b"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762848193128, "cdate": 1762848193128, "tmdate": 1762924771483, "mdate": 1762924771483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}