{"id": "EGDA43lVDR", "number": 3174, "cdate": 1757349459538, "mdate": 1759898104121, "content": {"title": "Coupling RNN with LLM: Does Their Integration Improve Highly Order-Sensitive Language Understanding?", "abstract": "Pretrained large language models (LLMs) have demonstrated remarkable success across various language modeling tasks. However, in domain-specific applications, particularly those involving highly order-sensitive data, general LLMs exhibit limitations in achieving state-of-the-art performance. One notable issue is that the contextual embeddings by LLMs still lack a strong positional inductive bias, especially for long and highly ordered sequences, leading to the \"lost in the middle\" problem. In this work, we utilized the potential of sequential models (RNNs) with LLMs to address the issue and investigate whether RNN integration improves LLM performance. The LLM generates rich contextual embeddings using the attention mechanism of the Transformer. The RNN further processes the LLM embeddings to capture the contextual semantics of long and order-sensitive dependencies. The LLM-RNN model leverages the potential of both Transformer and recurrent structures to enhance performance in domain-specific tasks. We perform a wide range of experiments leveraging multiple types of LLMs (encoder-only, encoder-decoder, and decoder-only) and RNNs (GRU, LSTM, BiGRU, and BiLSTM) across diverse public and real-world datasets to investigate the potential (either positive or negative) of LLM-RNN models. The experimental results highlight the superiority of the LLM-RNN model, showing improvements in commonsense reasoning, code understanding, and biomedical reasoning tasks.", "tldr": "We comprehensively investigate whether RNN integration improves LLM performance on Highly Order-Sensitive Language Understanding.", "keywords": ["Coupling RNN and LLM", "Highly Ordered-Sensitive", "Sequential Model", "LLM", "RNN"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc4f8043d4ee0c95c31289aea844a294bab55650.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates whether integrating RNN layers with LLMs can improve performance on tasks involving high order sensitivity. The authors propose a hybrid LLM-RNN architecture, where pretrained LLMs, including encoder-only, decoder-only, and encoder-decoder variants, generate contextual embeddings that are subsequently refined through RNN layers (LSTM, GRU, BiLSTM, or BiGRU). Evaluation results indicate a performance increase with LLM-RNN models on the classification task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper evaluates multiple LLM architectures (encoder-only, decoder-only, encoder-decoder) and RNN variants, providing a comprehensive assessment across different configurations.\n\n2. The hyperparameters and all the experimental settings are provided, facilitating replication and following researches.\n\n3. The paper is clearly written and easy to understand."}, "weaknesses": {"value": "1. The paper lacks theoretical analysis and primarily presents an empirical study, offering little insight into why the integration improves performance.\n\n2. The observed improvements are largely intuitive and expected, given that RNNs are naturally suited for modeling sequential dependencies.\n\n3. The performance gains reported in Table 1 are limited—mostly under 1%. This raises questions about the practical significance of the proposed approach.\n\n4. The coupling mechanism between the LLM and RNN is relatively shallow, simply passing LLM embeddings to an RNN without exploring deeper or more synergistic integration (e.g., incorporating recurrence within LLM layers).\n\n5. While the paper mentions various tasks such as code understanding, commonsense reasoning, and biomedical text analysis, the evaluation is limited to classification. Since LLM embeddings are widely used for generative and reasoning tasks, it remains unclear whether the RNN outputs preserve the generative or semantic richness of the original LLM representations. -- If LLM-RNN does not match or exceed pure LLM performance on other core tasks such as reasoning or generation, then their practical usefulness would be quite limited."}, "questions": {"value": "1. Beyond classification, have you tested the LLM-RNN model on other standard LLM tasks such as question answering, structured generation, or sequence completion?\n\n2. Have you conducted any theoretical analysis to compare the pure LLM and the LLM-RNN models, for example examining how differences in message passing or internal representation dynamics might account for the observed results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VJshZN0XAH", "forum": "EGDA43lVDR", "replyto": "EGDA43lVDR", "signatures": ["ICLR.cc/2026/Conference/Submission3174/Reviewer_Jwg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3174/Reviewer_Jwg8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760468203051, "cdate": 1760468203051, "tmdate": 1762916583727, "mdate": 1762916583727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach that integrates RNNs with LLMs to enhance the performance of order-sensitive tasks. The authors propose coupling LLMs with RNNs, leveraging the sequential processing power of RNNs to complement the contextual embeddings produced by LLMs. Experiments show that the LLM-RNN model outperforms LLMs in order-sensitive tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes an interesting idea about the improvement of LLM and the idea is simple but useful. \n\n2. The paper conducts a thorough evaluation using various LLMs and RNN across multiple tasks.\n\n3. This paper is good writing and easy to follow."}, "weaknesses": {"value": "1. The Transformer incorporating RNN can solve order-sensitive tasks well with the advantage of RNN while the performance can also be affected by the disadvantage of RNN. For example, LSTM and other RNN models always forget some information with the encoding while the long context in LLM can make it worse.\n\n2. Although the paper tests multiple RNN models, it would be helpful to explore how other advanced RNN could further enhance performance.\n\n3. The authors should also give some ablations about the comparision between larger LLM and LLM with RNN."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZmAUSDQ7ZX", "forum": "EGDA43lVDR", "replyto": "EGDA43lVDR", "signatures": ["ICLR.cc/2026/Conference/Submission3174/Reviewer_A4hM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3174/Reviewer_A4hM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941581722, "cdate": 1761941581722, "tmdate": 1762916583573, "mdate": 1762916583573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a simple hybrid: take hidden states from a pretrained LLM, project them, and feed them to different RNN models, followed by a linear head. The authors evaluate on datasets from different domains across tasks (sentiment, code understanding, and biomedical). Experiments indicate that the integration of LLM and RNN can achieve improvement versus the base pre-trained LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper is well-organized and easy to follow.\n\nS2. This paper spans three task families and multiple LLM families (e.g., BERT, GPT, DeepSeek) with four different RNN types, which help probe when RNN post-processing helps."}, "weaknesses": {"value": "W1. The paper is basically a simple hybrid that feeds a pretrained LLM’s hidden states into an RNN for classification across several domains. The experiments show small gains, but there is little deeper theoretical insight so the work reads as a simple incremental study. \n\nW2. The claims are overstated. The text says “in all cases RNNs consistently enhanced performance,” yet the tables show counterexamples, for example GPT-2 on Sentiment140 drops from 81.18 to 80.97, and DeepSeek-Coder on CodeXGLUE defect detection also declines. The “consistent” claim does not hold.\n\n\nW3. The motivation around order sensitivity and “lost in the middle” is weakly supported. Most benchmarks are short-context sentiment or code classification. There is no sequence-length stress test or long-context suite, and no controlled length-sweep analysis.\n\n\nW4.​​ The study compares LLM vs. LLM+RNN but not against other lightweight sequence shapers on top of LLM states, for example a small TCN or an extra self-attention block with relative position bias. There is also no ablation on where to attach the RNN, final layer vs. intermediate layers, or whether freezing the LLM changes the conclusion."}, "questions": {"value": "Please see Weaknesses.\n\nAdditional Questions:\n\n\nQ1. Under a matched hyperparameter search for base LLMs and LLM+RNN variants, do the hybrids still outperform, and how does this change on long-context (length-binned) evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MCauuOfsIg", "forum": "EGDA43lVDR", "replyto": "EGDA43lVDR", "signatures": ["ICLR.cc/2026/Conference/Submission3174/Reviewer_5Rk5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3174/Reviewer_5Rk5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149308894, "cdate": 1762149308894, "tmdate": 1762916583431, "mdate": 1762916583431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hybrid architecture where pre-trained language models (BERT, RoBERTa, GPT-2, CodeT5, DeepSeek) generate contextual embeddings that are then processed by RNN layers (GRU, LSTM, BiGRU, BiLSTM) before classification. The authors claim this helps capture sequential, order-sensitive dependencies that transformers miss.\n\n**Methodology:**\n- Freeze pre-trained model weights\n- Extract token embeddings from the LLM\n- Feed embeddings through RNN → FC classifier\n- Test on three domains:\n  - Sentiment analysis (IMDb, Twitter Airlines, Sentiment140)\n  - Code Defect detection (CodeXGLUE + 3 custom datasets)\n  - Biomedical reasoning Named entity recognition (NCBI disease dataset)\n\nThe main claim is RNNs refine LLM embeddings to better capture sequential order, improving performance on order-sensitive tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Tests multiple model architectures (5 base models × 4 RNN types) across 7 datasets"}, "weaknesses": {"value": "### 1. Poor Writing Quality - Fails Basic Scientific Communication Standards\n\nThe paper's writing is incoherent and makes it hard to understand what the authors want to say. **This is a basic requirement for a scientific paper.**\n**Concrete examples:**\n- **2nd paragraph**: Rambles about fine-tuning, LoRA, knowledge graphs with no clear connection to the paper's thesis\n- **3rd paragraph, Line 75-76**: Cites papers that finetune BERT/RoBERTa for sentiment classfication tasks to support LLM has limitation in sentiment analysis and code understanding. Why these two citations related to code understanding? They have no relation to code. Also, BERT/RoBERTa are not LLM at all. It makes no sense to claim LLM have trouble in sentiment analysis and code understanding, when previous show BERT/RoBERTa have trouble.\n- **Figure 1**: Shows \"embedding shift\" and \"saliency\" metrics but never defines them - no explanation of how they're computed or what they represent\n- The introduction reads like \"random talking instead of academic paper\" with no logical flow or clear problem motivation\n- The introduction also mentioned \"lost in the middle problem\" while the full paper didn't touch that problem at all. Also, that work is about 100K+ context retrieval, not relevant to their short sequences. \n- Conflates \"order sensitivity\" with general sequential modeling without clear definition\n\n### 2. Obsolete Approach with Questionable Motivation and Contradictory Results\n\n**The investigation was already done 2018-2020:** Adding RNN layers on top of transformer embeddings is not novel. See [\"Simple BERT Models for Relation Extraction and Semantic Role Labeling\"](https://arxiv.org/abs/1904.05255) and related work from that era. \n\n**Modern LLMs don't work this way:** Current best practices use generation-based classification (prompting) rather than fine-tuning classification heads. The paper provides **no evidence** that their approach is better than modern methods, nor any comparison.\n\n**Their own results contradict their claims:** In Table 3 , **DeepSeek-coder by itself without any RNN head works best**. This directly undermines their central thesis that RNNs improve order-sensitive understanding."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FfDyZbRuIy", "forum": "EGDA43lVDR", "replyto": "EGDA43lVDR", "signatures": ["ICLR.cc/2026/Conference/Submission3174/Reviewer_9tcA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3174/Reviewer_9tcA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3174/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213194099, "cdate": 1762213194099, "tmdate": 1762916583283, "mdate": 1762916583283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}