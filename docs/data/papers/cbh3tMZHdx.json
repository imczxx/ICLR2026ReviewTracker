{"id": "cbh3tMZHdx", "number": 22199, "cdate": 1758327661346, "mdate": 1759896880818, "content": {"title": "Which English Do LLMs Prefer? Quantifying American and British English Through a Postcolonial Lens", "abstract": "Large language models (LLMs) are increasingly deployed in high-stakes domains, yet they expose only limited language settings, most notably “English (US)”, despite the colonial history and global diversity of English. We frame dialectal asymmetries through a postcolonial lens, showing that they emerge not only as downstream failures but as structural artifacts of the LLM development pipeline itself. Using a curated lexicon of 1,813 American–British variants, we triangulate evidence across three stages: (i) audits of six major pretraining corpora reveal systematic skew toward American English, (ii) tokenizer analyses demonstrate that British forms incur higher segmentation costs, and (iii) generative evaluations with our proposed DiAlign metric show consistent preference for American variants. This constitutes the first systematic examination of dialectal asymmetries in standard English varieties within LLMs. We find that these models exhibit modeling bias that privileges American English as the de facto norm, shaped by geopolitical histories of data curation and linguistic standardization. Our study raises concerns of linguistic homogenization, epistemic injustice, and inequity in global AI deployment, while offering practical guidance for developing more dialectally inclusive language technologies.", "tldr": "We provide the first systematic study of how LLMs privilege American over British English across data, tokenization, and generation, exposing modeling bias rooted in historical and sociopolitical asymmetries through a postcolonial lens.", "keywords": ["large language models", "pretraining corpora", "tokenization", "generative preferences", "postcolonial lens", "modeling bias", "dialectal inclusivity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee0dd5caf3cabcb12af0e18b3e0f03db32fe061f.pdf", "supplementary_material": "/attachment/66353d1d46b107fbc3e00d9b3244f6a6d09aefac.zip"}, "replies": [{"content": {"summary": {"value": "Overall, this paper solved the problem of understanding how language models handle dialectal variation within English, particularly differences across regional and social varieties. This paper proposed a systematic evaluation framework to measure how LLMs respond to different English dialects and accents in both comprehension and generation settings, revealing biases and performance disparities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1/ The authors conducted a comprehensive empirical study across multiple varieties of English (e.g. American, British, Indian, Nigerian), showing clear and reproducible evidence of dialectal performance gaps in several state-of-the-art models.\n\n2/ The authors designed a well-structured benchmark and diagnostic framework that goes beyond surface lexical differences, incorporating syntactic and pragmatic features of dialects.\n\n3/ The analysis is insightful and socially relevant, highlighting that model biases persist even among English varieties, which has implications for fairness and inclusivity in language technology.\n\n4/ The methodology and dataset are transparent and replicable, with open-source components that can be reused for further research on dialectal robustness."}, "weaknesses": {"value": "1/ I think the paper could expand the linguistic interpretation of results—some disparities are reported quantitatively but not explained linguistically (e.g. why specific dialects lead to more errors).\n\n2/ The evaluation still relies heavily on automatic metrics,  which may not capture nuanced sociolinguistic variation. I suggest incorporating human-in-the-loop assessments for more robust validation.\n\n3/ The study focuses primarily on English varieties and does not clearly connect findings to broader cross-lingual or multilingual generalization—a missed opportunity to contextualize results.\n\n4/ Some experimental setups lack clarity, especially how prompts were standardized across dialects; a clearer description of prompt design and normalization would strengthen reproducibility."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AtjP22VzoO", "forum": "cbh3tMZHdx", "replyto": "cbh3tMZHdx", "signatures": ["ICLR.cc/2026/Conference/Submission22199/Reviewer_1BfZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22199/Reviewer_1BfZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873031524, "cdate": 1761873031524, "tmdate": 1762942111475, "mdate": 1762942111475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a study comparing the biases in data, tokenization, and generations between US and British English. First, they use a paired lexicon to estimate the prevalence of each dialect in common pre-training corpora (RQ1). Then, they use the same paired lexicon to measure the difference in token fertility between the dialects for several commonly used LM tokenizers (RQ2). Finally, they measure the level of alignment between model generations and dialect usage using an N-gram based measure in response to both AmE and BrE stimuli in two different benchmarks. In all 3 cases, the work finds that the LLM pipeline and supply chain are biased towards American English."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The work provides a well controlled set of studies to measure the dialect preference between AmE and BrE in several critical stages of the LLM training and usage that might influence these biases. The experiments are well designed and extensively executed over many relevant datasets, tokenizers, and models.\n- The work is very well written with clear figures and tables, such that it can be easily read and easily jumped around through to find core results.\n- The DiAlign measure could be combined with other existing regional variation corpora, beyond just AmE and BrE, as a reasonably well grounded score for dialect identification! This could be useful for broader pretraining analyses than presented here, especially for more global Englishes."}, "weaknesses": {"value": "- It's unclear to me to what degree these differences represent biases beyond differences in population size. While there is maybe a deeper argument to be made about which forms are used more commonly even outside Britain and the US, the work doesn't actually measure these. Instead, the vast majority of the tendency towards US English forms could largely be explained by the US having almost 5 times more people than the UK.\n\n- In general, while this is the type of work I myself find exciting, I'm not sure whether ICLR is the best home for it given it mostly focuses on analyzing linguistic features of LLMs rather than any aspect of their underlying representation or learning theory. This is not a weakness of the work overall, but I do worry the work would be more widely read if submitted to a more language oriented venue such as COLM or ACL. (This is primarily motivated by the bullet point in the reviewer form \"Are the results valuable to share with the broader ICLR community?\")\n\n- I'm not really sure how the postcolonial lens aspect pointed to in the title comes into play in the substance of the work. By and large, it's more of a shallow reference than deeper engagement with postcolonial theory from what I can tell so might be better removed entirely (especially since it makes the work likely more out of distribution for the ICLR audience)"}, "questions": {"value": "- Why wasn't DiAlign used for RQ1 as well as RQ3? I understand why for RQ2 it is helpful to have specific words to measure fertility for, but for RQ1, DiAlign seems like a strictly more powerful metric than the purely lexical one used.\n\n- Given the related work on dialect robustness in LLMs, is \"first systematic audit of dialectal asymmetries in LLMs\" a fair claim to make about this work? It seems a bit of a broad claim to me, but this is a nitpick and this phrasing could be defended I'm sure.\n\n- L160: \"The variant pairs were manually compiled from authentic linguistic sources and web-based lexicons.\" - Which sources and web-based lexicons are these? Cite your sources, especially since that helps readers get a sense of the expected quality of the data you use!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FbM3cicX2H", "forum": "cbh3tMZHdx", "replyto": "cbh3tMZHdx", "signatures": ["ICLR.cc/2026/Conference/Submission22199/Reviewer_Lvkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22199/Reviewer_Lvkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890943136, "cdate": 1761890943136, "tmdate": 1762942111249, "mdate": 1762942111249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this paper investigate how large language models encode and reproduce dialectal asymmetries between American English and British English , framing these disparities through a postcolonial lens. The authors argue that this bias arises not just from downstream generations but from other aspects within the entire LLM development pipeline from training data to tokenization, etc."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated. Most previous studies on dialectal bias primarily compare “low-resourced” dialects to Standard American English. While biases in such settings are often more severe and consequential than those in the current case studies, it is still valuable to see work examining underexplored biases toward British English.\n- The framework is solid and thorough, going from corpus-level audits to tokenizer biases, and up to the “utility” of LLMs, including their generative preferences for American over British English.\n- Results are not surprising, given that most stakeholders involved in developing LLMs are located in regions where Standard American English is spoken. However, they are still interesting because they provide rigorous experimental confirmation of these biases.\n- A direct impact of this work is its connection to understanding how biases against British English can permeate to non-standard dialects that were adopted due to colonialism."}, "weaknesses": {"value": "- It is not very clear in the paper how the authors control for vocabulary variants that mean the same thing across dialects but differ in form or part of speech (e.g., elevator vs. lift, apartment vs. flat). Context seems important when computing frequencies; analyzing words in isolation could overestimate or underestimate dialectal bias.\n- The results show that British forms yield higher fertility for vocabulary-based differences. Given the close similarity between British and American English, tokenization may depend heavily on contextual co-occurrence patterns. Analyzing fertility in isolation might wrongly estimate the contribution of dialectal differences to tokenization biases.\n- Observing small differences in tokenization parity is useful, and i appreciate the downstream experiments on generative preferences. However, from an efficiency point of view, are there really large detrimental impacts from overtokenization of British English? For example, in full sentences in full sentences with other function words, would two or three overtokenized words meaningfully increase sequence length? This might matter more for dialects with larger orthographic and lexical differences, especially those influenced by other languages in other regions, than for British vs. American English.\n- I think it might be a stretch to claim that models that the large vocabulary size of Gemma guided by dialect-aware corpora will dramatically improve dialect coverage in the tokenizers. Can you expand further on this ?"}, "questions": {"value": "- Recent LLM training data are often scraped from broad web sources across the globe. If tokenizers were designed to be more British-dialect aware, would this lead to many underused tokens in the vocabulary during training? \n- What are the consequences of these biases for countries that adopted British English due to colonialism? How severe might these biases be when interacting with non-standard dialects of British English? I think this is a really interesting topic that is missing from your paper, but seems relevant given your framing and paper title."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mL7bxAKsVU", "forum": "cbh3tMZHdx", "replyto": "cbh3tMZHdx", "signatures": ["ICLR.cc/2026/Conference/Submission22199/Reviewer_nDHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22199/Reviewer_nDHy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111686902, "cdate": 1762111686902, "tmdate": 1762942110977, "mdate": 1762942110977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the differences between American and British English dialects in language modelling. More specifically, the paper takes a postcolonial lens to examine this distinction (as a disclaimer, I feel unqualified to review how well this is done, as I am not entirely certain what a postcolonial lens is, exactly). \n\nConcretely, the paper first builds a corpus of British and American lexical items which constitute minimal pairs differing in: (i) spelling (e.g., modelling vs. modeling); (ii) vocabulary use (e.g., restroom vs. lou). They then examine these words: (i) frequency in pretraining corpora; (ii) length under different tokenisers; (iii) prevalence in LLMs’ outputs. They find a consistent preference for American English across language models, but with differences across them (e.g., Gemma displays a weaker preference for American English than most other models)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "This paper examines an important question, how specific dialectal differences may affect language model users. In particular, as the paper states:\n\n> “The United Nations affirms language rights as fundamental, with Article 19 of the Universal Declaration guaranteeing the freedom to communicate in one’s language of choice.”\n\nIt is thus important that language model users be able to use these technologies using their own dialect.\n\n\nThis paper also builds a new corpus to perform a more specific comparison between American vs. British English.\n\nFinally, the paper investigates statistics of these collected words in language models’: pretraining data, tokenisers, and outputs."}, "weaknesses": {"value": "The paper’s main practical contributions are: a dataset, and three relatively “simple” comparisons between British and American English. Beyond these, the paper also contributes with its research question, i.e., with the idea to compare two major dialects of English. I think that, if published, this paper could have a reasonable impact as it documents this disparity.\n\nWhile this paper addresses an important topic, however, I am unsure whether its contributions warrant an ICLR paper, so I am leaving my score as borderline reject. In particular, the paper makes no methodological contributions, and its analyses are all fairly straightforward. Further, the paper compares only two dialects of a single language. The paper is thus not comprehensive either in its experiments (i.e., only analysing simple word-level statistics) or in its objects of analysis (i.e., the set of analysed dialects). I choose this score with reservations, though, as I do believe the paper could have a reasonable impact (as said above), and I am open to increasing my score if convinced that I am missing some points of contribution. \n\nThe paper also restricts its analyses to single-word items. They justify this by stating that this “ensures consistency across analyses and is essential for the tokenizer study [RQ2 (§5)], where precise word-level comparisons are required to directly compare segmentation behavior.” However, several languages lack a clear concept of what a word is—and, as this paper investigates dialects from a postcolonial lens, using a Westernised notion of wordhood should warrant at least a short discussion. Furthermore, if two dialects exhibit systematic differences in how they segment “words”, this may lead to biased comparisons between them. Beyond that, from a technical standpoint, I see no reason why this restriction to single words is required in this paper, as all performed analyses seem to apply to multi-word expressions.\n\nMaybe this paper's contributions could also be strengthened by expanding on what exactly makes this paper's analyses postcolonial. How does a postcolonial lens influence this study, and in which way is it better than/different from a \"traditional\" analysis because of this lens? Or was the postcolonial lens relevant only in the paper's choice of research question, but not in its choice of methodology?"}, "questions": {"value": "> Title: Quantifying American and British English Through a Postcolonial Lens\n\nCould you expand on what exactly is different between “traditional” vs. “postcolonial-lens” analyses? In particular, as it applies to examining the prevalence of American vs. British Englishes in language models’ tokenisers and outputs, how does a postcolonial-lens analysis differ from a “typical” analysis. I believe several people in machine learning might similarly appreciate an exposition about this difference, so adding a dedicated discussion of this distinction to the paper could be helpful.\n\n> Tokenizer fairness.\n\nAs the paper states, “equivalent strings can receive uneven tokenization across languages [...] (Petrov et al., 2023).” Relatedly, Lesci et al. (2025) estimate the causal effect of such uneven tokenisation on language models’ outputs, showing that a single-token word may receive up to 17 times more probability than it would if tokenised as two tokens. This effect should add to the point that these uneven tokenisations across languages will be reflected in differences in a model's quality on them. Another potentially relevant paper here is Fourotan et al. (2025), who propose a new BPE variant which aims to improve cross-lingual fairness.\n\n\n> we retained only strict one-to-one word-level mappings, and excluded many-to-one\n\nI do not understand the restriction of the analysis to single-word units. How would multi-word units affect the analyses here?\n\n\n## References\n\n* Lesci et al. (2025). Causal Estimation of Tokenisation Bias. ACL. https://aclanthology.org/2025.acl-long.1374/\n* Fourotan et al. (2025). Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization. arXiv. https://arxiv.org/abs/2508.04796"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sFzbxEFDpU", "forum": "cbh3tMZHdx", "replyto": "cbh3tMZHdx", "signatures": ["ICLR.cc/2026/Conference/Submission22199/Reviewer_LFdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22199/Reviewer_LFdN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243379831, "cdate": 1762243379831, "tmdate": 1762942110566, "mdate": 1762942110566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}