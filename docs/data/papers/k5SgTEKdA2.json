{"id": "k5SgTEKdA2", "number": 17156, "cdate": 1758272850560, "mdate": 1759897193427, "content": {"title": "Rethinking LoRA Aggregation for Federated Fine-tuning of Foundation Models", "abstract": "The application of Low-Rank Adaptation (LoRA) in Federated Learning (FL) systems provides an effective solution for Foundation Models (FMs) to leverage distributed private data. However, the heterogeneous distribution of client-side data has hindered the performance of federated systems from reaching. Through an in-depth investigation of this issue, we discover that LoRA parameter aggregation among clients gives rise to fine-grained conflicts and introduces the cross-term noise interference for subsequent rounds. Both factors disadvantage the efficient convergence of federated fine-tuning performance. Based on these findings, we propose a Harmonious Federated Low-Rank Adaption method (HFLoRA), which first detects conflicts in LoRA row update directions between clients through a fine-grained joint regulation mechanism, then imposes inhibitory constraints on anomalous conflict rows using scaling factors. In addition, we have designed a global LoRA consistent re-decomposition strategy that further mitigates the impact of cross-term noise on FL by computing a pair of optimal low-rank matrices from the aggregated noise-free global LoRA. HFLoRA is also applicable to federated environments with heterogeneous LoRA and does not introduce additional communication costs. Extensive experiments across natural language generation and vision tasks demonstrate that HFLoRA consistently outperforms other state-of-the-art FL methods on different benchmarks. Our code is available at: https://anonymous.4open.science/r/HFLoRA.", "tldr": "We explore the issues of fine-grained LoRA aggregation conflicts and aggregation noise in federated fine-tuning, and propose a solution (HFLoRA) to further enhance the performance of federated fine-tuning for foundational models.", "keywords": ["Federated Fine-tuning", "Low-rank adaptation", "Foundation Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c0549998b3c3adebe138efc18212696b8c2c78c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work finds that LoRA parameter aggregation among clients gives rise to fine-grained conflicts and introduces the cross-term\nnoise interference for subsequent rounds. Based on these findings, the authors propose a Harmonious Federated Low-Rank Adaption method (HFLoRA), which first detects conflicts in LoRA row update directions between clients through a fine-grained joint regulation mechanism, then imposes inhibitory constraints on anomalous conflict rows using scaling factors. In addition, they design a global LoRA consistent re-decomposition strategy that further mitigates the impact of cross-term noise on FL by computing a pair of optimal low-rank matrices from the aggregated noise-free global LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this work is clear.\n2. The authors investigate the parameter conflicts across clients and cross-term noise issues in LoRA aggregation within federated learning, offering insights into the causes of performance degradation.\n3. The proposed HFLoRA mitigates LoRA row aggregation conflicts and implements the global LoRA consistent re-decomposition\nto enhance FL performance under data heterogeneity.\n4. Extensive experiments on NLP and CV tasks demonstrate the superiority of HFLoRA over existing methods."}, "weaknesses": {"value": "1. The font in Figure 1(a) is a bit small, and the experimental settings are not clearly explained. Is this the result for the IID case or the non-IID case? Will data heterogeneity have an impact on this?"}, "questions": {"value": "Though this work is well-motivated and well-written, I have some questions about it:\n1. Regarding the statements in lines 261-264. Is there any error in using the average of all clients as the anchor point? After all, this average value does not represent the consensus update direction. If this average can be used as the anchor point, does it imply that it is regarded as the consensus update direction? If so, why not simply use the average?\n2. In lines 279-280: \"Influenced by the experimental results (Fig.1a), we set δ as the percentile 85% of the statistical distribution of the conflict values C.\" Will the different models and datasets here be different? Is it optimal to set it directly to 85%?\n3. In lines 280-282: \"This threshold will be adjusted in accordance with the changes in parameter distribution during each round of aggregation, accommodating the dynamic instability and convergence of the model training at different stages.\" How exactly is this threshold adjusted?\n4. How is the optimization problem in Equation (11) solved? What is the time complexity? Furthermore, an experiment needs to be added. In this case, what is the true difference between the learned \\\\( B^{t+1} \\\\) and \\\\( A^{t+1}  \\\\) and \\\\( \\Delta W_{\\text{global}} \\\\)? Has the problem of aggregation errors really been resolved?\n5. In lines 322-323: \"The second term acts as a consistent regularization to prevent the optimized \\\\( B^{t+1} \\\\) and \\\\( A^{t+1}  \\\\) from deviating from the original parameters.\" Why is this term added? The original average \\\\( A_i \\\\) probably doesn't make much sense either, does it? Moreover, an ablation study needs to be included to evaluate the removal of these two terms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7z6NU5ZLYR", "forum": "k5SgTEKdA2", "replyto": "k5SgTEKdA2", "signatures": ["ICLR.cc/2026/Conference/Submission17156/Reviewer_1dtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17156/Reviewer_1dtz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621627773, "cdate": 1761621627773, "tmdate": 1762927141834, "mdate": 1762927141834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why standard LoRA aggregation under data heterogeneity underperforms in federated fine-tuning of foundation models and proposes **HFLoRA**, a two-part remedy.\n\nFirstly, the server forms the LoRA weight matrix per client and calculates a *conflict score* for each row using cosine distance to the mean-normalized row direction across clients. Rows whose conflict exceeds a threshold get scaled down by an exponential decay factor, and the paper proves variance reduction and convergence under standard FL assumptions.\n\nSecondly, instead of aggregating $B$ and $A$ separately, the server first aggregates the regulated full-rank LoRA matrices into a noise‑free $\\Delta W_g=\\sum_k p_k \\Delta \\widetilde{W}_k$ and then solves an optimization problem to obtain the next‑round low‑rank factors $(A^{t+1}, B^{t+1})$  that reconstruct $\\Delta W_g$ and remain close to the averaged $(A^t, B^t)$ to ensure parameter‑space consistency.\n\nExperiments cover LLaMA‑2‑7B instruction-tuning on Alpaca‑GPT4 and OpenR1‑Math, as well as ViT on Places365. HFLoRA generally improves over other baselines.\n\n---\n### LLM usage disclosure (reviewer)\nI used GPT‑5 to help polish and organize this review; I take full responsibility for the content."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Clear diagnosis of two concrete aggregation pathologies.** The cross‑term argument is clean and instructive, and the row‑level conflict visualization is compelling and clearly shows that only a minority of rows are strongly conflicting.\n\n**Simple server‑side implementation.** Computing row conflicts, scaling, and then aggregating is conceptually straightforward and compatible with standard communication.\n \n**Theoretical backing** Thm. 4.1–4.2 give variance reduction and a non‑convex FL‑style convergence rate under standard smoothness/variance assumptions plus a consensus update assumption tailored to the row‑level design. While high‑level, this is consistent with contemporary FL analyses."}, "weaknesses": {"value": "1. **Key assumptions and sensitivity not fully explored:** The core Consensus Update assumption posits that client row updates concentrate around a global direction with slight systematic bias. In realistic, highly non‑IID regimes (disjoint tasks, label/feature skew), this may fail. The method might over‑suppress legitimate client‑specific information. A robustness study varying heterogeneity (especially on NLG) and stress‑testing beyond $\\beta=0.5,1.0$ would strengthen the claim. \n\nThe conflict threshold $\\delta$ is set at the 85th percentile. The paper does not demonstrate sensitivity to $\\delta$ or $\\lambda$, nor does it provide a principled approach to setting them. This matters because $\\gamma_j$ can effectively zero out rows. \n\n2. **Details of the re‑decomposition solver are missing.**\n   Equation (11) defines a bi-linear least-squares problem with regularizers; however, the paper does not specify the optimization routine or configuration. \n\n3. **Heterogeneous ranks: how is the broadcast handled?** The method claims to support $r_k$ heterogeneity across clients, but it is unclear what rank the server uses for $(A^{t+1},B^{t+1})$ and how clients with different $r_k$ ingest the broadcast. Do clients project to their local ranks (and if so, how)? More detail is needed on the global rank selection and compatibility across rounds. \n\n4. **Computation and memory cost at the server**: This method requires reconstruction and operation on the full-rank matrices at the server, which increases the memory and computation significantly in comparison to low-rank aggregation.\nThe cost of the server for both reconstruction and decomposition is not reported, and the scalability is not clear."}, "questions": {"value": "Please look at the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mEhMKTXi9q", "forum": "k5SgTEKdA2", "replyto": "k5SgTEKdA2", "signatures": ["ICLR.cc/2026/Conference/Submission17156/Reviewer_bUNw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17156/Reviewer_bUNw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890519095, "cdate": 1761890519095, "tmdate": 1762927141606, "mdate": 1762927141606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the LoRA row conflict and cross-term noise problems in federated fine-tuning of foundation models. To address these issues, the proposed HFLoRA method combines fine-grained conflict regulation with global LoRA re-decomposition, achieving superior performance across diverse LoRA configurations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method offers a well-motivated and effective solution to key challenges in federated fine-tuning with low-rank adaptations."}, "weaknesses": {"value": "1. The experimental setup in Fig. 1(a) lacks sufficient explanation. What is the specific LoRA configuration used? Under which data setting were these experiments conducted? Additionally, which training round does this figure represent, early or late in the training process?\n\n2. There is a conceptual error in the discussion of FedSA (share-A)-LoRA in Line 156. In this setting, only matrix A is shared across clients, not matrix B.\n\n3. As a follow-up, since FedSA-LoRA shares only matrix A, and matrix B is typically zero-initialized, what global model was used in your evaluations? Based on their project’s description, it appears to be more aligned with a personalized FL method. Could you clarify?\n\n4. The issue of cross-term noise is not novel and has been discussed in many prior work.\n\n5. The paper omits relevant prior work on layer-wise efficient federated fine-tuning methods, such as FedRA [1] and Fed-HeLLo [2].\n\n6. Could you provide visualizations of the cosine distance for different tasks (visual v.s. language) as referenced in Line 259? These would help illustrate the variation in optimal directions across tasks. Are there any insights or takeaways from such visualizations?\n\n7. The threshold set in Line 279 appears to be empirically chosen. Was it evaluated under different tasks or heterogeneity settings?\n\n8. How is the optimization problem in Equation (11) solved? The paper does not provide sufficient details on the solution procedure or algorithm.\n\n9. There are several minor typos and formatting issues:\n\n– Line 83: use “this issue” twice\n\n– Line 245: “LORA” should follow consistent capitalization\n\n– Line 257: comma appears incorrectly at the beginning of the line\n\n10. Have you observed any similar conflicts in the columns (or channels, if applicable) of the weight matrices? If so, further analysis would be helpful.\n\n[1] FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients. ECCV 2024.\n\n[2] Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation. IEEE TNNLS 2025."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mSxPNvsjPv", "forum": "k5SgTEKdA2", "replyto": "k5SgTEKdA2", "signatures": ["ICLR.cc/2026/Conference/Submission17156/Reviewer_BJ4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17156/Reviewer_BJ4r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954410263, "cdate": 1761954410263, "tmdate": 1762927141389, "mdate": 1762927141389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies parameter-efficient federated fine-tuning with LoRA adapters and argues that two aggregation pathologies hinder performance under data heterogeneity:\n(i) Row-wise conflicts in the LoRA weight ΔW=BA, where a minority of row updates across clients point in incompatible directions; and (ii) cross-term noise from naively averaging A and B separately rather than aggregating the actual update ΔW."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies why naively averaging LoRA adapters across clients harms federated fine-tuning under non-IID data.\n- The paper crisply formalizes “cross‑term noise” in LoRA aggregation via the identity \n$∑p_k B_k A_k ≠ (∑p_k B_k)(∑p_k A_k)$\n(Eq. 3) and derives a compact expression for their difference (Eq. 4).\n- Broad evaluation across NLG and vision"}, "weaknesses": {"value": "- The paper argues lower communication than stacking. FLoRA indeed yields a global rank that grows with the number of clients (stacking), which can raise download costs; and discusses this trade-off explicitly. A numeric apples-to-apples communication/runtime study (same number of LoRA injected layers, same K, same ranks) would make Table 3 more convincing.\n- Limited analysis against LoRA-FAIR and FRLoRA, which share HFLoRA’s core goals. Both methods explicitly mitigate aggregation bias and initialization drift. More fine-grained comparisons e.g., robustness under varying heterogeneity β, cold-start behavior, and per-round convergence speed, would clarify whether HFLoRA truly outperforms them rather than offering a lateral variant.\n- Experiments use K ≤ 30 (vision) and ≤ 20 (LLM). In realistic cross-device FL, K ≫ 100, and many clients participate only intermittently. The per-round server overhead of reconstructing ΔWₖ = BₖAₖ and solving Eq. (11) could become significant. A concrete cost analysis (server FLOPs, GPU time, memory vs. K and r) is missing.\n- The method fixes δ to the 85th percentile of C and applies a decay λ (Eq. 9) but provides no study of how these choices affect performance or stability. Since the approach relies on suppressing a small subset of rows, the authors should plot accuracy and variance versus δ (70/80/90/95th percentiles), λ, and τ, as well as the fraction of suppressed rows per round.\n- Table 4 claims support for heterogeneous ranks, but Eq. (11) produces a single (A^{t+1}, B^{t+1}). How are clients with different rₖ handled via per-client SVD truncation, shared r projection, or multiple decompositions? What is the additional server cost? Baselines like FLoRA and FlexLoRA explicitly describe these redistribution strategies."}, "questions": {"value": "- Can you provide a clearer trade-off analysis among FLoRA, LoRA-FAIR, and FRLoRA, covering accuracy, communication, runtime, and heterogeneity robustness?\n- What optimization algorithm is used to solve Eq. (11) e.g., alternating least squares, proximal gradient, or an SVD-based closed form? how many iterations per round, and what are the typical time/memory costs per layer for LLaMA 2 7B? Does the added computation offset the claimed communication savings?\n- When clients use heterogeneous ranks, how is the server’s rank rt+1 chosen per layer? Is it fixed or adaptive? Any risk of rank deficiency hurting downstream performance?\n- Can you report the time required per round to transmit load and re-initialize LoRA weights on clients, and compare HFLoRA’s total per-round runtime (aggregation + re-decomposition + distribution) with baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VBVYqmaDul", "forum": "k5SgTEKdA2", "replyto": "k5SgTEKdA2", "signatures": ["ICLR.cc/2026/Conference/Submission17156/Reviewer_W2Sx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17156/Reviewer_W2Sx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17156/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961453229, "cdate": 1761961453229, "tmdate": 1762927140829, "mdate": 1762927140829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}