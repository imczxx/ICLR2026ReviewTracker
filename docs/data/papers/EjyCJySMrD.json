{"id": "EjyCJySMrD", "number": 10076, "cdate": 1758159957695, "mdate": 1763122801707, "content": {"title": "Generation by Search: Scaling Test-Time Compute for Autoregressive Image Generation", "abstract": "Image generation has made significant progress in recent years, but still faces difficulties in following challenging prompts and poor scalability with respect to inference-time computation. In this paper, we propose framing autoregressive image generation as a search problem, where the objective is to identify token sequences that maximize a chosen utility function at the test time. This framework enables fine-grained control over the generation process through flexible choices of utility functions and yields better scaling behavior as more test-time compute is used. Moreover, it is fully compatible with existing autoregressive generative models, which can be viewed as providing token-level priors during the search. To systematically investigate this framework, we organize the design space into four key axes and conduct studies across the choices of token structure (2D grid, 2D multi-scale, and 1D ordered), search algorithm (best-of-N, beam search, and lookahead search), verifier (optimizing for image-text and image-image alignment, as well as quality), and prior model (conditional and unconditional autoregressive models, and prior-free). Together, these findings establish search as a performant, controllable, and scalable approach to advancing image generation and provide practical guidance for future work in this direction.", "tldr": "We reframe image generation as a search problem, where searching over tokens enables direct generation, improves controllability, augments autoregressive models, and offers practical guidance for test-time scaling.", "keywords": ["image generation", "test-time scaling", "search algorithm", "autoregressive model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f266313f01736b2e40137580a23e360476ced54b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a test-time search framework for autoregressive image generation that treats generation as navigating a token-level search tree guided by verifier objectives. It systematically studies four axes, including token structure, search algorithm, verifier choice, and the AR prior. Across multiple AR models and benchmarks, SoT consistently improves alignment and controllability, exhibits favorable test-time scaling, enables zero-shot multimodal control via image verifiers, and shows that ordered tokenizations can even work with weak or no AR priors. The paper also analyzes verifier hacking risks and reports that ensembles and human-preference verifiers are more robust, providing practical guidance for deploying test-time search in AR image generation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper reframes autoregressive image generation as a search problem, unifying it with LLM-style test-time search.\n* It conducts a systematic study of test-time scaling with thorough experiments across verifiers, AR priors, and search algorithms.\n* The paper is overall simple, clear, and easy to follow."}, "weaknesses": {"value": "The core idea—applying test-time scaling to image generation—is straightforward and largely mirrors TTS in LLMs (e.g., beam search, best-of-N), with limited methodological novelty. The paper also omits key prior work in visual generation: for example, [1] as an early AR image-generation TTS method, and [2,3] as diffusion-based TTS approaches. Compared to these works, this paper contributes neither a new algorithmic technique nor fresh insights. For instance, while the authors discuss verifier hacking in TTS, they do not offer concrete mitigation strategies.\n\nThe experiments lack fair wall-clock comparisons. Reporting only NFE/FLOPs obscures practical deployment costs; comprehensive latency/throughput measurements (including detokenization overhead and verifier run-time) are needed for an apples-to-apples assessment.\n\n\n[1] Guo, Ziyu, et al. \"Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.\" arXiv preprint arXiv:2501.13926 (2025).\n\n[2] Zhuo, Le, et al. \"From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning.\" arXiv preprint arXiv:2504.16080 (2025).\n\n[3] Li, Shufan, et al. \"Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection.\" arXiv preprint arXiv:2503.12271 (2025)."}, "questions": {"value": "What exactly is the rollout strategy used in Figure 2? For example, in Janus, when only a small number of tokens have been generated early on, what does the detokenized image look like? Can the verifier reliably evaluate such partially decoded images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6qJrWVeuXU", "forum": "EjyCJySMrD", "replyto": "EjyCJySMrD", "signatures": ["ICLR.cc/2026/Conference/Submission10076/Reviewer_s9CM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10076/Reviewer_s9CM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761085935013, "cdate": 1761085935013, "tmdate": 1762921467196, "mdate": 1762921467196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "4OF31oz7UW", "forum": "EjyCJySMrD", "replyto": "EjyCJySMrD", "signatures": ["ICLR.cc/2026/Conference/Submission10076/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10076/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763122800696, "cdate": 1763122800696, "tmdate": 1763122800696, "mdate": 1763122800696, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper implements the test-time scaling algorithm for autoregressive (AR) image generation and adopts a tree-based search strategy that better aligns with the characteristics of AR models. The approach leads to a measurable performance improvement, supported by extensive experimental validation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The validation experiments are extensive and convincing, complemented by clear and informative figures and tables that make the results easy to interpret."}, "weaknesses": {"value": "1. The paper lacks clear novelty. It explores a well-known training-free approach for autoregressive (AR) image generation, but the proposed method is highly similar to existing works and does not offer distinctive innovations.\n2. As shown in Figure 2, decoding begins before the full image prediction is completed. This raises a concern: could such partial decoding introduce errors or instability in the verifier’s evaluation?\n3.Section 3.2 claims that the method follows a “coarse-to-fine” generation process consistent with human visual perception. However, true coarse-to-fine generation should occur at a global image level (as in diffusion models). Since this work only decodes partial tokens, it remains unclear how the claimed coarse-to-fine property is achieved.\n4.It is uncertain whether TTS provides consistent benefits throughout the entire generation process. For example, when predicting the second token, the model only conditions on the first token’s TTS result—does this provide meaningful guidance? Has the paper explored the possibility of selectively applying TTS to certain stages or tokens?"}, "questions": {"value": "Please provide responses to the issues raised in my Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NFo3GKdYee", "forum": "EjyCJySMrD", "replyto": "EjyCJySMrD", "signatures": ["ICLR.cc/2026/Conference/Submission10076/Reviewer_NgrF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10076/Reviewer_NgrF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969863378, "cdate": 1761969863378, "tmdate": 1762921466922, "mdate": 1762921466922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a test-time scaling framework for autoregressive (AR) image generation, formulating the generation process as a token-level search guided by verifiers. The method, termed Search over Tokens (SoT), explores multiple search algorithms (e.g., best-of-N, beam, lookahead) and verifier types to optimize image-text alignment and visual quality without retraining. Extensive experiments across benchmarks (GenEval, COCO, DreamBench++) demonstrate consistent improvements in alignment and controllability across various AR architectures. The study systematically analyzes four design axes—token structure, search algorithm, verifier, and AR prior—establishing test-time search as a scalable and general paradigm for enhancing AR image generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed approach is conceptually clear and easy to follow, making it accessible to readers familiar with autoregressive image generation.\n2. The experimental design is well thought out, with a logical structure that effectively supports the paper’s main claims."}, "weaknesses": {"value": "1. The contributions are limited. Although the introduction lists three claimed contributions, in practice only the first one—the design of a test-time scaling (TTS) framework—constitutes a substantive contribution. The second and third points (related to model choice, verifier design, search algorithm, and token structure) are more like implementation necessities rather than conceptual advances.\n2. In the introduction, when motivating the use of TTS, the authors only reference applications of TTS in large language models (LLMs). However, the idea of TTS was originally introduced and popularized in diffusion-based image generation methods, which should have been properly acknowledged.\n3. Diffusion-based TTS algorithms typically evaluate the entire image, whereas the proposed approach evaluates partially decoded images based on subsets of tokens. It seems that the former formulation might be more principled and semantically meaningful.\n4. The paper provides very limited discussion of the verifier component, which should be a central aspect of the design. It remains unclear how the verifier effectively assesses the impact of each partially decoded token on the global image quality, and how this aligns with the TTS objective."}, "questions": {"value": "Please refer to the \"Weakness\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aaURye2WXi", "forum": "EjyCJySMrD", "replyto": "EjyCJySMrD", "signatures": ["ICLR.cc/2026/Conference/Submission10076/Reviewer_mjcm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10076/Reviewer_mjcm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970386640, "cdate": 1761970386640, "tmdate": 1762921466615, "mdate": 1762921466615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reformulates autoregressive image generation as a token-level search problem and introduces the Search over Tokens (SoT) framework. SoT treats the pretrained AR model as a probabilistic prior and searches for token sequences that maximize verifier-based rewards such as CLIPScore and ImageReward. The framework systematically explores the design space across four axes: token structure, search algorithm, verifier type, and prior usage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a unified framework for test-time search in autoregressive image generation, connecting recent progress in inference-time scaling with structured token-level optimization.\n\n- It systematically evaluates a broad design space across tokenization, search strategy, and verifier composition, revealing empirical trends that can guide practical deployment.\n\n- The results show clear and reproducible scaling behavior under increased compute budgets, reinforcing the value of search-based generation as a general paradigm for AR models."}, "weaknesses": {"value": "- The contribution is largely empirical and heuristic, focusing on organizing and benchmarking existing components such as search algorithms, verifiers, and AR priors rather than introducing new theoretical insights or algorithmic principles.\n\n- The framework’s performance depends heavily on specific verifiers such as CLIP and ImageReward, but the paper offers limited analysis of verifier bias, reward overfitting, or robustness to unseen evaluation metrics.\n\n- Comparisons to other recent search-based or test-time scaling approaches including speculative decoding and inference-time optimization in diffusion or LLMs remain superficial, leaving unclear whether SoT offers fundamentally new capabilities beyond unification and benchmarking."}, "questions": {"value": "- How robust are SoT’s improvements under alternative verifiers or human evaluation when the optimization target differs from CLIP or ImageReward?\n\n- Can the authors clarify how SoT conceptually differs from other inference-time scaling or search-based generation frameworks, and whether its findings generalize beyond benchmark-specific settings?\n\n- Have the authors analyzed whether increasing search depth or relaxing the AR prior leads to diminishing returns or mode collapse, similar to behaviors observed in recent reward-based inference studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jKzAI3riUa", "forum": "EjyCJySMrD", "replyto": "EjyCJySMrD", "signatures": ["ICLR.cc/2026/Conference/Submission10076/Reviewer_Uvri"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10076/Reviewer_Uvri"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762533425142, "cdate": 1762533425142, "tmdate": 1762921466316, "mdate": 1762921466316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}