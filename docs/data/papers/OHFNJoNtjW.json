{"id": "OHFNJoNtjW", "number": 17263, "cdate": 1758274000413, "mdate": 1759897187353, "content": {"title": "Multimodal LLM-assisted Evolutionary Search for Programmatic Control Policies", "abstract": "Deep reinforcement learning has achieved impressive success in control tasks. However, its policies, represented as opaque neural networks, are often difficult for humans to understand, verify, and debug, which undermines trust and hinders real-world deployment. This work addresses this challenge by introducing a novel approach for programmatic control policy discovery, called **M**ultimodal Large **L**anguage Model-assisted **E**volutionary **S**earch (MLES). MLES utilizes multimodal large language models as programmatic policy generators, combining them with evolutionary search to automate policy generation. It integrates visual feedback-driven behavior analysis within the policy generation process to identify failure patterns and guide targeted improvements, thereby enhancing policy discovery efficiency and producing adaptable, human-aligned policies. Experimental results demonstrate that MLES achieves performance comparable to Proximal Policy Optimization (PPO) across two standard control tasks while providing transparent control logic and traceable design processes. This approach also overcomes the limitations of predefined domain-specific languages, facilitates knowledge transfer and reuse, and is scalable across various tasks, showing promise as a new paradigm for developing transparent and verifiable control policies.", "tldr": "We introduce MLES, a framework that employs multimodal large language models to analyze behavioral failures and guide evolutionary search, thereby enabling the efficient discovery of high-performance, human-readable programmatic policies.", "keywords": ["Automated control policy discovery", "Evolutionary computation", "Multimodal large language models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4943e055e18e8d1bb902630a630c402f57281ac4.pdf", "supplementary_material": "/attachment/47d593704c277666f0ebb78cfa957ec40cbae8cd.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Multimodal LLM Assisted Evolutionary Search (MLES) as an approach to automatically discovering control policies (as opposed to conventional deep reinforcement learning or genetic algorithms). MLES involves using multimodal LLMs to generate policies, which are then evaluated and summarized by a multimodal LLM to produce summary feedback, which is then provided to the policy generator to help cover behavior gaps that are not adequately reflected in the final score. In the midst of this loop, LLMs serve to orchestrate which policies are chosen to be evaluated/refined, LLMs summarize the feedback, and LLMs generate prompts for LLMs to write new policies. In particular, the authors claim that adding behavioral evidence (summaries of behavior that are not necessarily quantitative) is a key enabler of their method.\n\nThe MLES system is compared to a prior LLM + Evolutionary Search algorithm, to DQN, and to PPO on the Lunar Lander and Car-Racing gym environments. The scope of experiments demonstrates success on structured and unstructured state spaces, as well as continuous and discrete actions. The results are strong, showing that the MLES system solves tasks as effectively as conventional RL, sometimes more effectively, while maintaining a smaller standard error and yielding a finished policy that a compute scientist can inspect and potentially interpret.\n\nThe appendix is full of useful information, supplementary results, and further ablations of the method. It is clear that significantly care was taken to thoroughly examine the MLES system, and the authors have carefully explained the contributions from each individual component of the method (for example, studying the effects of different exploration operators and multimodal-modification operators). Finished policies (as code) are also included in the supplement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is very clearly explained and the MLES method is generally easy to understand. While there are certain elements that make more sense in theory than in practice, the overall design of the system is clear and the paper is likely reproducible.\n+ The method is benchmarked very thoroughly on two domains, LunarLander and CarRacing, with ablations of different components, comparisons to more conventional deep RL, and comparisons to prior LLM + evolutionary algorithms work.\n+ The understandability of the method is evaluated by real humans, and the final policies generalize well to new test instances of the same task. These are both very strong results and indicate the promise of the method."}, "weaknesses": {"value": "- Novelty: While the paper is interesting and the results are strong, the central ideas are not significantly different from prior work [1], down to the figure similarity. The central innovation in this work is the multimodality and the expression of “behavioral evidence”, although this is somewhat buried in the presentation of the entire system as a novel contribution. In reality, most of the system is ported from prior work, and the core novelty and contribution here seems to be on the VLM descriptions of failures and qualitative feedback to update the model’s heuristics.\n- Generalizability: MLES shows strong results on train and test environments, but it is not clear how effectively this method would generalize without (1) strong scaffolding code or heuristics, and (2) structured state data. While the CarRacing experiment does use unstructured data, the method also seems to rely on having semantic state information (such as speed) injected into the policy. \n- Exploration diversity: MLES makes a few design decisions that seem to bias it very strongly towards iterative refinement of a single strong heuristic, rather than a more conventional exploration into potentially highly negative rewards. While this may be desirable for safety-critical applications, it may be undesirable for discovering new behaviors.\n\n[1] Fei Liu, Tong Xialiang, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. In Forty-first International Conference on Machine Learning, 2024a."}, "questions": {"value": "* How does parent selection affect the method? Currently, parents are sampled according to their quantitative results via a method that biases very heavily towards the high-performers. Conventionally, genetic algorithms might consider a much broader sample of parents (such as the top 50%). Does a wider sampling method derail the approach? In other words, is there really only 1 viable strategy that MLES just iteratively refining?\n* How does E2 (the “generalize across both parent control policies” prompt) handle control policies that cannot be reconciled? If parent A suggests going left around an obstacle, and parent B suggests going right, there is no blending of these two policies.\n* Behavioral evidence is necessary, but policies are ranked on quantitative metrics. Is there no way to incorporate behavioral feedback into the ranking?\n* Is the compute comparison with PPO and DQN fair, given that you get 10K * 16 (rollouts) for the MLES method? \n* Low standard error suggests that there may not be much variability in the policies that are being tested or explored. Is this the case? Is the low standard error a reflection of the fact that MLES effectively iterates on the same couple of heuristics?\n* How well can MLES recover from a bad local optimum? For example, if the Lunar Lander was seeded with 1 heuristic that stays aloft the entire episode and 1 heuristic that attempts to land but crashes, it seems possible that the system would iteratively refine the \"stay aloft\" policy indefinitely, perhaps never learning to land because of the strong negative quantitative penalty to failing a landing (therefore leading that attempted landing policy to not be sampled for iterative refinement)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nL8garSayF", "forum": "OHFNJoNtjW", "replyto": "OHFNJoNtjW", "signatures": ["ICLR.cc/2026/Conference/Submission17263/Reviewer_EJft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17263/Reviewer_EJft"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761165897397, "cdate": 1761165897397, "tmdate": 1762927212585, "mdate": 1762927212585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the MLES (Multimodal LLM-assisted Evolutionary Search) framework, which uses Multimodal Large Language Models (MLLMs) combined with Evolutionary Computation (EC) to automatically discover programmatic control policies. Unlike the \"black-box\" policies of traditional DRL, MLES generates interpretable Python code. Its core innovation is integrating multimodal Behavioral Evidence (BE), especially visual feedback (e.g., images, trajectory maps), into the evolutionary loop. MLLMs analyze this BE alongside quantitative metrics (like reward scores) to identify failure patterns and guide policy improvements . Policies are represented as a combination of executable Code and natural language \"Thought\". Experiments on Lunar Lander and Car Racing show MLES generates transparent policies with performance comparable to the strong DRL baseline, PPO and DQN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Figure 5 is nice, and it would to nice have more like this in the appendix. Papers discussing programmatic policies usually only present they final intrepretable policies, but the procedure of discovering is missing. I am happy to see such a procedure with the help pf evoluationary search. \n2. Ablation study is provided, this is nice. This is especially helpful for a work which invoke MLLM with different prompts (E1, E2, M1_M, M2_M).\n3. The final policies, as well as the searching process are intepretable, yet the performance is comparable with agents trained in DQN or PPO."}, "weaknesses": {"value": "1. Usually this kind of framework is hard to generalize to other domains of tasks. This typically involve composing template carefully. The initial code template is good, but this also inject to much inductive bias. \n2. The framework is highly dependent on MLLM's capabilities. Did you perform any ablation study that how the choices and settings of MLLMs could affect the overall performance?\n3. Lunar Lander and Car Racing are important RL tasks, yet they are too simple. Did you ever apply this framework on other complex long-horizon RL tasks, even with image observation spaces? It would be very interesting if this work can be applied to neurosymbolic settings (i.e., utilizing higher-level programs to control lower-level policies). Such as the environment discussed in [1] and many others.\n\n[1] Qiu and Zhu, Programmatic Reinforcement Learning without Oracles, ICLR 2022"}, "questions": {"value": "1. See weaknesses 2.\n2. See weaknesses 3.\n3. MLLM API call could be expensive, could you please give an simple analysis on how many tokens are used in each training, and what are the prices?\n4. Can you elaborate how to constrcut multimodal few-shot prompt?\n5. Can you elaborate how to acquire behaviroal evidence? How many frames of images do you need?\n6. Can you give an example of \"thought\"? It would be nice to see the evolve of \"thoughts\" along with the training. \"Thoughts\" may change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZrAE00MHmF", "forum": "OHFNJoNtjW", "replyto": "OHFNJoNtjW", "signatures": ["ICLR.cc/2026/Conference/Submission17263/Reviewer_qyrE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17263/Reviewer_qyrE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524259821, "cdate": 1761524259821, "tmdate": 1762927212189, "mdate": 1762927212189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The black-box nature of neural networks makes typical reinforcement learning policies difficult for humans to interpret. To combat this challenge, previous works employ programs as policies to increase the interpretability. In this work, the authors propose **Multimodal Large Language Model-assisted Evolutionary Search (MLES)**, which leverages visual signals to refine these programs. They introduce a **Behavior Summarizer** to automatically encode the executing trajectories into MLLM-understandable formats for generating new programs. Experimental results indicate that MLES outperform baseline methods, including PPO and **Evolution of Heuristic (EoH)**."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation for including visual signals to improve the generated programs is intuitive and reasonable, as images often consist of richer information than text alone.\n- Employing **Python** rather than a **Domain-Specific Language (DSL)** improves the method’s generalization capability, making it more adaptable to various domains and tasks."}, "weaknesses": {"value": "- The experiments are conducted in only two environments, which makes the results less convincing.\n- The exclusion of the policy distillation baseline appears insufficiently justified. Although the proposed method is API-based and described as cost-efficient, invoking LLMs around 2000 times is still non-trivial. Distilling a program from a trained policy seems like an approach that could offer a reasonable trade-off between environmental interactions and LLM queries, even if the performance may decline, as the authors state.\n- It seems like the main differences between MLES and the previous work EoH lie in the use of the Behavior summarizer and the corresponding Evolutionary operators M1_M and M2_M, which are not clearly stated in the methodology section. It might be better to introduce EoH in a preliminary section and then specify how the proposed method extends it.\n- The main contribution of this work lies in the use of the Behavior Summarizer, which is not properly introduced in the main paper but only in the appendix.\n- The human evaluation appears somewhat informal; the authors could strengthen it by conducting a more detailed study to examine whether participants’ understanding of the programs truly aligns with their execution outcomes."}, "questions": {"value": "- Do all the evolutionary operators sample programs only once? Given that the temperature is set to 1, sampling multiple programs might introduce sufficient diversity to potentially accelerate the process.\n- In Figure 4, the performance curves of PPO and MLES appear to move in a highly synchronized manner. Did the authors observe any underlying reason for this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vWVZJWS4C1", "forum": "OHFNJoNtjW", "replyto": "OHFNJoNtjW", "signatures": ["ICLR.cc/2026/Conference/Submission17263/Reviewer_G7L3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17263/Reviewer_G7L3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640298451, "cdate": 1761640298451, "tmdate": 1762927211622, "mdate": 1762927211622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MLES, a framework that combines multimodal large-language models (MLLMs) and evolutionary computation to automatically discover *programmatic*, interpretable control policies. Unlike prior work using LLMs for reward shaping or heuristic tuning, MLES directly synthesizes executable code policies, enhanced by visual behavioural evidence that guides refinement.\n\nThe framework iteratively evolves policies represented as code + “thought” (rationale) + quantitative metrics + behavioural evidence, aiming to achieve human-readable, verifiable control logic.\n\nExperiments on two benchmarks (Lunar Lander and Car Racing) show that MLES reaches performance comparable to Proximal Policy Optimization (PPO) while producing transparent and explainable policies. Ablation studies highlight the contribution of visual feedback and multimodal reasoning to the policy-search efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The integration of visual behaviour-feedback into the policy-synthesis pipeline is a genuinely novel contribution, enabling richer signals beyond scalar metrics in MLLM-policy generation.\n- The paper is well-written and clearly laid out, with the framework, methodology, and experiments described in a comprehensible manner.\n- The experiments include a transparent evolution process, showing how policies evolve across generations and how visual evidence influences the search. I personally liked the presentation of Figure 5.\n- The authors provide the final generated programmatic policies (code) for the benchmark tasks, enabling inspection and verification of their results.\n- The approach successfully balances interpretability (through programmatic policies) with competitive performance (comparable to a strong baseline), which is notable."}, "weaknesses": {"value": "- The novelty is limited: the proposed method seems to be a direct modification of existing work EoH by including visual clues. In my opinion, simply adding visual cues does not count as a major contribution—rather, it relies heavily on the intrinsic ability of the MLLM.\n- The experiments are limited and the potential for generalization is questionable. The authors test only on two tasks (Lunar Lander and Car Racing), which are relatively simple and for which simple code-based policies might exist. In more sophisticated scenarios (e.g., inverted-pendulum), it is unclear that the proposed method will still work. Is the pipeline able to start from scratch, or must it be provided with a PID-like physical model so that the LLM finds parameters?"}, "questions": {"value": "1. Could the authors apply their pipeline to tasks like the inverted pendulum and ask LLM for a hard-coded policy, or for finding the optimal parameters of a PID controller? Would the proposed pipeline still succeed in both cases?\n2. How dependent is the method on the quality of the MLLM used? What happens if a smaller or less capable multimodal model is used? How often does the pipeline require manual intervention?\n3. Are there limitations when the environmental dynamics are high-dimensional, noisy, or partially observable? How does the method scale in those settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3OZxGVLikU", "forum": "OHFNJoNtjW", "replyto": "OHFNJoNtjW", "signatures": ["ICLR.cc/2026/Conference/Submission17263/Reviewer_BFU7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17263/Reviewer_BFU7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996152189, "cdate": 1761996152189, "tmdate": 1762927211183, "mdate": 1762927211183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}