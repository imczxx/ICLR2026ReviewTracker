{"id": "t9cOXsdpKg", "number": 18911, "cdate": 1758291922563, "mdate": 1763472277287, "content": {"title": "What Matters in Deep Learning for Time Series Forecasting?", "abstract": "Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. We discuss the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and for the need to focus on the foundational aspects of the forecasting problem when designing neural network architectures. As a step in this direction, we also propose an auxiliary forecasting model card, i.e., a template with a set of fields to characterize existing and new forecasting architectures based on key design choices.", "tldr": "", "keywords": ["time series forecasting", "architecture design", "deep learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fc0fa1b431dd4baa56bac018f3f52077e29a344.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors classifies design choices of time series forecasting models, finding that overlooked details might change the class of forecasting method and have an impact on experiment results. The authors call for future work to use an auxiliary forecasting model card for key design choices. The authors find that, (1) channel hybrid/global impacts model performance; (2) preprocessing would have an impact on time series benchmarkds; (3) no single model outperforms other models, questioning whether temporal model designing are important; (4) show that some spacial model design would give similar results, thus questioning the importance of spacial design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper calls for rethinking the benchmarks of time series forecasting domain, which I recognize is indeed very necessary and very important. \n2. The authors calls for better understanding of architecture's designing space, which might be a method to solve the phenomena that time series forecasting community has been making little progress in the past years."}, "weaknesses": {"value": "1. How are you sure that it's the `model card` rather than the `dataset and benchmarks` that have gone wrong? **Imagine that the CV community are using MNIST rather than CIFAR, ImageNet or other datasets, perhaps researchers could also publish hundreds of papers per year proposing all kinds of CNN/Transformer designs persuing $0.1\\%$ improvement on MNIST**. \"Oh, my method classifies MNIST better than existing sota\". **In that case, you could also do experiments and find \"hey, perhaps using vit is similar as convnets, perhaps swinTF is similar to ViT\"**. In this case, it is **rethinking, reusing, retargeting datasets and benchmarks** that would help with the problem, rather than making some model cards. Actually, very recently there has been work implying that some datasets for TSF might have gone saturated, for example (https://www.arxiv.org/abs/2510.02729; this paper is online Octobor this year and does not count to my down-rating your paper, but it contributes to my argument that perhaps it's the dataset and benchmarks that have gone wrong.). What's your opinion on this?\n2. There have been previous calls for rethinking and utilizing better and more robust time series forecasting and benchmarking. For example, NeurIPS 24 time series in the age of large models workshop Invited Talk by Christoph Bergmeir - Fundamental limitations of foundational forecasting models: The need for multimodality and rigorous evaluation (https://cbergmeir.com/talks/neurips2024/), and also some papers (for example, https://arxiv.org/abs/2502.14045). More recently, some researchers also raise concerns like the time series benchmarks have been saturated. (see weakness 1) **Perhaps the ultimate errors appear in the dataset and benchmarks we are using**, **not in the methods.** Of-course I'm not saying that the methods we propose are fine: saturated datasets and benchmarks might be misleading, resulting in not-that-effective methods. I'm saying that perhaps the dataset issues should be solved first."}, "questions": {"value": "Because a part of this paper seems similar to position paper, I would ask some related questions on behalf of a position paper reviewer, as listed:\n\nActually I agree that the time series forecasting area has gone wrong in the past several years. Perhaps we should stop overfitting those small simple naive datasets. Do you have some suggestions, advices or opinions on these?\n\nI'm looking forward to further discussions, and I'm potentially willing to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B59XIJUZAo", "forum": "t9cOXsdpKg", "replyto": "t9cOXsdpKg", "signatures": ["ICLR.cc/2026/Conference/Submission18911/Reviewer_uehw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18911/Reviewer_uehw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760512071850, "cdate": 1760512071850, "tmdate": 1762930901503, "mdate": 1762930901503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically shows that implementation details (local/global configuration, preprocessing, covariates) have larger impact than architecture choice (Transformer vs MLP) in time series forecasting. Simple baselines match SOTA when properly configured, exposing inconsistent benchmarking practices across recent work.\nContribution: No novel methods—purely empirical analysis exposing benchmarking flaws. Proposes a \"model card\" template to standardize future comparisons."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Timely and important: Addresses fundamental benchmarking issues affecting the entire time series forecasting community.\n- Rigorous empirical work: Comprehensive ablation studies with controlled comparisons across multiple design dimensions.\n- Actionable template: The forecasting model card could standardize future research and improve reproducibility."}, "weaknesses": {"value": "- Given the paper's broad claims about deep learning for time series forecasting, the experimental scope (4 datasets, long-range forecasting only, no probabilistic forecasting) seems insufficient to support such general conclusions.\n- While experienced practitioners may anticipate some findings (e.g., that preprocessing matters), the systematic quantification of these effects is valuable. However, the paper lacks surprising insights that would significantly advance our understanding.\n- The paper is more like a position-track paper, or even a benchmark-track paper, than a main track paper. It lacks theoretical insights, and it only raises issues without providing solutions. (I acknowledge that revealing an important issue is important to the community but the contribution of this paper slightly diverges from what we expect for a main track paper).\n- The usefulness of the proposed 'model card' template is uncertain."}, "questions": {"value": "- How confident are you that these findings generalize to short-term forecasting, probabilistic forecasting, and other domains (e.g., irregular time series, multivariate forecasting with true cross-variable dependencies)?\n- Beyond diagnosing problems and proposing the model card template, what concrete steps do you think should the community take?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3JQeH6ijad", "forum": "t9cOXsdpKg", "replyto": "t9cOXsdpKg", "signatures": ["ICLR.cc/2026/Conference/Submission18911/Reviewer_HBak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18911/Reviewer_HBak"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814882513, "cdate": 1761814882513, "tmdate": 1762930900635, "mdate": 1762930900635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines why deep learning architectures for time series forecasting yield inconsistent and often contradictory results. The authors aim to disentangle the factors influencing model performance and to identify which design elements truly matter in building effective forecasting systems.\n\nThe study employs a computational and experimental methodology, combining systematic benchmarking, empirical analysis, and architectural deconstruction. It introduces a framework for analyzing models along four key design dimensions: 1) Model configuration (local, global, hybrid), 2) Preprocessing and exogenous variables, 3) Temporal processing, and 4) Spatial processing.\n\nThrough controlled experiments on established benchmarks (Electricity, Weather, Traffic, Solar), the authors compare well-known models such as PatchTST, DLinear, TimeMixer, and Crossformer against a streamlined reference architecture designed to isolate the effects of specific design choices. Key findings include: 1) Many observed performance differences stem from overlooked implementation details—not from architectural innovation. 2) Global or hybrid models, when well-designed, can match or outperform complex state-of-the-art systems. 3) Exogenous variable inclusion and consistent preprocessing have a greater effect on performance than model type. 4) Spatial attention mechanisms contribute little to long-horizon forecasting accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper offers a meta-analytical and diagnostic contribution rather than a new predictive model. Its novelty lies in articulating a unified conceptual framework for analyzing deep time series forecasting architectures and demonstrating that benchmarking inconsistencies, not model innovation, explain many reported performance gains. The introduction of a forecasting model card is a valuable proposal for standardizing model documentation, enhancing reproducibility and interpretability across studies."}, "weaknesses": {"value": "- While comprehensive, the study focuses solely on deterministic point forecasting. This leaves out probabilistic and uncertainty-aware approaches, which are central to modern time series applications. The authors acknowledge this but could have elaborated on how their findings generalize to probabilistic settings.\n\n- Although the paper references major forecasting works, it under-engages with recent multimodal and foundation time series models (e.g., TFT, Chronos, pretrained time-series transformers) that might challenge or nuance its conclusions about architecture complexity.\n\n- Most comparisons are run with almost the same look-back window (W) and forecast horizon (H). In fact, they use W=96 for most tables (one table uses W=336, Solar excluded) and H=96 almost always. If we widen the settings (e.g., W=336–720; H=192–336), the relative strengths of architectures can change, so current conclusions may be setting-dependent.\n\n- For spatial models, the authors explicitly shrink W to 96 “to keep costs manageable,” and then conclude spatial attention adds limited value. But some cross-series patterns can only emerge with longer windows/horizons; the constraint itself may handicap spatial operators."}, "questions": {"value": "- If we sweep W ∈ {96, 336, 720} and H ∈ {96, 192, 336}, do the two core claims still hold: (i) simple models match SOTA, and (ii) spatial attention helps little? Where do rankings flip as context grows? (This matters because current runs mostly fix W=96 and H=96.)\n\n- Under probabilistic evaluation, do simpler models still lead? If not, the paper’s guidance should be framed as point-forecast-specific.\n\n\n- Model configuration (Local, Global, Hybrid) represents a core design choice that directly affects model behavior and capacity. Why do the authors argue that configuration effects should be factored out rather than treated as part of each model’s intended design?\nWould controlling for configuration risk removing meaningful aspects of a model’s inductive bias and thereby alter its intended behavior?\n\n- Table 1 compares models under Hybrid and Global setups, yet the paper does not clearly explain how each version was implemented.\nWhat exactly was modified in each model to create the “Global” or “Hybrid” configuration (e.g., were per-series normalization parameters or embeddings added/removed)? Providing this procedural detail would make the comparison reproducible and easier to interpret.\n\n- Is the proposed way of constructing a Hybrid model (shared parameters plus per-series components) a general framework that can be applied to any architecture, or is it specific to certain models like TimeMixer and Crossformer?\nClarifying this would help readers understand whether Hybrid configuration is a standardized recipe or an ad-hoc adjustment.\n\n- Could similar experiments be performed under a Local configuration (one model per time series) for at least a subset of the deep models?\nIf not, could the authors discuss the practical or computational reasons preventing this?\nSuch results would help establish a complete Local–Global–Hybrid comparison.\n\n- Linear baselines (e.g., Linear, DLinear) can in principle be trained under different configurations.\nIs it feasible to evaluate these models under Hybrid or Global settings as well?\nIncluding these variants could strengthen the paper’s conclusions by showing whether configuration effects are consistent across both deep and linear models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTcWDCw15M", "forum": "t9cOXsdpKg", "replyto": "t9cOXsdpKg", "signatures": ["ICLR.cc/2026/Conference/Submission18911/Reviewer_eXVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18911/Reviewer_eXVU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077349748, "cdate": 1762077349748, "tmdate": 1762930900051, "mdate": 1762930900051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New experiments and changelog:"}, "comment": {"value": "To address the reviewers’ comments and provide a more comprehensive evaluation, we have added to the paper the following additional results:\n\n## Main Paper\n- **Table 1** (**Section 4.1** – 'Design Dimension 1: model configuration’): added **iTransformer** to compare models with and without local parameters.\n- **Table 2** (**Section 4.2** – 'Design Dimension 2: preprocessing and exogenous variables’): Added **iTransformer** and **Crossformer** to compare models with and without covariates.\n\n## Appendix (new Section E: Additional results)\n\nWe added an extensive set of new experiments to broaden the empirical analysis:\n- **Multi-horizon results**: we added **Table 14** to extend the results in Table 3 (Section 4.3 – 'Design Dimension 3: temporal processing’) to longer horizons (96, 192, 336, 720).\n- **Extended ablation on iTransformer**: we added **Tables 15, 16, and 17** to expand the original ablation in Table 4.b (Section 4.4 – 'Design Dimension 4: spatial processing’) to additional forecasting **horizons** (96, 192, 336, 720) and **window** sizes (96, 336).\n- **Linear autoregressive models**: we added **Table 18** to compare **local, global, and hybrid** variants of Linear and DLinear.\n\nThe additional results further support the main claims of the paper and **reinforce our analysis** of Sections 4.3 and 4.4. Results on the linear baselines are also **consistent** with the findings discussed in Section 4.1.\n\n\n**We are currently running additional experiments** to further broaden the scope of the study. The manuscript will be updated with these results before the end of the rebuttal period, and the reviewers will be notified promptly as soon as they are available."}}, "id": "KksP46w9Pp", "forum": "t9cOXsdpKg", "replyto": "t9cOXsdpKg", "signatures": ["ICLR.cc/2026/Conference/Submission18911/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18911/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18911/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763475309860, "cdate": 1763475309860, "tmdate": 1763475309860, "mdate": 1763475309860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}