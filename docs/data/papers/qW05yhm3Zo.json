{"id": "qW05yhm3Zo", "number": 4440, "cdate": 1757681378633, "mdate": 1763471570172, "content": {"title": "Why is Your Language Model a Poor Implicit Reward Model?", "abstract": "Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Toward a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Overall, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.", "tldr": "", "keywords": ["Reward Models", "Language Models", "Generalization", "Distribution Shifts"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fa36938621c0277f359b910aea5fc8b5f9e0081.pdf", "supplementary_material": "/attachment/e1b58d615a3239daf3171a5665b4aaecbe67ddca.zip"}, "replies": [{"content": {"summary": {"value": "**Disclaimer**: I am not an expert in theoretical analysis, so my evaluation of the technical proofs should be taken with limited weight. My review primarily reflects my assessment of the paper’s motivation, empirical validation, and broader relevance to the reward modeling community.\n\n**Summary**:  \nThis paper provides a timely and insightful investigation into the generalization gap between explicit reward models (EX-RMs)—typically linear heads on top of frozen or fine-tuned LMs—and implicit reward models (IM-RMs), as used in DPO. Through a combination of theoretical analysis and carefully designed experiments, the authors convincingly argue that IM-RMs suffer from over-reliance on superficial token-level cues, which harms their robustness under token-level distribution shifts (e.g., paraphrasing or translation). In contrast, EX-RMs generalize better in such settings by leveraging semantic structure encoded in hidden representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practically important and currently underexplored question: *why* do EX-RMs often outperform IM-RMs despite near-identical training setups? The dual theoretical–empirical approach strengthens the validity of the conclusions.  \n2. The authors effectively refute a commonly held hypothesis—the “generation-verification gap”—by both proving that verification does not require strong generation capability and demonstrating this empirically on a Hamiltonian cycle task. This clarification is valuable for the field’s understanding of IM-RM limitations."}, "weaknesses": {"value": "While the focus on EX-RM vs. IM-RM is well-motivated, I would have appreciated some discussion or preliminary results involving **generative reward models (GenRMs)**—particularly those that instruct variant or thinking model variant. \nRecent empirical work suggests such GenRMs can be more robust to reward hacking and exhibit better out-of-distribution generalization than both scalar EX-RMs and DPO-style IM-RMs. Extending the analysis to this emerging class of reward models would significantly broaden the paper’s impact."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Aa5XPjSCRH", "forum": "qW05yhm3Zo", "replyto": "qW05yhm3Zo", "signatures": ["ICLR.cc/2026/Conference/Submission4440/Reviewer_zVNe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4440/Reviewer_zVNe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760975051905, "cdate": 1760975051905, "tmdate": 1762917367128, "mdate": 1762917367128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores reward models, specifically how implicit reward models generalize worse than explicit reward models. The authors present a theoretical analysis highlighting that implicit reward models rely more heavily on superficial token-level queues, which makes them less robust to token level distribution shifts.\n\nThe contributions of the work are as follows. First, the authors present a theoretical formulation of the gap between EX-RMs and IM-RMs. This is then empirically supported by a set of experiments. These experiments show that IM-RMs fail to generalize to paraphrased responses, and that IMs are less robust than EX-RMs to token-level distribution shifts. In real-world experiments, they train both EX-RMs and IM-RMs on a series of datasets in reasoning and general task domains.\n\nThe strengths of the work are as follows. First, the problem is relevant and training strong reward models are important for LLM training. Second, there is a strong theoretical component, showing that EX-RMs and IMs have a gap. Third, there is a strong empirical second, showing an identification of the root cause.\n\nThe weaknesses of the work are as follows. First, there is not a downstream performance analysis, and reward model performance does not always correlate with downstream performance. The work could benefit from a stronger empirical analysis that includes downstream model performance, although this is computationally challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the work are as follows.\n- First, the problem is relevant and training strong reward models are important for LLM training.\n- Second, there is a strong theoretical component, showing that EX-RMs and IMs have a gap.\n- Third, there is a strong empirical second, showing an identification of the root cause."}, "weaknesses": {"value": "The weaknesses of the work are as follows. First, there is not a downstream performance analysis, and reward model performance does not always correlate with downstream performance. The work could benefit from a stronger empirical analysis that includes downstream model performance, although this is computationally challenging."}, "questions": {"value": "Are there any concrete recommendations for when EX-RMs or IM-RMs should be used? Are there hybrid approaches?\nAre there any evaluation metrics considered beyond accuracy?\nWill code and trained models be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FAExLGlI6x", "forum": "qW05yhm3Zo", "replyto": "qW05yhm3Zo", "signatures": ["ICLR.cc/2026/Conference/Submission4440/Reviewer_haqk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4440/Reviewer_haqk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859553828, "cdate": 1761859553828, "tmdate": 1762917366890, "mdate": 1762917366890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the generalization gap between EX-RMs and IM-RMs, two nearly identical reward model architectures. This work's main contribution is identifying that IM-RMs rely more heavily on superficial, token-level cues, whereas EX-RMs leverage semantic information encoded in hidden representations. Through theoretical analysis of their learning dynamics and extensive empirical evaluation, the paper demonstrates that this reliance causes IM-RMs to perform poorly under token-level distribution shifts. The work also provides evidence against the alternative hypothesis that IM-RMs struggle due to a \"generation-verification gap.\" The experiments show that while EX-RMs are more robust to token-level shifts, IM-RMs can perform comparably or better under larger domain shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a well-defined and important problem: understanding the performance discrepancy between EX-RMs and IM-RMs, which are structurally very similar yet exhibit different generalization behaviors.\n- The paper effectively challenges the \"generation-verification gap\" hypothesis. It proves theoretically that verification with an IM-RM does not require generation and demonstrates this empirically on a synthetic Hamiltonian cycle task.\n- The paper’s claims are substantiated by a comprehensive set of experiments that systematically test the core hypothesis under various conditions."}, "weaknesses": {"value": "- The primary theoretical analysis in Section 4 and Appendix B relies on simplifying assumptions that are violated in the main experiments, potentially limiting the direct applicability of the theory.  1) Assumption 1 posits that hidden representations are fixed during training. However, the empirical results are generated by training all reward model parameters. The paper notes that its conclusions still hold empirically, but does not fully bridge the gap to explain why the dynamics under the simplifying assumption are still predictive. 2) The theoretical result in Theorem 2 is derived under the highly restrictive case of single-token responses. This is far from the practical scenario of long-form text generation.\n- While the mathematical analysis is a core strength, the presentation of some key concepts could be improved for better readability and intuition. For example, the coefficient ρ_k,l(v) in Equation (5)is central to the argument about IM-RMs' token-level dependency. A more intuitive explanation in the main text of how token identity (the indicator function) and distributional similarity (the inner product of next-token distributions) contribute to its value would be beneficial."}, "questions": {"value": "- Could this work elaborate on the intuition for why the theoretical conclusions derived under the fixed hidden representation assumption still seem to hold empirically when all parameters are fine-tuned?  Is it because the updates to the backbone are minimal? Or add a comparison experiment with a fixed backbone?\n- The results in Figures 2 and 5 consistently show IM-RMs performing well, and sometimes better than EX-RMs, under domain shifts. Does this work have a hypothesis to explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cvWDxxFvwl", "forum": "qW05yhm3Zo", "replyto": "qW05yhm3Zo", "signatures": ["ICLR.cc/2026/Conference/Submission4440/Reviewer_fVe7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4440/Reviewer_fVe7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939752129, "cdate": 1761939752129, "tmdate": 1762917366677, "mdate": 1762917366677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contrasts implicit reward models (IM-RMs; rewards via log-likelihood ratios) with explicit reward models (EX-RMs; linear head over hidden states) and asks why IM-RMs underperform despite near-identical training data and losses. The core claim is that IM-RMs lean on surface token cues, so they break under token-level distribution shifts (paraphrases/translations) even when they look fine in-distribution; meanwhile they can be competitive under domain shifts. The authors (i) prove that an IM-RM can verify without being able to generate correct responses (so the gen-vs-verify gap is not the culprit), (ii) analyze learning dynamics to show explicit token dependence in IM-RMs, and (iii) back this up with controlled Persona paraphrase tests and broader UltraFeedback/RewardMATH/RewardBench experiments across 1B–8B backbones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Sharp negative result against a popular hypothesis: verification ≠ generation for IM-RMs; the Hamiltonian task illustrates this cleanly.\n\nMechanistic story that matches data. The gradient-level analysis predicts exactly where IM-RMs fail (paraphrases/translations), and the controlled Persona setup nails the failure mode.\n\nBreadth and consistency. Multiple model families (1B–8B) and both general-chat and math settings; token-shift brittleness shows up systematically.\n\nPractical takeaway. If your evals include paraphrases or multilingual restatements, EX-RMs are the safer default; IM-RMs can be fine under domain shift but need help on surface variation."}, "weaknesses": {"value": "Paraphrase pipeline dependence. Robustness claims rest heavily on how paraphrases/translations were produced. I’d like BLEU/chrF ranges, style diversity checks, and a sanity control where paraphrases are fed back through the teacher to confirm semantic equivalence.\n\nLimited exploration of mitigations. The paper diagnoses token-level sensitivity but doesn’t dig into cheap fixes (representation freezing, unembedding regularization, token-dropout on reward paths, contrastive paraphrase augmentation).\n\nCalibration/uncertainty left on the table. If IM-RMs rely on surface cues, do they also become miscalibrated on paraphrases (reward margin vs. accuracy)? A short calibration slice (ECE/Brier) would make the case stronger.\n\nHead-vs-backbone entanglement. Theory assumes fixed representations; experiments train full models but don’t isolate the contribution of updating the unembedding vs. earlier layers."}, "questions": {"value": "Paraphrase hygiene. How do you ensure paraphrases preserve label semantics (beyond model intuition)? Any human spot-checks or automatic entailment filters, and what are their pass rates?\n\nCheap mitigations. Did you try (a) paraphrase-contrastive pairs during IM-RM training, (b) freezing the unembedding or adding token-dropout on the reward path, or (c) sharing a representation head but decoupling logits for reward vs. generation?\n\nReward margin & RL. You note EX-RMs yield larger normalized reward margins. Can you show an RL-phase consequence (e.g., better PPO/DPO-RFT optimization curves) on the same base policy to quantify end-to-end impact?\n\nDecoding sensitivity. Do IM-RM failures persist when evaluation decoding changes (nucleus vs. greedy; different temps) or when rewards are computed over masked tokens only (e.g., function words stripped)?\n\nCross-lingual asymmetry. In translation shifts, are failures symmetric (EN→FR vs FR→EN)? Any sign that subword overlap (BPE sharing) modulates the IM-RM gap?\n\nWhen would you still pick IM-RM? Beyond domain-shift scenarios in your figures, are there resource or deployment regimes (e.g., no extra head, log-prob reuse for TTS) where IM-RM’s simplicity outweighs its fragility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x5Rg6CZw5z", "forum": "qW05yhm3Zo", "replyto": "qW05yhm3Zo", "signatures": ["ICLR.cc/2026/Conference/Submission4440/Reviewer_D62L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4440/Reviewer_D62L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952139377, "cdate": 1761952139377, "tmdate": 1762917365144, "mdate": 1762917365144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}