{"id": "yiNVIcAfg9", "number": 20356, "cdate": 1758305037099, "mdate": 1763604440750, "content": {"title": "Utilizing Evolution Strategies to Train Transformers in Reinforcement Learning", "abstract": "We explore the capability of evolution strategies to train an agent with a policy based on a transformer architecture in a reinforcement learning setting. We performed experiments using OpenAI’s highly parallelizable evolution strategy to train Decision Transformer in the MuJoCo Humanoid locomotion environment and in the environment of Atari games, testing the ability of this black-box optimization technique to train even such relatively large and complicated models (compared to those previously tested in the literature). The examined evolution strategy proved to be, in general, capable of achieving strong results and managed to produce high-performing agents, showcasing evolution’s ability to tackle the training of even such complex models.", "tldr": "Exploring whether the evolution strategies are able to train transformer-based agents in reinforcement learning.", "keywords": ["Evolution strategies", "Transformers", "Policy optimization", "Reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/526afa298a304d4e9f93c75e0c29021dc923dbb5.pdf", "supplementary_material": "/attachment/1145c2eba9de886b7a28fb4b53cc75c297b15089.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores the use of OpenAI’s evolution strategies to train transformer-based reinforcement learning agents, specifically the Decision Transformer, in environments like MuJoCo Humanoid and Atari games. The study demonstrates that even simple evolution strategies can successfully train large and complex transformer models, though at a high computational cost. Gradient-based pretraining was tested but found to hinder rather than help performance. Overall, the work shows that evolution strategies can scale to transformer architectures, offering a robust and highly parallelizable alternative to gradient-based reinforcement learning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The key strength of this paper lies in its demonstration that evolution strategies, despite being simple and gradient-free, can effectively train large transformer-based reinforcement learning models like the Decision Transformer. This highlights the scalability, robustness, and strong parallelization potential of evolution strategies, extending their applicability to more complex neural architectures previously dominated by gradient-based methods."}, "weaknesses": {"value": "While the paper is clearly written and experimentally careful, its novelty is quite limited. The main claim that evolution strategies can train transformer-based reinforcement learning agents is not fundamentally new. Evolution strategies have already been shown to scale effectively to large, high-dimensional models, including transformer architectures, across various domains such as natural language processing, neural architecture search, and dynamic scheduling. The present work simply extends this observation to benchmark control problems like MuJoCo and Atari, without introducing any methodological advancement or conceptual insight beyond applying existing tools to a new but predictable setting.\n\nThe section on gradient-based pretraining provides little additional contribution. The authors attempt to combine supervised pretraining with evolutionary optimization and report that pretraining hinders rather than helps performance. However, the incompatibility between gradient-trained and evolution-trained models is a well-established finding in evolutionary computation and hybrid learning literature. The paper merely reaffirms this known phenomenon without introducing a new explanation, theoretical framework, or practical remedy that would deepen understanding of how these two paradigms might be effectively integrated.\n\nFrom an experimental perspective, the scope remains narrow and lacks diversity. The study tests only two standard environments, i.e., MuJoCo Humanoid and a single Atari game (Hero), which limits the generality of the conclusions. These environments, while computationally demanding, may not provide sufficiently compelling evidence that ES can outperform or even compete with modern gradient-based methods. The comparisons with baselines are weak; gradient-based methods were only minimally tuned, and the Decision Transformer’s gradient training was deemed “non-functional” without deeper investigation into stabilization techniques. As such, the experiments are more illustrative than conclusive.\n\nUltimately, the overall contribution of the paper is incremental. It offers an implementation and verification that ES can train transformer policies on classical control benchmarks, but this is an expected result given prior work showing ES success on transformers in other  complex domains. The paper does not deliver new algorithmic insights, hybrid methods, or theoretical understanding that could significantly advance the field. Consequently, while it may have some engineering value as a proof of concept, it falls short of making a substantive scientific contribution to reinforcement learning or evolutionary optimization research."}, "questions": {"value": "1. How does this work advance beyond prior studies that have already applied evolution strategies to transformer architectures in other domains?\n\n2. Why were only two standard control benchmarks (MuJoCo Humanoid and Atari Hero) used, and how do these limited experiments support broader claims about scalability or generality?\n\n3. Given that the incompatibility between gradient-based pretraining and evolutionary optimization is well-known, what new insights or mechanisms does this paper contribute to understanding or addressing this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V3ye79qVDp", "forum": "yiNVIcAfg9", "replyto": "yiNVIcAfg9", "signatures": ["ICLR.cc/2026/Conference/Submission20356/Reviewer_UVDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20356/Reviewer_UVDc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760561606513, "cdate": 1760561606513, "tmdate": 1762933811695, "mdate": 1762933811695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper covers an interesting idea of using evolutionary algorithms to train transformer-based RL agents. Using OpenAI-ES, the algorithm was tested on various Atari and a Mujoco environment, showing improved performance over TD3. The paper also includes an interesting discussion and insights on the strengths and weakness of evolution strategies and their possible future implications in the field of RL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper covers an interesting question of using evolutionary strategies to train transformer architectures in an RL setup. Given the recent interest in transformer architecture, whether in decision making (Decision Transformer) or reasoning (LLMs and Reasoning Models), this sheds lights into an interesting research direction. This paper also includes some promising results and insights, providing a useful starting point for future research into evolutionary strategies for transformer architectures.\n\nThe paper is also very well organized. It was quite easy and enjoyable to read the paper."}, "weaknesses": {"value": "While this paper offers a promising vantage point into future research, I think there are several improvements that can be made to offer more to the RL community.\n\nFirst, while the experiment only includes a dotted-line comparison with TD3, I think it would be better to include a more though comparison with more RL algorithms (PPO, SAC, etc) as well as their resource usage plots. While evolutionary strategies often consume a lot more computational resource than more traditional RL algorithms, many of their process can easily be parallelized and with the advent of JAX, this approach is much more accessible. Therefore, a comparison with existing RL algorithm in terms of computational resources used and time elapsed would present a more useful information for future researchers whether genetic algorithms would be applicable to their research topics. \n\nRelated to above, including tasks other than Mujoco and Atari would be more useful. Various RL tasks, from autonomous driving to GRPO used for LLMs, has their own unique challenges and purposes. Providing a more detailed understanding on where the evolutionary strategy works and have bonus over existing RL algorithms would provide a useful markers for future researchers whether they should use an evolutionary strategy or not.\n\nThe fact that pretraining does not help seems like a critical issue, as there are some tasks where the dataset is not big enough or the model is too big that it would be impossible to train the agent without pretraining on a similar and more simple set of tasks. Therefore, a comprehensive experiments on where and why pretraining works or not works would be useful.\n\nWhile the discussions were very clear, honest, and shared many useful insights about the approach, there were some cases where the claims were not well supported and simply marked 'We think'. While such clarity is certainly welcomed, I think it would be nice to conduct some ablation tasks to validate theories presented in the discussions."}, "questions": {"value": "Overall, this is a promising research and for me, the biggest factors for improving score would be\n\n1) comparison in resource usage for genetic strategies compared to regular RL algorithms\n2) more ablation studies to support the thoughts and claims made in the discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0zV1JukK8W", "forum": "yiNVIcAfg9", "replyto": "yiNVIcAfg9", "signatures": ["ICLR.cc/2026/Conference/Submission20356/Reviewer_7Pvx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20356/Reviewer_7Pvx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489393989, "cdate": 1761489393989, "tmdate": 1762933811207, "mdate": 1762933811207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether Evolution Strategies (ES) can train transformer-based RL agents such as Decision Transformers. The authors test this on MuJoCo Humanoid and the Atari game HERO. They also try training a model after behavior-cloning pretraining. The main claim is that ES can scale to transformer models and that ES-trained policies tend to become robust during training."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very relevant.\n\nThe motivation is clear. The authors want to understand if ES can handle modern, large RL architectures. This is timely and relevant.\n\nThe experimental setup shows solid engineering effort. They re-implemented OpenAI-ES and carefully described the main design decisions.\n\nThe paper presents an interesting observation. ES initially weakens a pretrained model before improving it. The authors argue that ES first improves robustness in parameter space. This is a useful insight for hybrid ES + gradient training.\n\nThe results show that ES can train a Decision Transformer from scratch on a continuous-control task. This is non-trivial and demonstrates feasibility."}, "weaknesses": {"value": "The contribution feels more like a feasibility study than a new method or strong theoretical insight. The paper could benefit from clearer framing about what new knowledge is gained beyond “ES works on transformers.”\n\nThe baseline using TD3 with a Decision-Transformer-style model is not strong. It mostly shows that standard RL fails here. Stronger or more relevant baselines (e.g., modern online sequence models or RvS approaches) would help.\n\nThe return-to-go signal essentially gets ignored when using ES. The paper mentions this but does not analyze the reason deeply. Some diagnostic experiments or further discussion would strengthen the argument.\n\nAtari results only include one game (HERO). More environments would make the general claims more convincing.\n\nES is known to be compute-heavy. The paper shows runtime, but there is little discussion of compute vs. performance or sample efficiency. This would help place the results in context."}, "questions": {"value": "Can you provide results for more Atari games? There has, after all, been more than a month since you submitted your paper.\n\nCan you provide more info on sample efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BH3Y8sT4bN", "forum": "yiNVIcAfg9", "replyto": "yiNVIcAfg9", "signatures": ["ICLR.cc/2026/Conference/Submission20356/Reviewer_RjaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20356/Reviewer_RjaQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937818621, "cdate": 1761937818621, "tmdate": 1762933810856, "mdate": 1762933810856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}