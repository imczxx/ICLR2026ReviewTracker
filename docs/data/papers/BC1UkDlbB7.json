{"id": "BC1UkDlbB7", "number": 14429, "cdate": 1758235181367, "mdate": 1759897370784, "content": {"title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "abstract": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.", "tldr": "We train a multilingual modern encoder on 3T tokens, outperforming all previous encoder models while having open-data and even beating Gemini and o3 in classification.", "keywords": ["pretraining", "encoders", "retrieval", "multilingual", "classification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c923a6ca235f1b815f8562e79aa6bf0a7768ac1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new multilingual encoder-only language model called mmBERT. The authors train it with a novel annealing method that increases the uniformity of language sampling throughout training -- this should help the model to support over 1800 languages without overfitting on the low-resource ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper comes with the release of two language models that might be very useful for NLP practitioners around the world. This is a great strength of this paper, especially considering the cost of training those models on 3T tokens. The performance of these models seems very good and they should be very efficient to run thanks to the ModernBERT architecture they are based on."}, "weaknesses": {"value": "My main issue is that the paper has no real scientific contributions. The paper mentions 3 pretraining contributions: *\"(1) an inverse mask schedule learning rate (high ⇒low), (2) an annealing language schedule (more biased ⇒more uniform), and (3) increasing the number of languages at each training phase (60→110→1833), allowing for maximal impact of the smaller amount of data.\"* -- but their relevance is greatly overstated:\n1) The first contribution is a large problem: This paper plagiarizes \"Dynamic Masking Rate Schedules for MLM Pretraining\" [1]. That paper introduces exactly the same method as advertised here; as with exactly the same hyperparameters, they decay the masking rate from 30% to 15%. Yet, this paper is never cited here and the method is even presented as a novel contribution of this paper.\n2) The second contribution sounds interesting and makes intuitive sense, however it is never empirically evaluated in this paper. It might be possible that the observed performance improvements come from the high-quality data or improved transformer architecture, not from the \"annealing language schedule\". The Result's subsection 4.4 \"Annealing language learning\" seems evaluating this method, but in fact it only checks whether adding new languages at the of training improves the performance on these languages -- which it obviously does when compared to a baseline that has never seen those languages. But this is not a proper evaluation of the contribution 2); the baseline used in previous work is to have constant sampling throughout the whole training, the proposed method should be compared to that.\n3) The third contribution is actually the same as the second one, it just means that the language schedule start at 0 for some languages.\n\nOther than that, the contribution is the release of mmBERT, which I noted as a clear strength, but that alone does not bring any new scientific finding.\n\n**Some other mistakes and issues:**\n- The paper completely ignores the existence of mDeBERTa (coincidentally an ICLR paper) [2] and claims that \"XLM-R is still SOTA\" without any support. It is not true in practice as well as according to [2]. The bigger issue though is that mDeBERTa is missing from the evaluation, which might exaggerate the performance of the model proposed in this paper.\n- There has been important work in multilingual language modeling that is completely ignored by the authors, in particular the UniMax sampling of languages used for training the umT5 models [3], as well as the modular multilingual mmT5 [4].\n- *\"encoder-only language models cannot generate text\"* (line 31) -- that's clearly not true, as demonstrated by [5, 6] and as demonstrated by the countless recently popular masked diffusion language models.\n- *\"However, data quality has significantly increased since 2019, allowing us to achieve higher scores with only half of the tokens (Penedo et al., 2024).\"* (line 96) -- this claim is true for causal-only models, not for encoder-only models (used in this paper) that have a completely different training behavior.\n- Saying *\"we achieve similar classification performance to models like OpenAI’s o3 and Google’s Gemini 2.5 Pro\"* is a clear overstatement over-selling this work. It is a well-known fact that finetuning BERT-like models on in-domain data can outperform LLMs evaluated zero-shot. It is not a special property of this particular model and it is not a fair comparison.\n\n____\n\n**References:**\n- [1] Dynamic Masking Rate Schedules for MLM Pretraining -- https://aclanthology.org/2024.eacl-short.42/\n- [2] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n -- https://openreview.net/forum?id=sE7-XhLxHA\n- [3] UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining -- https://openreview.net/forum?id=kXwdL1cWOAi\n- [4] mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations -- https://aclanthology.org/2023.findings-emnlp.132/\n- [5] BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model -- https://aclanthology.org/W19-2304/\n- [6] BERTs are Generative In-Context Learners: https://proceedings.neurips.cc/paper_files/paper/2024/hash/04ea184dfb5f1babb78c093e850a83f9-Abstract-Conference.html"}, "questions": {"value": "- Why did you choose to use the tokenizer from Gemma 2? Wouldn't it be more appropriate to train a new tokenizer on the specific distribution of languages you're using? This way, the tokenization (and thus the whole model) might be very inefficient for language underrepresented in the Gemma 2 tokenizer (it's unclear how many languages are supported by Gemma 2, but the subsequent Gemma 3 only supports 140+ languages, an order of magnitude less than mmBERT). This seems to defeat the main purpose of your multilingual language model.\n- You have included a substantial amount of programming code in the data mixture, is it treated as \"English\" language, as \"code\" language, or in some other way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "KV6eKQwdJ7", "forum": "BC1UkDlbB7", "replyto": "BC1UkDlbB7", "signatures": ["ICLR.cc/2026/Conference/Submission14429/Reviewer_QX8Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14429/Reviewer_QX8Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861833078, "cdate": 1761861833078, "tmdate": 1762924836722, "mdate": 1762924836722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces mmBERT, a modern multilingual encoder-only language model trained on 3T tokens across 1800+ languages. The core contributions are: In terms of specific training mechanisms the authors use: inverse mask ratio schedule (30% → 15% → 5%) and a cascading annealed language learning to progressively add languages during learning. Finally, model merging across different decay phase variants is used to improve final model. In experiments, mmBERT demonstrates competitive performance on benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The annealed language learning is well-motivated and seems to be useful. In general, the large amount of language covered is a nice strength of the paper.\nmmBERT shows trong empirical results across diverse benchmarks\nOverall, this is a meaningful contribution to an underserved area (multilingual encoders)"}, "weaknesses": {"value": "The authors acknowledge they couldn't ablate inverse masking due to compute, but this is an interesting contribution that could be tested more directly.\n\nOnly 2 languages are tested for demonstrating the benefit of the annealed language learning contribution. In general, with many languages covered it is difficult to assess whether the encoder really does a decent job on all languages and the evaluation over the long tail of languages is very superficial.\n\nThe tables of results do not report variability making it impossible to assess when and whether improvements are statistically significant. This would be important for rigorous science to at least report some notion of the variability (std, confidence intervals)"}, "questions": {"value": "How were the hyperparameters choosen, especially for masking ratio and ALL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DZfgOYbRit", "forum": "BC1UkDlbB7", "replyto": "BC1UkDlbB7", "signatures": ["ICLR.cc/2026/Conference/Submission14429/Reviewer_Vo7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14429/Reviewer_Vo7i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912679785, "cdate": 1761912679785, "tmdate": 1762924836277, "mdate": 1762924836277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents mmBERT, an encoder-only language model trained on 3T tokens across over 1800 languages, introducing training strategies such as an inverse mask ratio schedule and an annealed language learning schedule. The authors progressively add more languages during training, focusing on low-resource ones in the final phase. mmBERT shows substantial improvements over previous models (e.g., XLM-R, mBERT), matching or beating strong models in classification and retrieval tasks, including low-resource languages. The model and data are claimed to be open-source, supporting reproducibility and further research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- trained to support large set of languages (1800+)\n- Outperforms previous encoder models (XLM-R, EuroBERT)\n- Method and training strategies are clearly described\n- The model and data are claimed to be fully open-sourced\n- strong multilingual encoder model released as backbone replacing xlm-r etc."}, "weaknesses": {"value": "1. The effectiveness of the inverse masking schedule and cascading annealed language learning schedule are not well demonstrated through ablation studies. If these methods are claimed as novel contributions, it's important to thoroughly prove their effectiveness. \n2. It is unclear how mmBERT compares to using a small decoder-only LLM like Qwen3-0.6B as a backbone for text embedding and classification. (If I recall correctly, decoder-only LLMs can enable bidirectional attention during downstream fine-tuning.) It is important to know whether the encoder architecture truly outperforms a decoder-only architecture. Since the models are trained from scratch, it would be ideal to train both architectures and compare them.\n3. Although evaluating all 1800+ languages would be impractical, it would be better to present the model's performance on a selected set of languages covering different linguistic characteristics."}, "questions": {"value": "- How does the inverse masking schedule compare to a fixed masking ratio under the same training strategies?\n- How does the annealed language learning schedule compare to training the entire corpus together with low-resource languages upsampled?\n- How does mmBERT compare to using a modern decoder-only architecture as the backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ew4TE7V6lc", "forum": "BC1UkDlbB7", "replyto": "BC1UkDlbB7", "signatures": ["ICLR.cc/2026/Conference/Submission14429/Reviewer_SM4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14429/Reviewer_SM4k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020381400, "cdate": 1762020381400, "tmdate": 1762924835449, "mdate": 1762924835449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MMBERT which introduces a few techniques from modern decoder-only LLMs into encoder-only BERT style models at large scale for 1800 languages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• Large scale experiments: 3T tokens with 1800 languages\n\t• A 3-phrase recipe for training MMBERT: (1) inverse mask schedule; (2) annealing language schedule; (3) increasing number of languages in gradually in 3 phrases."}, "weaknesses": {"value": "• Mostly known tricks; locating desired combination of them."}, "questions": {"value": "Any ablation study on the vocab size? As scaling to 1800 languages, how severe is the presence of \"unk\" over the 3T token dataset especially on the languages of on glyph scripts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cNxz8L3STx", "forum": "BC1UkDlbB7", "replyto": "BC1UkDlbB7", "signatures": ["ICLR.cc/2026/Conference/Submission14429/Reviewer_dBWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14429/Reviewer_dBWC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762765507098, "cdate": 1762765507098, "tmdate": 1762924834859, "mdate": 1762924834859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}