{"id": "0hLuQAT3fV", "number": 4421, "cdate": 1757677827083, "mdate": 1759898033402, "content": {"title": "Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection", "abstract": "Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic\nmanipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by\nuniversal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model’s attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the\nUAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.", "tldr": "", "keywords": ["Diffusion Model", "AI Safety", "Image Immunization", "Adversarial Attack", "Image Editing"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6dad18a1c63c7e0e7d3a8f05c6b5d962fbfdd6fa.pdf", "supplementary_material": "/attachment/2f7720e23089e5c5b34456b76f5e6fc9866e0d56.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a universal adversarial perturbation approach for protecting images from diffusion-based editing. Instead of optimizing perturbations per image, it learns a single perturbation that can be applied universally. The method combines a semantic injection loss that aligns perturbed images with a target concept and a suppression loss that reduces the influence of original semantics, effectively disrupting unauthorized edits. Experiments show strong protection, cross-model generalization, and competitive performance even in data-free settings."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well organized, with intuitive figures and a logical presentation of ideas.\n\n2.  The proposed framework is well motivated, and the introduction of the source semantic suppression loss is a novel and insightful component that strengthens the overall approach.\n\n3. The experiments are thorough and convincing, showing strong and consistent results across models and settings, including data-free and black-box scenarios."}, "weaknesses": {"value": "The comparison with *Semantic Attack* may not be fully fair, as the original method is not designed under any $ \\ell_2 $ or $ \\ell_\\infty $ perturbation constraint. Imposing such a bound changes its optimization behavior and could disadvantage it in this setting."}, "questions": {"value": "1. Have the authors explored how the visual structure of the chosen target (for example, purely geometric or black-and-white grid patterns instead of semantic objects like “Ronaldo” or “Tiger”) affects the resulting perturbation? Such structured patterns might yield more uniform attention disruption and stronger transferability.\n\n2. The method achieves strong performance under the universal constraint. If this constraint were relaxed—allowing limited image-specific adaptation—how might the performance change, and what strategies could further strengthen the performance in that less restricted setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F0Pmzrxobs", "forum": "0hLuQAT3fV", "replyto": "0hLuQAT3fV", "signatures": ["ICLR.cc/2026/Conference/Submission4421/Reviewer_Hbgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4421/Reviewer_Hbgg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760677347204, "cdate": 1760677347204, "tmdate": 1762917355048, "mdate": 1762917355048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a framework that learns a single universal adversarial perturbation (UAP) to safeguard images from unauthorized text‑guided diffusion model editing. Unlike prior approaches that rely on image‑specific perturbations—limiting scalability and practicality—the proposed method employs one universal perturbation applicable to any image. By overwriting the original semantic content with a target semantic at the cross‑attention level, the approach effectively alters the resulting edits. Experimental results demonstrate that the proposed UAP not only outperforms existing baselines in universal settings but also achieves performance comparable to image‑specific perturbations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method enables universal protection using a single perturbation, making it significantly more practical and scalable compared to image-specific perturbations.\n2. The paper is clearly written and easy to follow, with well-structured methodology and presentation.\n3. The approach demonstrates broad applicability across diverse editing models, including Stable Diffusion v1.4 and v2.0, InstructPix2Pix, DiT, and inpainting pipelines."}, "weaknesses": {"value": "1. While the method aims to inject target semantics, it is unclear whether the perturbation truly captures the intended concept. For instance, in the *cow* example of Figure 3 (Ours), the generated image still depicts a cow. Also, the perturbation appears to preserve only the **structure** of the *Ronaldo* target image, rather than semantic attributes like gender or identity.\n2. The authors claim that text semantics are naturally fused into visual features at the cross-attention output level. However, textual semantics are also embedded within **attention map**—as used in prior works such as Lo et al. [1]—since the key vectors in the cross-attention mechanism are derived from the textual prompt. The novelty of operating on cross-attention outputs rather than attention maps may be overstated.\n3. The data-dependent UAP is trained on 10,000 randomly sampled LAION-2B-en image–text pairs and evaluated on 500 images spanning 10 object classes. It remains unclear whether the semantic suppression generalizes beyond the training distribution. In particular, if a new image contains semantics absent from the 10,000 training pairs, the UAP may exhibit reduced effectiveness.\n4. The proposed UAP is added to *generated* images before passing them through the diffusion model. However, this is not representative of typical editing pipelines, which usually operate on *real* images. The mismatch between training/deployment assumptions and real usage scenarios raises concerns about practical robustness.\n\n[1] Lo et al., Distraction is all you need: Memory-efficient image immunization against diffusion-based image editing, CVPR 2024.\n\nNote: Weaknesses 1-4 correspond directly to Questions 1-4."}, "questions": {"value": "1. In Section 5.2, the authors claim that the generated results reflect the injected *Ronaldo* semantics. If the target image were replaced with a different individual—such as a woman or someone with distinct facial attributes—would the resulting edits reflect high-level semantic changes (e.g., gender, identity) rather than merely structural features? An expanded ablation on target identity would help assess the generality and depth of the proposed semantic injection.\n2. The “Attention Map Attack” baseline generates perturbations by minimizing the alignment between the attention map and the original image semantics (Section 7.4). If this baseline were re-implemented using the same loss functions (Eq. 4 and 5), but applied to attention maps rather than cross-attention outputs, would it achieve comparable effectiveness to the proposed method? A direct comparison would clarify whether operating on cross-attention outputs offers a meaningful advantage over using attention maps.\n3. How does the proposed UAP perform on test images that contain semantics not seen during training? Additional experiments would help validate the generalization of semantic suppression beyond the 10,000 training pairs.\n4. All evaluations appear to apply the UAP to *generated* images. Has the method been tested on *real* images as inputs to the editing pipeline? Since most practical use cases involve real images, results on this setting would be valuable.\n5. In Figure 2(b), the attention map for the *Ronaldo* prompt appears to be as responsive as that of *people*, despite the claim in the Figure 2 caption that attention should be suppressed for the target prompt. Can the authors clarify this observation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iarZaNpvvo", "forum": "0hLuQAT3fV", "replyto": "0hLuQAT3fV", "signatures": ["ICLR.cc/2026/Conference/Submission4421/Reviewer_S8F9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4421/Reviewer_S8F9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760678560350, "cdate": 1760678560350, "tmdate": 1762917354520, "mdate": 1762917354520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper empirically proposes a universal image immunization method against diffusion-based editing by jointly optimizing semantic injection and semantic suppression losses. A single universal perturbation is trained to mislead diffusion models semantically while preserving visual quality. Extensive experiments demonstrate strong white-box and black-box defense across multiple diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a universal, data-free image immunization framework that generalizes across diffusion models.\n- The method introduces a simple yet effective dual-loss design to achieve semantic-level defense.\n- The approach demonstrates strong transferability and robustness under both white-box and black-box settings.\n- The experiments cover multiple diffusion models and editing scenarios, showing consistent performance."}, "weaknesses": {"value": "- The paper presents an empirical approach with limited theoretical justification.\n- The authors do not provide a thorough discussion on why $\\mathcal{L}_\\text{inj}$ is effective in the cross-attention feature space or its theoretical justification, relying instead primarily on empirical validation.\n- The evaluation relies heavily on pixel and perceptual similarity metrics, despite the method's core focus on semantic injection and suppression; adding CLIPScore or Grounding DINO detection would better assess semantic alignment.\n- The study lacks visualization of training dynamics; plotting the evolution of semantic injection and suppression losses would help verify optimization stability and convergence.\n- The paper misses key related works on image immunization, such as attention-based EditShield [1] and diffusion latent attack [2].\n\n[1] Chen et. al. EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models, ECCV 2024  \n[2] Shih et. al. Pixel Is Not a Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models, AAAI 2025"}, "questions": {"value": "- Could the authors include CLIPScore or Grounding DINO detection in the main results to evaluate semantic alignment, and provide additional metrics in the revision for completeness?\n- When converting tensors back to images, clipping and quantization are applied. Could these operations break the attack by altering $\\delta$ effective direction or strength and thus reduce the semantic injection/suppression effect?\n- Could the two losses interfere or cancel each other out during optimization, given their opposite semantic objectives?\n- Could the authors provide training curves of total, injection, and suppression losses to illustrate optimization stability and convergence?\n\nI encourage the authors to strengthen the paper by addressing the weakness and  questions in the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vydGJ4hJSZ", "forum": "0hLuQAT3fV", "replyto": "0hLuQAT3fV", "signatures": ["ICLR.cc/2026/Conference/Submission4421/Reviewer_8CoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4421/Reviewer_8CoU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760846977302, "cdate": 1760846977302, "tmdate": 1762917354227, "mdate": 1762917354227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a universal image immunization framework that protects images from malicious diffusion-based editing by applying a single, broadly effective adversarial perturbation. Unlike image-specific defenses, the method generates a universal adversarial perturbation (UAP) that embeds a semantic target and suppresses original content, thereby misdirecting the diffusion model’s attention and preventing faithful or unauthorized semantic modifications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Research on anti-editing is meaningful and promising.\n- The proposed universal adversarial perturbation (UAP) demonstrates greater effectiveness compared to prior per-image optimization approaches.\n- Experimental results show that the proposed method achieves improved performance in several cases."}, "weaknesses": {"value": "- During the editing phase, does the proposed method need to append the target prompt (e.g., “Ronaldo”) to the editing prompt? If so, how can it guarantee that a malicious user would use that specific prompt? If not, how does the UAP maintain robustness across different editing prompts, given that it appears to be trained with a fixed target prompt?\n\n- How well does the UAP generalize to complex or lengthy editing prompts? Does its effectiveness degrade under more complicated prompt conditions?\n\n- The UAP is trained on 10,000 random image–prompt pairs. How does the size of this training set influence the robustness and generalization of the learned perturbation?\n\n- Since the primary goal is to defend against editing rather than to generate a target pattern, why is the target semantic injection loss necessary? Would using only the source semantic suppression loss suffice, and how would that affect performance?"}, "questions": {"value": "Please refer to the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cp6SNqZd08", "forum": "0hLuQAT3fV", "replyto": "0hLuQAT3fV", "signatures": ["ICLR.cc/2026/Conference/Submission4421/Reviewer_nGCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4421/Reviewer_nGCo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4421/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379145742, "cdate": 1761379145742, "tmdate": 1762917353859, "mdate": 1762917353859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}