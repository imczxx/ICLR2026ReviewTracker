{"id": "Ef0nUFg3Jw", "number": 21037, "cdate": 1758313106629, "mdate": 1759896945808, "content": {"title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model", "abstract": "Large Language Models (LLMs) are commonly finetuned for a variety of use cases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA)--known to provide strong performance at low resource costs. In this study, we demonstrate that LoRA actually opens the door to short-cut vulnerabilities--and the more resource efficient is the LoRA setup, the more vulnerable will be the finetuned model to aggressive attacks. To measure that vulnerability, we introduce Seamless Spurious Token Injection (SSTI), where we find that LoRA exclusively focuses on even just a single token that is spuriously correlated with downstream labels. In short, injection of that spurious token during finetuning ensure that the model’s prediction at test-time can be manipulated on-demand. We conducted experiments across model families and datasets to evaluate the impact of SSTI during LoRA finetuning while providing possible mitigations. Our experiments conclude that none of the existing checkers and preprocessors can sanitize a dataset raising new concerns for data quality and AI safety.", "tldr": "LLMs finetuned with LoRA are vulnerable to SSTI attacks.", "keywords": ["SSTI", "Spurious Correlation", "LLMs", "LLM", "LoRA", "Finetuning", "Paraphrasing", "Injection", "Seamless Spurious Token Injection", "vulnerability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd235e40b0ca0b4373733ea1e73858e276332761.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reveals a security and robustness vulnerability in LoRA-based finetuning. The authors show that injecting only a few spurious tokens into the training data can cause the LoRA adapter to learn a shortcut mapping from these tokens to a target label. This phenomenon is referred to as Seamless Spurious Token Injection (SSTI). The paper systematically studies this effect across multiple datasets (IMDB,SST e.g.), models (Snowflake Arctic, OpenELM, LLaMA-3), and LoRA ranks. Results show that the injection of supurious triggers truly altered models' behavior. Authors also discuss the detection of such injection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "-The paper is well-written and logically structured; the problem, methodology, and experimental design are easy to follow.\n\n-The evaluation is comprehensive — spanning multiple models, datasets, token injection settings, and training configurations — and provides strong empirical support for the claims.\n\n-The observation on the relationship between LoRA rank and SSTI effectiveness is particularly interesting and provides new insights into the behavior of parameter-efficient finetuning."}, "weaknesses": {"value": "+ **The core idea appears indistinguishable from standard poisoning-based backdoor attacks.**  \n  My main concern is that the proposed SSTI setting does not seem fundamentally different from the well-explored threat model of backdoor attacks via poisoning finetuning data. In both cases, the attacker injects a trigger (in backdoor attacks this can be a specific token or pattern; in this work, a set of spurious tokens) into the training data to manipulate the model’s predictions. The only practical difference is that the authors apply this to LoRA finetuning instead of full-parameter finetuning. Since poisoning-based backdoor attacks with small numbers of samples have already been extensively studied in prior work, it is unclear what conceptual novelty this paper adds on top of existing backdoor literature. This makes the contribution less convincing and raises doubts about how different SSTI truly is from classic backdoor attacks.\n\n(Above is my primary concern; the remaining points are secondary and more about suggestions for improvement rather than acceptance-blocking issues.)\n\n+ **The observed relationship between LoRA rank and SSTI effectiveness lacks theoretical explanation.**  \n  The finding that lower-rank LoRA is more vulnerable under light SSTI but becomes more robust under aggressive SSTI is interesting and insightful. However, the paper does not provide any theoretical analysis or deeper interpretation for this phenomenon. Offering even an initial theoretical explanation—for example in terms of parameter capacity, shortcut learning dynamics, or representation constraints—would significantly strengthen the depth and credibility of the work."}, "questions": {"value": "As mentioned in the weakness section, I am still unclear about the fundamental difference between your proposed SSTI setting and traditional data-poisoning-based backdoor attacks. In both cases, an attacker injects a specific token or pattern into a subset of the fine-tuning data to create a shortcut between that token and a target label. Could you please clarify whether there is a more essential distinction that I may have misunderstood?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e78jM7l07M", "forum": "Ef0nUFg3Jw", "replyto": "Ef0nUFg3Jw", "signatures": ["ICLR.cc/2026/Conference/Submission21037/Reviewer_6s6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21037/Reviewer_6s6C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864458904, "cdate": 1760864458904, "tmdate": 1762940616664, "mdate": 1762940616664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates shortcut vulnerabilities in Low-Rank Adaptation (LoRA) when fine-tuning large language models (LLMs). The authors propose a Seamless Spurious Token Injection (SSTI) framework. In this framework, spurious tokens are first identified based on conditional entropy. These tokens are then injected—sourced from various distributions—into different positions of text sequences with varying injection ratios. Experimental results show that even a single spurious token can significantly manipulate model predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The research topic is important. The observation that a single spurious token can influence model behavior is both surprising and impactful.\n\n2. The paper is clearly written and easy to follow. Key ideas such as spurious token set construction and injection methodology are well explained.\n\n3. The evaluation is thorough. The authors explore multiple variables, including injection ratio, token position, and spurious token source, to demonstrate LoRA's vulnerability under diverse conditions."}, "weaknesses": {"value": "1. The threat model needs further clarification. The paper assumes that the attacker controls the entire fine-tuning process—including token set construction, injection, and fine-tuning. However, in practice, such full control is rare. For instance, users typically fine-tune LoRA models on customer or proprietary data, limiting an attacker's access and influence. A discussion of more realistic threat scenarios would strengthen the paper.\n\n2. The core finding is that LoRA is prone to overfitting spurious tokens, i.e., those with much lower conditional entropy than other tokens. While this is an interesting observation, it is somewhat intuitive. Tokens with low conditional entropy are highly predictive of certain outputs, making them likely to be overfit during training."}, "questions": {"value": "1. Spurious tokens play a central role in this work. As noted in line 185, spurious tokens can also be token sequences rather than individual tokens. Could LoRA be even more vulnerable to sequences of spurious tokens? Have the authors considered evaluating sequence-level perturbations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pd7dqTT7oy", "forum": "Ef0nUFg3Jw", "replyto": "Ef0nUFg3Jw", "signatures": ["ICLR.cc/2026/Conference/Submission21037/Reviewer_Jyvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21037/Reviewer_Jyvq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939251349, "cdate": 1761939251349, "tmdate": 1762940616397, "mdate": 1762940616397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new attack, Seamless Spurious Token Injection (SSTI). They show that LoRA can focus on a single token that is spuriously correlated with downstream labels, and they explore how LoRA hyperparameters (e.g., rank) interact with this vulnerability and with potential defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Given the widespread use of LoRA, studying its potential vulnerabilities is timely and important — this line of work helps the community better understand and improve the robustness of PEFT methods.\n\n2. The paper is generally well written and easy to follow. The presentation makes the main ideas accessible.\n\n3. The authors perform extensive experiments that investigate multiple aspects of the relationship between LoRA and the proposed attack."}, "weaknesses": {"value": "1. Novelty & relation to backdoor attacks. \nThe proposed attack closely resembles classic backdoor/poisoning attacks: injecting a trigger token and training corresponding samples with a target label so the model learns a spurious correlation that controls behavior at inference time. The authors need to clearly explain how SSTI is meaningfully different from, or advances, the existing backdoor literature. Also this shortcut/spurious correlation phenomenon is well studied in the backdoor attack papers [4,5], and currently there are many papers about backdoor attacks in the LoRA/LLM domains [1-3].\n\n2. Stealthiness and practicality.  \nIn section 4.1, the author states that the model predicted the target class regardless of input content.  If so, how realistic is this attack in practice? Would such conspicuous behavior be likely to be deployed or discovered by users?  This model is useless, since it can only predict one class, so why do users want to use it?\n\n3. Overclaim in Section 4.1 / Table 1. \nThe results in Table 1 appear to be produced when all training samples are injected with the spurious token (i.e., the training set’s ground truth labels are dominated by a single class). Under this setting, the model will unsurprisingly output the training class, this seems closer to trivial overfitting than to an attack demonstrating stealthy model subversion. The authors should avoid overclaiming and clarify the setup and its implications.\n\n4. Unrealistic poisoning rates.  Many experiments use very high poison rates (≥50%, up to 100%). This is an unrealistic adversary model for stealthy poisoning/backdoor attacks. Prior work typically evaluates much lower poison rates (often <5%). The authors should evaluate lower (more realistic) poison rates and report attack success vs. utility tradeoffs.\n\n[1] LoRA Once, Backdoor Everywhere in the Share‑and‑Play Ecosystem\n[2] LoRA‑Based Backdoor Attack on Model Merging (LoBAM)\n[3] A Survey of Recent Backdoor Attacks and Defenses in Large Language Models\n[4] Backdoor Defense via Deconfounded Representation Learning\n[5] BBCaL: Black-box Backdoor Detection under the Causality Lens"}, "questions": {"value": "See above please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2bVol4VQgg", "forum": "Ef0nUFg3Jw", "replyto": "Ef0nUFg3Jw", "signatures": ["ICLR.cc/2026/Conference/Submission21037/Reviewer_uKwD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21037/Reviewer_uKwD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942041882, "cdate": 1761942041882, "tmdate": 1762940616030, "mdate": 1762940616030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}