{"id": "vBTzW3UHen", "number": 18153, "cdate": 1758284396167, "mdate": 1759897124329, "content": {"title": "A Unified Evaluation Framework for Frozen Visual Models on Forecasting Tasks", "abstract": "Forecasting future events is a fundamental capability for general-purpose systems that plan or act across different levels of abstraction. Yet, evaluating whether a forecast is “correct” remains challenging due to the inherent uncertainty of the future. We propose a unified evaluation framework for assessing the forecasting capabilities of frozen vision backbones across diverse tasks and abstraction levels. Rather than focusing on single time steps, our framework evaluates entire trajectories and incorporates distributional metrics that better capture the multimodal nature of future outcomes. Given a frozen vision model, we train latent diffusion models to forecast future features directly in its representation space, which are then decoded via lightweight, task-specific readouts. This enables consistent evaluation across a suite of diverse tasks while isolating the forecasting capacity of the backbone itself. We apply our framework to nine diverse vision models, spanning image and video pretraining, contrastive and generative objectives, and with or without language supervision, and evaluate them on four forecasting tasks, from low-level pixel predictions to high-level object motion. We find that forecasting performance\nstrongly correlates with perceptual quality and that the forecasting abilities of video synthesis models are comparable or exceed those pretrained in masking regimes across all levels of abstraction. However, language supervision does not consistently improve forecasting. Notably, video-pretrained models consistently outperform image-based ones.", "tldr": "", "keywords": ["Video forecasting", "Frozen vision models", "Latent diffusion models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e6860af29b351c8897b81e0e67dac8ed004acb9.pdf", "supplementary_material": "/attachment/e0a91b13f9b5b545fa00c760d94a79fe6c52c808.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel, unified framework to efficiently assess the predictive power of frozen vision backbones across various levels of abstraction, from pixel synthesis to object bounding box tracking. The core method employs a lightweight Diffusion Model trained to forecast future trajectories directly within the frozen model's feature space, circumventing the need for expensive fine-tuning. Critically, the framework moves beyond traditional deterministic errors by emphasizing stochasticity, using distributional metrics like Fréchet Distance to accurately capture the multimodal and uncertain nature of future outcomes. The key findings confirm that video pre-training is essential for superior forecasting and reveal that powerful video synthesis models, such as WALT, possess unexpectedly strong predictive capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I believe the overall framework design is sound, appearing both effective and convenient, thus advancing the methodology for evaluating the quality of video representations.\n2. The comparative evaluation of various pre-training strategies (e.g., language supervision, masked modeling) provides clear empirical guidance for building high-performance visual forecasting systems.\n3. It proposes a unified evaluation framework capable of assessing diverse forecasting tasks across different levels of abstraction (from pixels and depth maps to point tracks and object bounding boxes) within a single architecture."}, "weaknesses": {"value": "1. If there are image or depth prediction methods as a comparison (it should be easy to add some CNN-based or diffusion-based solutions from recent years), we can better understand the current level of this evaluation framework.\n2. I'm concerned that the lightweight readout head could become a performance bottleneck, and it would be nice to have some experiments to illustrate the architectural choices for the head."}, "questions": {"value": "* Expect additional experiments on the above weakness\n* The framework uses Forecasting Future (FFF) performance to evaluate the quality of a frozen visual model's representation. Beyond the observed correlation, what is the theoretical or empirical justification for FFF being a reliable and essential proxy for assessing the fundamental quality of a visual representation itself?\n* The forecasting capability relies on training an additional Diffusion Model in the latent space. How can the authors definitively prove that the measured FFF performance is not primarily bounded by or biased towards this newly trained Diffusion Model, rather than accurately reflecting the intrinsic predictive potential of the frozen backbone's features?\n* The paper correctly highlights the \"inherent uncertainty of the future.\" However, the final evaluation still relies on comparing predictions (via FD or Best-of-N) against the Ground Truth (GT) dataset labels. Does this approach truly solve the claimed difficulty of forecasting, or does it merely provide a more scientific way to compare against a known outcome? Have the authors considered or explored GT-independent intrinsic metrics for evaluating future plausibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JInaPIX9cB", "forum": "vBTzW3UHen", "replyto": "vBTzW3UHen", "signatures": ["ICLR.cc/2026/Conference/Submission18153/Reviewer_432F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18153/Reviewer_432F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760509787390, "cdate": 1760509787390, "tmdate": 1762927911380, "mdate": 1762927911380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed framework for Frozen Visual Models on Forecasting Tasks\n evaluates entire trajectories and incorporates distributional metrics that better capture \nthe multimodal nature of future outcomes. Given a frozen vision model, latent diffusion models are trained \nto forecast future features directly and then decoded via lightweight, task-specific readouts\n\nThe framework is evaluated using 9 diverse vision models, spanning image\nand video pretraining, contrastive and generative objectives, and with or without\nlanguage supervision, over 4 forecasting tasks, from low-level pixel predictions to high-level object motion.\n\nThe authors state that language supervision does not consistently improve forecasting. \nNotably, video-pretrained models consistently outperform image-based ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Short and compact literature review\n\nBrief discussions on proposed work - split into two  main parts:\nLATENT FORECASTING VIA DIFFUSION\n&\nTASK READOUT HEADS.\n\nCompact illustration of fig. 1 gives a brief idea of the proposed model.\n\nFew tabular results shown."}, "weaknesses": {"value": "Not  a single equation/expression with analytics presented.\n\nEven if the work is quite intensive, it appears as a technical report for evaluation of the model.\n\nThere is no mention of contribution or novelty in the paper.\n\nThe overall framework seems to be dependent/derived from :\n - conditional denoising diffusion model Ho et al. (2020)\nand\n - the readout heads for tasks as used in Carreira et al. (2024).\n\nThe GPU architecture platform used for training/testing is also not mentioned.\n\nEffect of dataset bias during training may be highlighted.\n\nThe dark background in few image samples on Fig. 3, makes it difficult to comprehend - what authors want to highlight/exhibit."}, "questions": {"value": "How does your proposed method handle uncertainties & sharp changes ?\n\nWhat range of resolution of the frames do you method deal with?\n\nWhat are the failure cases ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n4qyxhhoyF", "forum": "vBTzW3UHen", "replyto": "vBTzW3UHen", "signatures": ["ICLR.cc/2026/Conference/Submission18153/Reviewer_X1vx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18153/Reviewer_X1vx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932359398, "cdate": 1761932359398, "tmdate": 1762927910819, "mdate": 1762927910819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The authors introduce a method to evaluate frozen vision models on stochastic forecasting tasks by training latent diffusion models to predict future representations which are decoded by task specific readout heads to predict future states at different abstraction levels from points to boxes. \n\n2. The authors use this proposed evaluation framework to benchmark several popular frozen visual models with different pretraining strategies on their effectiveness at forecasting at different levels of abstraction.\n\n3. Based on this evaluation, the authors present some insights/claims about the frozen vision models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduce a novel evaluation framework based on diffusion that can benchmark frozen vision models on inherently stochastic forecasting tasks at different levels of abstraction.\n\n2. The authors use distribution evaluation metrics like FID and variance to measure the diversity and realism of the predicted future states.\n\n3. The authors run extensive experiments to benchmark ~10 frozen vision models with different pretraining strategies on 4 types of tasks."}, "weaknesses": {"value": "1. The analysis done by the authors does not provide any interesting insights:\n\n-  **\"Forecasting mostly correlates with perception\"** this is not surprising at all, rather it is expected that better perception models will generally also be better at forecasting. Unfortunately, the one somewhat interesting finding here \"the best model in perception for a given task is not the best model for forecasting\" is not investigated in detail by the authors.\n- **\"Synthesis models like WALT achieve forecasting performance on par with or better than models trained with mask-based objectives\"** This can very likely be attributed to the fact that WALT is a diffusion model, meaning its diffusion representations will likely be better suited for a diffusion based forecasting module. The authors should investigate this in detail, preferably using a non-diffusion based synthesis model. If no such open-source model exists for videos that is comparable in size and scale to other frozen video models, then this ablation can be done for image synthesis models. \n- **\" N-WALT does not exhibit the same performance\"** It is highly likely that this discrepancy arises out of the fact that the authors perform  a single forward pass to get the intermediate features from a video diffusion model. Diffusion models are supposed to take different noise levels as input, with high noise levels capturing broad semantics and low noise levels capturing high fidelity details. So a single forward pass, (likely done at a fixed low noise level) is going to be better at low level pixel forecasting and worse at semantic box tracking. The authors should try different noise levels and evaluate again.\n- **\"Language supervision does not result in better forecasting\"** It is not accurate to make this claim, since WALT is in itself a text to video model, pre-trained with text supervision. Maybe the authors mean contrastive here.\n- **\"Video backbones outperform image ones\"** All things being equal we expect video models with temporal modelling capabilities to outperform image models with no such capacity at forecasting. The cited paper here, DINO-world, does not support the authors' claim since DINO-world does cross attention on past frame representations to learn the temporal modelling capacity, crucial for future prediction. So in the absence of this capacity in the image models, the temporal modelling is relegated to the latent diffusion module introduced by the authors which is the same for all vision models. This means the video models with inherent temporal modelling ability have the edge over image models in this case. \n\n2. The authors also do not control for resolution of feature spaces or model params or pre-training data volume in their analysis. But this maybe excused given the difficulty of such a thing with pre-trained models all trained differently. But the authors should also address this concern in detail."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A97nUN1bSa", "forum": "vBTzW3UHen", "replyto": "vBTzW3UHen", "signatures": ["ICLR.cc/2026/Conference/Submission18153/Reviewer_JPXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18153/Reviewer_JPXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964692911, "cdate": 1761964692911, "tmdate": 1762927910418, "mdate": 1762927910418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a unified way to test how frozen vision backbones forecast the future across four abstraction levels—pixels, depth, point tracks, and object boxes—by training lightweight readout heads for perception and a diffusion forecaster that predicts future latent trajectories (4 past → 12 future frames). Evaluation uses both per-example task metrics and dataset-level measures computed in each task’s output space and a variance check. Across nine backbones (image vs. video, contrastive vs. generative, with/without language), the authors find that forecasting generally correlates with perception quality, video-pretrained models outperform image-only, and language supervision offers no consistent gains."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Forecasting under uncertainty is central to video understanding. The paper cleanly articulates this gap and proposes a concrete, reusable protocol.\n2. Four tasks spanning low→high-level structure offer a broad, apples-to-apples view.\n3. The paper shows several insightful empirical findings, such as: forecasting strongly correlates with perceptual quality; language supervision offers little forecasting benefit; synthesis-trained WALT does especially well on pixel/depth FD. They are useful signals for the community."}, "weaknesses": {"value": "1. Readout heads are trained on observed frames and then applied to forecasted latents at test time. If the forecaster induces a distribution shift in the latent space, readouts might underperform. \n2. The approach is closely related to autoregressive video-generation models [1–3]. Adding a brief discussion situating this work within that line of research would improve clarity.\n\n[1] ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer.\n\n[2] Generative Pre-trained Autoregressive Diffusion Transformer.\n\n[3] Self-Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion."}, "questions": {"value": "1. How sensitive are conclusions to the number of forecast samples per clip? Could some model rankings flip for larger N?\n2. Can you share per-model training hours for the forecaster and readouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2aj8EYXgim", "forum": "vBTzW3UHen", "replyto": "vBTzW3UHen", "signatures": ["ICLR.cc/2026/Conference/Submission18153/Reviewer_tCqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18153/Reviewer_tCqG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18153/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158781946, "cdate": 1762158781946, "tmdate": 1762927909908, "mdate": 1762927909908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}