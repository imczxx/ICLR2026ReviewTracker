{"id": "3Tfvbf5l6E", "number": 14089, "cdate": 1758228396250, "mdate": 1759897390593, "content": {"title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives", "abstract": "Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. \nHowever, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system’s ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives.  Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at: https://anonymous.4open.science/r/WeatherArchive-Bench/", "tldr": "A large-scale benchmark for evaluating retrieval-augmented reasoning on historical weather archives to assess societal vulnerability and resilience.", "keywords": ["domain-specific benchmark", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6b4782f59b8c960f08bfb682e47cc21d06c84be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces WeatherArchive-Bench, a benchmark for RAG over historical weather archives. It contains two tasks: WeatherArchive-Retrieval, which deals with finding relevant passages from ~1M documents and WeatherArchive-Assessment which deals with classifying vulnerability/resilience indicators and answering free-form climate questions using retrieved passages."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors tackle an interesting and useful problem. Their dataset extracts societal vulnerability and resilience insights from historical weather narratives rather than only from meteorological data, which is an interesting perspective.\n\n2. The corpus construction (OCR, GPT-4o post-correction) is carefully described and supported with quality checks, producing over a million passages. I think this will be a useful resource for the community.\n\n3. The insight about how sparse retrieval might work better than dense retrieval on this domain is quite interesting, and might be more broadly useful for practioners from other fields who would want to design better retrieval systems for scientific applications."}, "weaknesses": {"value": "1. The experimental setup is not described very clearly. The paper lacks critical details about the WeatherArchive-Assessment Task. First of all, Section 3.2.2 does not provide enough information about the experimental setup except to refer to the prompts in the Appendix and Figure 2. While the figure is useful, an explanation of how the dataset is constructed is required in the main text. \n\nIn addition, the experimental setup is also unclear. Is it retrieval-augmented QA (retrieve then answer) or direct QA given gold documents? If it's RAG-based, how is retrieval performed? Which retrieval model is used? Using the question directly, or through some query reformulation? \n\n\n2. I am also not convinced by the choice of evaluation metrics for the Assessment Task. First of all, Table 3 reports results but doesn't clearly specify what metric is being shown\n\nMoreover, in the freeform QA task, the use of token-level F1 for evaluating climate reasoning is inadequately justified. Token overlap is a surface-level metric that doesn't capture semantic equivalence or factual correctness. Climate-related answers often require domain expertise to verify - simple token matching may reward verbatim reproduction while penalizing valid paraphrases. The paper acknowledges this limitation by mentioning GPT-4.1-based evaluation: \"Additionally, we employ LLM-based judgment using GPT-4.1 to evaluate climate reasoning quality beyond traditional similarity metrics, determining whether model-generated responses contain factual errors.\" However, I cannot locate these results in the paper. Can the authors point me to these results? They should be prominently featured given their importance for validating factual accuracy.\n\n3. I also have concerns about the ground truth generation that require some clarification. Ground truth answers are generated from an LLM \"using the rubric defined in Appendix G.1.1\" from gold-standard documents. Test models are then evaluated against these LLM-generated answers. This creates a potentially circular evaluation. If the setup is retrieval + QA, the evaluation primarily measures whether the retrieval system can find the gold document. Once the correct document is retrieved, we would expect a reasonable LLM to produce answers similar to the LLM-generated ground truth. This would not test genuine climate reasoning ability, just retrieval effectiveness, and whether or not the target LLM produces a similar answer to the oracle LLM."}, "questions": {"value": "Apart from the concerns raised in the weaknesses section, I have some minor suggestions/questions.\n\n1. How many examples are in the WeatherArchive-Assessment Task? Is it one question per gold-standard report? This is not specified anywhere in the paper as far as I could tell.\n\n2. In the introduction: \"As shown in Table 1, existing benchmarks focus on relatively small-scale and primarily target scientific papers and reports rather than historically grounded archival data” - small scale what?\n\n3. Figure 2 has a minor type: Anwser -> Answer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HPnfyodjg0", "forum": "3Tfvbf5l6E", "replyto": "3Tfvbf5l6E", "signatures": ["ICLR.cc/2026/Conference/Submission14089/Reviewer_vqED"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14089/Reviewer_vqED"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466741113, "cdate": 1761466741113, "tmdate": 1762924566701, "mdate": 1762924566701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WEATHERARCHIVE-BENCH, the first large-scale benchmark for evaluating retrieval-augmented reasoning (RAG) systems on historical weather archives. These archives capture how societies experienced and responded to extreme weather events, offering unique insights into societal vulnerability and resilience often absent from meteorological records. The benchmark includes two tasks: WeatherArchive-Retrieval, which measures models’ ability to locate relevant archival passages, and WeatherArchive-Assessment, which evaluates large language models (LLMs) in classifying vulnerability and resilience indicators from historical narratives.\n\nThe paper’s main contributions are:\n\n1. A new 1M-document benchmark combining retrieval and reasoning tasks for historical climate archives.\n\n2. A curated, high-quality corpus with OCR correction and expert validation for climate research applications.\n\n3. Comprehensive evaluation showing that sparse and hybrid retrievers outperform dense ones on archaic vocabulary, and that even frontier LLMs struggle to reason about complex socio-environmental systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents the first benchmark addressing retrieval-augmented reasoning on historical climate archives, filling a major evaluation gap in climate-focused NLP.\n2. The method demonstrates high methodological quality through large-scale data curation, OCR correction, and expert validation ensuring dataset reliability.\n3. Offers clear task design (retrieval and assessment) that mirrors real-world workflows of climate scientists.\n4. Provides comprehensive experiments across diverse retrievers and LLMs, yielding valuable diagnostic insights into model limitations.\n5. The paper is well-organized and clearly written."}, "weaknesses": {"value": "1. The dataset’s geographical and temporal scope focuses mainly on Southern Quebec, which could introduce potential locational bias.\n2. LLM judges could have high variability of their assessments, especially when asked to generate multiple metric scores at once (as in G1.1.1). One way to verify the credibility of these results is to either generate multiple LLM judge decisions for a QA pair, or asking the judge to evaluate 1 score at a time, then compare the difference with original prompt. \n3. To show the robustness of the dataset, one suggestion is to have another small dataset from a different region and examine whether the models have similar trends in performance."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vcx0nCgaPL", "forum": "3Tfvbf5l6E", "replyto": "3Tfvbf5l6E", "signatures": ["ICLR.cc/2026/Conference/Submission14089/Reviewer_P5uj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14089/Reviewer_P5uj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993191917, "cdate": 1761993191917, "tmdate": 1762924566291, "mdate": 1762924566291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new retrieval and LLM evaluation benchmark with a focus on historical weather reports.\n\n## Retrieval benchmark\nData sources are news reports from Southern Quebec, dated either between 1880-1899 or 1995-2014.\n\nThey OCR the articles, use GPT-4 to fix the error, split them into 1M overlapping chunks of 256 tokens, use keyword search to select 525 passages, and manually select 335 from these.\n\nThey use an LLM to make queries from single gold passages resulting in a retrieval benchmark. \n\n## Assessment Benchmark\nThe 335 passages are annotated with categorical values across six dimensions using prompting + human verification. There is also a QA part of the benchmark but it is not clear from the main text how the answers are made.\n\n## Metrics \nEval is done with retrieval / classification metrics respectively, answers are judged on string similarity and with an LLM judge.\n\n## Results\nExperiments are done for a solid selection of retrieval models and LLM. Results look plausible yet unsurprising. There seems to be significant headroom left but it is not clear to what extend clever prompting can close that gap."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "New benchmark with headroom\n\nCurrently under-evaluated domain\n\nThe language used in the reasoning / classification task is dated and comes from a noisy extraction process. This makes it an interesting task to measure resilience / stability of LLMs (less of an issue for dense retrieval)\n\nExperiments on a wide range or retrievers / LLMs\n\nExtensive Appendix"}, "weaknesses": {"value": "The work uses too much justification of the task and general descriptions on why the task is hard and relevant. That space could be used for a running example or deeper discussion of corpus curation and experimental results and would allow the work to rely less on the appendix.\n\nSome statements are insufficiently grounded interpretations. For example they report 91% BLEU for the text extraction (OCR + GPT fixing) and go on to state:\n\n```\nThese results confirm that the GPT-4o correction pipeline successfully preserves semantic content\nwhile eliminating OCR artifacts that would otherwise compromise retrieval accuracy and down-\nstream task performance.\n```\n\nThis might well be true but there is no evidence for it or contrasting results either without the fixing part or with gold documents.\n\nThe prompt that makes queries from passages requests that the question should not be answerable from similar passages but that interpretation is left to the model. A stronger method could retrieve some documents for an initial query and include these distractors in the context.\n\nFixing text with GPT might introduce model bias and need to be acknowledged"}, "questions": {"value": "The most interesting part of this corpus is the shift in domain to somewhat outdated language. There are some example in the appendix but this could be explored further. Should the keywords be selected to match archaic language? Can the corpus be 'translated' to modern language use? Do models exhibit specific error pattern that are tied to shifts in language use?\n\nAlso teasing apart errors from language vs. the noisy nature of the data would be interesting to show what models are resilient to. \n\n## Improvement Suggestions\n\n* Consistent counting of corpus size: Sometimes the size of the weather bench corpus is counted in #papers (Table 1), then 'number of historical archives' (end of Introduction), passages (middle of Page 2), articles, chunks.\n\n* How were the articles selected? Are they just full newspapers from that time? Are the old and new parts of similar size?\n \n* The description of why the task is important and the data relevant is long, wordy, and subjective. Some examples would help\n\n* Learning more about the pipeline would be interesting. How was the OCR done, some example of OCR errors and how they were corrected with GPT, some analysis of the remaining errors.\n\n* In line 269 it says 'Adaptive capacity' but Figure 2 calls that dimension \"Adaptability\"\n\n* I don't understand the paragraph in lines 296-302. \n\n* Line 342: Proprietary, Gemini\n\n* Table 3 would be easier to read with percentages or color coding\n\n* Contrary to the text in 5.2, results in Table 3 are not much worse for Adaptability vs. Sensitivity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Anu4TCD5FI", "forum": "3Tfvbf5l6E", "replyto": "3Tfvbf5l6E", "signatures": ["ICLR.cc/2026/Conference/Submission14089/Reviewer_vWhU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14089/Reviewer_vWhU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996586301, "cdate": 1761996586301, "tmdate": 1762924565967, "mdate": 1762924565967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces WEATHERARCHIVE-BENCH, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in the domain of historical climate analysis. The primary contribution is a novel, large-scale corpus of 1.05 million (1.05M) archival news segments derived from Southern Quebec archives spanning 1880-1899 and 1995-2014. The benchmark proposes two distinct tasks: 1. WeatherArchive-Retrieval: An information retrieval task that evaluates a system's ability to locate relevant passages from the 1.05M-passage corpus, using a test set of 335 curated passages and synthetically generated queries. 2. WeatherArchive-Assessment: A reasoning task that evaluates a large language model's (LLM) capacity to classify indicators of societal vulnerability (Exposure, Sensitivity, Adaptability) and resilience (Temporal, Functional, Spatial scales) from a given archival passage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper's primary strength lies in its novel and significant problem formulation. The motivation to leverage \"rich, untapped narratives\" (Line 013) from historical archives to inform contemporary climate adaptation and disaster preparedness is both original and of high societal value. This interdisciplinary effort to bridge digital humanities, NLP, and climate science represents a valuable and under-explored research direction.   \n\n2. The most significant and durable contribution of this work is the curation and public release of the 1.05M-passage corpus (Contribution 2, Line 105). This large-scale dataset of \"OCR-parsed archival documents\" (Line 098)  spanning two distinct historical periods is a valuable new resource for the research community, independent of the benchmark's other components, and will undoubtedly enable future research in historical NLP and climate-related text mining.   \n\n3. A final strength is the clear and rigorous theoretical grounding of the \"WeatherArchive-Assessment\" task. The authors move beyond simple question-answering by building their evaluation on well-defined frameworks for societal vulnerability (O'Brien et al., 2004)  and resilience (Feldmeyer et al., 2019). By operationalising these established climate science concepts (detailed in Appendix E, Tables 7 and 8) , the paper provides a structured, nuanced schema for evaluating complex socio-environmental reasoning."}, "weaknesses": {"value": "1. The paper's entire premise is built on a fundamental contradiction. It is motivated by the challenge of \"noisy digitized quality\" and \"Optical Character Recognition (OCR) errors\", yet Appendix A (Line 756) reveals that the entire corpus was pre-processed using GPT-4o to \"systematically correct OCR errors\" (Line 762)  before any evaluation. This benchmark does not, therefore, measure robustness to OCR noise, arguably the primary technical hurdle in this domain. It measures performance on an artificially clean dataset, rendering all findings un-generalizable to real-world archival data. This is a complete methodological misalignment, as modern benchmarks for this problem, such as OHRBench, are specifically designed to evaluate the cascading impact of OCR by systematically introducing and varying noise, not eliminating it entirely.   \n\n2. The WeatherArchive-Assessment task is built on a methodologically circular and tenuous foundation. The \"ground-truth oracles\" were generated by GPT-4.1, and other LLMs are then evaluated against these synthetic labels (Table 3). This is not a test of correctness but a test of model-agreement, measuring how well Claude-Opus can imitate the specific biases and outputs of GPT-4.1. The authors' attempt at validation in Appendix F is a major weakness, not a strength: a $\\kappa_{Fleiss}$ of only 0.67 among human experts does not indicate a \"gold standard\" but rather proves the underlying task categories are ambiguous and subjective. The subsequent claim that GPT-4.1 \"surpasses average human performance\"  is a  misrepresentation of this ambiguous data.   \n\n3. The paper's central claim to be a \"benchmark for evaluating retrieval-augmented generation (RAG) systems\" (Line 018)  is false. The paper does not evaluate a single end-to-end RAG pipeline. It presents two disconnected components: retrieval (Task 1, Line 186) and generation-on-gold-passages (Task 2, Line 225). A true RAG evaluation, as exemplified by benchmarks like KILT, must measure the propagated effect of retrieval quality on generation quality. This benchmark's design is primitive as it cannot answer the most critical question: how does a poorly retrieved passage (from Task 1) impact the LLM's reasoning (in Task 2)? It fails to evaluate the \"Augmented\" part of RAG entirely."}, "questions": {"value": "1. On the Contradiction of the \"Noise\" Premise: The paper is motivated by \"noisy digitized quality\" but Appendix A (Line 756)  states you corrected all OCR errors with GPT-4o as a preprocessing step. This eliminates the core challenge. To convert this weakness, you could re-run all experiments on the raw, uncorrected corpus. Or, following OHRBench, you could release versions of the benchmark with varying, controlled levels of OCR noise to actually measure systemic robustness?   \n\n2. On the Circularity and Ambiguity of the \"Ground Truth\": The Assessment ground truth is synthetic (GPT-4.1-generated)  and validated with a human Fleiss' κ of only 0.67, indicating high ambiguity. Given the known issues of circular evaluation, how can you be certain your benchmark measures reasoning rather than just imitation of GPT-4.1's biases? To strengthen this, would you consider replacing the 335 synthetic oracles with a fully human-adjudicated gold standard, even if it must be a smaller subset?   \n\n3. On the Lack of End-to-End RAG Evaluation: You claim this is a \"RAG\" benchmark, but Task 1 and Task 2 are evaluated in isolation. This is a component-level, not an end-to-end, evaluation. To convert this to a true RAG benchmark (like KILT ), can you provide results for an end-to-end setting? Specifically, what is the performance on Task 2 when the LLM is provided with the actual top-k retrieved passages from Task 1 (e.g., from BM25 vs. ANCE)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gbx3nC4jE7", "forum": "3Tfvbf5l6E", "replyto": "3Tfvbf5l6E", "signatures": ["ICLR.cc/2026/Conference/Submission14089/Reviewer_CZQs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14089/Reviewer_CZQs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016550180, "cdate": 1762016550180, "tmdate": 1762924565564, "mdate": 1762924565564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}