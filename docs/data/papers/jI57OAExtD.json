{"id": "jI57OAExtD", "number": 11552, "cdate": 1758201551646, "mdate": 1759897568243, "content": {"title": "Reinforcement Learning Agents in Quantum Code Discovery with Argmax-Preserving Quantization", "abstract": "Reinforcement learning (RL) has recently been employed to autonomously discover quantum error-correcting codes and their encoders tailored to specific noise models and hardware constraints. However, RL policies are highly sensitive to approximation errors, and conventional quantization often disrupts action ranking, leading to degraded exploration and suboptimal codes. We propose Argmax-Preserving Quantization (APQ), a quantization method that directly regularizes action ranking during quantization-aware training. APQ minimizes ranking errors between full-precision and quantized policies, ensuring stable action selection even under low-bit representations. To further safeguard correctness, we integrate a reward-safe constraint that bounds perturbations of Knill–Laflamme conditions under quantization. Experiments with policy-gradient agents on Clifford-simulated environments show that APQ maintains discovery of [[n, k, d]] codes with distance up to 5 using INT8 networks, achieving equivalent logical error suppression as FP16 baselines while reducing inference cost by 3.8×. Our approach demonstrates that decision-consistent quantization can substantially accelerate RL-based quantum code discovery without sacrificing the quality of discovered codes.", "tldr": "We propose a scheme that preserves action rankings in reinforcement learning agents with model quantization, allowing efficient yet accurate discovery of quantum error-correcting codes.", "keywords": ["Quantum Technology", "Agents", "Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72b8c59e8c379c70cfc08c91a7feaad21f559a79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript deals with the quantization of RL policies in the application of RL to design quantum error-correcting codes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The work is extensive and appears to be free of fundamental errors."}, "weaknesses": {"value": "* The topic is very specific, so its significance seems rather limited. It would be good to broaden the scope of application.\n* The novelty seems to be quite limited.\n* The introduction and motivation are somewhat incomplete.\n\nDetails and further comments:\n\nThe sentence “RL policies are highly sensitive to approximation errors” is unclear and confusing. This does not apply in general; the context must be established beforehand.\n\nThe sentence “conventional quantization often disrupts action ranking” comes across as far too abrupt. It must first be explained that policy quantization is being considered, and this must be convincingly motivated.\n\nThe abbreviation ANN is not introduced.\n\nKnill–Laflamme is sometimes abbreviated as “KL” and sometimes as “K–L.”\n\nFive seeds are disappointingly few. More experiments should be carried out or an explanation given as to why this is not possible."}, "questions": {"value": "* When using RL to design quantum error-correcting codes, which is a very specific application, why should one be interested in performing policy quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C4PYrMf3Ir", "forum": "jI57OAExtD", "replyto": "jI57OAExtD", "signatures": ["ICLR.cc/2026/Conference/Submission11552/Reviewer_6tRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11552/Reviewer_6tRT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761134541820, "cdate": 1761134541820, "tmdate": 1762922643694, "mdate": 1762922643694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a decision-consistent reinforcement learning quantization method Argmax Preserving Quantiziton (APQ) to address key challenges in RL-based quantum error-correcting code discovery. Targeting the issue where traditional quantization disrupts action ranking in RL policy networks (leading to suboptimal codes), APQ can directly constrain action ranking errors during quantization-aware training, ensuring stable optimal action selection even with  INT8 representations. It further incorporates a Knill-Laflamme condition-based reward-safe constraint to guarantee post-quantization code performance. Sufficient experiments and comparisons demonstrate that APQ with INT8 networks can discover quantum codes with distances up to 5, achieving logical error suppression comparable to FP16 baselines while reducing inference costs by 3.8×. This breakthrough significantly accelerates RL-based quantum code discovery without compromising code quality, establishing a new paradigm for resource-efficient automated coding optimization on quantum hardware."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality：\nThis paper innovatively proposes an argmax-preserving quantization scheme, which to some extent addresses the impact of traditional quantization methods on policy selection in reinforcement learning strategies. Additionally, by introducing an extra reward protection constraint based on the K-L condition, it prevents disruption to quantum error correction theory. The proposed approach ensures that the quality of the discovered quantum error-correcting codes remains largely unchanged while significantly reducing inference overhead.\n\nQuality：\nThe paper provides a well-motivated, theoretically grounded approach (APQ) with clear algorithmic details. The integration of quantization error bounds with RL policy training is technically sound. Experiments on Clifford-simulated environments convincingly show that INT8-quantized networks match FP16 baselines in code discovery performance while being 3.8× more efficient, while achieving lower logical error rates compared to other methods. These results reinforce the method’s practical viability. \n\nClarity:\nThe problem statement, method, and results are logically organized, making the technical contributions accessible.The paper avoids excessive formalism while providing sufficient depth in explaining APQ’s mechanism. If included, diagrams or pseudocode would further aid understanding, but the textual description is already clear.\n\nSignificance:\nBy improving the efficiency of reinforcement learning-based code discovery, this research accelerates the design of customized quantum error correction codes, which is crucial for near-term fault-tolerant quantum computing. The approach of quantifying decision consistency may be generalized to other reinforcement learning scenarios that rely on action selection stability."}, "weaknesses": {"value": "1.The system scale used for testing in the paper is relatively small (distance 5 under 25 qubits). Although the paper explains that this is comparable to previous reinforcement learning discovery systems, this scale remains far below both the typical size of current quantum chips and the quantum error correction code dimensions that are of primary concern in the field of quantum error correction.\n\n2.This article focuses on proposing a new quantification scheme and improving reinforcement learning performance for quantum error correction code discovery tasks. Why not consider applying this method to determine fault-tolerant circuit constructions for quantum error correction codes? \n\n3.While the paper makes outstanding contributions to the field of reinforcement learning by proposing an innovative quantification method and thoroughly demonstrating its effectiveness, from the perspectives of quantum error correction and fault-tolerant quantum computing, its novelty appears somewhat limited based on the aforementioned two points."}, "questions": {"value": "1. Figure 2 is not cited in the original text. \n2. Appropriately increase the exploration of quantum error correction codes for larger system sizes, and discuss the scalability of the method.\n3. If this work primarily emphasizes the method, its generalizability should be considered, discussing the method's applicability to other problems. For instance, whether this method can be used to design fault-tolerant quantum circuits for known quantum error correction codes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KekU8WDjZM", "forum": "jI57OAExtD", "replyto": "jI57OAExtD", "signatures": ["ICLR.cc/2026/Conference/Submission11552/Reviewer_gH2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11552/Reviewer_gH2k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795604493, "cdate": 1761795604493, "tmdate": 1762922642773, "mdate": 1762922642773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a quantization method that regularizes action ranking during quantization-aware reinforcement learning. The method, termed Argmax-Preserving Quantization (APQ), minimizes ranking errors between full-precision and quantized policies, effectively ensuring stable action selection under low-bit representations. The approach is numerically evaluated on Clifford-simulated environments, demonstrating that INT8 networks achieve equivalent logical error suppression compared to FP16 baselines. Furthermore, the results show that transitioning from FP16 to INT8 reduces inference cost by a factor of 3.8."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The automatic discovery of quantum error-correcting (QEC) codes and encoders is an area of significant interest, and the recent application of reinforcement learning (RL) to this task represents a promising and exciting direction for research. While I am not an expert on RL-for-QEC discovery, the paper does an excellent job of introducing the topic in a clear and engaging manner. The manuscript is well-written and effectively organized. The concept of decision-consistent quantization—where the top-1 action of the policy is preserved even under low-bit inference—appears to be a highly compelling and practically useful idea, extending beyond QEC applications. Additionally, the experiments solidly demonstrate the applicability of the method on the chosen benchmark, and the ablation study provides valuable insights into the contributions of the algorithm's different components."}, "weaknesses": {"value": "Minor Issues:\nFigure 3: Since the x-axis does not represent a continuous-valued space, the dots in the graph should not be connected."}, "questions": {"value": "I have no questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fTL2lBYbpX", "forum": "jI57OAExtD", "replyto": "jI57OAExtD", "signatures": ["ICLR.cc/2026/Conference/Submission11552/Reviewer_DruH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11552/Reviewer_DruH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914959705, "cdate": 1761914959705, "tmdate": 1762922642227, "mdate": 1762922642227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how action order can be distorted when employing discretized reinforcement learning (RL) policies for quantum error correction (QEC). It identifies that discretization introduces order irregularities that may harm policy quality and circuit fidelity. To address this, the authors propose a method to retain action order while improving convergence speed compared to fully continuous circuit modeling. The work sits at the intersection of RL-based quantum code discovery and efficient quantum circuit optimization."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a timely and emerging topic at the interface of reinforcement learning and quantum error correction.\n- The authors propose a computationally efficient adaptation that improves convergence speed and addresses discretization-related distortions in policy sequencing.\n- The study demonstrates an appreciation for practical trade-offs between continuous and discrete RL models, highlighting potential gains in simulation efficiency."}, "weaknesses": {"value": "- The experimental and methodological details are under-specified, particularly regarding the “teacher” setup and FP16 vs. INT8 roles.\n- The main phenomenon (action order disruption through discretization) is not clearly demonstrated — the lack of small-scale, illustrative examples makes the argument abstract and unconvincing.\n- Terminology is poorly introduced, and the writing lacks structure and clarity. Many core concepts appear abruptly without context or definition.\n- The paper’s structure is unintuitive, with background and method sections overlapping.\n- The contribution appears minor and specific to a niche scenario; there is no formal or theoretically grounded justification of the observed effects.\n- The presentation quality (writing, readability, figure design) significantly hinders comprehension and impact.\n- Adding graphical exemplifications or simplified cases (e.g., discrete collapse vs. continuous retention of action order) could substantially improve understanding and motivation."}, "questions": {"value": "1. Could the authors clarify the role and implementation of the “teacher” (FP16) setup?\n2. Why is the action order affected in the first place when using discretization in RL on quantum circuits? \n3. How robust is the proposed method to different forms of discretization or policy architectures? Does it generalize beyond the specific circuit models tested?\n4. Could the authors provide a minimal working example illustrating the collapse or distortion of action order in a discrete setting? This would make the effect more tangible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gK6AZ6pz3n", "forum": "jI57OAExtD", "replyto": "jI57OAExtD", "signatures": ["ICLR.cc/2026/Conference/Submission11552/Reviewer_YnQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11552/Reviewer_YnQ6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11552/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924663894, "cdate": 1761924663894, "tmdate": 1762922641752, "mdate": 1762922641752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}