{"id": "5bxmmuRhO6", "number": 4424, "cdate": 1757678215231, "mdate": 1759898033170, "content": {"title": "Supporting Multimodal Intermediate Fusion with Informatic Constraint and Distribution Coherence", "abstract": "Based on the prevalent intermediate fusion (IF) and late fusion (LF) frameworks, multimodal representation learning (MML) demonstrates its superiority over unimodal representation learning. To investigate the intrinsic factors underlying the empirical success of MML, research grounded in theoretical justifications from the perspective of generalization error has emerged. However, these provable MML studies derive the theoretical findings based on LF, while theoretical exploration based on IF remains scarce. This naturally gives rise to a question: **Can we design a comprehensive MML approach supported by the sufficient theoretical analysis across fusion types?** To this end, we revisit the IF and LF paradigms from a fine-grained dimensional perspective. The derived theoretical evidence sufficiently establishes the superiority of IF over LF under a specific constraint. Based on a general $K$-Lipschitz continuity assumption, we derive the generalization error upper bound of the IF-based methods, indicating that eliminating the distribution incoherence can improve the generalizability of IF-based MML methods. Building upon these theoretical insights, we establish a novel IF-based MML method, which introduces the informatic constraint and performs distribution cohering. Extensive experimental results on multiple widely adopted datasets verify the effectiveness of the proposed method.", "tldr": "", "keywords": ["Multimodal representation learning; Generalization error;  Informatic constraint; Distribution cohering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d5446ebb98ce2f97ab7e6e0353d34316c9751e43.pdf", "supplementary_material": "/attachment/5a4d83908dc7478befebee2a67684e8384f96dab.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides a theoretical analysis of multimodal representation learning (MML) with a focus on intermediate fusion (IF) and late fusion (LF) frameworks. Unlike prior studies centered on LF, the authors theoretically justify the superiority of IF under certain constraints and derive a generalization error bound based on a K-Lipschitz continuity assumption. They further propose an IF-based MML method that introduces an informatic constraint and enforces distribution coherence, achieving strong empirical results on multiple benchmark datasets. However, some parts of the theoretical derivation appear to rely on assumptions that may require further clarification or justification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel approach to multimodal representation learning (MML) by integrating theoretical analysis with practical model design. It provides a fine-grained theoretical investigation of intermediate fusion (IF) and late fusion (LF) frameworks, deriving a generalization error bound under a K-Lipschitz continuity assumption. Based on these theoretical insights, the authors propose an IF-based MML method that introduces an informatic constraint and performs distribution cohering to reduce distribution incoherence. This theory-driven design is validated through extensive experiments on multiple benchmark datasets, showing improved performance consistent with the theoretical predictions. The approach is innovative in that it connects theoretical guarantees with concrete architectural and training strategies, offering both conceptual and empirical contributions to the field."}, "weaknesses": {"value": "While the paper presents a novel theory-driven approach to multimodal representation learning, there are some concerns regarding the theoretical assumptions. In particular, the analysis assumes that each dimension of the latent features can be strictly partitioned into task-dependent semantics and task-independent noise. This assumption provides a clear analytical framework but may be overly strong or unrealistic in practical deep learning settings, where latent features are often entangled and do not exhibit such a clean separation. As a result, some of the theoretical guarantees derived under this assumption might not fully hold in practice. Additionally, the theoretical derivations build upon prior results rather than introducing entirely new theorems, so the novelty in the theoretical contributions is somewhat incremental. Nevertheless, the empirical results indicate that the proposed IF-based method, with its informatic constraint and distribution cohering, effectively improves performance and is consistent with the theoretical intuitions, partially mitigating the concerns about the assumptions."}, "questions": {"value": "1. The theoretical analysis assumes that each latent feature dimension can be strictly partitioned into task-dependent semantics and task-independent noise. Is there any evidence in the existing experimental results that indirectly supports this assumption?\n\n2. In the proof, it is unclear how the first equality in Equation 31 connects the empirical error to the expected error. Could the authors clarify the assumptions or steps that justify this equality? Specifically, what conditions are required for this transition, and are they satisfied in the current setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ggcqo8xYpA", "forum": "5bxmmuRhO6", "replyto": "5bxmmuRhO6", "signatures": ["ICLR.cc/2026/Conference/Submission4424/Reviewer_5Xqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4424/Reviewer_5Xqu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760954110544, "cdate": 1760954110544, "tmdate": 1762917357863, "mdate": 1762917357863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-examines two mainstream fusion approaches in multimodal learning‚ÄîIntermediate Fusion (IF) and Late Fusion (LF)‚Äîfrom a fine-grained perspective. It demonstrates that IF outperforms LF under specific constraints and further derives the generalization error upper bound of the IF method. Based on theoretical analysis, it proposes an IF method that incorporates information constraints and distribution consistency. Experimental results on a large number of datasets verify the effectiveness of this method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1ÔºâThe motivation is clear and the research significance is prominent. The paper identifies the issue that current theoretical research on multimodality mostly focuses on LF, while there is insufficient theoretical analysis on IF, and possesses a clear motivation to fill this gap.\n\n2ÔºâThe theoretical analysis is in-depth. It provides a comparison between IF and LF from the perspective of fine-grained dimensions, derives the generalization error upper bound, and makes significant theoretical contributions.\n\n3ÔºâThe ablation experiments are comprehensive and validate the necessity of the modules. Ablation experiments are conducted on the two core modules (information constraints and distribution consistency) to verify their respective effectiveness."}, "weaknesses": {"value": "1ÔºâThe experimental comparisons have gaps, which weakens the persuasiveness of the method's innovation. Current experiments mainly compare IID with LF-based SOTA methods (e.g., PDF, QMF) or general fusion models. However, direct comparisons with advanced methods specifically designed for the IF framework in recent years are lacking. It is suggested that the authors supplement relevant experiments, as such comparisons can more clearly highlight the unique advantages and contributions of IID under the specific paradigm of \"Intermediate Fusion\".\n\n2ÔºâThe analysis of computational complexity is insufficient. Although the paper identifies the computational bottleneck of high-dimensional Wasserstein distance and proposes an efficient dimensionality reduction method, it lacks quantitative analysis of the computational overhead of the overall IID model (including the two newly proposed modules) during training and inference. This makes it difficult for readers to comprehensively evaluate the efficiency of the method in practical applications."}, "questions": {"value": "See the Weaknesses section for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "79tTBEN9x8", "forum": "5bxmmuRhO6", "replyto": "5bxmmuRhO6", "signatures": ["ICLR.cc/2026/Conference/Submission4424/Reviewer_coeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4424/Reviewer_coeM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654783744, "cdate": 1761654783744, "tmdate": 1762917357591, "mdate": 1762917357591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for multimodal representation learning, with a theoretical analysis centered on intermediate and late fusion mechanisms. The proposed theory further leads to the implementation of the IID method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Starting from a theoretical perspective, this paper systematically compares Intermediate Fusion (IF) and Late Fusion (LF) from a dimensional viewpoint, and provides rigorous proofs for both prediction error and generalization error.\n\n2.The paper is well-structured and written in a professional academic style."}, "weaknesses": {"value": "1.The errors in Figures 1 and 5 need to be corrected.\n\n2.In the case of Intermediate Fusion (IF), it is generally assumed that Concat and Sum are equivalent, which often leads to the issue of modality laziness. However, the authors did not take this into account.\n\n3.The notations are confusing. For instance, in the figures, w is shown as a vector, whereas in the main text it appears to be used as a scalar. It is recommended to include a notation table for clarification.\n\n4.Although module D is theoretically derived, it appears to be meaningless from an empirical perspective, as the hyperparameter experiments (Œ≤ ‚àà {1e‚àí10, ‚Ä¶, 1e‚àí14}) suggest negligible effects. The rationale for including module D should be further explained.\n\n5.The proposed method is based on Intermediate Fusion (IF), yet the comparison with other methods is insufficient and somewhat unfair. Many advanced approaches within the IF framework are not considered.\n\n6.The study lacks experiments involving additional modalities."}, "questions": {"value": "1.In Table 2, the Concat method consistently outperforms IID-L. Does this indicate that modules I and D are not meaningful or contribute little to the model‚Äôs performance?\n\n2.Taking PDF as an example, within the IID-P framework, larger weights are assigned to the dominant modality while smaller weights are given to the weaker ones. Consequently, the dominant modality converges faster whereas the weaker modality remains under-optimized. In this context, is it reasonable for module D to reduce the distribution discrepancy between features of well-converged and under-optimized modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bdJ2FZBESq", "forum": "5bxmmuRhO6", "replyto": "5bxmmuRhO6", "signatures": ["ICLR.cc/2026/Conference/Submission4424/Reviewer_Xdxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4424/Reviewer_Xdxj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911255057, "cdate": 1761911255057, "tmdate": 1762917357344, "mdate": 1762917357344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores why Intermediate Fusion (IF) outperforms Late Fusion (LF) in multimodal learning. The paper analyzes: (i) an existence result showing that IF performs no worse than LF under linear target mappings; and (ii) the generalization bound of IF under the K-Lipschitz classifier, where the distribution inconsistency between the feature distributions of each modality and the fusion distribution determines performance. Based on this, the authors propose the IID algorithm, which combines information constraints and distribution consistency. In six benchmark tests (visual language, RGB-D), IID outperforms the robust LF baseline and its IF variant; ablation experiments show that both modules contribute to the performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors prove that combining features from different modalities before classification (IF) is no worse than classifying them individually before weighting (LF). They then provide an upper bound on the generalization error, highlighting that the key factor affecting IF performance is the inconsistency between the feature distributions of different modalities and the combined distribution.\n2. The authors use FFT sparsification, RIP projection, and unbalanced OT (Sinkhorn with KL relaxation) to approximate Wasserstein distances, the method is computationally practical and achieves consistent improvements across six benchmarks; ablations indicate both modules contribute."}, "weaknesses": {"value": "1. The existence result IF better than LF assumes the target mapping ùëî is linear. In practice, post-fusion heads are often nonlinear. Can the claim be extended to nonlinear but K-Lipschitz heads? Please also include an experiment comparing a linear head vs. a two-layer MLP head to delineate the applicable regime.\n2. The method minimizes unconditional (marginal) discrepancies between modalities, whereas decisions depend on class-conditional distributions. Under what conditions does shrinking marginal discrepancy guarantee per-class alignment and improved decision boundaries?\n3. The bounds in the method involve the distance from each modality to the fusion distribution , but the objective function minimizes the distance between pairs of modes. Please specify under what assumptions minimizing the pairwise objective function is equivalent to the objective term from each mode to the fusion distribution.\n4. The objective maximizes $I(z;y)-\\sum_{m=1}^{M} I(z_m;y)$ and minimizes pairwise modality discrepancies, yet neither term explicitly penalizes cross-modal redundancy; indeed, alignment may amplify label-irrelevant shared patterns.\n- **(i)** How does IID prevent reinforcing such redundant signals?\n- **(ii)** It's better to refer to this paper for handling redundant information in related works.\n- [1] X. Xiao, ‚ÄúNeuro-inspired information-theoretic hierarchical perception for multimodal learning,‚Äù in Proc. 12th Int. Conf. Learn. Represent., 2024, pp. 1‚Äì29."}, "questions": {"value": "Same as mentioned in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KQpA5n956I", "forum": "5bxmmuRhO6", "replyto": "5bxmmuRhO6", "signatures": ["ICLR.cc/2026/Conference/Submission4424/Reviewer_Daym"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4424/Reviewer_Daym"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995188340, "cdate": 1761995188340, "tmdate": 1762917357166, "mdate": 1762917357166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}