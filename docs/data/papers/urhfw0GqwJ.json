{"id": "urhfw0GqwJ", "number": 4364, "cdate": 1757666629824, "mdate": 1759898037247, "content": {"title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts", "abstract": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: **hierarchical compliance** under **instruction conflicts** (system–user, peer–peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) **Diagnose** - *Contextualized Role Adherence Score* (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) **Localize** - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) **Align** - *Surgical Alignment of Instruction Layers (SAIL)*, which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning. The code is available at [https://anonymous.4open.science/r/DLA-ICLR-6DF6/](https://anonymous.4open.science/r/DLA-ICLR-6DF6/).", "tldr": "", "keywords": ["LLM", "LLM Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86e76dd6dd970afc339ec14d426b2a5e2a6426b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a complete framework for diagnosing conflicting commands in multi-agent systems, locating the key part of the model that resolves these conflicts, and fixing it through an efficient \"surgical\" fine-tuning method that makes the entire system more reliable in the face of complex and contradictory commands while avoiding costly and time-consuming retraining of the entire model. This makes the entire system more reliable in the face of complex and contradictory instructions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Multi-agent system diagnosis is a significant problem when applied to real-world scenarios. In this paper, the author proposes a three-step solution for identifying, locating, and improving LLMs for a multi-agent system, making it a meaningful candidate for publication in the conference.\n- The author's experiment does not involve any closed-weight language model, making the result completely reproducible.\n- Introducing a new metric called CRAC for evaluating the multi-agent system trajectories, which brings interpretability to the multi-agent system diagnosis."}, "weaknesses": {"value": "- Lacking implementation details. Please see the questions.\n- The author proposed an LLM-based evaluation metric called CRAS for the preference evaluation, but there is no ablation study on CRAS to show the correlation between CRAS and human evaluation, only the correlation between CRAS and downstream performance.\n- In `Section 3.2`, the author builds a parameter localization method upon CRAS by comparing the CRAS difference between a non-conflicted and a conflicted prompt (`Equation (8)`). But according to `Figure 2c`, the author shows that the CRAS is positively correlated with the downstream performance. This raises the question about the necessity of CRAS: why can't we just sample a few examples using two different prompts and use their average performance instead of CRAS? It's unclear how many benefits CRAS brings to the localization and the following SAIL. Therefore, the author should add a comparison between using CRAS and the average accuracy.\n- Based on the localization method introduced in `Section 3.2`, the author further proposes a surgical fine-tuning method by adding a LoRA layer to the located conflicting transformer layer. However, the best performance of LoRA has been demonstrated by many works that happen when adding LoRA layers to all transformer layers. In this paper, there are no experiments of: (1) fine-tuning all layers with LoRA and the author's SAIL method, (2) full parameter SFT for the selected layer, and (3) the cost/learning dynamics/training time between these methods. Lacking these experiments significantly decreases the soundness of this paper."}, "questions": {"value": "- What is the backbone model you used for CARS evaluation, ie, the model parameterized by $\\theta_{\\text{gen}}$?\n- What are `Figure 7: model CRAS sensitivity to learning rate` and `Figure 8: Model CRAS sensitivity to LoRA rank` for? Does the CRAS model also evolve during training?\n- For the paper arrangement, I would suggest reducing the use of bolding and putting Figure 6 beside Figure 9 for better readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CYvPDTjV2l", "forum": "urhfw0GqwJ", "replyto": "urhfw0GqwJ", "signatures": ["ICLR.cc/2026/Conference/Submission4364/Reviewer_Y2Xc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4364/Reviewer_Y2Xc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760479968047, "cdate": 1760479968047, "tmdate": 1762917317779, "mdate": 1762917317779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DLA to improve the reliability and alignment of MAS especially under instruction hierarchies where system level directives may override or conflict with agent level goals."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clarity: the pipeline is clear and key metrics/quantities are formally defined. It’s well-written and easy to understand.\n2. Significance: The results shows consistent CRAS gains and frequent ACC gains across different backbones and MAS frameworks."}, "weaknesses": {"value": "1. Evaluation: While the paper reports ACC on MMLU, SciBench, GPQA, and MedQA to demonstrate that SAIL does not harm general capability, these benchmarks are not designed to measure hierarchical or role-conflict reasoning. Thus, improvements in CRAS primarily reflect optimization toward the internal rubric rather than validated gains on real hierarchical tasks. ACC only confirms the model’s base competence remains decent, not that the claimed alignment transfers beyond CRAS. I would recommend authors to check related benchmarks not under the context of MAS as a reference, such as [1] [2] to see what other metrics can be used to show the success of your method. This seems to be a very fundamental flaw of this paper.\n2. LLM as a judge for CRAS can be bittle: the rubrics in CRAS are prompt-based in nature. And there are many prior works shows that these types of methods can be biased [3,4].\n3. Novelty: The alignment loss is explicitly token-weighted DPO-style with LoRA adapters, i.e., an adaptation of DPO and LoRA rather than a novel training objective or adapter mechanism. For conflict sensitivity analysis, there are potentially concurrent works such as [5] and it’s good to have some discussion can compare the conclusion with the one in the paper.\n\n[1] Geng, Yilin, et al. \"Control illusion: The failure of instruction hierarchies in large language models.\" arXiv preprint arXiv:2502.15851 (2025).\n\n[2] Zhang, Zhihan, et al. \"IHEval: Evaluating language models on following the instruction hierarchy.\" arXiv preprint arXiv:2502.08745 (2025).\n\n[3] Li, Songze, et al. \"LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge.\" arXiv preprint arXiv:2506.09443 (2025).\n\n[4] Li, Haitao, et al. \"Llms-as-judges: a comprehensive survey on llm-based evaluation methods.\" arXiv preprint arXiv:2412.05579 (2024).\n\n[5] Zeng, Siqi. \"Dissecting Role Conflicts in Instruction Following.\" Mechanistic Interpretability Workshop at NeurIPS 2025."}, "questions": {"value": "See weaknesses above, and 1 more question:\n1. The conflict datasets in D.1 cover seven synthetic categories. Could you clarify how representative these conflicts are of real multi-agent innteraction and what are the principles to choose them? Do you plan to evaluate on the performance on held out conflict types, such as conflicting tool actions when two agents both attempt to write to or modify the same shared file overwriting each other’s output yet these agents have different hierarchical roles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JauvGBDGTS", "forum": "urhfw0GqwJ", "replyto": "urhfw0GqwJ", "signatures": ["ICLR.cc/2026/Conference/Submission4364/Reviewer_MSs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4364/Reviewer_MSs9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967498477, "cdate": 1761967498477, "tmdate": 1762917317433, "mdate": 1762917317433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a three-stage (Diagnose-Localize-Align) framework for enhancing reliability in LLM-based multi-agent systems (MAS) under instruction conflicts, introducing the Contextualized Role Adherence Score (CRAS) for diagnosis, identifying conflict-sensitive middle layers via attention drift analysis, and developing Surgical Alignment of Instruction Layers (SAIL) for targeted optimization. It validates the framework across benchmarks (e.g., MedQA, SciBench) and MAS frameworks (e.g., AutoGen), showing improved instruction hierarchy compliance without full-model finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: Combines diagnostic scoring, attention-based localization, and targeted alignment into a cohesive pipeline for MAS instruction conflict resolution.\n2. Quality: Validates results across diverse backbones, datasets, and MAS frameworks, ensuring generalizability.\n3. Clarity: Structures the three-stage framework logically, with clear definitions of key components (CRAS dimensions, SAIL’s LoRA deployment).\n4. Significance: Addresses a real-world barrier to reliable MAS deployment, providing actionable tools (CRAS, SAIL) for practitioners."}, "weaknesses": {"value": "1. Relies heavily on existing techniques with incremental adjustments, lacking breakthrough innovations in alignment or localization.\n2. Ablation studies for reward mechanisms (Table 2) do not analyze the patterns or reasons that \"Constant Reward\" underperforms beyond surface observations.\n3. The CRAS rubric’s programmatic generation lacks step-by-step transparency, making replication challenging.\n4. Fails to test the framework on long-horizon or real-world MAS tasks, limiting evidence of practical scalability.\n5. Provides no quantitative analysis of how frequently \"instruction conflicts\" occur in real MAS deployments, raising doubts about the problem’s prevalence and the framework’s broader applicability."}, "questions": {"value": "Address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8cu0sUUNDb", "forum": "urhfw0GqwJ", "replyto": "urhfw0GqwJ", "signatures": ["ICLR.cc/2026/Conference/Submission4364/Reviewer_kYLX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4364/Reviewer_kYLX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988307550, "cdate": 1761988307550, "tmdate": 1762917317109, "mdate": 1762917317109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive three-stage framework (\"Diagnose, Localize, Align\") designed to enhance the reliability of LLM-based multi-agent systems (MAS), with a specific focus on resolving instruction conflicts that emerge between system-level and user-level directives. The key contributions include the Contextualized Role Adherence Score (CRAS), a context-aware and multi-dimensional metric for evaluating agents' adherence to role and instruction hierarchies; an attention-head/layer localization analysis that identifies the internal model regions responsible for instruction arbitration under conflict; and Surgical Alignment of Instruction Layers (SAIL), a parameter-efficient fine-tuning method that integrates LoRA adapters exclusively into attention focal layers identified through attention drift analysis, optimizing a weighted focal-head preference loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important problem in LLM-based multi-agent systems — role adherence under instruction conflicts — which extends beyond traditional single-agent instruction following, offering both practical relevance and academic value.\n2. The proposed method is validated across multiple benchmarks (MMLU, SciBench, GPQA, MedQA) and MAS frameworks (Dylan, MacNet, AutoGen, SelfConsistency), with comprehensive experimental comparisons."}, "weaknesses": {"value": "1. Although CRAS employs a well-defined rubric and prompting strategy, it still relies on another LLM evaluator for scoring, which may introduce evaluation bias or inconsistency.\n2. Artificial definition of “conflict”:The conflict dataset is generated via templates covering seven conflict types. While systematic, it may not fully capture the complexity of real-world multi-turn dialogues, limiting generalization.\n3. Lack of theoretical rigor: Despite formal notation, the paper lacks theoretical guarantees — e.g., there is no formal proof of SAIL’s optimality or convergence, nor justification for why focal-layer adaptation does not compromise global model capacity.\n4. Limited coverage of dynamic or long-horizon interactions: The work primarily focuses on static roles and single-turn conflicts, without addressing multi-turn dialogues, dynamic role switching, or long-term memory effects."}, "questions": {"value": "1. Since CRAS evaluation fully depends on another LLM as the scorer, have you conducted human evaluation or cross-model validation to verify its consistency and reliability?\n2. Your attention drift analysis suggests that instruction arbitration predominantly occurs in mid layers, which is an intriguing finding. Do you have any theoretical explanation or prior hypothesis supporting this phenomenon?\n3. Given that CRAS depends on a rubric and prompt design, to what extent is it sensitive to prompt variation? Have you tested CRAS stability across different prompt formulations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ifCW8LE6qz", "forum": "urhfw0GqwJ", "replyto": "urhfw0GqwJ", "signatures": ["ICLR.cc/2026/Conference/Submission4364/Reviewer_tS9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4364/Reviewer_tS9D"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988986140, "cdate": 1761988986140, "tmdate": 1762917316871, "mdate": 1762917316871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}