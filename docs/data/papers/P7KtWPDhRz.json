{"id": "P7KtWPDhRz", "number": 9250, "cdate": 1758116306742, "mdate": 1759897735337, "content": {"title": "Narrative Knowledge Weaver: A Multi-Agent Framework for Knowledge Graph Construction and Analysis from Complex Narratives", "abstract": "Long-form narratives such as screenplays and novels present rich but challenging material for knowledge extraction: characters evolve, events intertwine, and causal chains unfold across hundreds of pages. Existing large language model (LLM) approaches to knowledge graph construction often capture only surface-level entities and relations, leaving narrative structures underexplored. We introduce Narrative Knowledge Weaver, a multi-agent framework that integrates schema induction, reflection-driven extraction, and event-centric refinement to build coherent graphs from complex narratives. Our system leverages graph probing to induce task-specific schemas, employs reflection loops for stable entity and relation extraction, and promotes events into structured representations that support causal and plot-level analysis. We evaluate Narrative Knowledge Weaver across NarrativeQA and a complementary collection of practitioner-style screenplays, showing consistent improvements in entity disambiguation, relation accuracy, and event causality inference over strong baselines. Beyond quantitative gains, the resulting graphs enable interpretable queries about narrative dynamics—from character states to causal chains—demonstrating how LLM-based graph construction can transform long-form stories into structured, analyzable knowledge.", "tldr": "Narrative Knowledge Weaver: a multi-agent framework that builds narrative knowledge graphs with schema induction, disambiguation, and event-centric refinement, boosting QA over screenplays and novels.", "keywords": ["Knowledge", "Graph", "LLM Agent", "Retrieval-Augmented Generation", "Multi-Agent Systems"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26bed04e2606bc7dd7b9979f08703ba4e080c487.pdf", "supplementary_material": "/attachment/6f80bfada9a3aa78191453346ca26404970058cf.zip"}, "replies": [{"content": {"summary": {"value": "this paper introduces Narrative Knowledge Weaver framework which embeds a multi-agent system for building and reasoning over knowledge graphs extracted from long-form narratives like novels or screenplays. It integrates reflection-driven extraction, adaptive schema induction, and event-centric graph refinement to create coherent, causal, and interpretable representations of stories. Experiments on NarrativeQA and screenplay benchmarks show its advantages in entity disambiguation, relation accuracy, and narrative question answering over baseline LLM and retrieval systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper tackles a genuinely hard and under-explored problem: how to turn long, messy narratives—novels, screenplays—into structured knowledge graphs that preserve coherence and causality. The multi-agent setup is a wise and original move, and the combination of reflection loops, adaptive schema induction, and event-centric refinement are also well designed. \n\nEmpirically, the work is thorough. The authors run solid comparisons on both public and custom datasets and include ablations that tell a story about which parts of the system matter. The results make sense—better recall, cleaner relations, and clear QA gains. The writing is clear and the figures help; it’s easy to follow the logic from motivation to experiments."}, "weaknesses": {"value": "Some parts of the system—especially the Graph Probing Agent and reflection loops—feel more like engineering heuristics than rigorously analyzed methods.\n\nThe new “Practitioner Screenplay QA” benchmark is promising but too small and language-specific to fully support the generality claims. Results on causal reasoning could be more convincing with human evaluation.\n\nThe interaction between agents is a black box—some transparency about how reflection feedback is shared or reconciled would make the framework easier to reproduce."}, "questions": {"value": "1. How well does the system scale to extremely long narratives (hundreds of thousands of tokens)? \n\n2. Does the induced schema generalize across genres or narrative styles, or does it need retraining for each domain?\n\n3. How effective is the SABER pruning compared to simpler cycle-breaking methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RGGATHjysr", "forum": "P7KtWPDhRz", "replyto": "P7KtWPDhRz", "signatures": ["ICLR.cc/2026/Conference/Submission9250/Reviewer_4hCa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9250/Reviewer_4hCa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719583286, "cdate": 1761719583286, "tmdate": 1762920902993, "mdate": 1762920902993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of improving LLM understanding of long form narratives (i.e. novels and screenplays). Unlike existing knowledge graph extraction techniques which struggle with modeling long-term dependencies, this work explores a multi-agent approach for decomposing narratives into easy to interpret knowledge graph structures. They rely on expert agents to extract entities and relations, finetune the extracted schema via self reflection. They also introduce pipelines to normalize/disambiguate the set of all entities and remove duplicates. Finally, they also propose an event-centric graph which can be used to model higher level relationships of plot elements. They test this method across knowledge graph creation and question-answering."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed problem of answering questions about long-form narratives is interesting and would be important as LLMs may be used for more complex literary tasks. For that reason, I think that the problem is relatively well-motivated. Additionally, the paper presents a major implementation effort for improving performance on these tasks, drawing upon multi-agent framework. The use of multiple agents is promising for completing complex reasoning tasks using a large context. The provided results show significant improvement which validates their methodology. The results are particularly interesting as they evaluate on a dataset of screenplays -- going beyond existing benchmarks. They also perform some ablations of their proposed method, which validates the design choices they make. Overall, they describe their method and evaluation in careful detail allowing their results to be properly understood."}, "weaknesses": {"value": "**Presentation and Clarity** In my opinion, this paper was slightly difficult to read. The bulk of the paper is concentrated on defining the implementation details of their method. While the thoroughness of their explanation is certainly commendable, I believe that the authors could better structure this and judiciously defer certain details to the appendix. Otherwise, it is quite difficult for a reader to parse which are the higher-level contributions/insights of their framework and what are lower-level design choices/implementation details. They could also do a better job of providing a high-level architecture of their framework which actually reflects the specific components that they mention in the remainder of the paper.\n\nFor example, in the beginning of Section 3, they define their architecture as:\n> (i) Data Preprocessing, which handles metadata extraction and discourse-aware chunking; (ii) Multi-Agent Knowledge Processing, where specialized agents conduct schema induction, reflective extraction, and entity disambiguation; (iii) Hybrid Storage, integrating graph and vector stores; and (iv) the Application Layer\n\nHowever, this does not match up with the (sub)sections that are actually present in the remainder of Section 3.  For example,  they have a > Section 3.3  EVENT-CENTRIC GRAPH REFINEMENT \nwhich is not present in the initial summary of the architecture. Similarly, they never address their final two stages (Hybrid Storage and Application Layer) in Section 3. Given the complexity of their method, I believe that this makes the paper significantly harder to parse than necessary. Additionally, they at various points introduce multiple terms and use them inconsistently \n> AGENT-BASED KNOWLEDGE EXTRACTION AND REFINEMENT \nv.s. \n> ITERATIVE KNOWLEDGE EXTRACTION WITH REFLECTION\n\n\n**Generality of the Proposed Method**\nAlthough the authors discuss the specific implementation details, it raises the question of how general the contributions of this work is. For example,  the Event Graph modules appears to be heavily engineered to stories/screenplays etc. However, would a similar module work on other types of texts (for example nonfiction). The proposed method seems over-engineered in some sense and evaluation of the paper would be improved by addressing in what ways the contributions of the paper are applicable more generally."}, "questions": {"value": "Please respond to the points mentioned in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yvr8K0nQEn", "forum": "P7KtWPDhRz", "replyto": "P7KtWPDhRz", "signatures": ["ICLR.cc/2026/Conference/Submission9250/Reviewer_pXmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9250/Reviewer_pXmA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879598607, "cdate": 1761879598607, "tmdate": 1762920902555, "mdate": 1762920902555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent framework for extracting and reasoning over knowledge from long-form narratives. The system converts narrative texts (e.g., novels or screenplays) into structured knowledge graphs and event plots through a reflection-based, multi-stage pipeline including schema probing, entity normalization, event refinement, and the SABER algorithm for event linking."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive system design: The multi-agent framework with specialized roles (Graph Probing, Extraction, Disambiguation, Query agents) is well-architected and addresses different aspects of narrative understanding systematically.\n2. Novel contributions for narrative-specific challenges: The Event Plot Graph (EPG) construction and SABER pruning algorithm represent interesting approaches to handling causal relationships and plot structures in narratives.\n3. New benchmark contribution: The Practitioner Screenplay QA benchmark with domain expert annotations adds value to the community."}, "weaknesses": {"value": "1. While the system integration is comprehensive, individual components largely combine existing techniques (reflection loops, multi-agent coordination, RAG). The core innovations (SABER, EPG) are relatively incremental.\n2. The large gains are primarily on the authors' own benchmark, raising concerns about generalizability.\n3. Comparisons are against zero-shot LLM and EDC (2024), but no comparison with recent strong systems like GraphRAG, KAG, or other 2025 methods mentioned in related work.\n4. The pipeline involves numerous LLM calls with reflection loops and multiple agents. No analysis of computational costs, latency, or scalability is provided."}, "questions": {"value": "1. What is the total computational cost (# of LLM calls, processing time) for a typical screenplay? How does this scale with document length?\n2. If possible to provide comparisons with GraphRAG, KAG, or other recent (2024-2025) knowledge graph construction methods?\n3. How was the 89.8% score on the Screenplay QA benchmark judged? What LLM-judge prompt and inter-annotator agreement (IAA) were used? Have you validated the judge model with human evaluation or cross-model verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KcI1UHapCu", "forum": "P7KtWPDhRz", "replyto": "P7KtWPDhRz", "signatures": ["ICLR.cc/2026/Conference/Submission9250/Reviewer_FG1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9250/Reviewer_FG1o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902294297, "cdate": 1761902294297, "tmdate": 1762920902261, "mdate": 1762920902261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce \"Narrative Knowledge Weaver,\" a multi-agent framework designed to construct knowledge graphs from long-form narratives such as screenplays and novels. The system consists of four main components: a data preprocessing layer that performs discourse-aware text chunking and metadata extraction; a multi-agent knowledge processing layer with specialized agents for schema induction, entity/relation extraction with reflection loops, and entity disambiguation; a hybrid storage layer combining graph and vector databases; and an application layer for question answering. The authors include a Graph Probing Agent that induces narrative-specific schemas, a reflection-driven extraction mechanism where agents iteratively refine their outputs, and an Event Plot Graph construction pipeline that uses their SABER algorithm to prune causal relationships and promote event chains into higher-level plot structures. \nThe authors evaluate their system on two benchmarks, a subset of NarrativeQA and a new proposed benchmark consisting of 5 Chinese screenplays. They further provide ablation results over the graph probing schema-induction step and the Event-centric Graph Refinement module."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I do think there’s alot of things to like about this paper:\n\n- I like the idea of turning narrative conceptual extraction into a multi-agent task. This makes sense and is well motivated. \n- I also appreciate the level of detail about the system in both the main paper and the appendix (I have some other thoughts about this too that I will elaborate on in ‘weaknesses’). \n- SABER is an interesting approach for pruning cycles in causal graphs in leveraging an LLM to identify which connections are more ‘narratively’ relevant and should be preserved vs less important connections. \n- The framework and all of its parts have some sort of evaluation at each step\n- The Practitioner Screenplay QA Benchmark, but this is also not so good for its own reasons (see weaknesses)"}, "weaknesses": {"value": "Weaknesses:\n- The actual model used for evaluation isn’t present in the main paper. This gets referenced in the appendix (Qwen3-235B-A22B-FP8) but this is definitely not something that should be only found in the appendix.\n- While I like the idea and motivation behind the Practitioner Screenplay QA Benchmark, I have mixed feelings about this not being released alongside the paper. As is, the results within the paper are not reproducible.\n- Lack of statistical rigor in that it appears the results reported are only done over 1 seed (unless I missed this somewhere) but given the complexity of the system this is more of a ‘not having this isnt a demerit but having it would have been better’.\n- No real investigation about where this system fails and why\n- The authors dont evaluate against other knowledge graph construction methods, despite citing them. I would’ve liked to see at least one or two other baselines rather than just zero-shot and EDC.\n- Related to the above: the model for the zero-shot experiment is not mentioned, and if it is the same as the underlying system, I would have liked to see some other closed-source (or just another model) evaluation."}, "questions": {"value": "- The tool usage analysis in the appendix (i really like this but understand why it wasnt included in main paper) shows many tools are used less than 2% of the time. Was there any investigation as to what happened if these tools were removed?\n- What model was used for the zero-shot baseline?\n- Will the Practitioner Screenplay QA be fully released\n- Why not include other KG construction methods as baselines, such as those in the related works?\n- Where does the system fail and why?\n- Does performance change across genre? Does the length of the input (ie, a short novel vs something like the entire Lord of the Rings trilogy) have a significant impact on performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "elcpoizkpi", "forum": "P7KtWPDhRz", "replyto": "P7KtWPDhRz", "signatures": ["ICLR.cc/2026/Conference/Submission9250/Reviewer_spCK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9250/Reviewer_spCK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9250/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234190813, "cdate": 1762234190813, "tmdate": 1762920901821, "mdate": 1762920901821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}