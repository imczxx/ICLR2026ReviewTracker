{"id": "Jr3CfqlWnz", "number": 18909, "cdate": 1758291899174, "mdate": 1759897074016, "content": {"title": "LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents", "abstract": "Language models have shown to excel at a variety of tasks (e.g., mathematical reasoning and coding) which are fundamental to solving more general goal-oriented feedback-driven agentic problems. However, based on recent findings, two key points are evident: (a) agentic problems require a variety of skills such as long-context reasoning, planning and decision making, and efficient exploration;  (b) even large frontier models under-perform in these family of tasks, especially in problems requiring long-horizon understanding. For example, Qwen3-235B has a 44.5\\% accuracy on BFCLv3 multi-turn.  In this paper, our goal is to understand the relation between the two, by examining which skills are necessary for solving multi-turn problems. We work towards this goal using an oracle counter-factual framework that allows us to answer the question: what if the agent could leverage a specific oracle skill to achieve its goal? To enable this framework, we introduce a set of procedurally-generated game-like tasks whose complexity can be controlled. For these controlled environments, we can provide accurate oracle interventions to guide the agent towards the goal. Our findings suggest that while most interventions (e.g., planning) are generally beneficial, for some interventions the utility depends on the intricacies of the benchmark (e.g., ability to track state while iteratively modifying python lists).", "tldr": "oracle intervention framework and analysis to determine skills needed for a multi-turn LLM-based agent", "keywords": ["LLM", "multi-turn", "agents", "long-horizon"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28506b05966a43ed0e15025cb491c1176ea0b618.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To study how agents perform in long-horizon tasks given a specific oracle skill, the authors construct three text-based worlds. They also focus on long-horizon tasks for agents that can be modeled as Partially-observable Markov Decision Processing, where the oracle intervention is that  agent can recover the belief state of the POMDP accurately under such intervention. They have found out that while the skills can improve LLM's policies, the effectiveness of each skill is influenced by the model size and environment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Construction of three worlds to enable oracle skill control contributes to the research community. The worlds enable faithfully constructed oracle skills to study the behavior and performance of LLM agents, which is helpful to understand what affects LLM's performance.\n2. The finding about LLMs excel at each step but performs relatively poorly in the entire horizon is interesting."}, "weaknesses": {"value": "1. I would like to see stronger models' performance, like Qwen3-235B you have mentioned in the abstract, and also GPT-4o, maybe GPT-5. I would also like to see how o3 or o4-mini models performs. I am concerned about those tasks maybe only hard enough for small open source models (Qwen3-4b can get 86% with state tracking and planning in grid world). If this is the case, those worlds are still useful but limited.\n2. The oracle formulation accommodates hints, planning, state tracking and history pruning. Those are reasonable and common considerations. However, Since hint, planning, history pruning are somehow \"common practice\" to augment LLMs in different tasks,  I would like to see some oracle interventions that is tailored to POMDP specially other than state tracking."}, "questions": {"value": "1. Planning, State Tracking and History Pruning are common skills. Any other skills that can be considered? (Connection to Weakness 1)\n2. Would you mind providing results of Qwen3-235B/GPT-4o and o3/o4-mini/Deepseek R1 on three worlds?\n3. Can we view reasoning models (e.g. o3) as one LLM agent equipped with $O^{\\text{plan}}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KGvk7K7W6q", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_YWjF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_YWjF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852937310, "cdate": 1761852937310, "tmdate": 1762930900194, "mdate": 1762930900194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the performance gap between LLMs on single-turn tasks and their underperformance on multi-turn tasks. The authors attribute this gap to additional skills required in multi-turn settings, including long-context reasoning, planning and decision making, and state tracking. Experiments conducted in controlled environments shows that compounding errors constitute a primary source of failure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides a clear analysis of compounding errors in long-horizon tasks, showing how small step-wise mistakes accumulate to reduce overall success. It further disentangles specific agentic skills and introduces controlled environments to assess their individual contributions. Experiments across multiple skill combinations and model scales reveal that larger models can leverage longer contextual dependencies more effectively."}, "weaknesses": {"value": "1.  The proposed environments are symbolic and fully rule-defined, omitting key challenges of real-world tasks such as parsing unstructured feedback. It is therefore unclear whether the identified bottlenecks generalize to real-world tasks.\n\n2. Some of the findings have been reported in other works, which may limit the novelty of the results. For instance, recent studies on memory-augmented and planning-based agents have shown that these components can substantially influence performance. \n\n3. All experiments are conducted with the Qwen-3 model family, limiting the findings to Qwen-3’s scaling behavior. Other model families, such as Llama or Mistral, may exhibit different scaling dynamics and bottleneck characteristics."}, "questions": {"value": "1. The study reports binary success rates as the primary metric. Could the authors provide additional measures, such as the ratio of actual-to-optimal path length, to better assess the impact of agent skills?\n\n2. The results suggest that larger models can leverage longer contextual dependencies more effectively than smaller models. Could the authors evaluate how interaction history length affects performance across model scales, as shorter histories may benefit smaller models while excessively long histories could reduce performance for larger models?\n\n3. The paper notes that performance declines when in-context examples are not aligned with oracle feedback, suggesting sensitivity to prompt design. Ablation studies on prompt wording or structure could clarify this.\n\n4. The paper mentions that $O^{plan}$ provides a description of a single-turn subtask, but its precise nature is unclear. Could the authors clarify whether this corresponds to step-by-step guidance or a lower-level specification of the immediate action, and how each type affects model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VjuxPrOtdQ", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_M8ES"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_M8ES"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865874504, "cdate": 1761865874504, "tmdate": 1762930899518, "mdate": 1762930899518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LUMINA offers a principled way to analyze why LLM agents fail in complex, multi-step interactions — by decomposing agent behavior into modular skills and testing them systematically. To address this, the authors introduce LUMINA, a controlled evaluation framework using oracle counterfactual interventions: They design procedurally generated game-like environments where agent goals and task complexity can be precisely controlled. The oracle can intervene to provide specific “skills” (e.g., planning, exploration, state tracking), allowing researchers to test how each skill contributes to final performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a crucial and underexplored limitation in current LLM-based agents—their inability to maintain robust long-horizon reasoning across multiple turns. The motivation is well-grounded in empirical evidence (e.g., low success rates despite high per-step accuracy), and the authors effectively position long-horizon understanding as a distinct capability beyond standard reasoning or planning.\n\nThe introduction of an oracle counterfactual intervention framework is a major methodological strength. By isolating specific skills (e.g., planning, tracking belief state, context reformulation) and testing their contribution to success, the paper provides a systematic, interpretable way to analyze agentic competence—something rarely achieved in prior multi-turn benchmarks that often rely on end-to-end success metrics."}, "weaknesses": {"value": "While the proposed environments (ListWorld, TreeWorld, GridWorld) are carefully controlled and effective for isolating individual skills, they remain relatively synthetic and detached from widely adopted agentic benchmarks such as ScienceWorld, OSWorld, or TravelPlanner. As a result, the paper provides valuable mechanistic insight but lacks direct evidence that the identified skill bottlenecks generalize to real-world multi-turn tasks. This limitation weakens the practical applicability and external validity of the findings.\n\nThe paper’s focus is primarily diagnostic rather than improvement-oriented. Although the oracle intervention analysis yields interpretive insights, it does not translate into a clear enhancement of actual agent performance. The study stops short of proposing or validating concrete training or inference strategies that could operationalize these insights to improve long-horizon reasoning capabilities. Thus, the contribution remains more analytical than actionable."}, "questions": {"value": "as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hDMHv6shxN", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_WkEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_WkEG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969798236, "cdate": 1761969798236, "tmdate": 1762930898466, "mdate": 1762930898466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at why language agents struggle in complex environments. The authors argue that we should analyze this by breaking agent behavior down into basic modular skills, like a Debugger. To do this, they introduce LUMINA, a framework that uses progressively generated games to control task complexity. A key Idea is the use of counterfactual interventions, basically, the oracle can step in to provide specific skills (like planning or state tracking), which allows researchers to isolate exactly how much each skill contributes to the overall performance. It is a really great scientific control study."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper tries to solve and observe a critical bottleneck in LLM agents, the inability to maintain reasoning and planning over multiple turns [1]. The authors grounded this motivation in the empirical in step-level and task-level performance.\n\nThe proposed intervention framework is another innovation like XAI. Instead of just reporting failure rates, it provides a principled way to dissect why agents fail. The ability to isolate modular basic skills makes it a significant methodological step forward compared to existing multi-turn benchmarks.\n\n[1] Epperson, Will, et al. \"Interactive debugging and steering of multi-agent ai systems.\" Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 2025."}, "weaknesses": {"value": "My main concern lies in the experimental setting. I understand that environments like ListWorld and TreeWorld offer precise control; they feel quite synthetic compared to established real-world benchmarks like ScienceWorld [1] or OSWorld[2], or TravelPlanner[3]. Maybe it is more convincing that the bottlenecks identified in these tasks will necessarily generalize to messy, real-world multi-turn scenarios (That may be interesting).\n\nAdditionally, I found the work to be a bit incomplete. It does a great job diagnosing why agents fail, but it stops short of offering a solution. I was hoping to see some concrete training or inference strategies that leverage these insights to actually improve the language agent's performance. If these concerns are addressed, I will increase my score.\n\n[1] Wang, Ruoyao, et al. \"ScienceWorld: Is your Agent Smarter than a 5th Grader?.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[2] Xie, Tianbao, et al. \"Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments.\" Advances in Neural Information Processing Systems 37 (2024): 52040-52094.\n\n[3] Xie, Jian, et al. \"Travelplanner: A benchmark for real-world planning with language agents.\" arXiv preprint arXiv:2402.01622 (2024)."}, "questions": {"value": "as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hDMHv6shxN", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_WkEG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_WkEG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969798236, "cdate": 1761969798236, "tmdate": 1763662489898, "mdate": 1763662489898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This studies what specific skills make LLMs effective as multi-turn agents. The authors build an oracle intervention framework that can add idealized capabilities to an LLM policy during rollouts, such as planning hints, belief-state summaries, and context pruning. They introduce 3 generated environments—ListWorld (iterative list edits), TreeWorld (graph traversal), and GridWorld (2D navigation)—with controllable complexity and computable optimal actions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Three procedurally generated environments enable controllable complexity.  They are designed with simple action spaces and trajectory-level annotations, supporting accurate measurement of optimal actions."}, "weaknesses": {"value": "The idea of using oracle-based counterfactual interventions to dissect agent capabilities is interesting.\nHowever, I have some concerns. The three oracle modules are treated as independent switches, but they interact tightly. Oplan converts the decision into a one-step optimal subtask, inherently reducing the need for state inference or history recall. Ostate summarization may already encode most of the historical trajectory.\nAlso  the simplification of Ohistory as truncate earlier steps is questionable, making it unclear whether the observed effects are from history or from the artificial deletion of essential cues."}, "questions": {"value": "-  The abstract claims that “LLMs perform well on mathematics and code generation” are too broad; these capabilities were acquired after domain-specific fine-tuning.\n- The reported 44.5% accuracy of Qwen3-235B on the BFCLv3 multi-turn benchmark lacks citation or reproducibility details.\n- The notation alternates between (O_{state}), (O_{belief}), and (O_{context}). Consistent naming would improve readability.\n- Several grammatical errors should be fixed, e.g., *“a oracle” → “an oracle”*, *“recieves” → “receives”* (line 139)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5NNtZcO76", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_aTbk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_aTbk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018877757, "cdate": 1762018877757, "tmdate": 1762930897644, "mdate": 1762930897644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Language models have demonstrated strong performance across a wide range of tasks, such as mathematical reasoning and coding, which are fundamental to solving more general goal-oriented and feedback-driven agentic problems. However, such agentic problems require a diverse set of capabilities, including long-context reasoning, planning and decision making, and efficient exploration. Even large frontier models still underperform on this family of tasks, particularly those involving long-horizon understanding. For example, Qwen3-235B achieves only 44.5% accuracy on the BFCLv3 multi-turn benchmark. This paper aims to investigate which specific skills are essential for effectively solving multi-turn problems. To this end, it introduces an oracle intervention framework that evaluates the importance of different skills by posing counterfactual questions. The study finds that while most interventions, such as improving planning, are generally beneficial, the utility of certain interventions depends on the nuances of the benchmark, for example, the ability to accurately track state while iteratively modifying Python lists."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper designs three procedurally generated multi-turn environments, which facilitate the study of which skills have the greatest impact on agent capability.\n\nSignificance: Analyzing which skills, or combinations of skills, constitute the main bottlenecks to advancing capable multi-turn agents is highly meaningful, as it provides guidance for targeted improvements.\n\nClarity: The paper is clearly written."}, "weaknesses": {"value": "Quality: The capability improvements observed in simulation environments may not necessarily transfer to real-world settings.\n\nSignificance: Can the conclusions drawn from simulation environments be applied to benchmarks in real-world scenarios?"}, "questions": {"value": "1. How are the different agent skills defined and categorized? Why does the paper focus only on the three skills: planning, state tracking, and history planning?\n2. Are ( $O^{state}$ ) and ( $O^{belief}$ ) referring to the same concept in line 158?\n3. In the experiments comparing the impact of different skills on performance, the trajectories are multi-step. Is the specific skill intervention applied at every step of the trajectory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TCB3PryYkY", "forum": "Jr3CfqlWnz", "replyto": "Jr3CfqlWnz", "signatures": ["ICLR.cc/2026/Conference/Submission18909/Reviewer_hdG4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18909/Reviewer_hdG4"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094368661, "cdate": 1762094368661, "tmdate": 1762930887359, "mdate": 1762930887359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}