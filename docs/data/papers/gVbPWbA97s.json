{"id": "gVbPWbA97s", "number": 614, "cdate": 1756754899134, "mdate": 1759898250363, "content": {"title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams", "abstract": "Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage.\nProcessing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation.\nIn this paper, we introduce **StreamingVLM**, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. \nDuring inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. \nThis streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts.\nFor evaluation, we build **Inf-Streams-Eval**, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text.\nOn Inf-Streams-Eval, **StreamingVLM** achieves a **66.18%** win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100.\nNotably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96.\nCode will be released upon publication.", "tldr": "We present StreamingVLM, a model designed for real-time, stable understanding of infinite visual input.", "keywords": ["Machine learning", "Vision Language Model", "ML System"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43edc31811feea7ba3f9d321040cb911120aa926.pdf", "supplementary_material": "/attachment/25211eab2e63d1522a648a5122c34768d11a878d.zip"}, "replies": [{"content": {"summary": {"value": "This work presents StreamingVLM, a vision–language model trained for near-infinite video understanding. To enable low-latency inference under a fixed GPU-memory budget, the method maintains a compact KV cache that reuses (i) persistent attention-sink states, (ii) a short window of recent vision tokens, and (iii) a long window of recent text tokens. To teach the model this input pattern without training on prohibitively long contexts, the authors adopt an overlapped-chunk, full-attention SFT strategy that mimics the inference-time attention layout. They also curate a sports-focused training set and introduce a benchmark, Inf-Streams-Eval, targeting long-video understanding(Captioning) with real-time narration. On this benchmark, StreamingVLM reports gains over strong long-video baselines and GPT-4o, and it shows modest improvements on general VQA benchmarks as well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The KV cache design including persistent attention sinks, a long text window, a short vision window, plus contiguous RoPE shifting is conceptually sound and easy to implement. It delivers lower latency and reduced GPU memory usage while outperforming baselines on the proposed benchmark.\n\n\n2. The overlapped-chunk, full-attention SFT strategy is an intuitive way to teach the model the streaming cache pattern without training on prohibitively long contexts.\n\n\n3. The curated long-video captioning corpus and the Inf-Streams-Eval benchmark fill a gap for real-time, long-horizon evaluation in VLMs and should be valuable to the community.\n\n\n4. The paper is well organized and clearly written, making the method and experiments easy to follow."}, "weaknesses": {"value": "1. The choice to retain 512 sink tokens, a 512-token text window, and a 16-second vision window appears empirical and manually tuned. Moreover, the ablation supporting this setting is run primarily on a basketball-only subset, which raises concerns about domain generality. A single fixed policy may either waste KV budget in slow scenes or evict critical evidence too early in fast ones. Therefore, more adaptive and fine-grained design would be expected. \n\n2. Current KV cache eviction follows a naive FIFO rule, and there is no scoring or compression to retain semantically salient frames. This may harm long-horizon reasoning, especially for sparse-action videos. Therefore, this weakness can undermine the generalization of the proposed method.\n\n3. The proposed benchmark has leakage risk. Sports broadcasts are heavily duplicated and often reuse the same commentary audio, so near-duplicates can slip across training dataset and benchmark. The paper does not document near-duplicate filtering, making memorization of phrasing/style a real possibility that could inflate results. \n\n4. This work only shows results on SFT Qwen2.5-VL-Instruct-7B, making it hard to claim the method is architecture agnostic or base model agnostic."}, "questions": {"value": "1. From Table 5, we can see that when the values of $T_{sink}=512$ and $T_{window}=512$ in the inference stage are the same as those in the training stage, the overall performance is the best. This suggests the model is tightly tuned to a single window geometry, and performance degrades when the inference budget diverges, indicating limited robustness. What happens if you vary these values during training? Have you tried other values in SFT or using a curriculum scheme(start larger, anneal to target), then evaluating under both matched and mismatched inference windows? \n\n2. Why does StreamingVLM leverage 512 sink tokens? How sensitive is performance to reducing number of sinks​? and is there a saturation point where adding more sinks yields little or no gain?\n\n3. Table 3 does not compare StreamingVLM with Livecc-7B-Instruct, what does Livecc-7B-Instruct perform on those general VQA tasks? \n\n4. How much of the reported improvement (e.g. Table 6) is due to the streaming method (KV-reuse + contiguous RoPE + in-domain SFT) versus the in-domain SFT data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DMke7rPt1C", "forum": "gVbPWbA97s", "replyto": "gVbPWbA97s", "signatures": ["ICLR.cc/2026/Conference/Submission614/Reviewer_qjFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission614/Reviewer_qjFR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686404784, "cdate": 1761686404784, "tmdate": 1762915566756, "mdate": 1762915566756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StreamingVLM, a vision-language model framework designed for real-time, long-horizon video understanding. \n\nThis framework aligns training and inference by training on short chunks with original attention, and during inference maintaining a compact KV cache with attention sink, short visual window, and long text window.\n\nThis paper also introduces a new dataset and Inf-Streams-Eval benchmark which consisting of long sports videos with ASR and commentary annotations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.Clear motivation. The paper focuses on a real and underexplored problem—achieving real-time video understanding under limited latency and memory constraints.\n\n2. Simple and clear method. The proposed attention sink + sliding window + contiguous RoPE mechanism is simple, elegant, and demonstrates good empirical performance.\n\n3. Valuable dataset. The introduced dataset makes a meaningful contribution to the community of real-time long-video understanding.\n\n4. Clear writing and easy to follow."}, "weaknesses": {"value": "1. Experiments focus mainly on sports videos. It remains unclear how well the model generalizes to other domains such as egocentric or instructional videos.\n\n2.  Although the Inf-Streams-Eval benchmark is valuable, it relies on GPT-based judgment for scoring, which may introduce bias."}, "questions": {"value": "1. Could the authors provide more details on the data annotation and filtering process, such as examples of removed or edited segments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VSA8Mzm7KJ", "forum": "gVbPWbA97s", "replyto": "gVbPWbA97s", "signatures": ["ICLR.cc/2026/Conference/Submission614/Reviewer_tsyX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission614/Reviewer_tsyX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916045340, "cdate": 1761916045340, "tmdate": 1762915566447, "mdate": 1762915566447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to tackle that challenge of enabling vision-language model to process long video streams in real-time. To achieve that, the authors propose to to align the training and streaming inference using two main design ideas:\n* It uses a compact streaming-aware KV cache mechanism, which only keeps a small of attention-sink tokens, a text window and a short visual token window. \n* It introduces a contiguous RoPE to prevent positional drift by reindexing the tokens to stay within a bounded range. \n\nTo evaluate the algorithm, it introduced two new datasets for streaming the video understanding. \n* It introduce Inf-streams-train, a 4000-hour dataset sports commentary dataset curated using ASR and GPT. \n* A benchmark dataset Inf-Streams-Eval with good per-second alignment of frames and text to test the infinite streaming. \nThe authors provide extensive evaluations using model comparisons and show the proposed design is effective to handle long videos, and can improve step by step when incorporating the proposed mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall the paper is well motivated, and demonstrates convincing results that can advances real-time long-horizon video understanding. \n* The proposed KV-cache mechanism composed with attention-sink tokens, long text window and short vision window is effective from the evaluation, and the contiguous ROPE can further yield stable output with improved performance. \n* It further proposed a dataset that can train the model with higher quality dataset, and an evaluation mechanism to benchmark the progress. The process to create the dataset is legit and considers the important factors that limits existing datasets. If they author can share them out, it will benefit the community a lot."}, "weaknesses": {"value": "Overall the paper aces well in a number of engineering factors to make the current system solid. However, there are many design choices that embed strong heuristics in them and unclear what they will terminate at. \n* The model is trained on overlapped short video chunks and never experiences true streaming behavior with recurrent KV use. It is not quite clear whether it is \"training-inference alignment\" claimed by the author. In Table 2, it shows alignment is important (where ReKV completely fails), while it is a very differnet mechanism and not optimized for multimodal use. It could be done better if the author can show a comparison to the same model developed (but without stream-aware training), and shows the difference. \n* There are also finite token length limit for attention-sink, visual token and text token windows. It is not quite clear to me how they impact the final results if the scenario varies. \n* From the results, we can clearly see the model can already achieve really good performance compared to the baselines (GPT-4, LiveCC) even without using T_sink or T_window. I wonder how much does the base model and training on the created dataset contribute. Ideally, we want to factor out them out in performance evaluations."}, "questions": {"value": "I enumerate a few questions in the weakness part, which are mostly about how the paper improve the clarity. Hope the authors can provide me a few evidences to address them in the easiest setting, or pointing me to the right source if I missed any. \n\nThe dataset used for training and benchmark is very important part in this part to make model great and evaluation solid. I wonder whether they are available to the community in some ways. Together with the trained model, I wonder whether are open source plans."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fnJQ8BDPM4", "forum": "gVbPWbA97s", "replyto": "gVbPWbA97s", "signatures": ["ICLR.cc/2026/Conference/Submission614/Reviewer_Ep3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission614/Reviewer_Ep3A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958317779, "cdate": 1761958317779, "tmdate": 1762915566341, "mdate": 1762915566341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents StreamingVLM, a framework capable of understanding continuous visual input in real time. Specifically, the key ideas include: (1) an efficient inference scheme that maintains sink text tokens, a long window of the most recent text tokens, and a short window of the most recent vision tokens; (2) a dedicated training strategy that splits the input into overlapping chunks and trains with full attention to approximate the aforementioned efficient inference scheme; and (3) a dataset providing long-horizon data for fine-tuning and evaluation. Experiments conducted on publicly available datasets, as well as the newly created dataset, demonstrate improved real-time captioning and video understanding capabilities compared to both in-house and open-source models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Solid presentation: I especially appreciate the contribution of the newly created SFT dataset, and I found the demo video to be a convincing demonstration of the practical value of this work.\n\n2. Clear performance improvement over baselines: The improvements in captioning and video understanding are significant, supported by both qualitative and quantitative comparisons in the manuscript.\n\n3. Comprehensive coverage of prior work: The paper provides a thorough literature review and is overall well written."}, "weaknesses": {"value": "1. Clarification of differences in streaming-aware KV cache: The distinction between the proposed approach and StreamingLLM is not sufficiently clear. Based on my understanding, the main difference lies in using different window sizes and eviction strategies for text and visual tokens. It would be helpful to explicitly explain this difference and discuss whether the proposed StreamingVLM training strategy could be applied to StreamingLLM.\n2. Generalizability of hyperparameters: The window sizes for text and visual tokens are clearly important factors (as shown in Table 5). However, I am concerned that these hyperparameters may be highly task-dependent. For example, a 16-second visual token window might work well for basketball videos but may not generalize to other scenarios. Additional discussion on how to tune or generalize these hyperparameters would strengthen the work.\n3. Smaller gains in VQA compared to captioning: While the method improves performance on both VQA and captioning tasks, the gains in VQA are relatively smaller. Providing an explanation for this observation would help readers better understand the strengths and limitations of the proposed approach."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5QimyLuvDa", "forum": "gVbPWbA97s", "replyto": "gVbPWbA97s", "signatures": ["ICLR.cc/2026/Conference/Submission614/Reviewer_bkLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission614/Reviewer_bkLD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978518887, "cdate": 1761978518887, "tmdate": 1762915566242, "mdate": 1762915566242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}