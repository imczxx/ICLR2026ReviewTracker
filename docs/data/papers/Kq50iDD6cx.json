{"id": "Kq50iDD6cx", "number": 20336, "cdate": 1758304871476, "mdate": 1759896983159, "content": {"title": "From Many Imperfect to One Trusted: Imitation Learning from Heterogeneous Demonstrators with Unknown Expertise", "abstract": "Imitation learning (IL) typically depends on large-scale demonstrations collected from multiple human or algorithmic demonstrators. Yet, most existing methods assume these demonstrators are either homogeneous or near-optimal---a convenient but unrealistic assumption in many real-world settings. In this work, we tackle a more practical and challenging setting: IL from heterogeneous demonstrators with unknown and widely varying expertise levels. Instead of assuming expert dominance, we model each demonstrator's behavior as a flexible mixture of optimal and suboptimal policies, and propose a novel IL framework that jointly learns (a) a state-action optimality scoring model and (b) the latent expertise level of each demonstrator, using only a handful of human queries.  The learned scoring model is then integrated into an policy optimization procedure, where it is fine-tuned with offline demonstrations, on-policy rollouts, and a fine-grained mixup regularizer to produce informative rewards.  The agent is trained to maximize these learned rewards in an iterative fashion. Experiments on continuous-control benchmarks show that our approach consistently outperforms baseline methods. Even when all demonstrators are highly suboptimal, each exhibiting only 5-15% optimality, our method achieves performance comparable to a baseline trained on purely optimal demonstrations, despite our lack of optimality labels.", "tldr": "", "keywords": ["Imitation Learning", "Heterogeneous Demonstrators", "Optimality Estimation", "Unknown Expertise"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0766580bf4480ddc1933cc7e313da486adc73734.pdf", "supplementary_material": "/attachment/ae98c3a4aa1e057dcafb4aa8c4a1307309325ddf.zip"}, "replies": [{"content": {"summary": {"value": "The submission studies imitation learning from demonstrators with different levels of expertise, assigning each a scalar expertise value and treating trajectories as mixtures of optimal and suboptimal behavior. It introduces a two-stage pipeline: an EM-style algorithm that learns an optimality score function and the expertise levels, followed by policy training that fine-tunes the policy with stopping, mixup, and an agent-matching penalty. Stage 1 has a convergence guarantee. On MuJoCo tasks across general and very low expertise regimes, the method outperforms GAIL, RIL, and WGAIL, with ablation showing the effectiveness of relabeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem formulation is clear, and the approach is reasonable.\n- A standard convergence analysis of the EM algorithm is provided for the first stage of the proposed method.\nThe proposed method is tested on MuJoCo and Gymnasium tasks.\n- An ablation study on relabeling, early stopping, and top-k selection.\n- Two sets of experiments—the “general expertise test” and “low expertise test”—demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "- The model for the expertise level is too simple compared to (https://arxiv.org/pdf/2202.01288) mentioned by the paper. The current formulation may miss demonstrators who are experts in some regions and poor in others.\n- The criteria (line 253 and line 264) discussed after Theorem 1 make the approach ad hoc and require high quality supervision (albeit a small amount).\n- ILEED is missing from the experiment."}, "questions": {"value": "- What is the usage of alpha-hat_i in Algorithm 2?\n- Why is it fine to choose alpha’ = 0.5 (line 209)? What could a misspecified outcome be? How can the misspecified case be handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "69Ja68KjEa", "forum": "Kq50iDD6cx", "replyto": "Kq50iDD6cx", "signatures": ["ICLR.cc/2026/Conference/Submission20336/Reviewer_Hadp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20336/Reviewer_Hadp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316736120, "cdate": 1761316736120, "tmdate": 1762933794208, "mdate": 1762933794208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles imitation learning (IL) when demonstrations come from heterogeneous demonstrators of unknown and widely varying expertise. The authors model each demonstrator as a mixture policy under a latent optimality label on state–action pairs. Stage 1 learns (i) a state–action optimality scorer via surrogate demonstrator classification, and (ii) demonstrator expertise levels by an EM‑style alternating procedure. Stage 2 uses a surrogate reward to train a policy with SAC, while iteratively refining using on‑policy rollouts, a negative (agent‑matching) term, and a mixup regularizer, plus a top‑k pseudo‑labeling scheme with an early‑stopping heuristic. On MuJoCo control tasks, the method outperforms GAIL, RIL, WGAIL and approaches performance of an oracle GAIL trained on hand‑selected optimal subsets, including regimes where all demonstrators are highly suboptimal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a realistic data regime, i.e., many imperfect demonstrators with unknown quality, and proposes a concrete way to mine signal without explicit optimality labels.\n- Leveraging surrogate set classification (SSC) to recover P(z=1∣x) from multi‑set membership is a smart reduction that connects demonstrator‑ID prediction to optimality scoring with a known transform.\n- The refinement loop (on‑policy rollouts, an agent‑matching penalty, and mixup) directly targets covariate shift and over‑confidence issues that often hurt IL, and the ablations help isolate these effects."}, "weaknesses": {"value": "- The EM‑style analysis is potentially incorrect in places. In Theorem 1, the “M‑step” is written with a minimization over $\\(\\phi, \\alpha\\)$ that still plugs in $\\phi_t$, and the “E‑step” uses a hard 0.5 threshold instead of the expected posterior E[z∣s,a]. More importantly, identifiability of the expertise priors $\\{\\alpha_i\\}$ and the optimality scorer via SSC requires strong conditions (e.g., class‑conditional distributions invariant across sets and “mutual irreducibility” assumptions); these are not clearly stated in the main text, yet they are central to SSC’s guarantees. Without them, $\\alpha$ and $f_\\phi$ can be non‑identifiable or flip‑ambiguous. It it necessary to make the assumptions explicit and align the proof to standard EM or variational lower‑bound updates.\n- Eq. (1) implicitly assumes a single shared suboptimal policy for all demonstrators. In real data, different novices commit different types of errors and visit different states. SSC’s reduction typically assumes shared class‑conditionals p(x∣z) across sets; the paper’s occupancy‑measure formulation includes demonstrator‑dependent state visitation, which can violate these assumptions and bias the recovered scorer. What if each demonstrator has a different structured suboptimality with domain shift?\n- Here are places where the exposition is inconsistent or confusing: the top‑k selection text conflicts with the $f_\\phi(s, a) > 0.5$ rule (lowest vs. highest scores), Algorithm 1’s steps don’t align with the proof, and Theorem 1’s objective/updates are misstated.\n- Closely related works are not cited or are under‑discussed:\n  - AIRL for learning disentangled, portable rewards from demos [1].\n  - T‑REX/D‑REX for better‑than‑demonstrator performance from suboptimal data via ranking [2–3].\n  - VPIL for vague feedback over demos [4].\n\n## References\n\n[1] Learning Robust Rewards with Adversarial Inverse Reinforcement Learning.\n\n[2] Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations.\n\n[3] Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations.\n\n[4] Imitation Learning from Vague Feedback."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0qkKdizzOg", "forum": "Kq50iDD6cx", "replyto": "Kq50iDD6cx", "signatures": ["ICLR.cc/2026/Conference/Submission20336/Reviewer_yeCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20336/Reviewer_yeCu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709804934, "cdate": 1761709804934, "tmdate": 1762933792942, "mdate": 1762933792942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the imitation learning problem from heterogeneous demonstrators of unknown expertise, proposing a two-stage EM-style framework that jointly estimates demonstrator expertise and learns a state–action optimality scoring model, which is then used as a for policy optimization. The paper is an extension of ILEED (Beliaev et al., ICML 2022) which considered unsupervised expertise estimation for heterogeneous demonstrators. In the authors' formulation, instead of modeling state-dependent embeddings, it models demonstrator expertise as a global scalar mixture coefficient to have a more structured suboptimality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is important as heterogeneous and imperfect demonstrations are the norm in large-scale IL.\n- The paper presents a clean EM formulation, with a clear separation between expertise estimation and policy learning."}, "weaknesses": {"value": "- The novelty is quite incremental relative to ILEED. The new formulation simplifies expertise modeling from state-dependent embeddings to demonstrator-level mixture coefficients and reframes the joint estimation as a classification-based EM procedure. \n- The paper should have direct numerical comparison with ILEED as well as other baselines specifically designed for suboptimal demonstrations. The current basedlines mostly cover standard IL methods.\n- Evaluations are conducted on synthetic MuJoCo environments where suboptimality is simulated by mixing optimal and degraded SAC policies. The paper will benefit from real human demonstrations like the robomimic dataset.\n- The theoretical contribution (EM convergence) is modest and to my best knowledge a well-known proof."}, "questions": {"value": "Can you provide quantitative comparison with ILEED and stronger evidence of scalability or generalization using real human demonstrations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2KQtToMBz3", "forum": "Kq50iDD6cx", "replyto": "Kq50iDD6cx", "signatures": ["ICLR.cc/2026/Conference/Submission20336/Reviewer_JbYb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20336/Reviewer_JbYb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851296397, "cdate": 1761851296397, "tmdate": 1762933792527, "mdate": 1762933792527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses imitation learning (IL) from heterogeneous demonstrators with unknown and varying expertise levels. The authors propose a two-stage framework: (1) jointly learning demonstrator expertise levels and an optimality scoring model through an EM-style iterative procedure, and (2) using this scoring model as a surrogate reward function for policy learning with progressive refinement. The method is evaluated on MuJoCo continuous control tasks under two challenging scenarios - general expertise (0.1-0.9) and low expertise (0.05-0.15) settings. The approach claims to achieve performance comparable to oracle methods trained on purely optimal demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a realistic scenario where demonstrations come from multiple sources with unknown, heterogeneous expertise levels - a common real-world challenge.\n- Provides convergence guarantee for the EM-style optimization (Theorem 1), giving the approach theoretical grounding.\n- Thorough evaluation across multiple environments, expertise settings, and ablations. The low-expertise test (0.05-0.15) is particularly challenging and demonstrates robustness."}, "weaknesses": {"value": "- Theorem 1 only guarantees convergence to a stationary point, not optimality\n- No sample complexity analysis or bounds on expertise estimation error\n- The connection between surrogate classification accuracy and IL performance isn't theoretically characterized\n- The \"Optimality Alignment Criterion\" requires human queries, making the approach not fully unsupervised\n- The EM procedure requires multiple random initializations with variance-based selection, which could be computationally expensive"}, "questions": {"value": "- How sensitive is the method to the number of human queries? The paper uses only 5 queries but doesn't provide ablation on this critical parameter.\n- Can you provide theoretical analysis on the sample complexity? How many demonstrations are needed for reliable expertise estimation?\n- Why not compare with ILEED directly? The paper mentions it but doesn't include it in experiments despite addressing the same problem.\n- What's the computational overhead of the multiple random initializations? How many initializations are typically needed in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "di9dzQNRDL", "forum": "Kq50iDD6cx", "replyto": "Kq50iDD6cx", "signatures": ["ICLR.cc/2026/Conference/Submission20336/Reviewer_Ck5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20336/Reviewer_Ck5u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20336/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762527774465, "cdate": 1762527774465, "tmdate": 1762933791901, "mdate": 1762933791901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}