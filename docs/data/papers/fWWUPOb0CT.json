{"id": "fWWUPOb0CT", "number": 11019, "cdate": 1758187071602, "mdate": 1759897614102, "content": {"title": "SpatiaLab: Can Vision–Language Models Perform Spatial Reasoning in the Wild?", "abstract": "Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision–language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce **_SpatiaLab_**, a comprehensive benchmark for evaluating VLMs’ spatial reasoning in realistic, unconstrained contexts.\n**_SpatiaLab_** comprises 1,400 visual question–answer pairs across six major categories: *Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation,* and *3D Geometry*, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation.\nExperiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10–25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry.\nBy providing a diverse, real-world evaluation framework, **_SpatiaLab_** exposes critical challenges and opportunities for advancing VLMs’ spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. We will release **_SpatiaLab_**.", "tldr": "", "keywords": ["Spatial reasoning", "Vision–language models", "Large languge models", "Reasoning Models", "LLM Evaluation", "Spatial Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/734f5e97514373484089e63afb683ff51a81b8cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SpatialLab, a benchmark designed to evaluate spatial reasoning in vision-language models (VLMs). It features real-world images and carefully annotated questions that span a diverse range of spatial reasoning tasks. Using this benchmark, the authors assess various off-the-shelf VLMs and further analyze performance improvements achieved through different enhancement strategies, including prompt-based, multi-agent-based, and supervised fine-tuning (SFT)-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Carefully curated set of images and questions covering major spatial reasoning types.\n- Comprehensive analysis showing that even proprietary VLMs perform worse than humans across nearly all subtasks.\n- Additional image complexity analysis provides insights into which visual domains require more attention in future training datasets.\n- In-depth quantitative and qualitative error analysis.\n- Evaluation of multiple strategies for improving spatial reasoning in VLMs (prompt-based, multi-agent-based, SFT-based, etc.) — particularly valuable in Section 5.4."}, "weaknesses": {"value": "- The evaluation section (5.4) could be further strengthened by including reinforcement learning (RL)-based approaches for comparison, though this is not strictly necessary."}, "questions": {"value": "- In L1132, the authors mention “we review prior benchmarks … analyze their limitations.” Could the authors elaborate on the specific limitations identified in each benchmark? A comparison table summarizing these would greatly aid readers and inform future benchmark design.\n- [Suggestion] While page limits are understandable, brief descriptions of each improvement approach in Section 5.4 would improve clarity. For instance:\n  - What self-reflection prompt was used?\n  - Which dataset and dataset size were used for SFT fine-tuning?\n  - What base VLM was used?\n  - Could the authors provide a one-sentence description of SpatialXolver?\n- [Suggestion] Adding a table of contents to the appendix would help readers navigate the paper more easily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6B4sEBC4gV", "forum": "fWWUPOb0CT", "replyto": "fWWUPOb0CT", "signatures": ["ICLR.cc/2026/Conference/Submission11019/Reviewer_rSCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11019/Reviewer_rSCs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928844834, "cdate": 1761928844834, "tmdate": 1762922199571, "mdate": 1762922199571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPATIALAB, a comprehensive benchmark for evaluating spatial reasoning in vision–language models (VLMs) under realistic, unconstrained visual conditions. It contains 1,400 visual question–answer pairs spanning six major categories and 30 subcategories, each testing aspects like depth, occlusion, orientation, and navigation.\nThe benchmark supports both multiple-choice and open-ended formats, enabling comparison between discriminative and generative reasoning.\nExtensive experiments on 25+ models (open-source, proprietary, reasoning-tuned, and spatially specialized) reveal a substantial gap between human and model performance (e.g., 54.9% vs. 87.6% on MCQ; 40.9% vs. 64.9% on open-ended).\nThe paper provides error analysis, fine-tuning experiments, and attempts at improvement via CoT prompting, self-reflection, SFT, and multi-agent systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Comprehensive Evaluation\n- Evaluates over 25 VLMs, including open-source and proprietary systems, and human baselines.\n- Dual-format testing (MCQ + open-ended) is valuable, revealing a 20–25% accuracy gap between the two modes\n\n\nDiagnostic and Actionable Insights\n- The benchmark reveals concrete gaps (e.g., geometry-aware supervision, spatial chaining, embodied data) that can guide future research. The diagnostic perspective makes it a useful community tool even without conceptual novelty."}, "weaknesses": {"value": "Limited Conceptual Novelty\n- Many recent benchmarks (OmniSpatial, BLINK-Spatial, Spatial-MM, SpatialRGPT, EmbSpatial) already use real-world imagery, multiple spatial categories, and QA-based evaluation. SPATIALAB’s innovation lies mainly in breadth and integration, not in introducing new reasoning types or data modalities\n\nLimited Guidance on Model Improvement\n- Although weaknesses of current VLMs are carefully diagnosed, the paper offers little practical guidance or insight into how to overcome them. The discussion remains observational (what fails) rather than prescriptive (how to fix it), limiting its utility for researchers aiming to design better spatial reasoners.\n\nModerate Dataset Scale\n- Despite 1,400 QA pairs sounding large, it is relatively small compared to existing multimodal datasets (often tens or hundreds of thousands). The modest size restricts its usefulness for training or fine-tuning and confines SPATIALAB to evaluation only."}, "questions": {"value": "1. How exactly does SPATIALAB differ from OmniSpatial or Spatial-MM beyond taxonomy size and annotation detail?\n\n2. Are there reasoning types uniquely represented here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mSFuq1MnRX", "forum": "fWWUPOb0CT", "replyto": "fWWUPOb0CT", "signatures": ["ICLR.cc/2026/Conference/Submission11019/Reviewer_w6pQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11019/Reviewer_w6pQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995194300, "cdate": 1761995194300, "tmdate": 1762922199169, "mdate": 1762922199169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPATIALAB, a new benchmark designed to evaluate the spatial reasoning abilities of vision–language models (VLMs) in real-world, unconstrained settings.\n\nThe dataset comprises 1,400 visual question–answer pairs across six high-level categories (e.g., Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry) and supports both multiple-choice and open-ended evaluations.\n\nThe authors benchmark over 25 state-of-the-art models (open- and closed-source) and provide quantitative comparisons against human baselines, together with error analyses and several reasoning interventions (e.g., Chain-of-Thought prompting, supervised fine-tuning, and multi-agent reasoning).\n\nThe work aims to reveal systematic weaknesses in current VLMs’ spatial reasoning and propose SPATIALAB as a comprehensive diagnostic framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**Comprehensive empirical evaluation.** The authors test a wide range of modern VLMs under multiple evaluation formats, which provides useful diagnostic data and an updated empirical snapshot of model limitations in spatial reasoning.\n\n**Well-structured benchmark.** The dataset taxonomy (6 categories × 5 subcategories) is clearly defined and covers a broad set of spatial reasoning tasks beyond synthetic toy examples.\n\n**Clear presentation.** The paper is readable and systematically organized, with detailed tables and qualitative examples that make the results easy to interpret.\n\n**Reproducibility focus.** The authors discuss data collection, annotation, and quality control in detail and commit to open release, which is commendable for community benchmarking."}, "weaknesses": {"value": "**Lack of methodological contribution.** The work’s novelty lies almost entirely in dataset construction and large-scale evaluation.\nThere is no new modeling approach, algorithm, or analytical framework proposed.\nWhile benchmarks can be valuable, ICLR typically expects either new learning methodology, representation insights, or deeper diagnostic mechanisms beyond dataset release.\n\n**Limited depth of analysis.** Despite extensive tables, the analysis remains descriptive rather than mechanistic.\nThe paper identifies “what fails” (e.g., navigation and occlusion tasks) but not “why” in terms of representation or model architecture.\nThere are no probing studies, attention analyses, or causal/intervention experiments that explain the underlying representational failure modes.\nMany claims (e.g., “models lack geometric grounding”) are plausible but unsupported by direct evidence."}, "questions": {"value": "Could the authors please justify their contributions in the \"in-depth analysis\" and provide key takeaways from it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nIu5UHTcdV", "forum": "fWWUPOb0CT", "replyto": "fWWUPOb0CT", "signatures": ["ICLR.cc/2026/Conference/Submission11019/Reviewer_tmjh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11019/Reviewer_tmjh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762541683307, "cdate": 1762541683307, "tmdate": 1762922198732, "mdate": 1762922198732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce SpatiaLab, a new benchmark dataset of 1,400 visual question-answer pairs designed to evaluate VLM spatial reasoning in \"in-the-wild\" contexts. The benchmark spans six categories (30 subcategories) and supports both multiple-choice and open-ended evaluation formats. The authors conduct a large-scale evaluation of over 25 VLMs, revealing a substantial performance gap between SOTA models (e.g., InternVL3.5-72B at 54.93%) and a human baseline (87.57%). The analysis is extended to improvement strategies (SFT, CoT, Agents), which are shown to provide limited or inconsistent gains, suggesting current models lack fundamental spatial grounding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-Motivated Problem**: The work correctly identifies a critical flaw in existing benchmarks: an over-reliance on synthetic, \"puzzle-like\" setups that fail to capture real-world visual complexity. The focus on cluttered, \"in-the-wild\" imagery is a necessary contribution.\n\n2. **Dual-Format Analysis**: The direct comparison of MCQ and Open-ended formats is a key strength. It provides a quantitative basis for the intuition that MCQ overestimates model capabilities, highlighting a significant $\\approx$23% average performance drop.\n\n3. **Failure Analysis**: The demonstration that standard improvement techniques (CoT, SFT) provide marginal, inconsistent, or even negative gains is an important finding for the field, pointing to deeper representational deficits."}, "weaknesses": {"value": "1. **Statistical Robustness of the Benchmark**: The primary methodological flaw is the dataset's scale. 1,400 items spread across 30 subcategories means each sub-task is evaluated with a small sample (as few as 25 items, averaging <50). This $n$ is insufficient to draw robust, fine-grained conclusions. The granular analysis in Tables 5-9, while interesting, risks being statistically noisy. \n\n2. **Perplexing SFT Dynamics**: The SFT analysis (Sec 5.4, Fig 4)  is central to the paper's insight, but the results are anomalous and underexplored. The sharp U-shaped curve in open-ended performance (dropping from 34.4% to 12.6% before recovering to 35.5%)  is highly non-trivial. The paper gestures at \"catastrophic forgetting\"  but provides no direct investigation. This dynamic must be rigorously explained (e.g., via representational analysis or multi-seed validation) to be a credible scientific finding rather than a training artifact.\n\n3. **Marginal Novelty in a Crowded Field**: The paper's own related work (Table 1) demonstrates this is an extremely crowded and concurrent field (e.g., SpatialMM, OmniSpatial, BLINK, VLMAD). The claim to novelty rests on \"real-world complexity\" , yet several competitors also use \"Internet\" or \"Mix\" data sources. The authors must provide a much sharper justification for why SPATIALAB's 1,400 \"manual\" items provide fundamentally different insights than the (often larger) concurrent datasets.\n\n4. The model set omits several state-of-the-art commercial VLMs (e.g., GPT o3/5, Google Gemini 2.5 pro), which weakens the headline claim about “current VLMs.” They have more robust and powerful ability."}, "questions": {"value": "I am curious about the true value of the dataset, as I strongly suspect that it may merely overfit to its own format.\n\nIf a base model (e.g., Qwen-VL) could be fine-tuned on this dataset and subsequently demonstrate performance gains on other benchmarks (such as VSI-Bench, OmniSpatial and SPACE), I would be much more inclined to recognize the dataset’s contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WbJb8t08tc", "forum": "fWWUPOb0CT", "replyto": "fWWUPOb0CT", "signatures": ["ICLR.cc/2026/Conference/Submission11019/Reviewer_e5YW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11019/Reviewer_e5YW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762702300584, "cdate": 1762702300584, "tmdate": 1762922198391, "mdate": 1762922198391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}