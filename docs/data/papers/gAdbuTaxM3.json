{"id": "gAdbuTaxM3", "number": 16635, "cdate": 1758267028313, "mdate": 1759897228054, "content": {"title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "abstract": "Efficiently adapting large vision models is critical, especially with tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA offer limited granularity and effectiveness in few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT method that learns highly sparse updates in the wavelet domain of residual matrices. WaveFT allows precise control of trainable parameters, offering fine-grained capacity adjustment and excelling with remarkably low parameter count, substantially fewer than LoRA's minimum, making it ideal for extreme parameter-efficient scenarios. Evaluated on personalized text-to-image generation using Stable Diffusion XL as baseline, WaveFT significantly outperforms the state-of-the-art in PEFT, especially at low parameter counts; achieving superior subject fidelity, prompt alignment, and image diversity.", "tldr": "We propose WaveFT, an extensible, flexible and more efficient Parameter Efficient Fine Tuning (PEFT) method than existing methods.", "keywords": ["PEFT", "Fine Tuning", "Diffusion", "Image Generation", "Personalization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7899ec3055bd27ef603c6ea259f27d2f0db6ef8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents WaveFT, a parameter-efficient finetuning (PEFT) approach designed for post-training large vision models. WaveFT introduces a wavelet-based transformed parameterization, which analyzes model weights in the wavelet domain to enable low-dimensional finetuning. The authors provide both theoretical analysis and empirical validation, comparing WaveFT with several existing PEFT methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of employing wavelet transforms as an extension to Fourier-based finetuning (e.g., FourierFT) is a natural and interesting direction, offering potential benefits in localized frequency representation.\n\nThe ablation studies on design choices are reasonably comprehensive and help clarify the effects of lambda and # of trainable parameters."}, "weaknesses": {"value": "1. The paper does not clearly articulate why \"wavelet transforms\" are beneficial in this context. While the connection to FourierFT is intuitive, the motivation for moving to wavelets should be explicitly discussed and justified.\n\n2. Since WaveFT is conceptually a wavelet-domain analogue of FourierFT, the absence of **direct comparisons with FourierFT** significantly weakens the empirical evidence. Including FourierFT results is essential to demonstrate the practical advantage (or complementarity) of the proposed method.\n\n3. As shown in Figure 6, different values of the regularization parameter $\\lambda$ yield the best performance for different metrics. This raises an important question of how $\\lambda$ should be selected for general-purpose finetuning. The paper should provide a clearer strategy (e.g., heuristic or adaptive selection) for practical applicability.\n\n4. The discussion of results in Section 5.3 is brief and lacks deeper interpretation. Further analysis would help clarify why WaveFT performs worse than SHiRA and better than LoRA. Again, the absence of **FourierFT results** limits the completeness of this analysis."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sh532vA2uG", "forum": "gAdbuTaxM3", "replyto": "gAdbuTaxM3", "signatures": ["ICLR.cc/2026/Conference/Submission16635/Reviewer_SjRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16635/Reviewer_SjRX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823049647, "cdate": 1761823049647, "tmdate": 1762926701681, "mdate": 1762926701681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Wavelet Fine-Tuning (WaveFT), a new and more efficient method for adapting large vision models, especially when computing power and memory are severely limited. The authors argue that existing methods, such as LoRA, are not precise or effective enough when only an extremely small number of parameters can be trained. The authors prove the effectiveness through empirical experiments of finetuning stable diffusion xl for personalized generation and theoretical analysis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- the author proposes an extremely parameter efficienct parameterization utlizing the wavelet transform, opening many interesting questions to explore\n- the author included an extensive limitation section to discuss the limitation of the waveft method\n- the author includes theoretical analysis of sparse finetuning methods"}, "weaknesses": {"value": "- the waveft seems to be a generic peft method and not constrained for finetuning diffusion models, and the author also did not claim that it is only valid for diffusion models, therefore it should include at least one finetuning tasks from other domains, for example finetuning large language models\n- waveft, because of its efficient parameterization using wavelet, seems to be extremly parameter efficient, however, parameter-efficiency does not directly translate to compute- and memory-efficiency, it would be nice if the authors have benchmarked the training speed and the actual memory footprint compared to lora, since the authors claims that they target the extreme parameter-efficienc scenarios, indicating limited computer resources\n- it seems to me that the FourierFT to be a very relevant baseline, as fourier transform and wavelet transform are inherently connected, for the existing experiments, additional results with FourierFT could strengthen the paper"}, "questions": {"value": "- figure 5: why does the clip score gets worse for higher adapter complexity? and also it seems that clip-t and cmmd, the adapter complexity beyond seems to be not beneficial for the final performance?\n- the adapter parameters are part of the memory usage, if the parameter-efficiency is also directly reflected into the memory efficiency, then it can indeed greatly reduce the memory usage for optimizer states/gradients, however, for true scalability, it is required to further perform quantized finetuning, like QLoRA, I am wondering whether this kind of approach IDWT(C) is also compatible, if the weight is stored in a lower precision\n- the author provided experimental results with extreme less paramter budget, r=1,2,3,4 (depend on the task Figure 5 or Figure 6), but LoRA finetuning is normally performed with much higher rank, does wavelet still perform as well for more complex task? I am not questioning the choice of hyperparamter of lora as parameter budget, in fact, i think for the personalized generation task it might be a good  choice, I am just wondering, for finetuning diffusion models on more complex tasks, where more parameter budget is needed, whether waveft can match or even outperform the lora baseline"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FOsh3wSONQ", "forum": "gAdbuTaxM3", "replyto": "gAdbuTaxM3", "signatures": ["ICLR.cc/2026/Conference/Submission16635/Reviewer_pQD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16635/Reviewer_pQD9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954332927, "cdate": 1761954332927, "tmdate": 1762926701131, "mdate": 1762926701131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Wavelet Fine-Tuning (WaveFT), a novel Parameter-Efficient Fine-Tuning (PEFT) method for large vision models. The core idea is to perform fine-tuning by learning a highly sparse set of coefficients in the wavelet domain of the weight update matrix (ΔW). These sparse coefficients are then transformed back to the weight domain via the Inverse Discrete Wavelet Transform (IDWT). This approach offers fine-grained control over the number of trainable parameters, allowing for adaptation in extremely low-parameter regimes where methods like LoRA are not applicable. The authors provide a theoretical analysis arguing that sparse methods (including WaveFT and the baseline SHiRA) can achieve high-rank updates, contrasting with the low-rank bottleneck of LoRA, which they posit leads to greater representational capacity and output diversity. Through extensive experiments on personalized text-to-image generation with Stable Diffusion XL, the paper demonstrates that WaveFT significantly outperforms existing PEFT methods, including LoRA and the sparse weight-domain baseline SHiRA, particularly in terms of subject fidelity and image diversity at low parameter counts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a genuinely novel approach to PEFT by leveraging sparse updates in the wavelet domain. The motivation is clear and compelling: it directly addresses the granularity limitation of LoRA's rank-based parameterization and proposes an elegant solution that allows for precise, continuous control over the parameter budget, even below LoRA's minimum. The use of a transformed domain is an insightful direction for PEFT research.\n\n2. A significant strength is the inclusion of a theoretical framework to explain the representational power of sparse fine-tuning methods. The paper effectively uses existing results on the rank of random sparse matrices (Lemma 1) to argue that WaveFT and SHiRA can produce high-rank updates. This is contrasted with a clear explanation of LoRA's \"subspace bottleneck\" (Lemma 2). This analysis provides a solid theoretical hypothesis for the empirically observed increase in output diversity."}, "weaknesses": {"value": "1. The high-rank updates enabled by sparse parameterization (also verified in SaRA) are a double-edged sword—while expanding representational capacity, they excessively enhance the model’s fitting ability, making it more prone to overfitting, which is unacceptable for parameter-efficient fine-tuning tasks that prioritize generalization.\n\n2. As evidenced by the CLIP-T scores in Figure 5, both SHiRA and WaveFT exhibit a sharp decline in prompt alignment performance as the number of trainable parameters increases, a clear indicator of severe overfitting that undermines their practical utility in parameter-scalable scenarios.\n\n3. The experimental scope is insufficiently comprehensive. The authors primarily evaluate WaveFT on the DreamBooth personalized generation task, which fails to demonstrate its effectiveness across broader application scenarios. Notably, direct fine-tuning on specific domains to assess domain transfer capability and prior knowledge preservation—core benchmarks for validating PEFT methods—are absent.\n\n4. The comparison with competing methods is limited. WaveFT is only benchmarked against LoRA and SHiRA, which is insufficient to rigorously validate its superiority. A more convincing evaluation should include comparisons with a wider range of recent state-of-the-art PEFT techniques.\n\n\n1. While the theory section effectively argues for sparsity (high-rank updates), it does not provide any theoretical insight into why the wavelet domain is superior to the standard weight domain. The theoretical analysis treats WaveFT and SHiRA (sparse updates in the weight domain) as largely equivalent, as the IDWT is a rank-preserving linear transform. The paper's central claim is the superiority of the wavelet domain, yet this is only supported empirically; the theory does not explain why WaveFT consistently outperforms SHiRA on the main task. The initial motivation about \"semilocal structure\" is not formally connected to the properties of weight update matrices.\n\n2. The MNIST classification experiment (Section 5.3) reveals that SHiRA actually outperforms WaveFT. The paper acknowledges this but dismisses it with a brief statement that \"there are different trade-offs for different tasks.\" This significantly weakens the claim of WaveFT's general applicability. A deeper investigation is needed to understand why the wavelet transform is beneficial for a complex generative task but detrimental for a simpler discriminative one. This finding suggests the method's advantages may be more specialized than presented.\n\n3. The appendix (A.1.3) reveals that WaveFT's training time is approximately 50% longer than SHiRA's (34 minutes vs. 22 minutes) due to the DWT/IDWT operations. While still efficient compared to full fine-tuning, this is a non-trivial overhead. This trade-off—achieving better subject fidelity at the cost of increased training time—should be more explicitly discussed in the main body of the paper as a practical consideration and potential limitation.\n\n4. Several key results and details are placed in the appendix, hindering readability. For instance, the main comparison table against all baselines (Table 4) is in the appendix, while the main paper features a less clear summary plot (Figure 4). Presenting this crucial table in the main experimental section would significantly improve the clarity and impact of the paper's core empirical contributions. The visualization in Figure 4 is also somewhat difficult to interpret, with performance metrics mapped to different visual properties (axes, size) in a non-intuitive way."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "e3qjXw6fte", "forum": "gAdbuTaxM3", "replyto": "gAdbuTaxM3", "signatures": ["ICLR.cc/2026/Conference/Submission16635/Reviewer_rHTh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16635/Reviewer_rHTh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981328264, "cdate": 1761981328264, "tmdate": 1762926700670, "mdate": 1762926700670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed WaveFT is a PEFT method that learns a sparse set of coefficients in the wavelet domain of the weight update and maps them back via IDWT to form delta_w , which is then merged into the base weights with zero inference overhead; the capacity knob is the exact number of trainables p, offering finer control than LoRA’s integer rank. The authors motivate wavelets for semi-local, structured updates and position WaveFT within transformed parameterizations. They provide capacity arguments (random sparse matrices become high-rank; LoRA suffers a subspace bottleneck), then evaluate on SDXL DreamBooth with budget-matched baselines, reporting better subject fidelity at very low budgets (while remaining competitive on prompt alignment/diversity)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Sparse coefficients in a transform domain with one-shot merge keep inference cost unchanged and expose a precise capacity knob.\n2. Theoretical lemmas justify why sparse (transform-domain) updates can be high-rank, in contrast to LoRA’s low-rank subspace, the empirical rank plots support this at SDXL scales.\n3. On SDXL personalization (30 subjects), WaveFT consistently improves subject fidelity (DINO/CLIP-I) at very small budgets, while remaining competitive on text alignment/diversity."}, "weaknesses": {"value": "1. Despite framing WaveFT as broadly applicable, substantive experiments are confined to image generation (SDXL personalization). There has been no results on controllable images generations which are common tasks in this field.\n\n2. Transformed-parameterization positioning, incomplete baselines. The paper compares to FourierFT and SHiRA, but not to orthogonal/rotation-constrained adapters (OFT-style) that are explicitly discussed in Related Work; these belong to the same “change the weight geometry/parameterization” family and should be included under equal parameter budgets.\n\n3. This is more a question rather than weakness, If WaveFT’s value is escaping LoRA’s subspace bottleneck at tiny budgets, LLMs are a key testbed (instruction-tuning, domain adaptation, entity personalization). The absence of any LLM experiment undermines the claimed generality. (Paper currently provides only vision + a toy classifier.)\n\n4. For PEFT method, the operator efficiency during finetuning is actually a key factor, is there any comparison with LoRA?"}, "questions": {"value": "I hope the authors can address my questions and concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qceXlC61U", "forum": "gAdbuTaxM3", "replyto": "gAdbuTaxM3", "signatures": ["ICLR.cc/2026/Conference/Submission16635/Reviewer_kkAk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16635/Reviewer_kkAk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762072155286, "cdate": 1762072155286, "tmdate": 1762926700160, "mdate": 1762926700160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}