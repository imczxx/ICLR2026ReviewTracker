{"id": "pen4NG41j6", "number": 1340, "cdate": 1756873038450, "mdate": 1759898214112, "content": {"title": "VLMShield: Efficient and Robust Defense of Vision-Language Models against Malicious Prompts", "abstract": "Vision-Language Models (VLMs) face significant safety vulnerabilities from malicious prompt attacks due to weakened alignment during visual integration. Existing defenses suffer from efficiency and robustness. To address these challenges, we first propose the **M**ultimodal **A**ggregated **F**eature **E**xtraction (**MAFE**) framework that enables CLIP to handle long text and fuse multimodal information into unified representations. Through empirical analysis of **MAFE**-extracted features, we discover distinct distributional patterns between benign and malicious prompts. Building upon this finding, we develop **VLMShield**, a lightweight safety detector that efficiently identifies multimodal malicious attacks as a plug-and-play solution. Extensive experiments demonstrate superior performance across multiple dimensions, including robustness, efficiency, and utility. Through our work, we hope to pave the way for more secure multimodal AI deployment.", "tldr": "", "keywords": ["Vision-Language Models", "Malicious Prompts", "Defense", "Efficiency", "Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/378b6fbfe0a413bbfbd93b019603077c52a18663.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a defense method against jailbreak and direct malicious attacks against VLM. The methods has two parts. For any text-image input pair, the method first uses CLIP to obtain a concatenation of text and image embeddings, and then pass the embedding to a three layer fully connected model to obtain a classification of whether the sample is malicious. The text input is divided into chunks that meets the token limit of CLIP text encoder. The detector is trained on a labeled dataset aggregated from multiple datasets with benign and jailbreak samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed defense method outperforms baselines significantly with respect to ASR and time complexity, while maintains benign sample precision as good as, if not better than the baselines.\n\n2. The paper is clear written and easy to follow. The experiment section is especially well organized.\n\n3. The paper presents detailed distribution analysis of benign and malicious inputs, and ablations with respect to various ways of extracting the visual and textual features."}, "weaknesses": {"value": "1. While the propose method already has good benign sample precision, it still has non-trivial amount of miss-classified samples. Directly applying the method in production may lead to user dissatisfaction. Is it possible to provide an ablation on the benign precision and ASR with respect to different threshold of the detector?\n\n2. There is some unclarity in the paper. For details, please see Questions."}, "questions": {"value": "Q1. On line 176, it says \"divide the text into overlapping chunks of 75 tokens\", is it a typo supposed to be non-overlapping. If not, how is the overlap decided?\n\n\nQ2. The authors notes some downsides of the existing external defense methods \n\n> cannot simultaneously detect both modalities for input filtering \n\nIs it possible to just apply multiple single modality input filter defenses at the same time? If cost is a concern, how much additional cost does it incur?\n\n> require multiple output generations\n\nWhy would output monitoring defense requires multiple generations? If harmful contents are detected, can the model just respond with a standard disclaimer?\n\n\nQ3. The image attack specific detectors CIDER and MirrorCheck have very high ASR against image attacks, as shown in Table 2,3. Is this expected?\n\nQ4. Is it possible to implement a baseline the does not regenerate responses when harmful response is detected. Would this baseline have lower ASR than ECSO?\n\nQ5. Is there a reason Table 2 does not have direct malicious attack results?\n\nQ6. Regarding to adaptive attacks, is it possible to utilize the fact that the text input are partitioned into chucks, and provide a very long prompt where the malicious part is only concentrated in one small segment. Then particular chunk may only have very little contribution to the weighted text embedding, if the prompt is divided into hundreds of chunks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mG4xqIEKB", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_8s2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_8s2m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525599342, "cdate": 1761525599342, "tmdate": 1762915742992, "mdate": 1762915742992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VLMShield, an external plug-and-play safety detector for Vision-Language Models (VLMs) based on a CLIP-derived Multimodal Aggregated Feature Extraction (MAFE) framework. MAFE enables efficient fusion of long text and image inputs into unified multimodal representations. By empirically showing marked separability between benign and malicious prompt features, the authors design a lightweight three-layer neural network classifier as VLMShield, which aims to robustly detect diverse multimodal attacks. The paper conducts extensive experiments benchmarking against both internal and external baselines, reporting strong results in robustness, efficiency, and benign utility across various attack types and VLM architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological originality and practicality: The MAFE framework extends CLIP to handle long texts and construct unified multimodal representations by progressive text aggregation and cross-modal fusion, which is an elegant solution to CLIP’s token limit and modality separation.\n2. Well motivated design: The paper convincingly demonstrates via both t-SNE and PCA visualizations and quantitative MMD scores that the extracted features yield substantial separability between benign and malicious prompts.\n3. Results demonstrate that VLMShield achieves impressively low Attack Success Rates (ASR) on both in-domain and out-of-domain attacks, low FNR/FPR, and high benign accuracy, outperforming both internal and external baselines. The runtime and efficiency comparisons confirm very low overhead (0.34s per sample)."}, "weaknesses": {"value": "1. Lack of analysis regarding adaptivity and defense circumvention: The adaptive attack evaluation is rather brief and does not provide details of unsuccessful evasion strategies, except for a minimal effective ASR rate. There is no in-depth examination of whether the MAFE representation can be reverse-engineered or if the detector is robust to longer-term adversarial adaptation.\n2. Insufficient investigation of failure cases and false positives/negatives: Although the paper does report FNR and FPR, there is no qualitative error analysis or discussion of when and why the model misclassifies (“benign as malicious” or “malicious as benign”). For example, what types of benign prompts are most likely to be flagged? Where (if at all) do adaptive attacks begin to succeed?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dt8MrFedq8", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_vPvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_vPvD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922862866, "cdate": 1761922862866, "tmdate": 1762915742881, "mdate": 1762915742881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents unified detection of both text and image-based attacks via the introduction of Multimodal Aggregated Feature Extraction (MAFE) which fuses text and image tokens into a common representation. Development of a detection mechanism VLMShield in this joint representation that demonstrates clear separation of malicious from benign threats, demonstrating very low ASR on both in-domain and OOD queries while maintaining a low FPR. The authors provide empirical results supporting the value of the work and also make the case that VLMShield is a more efficient approach than many SOTA approaches."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Unified representation space built on top of a popular open-source VLM, making the work easily adoptable and reproducible.\n* MAFE addresses text processing limitations in CLIP by handling sequences longer than the 77 token limit."}, "weaknesses": {"value": "* MAFE itself is not especially groundbreaking- many papers have developed fused representations using the concatenation or average of the image and text embeddings from CLIP. As such the main contribution is the extension to longer text.\n* The empirical analysis raises suspicions that there are some tell-tale features distinguishing datasets and deeper investigation is needed to rule out the possibility the images and text are simply sampled from different distributions, making discrimination trivial. The too-good-to-be-true results in Table 3 are a smoking gun, where VLMShield is outperfoming SOTA methods by a huge margin. For example, it could be the case that all images in one dataset are 320x240 and in another dataset they are all 640x480- these systemic differences may yield detectable differences in encoding and should be carefully ruled out.\n* The classification model is dependent on a corpus of labeled images which in an adversarial setting may open the door to novel attack methods that require manual detection and retraining."}, "questions": {"value": "How does MAFE handle text segmentation where, say, a single sentence runs for more than 75 tokens? How do you achieve \"semantic completeness and coherence\"?\n\nFormatting: margins between images and text are compressed making it difficult to discriminate between image captions and text body. These practices risk desk rejection. \n\nThe kernel-density representation of the input text (Eq 4) seems like a good choice- did you perform any ablations over alternatives (eg the average of the embeddings)?\n\n\"this framework can handle multimodal and single-modality inputs seamlessly\" - you don't describe how you handle single-modal inputs- do you just use a zero vector for the missing modality? \n\nFigure 3 shows not only clear separation between benign and malicious but also clear separation between the dataset sources, suggesting there may be other structural reasons why these datasets are well-separated. This sort of too-good-to-be-true result should be treated with suspicion.  Have you explored mixing images and prompts across the benign and direct malicious datasets to rule out tell-tale features?  Selective mixing of some datasets in Fig 6/7/8 also point to some structural giveaways- it would be worthwhile to better understand and explain why some of the datasets selectively collapse onto others under selective modality changes- eg image-based onto direct malicious in Figs 7 and 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3c1nyL2UeQ", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_qPm8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_qPm8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929332125, "cdate": 1761929332125, "tmdate": 1762915742733, "mdate": 1762915742733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VLMShield, a safety detector for multimodal jailbreak attacks. It relies on CLIP with MAFE, which can aggregate and extract longer text and concatenates text and image embeddings and classify the prompts as benign or harmful. Experiments show high in-domain and out-of-domain robustness and low latency when compared to existing defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is model-agnostic and does not require modifying or re-training the underlying VLM, which is useful in real deployment settings.\n\n2. The detector is lightweight, making it suitable for real-time or production use.\n\n3. The external structure also does not affect the downstream model performance"}, "weaknesses": {"value": "1. Motivation for using CLIP instead of the LVLM itself is unconvincing.\nThe authors argue that the goal is to build a unified detector that can process both text and image inputs, and CLIP is used because it offers modality-specific encoders and is extended via MAFE to handle long text. However, LVLMs already jointly process image and text inputs and naturally support long context. Therefore, the same effect could be achieved by performing classification directly within the LVLM, without introducing an external CLIP-based detector.\nAdditionally, recent work has shown that LVLM robustness can be improved directly, or that safety classification can be performed using intermediate hidden states within the LVLM itself, without requiring external encoders. See [1–4].\n\n2. The claim that “existing defenses cannot efficiently handle multimodal inputs” is overstated.\nMany LVLM-based safety moderation pipelines already support multi-image, multi-turn, and long-context moderation with modest overhead. Harmful intent can often be detected in the first few decoding steps, so the argument that LVLM-based detection is inherently inefficient is not well supported.\n\n3. Distributional separation experiments may be misleading due to dataset bias.\nEach malicious category corresponds to a different dataset, so the observed separation in t-SNE/MMD may reflect dataset-level differences rather than semantic differences in attack style. To support the claim that MAFE captures modality-specific attack patterns, the authors should evaluate clustering consistency across multiple datasets within the same attack category. Without that, the conclusion about capturing attack-type features is not justified.\n\n4. Benign dataset coverage is weak, and the reported benign accuracy appears unrealistically high.\nOnly two out-of-domain benign datasets are used, and both are caption-style tasks, which are structurally simpler than real conversational or instruction-based benign prompts. More diverse benign data should be included to assess false positive behavior.\nAdditionally, the reported benign accuracy is unusually high. For example, the official Qwen2.5-VL-7B performance is around 67.1% on MM-Vet and 82.6% on MMBench, yet the paper reports nearly 100% accuracy. It is unclear whether standard splits and evaluation protocols were used. The paper should also report the performance of the unmodified Qwen2.5-VL model on the same test set to verify consistency. Clarification is needed to rule out evaluation mismatch or data leakage.\n\nReferences\n[1] Renjie Pi et al., \"MLLM-Protector: Ensuring MLLM’s Safety Without Hurting Performance\", arXiv:2401.02906, 2024.\n[2] Zhendong Liu et al., \"Enhancing Vision-Language Model Safety Through Progressive Concept-Bottleneck-Driven Alignment\", arXiv:2411.11543, 2024.\n[3] Wenhan Yang et al., \"Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap\", arXiv:2505.24208, 2025.\n[4] Zhenhong Zhou et al., \"How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States\", arXiv:2406.05644, 2024."}, "questions": {"value": "1. Why is CLIP preferred over simply using the VLM’s own joint embedding space for classification? Did you try a simple linear classifier over the VLM’s pooled embeddings?\n2. How do you control for dataset-induced feature separation in your MMD experiments? \n3. How well does the detector handle benign prompts requiring complex reasoning, rather than descriptive captioning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fQvr2F9qmr", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_SYBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_SYBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952186761, "cdate": 1761952186761, "tmdate": 1762915742522, "mdate": 1762915742522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLMShield, a lightweight and black-box defense for multimodal large models. The method first builds a Multimodal Aggregated Feature Extractor (MAFE) based on CLIP to handle long text inputs by segmenting and aggregating them through [EOS] similarity weighting, then fuses the text and image representations ([EOS] and [CLS]) into a 1536-dim feature. A small MLP classifier is trained to distinguish benign from malicious inputs. The approach serves as a plug-and-play front-end filter before any VLM. Experiments across multiple jailbreak scenarios demonstrate low attack success rates and negligible latency, with good generalization to unseen datasets. The authors also evaluate adaptive attacks and release anonymized code for reproduction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation is clear and realistic. The paper focuses on a deployable, black-box input-level defense that fits real-world multimodal systems.\n\n2.The design is simple but effective. CLIP-based long-text aggregation and multimodal fusion generate separable features, and a tiny MLP achieves strong detection accuracy with minimal cost.\n\n3.The evaluation is broad and convincing. The authors include IND/OOD datasets, benign accuracy, efficiency, and adaptive attacks, comparing against several representative baselines such as ASTRA, VLMGuard, JailGuard, and MirrorCheck.\n\n4.Results are impressive: the defense achieves very low ASR (<2% in most cases) and maintains almost full benign accuracy with <10% latency overhead.\n\n5.Training details, hyperparameters, and code are clearly provided for reproducibility."}, "weaknesses": {"value": "1.The methodological novelty is limited. The approach mainly combines CLIP feature aggregation and a small classifier—solid engineering, but conceptually incremental.\n\n2.The adaptive threat model is relatively weak. Stronger surrogate or gradient-based attacks directly targeting the MAFE layer are not explored, so the robustness might be overestimated.\n\n3.The evaluation lacks coverage on larger or closed-source VLMs, and does not report ROC or AUROC curves to support threshold selection and deployment tuning."}, "questions": {"value": "1.How sensitive is the method to design choices such as chunk size, similarity weighting, and the choice of CLIP backbone (e.g., RN50, ViT-H, SigLIP)? Please provide a more complete sensitivity or ablation analysis.\n\n2.How would VLMShield perform under stronger adaptive attacks that jointly optimize over MAFE representations or combine layout/typographic perturbations? Can the authors discuss or experiment on this scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "14ftfAmSgh", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_cXjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_cXjt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117738179, "cdate": 1762117738179, "tmdate": 1762915742388, "mdate": 1762915742388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}