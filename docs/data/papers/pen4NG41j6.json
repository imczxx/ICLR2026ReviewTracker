{"id": "pen4NG41j6", "number": 1340, "cdate": 1756873038450, "mdate": 1763724402397, "content": {"title": "VLMShield: Efficient and Robust Defense of Vision-Language Models against Malicious Prompts", "abstract": "Vision-Language Models (VLMs) face significant safety vulnerabilities from malicious prompt attacks due to weakened alignment during visual integration. Existing defenses suffer from efficiency and robustness. To address these challenges, we first propose the **M**ultimodal **A**ggregated **F**eature **E**xtraction (**MAFE**) framework that enables CLIP to handle long text and fuse multimodal information into unified representations. Through empirical analysis of **MAFE**-extracted features, we discover distinct distributional patterns between benign and malicious prompts. Building upon this finding, we develop **VLMShield**, a lightweight safety detector that efficiently identifies multimodal malicious attacks as a plug-and-play solution. Extensive experiments demonstrate superior performance across multiple dimensions, including robustness, efficiency, and utility. Through our work, we hope to pave the way for more secure multimodal AI deployment.", "tldr": "", "keywords": ["Vision-Language Models", "Malicious Prompts", "Defense", "Efficiency", "Robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8ef5c8ce2b421cbcad307a656b428321c3bed66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose a defense method against jailbreak and direct malicious attacks against VLM. The methods has two parts. For any text-image input pair, the method first uses CLIP to obtain a concatenation of text and image embeddings, and then pass the embedding to a three layer fully connected model to obtain a classification of whether the sample is malicious. The text input is divided into chunks that meets the token limit of CLIP text encoder. The detector is trained on a labeled dataset aggregated from multiple datasets with benign and jailbreak samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed defense method outperforms baselines significantly with respect to ASR and time complexity, while maintains benign sample precision as good as, if not better than the baselines.\n\n2. The paper is clear written and easy to follow. The experiment section is especially well organized.\n\n3. The paper presents detailed distribution analysis of benign and malicious inputs, and ablations with respect to various ways of extracting the visual and textual features."}, "weaknesses": {"value": "1. While the propose method already has good benign sample precision, it still has non-trivial amount of miss-classified samples. Directly applying the method in production may lead to user dissatisfaction. Is it possible to provide an ablation on the benign precision and ASR with respect to different threshold of the detector?\n\n2. There is some unclarity in the paper. For details, please see Questions."}, "questions": {"value": "Q1. On line 176, it says \"divide the text into overlapping chunks of 75 tokens\", is it a typo supposed to be non-overlapping. If not, how is the overlap decided?\n\n\nQ2. The authors notes some downsides of the existing external defense methods \n\n> cannot simultaneously detect both modalities for input filtering \n\nIs it possible to just apply multiple single modality input filter defenses at the same time? If cost is a concern, how much additional cost does it incur?\n\n> require multiple output generations\n\nWhy would output monitoring defense requires multiple generations? If harmful contents are detected, can the model just respond with a standard disclaimer?\n\n\nQ3. The image attack specific detectors CIDER and MirrorCheck have very high ASR against image attacks, as shown in Table 2,3. Is this expected?\n\nQ4. Is it possible to implement a baseline the does not regenerate responses when harmful response is detected. Would this baseline have lower ASR than ECSO?\n\nQ5. Is there a reason Table 2 does not have direct malicious attack results?\n\nQ6. Regarding to adaptive attacks, is it possible to utilize the fact that the text input are partitioned into chucks, and provide a very long prompt where the malicious part is only concentrated in one small segment. Then particular chunk may only have very little contribution to the weighted text embedding, if the prompt is divided into hundreds of chunks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mG4xqIEKB", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_8s2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_8s2m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525599342, "cdate": 1761525599342, "tmdate": 1762915742992, "mdate": 1762915742992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[3/3] General Response #3—Ablation Study (R-cXjt, R-qPm8 & R-8s2m)"}, "comment": {"value": "We thank all reviewers for these valuable suggestions. We have conducted comprehensive ablation studies in ***Section-6 (RQ5)*** and ***Appendix-D***, covering five key aspects:\n\n### **1. Chunk Size.**\n\nThe choice of 75-token chunks is motivated by CLIP's architectural constraints: CLIP processes sequences of 77 tokens, with 2 positions reserved for special tokens ([SOS] and [EOS]), leaving 75 positions for actual content. This design maximally utilizes CLIP's token capacity.\n\nWe evaluated performance on MM-Vet (benign) and text_based_jailbreak_28K (malicious) datasets:\n\n| Chunk Size | Overlap | Benign ACC(%) | ASR(%) | Detection Time(s) |\n|------------|---------|---------------|--------|-------------------|\n| 50 | 10 | 96.33 | 0.00 | 0.37 |\n| 75 | 10 | 96.33 | 0.00 | 0.34 |\n\n**Analysis:** Chunk size variations show minimal impact on detection effectiveness but directly affect computational efficiency. The 75-token configuration achieves the same high accuracy while reducing detection time (0.34s vs 0.37s). Therefore, we select CLIP's maximum acceptable length (75 tokens) to minimize the number of chunks and maximize detection efficiency.\n\n### **2. Overlap Size.**\n\nWe compared different overlap sizes (0, 5, 10, 20 tokens) on the same datasets:\n\n| Chunk Size | Overlap | Benign ACC(%) | ASR(%) | Detection Time(s) |\n|------------|---------|---------------|--------|-------------------|\n| 75 | 0 | 96.28 | 0.47 | 0.30 |\n| 75 | 5 | 96.30 | 0.36 | 0.33 |\n| 75 | 10 | 96.33 | 0.00 | 0.34 |\n| 75 | 20 | 96.36 | 0.00 | 0.47 |\n\n**Analysis:** The 10-token overlap provides optimal balance: maintaining semantic continuity across chunk boundaries while avoiding excessive computational overhead (0.47s vs 0.34s for 20-token overlap).\n\n### **3. Text Aggregation Method.**\n\nWe compared three aggregation strategies on MM-Vet (benign) and text_based_jailbreak_28K (malicious):\n\n| Aggregation Method | Benign ACC(%) | ASR(%) | MMD |\n|-------------------|---------------|--------|-----|\n| Simple Average | 96.30 | 1.46 | 0.692 |\n| Similarity-weighted (Ours) | 96.33 | 0.00 | 0.835 |\n| MAX-Pooling | 94.29 | 5.39 | 0.507 |\n\n**Analysis:** Simple averaging treats all chunks equally, potentially introducing harmful content into benign representations. MAX-Pooling captures extreme features but loses overall context. Our similarity-weighted approach automatically emphasizes semantically central content (malicious or benign), achieving superior separability (higher MMD) and lower ASR while reducing the need for manual intervention.\n\n### **4. CLIP Backbone.**\n\nWe evaluated different CLIP architectures on MM-Vet and text_based_jailbreak_28K, assessing the impact of backbone choice beyond ResNet-based alternatives (which showed inferior performance in dataset bias experiments):\n\n| CLIP Backbone | Benign ACC(%) | ASR(%) | Detection Time(s) |\n|--------------|---------------|--------|-------------------|\n| ViT-L/14 | 96.33 | 0.00 | 0.34 |\n| ViT-H/14 | 97.04 | 0.00 | 0.57 |\n| SigLIP-L | 95.17 | 3.05 | 0.35 |\n\n**Analysis:** ViT-H/14 offers slightly better performance but increased computational cost. ViT-L/14 provides the best balance for safety detection. SigLIP-L shows reduced performance, possibly due to its sigmoid-based training diverging from CLIP's contrastive learning approach.\n\n### **5. Detection Threshold.**\n\nWe evaluated performance across different classification thresholds on MM-Vet (benign) and MM-SafetyBench (malicious):\n\n| Threshold | Benign ACC(%) | ASR(%) |\n|-----------|---------------|--------|\n| 0.3 | 100.00 | 10.04 |\n| 0.4 | 99.34 | 5.27 |\n| 0.5 | 96.33 | 0.00 |\n| 0.6 | 90.46 | 0.00 |\n| 0.7 | 83.84 | 0.00 |\n\n**Analysis:** Our default threshold of 0.5 achieves optimal balance, reaching 96.33% benign accuracy with 0.00% ASR. Lower thresholds (0.3-0.4) achieve higher benign accuracy (99.34-100%) but allow more attacks (ASR 5.27-10.04%). Higher thresholds (0.6-0.7) maintain perfect attack blocking (ASR 0.00%) but sacrifice benign accuracy (83.84-90.46%). The threshold validates our design's rationality, balancing attack success prevention with high benign accuracy. For deployment, practitioners can adjust thresholds based on specific requirements: security-critical applications may prefer ≥0.6 (accepting lower benign accuracy), while our default 0.5 provides the best overall trade-off."}}, "id": "Nx6Mc0FzHO", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724232750, "cdate": 1763724232750, "tmdate": 1763724232750, "mdate": 1763724232750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VLMShield, an external plug-and-play safety detector for Vision-Language Models (VLMs) based on a CLIP-derived Multimodal Aggregated Feature Extraction (MAFE) framework. MAFE enables efficient fusion of long text and image inputs into unified multimodal representations. By empirically showing marked separability between benign and malicious prompt features, the authors design a lightweight three-layer neural network classifier as VLMShield, which aims to robustly detect diverse multimodal attacks. The paper conducts extensive experiments benchmarking against both internal and external baselines, reporting strong results in robustness, efficiency, and benign utility across various attack types and VLM architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Methodological originality and practicality: The MAFE framework extends CLIP to handle long texts and construct unified multimodal representations by progressive text aggregation and cross-modal fusion, which is an elegant solution to CLIP’s token limit and modality separation.\n2. Well motivated design: The paper convincingly demonstrates via both t-SNE and PCA visualizations and quantitative MMD scores that the extracted features yield substantial separability between benign and malicious prompts.\n3. Results demonstrate that VLMShield achieves impressively low Attack Success Rates (ASR) on both in-domain and out-of-domain attacks, low FNR/FPR, and high benign accuracy, outperforming both internal and external baselines. The runtime and efficiency comparisons confirm very low overhead (0.34s per sample)."}, "weaknesses": {"value": "1. Lack of analysis regarding adaptivity and defense circumvention: The adaptive attack evaluation is rather brief and does not provide details of unsuccessful evasion strategies, except for a minimal effective ASR rate. There is no in-depth examination of whether the MAFE representation can be reverse-engineered or if the detector is robust to longer-term adversarial adaptation.\n2. Insufficient investigation of failure cases and false positives/negatives: Although the paper does report FNR and FPR, there is no qualitative error analysis or discussion of when and why the model misclassifies (“benign as malicious” or “malicious as benign”). For example, what types of benign prompts are most likely to be flagged? Where (if at all) do adaptive attacks begin to succeed?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dt8MrFedq8", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_vPvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_vPvD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922862866, "cdate": 1761922862866, "tmdate": 1762915742881, "mdate": 1762915742881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[2/3] General Response #2—Adaptive Attacks (R-cXjt, R-vPvD & R-8s2m)"}, "comment": {"value": "We sincerely thank all reviewers for their concerns about adaptive robustness. We address these concerns from two perspectives:\n\n### **1. Clarification of Our Original Adaptive Attack Method.**\n\nAs described in ***Line-481~504***, our original adaptive attacks optimize text or image inputs to alter MAFE representations for evading VLMShield detection, rather than directly modifying MAFE features. This is because directly manipulating MAFE representations cannot be reverse-engineered back into valid text or image inputs that VLMs can accept. Specifically, [EOS] and [CLS] tokens undergo complex transformations through multiple Transformer layers before MAFE aggregation, and these intermediate computational steps are not directly invertible for meaningful input recovery. Therefore, MAFE representations are fundamentally resistant to reverse-engineering, making direct feature manipulation infeasible—attackers must instead optimize actual text or image inputs to influence the resulting MAFE representations.\n\n\n### **2. Additional Adaptive Attacks.**\n\n**2.1 Combined Perturbation Attacks.**\n\nWe implement two types of combined attacks that simultaneously exploit multiple attack vectors:\n\n**(1) Joint Text+Image Optimization.**\n\nWe simultaneously apply GCG optimization (targeting text EOS embeddings) and HADES optimization (targeting image CLS embeddings) to jointly manipulate the concatenated MAFE representations:\n\n$$L_{joint} = (1 - \\lambda) \\cdot (L_{adv}^{text} + L_{adv}^{image}) + \\lambda \\cdot L_{evade}^{joint}$$\n\nwhere the loss jointly optimizes both text and image perturbations to evade MAFE-based detection while maximizing harmful output generation.\n\n- **Results:**\n\n| Attack Type | Strategy | ASR (%) ↓ | HGR (%) ↓ | Effective ASR (%) ↓ |\n|-------------|----------|-------|-------|-----------------|\n| **Joint Text+Image Optimization** | λ=0 | 1.63 | 100 | 1.63 |\n| | λ=0.5 | 4.83 | 42.74 | 2.06 |\n| | λ=1 | 6.02 | 12.37 | 0.74 |\n\n**(2) Multi-Perturbation Image Attacks.**\n\nWe combine multiple image-based attack strategies targeting the image modality: combining FigStep with HADES adversarial perturbations.\n\n- **Results:**\n\n| Attack Type | Strategy | ASR (%) ↓ | HGR (%) ↓ | Effective ASR (%) ↓ |\n|-------------|----------|-------|-------|-----------------|\n| **Multi-Perturbation Image** | λ=0 | 1.25 | 100 | 1.25 |\n| | λ=0.5 | 4.06 | 45.02 | 1.83 |\n| | λ=1 | 5.63 | 14.04 | 0.79 |\n\nThese results demonstrate that even under stronger combined adaptive attacks, VLMShield maintains low effective ASR, validating the fundamental insight that **successfully evading VLMShield reduces the harmfulness of generated content, resulting in low effective ASR.**\n\n\n**2.2 Dilution Attacks.**\n\nWe implement a new attack strategy that directly exploits text chunking by embedding minimal malicious content within extensive benign context, systematically varying the length disparity.\n\n- **Experimental Design:**  Using AdvBench-M prompts, we created scenarios with varying chunk counts (5, 10, 20, 50, 100 chunks), where malicious content occupies only 1 chunk, with remaining chunks filled with benign content from GPT4V-Caption dataset.\n\n- **Results:**\n\n| Total Chunks | Malicious Chunks | ASR (%) | HGR (%) | Effective ASR (%) |\n|--------------|------------------|---------|---------|-------------------|\n| 5 | 1 | 0.48 | 100 | 0.48 |\n| 10 | 1 | 0.83 | 94.2 | 0.78 |\n| 20 | 1 | 1.12 | 78.5 | 0.90 |\n| 50 | 1 | 4.97 | 62.7 | 3.12 |\n| 100 | 1 | 8.73 | 43.8 | 3.82 |\n\nEven with 100 chunks, VLMShield maintains stable performance with effective ASR < 4%. We also tested variations in malicious content placement (beginning/middle/end) and malicious content distribution across multiple chunks—detection effectiveness remained consistently high.\n\nThe fundamental reason lies in CLIP's embedding space characteristics. Malicious intent inherently represents core semantic content, which naturally receives higher similarity weights during MAFE aggregation, displaying better centrality even under dilution. As dilution increases, harmful generation rate dramatically drops (100% → 43.8%), because excessive benign context confuses the downstream VLM, creating an inherent trade-off that fundamentally limits attack effectiveness.\n\nCurrent results demonstrate that **VLMShield effectively resists sophisticated dilution-based adaptive attacks with effective ASR below 4%**, maintaining practical security. Future work could further reduce the ASR through enhanced mechanisms such as anomaly detection and maximum pooling strategies.\n\n### **3. Conclusion.**\n\nThrough the evaluation of multiple adaptive attack strategies, we demonstrate that VLMShield maintains robust defense with maximum effective ASR of 3.82%. The fundamental trade-off between evasion success and attack effectiveness, validates VLMShield's robustness against sophisticated adaptive adversaries with full knowledge of our defense mechanism."}}, "id": "K9DSw3xIWI", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724285433, "cdate": 1763724285433, "tmdate": 1763724285433, "mdate": 1763724285433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents unified detection of both text and image-based attacks via the introduction of Multimodal Aggregated Feature Extraction (MAFE) which fuses text and image tokens into a common representation. Development of a detection mechanism VLMShield in this joint representation that demonstrates clear separation of malicious from benign threats, demonstrating very low ASR on both in-domain and OOD queries while maintaining a low FPR. The authors provide empirical results supporting the value of the work and also make the case that VLMShield is a more efficient approach than many SOTA approaches."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Unified representation space built on top of a popular open-source VLM, making the work easily adoptable and reproducible.\n* MAFE addresses text processing limitations in CLIP by handling sequences longer than the 77 token limit."}, "weaknesses": {"value": "* MAFE itself is not especially groundbreaking- many papers have developed fused representations using the concatenation or average of the image and text embeddings from CLIP. As such the main contribution is the extension to longer text.\n* The empirical analysis raises suspicions that there are some tell-tale features distinguishing datasets and deeper investigation is needed to rule out the possibility the images and text are simply sampled from different distributions, making discrimination trivial. The too-good-to-be-true results in Table 3 are a smoking gun, where VLMShield is outperfoming SOTA methods by a huge margin. For example, it could be the case that all images in one dataset are 320x240 and in another dataset they are all 640x480- these systemic differences may yield detectable differences in encoding and should be carefully ruled out.\n* The classification model is dependent on a corpus of labeled images which in an adversarial setting may open the door to novel attack methods that require manual detection and retraining."}, "questions": {"value": "How does MAFE handle text segmentation where, say, a single sentence runs for more than 75 tokens? How do you achieve \"semantic completeness and coherence\"?\n\nFormatting: margins between images and text are compressed making it difficult to discriminate between image captions and text body. These practices risk desk rejection. \n\nThe kernel-density representation of the input text (Eq 4) seems like a good choice- did you perform any ablations over alternatives (eg the average of the embeddings)?\n\n\"this framework can handle multimodal and single-modality inputs seamlessly\" - you don't describe how you handle single-modal inputs- do you just use a zero vector for the missing modality? \n\nFigure 3 shows not only clear separation between benign and malicious but also clear separation between the dataset sources, suggesting there may be other structural reasons why these datasets are well-separated. This sort of too-good-to-be-true result should be treated with suspicion.  Have you explored mixing images and prompts across the benign and direct malicious datasets to rule out tell-tale features?  Selective mixing of some datasets in Fig 6/7/8 also point to some structural giveaways- it would be worthwhile to better understand and explain why some of the datasets selectively collapse onto others under selective modality changes- eg image-based onto direct malicious in Figs 7 and 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3c1nyL2UeQ", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_qPm8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_qPm8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929332125, "cdate": 1761929332125, "tmdate": 1762915742733, "mdate": 1762915742733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "[1/3] General Response #1—Dataset Distributional Separation Analysis (R-SYBb & R-qPm8)"}, "comment": {"value": "We thank all reviewers for raising this concern about potential dataset-level artifacts. We clarify that **each dataset** in our experiments contains samples with **diverse characteristics**: images vary widely in resolution, aspect ratios, and content types, while text prompts range from short queries to lengthy instructions with diverse linguistic patterns. This natural diversity within each dataset makes trivial discrimination based on superficial features (e.g., uniform image size) unlikely.\n\nTo rigorously validate that MAFE captures genuine semantic attack patterns rather than dataset artifacts, we have conducted extensive additional analysis in ***Line-250~260*** and ***Appendix-A.3*** (***Page-15~17***). Specifically, we compare three feature extraction approaches across different dataset configurations:\n\n### 1. **Feature Extraction Methods.**\n\n- Traditional Feature Extraction: ResNet for image features + TF-IDF for text features, concatenated.\n- VLM Internal Representations: Embeddings from the last hidden layer of VLMs (Qwen2.5-VL-7B-Instruct).\n- Our Proposed MAFE: CLIP-based multimodal aggregated features.\n   \n\n### 2. **Experimental Configurations.**\n\n-  Cross-Category Datasets: We select one representative dataset from each of the three malicious categories (Image-based Jailbreak, Text-based Jailbreak, Direct Malicious) and visualize their feature distributions against benign datasets.\n\n-  Within-Category Datasets: For each malicious category, we separately evaluate multiple datasets from the same category against benign datasets to test clustering consistency.\n\n\n### 3. **Results.**\n\n**(1) Cross-Category Analysis (Figure-9, Appendix-A.3.1).** \n- Traditional features and VLM internal representations fail to achieve clear separation between benign and malicious samples.\n- Only MAFE successfully separates malicious from benign samples while maintaining consistent patterns across different datasets within the same attack category.\n\n**(2) Within-Category Analysis (Figure-10~12, Appendix-A.3.2).**\n- Across Image-based Jailbreak datasets (***Figure-10***), Text-based Jailbreak datasets (***Figure-11***), and Direct Malicious datasets (***Figure-12***), MAFE consistently maintains a clear benign-malicious separation.\n- Different datasets within the same attack category converge in MAFE's feature space, where traditional and VLM-based features show inconsistent patterns across datasets from the same category.\n\n### 4. **Conclusion.**\nThese experiments demonstrate that:\n- MAFE successfully captures attack patterns that generalize across multiple datasets within the same category.\n- Without MAFE processing, standard feature extraction methods fail to achieve meaningful separation, indicating that clear clustering is not an artifact of dataset-level differences.\n\nWe hope these extensive additional experiments convincingly address the concern about dataset bias and validate that MAFE's discriminative power stems from capturing genuine multimodal attack semantics."}}, "id": "a4P14KmtsD", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724364571, "cdate": 1763724364571, "tmdate": 1763724364571, "mdate": 1763724364571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VLMShield, a safety detector for multimodal jailbreak attacks. It relies on CLIP with MAFE, which can aggregate and extract longer text and concatenates text and image embeddings and classify the prompts as benign or harmful. Experiments show high in-domain and out-of-domain robustness and low latency when compared to existing defenses."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is model-agnostic and does not require modifying or re-training the underlying VLM, which is useful in real deployment settings.\n\n2. The detector is lightweight, making it suitable for real-time or production use.\n\n3. The external structure also does not affect the downstream model performance"}, "weaknesses": {"value": "1. Motivation for using CLIP instead of the LVLM itself is unconvincing.\nThe authors argue that the goal is to build a unified detector that can process both text and image inputs, and CLIP is used because it offers modality-specific encoders and is extended via MAFE to handle long text. However, LVLMs already jointly process image and text inputs and naturally support long context. Therefore, the same effect could be achieved by performing classification directly within the LVLM, without introducing an external CLIP-based detector.\nAdditionally, recent work has shown that LVLM robustness can be improved directly, or that safety classification can be performed using intermediate hidden states within the LVLM itself, without requiring external encoders. See [1–4].\n\n2. The claim that “existing defenses cannot efficiently handle multimodal inputs” is overstated.\nMany LVLM-based safety moderation pipelines already support multi-image, multi-turn, and long-context moderation with modest overhead. Harmful intent can often be detected in the first few decoding steps, so the argument that LVLM-based detection is inherently inefficient is not well supported.\n\n3. Distributional separation experiments may be misleading due to dataset bias.\nEach malicious category corresponds to a different dataset, so the observed separation in t-SNE/MMD may reflect dataset-level differences rather than semantic differences in attack style. To support the claim that MAFE captures modality-specific attack patterns, the authors should evaluate clustering consistency across multiple datasets within the same attack category. Without that, the conclusion about capturing attack-type features is not justified.\n\n4. Benign dataset coverage is weak, and the reported benign accuracy appears unrealistically high.\nOnly two out-of-domain benign datasets are used, and both are caption-style tasks, which are structurally simpler than real conversational or instruction-based benign prompts. More diverse benign data should be included to assess false positive behavior.\nAdditionally, the reported benign accuracy is unusually high. For example, the official Qwen2.5-VL-7B performance is around 67.1% on MM-Vet and 82.6% on MMBench, yet the paper reports nearly 100% accuracy. It is unclear whether standard splits and evaluation protocols were used. The paper should also report the performance of the unmodified Qwen2.5-VL model on the same test set to verify consistency. Clarification is needed to rule out evaluation mismatch or data leakage.\n\nReferences\n[1] Renjie Pi et al., \"MLLM-Protector: Ensuring MLLM’s Safety Without Hurting Performance\", arXiv:2401.02906, 2024.\n[2] Zhendong Liu et al., \"Enhancing Vision-Language Model Safety Through Progressive Concept-Bottleneck-Driven Alignment\", arXiv:2411.11543, 2024.\n[3] Wenhan Yang et al., \"Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap\", arXiv:2505.24208, 2025.\n[4] Zhenhong Zhou et al., \"How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States\", arXiv:2406.05644, 2024."}, "questions": {"value": "1. Why is CLIP preferred over simply using the VLM’s own joint embedding space for classification? Did you try a simple linear classifier over the VLM’s pooled embeddings?\n2. How do you control for dataset-induced feature separation in your MMD experiments? \n3. How well does the detector handle benign prompts requiring complex reasoning, rather than descriptive captioning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fQvr2F9qmr", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_SYBb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_SYBb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952186761, "cdate": 1761952186761, "tmdate": 1762915742522, "mdate": 1762915742522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VLMShield, a lightweight and black-box defense for multimodal large models. The method first builds a Multimodal Aggregated Feature Extractor (MAFE) based on CLIP to handle long text inputs by segmenting and aggregating them through [EOS] similarity weighting, then fuses the text and image representations ([EOS] and [CLS]) into a 1536-dim feature. A small MLP classifier is trained to distinguish benign from malicious inputs. The approach serves as a plug-and-play front-end filter before any VLM. Experiments across multiple jailbreak scenarios demonstrate low attack success rates and negligible latency, with good generalization to unseen datasets. The authors also evaluate adaptive attacks and release anonymized code for reproduction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation is clear and realistic. The paper focuses on a deployable, black-box input-level defense that fits real-world multimodal systems.\n\n2.The design is simple but effective. CLIP-based long-text aggregation and multimodal fusion generate separable features, and a tiny MLP achieves strong detection accuracy with minimal cost.\n\n3.The evaluation is broad and convincing. The authors include IND/OOD datasets, benign accuracy, efficiency, and adaptive attacks, comparing against several representative baselines such as ASTRA, VLMGuard, JailGuard, and MirrorCheck.\n\n4.Results are impressive: the defense achieves very low ASR (<2% in most cases) and maintains almost full benign accuracy with <10% latency overhead.\n\n5.Training details, hyperparameters, and code are clearly provided for reproducibility."}, "weaknesses": {"value": "1.The methodological novelty is limited. The approach mainly combines CLIP feature aggregation and a small classifier—solid engineering, but conceptually incremental.\n\n2.The adaptive threat model is relatively weak. Stronger surrogate or gradient-based attacks directly targeting the MAFE layer are not explored, so the robustness might be overestimated.\n\n3.The evaluation lacks coverage on larger or closed-source VLMs, and does not report ROC or AUROC curves to support threshold selection and deployment tuning."}, "questions": {"value": "1.How sensitive is the method to design choices such as chunk size, similarity weighting, and the choice of CLIP backbone (e.g., RN50, ViT-H, SigLIP)? Please provide a more complete sensitivity or ablation analysis.\n\n2.How would VLMShield perform under stronger adaptive attacks that jointly optimize over MAFE representations or combine layout/typographic perturbations? Can the authors discuss or experiment on this scenario?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "14ftfAmSgh", "forum": "pen4NG41j6", "replyto": "pen4NG41j6", "signatures": ["ICLR.cc/2026/Conference/Submission1340/Reviewer_cXjt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1340/Reviewer_cXjt"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission1340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117738179, "cdate": 1762117738179, "tmdate": 1762915742388, "mdate": 1762915742388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}