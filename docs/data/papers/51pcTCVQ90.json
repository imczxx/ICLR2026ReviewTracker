{"id": "51pcTCVQ90", "number": 5458, "cdate": 1757911748951, "mdate": 1759897973471, "content": {"title": "Greedy Distill: Efficient Video Generative Modeling with Linear Time Complexity", "abstract": "Due to bidirectional attention dependencies, video generation models generally suffer from $O(n^2)$ computational complexity. In this work, we find the “local inter-frame information redundancy\" phenomenon which indicates strong local temporal dependencies in video generation, with global attention to distant frames contributing only marginally. Built upon this finding, we introduce a novel distillation training paradigm for video diffusion models, namely GREEDY DISTILL. \nSpecifically, to generate the next frame using only the 0-th and the last frames, we propose the Streaming Diffusion Decoder (SDD) as the “Greedy Decoder\" to avoid redundant computational costs from the other frames. \nMeanwhile, to our knowledge, we introduce Efficient Temporal Module (ETM) to capture the global temporal information across frames.\nThese two modules achieve the computational complexity reduction from $O(n^2)$ to linear. Moreover, we make the first attempt to apply RL fine-tuning to address the error accumulation during streaming generation.\nOur method achieves an overall score of 84.60 on the VBench benchmark, surpassing previous state-of-the-art methods by large margins(+4.18%). Qualitative results also demonstrate superior performance. \nLeveraging its efficient model structure and KV cache, it is able to rapidly generate high-quality video streams at 24 FPS (nearly 50% faster) on a single H100 GPU.", "tldr": "we propose a new asymmetric structural distillation method that produce videos with superior quality.", "keywords": ["Video Generation", "Diffusion based models Tr", "model distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ab389eeb082fc8a1ee8c776732f665b773abb2e.pdf", "supplementary_material": "/attachment/3bf36f9e67d1c42357dcc2334c62c4e8c72138b6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Greedy Distill, an asymmetric distillation that turns a bidirectional DiT video diffusion teacher into a fast student composed of an Efficient Temporal Module (chunked AR Transformer with sliding-window attention) and a Streaming Diffusion Decoder with KV caching. A rollout/RL-style fine-tuning step aims to curb exposure bias. The claim is near-linear time in frames while maintaining teacher-level quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Quality: Sensible two-stage training; clear ablations indicating ETM/RL contributions; competitive throughput/latency.\n* Clarity: Architecture, complexity intuition, and training pipeline are easy to follow."}, "weaknesses": {"value": "* Claim vs. demos: Fig. 7 and some other provided demos appear to show lower dynamics than baselines. This suggests a locality bias that may trade motion amplitude/scene changes for stability.\n* Dynamics not quantified: No direct evaluation of motion strength; please report metrics like VBench Dynamic-Degree, optical-flow magnitude/variance, or long-horizon motion persistence.\n* Writing problem: Sec 3.3 title (row 372) overlaps with the previous section"}, "questions": {"value": "I don't understand the necessity to formulate the finetuning as an RL process. Any fundamental differences between this and directly using the MSE loss based on the KL divergence? Could you explain more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BXJg0kY8vE", "forum": "51pcTCVQ90", "replyto": "51pcTCVQ90", "signatures": ["ICLR.cc/2026/Conference/Submission5458/Reviewer_pNtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5458/Reviewer_pNtk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754156341, "cdate": 1761754156341, "tmdate": 1762918074658, "mdate": 1762918074658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the quadratic complexity bottleneck in diffusion-based video generation and introduces GREEDY DISTILL, a teacher-student distillation framework that reduces complexity to linear. The core idea is to decouple temporal modeling (via an autoregressive Efficient Temporal Module with sliding-window attention) from frame synthesis (via a Streaming Diffusion Decoder that only conditions on the 0-th and last frames), followed by reinforcement-learning fine-tuning to suppress exposure bias. Extensive experiments on VBench and human evaluations show good performance of GREEDY DISTILL. The manuscript is clearly written, with project page, detailed ablations, and supplementary material."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The authors propose a novel asymmetric architecture that enables linear-time streaming synthesis and provide theoretical complexity analysis.\n\n- Experiments are comprehensive: both real-time and long-duration generation, human preference studies, and ablations of components, using publicly available benchmarks and code.  \n\n- The writing is clear, with intuitive figures, step-by-step algorithm boxes, and a detailed reproducibility statement that facilitates follow-up research."}, "weaknesses": {"value": "- The experiments are mainly conducted on Wan2.1. It would be more convincing to include other backbones such as HunyuanVideo or CogVideoX to demonstrate the generality of the proposed framework.\n\n- The writing could be improved for better readability, e.g., adding punctuation after equations and providing clearer captions for figures. Several typos such as “stege” → “stage” in Section 3.1\n\n- The reinforcement learning fine-tuning is an interesting addition, but its novelty seems mainly in the application context rather than in algorithmic design."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2CKfoFRRjx", "forum": "51pcTCVQ90", "replyto": "51pcTCVQ90", "signatures": ["ICLR.cc/2026/Conference/Submission5458/Reviewer_T4r8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5458/Reviewer_T4r8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900553024, "cdate": 1761900553024, "tmdate": 1762918074448, "mdate": 1762918074448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"Greedy Distill: Efficient Video Generative Modeling with Linear Time Complexity\" introduces a novel distillation training paradigm that significantly reduces the computational complexity of video generation models. The proposed method achieves state-of-the-art performance in terms of both speed and quality, making it a valuable contribution to the field. The paper is well-written and the results are compelling, but there are opportunities for further improvement in terms of detailed comparisons, additional experiments, and discussion of future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "see Summary"}, "weaknesses": {"value": "The number of displayed videos is quite limited. It is difficult to fully assess the method's effectiveness and robustness with such a small sample size.\n\nSuggestion: The authors should provide a detailed breakdown of the user study results. This should include statistical analysis, user feedback, and any significant findings. Additionally, the paper should discuss how these results align with the quantitative metrics and what insights they provide into the overall performance of the proposed method."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8w5GtPlEy1", "forum": "51pcTCVQ90", "replyto": "51pcTCVQ90", "signatures": ["ICLR.cc/2026/Conference/Submission5458/Reviewer_MtgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5458/Reviewer_MtgK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996791034, "cdate": 1761996791034, "tmdate": 1762918074150, "mdate": 1762918074150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new architecture and training algorithm for autoregressive video diffusion models. \n\nArchitecture: Instead of a single diffusion transformer with causal attention, it uses two models - one causal transformer with local attention (ETM), and another causal diffusion model (SDD) that takes the first frame, the previous frame, and the output of ETM to denoise the current frame.\n\nTraining: It first uses teacher forcing with diffusion loss to finetune SDD + ETM, both initialized from Wan2.1 with bidirectional attention. It is then trained with a variant of Self-Forcing + DMD that uses the student model itself as the fake score network (instead of fine-tuning a seperate one).\n\nExperiments demonstrate higher efficiency and less error accumulation than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The student splits temporal modeling (ETM) from per-frame generation (SDD). This architecture intuitively makes sense, and the observation (Fig. 2) motivates restricting attention to nearby frames and justifies ETM’s sliding window as a principled efficiency/fidelity trade-off rather than a heuristic.\n- On Wan 2.1, the distilled student runs at 24 FPS reaches VBench 84.60, with qualitative long-video examples showing reduced error accumulation vs. Self-Forcing and CausVid. The paper also includes ablation evidence showing the effectiveness of ETM and \"RL fine-tuning\"."}, "weaknesses": {"value": "- The title/teaser focuses on “linear time,” but that follows directly from sliding-window causal attention. It is generally well known that local attention can produce coherent videos in linear time with the downside of sacrificing long term memory. Consider reframing the main contribution around the new two-stage architecture (ETM+SDD) and the training algorithm.\n- Section 2.2.2 casts training as deterministic continuous policy gradients over a reverse-KL reward, yet the practical algorithm is very close to Self-Forcing. The paper repeatedly claims “first attempt to apply RL to address error accumulation,” which feels overstated given the close methodological overlap with existing work. Consider recasting this as a conceptual bridge: “We show Self-Forcing-style rollout training admits an RL interpretation. \n- Also, there is actually a big difference between the proposed algorithm (and Self-Forcing) versus RL algorithms (e.g., DDPG Lillicrap et al., 2015) that the paper cites. In Self-Forcing, gradients flow through the rollout trajectory; in DDPG, rollouts are off-policy in a replay buffer and are detached, and gradients flow through the reward estimator. The proposed algorithm follows Self Forcing closely but materially deviates from the RL algorithms cited.\n- The sentence \"...more broadly known as exposure bias, where a model is trained exclusively on ground-truth context but must rely on its own imperfect predictions at inference time, resulting in a distributional mismatch that compounds errors as generation progresses.\" is exactly copied from a sentence in the paper of Self Forcing. Please paraphrase and cite the original source.\n- Minor spacing/format issues. In many sentences there is no space before punctuation symbols. vspace issues in e.g. L372\n- The paper proposes to use the student model itself to estimate the score function of the student distribution. However, the student, when trained with a few-step prediction objective, actually no longer predicts the score function. For example, given x_T input, an ideal score predictor would predict the dataset mean as x0 prediction. However, the student model is trained to predict a realistic output. The idea of using the few-step student model itself to estimate the score function does not seem to be theoretically correct."}, "questions": {"value": "- Some design choices of ETM + SDD can be better justified. For example, why does SDD see the first frame sink, but not ETM? Abaltion studies on window size would also be helpful.\n- How is the 0.24s latency calculated and is it the time to generate the first block of frames? The model should not seem to be more efficient than Self Forcing/CausVid initially since the local attention do not provide speedup benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q5YQq44KwP", "forum": "51pcTCVQ90", "replyto": "51pcTCVQ90", "signatures": ["ICLR.cc/2026/Conference/Submission5458/Reviewer_so5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5458/Reviewer_so5Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5458/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038022614, "cdate": 1762038022614, "tmdate": 1762918073844, "mdate": 1762918073844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}