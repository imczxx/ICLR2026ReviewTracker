{"id": "YWeNCMxdhM", "number": 6223, "cdate": 1757959213233, "mdate": 1763730614760, "content": {"title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "abstract": "Vision–Language–Action (VLA) models adapt large vision–language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions autoregressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv–Bridge, improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.", "tldr": "", "keywords": ["Embodied manipulation", "Discrete diffusion", "Vision-language-action model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/649b5ddb9c0a44e40083e8d26c93085d241764d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Discrete Diffusion VLA: a method to replace the autoregressive action token decoding or continuous diffusion action headers of the current VLA models with discrete diffusion action decoding. Specifically, it provides a pipeline to train and inference with discrete diffusion for action tokens, and also provides two decoding strategies to improve the inference performance. Evaluation on multiple simulation datasets shows the good performance of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Good performance across different datasets.\n\nThe proposed method achieves better performance than prior methods in two popular simulation datasets. Although it's hard to say that discrete diffusion has an absolutely significant margin over prior work, these results show good potential of it.\n\n2. Extensive and interesting ablation studies.\n\nI also appreciate the extensive and interesting studies in the paper; For example, Figure 4 shows a very interesting relationship between compute and SR by adjusting the number of denoise rounds.\n\n3. Clear and solid representation.\n\nThe presentation of this paper is clear and solid:\n- The formal technical derivation in the method section is clean and clear, which could benefit a broad range of readers interested in this area.\n- The figures are very clear.\n- The connections between experimental results and each of the arguments in the paper is also very strong."}, "weaknesses": {"value": "1. Technical contribution is not super novel.\nThis paper basically combines discrete diffusion model with VLA. Although it is a reasonable choice, I don't think this paper has significant new technical contribution. This is not a super big deal, but under this case one would expect more thorough analysis of the proposed combination of ideas, which is not super sufficient in my opinion (see the two points below).\n\n\n2. Lack of efficiency comparison with continuous diffusion models.\n\nAlthough in Section 4.4 the authors compared the inference efficiency of the proposed method against prior AR models, it is hard to know if the proposed method is really efficient compared with another kinds of important VLA models: continuous diffusion models. There are multiple questions regarding this point:\n- Does making the diffusion process discrete help reduce the number of required denoise rounds in the inference stage?\n- Because the discrete diffusion requires forwarding through the heavy base VLM models multiple times, is this really efficient compared with the continuous diffusion action head that needs forwarding through VLM models once?\n\nAlthough I don't expect the proposed method to be faster than the continuous diffusion models, I think such a discussion in the paper is necessary as inference efficiency is one of the main contributions of the paper.\n\n3. Does not really show Discrete Diffusion VLA inherits unified-transformer scaling behavior.\n\nRelated to the last point, I do not expect discrete diffusion VLA achieve better efficiency than continuous diffusion VLA models; as this is not the goal. As mentioned in the paper, the real goal is to make it able to use a unified model for both VLM perception and action generation, and therefore inherits unified-transformer scaling behavior. However, this property is not studied at all in the paper. I think it would be nice to have some initial experiments regarding this to show this main goal."}, "questions": {"value": "Please refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5CGdL18tXQ", "forum": "YWeNCMxdhM", "replyto": "YWeNCMxdhM", "signatures": ["ICLR.cc/2026/Conference/Submission6223/Reviewer_McMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6223/Reviewer_McMu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620407756, "cdate": 1761620407756, "tmdate": 1762918555283, "mdate": 1762918555283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Discrete Diffusion VLA, the first framework to unify vision, language, and action generation within a single transformer architecture. Instead of autoregressively predicting actions or using separate decoders, it performs parallel masked-token denoising on discretized action chunks, and secondary re-masking for error correction. This approach breaks the left-to-right bottleneck, reduces inference cost from linear to constant in sequence length, and maintains structural consistency with pretrained VLM backbones. Experiments on multiple robot platforms demonstrate state-of-the-art performance with significantly fewer inference efficiency in number of function evaluations (NFEs) than autoregressive and continuous diffusion baselines. Overall, the method enables a more scalable and unified Vision-Language-Action model design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes the first unified-transformer VLA with discrete diffusion-based action decoding directly inside the VLM backbone. The adaptive masked-token refinement with secondary re-masking is a novel mechanism that enables parallel decoding and robust error correction.\n\n- Architectural contrasts with AR and continuous diffusion approaches are clearly visualized, and the overall method is communicated cleanly through effective figures and paradigm comparisons.\n\n- Extensive evaluations across three robot platforms and multiple benchmark suites demonstrate consistent improvements over strong AR, MLP, and continuous diffusion baselines, highlighting both practical value and scalability toward larger unified VLA systems."}, "weaknesses": {"value": "- While the method reduces NFEs compared to autoregressive (AR)models, secondary re-masking may introduce additional latency and memory overhead relative to AR or other paradigms. Reporting end-to-end on-robot latency against other paradigms' baselines may clarify this concern.\n\n- The paper is based on the unified-transformer architecture and combines adaptive decoding and secondary re-masking. But does not fully disentangle their individual contributions to accuracy and efficiency. Additional ablations would help determine whether improvements primarily come from the unified-transformer architecture or from the proposed decoding mechanisms."}, "questions": {"value": "- Could authors quantify the individual impact of (i) adaptive decoding and (ii) secondary re-masking? Additionally, is secondary re-masking specifically needed because adaptive decoding may commit low-confidence tokens, or does it improve performance even under other decoding strategies?\n\n\n- How much of the performance gain is attributable to integrating action denoising inside the VLM backbone? Since adaptive decoding and secondary re-masking could, in principle, be applied to a separate action head, a comparison against such variants would help clarify whether the unified transformer structure is a primary driver of improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tpNorZJE9p", "forum": "YWeNCMxdhM", "replyto": "YWeNCMxdhM", "signatures": ["ICLR.cc/2026/Conference/Submission6223/Reviewer_xPs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6223/Reviewer_xPs3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761770393405, "cdate": 1761770393405, "tmdate": 1762918554504, "mdate": 1762918554504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified transformer-based VLA policy that employs discrete diffusion for action decoding. Built upon a VLM (Prismatic-7B), the model discretizes the continuous control signals into a vocabulary of action tokens, and iteratively denoises masked action tokens with vision and language input as conditions. The proposed Discrete Diffusion VLA achieves an improved success rate on LIBERO, SimplerEnv-Fractal, and Simpler-Bridge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first paper that models VLA policies under a discrete diffusion framework.\n\n2. Consistent improvements across different simulation benchmarks, achieving state-of-the-art results.\n\n3. The proposed model is compatible with pre-trained VLMs and does not require additional continuous heads or separate diffusion modules."}, "weaknesses": {"value": "1. The remasking step shares a similar high-level idea as ReMDM [1], though the techniques are not alike implementation-wise. The proposed method requires two hyperparameters ($\\eta_t^\\mathrm{abs}$ and $\\eta_t^\\mathrm{drop}$) while ReMDM only requires one (remasking schedule), which seems simpler to tune. How well would ReMDM work in this case?\n\n[1] Wang, et al. Remasking Discrete Diffusion Models with Inference-Time Scaling. NeurIPS 2025\n\n2. The intuition behind discrete diffusion is that the adaptive decoding order \"resolves easy action elements before hard ones\". However, the manuscript lacks an analysis of the final learned decoding policy that supports the intuition. A good example of this is how MaskGIT [2] parallel-decodes images.\n\n[2] Chang et al. \"Maskgit: Masked generative image transformer.\" CVPR 2022.\n\n3. The inference efficiency section analyzes the reduction of NFEs. However, the latency in practice is still missing. Given the fact that the underlying model is 7B, this value is likely not negligible. This number will also serve as a good reference for further work that pushes towards efficiency. \n\n4. No real-world validation. The results are in simulation, leaving questions about the sim-to-real robustness."}, "questions": {"value": "My questions mainly include (1) different implementations of remasking, (2) more analysis (visualization will be more ideal) that supports the intuition, and (3) concrete values about the inference speed. All questions have been raised before. Please find more details in the weakness section. Given the fact that ICLR is a learning conference, I will *not* penalize the paper for lack of real-world validation (weakness 4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ExDgaaVnNC", "forum": "YWeNCMxdhM", "replyto": "YWeNCMxdhM", "signatures": ["ICLR.cc/2026/Conference/Submission6223/Reviewer_oWUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6223/Reviewer_oWUx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970492385, "cdate": 1761970492385, "tmdate": 1762918554085, "mdate": 1762918554085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Discrete Diffusion VLA, a unified transformer that integrates vision, language, and action modeling through discrete diffusion instead of autoregressive decoding. By refining discretized action tokens with adaptive re-masking, it achieves more accurate and efficient robotic action generation, outperforming prior VLA models across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper motivation is clear and overall, the text is cleanly written, with all important details regarding the proposed method explained.\n- Proposed method achieves state of the art results on LIBERO and SimplerEnv\n- Proposed method is really simple, which is a plus, compared to many previous approaches"}, "weaknesses": {"value": "The contribution is interesting and technically novel in the VLA context, however the overall approach appears to be a relatively straightforward adaptation of existing techniques. The paper would benefit from a clearer discussion (with evidence) of what discrete diffusion decoding uniquely contributes beyond existing approaches. I feel that currently authors failed to demonstrate that. \n\nYes, it achieves state-of-the-art results (although only in simulation and LIBERO is already saturated), but as almost any other recent VLA paper, which all propose a wide variety of modifications. There is evidence for parallel decoding and reduced number of function evaluations (Section 4.4), however there are no comparisons with the prior AR and non-AR baselines beyond hypothetical case. Can you provide a speed up compared to, e.g. OpenVLA-OFT? Theoretical speedup is not enough. A good example is linear alternatives to attention. Although they are faster asymptotically, FlashAttention is much faster in practice due to its efficient kernels.\n\nMost critical weakness is a strong claim that proposed scheme “preserves pretrained vision-language priors”, which is mentioned multiple times throughout the paper. However, there is zero evidence or experiment supporting that claim. Thus, I think that either that claim should be completely removed, or authors should provide experiments to support it. For example, by comparing how much general knowledge VLM retained after proposed funetuning (see https://arxiv.org/abs/2505.23705, https://arxiv.org/abs/2505.03500, https://arxiv.org/abs/2509.22195, https://arxiv.org/abs/2502.14420, https://arxiv.org/abs/2505.21906) , in comparison to other methods, or additionally ablating the novel language following abilities, beyond seen in the training data, e.g. by augmentation of existing prompts (see https://arxiv.org/abs/2509.11417). Simply providing good results on LIBERO is not enough, as VLA could easily overfit to the tasks and ignore language instructions.\n\nI may increase the score if the last concern is addressed, but currently I can not recommend acceptance even with strong results on LIBERO & SimplerEnv."}, "questions": {"value": "1. Why actions discretization from the OpenVLA and not FAST?\n2. In appendix you write “we apply FiLM … to enhance the language grounding abilities of our model”. Isn't your main claim that you eliminate additional components by unifying the pipeline into a single model? The VLM is already understand language, why does it requires additional conditioning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TpNmqtUjk2", "forum": "YWeNCMxdhM", "replyto": "YWeNCMxdhM", "signatures": ["ICLR.cc/2026/Conference/Submission6223/Reviewer_QZbY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6223/Reviewer_QZbY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6223/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989771931, "cdate": 1761989771931, "tmdate": 1762918553692, "mdate": 1762918553692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}