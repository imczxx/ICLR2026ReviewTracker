{"id": "RzcxXjLCsm", "number": 10141, "cdate": 1758161842912, "mdate": 1759897671505, "content": {"title": "Align and Filter: Improving Performance in Asynchronous On-Policy RL", "abstract": "Distributed training and increasing the gradient update frequency are practical strategies to accelerate learning and improve performance, but both exacerbate a central challenge: _policy lag_, which is the mismatch between the behavior policy generating data and the learning policy being updated. Policy lag can hinder the scaling of on-policy learning algorithms to larger problems. In this paper, we identify the sources of policy lag caused by distributed learning and high update frequency. We use the findings to propose _total Variation-based Advantage aligned Constrained policy Optimization (VACO)_ as a practical approach to mitigate policy lag. We empirically validate our method and show that it offers better robustness to policy lag in classic robotics RL tasks and a modern RL for LLM math reasoning task.", "tldr": "We propose a new policy gradient algorithm that uses the TV divergence to improve training robustness to increased asynchronicity.", "keywords": ["On-Policy RL", "Asynchronous RL", "Policy Lag", "RLHF"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebbe51766383d99da057b9b4d029ef114f25afe8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on the policy lag issue in asynchronous on-policy reinforcement learning. The policy lag refers to the mismatch between the behavior policy used to collect the data and the evolution of the target policy while being updated. The paper analyzes the policy lag, noting that it is composed of two more fundamental components: the backward lag (due to an initial difference between the target policy and the asynchronously collected data), and the forward lag, due to the policy updates that further move the policy away from the collected data. On-policy methods like TRPO and PPO use a trust-region method designed to contain the forward policy lag. However, in PPO, the clipping surrogate objective does not effectively contain the policy lag. \n\nThe paper develops a novel update rule, combining two powerful ideas: one is to construct a total-variation gradient update that targets both components of the policy lag, while precisely filtering \"bad gradients\", and 2) better aligns the advantage estimation (which is used in the gradient updates) by using the v-trace target (from IMPALA) targeting the backward lag. \n\nThe novel algorithm (VACO: (Total Variation-based Advantage-aligned Constrained policy Optimization) can be used combined with other algorithmic architectures.\n\nFor this reason, the authors investigate the use of their updates alongside PPO on MuJoCo tasks and alongside GRPO on Grade School Math using an LLM architecture. The results obtained with 4 and 3 seeds, respectively, show improved performance compared to classic baselines."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "This paper addresses a significant problem in asynchronous reinforcement learning. The forward aspect of the policy lag is also common in synchronous on-policy reinforcement learning (where consecutive policy updates actually create a distribution mismatch that is often overlooked in the on-policy literature). \n\nThe idea of using a total-variation filtering to prevent policy degradation and a \"re-alignment\" of the advantage estimation is sound. \n\nThe use of the performance difference lemma to show the backward and forward components of the policy lag is also sound, connecting the authors' intuition to an objective, mathematical understanding. The gradient in equation 15 is, to the best of my knowledge, novel, and the filtering mechanism presented in eq. 16 is sound. \n\nOverall, the paper addresses a significant problem and proposes a sound, interesting analysis and solution."}, "weaknesses": {"value": "The paper, in my opinion, contains several flaws that undermine the strengths highlighted above.\n\nClarity\n---------\n\nI find several aspects of the paper obscure. The most important thing is the mathematical notation. The policies $\\pi, \\pi_T, \\beta_T$ have never been properly introduced. The description in Lemma 2 is not sufficient. What does \"the iteration T\" refer to? Collection iteration or policy improvement iteration? And why would the behavior policy change during the improvement iteration? And what is the difference between $\\pi$ and $\\pi_T$. This notation, used in the paper to present both the analysis and the results, must be clarified. Otherwise, it obscures the analysis and understanding of the algorithm. Figure 1 is not clear to me, as I am unsure what the authors want to convey. Figure 2, IMPALA vs Advantage Realignment, remains also obscure to me. I think I understand what is the difference beween the advantage realignment proposed by the authors and the one in IMPALA, but the figure itself, is not adding any information that I am able to understand. \n\nThe decision to use \"advantage realignment\" only at the beginning of the optimization seems poor or not well justified to me, especially given the author's objective of reducing the policy lag's impact on the policy update. IMPALA continuously re-aestimated the advantage exactly to target the \"forward lag\". I do not find convincing the authors's motivation that \"this avoidance of repeated calculations makes their algorithm significantly more computationally efficient\": what makes policy-gradient algorithms is computing gradients, but the V-trace simply corrects the GAE by using clipped importance sampling ratios. I would find much more credible to avoid repeating V-trace estimates in case they would increase too much the estimation variance (known problem for these kind of importance-sampling estimators). This point, if left unclarified, undermines the soundness of the proposed algorithm.\n\nThe authors claim in the introduction that the total variation captures both forward and backward lag (which I agree), but later state that the TV divergence targets the forward lag only (053--074). \n\nEmpirical Evaluations\n-----------------------------\n\nThe paper's contribution revolves around two ideas: 1) using TV filtering and 2) using advantage re-alignment. I am unsure why the authors would not check the _independent_ contributions of these two tools. The empirical evaluation is solely focused on testing the whole recipe, leaving unquestioned the contributions of the two components. \nIn particular, 1) the intuition of TV-filtering depicted in Figure 2, where the filtering mechanism should remove gradients pointing outside the trust region, is not supported empirically. 2) The intuition that the advantage realignment improves the backward lag effect is also not supported empirically. \n\n__Using four seeds for the MuJoCo tasks is absolutely not enough__. It is not clear on which data-aggregation level the median, quantile, and mean statistics refer: on four data points (one for each seed), or on n_environment x n_seed, or on n_episodes x n_environments x n_seeds? \nThe authors do not explain what the optimality gap is, and if it is simply 1 - normalized_score, it would be redundant information given the rest of Figure 3. \nIt is not clear to me why the authors use PPO-KL (the explanation in 410-411 is not sufficient to me), and why they do not show PPO-Clip.\n\nThe paper does not present any hyperparameter sensitivity.\n\nSummary of Weakness Points\n-----------------------------------------\n\n1) Some fundamental parts of the paper have been poorly introduced.\n2) It is not clear whether the TV filtering targets only the forward or both components of the policy lag. \n3) It is not clear why the authors use the V-trace only at the beginning of the optimization, and not continuously: I find the computational efficiency argument unconvincing\n4) The two main contributions of the paper have not been tested independently.\n5) The number of seeds used in the paper is not sufficient to guarantee statistical significance. This can be understandable for the LLM experiment, but not for the MuJoCo environment. \n6) It is not clear why the authors do not compare to PPO-Clip in the experimental section. \n7) No hyperparameter sensitivity has been conducted (i.e., what is the influence of $\\delta$)."}, "questions": {"value": "Suggestions/Questions\n-------------------------------\n\n1) Clarify all points I wrote(/questioned) above.\n2) The sentence in 297--299 is unclear.\n3) Balance the parentheses in equation 10 using \\left and \\right.\n4) Figure 1 looks out of place, or is not well explained, and it is presented on page 2, while only referenced on page 8. \n5) I would encourage the authors to present a bit better the whole asynchronous setup, contextualizing and clarifying the mathematical notation. This can be done in the background section.\n6) The authors claim to have performed 100M steps in the MuJoCo environment. Is this a sum of all experiments/algorithms, or is it intended per single run? 100M per single run would seem excessive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UvNk8iYvf8", "forum": "RzcxXjLCsm", "replyto": "RzcxXjLCsm", "signatures": ["ICLR.cc/2026/Conference/Submission10141/Reviewer_H8Np"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10141/Reviewer_H8Np"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761562435067, "cdate": 1761562435067, "tmdate": 1762921513181, "mdate": 1762921513181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles policy lag in on-policy RL, and categorizes lag into backward lag and forward lag. This paper derives performance bounds using TVD and then propose VACO which combines advantage realignment and TV-based filtering. Experiments on MuJoCo and LLM reasoning tasks show the effectiveness of VACO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Policy lag is a real bottleneck in scaling on-policy RL and LLM fine-tuning. The paper’s framing of forward vs backward lag is conceptually clear and useful.\n2. Demonstrating both robotics and LLM reasoning experiments is good."}, "weaknesses": {"value": "1. The filtering rule (Eq. 16) is heuristic; there is no formal proof that it guarantees constraint satisfaction or unbiased gradient estimates.\n2. The use of TV divergence in continuous action spaces is only approximated by sample-based density ratios (Eq. 5).\n3. 4 seeds are statistically insufficient for bootstrap confidence intervals, the CIs may not be meaningful.\n4. Here is my major concern. It’s quite surprising that VACO needs 100 million steps to show improvements on MuJoCo, these are relatively simple continuous-control tasks where PPO usually reaches strong performance within a few million steps. That makes VACO’s sample efficiency look questionable. It seems the method only starts to outperform after very long training, possibly because the filtering and advantage realignment slow down learning. Also, the paper only shows final results, without any training curves, so it’s hard to tell whether VACO is truly more robust or just learning more slowly over time. If the authors can provide a clear explanation for why VACO requires such an extensive training horizon and clarify its sample efficiency compared to PPO, I would be willing to raise my score."}, "questions": {"value": "1. The theory uses TV divergence, but empirical implementations rely on a sampled absolute ratio. Does this sample-based surrogate preserve the same monotonic improvement guarantees, especially in continuous space? Further, given the Pinsker's inequality, why not use the KL-divergence for your theory part? As I find in page 8, you refer to Pinsker'sinequality to minimize policy lag for PPO. \n\n2. Claiming “no extra hyperparameters” is misleading. $\\delta$ although is fixed to 0.2, but it seems to be a effective parameter. Also, how sensitive is VACO to $\\delta$? Does the filtering frequency oscillate, causing instability across updates?\n\n3. Is the ratio computed per dimension, per sample, or approximated by log-prob differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tkV7nZRIEA", "forum": "RzcxXjLCsm", "replyto": "RzcxXjLCsm", "signatures": ["ICLR.cc/2026/Conference/Submission10141/Reviewer_75uv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10141/Reviewer_75uv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901993406, "cdate": 1761901993406, "tmdate": 1762921512841, "mdate": 1762921512841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies policy lag in asynchronous on-policy RL. In particular, they paper defines two types of lag: backward lag (a mismatch between behavior and learner at the start of optimization), and forward lag (policy starts diverging from the data distribution after multiple updates on the same batch of data). To mitigate lag, the paper proposes VACO which has two core features: (1) advantage realignment, a V-trace-style off-policy estimate aligned to the learner, and (2) TV-divergence–based filtering which removes per-sample gradients predicted to increase TV beyond a threshold. Experiments show robustness benefits over PPO and SPO in MuJoCo tasks and in a simple RLVR tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper theoretically characterizes lag. While it seems obvious to me that lag would interfere with policy optimization, it's very useful to show this mathematically.\n2. Experiments on both RLVR for LLMs and more standard RL tasks (MuJoCo) is a nice plus. The \"RL for LLMs\" feels too distinct from the broader RL community, so I'm glad that people are writing papers that evaluate on both setups."}, "weaknesses": {"value": "I lean to reject due to limited experiments and missing discussion on existing works for managing off-policy-ness during on-policy learning.\n\n1. **On-policy learning with Off-policy data**. My understanding is that the policy lag problem is simply the problem of trying to perform on-policy updates with off-policy data. Several works have studied how to leverage off-policy data for on-policy updates, especially importance sampling methods like Queeney et al 2021, but the only baselines considered are PPO with different KL penalties and SPO. In the  Why not compare to importance sampling methods? Is there something about the asynchronous setup such that making existing off-policy correction methods impractical?\n\n2. **Statistical Significance.** Results in Figure 3 are averaged over 4 seeds, which is too few to draw reliable conclusions. Prior work (Henderson et al., AAAI 2018) shows that even identical RL algorithms can appear statistically distinct when evaluated on a small number of trails (like n=5). \n\n3. **Novelty.** To my knowledge, the formal analysis of lag is novel, though VACO itself is quite similar to TRPO’s TV-based bounds and IMPALA’s V-trace. The paper claims two differences: (1) single-shot advantage realignment (vs. IMPALA’s continuous realignment) and per-sample TV-aligned filtering (vs. PPO clipping). What can VACO achieve that a well-tuned KL-penalized PPO or SPO cannot?"}, "questions": {"value": "1. To preface this question, I think the decision to focus on on-policy methods in this paper is reasonable and justified. Nevertheless, in asynchronous settings with significant lag, wouldn't it make more sense to revert back to off-policy methods rather than try to learn on-policy with increasingly off-policy data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D43yfglZxN", "forum": "RzcxXjLCsm", "replyto": "RzcxXjLCsm", "signatures": ["ICLR.cc/2026/Conference/Submission10141/Reviewer_rsdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10141/Reviewer_rsdg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974079966, "cdate": 1761974079966, "tmdate": 1762921512429, "mdate": 1762921512429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RL algorithm named Total Variation–based Advantage-aligned Constrained policy Optimization (VACO) to mitigate policy lag in distributed/asynchronous on-policy RL. It provides supporting theory and validates the approach on MuJoCo control and LLM reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers solid, well-structured theoretical analysis that clearly supports the method.\n\n2. The proposed approach targets a relevant problem in distributed/on-policy RL.\n\n3. The experiments stay focused on the core question, with detailed comparisons provided in the appendix."}, "weaknesses": {"value": "Limited sensitivity analysis of the TV threshold. The method fixes the throshold and does not ablate it. It’s unclear how training stability and performance depends on throshold ,"}, "questions": {"value": "Could the authors provide an ablation and guidance on choosing the throshold in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xB9urC6epy", "forum": "RzcxXjLCsm", "replyto": "RzcxXjLCsm", "signatures": ["ICLR.cc/2026/Conference/Submission10141/Reviewer_MnL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10141/Reviewer_MnL1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975509340, "cdate": 1761975509340, "tmdate": 1762921512012, "mdate": 1762921512012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}