{"id": "5XvwXzPRE5", "number": 6137, "cdate": 1757954139955, "mdate": 1759897933509, "content": {"title": "BasePrompt: Self-Prompting Genome Language Models for RNA Fitness Prediction", "abstract": "Genome Language Models (GLMs) pre-trained on trillions of nucleotides already exhibit strong zero-shot RNA fitness predictors, yet they cannot be steered toward a specific assay the way a language model is steered by a prompt.\nWe close this gap by letting GLMs prompt themselves.\nOur method, BasePrompt, asks GLMs to propose short nucleic-acid prefixes and postfixes that maximally activate the fitness signal for a given sequence.\nTo overcome the causal, forward-only nature of most GLMs, we exploit reverse-complement symmetry and generate upstream as well as downstream prompts without ever updating weights or using labeled variants.\nFor zero-shot RNA fitness prediction on RNAGym, BasePrompt achieves a 6.0\\% relative improvement over the SOTA Evo2 7B model and 6.6\\%–16.4\\% over other GLMs, as measured by Spearman correlation.\nAuxiliary DNA tasks show the same prompting method compresses native-context information into shorter, model-aligned tokens, boosting pathogenicity classification and next-k-base prediction.", "tldr": "BasePrompt enables Genome Language Models (GLMs) to self-prompt for RNA fitness prediction, improving performance without labeled data by leveraging reverse-complement symmetry, achieving significant gains over existing models.", "keywords": ["RNA fitness prediction", "Prompting", "Genome Language Model", "DNA", "Inference-time", "Test-time"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/344f807ecb2b95bc572f598902cd3f56863d200b.pdf", "supplementary_material": "/attachment/3f0c673ffc39e69690799ac24c1a83488c32f2b1.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents BasePrompt, an inference-time approach that asks a GLM to generate short 5′/3′ DNA prompts and concatenates them to nucleotide sequences to improve zero-shot fitness or variant effect prediction. The authors report consistent improvements on RNAGym and other benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Adapting prompting to genomic sequences and leveraging reverse-complement symmetry is an interesting and effective idea.\n- Consistent improvements are reported on several benchmarks.\n- Figures are easy to follow."}, "weaknesses": {"value": "Major\n- Reported gains (e.g., Spearman from 0.271 to 0.289 in Figure 5) lack confidence intervals or statistical tests. Without per-assay distributions and significance testing, one cannot tell whether improvements are robust.\n- The abstract and introduction claim the method \"asks GLMs to propose... prefixes and postfixes that maximally activate the fitness signal for a given sequence.\" This description strongly implies a task-aware optimization process. However, the actual method described in this paper is standard, task-agnostic autoregressive generation.\n\nMinor\n- Figure 9: Complement Srtand -> Complement Strand\n- Figure 10-12: Stand -> Strand."}, "questions": {"value": "- Can you please provide the absolute (non-normalized) Spearman r and AP values for the experiment in Figure 6?\n- Does using random sequences (like RandSeq or genomic sampling) as prompts improve performance on the RNAGym benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5A6upYPq7Z", "forum": "5XvwXzPRE5", "replyto": "5XvwXzPRE5", "signatures": ["ICLR.cc/2026/Conference/Submission6137/Reviewer_w9Ln"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6137/Reviewer_w9Ln"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822463099, "cdate": 1761822463099, "tmdate": 1762918494592, "mdate": 1762918494592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BasePrompt, a self-prompting method for Genome Language Models (GLMs) that generates contextual sequence prefixes and suffixes to improve zero-shot RNA fitness prediction. Unlike text prompts, these are biologically meaningful subsequences constructed using reverse-complement symmetry, enabling bidirectional context without fine-tuning. Evaluations on RNAGym (70 RNA DMS assays), ClinVar, and Next-base Prediction tasks show consistent gains—up to 16% improvement in Spearman correlation over strong GLM baselines (Evo1–Evo2, GENERator). BasePrompt introduces no extra training and minimal inference overhead, establishing a new paradigm for in-context learning in genomics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work presents a neat and simple trick for DNA language models, extending prompting to non-text biological sequences through a self-generated, symmetry-aware context mechanism.\n* It achieves state-of-the-art zero-shot performance across multiple benchmarks, outperforming larger GLMs without any retraining.\n* The method is architecturally general, working consistently across diverse GLMs and demonstrating robustness and scalability."}, "weaknesses": {"value": "I did not observe any obvious weaknesses in this approach. However, it’s worth noting that the method remains fundamentally prompting-based in nature."}, "questions": {"value": "1. When performing the inverse transcription process, how is RNA splicing handled? Certain RNA types such as mRNA contain introns in their original DNA sequences. Have the authors conducted any experiments or analyses related to this?\n2. Regarding the prompting mechanism, what is the exact value of N used for both ends? How was the optimal prompt length determined? Was it tuned empirically, or did the model simply continue until the end of the sequence token?\n3. In Table 9, performance drops are observed for Evo-1 on tRNA and Evo-2 on Ribozyme. Have the authors conducted any deeper analysis to understand the cause of these declines?\n4. Could the authors provide an assay-level performance breakdown of their methods in the RNAGym benchmark similar to Supplementary Figure 2?\n5. Could the authors also present a species-level performance breakdown, for example perf. changes on viral and eukaryotic RNAs, to better interpret the generalization behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ClhuITUAx6", "forum": "5XvwXzPRE5", "replyto": "5XvwXzPRE5", "signatures": ["ICLR.cc/2026/Conference/Submission6137/Reviewer_KCcA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6137/Reviewer_KCcA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933202047, "cdate": 1761933202047, "tmdate": 1762918494268, "mdate": 1762918494268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BasePrompt is a self-prompting framework for genome language models (GLMs) that generates bidirectional nucleotide prompts to enhance zero-shot RNA fitness prediction.\nThe method addresses two key challenges: the lack of effective task-specific steering mechanisms for genomic LMs, and the inherent unidirectionality of causal language models, which limits contextual information extraction.\nTo overcome these, BasePrompt leverages the reverse-complement symmetry of DNA to generate both upstream (5′) and downstream (3′) prompts autoregressively, enabling the GLM to simulate bidirectional context without any model fine-tuning or supervision.\nIt is evaluated on RNAGym, a large benchmark comprising 70 deep mutational scanning assays, and compared against several genome-scale GLMs, showing consistent zero-shot improvements across multiple metrics; additional DNA tasks such as ClinVar variant classification and next-base prediction support the method’s generality.\nBasePrompt is efficient, model-agnostic, and improves predictive accuracy without additional training, but its evaluation omits comparisons to mRNA-specific models and established biological UTRs, limiting the strength and generality of its conclusions.\nIn summary, BasePrompt is a novel and computationally efficient method that introduces self-prompting to genome language models, yielding consistent but modest zero-shot gains in RNA-related prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Innovative Self-Prompting Concept\n\nThe idea of letting a genome language model prompt itself is novel and interesting. It creatively adapts the prompt engineering paradigm from NLP to the genomics domain, where “prompting” is not straightforward. The use of reverse-complement sequences to create both 5’ and 3’ prompts is a clever solution to exploit bidirectional context despite using a unidirectional (causal) model.\n\n## Robustness and Efficiency\n\nThe method is shown to work across multiple model sizes and variants, indicating generality. Moreover, it’s computationally efficient – prompts are generated only for a fixed set of reference sequences (e.g. the 70 assay reference RNAs in RNAGym) rather than for every single variant. This means the overhead is low relative to the huge number of predictions made, making the approach practical for large-scale inference."}, "weaknesses": {"value": "## Omission of relevant mRNA-specific baselines\n\nA major concern is that the comparisons focus only on genome-scale or noncoding RNA language models that were never trained on mRNA data, whereas the task of RNA fitness prediction is about mRNA. In effect, the authors compare BasePrompt-augmented models against baselines that are out-of-domain for mRNA (e.g. generic genomic models or models trained on lncRNAs), which is akin to comparing against a misaligned or even random baseline for those specific tasks. Crucially, the evaluation excludes leading mRNA-focused models such as mRNA-FM, CaLM, and Uni-RNA, which are large pretrained models explicitly trained on coding RNA/transcript data. These specialized models likely represent the strongest baselines for mRNA-related fitness predictions, and ignoring them undermines the rigor and fairness of the evaluation. As a result, the reported improvements might be overstated – we cannot tell if BasePrompt would still outperform or even match these state-of-the-art mRNA-trained models, since they were not included. This gap in baseline selection affects the soundness of the experimental claims: without directly comparing to the most relevant prior models, the paper’s claim of superior performance is not fully convincing or generalizable to practical settings where one would naturally use an mRNA-trained model.\n\n## Lack of real-world UTR baseline comparison\n\nAnother weakness is in how the paper evaluates the quality of the prompt sequences (especially for tasks involving untranslated regions, e.g. 3’ or 5’ UTRs). The authors generate synthetic upstream/downstream prompts, but they do not compare these prompts against any well-known, biologically-established UTR sequences. In synthetic biology and mRNA engineering, certain UTR sequences are commonly used because they are known to strongly influence expression. For example, the β-globin 3′ UTR is a widely used “classic” UTR element (often appended to mRNAs to enhance stability/translation). Such sequences provide a real-world baseline for performance. By failing to compare BasePrompt’s generated UTR prompts to any standard UTR (like β-globin or other commonly used regulatory UTRs), the authors miss an important check: Are the prompts that BasePrompt finds actually more informative or effective than simply using a known strong UTR? Without this comparison, it is hard to gauge the biological plausibility and practical advantage of the prompts. This omission limits the generality of the conclusions – for instance, if a simple known UTR could achieve similar or better fitness prediction signal when appended, then BasePrompt’s advantage might not be as significant in real laboratory or clinical contexts. In summary, not benchmarking against real-world sequences means we cannot yet conclude that BasePrompt’s prompt-generation is yielding truly superior or meaningful sequences compared to what practitioners already use."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oqMBs2RemU", "forum": "5XvwXzPRE5", "replyto": "5XvwXzPRE5", "signatures": ["ICLR.cc/2026/Conference/Submission6137/Reviewer_H8MK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6137/Reviewer_H8MK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075971243, "cdate": 1762075971243, "tmdate": 1762918493761, "mdate": 1762918493761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BasePrompt, a method that enhances genome language models (GLMs) for zero-shot RNA fitness prediction by letting them generate their own prompts. Unlike traditional NLP prompting, where users manually provide input cues, BasePrompt allows GLMs to create short nucleic acid prefixes and suffixes to boost prediction accuracy, without needing labeled data or model fine-tuning.\n\nTo overcome the unidirectional nature of autoregressive GLMs (which only look forward), the authors exploit the reverse-complement symmetry of DNA, enabling bidirectional prompt generation (both upstream and downstream of a sequence). These prompts are then concatenated to RNA sequence variants before prediction, improving performance across tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual Originality\n\nThe paper proposes the idea of self-prompting genome language models (GLMs) that utilize reverse-complement symmetry for bidirectional inference. This represents a creative adaptation of prompt-based reasoning from NLP to genomics. While not groundbreaking, it introduces a fresh perspective on how pretrained sequence models can internally generate task-relevant prompts. However, since reverse-complement techniques are well-known in genomic modeling, the originality is moderate rather than strong.\n\n2. Methodological Quality\n\nThe experimental results show consistent but modest improvements across several Evo-series models, with reported gains of 6–16% in Spearman correlation. The methodology is clearly described at a conceptual level, and the experiments are competently executed on the RNAGym benchmark. Nonetheless, the paper lacks statistical validation, ablation studies, and robustness analyses, limiting confidence in its empirical rigor. As a result, the technical quality is adequate but below top-tier standards.\n\n3. Presentation and Clarity\n\nThe paper is well-structured and easy to follow, with clear motivation, figures, and narrative flow. The visual explanations of BasePrompt’s bidirectional prompting mechanism are particularly helpful. However, some implementation details—such as prompt generation procedures, sampling settings, and parameter sensitivity—are under-specified. This reduces transparency and reproducibility. Overall, the presentation is clear but not deeply explanatory.\n\n4. Scientific and Practical Significance\n\nBasePrompt touches on a promising intersection between language modeling and biological inference, suggesting that large genome models may self-optimize through internal prompting. Despite this potential, the impact is limited by the narrow experimental scope (focused mainly on RNA fitness prediction) and the strong dependence on Evo-series GLMs. The contribution is thus incremental rather than transformative for the field of computational genomics."}, "weaknesses": {"value": "1. Insufficient Theoretical Foundation and Interpretability\n\nThe paper lacks a deep theoretical explanation for why the proposed method is effective. The performance improvements of BasePrompt are demonstrated empirically, but without strong biological or information-theoretic justification. Moreover, the biological meaning of the generated “self-prompts” is not explored — it remains unclear whether they correspond to functional motifs or biologically relevant patterns.\n\n2. Limited Experimental Scope and Questionable Generalizability\n\nExperiments are mainly conducted on a single benchmark, RNAGym, without evaluation on other RNA-related tasks such as secondary structure prediction, RNA–protein binding, or splicing. This narrow scope limits the evidence for BasePrompt’s robustness and general applicability across broader biological contexts.\n\n3. Strong Dependence on Specific Models and Lack of Statistical Validation\n\nThe method heavily depends on the Evo-series genome language models (GLMs), with limited testing on other architectures. Additionally, the paper does not include statistical significance testing or detailed error analysis, making it difficult to assess the reliability and generality of the reported performance gains.\n\n4. Limited Novelty and Biological Validation\n\nThe main innovation—using reverse-complement symmetry for bidirectional prompting—builds on ideas already explored in genomic modeling. While the approach provides an inference-time improvement, it lacks deeper biological validation or theoretical advancement. As a result, both its novelty and biological significance are somewhat limited."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f6BIRDyUc3", "forum": "5XvwXzPRE5", "replyto": "5XvwXzPRE5", "signatures": ["ICLR.cc/2026/Conference/Submission6137/Reviewer_gC5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6137/Reviewer_gC5x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174628858, "cdate": 1762174628858, "tmdate": 1762919952177, "mdate": 1762919952177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}