{"id": "E3bBZ02Qcc", "number": 12775, "cdate": 1758210203846, "mdate": 1759897487517, "content": {"title": "Rethinking Continual Learning with Progressive Neural Collapse", "abstract": "Continual Learning (CL) seeks to build an agent that can continuously learn a sequence of tasks, where a key challenge, namely Catastrophic Forgetting, persists due to the potential knowledge interference among different tasks. On the other hand, deep neural networks (DNNs) are shown to converge to a terminal state termed Neural Collapse during training, where all class prototypes geometrically form a static simplex equiangular tight frame (ETF). These maximally and equally separated class prototypes make the ETF an ideal target for model learning in CL to mitigate knowledge interference. Thus inspired, several studies have emerged very recently to leverage a fixed global ETF in CL, which however suffers from key drawbacks, such as *impracticability* and *limited performance*. To address these challenges and fully unlock the potential of ETF in CL, we propose **Progressive Neural Collapse (ProNC)**, a novel framework that completely removes the need of a fixed global ETF in CL. Specifically, ProNC progressively expands the ETF target in a principled way by adding new class prototypes as vertices for new tasks, ensuring maximal separability across all encountered classes with minimal shifts from the previous ETF. We next develop a new CL framework by plugging ProNC into commonly used CL algorithm designs, where distillation is further leveraged to balance between target shifting for old classes and target aligning for new classes. Extensive experiments show that our approach significantly outperforms related baselines while maintaining superior flexibility, simplicity, and efficiency.", "tldr": "", "keywords": ["Continual Learning", "Neural Collapse"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78438597c73e8f284bdd15591fe26f8edaf731ac.pdf", "supplementary_material": "/attachment/7677b9a52e42ff0f06b657fca13f2994ae0922c6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Progressive Neural Collapse (ProNC), a continual learning framework inspired by *Neural Collapse*. This phenomenon describes how deep networks, at convergence, produce class features that collapse to a single point for each class creating simplex equiangular tight frames (ETFs), creating orthogonal class prototypes and maximizing separation between classes. ProNC proposes to progressively expand the ETF as new tasks arrive, maintaining geometric consistency and feature separability without prior knowledge of all class counts. The framework integrates three loss components: cross-entropy for supervision, an alignment loss enforcing ETF-based feature geometry, and a distillation loss preserving past representations (to mitigate forgetting). The method is evaluated on standard benchmarks (Seq-CIFAR10/100 and Seq-TinyImageNet) under Class-IL and Task-IL setups, reporting substantial performance gains across datasets and memory budgets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of progressively adapting the ETF target during continual learning without knowing the number of total classes in advance is novel and addresses the shortcomings of fixed ETF methods for CL.\n\nThe paper is built on a convincing motivation. The reasoning is coherent and carefully developed, making the overall argument both logical and easy to follow."}, "weaknesses": {"value": "### **Major Weaknesses**\n\n1. **Questionable baseline performance values**: in Table 1, several baseline results (Co$^2$L, CILA, MNC$^3$L, STAR) are notably lower than those reported in their original papers (where they surpass the results from the proposed ProNC). This discrepancy indicates possible reproduction or configuration issues, undermining the fairness and credibility of the comparison and invalidating the paper’s main “state-of-the-art” claim.\n\n2. **Missing baselines**: some important methodologies are missing in the experimental evaluation:\n   - XDER (Boschini et al., TPAMI 2022, also cited by the authors);\n   - GCR (Tiwari et al., CVPR 2022, also cited by the authors);\n   - LODE (Liang and Yi, NeurIPS 2023).\n\n3. **Limited novelty relative to NCT**: NCT (Yang et al., 2023, also cited by the authors and reported in the main table) already introduced ETFs in class-incremental learning. As far as I can tell, the main innovation of the proposed method compared to NCT is the removal of the \"known number of classes in advance\" assumption. While this is an interesting modification, it alone does not seem sufficient to constitute a truly novel methodology.\n\n4. **Limited significance of ablation studies**: most ablation experiments in Figure 2 examine only a very narrow range of values (from about 2e-1 at best to 1e-2 in some cases) and consider just one dataset. Such a limited scope restricts the interpretability of the results, as the observed variations may not represent a consistent behavior across the full cosine similarity range (–1 to 1).\n\n### **Minor Weaknesses**\n5. Clarity issue in Section 3.1 (point 2): the explanation of the proposed methodology in this part is somewhat convoluted and would benefit from clearer exposition to improve readability and understanding.\n\n6. Missing no-buffer baseline for ProNC: Table 1 does not report ProNC’s performance without a replay buffer (which is reported in the text only). Also inserting it in the main table would increase clarity and allow a fair comparison w.r.t. replay-free methods."}, "questions": {"value": "While initializing the ETF at the end of the first task close to the optimal solution of the optimization problem is an interesting idea, for subsequent tasks the ETFs are initialized orthogonally to the previous ones but otherwise randomly, without any guiding heuristic. Do you have any thoughts on how this initialization process could be improved for later tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wB1NuCxuIx", "forum": "E3bBZ02Qcc", "replyto": "E3bBZ02Qcc", "signatures": ["ICLR.cc/2026/Conference/Submission12775/Reviewer_fz6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12775/Reviewer_fz6X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793953579, "cdate": 1760793953579, "tmdate": 1762923587184, "mdate": 1762923587184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Progressive Neural Collapse (ProNC), a continual learning framework inspired by the Neural Collapse phenomenon.\nProNC progressively constructs and expands an Equiangular Tight Frame (ETF) to align class features across tasks.\nAfter each task, it estimates the “closest ETF” from class means and expands the basis for new classes using Gram–Schmidt orthogonalization, keeping all class representations approximately equiangular.\nThe model jointly optimizes cross-entropy, feature-alignment, and distillation losses.\nExperiments on Seq-CIFAR-10/100 and Seq-TinyImageNet under both Class-IL and Task-IL show consistent gains over strong baselines (DER++, NCT, Co2L, STAR) with good efficiency and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Grounded in Neural Collapse geometry, offering an interpretable view of feature alignment in continual learning. Achieves strong results without complex contrastive or generative modules.\n2. Works as a plug-in regularizer across different CL frameworks (e.g., ER, iCaRL, DER++)."}, "weaknesses": {"value": "1.\tThe method assumes clear task segmentation (task-aware setting); its applicability to task-free or online CL remains untested.\n2.\tAs the ETF expands over many tasks, orthogonality may gradually degrade; this possible effect is not analyzed experimentally.\n3.\tGram–Schmidt expansion could become unstable when the number of classes approaches the embedding dimension; only small-scale datasets and ResNet-18 (d ≤ 512) were tested."}, "questions": {"value": "When the number of tasks grows large, does ETF orthogonality noticeably degrade? Would periodic re-fitting help?\nCan ProNC remain stable with higher-dimensional embeddings (e.g., ViT features) or larger datasets such as ImageNet-100?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jhwcyoYvz4", "forum": "E3bBZ02Qcc", "replyto": "E3bBZ02Qcc", "signatures": ["ICLR.cc/2026/Conference/Submission12775/Reviewer_hVBn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12775/Reviewer_hVBn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832995117, "cdate": 1761832995117, "tmdate": 1762923586680, "mdate": 1762923586680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the Neural Collapse Terminus (NCT; Yang et al., 2023b, arXiv) and the ICLR 2023 work by Yang et al. (2023a), aiming to address their limitations when applying the neural collapse (NC) phenomenon to continual learning (CL). In NC-based CL, the target ETF (simplex equiangular tight frame) is typically predefined, which requires knowledge of the total number of classes and may degrade discriminability when the class number becomes large. To overcome this, the paper proposes Progressive Neural Collapse (ProNC), a method that dynamically adjusts and expands the target ETF throughout the CL process, building on the NCT framework. Experimental results show that ProNC achieves consistent and significant improvements over existing CL baselines, particularly compared with NCT. Moreover, the proposed regularization approach proves beneficial even when combined with other CL frameworks, as confirmed through ablation studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a clear exposition of the background NC theory and explains its own contributions in a well-structured manner.\n2. The proposed method is well-motivated by the identified limitations of prior NC-based CL works, and the use of Theorem 1 introduces a moderately novel and theoretically grounded component.\n3. Comprehensive experiments demonstrate consistent and noticeable gains over a range of baselines, supporting the empirical validity of the approach."}, "weaknesses": {"value": "1. The technical novelty remains limited compared with the preliminary works (Yang et al., 2023a,b). The paper reads largely as a continuation of this prior line of research, where NC-based CL formulations have already been thoroughly explored.\n2. The second main contribution—the ProNC-based CL framework—largely mirrors the loss formulation of NCT (Yang et al., 2023b). While Section 3.1 introduces a genuinely new idea, Section 3.2 appears nearly identical to the corresponding part in NCT.\n3. Some reproduced baselines yield noticeably lower accuracies than those reported in their original papers. For example, ER on Task-IL with Seq-CIFAR-100 using ResNet-18 has been reported above 70% (e.g., GPM, ICLR 2021), yet only 60.19% here, raising concerns about the faithfulness of baseline reproduction."}, "questions": {"value": "1. Could the authors elaborate more explicitly on how the proposed ProNC framework differs technically from NCT (Yang et al., 2023b)?\nBeyond the dynamic ETF expansion, are there any additional algorithmic or theoretical components that are genuinely new rather than adapted from NCT?\n2. Some reproduced results (e.g., ER on Seq-CIFAR-100 Task-IL) are considerably lower than in prior works such as GPM (ICLR 2021).\nCould the authors detail the reproduction settings (e.g., data augmentation, optimizer, training epochs) and justify whether these differences could account for the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c8bW3Az4tz", "forum": "E3bBZ02Qcc", "replyto": "E3bBZ02Qcc", "signatures": ["ICLR.cc/2026/Conference/Submission12775/Reviewer_cHhJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12775/Reviewer_cHhJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012012433, "cdate": 1762012012433, "tmdate": 1762923586436, "mdate": 1762923586436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}