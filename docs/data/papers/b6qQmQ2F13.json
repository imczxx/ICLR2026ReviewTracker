{"id": "b6qQmQ2F13", "number": 1869, "cdate": 1756955828813, "mdate": 1763738690085, "content": {"title": "Not All Bits Are Equal: How Model Scale Changes Memory-Optimal Reasoning", "abstract": "While 4-bit quantization has emerged as a memory-optimal choice for non-reasoning models and zero-shot tasks across scales, we show that this universal prescription fails for reasoning models, where KV cache rather than model size can dominate memory. \nThrough systematic experiments on mathematical and knowledge-intensive reasoning tasks, we find a scale-dependent trade-off: models with an effective size below 8-bit 4B parameters achieve better accuracy by allocating memory to larger weights, rather than longer generation, while larger models benefit from the opposite strategy. \nThis scale threshold also determines when parallel scaling becomes memory-efficient and whether KV cache eviction outperforms KV quantization. \nOur findings show that memory optimization for LLMs cannot be scale-agnostic, while providing principled guidelines: for small reasoning models, prioritize model capacity over test-time compute, while for large ones, maximize test-time compute. \nOur results suggest that optimizing reasoning models for deployment requires fundamentally different strategies than those established for non-reasoning ones.", "tldr": "", "keywords": ["large language models", "reasoning", "efficiency", "model compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cd7823d7294a6223a9aa1694fb69e113d6f08fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how to best spend limited memory for reasoning-time compute: more bits for weights vs. longer generations vs. parallel sampling, and how KV-cache strategy (quantize vs. evict) shifts with model scale. Experiments center on Qwen3 variants and two reasoning benchmarks, AIME25 and GPQA-Diamond, claiming a threshold around an “8-bit 4B effective size”, below which weight precision pays more and above which test-time compute (longer or parallel) pays more. The setup is careful inside its sandbox, but it feels like a closed kitchen rather than a street test."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors execute a clean empirical grid: weight quantization (GPTQ-style), KV compression (HQQ, eviction), token budget, and Best-of-N voting. Within this box, the trends are coherent and useful for practitioners who must fit models under tight VRAM. The memory accounting for weights and KV is neat and transparent, and the two chosen benchmarks are recognizable to the community: AIME-style math and GPQA-Diamond for graduate-level STEM knowledge (AIME25 and GPQA-Diamond are standard touchstones by now)."}, "weaknesses": {"value": "For reasoning under fixed memory, there is now a whole ecosystem of **external** baselines that you neither compare nor even acknowledge with experiments. Self-Consistency (SC) is the canonical Best-of-N baseline for chain-of-thought; if you do parallel sampling, you must include SC as a reference decoding policy and report its marginal gains under the same memory budget. Also, **verifier-based** decoding and reranking—PRMs from *Let’s Verify Step by Step* and newer math critics—are exactly what deployment people try before touching KV tricks; ignoring them weakens the claim that “capacity vs. test-time compute” is the main axis. These are not exotic: SC is classic, PRM800K and process reward models are standard, and recent math verifiers show real lift. \n\nOn the KV story, the paper pits quantization vs. a couple of eviction settings, but the field moved fast: StreamingLLM (attention-sink), SnapKV, Ada-KV and follow-ups, and fresh 2025–2026 eviction analyses and hybrids. If you argue “evict beats quantize” in certain regimes, please test across these families, otherwise the conclusion feels fragile to method choice. Recent studies also show fragility of eviction heuristics across layers/heads/tasks; you must address that, or at least show sensitivity analyses. \n\nThe evaluation set is too narrow. AIME25 and GPQA-Diamond are fine sanity checks, but the claim is about *reasoning deployment under memory constraints*. Put your recipe into the streets: long-context multi-doc QA (LongBench/v2), multi-hop (HotpotQA), program synthesis/real repos (SWE-bench), and cross-domain text-to-SQL (Spider). If your prescriptions survive these, the paper's claim will be more valid."}, "questions": {"value": "If you add SC and PRM-reranked Best-of-N under the same VRAM cap, do your conclusions still hold? On KV methods, do SnapKV/Ada-KV/StreamingLLM change the “evict beats quantize for small models” rule, or only for certain context mixes? Finally, can you show one real-world slice, say, LongBench multi-doc QA and SWE-bench Verified Mini, where your policy improves both accuracy and cost per solved issue compared to SC or PRM-reranked decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bHIdVv2fkJ", "forum": "b6qQmQ2F13", "replyto": "b6qQmQ2F13", "signatures": ["ICLR.cc/2026/Conference/Submission1869/Reviewer_SZ2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1869/Reviewer_SZ2z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607804055, "cdate": 1761607804055, "tmdate": 1762915921844, "mdate": 1762915921844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates memory-optimal inference strategies for reasoning models under fixed memory budgets. Through systematic experiments on the Qwen3 model family (0.6B to 32B parameters) across mathematical and knowledge-intensive reasoning tasks, the authors explore trade-offs between model size, weight precision, generation length, parallel scaling, and KV cache compression. The key finding is that optimal strategies are scale-dependent rather than universal: models effectively smaller than 8-bit 4B parameters achieve better accuracy by allocating memory to larger model weights rather than extended generation, while larger models benefit from maximizing test-time compute. Additionally, mathematical reasoning tasks require higher weight precision (8/16-bit) compared to knowledge-intensive tasks where 4-bit quantization is optimal. The work challenges the conventional wisdom that 4-bit quantization is universally memory-optimal and provides principled guidelines for practitioners deploying reasoning models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's main strength is its comprehensive and rigorous empirical investigation of a practically important problem. The systematic exploration of over 1,700 configurations across multiple optimization dimensions provides valuable insights that challenge existing practices. The finding that memory-optimal strategies are fundamentally scale-dependent—with the \"8-bit 4B\" threshold determining whether to prioritize model capacity or test-time compute—is significant for practitioners. The discovery that task characteristics matter (mathematical reasoning requiring higher precision than knowledge-intensive tasks) suggests important considerations for task-specific deployment. The experimental methodology is thorough with clearly defined memory equations, and the presentation using Pareto frontier analysis effectively communicates complex trade-offs. The paper addresses a critical practical challenge as reasoning models with extended generation make KV cache memory a dominant factor, requiring rethinking of compression strategies."}, "weaknesses": {"value": "The primary limitation is the narrow evaluation scope—findings are based solely on the Qwen3 model family and two benchmarks (AIME25 and GPQA-Diamond), raising significant questions about generalizability to other architectures and reasoning domains. The paper identifies \"8-bit 4B\" as a critical threshold across multiple findings but provides no theoretical justification for why this specific scale matters, making it unclear whether this threshold would transfer to other model families or evolve as architectures change. The analysis is primarily observational, documenting empirical trade-offs without providing mechanistic understanding of why mathematical reasoning is more sensitive to quantization or why the inflection point occurs at this particular scale. The exclusion of methods using external models (verifiers, process reward models) limits practical applicability, as these are commonly used in production. While Appendix C.2 addresses batched inference scenarios, the main analysis assumes single-inference settings which may not reflect typical deployment constraints where model weights are amortized across requests."}, "questions": {"value": "The \"8-bit 4B\" threshold emerges as critical across multiple findings. Can you provide insight into why this specific effective size serves as the inflection point? Is there something fundamental about this capacity level, or could it be an artifact of the Qwen3 architecture? Have you explored whether this threshold shifts for other model families or architectural choices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N4tgJBmaQS", "forum": "b6qQmQ2F13", "replyto": "b6qQmQ2F13", "signatures": ["ICLR.cc/2026/Conference/Submission1869/Reviewer_Daoc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1869/Reviewer_Daoc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983154868, "cdate": 1761983154868, "tmdate": 1762915921413, "mdate": 1762915921413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates memory-optimal inference strategies for LLMs on reasoning tasks, operating under a fixed total memory budget. The authors challenge the prevailing consensus that 4-bit quantization is universally optimal, demonstrating that for long-generation reasoning tasks, the KV cache can become the dominant memory bottleneck. Through a systematic empirical study (over 1,700 configurations) on the Qwen3 model family, the paper analyzes the trade-offs between model size, weight precision, parallel scaling, and KV cache compression. The paper provide several key findings on the trade-offs and propose some guideline to achieve better memory efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Exhaustive Experimentation: The study is commendable for its comprehensive empirical approach, evaluating a wide range of model sizes (0.6B to 32B) and systematically comparing various KV cache optimization methods against a baseline.\n\n2. Focus on a Practical Metric: The paper correctly centers its analysis on accuracy as the core metric for evaluating inference efficiency under memory constraints. This approach has direct and significant value for real-world deployment scenarios.\n\n3. Systematic, Multi-Dimensional Analysis: The work provides a strong, multi-dimensional analysis of the complex trade-offs involved in model deployment. The resulting findings offer principled guidelines and empirical rules that can genuinely inform practical decisions in model configuration."}, "weaknesses": {"value": "1. Choice of AIME25 Benchmark: The AIME25 dataset may not be a suitable benchmark for drawing generalizable conclusions. Its small size (only 30 problems) can lead to high variance in results, and its extreme difficulty is heavily reliant on a model's peak mathematical reasoning capability. The small performance differences observed may not be sufficient evidence of a decisive performance gap.\n\n2. Model-Specific Findings: As the authors acknowledge in the limitations, the study is confined exclusively to the Qwen3 model family. This focus risks overfitting the conclusions to the specific capabilities and performance characteristics of the Qwen3 architecture.\n\n3. Limited Task Diversity: The selected benchmarks (AIME25, GPQA) primarily test long-generation reasoning. However, many real-world LLM applications involve a much wider variety of scenarios, including short-output generation and general instruction-following. The test set should ideally cover a more diverse range of practical use cases to improve the applicability of the conclusions."}, "questions": {"value": "1. The field is showing significant interest in Mixture-of-Experts (MoE) architectures. How do the authors anticipate that the difference between MoE models and Dense models would influence the conclusions presented in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eba1xYRbwJ", "forum": "b6qQmQ2F13", "replyto": "b6qQmQ2F13", "signatures": ["ICLR.cc/2026/Conference/Submission1869/Reviewer_EX7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1869/Reviewer_EX7S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995507392, "cdate": 1761995507392, "tmdate": 1762915921185, "mdate": 1762915921185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigate a set of critical research questions to improve the ability of reasoning models given limited memory constraints. The authors find that the standard 4-bit quantization used for non-reasoning models fails for reasoning-focused Large Language Models (LLMs), as their memory is often dominated by the Key-Value (KV) cache rather than the model size itself . Through systematic experiments, the authors identify a scale-dependent trade-off: smaller models (below 8 billion effective parameters) achieve better accuracy by allocating memory to higher-precision weights, while larger models benefit from allocating memory to support longer generations . This leads to the central guideline that for small reasoning models, one should prioritize model capacity, but for larger ones, the focus should be on maximizing test-time compute . This scale threshold also determines when parallel scaling becomes efficient and whether KV cache eviction is a better strategy than KV quantization. While there are enumerous new compression algorithms proposed nowadays, it is critical to review these methods from a unified view, where the memory constraint could be a good dimension."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an important and fundamental research question: how to find the best strategy to improve the reasoning ability of the models given limited memory. \n\n- The authors conduct solid experiments to verify each dimension of the problem: the precision, generation lengths, parallel sampling, KV-cache eviction, etc.\n\n- The paper is well structured and easy to follow.  The main conclusions are properly highlighted and summarized.  Figures are nice to read."}, "weaknesses": {"value": "Although the paper conducts solid experiments, I believe it is still a snapshot of the current situation, and the conclusion may still vary with different LLM families, tasks, or settings.  The paper would be more complete if the authors could provide results on\n\n- more LLM families, such as the DeepSeek-R1-distilled-Qwen families, Seed-OSS 36B, etc.;\n\n- more compression algorithms (e.g., weight–activation quantization or MXFP4/NVFP4 quantization, etc.).\n\nIn addition, it would also be helpful if the authors could compare the scale changes in more domains (especially non-reasoning tasks) under a limited-memory constraint."}, "questions": {"value": "- How can the findings of this paper be applied in practice?  Usually the LLM is served in the cloud, where each batch of user queries has context from various domains.  The optimal scaling strategy could vary from task to task.\n\n- How do you implement the parallel sampling with different group sizes?  Is it by Majority@K?\n- Some recent related works could be incorporated.\n  1. Li Z, Su Y, Yang R, et al.  Quantization meets reasoning: Exploring LLM low-bit quantization degradation for mathematical reasoning[J].  arXiv preprint arXiv:2501.03035, 2025.\n  2. Liu R, Sun Y, Zhang M, et al.  Quantization hurts reasoning?  An empirical study on quantized reasoning models[J].  arXiv preprint arXiv:2504.04823, 2025.\n  3. Zhang N, Zhang Y, Mitra P, et al.  When reasoning meets compression: Benchmarking compressed large reasoning models on complex reasoning tasks[J].  arXiv preprint arXiv:2504.02010, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lnTAyq1L7n", "forum": "b6qQmQ2F13", "replyto": "b6qQmQ2F13", "signatures": ["ICLR.cc/2026/Conference/Submission1869/Reviewer_tfiR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1869/Reviewer_tfiR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189319217, "cdate": 1762189319217, "tmdate": 1762915920073, "mdate": 1762915920073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}