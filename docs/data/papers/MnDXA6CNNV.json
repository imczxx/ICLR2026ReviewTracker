{"id": "MnDXA6CNNV", "number": 4898, "cdate": 1757789988205, "mdate": 1759898006539, "content": {"title": "Self-Questioning Language Models", "abstract": "Can large language models improve without external data -- by generating their own questions and answers? We hypothesize that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, we propose Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. We study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets.", "tldr": "We explore the possibility that LLMs could improve without external data, via asymmetric self-play: by generating their own questions as a proposer and answering them as a solver.", "keywords": ["reinforcement learning", "language models", "reasoning", "self-play"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66a000c2d68aad3855ccbf892b4431797c893569.pdf", "supplementary_material": "/attachment/72050d3f90ebbd17bb86854ef9bb29d8eb91812d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method called Self-Questioning Language Models (SQLM), which aims to enable large language models to improve themselves without any external supervision or labeled data.\n\nThe core idea is to introduce an asymmetric self-play framework in which the model takes on two roles:\n- Proposer that generates questions or problems, and\n- Solver that attempts to answer them\n\nBoth agents are optimized through reinforcement learning in an alternating fashion: the proposer is rewarded for generating questions that are neither too easy nor too difficult, while the solver receives rewards based on majority voting or unit-test correctness signals.\n\nExperiments conducted on arithmetic, algebra, and code-generation tasks demonstrate that the model can achieve notable performance gains even without any external data.\n\nOverall, the paper shows that large language models can potentially form a self-improving learning loop—generating, solving, and refining their own data—thus highlighting an interesting direction toward autonomous language model training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written with clear notations and equations.\n- General framework: The proposed approach is conceptually general and, in principle, extendable to various tasks such as reasoning, planning, and code generation."}, "weaknesses": {"value": "- Limited experimental scale: The experiments are conducted only on Qwen 2.5 3B instruct models, lacking results for smaller or larger, or different model families of models.\n- Potentially noisy reward signals: The majority-vote reward may reinforce model self-consistency bias, potentially amplifying incorrect consensus among sampled outputs. This could make the model learn inherent biases that are hard to correct later on.\n- It seems that the reward is constant for proposer in equation 2 as long as not all the answer or none of the answer is majority. Have authors studied different thresholds?"}, "questions": {"value": "- Have the authors considered evaluating the proposed framework on larger-scale models (e.g., 14B or 32B) and across a broader range of LLM families? Currently, experiments are limited to the Qwen and Llama series.\n- How stable is the training process between the proposer and solver during self-play? If one agent learns substantially faster than the other, does the system exhibit instability (e.g., oscillations or collapse)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DOkqNvXMu6", "forum": "MnDXA6CNNV", "replyto": "MnDXA6CNNV", "signatures": ["ICLR.cc/2026/Conference/Submission4898/Reviewer_2BYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4898/Reviewer_2BYK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769192216, "cdate": 1761769192216, "tmdate": 1762917744800, "mdate": 1762917744800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-Questioning Language Models, a training method in which two LLMs play a generator-discriminator game where a proposer model is rewarded for providing problems of reasonable difficulty, and the solver model is rewarded for solving the problems consistently/correctly. The models are trained using RL on these rewards. Since problems and solution verifiers are all generated/determined by the models being trained, this method works without external training data, and in principle, without verifiable problem solutions. Experiments on 3B scale LLMs show a statistically significant improvement in solve rates on simple arithmetic, linear equation solving, and LeetCode easy problems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper lie in its interesting perspective on discriminator-generator training for LLMs in order to \"bootstrap\" reasoning ability.\n\nS1. While the idea of training neural networks with discriminator-generator min-max style games is not novel, applying this to LLM reasoning as an RL objective is a promising approach. \n\nS2. The method is explained clearly and avoids indulging in unnecessary complexity.\n\nS3. The paper demonstrates statistically significant improvement in math and coding, providing a minimum level of significance.\n\nS4. The reward design is especially interesting as it does not rely on any external data or verification algorithms, providing some novelty when compared to mainstream RL techniques for LLM reasoning."}, "weaknesses": {"value": "While the core idea of asymmetric self-play for LLM training without external data is novel, the paper has a mismatch between its claimed contributions and experimental validation. The experiments are conducted on domains where the proposed approach is least needed, and lack sufficient analysis to demonstrate practical significance.\n\nW1. The paper claims to address settings with scarce training data and difficult verification. However, all experiments use arithmetic, algebra, and LeetCode easy problems, domains with abundant training data and straightforward verification. This undermines the core motivation: if verification is easy (arithmetic) or unit tests are available (coding), why not use standard GRPO with existing datasets?\n\nW2. No comparison to simply training on existing curated datasets (e.g., GSM8K for arithmetic, MATH for algebra, existing coding datasets). Without this baseline, it's unclear whether the RL overhead and complexity are justified for the improvements observed\n\nW3. All models tested are small (3-8B parameters). No evidence the approach scales to larger, more capable models. Only one model family primarily tested in main results (Qwen2.5), with limited Llama results in appendix. No experiments on domains where the method would be most useful, i.e, hard-to-verify problems (not in the computational sense, but in the epistemological sense).\n\nW4. Figure 2's PCA visualization lacks details: What is the embedding model? What features are being projected? How is diversity quantified? No systematic analysis of what problem distributions emerge over training. \n\nW5. No investigation of whether models learn general reasoning principles vs. task-specific pattern matching. The acknowledged risk of \"reinforcing systematic errors\" is not empirically analyzed or addressed. Failure mode analysis and limitations of the majority voting approach would also be nice to see."}, "questions": {"value": "Along with questions raised by the points raised in the 'Weaknesses' section, here are some additional questions.\n\nQ1. During experimentation, did you observe any degenerative cases, where training failed due to biases in the proposer? As mentioned in the paper, these methods are quite sensitive and can suffer from mode collapse. Could the authors provide quantitative data demonstrating the reliability of this approach across various hyperparamters and input prompts?\n\nQ2. A key concern that comes to mind is that, because of the majority voting reward used, there is no way in which the solver LLM can receive any signal on how to correctly solve problems that have common wrong answers. I.e, if it is confidently incorrect, it will never learn to rectify such mistakes. Could the authors comment on how/if this issue is addressed?\n\nQ3. What is the reliability of the majority voting objective? Could authors provide information on the accuracy of the majority voting reward, i.e, how often is the \"correct solution\" derived from the majority vote actually correct?\n\nQ4. I apologize If I missed this, but it was unclear to me how the solver's solutions are sampled such that they can be considered independent samples. Also, could the authors clarify how many solutions are sampled per problem, and its effect on training behavior?\n\nThank you for the interesting contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FIkgsvroTY", "forum": "MnDXA6CNNV", "replyto": "MnDXA6CNNV", "signatures": ["ICLR.cc/2026/Conference/Submission4898/Reviewer_xNos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4898/Reviewer_xNos"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846457907, "cdate": 1761846457907, "tmdate": 1762917744476, "mdate": 1762917744476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at improving the performance of an LLM for performing a task by prompting the LLM to generate problems and solutions for the task and using that synthetic data to finetune the model and improve it's performance. It describes basic approaches to verify the problems and solutions generated are good data to finetune on, and then shows the approach works on some different domain types."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I agree the approach described of leveraging LLMs to generate synthetic data to finetune on is very promising and successful and should be explored further. The paper describes an implementation of that approach and shows it works on some simple datasets."}, "weaknesses": {"value": "Leveraging LLMs for synthetic data generation to finetune on is a well known and successful approach, and the paper doesn't seem to me to propose anything novel to the technique.\n\nThe paper provides very limited experimental results on very simple datasets. \n\nI feel like in 2022 or 2023 this paper would have been considered novel, but in 2025 this approach has been pushed quite a bit farther by many other papers over the last couple years."}, "questions": {"value": "In comparison to the many other synthetic data generation papers out there, does this paper describe any significantly unique or novel approaches?\n\nDoes the approach implemented in this paper show SOTA results on datasets that other papers have tried?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lIo04JSx98", "forum": "MnDXA6CNNV", "replyto": "MnDXA6CNNV", "signatures": ["ICLR.cc/2026/Conference/Submission4898/Reviewer_YtVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4898/Reviewer_YtVC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149096363, "cdate": 1762149096363, "tmdate": 1762917744187, "mdate": 1762917744187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a self-questioning framework where a small “proposer” model generates problems and a “solver” model answers them and a subsequent  agreement via majority voting serves as an unsupervised reward signal. By intermittently updating the proposer, the system induces a moving curriculum intended to improve reasoning without curated labels. Experiments on lightweight tasks (e.g., three-digit arithmetic, a small linear-equation subset, and a narrow Codeforces slice) show gains over simple baselines, with ablations around proposer update frequency and sample counts. Overall the contributions of the paper need to be better positioned and argued and the supporting experimental design extended."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Timeliness and relevance of the topic. Label-free learning loops for supporting reasoning tasks.\n\n- Overall clear narrative."}, "weaknesses": {"value": "- Narrow experimental design. Lack of clear selection criteria for the target tasks. Limited baselines. Comparisons focus on simple alternatives (e.g., format-reward) rather than strong self-training/self-play methods under matched conditions.\n\n- Lack of a better positioning of the contributions of the work wrt to related work."}, "questions": {"value": "- Can you state more formally the key contributions (articulating against related work)?\n\n- How well do majority-vote and unit-test proxies correlate with expert human judgments of correctness and reasoning quality?\n\n- What mechanisms prevent the proposer from generating flawed, unsafe, or degenerate problems, and how often does this occur in practice?\n\n- Why is the proposer rewarded only for partial agreement, and how do alternative reward designs (e.g., calibrated difficulty, solver uncertainty) affect outcomes?\n\n- How sensitive is performance to proposer update frequency and sampling temperature. Do training curves show oscillations or mode collapse?\n\n- What happens when increasing model size, context length, or moving to harder domains?\n\n- Do gains persist on truly out-of-distribution tasks and adversarially perturbed prompts; how brittle is the solver to noisy or trick questions?\n\n- Why not include stronger self-training/self-play baselines under matched compute and data budgets. How were prompts, sampling, and budgets controlled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dhgqGtdUNk", "forum": "MnDXA6CNNV", "replyto": "MnDXA6CNNV", "signatures": ["ICLR.cc/2026/Conference/Submission4898/Reviewer_94Aq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4898/Reviewer_94Aq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4898/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157977378, "cdate": 1762157977378, "tmdate": 1762917743683, "mdate": 1762917743683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}