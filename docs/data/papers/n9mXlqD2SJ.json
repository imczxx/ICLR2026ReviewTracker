{"id": "n9mXlqD2SJ", "number": 15954, "cdate": 1758257579213, "mdate": 1759897270992, "content": {"title": "Difficulty–Diversity Collaborative Filtering for Data-Efficient LLM Fine-Tuning", "abstract": "The performance of fine-tuned language models is heavily influenced by the quality and quantity of their fine-tuning data. While scaling laws suggest that larger models benefit from more data during pretraining, the Less-is-More hypothesis highlights that downstream fine-tuning often requires only a small but high-quality dataset to effectively elicit a model’s pretrained knowledge. However, identifying such premium data, particularly in terms of difficulty and diversity, typically relies on human expertise, and existing methods offer limited guidance for automatic selection from large unannotated corpora. This work presents a novel quantitative framework that formalizes the interplay between question difficulty and diversity, and introduces *Difficulty–Diversity Collaborative Filtering* (DDCF): an automated approach that tailors data selection to the unique characteristics of each language model via collaborative filtering. By leveraging a small seed dataset to predict correctness across a large unannotated corpus, our method reduces the annotation cost by $100-200\\times$, while maintaining downstream performance comparable to full-corpus fine-tuning.", "tldr": "A data selection approach balancing difficulty and diversity for efficient fine-tuning of LLMs with minimal annotation effort.", "keywords": ["Large Language Models", "Reasoning", "Data Efficiency", "Supervised Fine-Tuning", "Collaborative Filtering"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03f450aca89c9fcb0c8e947de0f6bac3323515cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new strategy for selecting a set of high-quality samples that can be used to improve fine-tuning of LLMs on different tasks. The method is designed to work with unlabelled and limited data by first training a simple linear model for predicting the difficulty of the samples based on the behaviour of different LLMs on the questions. Afterwards, the difficulty is balanced with diversity of the samples to select the set of samples. Through extensive experiments the paper shows such selection leads to set of samples that can result in better accuracy after fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple, easy to understand and builds on the previous advances or other methods that were shown to work well. As such, it is not overly complicated and is easy to use which may allow its widespread use.\n\nThe method is evaluated through multiple experiments, ablation studies and multiple datasets and models.\n\nWriting is very clean and paper is easy to read and understand."}, "weaknesses": {"value": "**Missing related approaches for sample selection**\n\nAlthough the paper compares with multiple baselines, there are 2 approaches that are very similar to the DDCF method, which are completely missing from the paper. This includes the \"Datamodels\" selection [1, 2], which also trains a linear model to predict the benefit of each sample to the model (where the DDCF can be viewed as an extension of this approach that utilises multiple models at the same time when calculating the difficulty); and \"Dataset Cartography\" [3, 4] which determines the difficulty of the samples based on their training dynamics and shows that combination of easy and ambiguous samples provides benefit for training. In both cases, these methods are more related to determining the difficulty of the samples and so the comparison with them would strengthen the paper -- also including them in the related work section. I acknowledge that they are mostly used for selecting in-context examples, but they are not constrained by the typical approaches there (that select for specific test sample) and so could be used here, especially considering how similar they are to the proposed method.\n\nThere are also other methods for selecting in-context examples that balance multiple properties (including diversity) that could be included as part of the related work [5, 6, 7, 8].\n\n**Missing cost comparison across different approaches**\n\nBased on the experimental setup, for the DDCF method to work, you run ~19.5k inferences on using 23 different models. One of the main claims of the paper is that this method is less expensive than other approaches. As such, I would expect there to be a comparison in cost with different methods across different settings -- for example, if I am interested in selecting for samples for one specific model, running the extensive inferences from the DDCF method may not be the best solution. I would suggest adding a more in-depth discussion for the cost-performance trade-off. I acknowledge that there in Appendix C, there is an analysis how the DDCF difficulty evaluation performs when changing the number of models and questions that are requires, but I would expect to evaluate this not only based on the \"correctness\" of the difficulty predictor but based on the samples chosen in these data-contrained settings instead. \n\nSimilarly, based on the results, the random selection achieves strong accuracy already -- it is close to the proposed DDCF method and the best-performing approaches. I would suggest adding a discussion on this in regards to the trade-offs, as random selection is quite cheaper (and does not require labels). It might be an effect of the data, i.e., some datasets that have high-quality samples already do not benefit as much from more sophisticated selection, while on others (like AIME24) that may be more complicated or noisy, it might produce significantly better results. As such, I would suggest doing a more detailed analysis for these characteristics.\n\nA little side note: I would also suggest better references to the Appendix B and C that to a certain extent deal with this problem, but it is not as obvious from the text that it is included.\n\n\n**Results presented withouth consideration for LLM randomness**\n\nAll the results in the paper are presented from a single run, although the LLMs (and fine-tuning) are affected by different effects of randomness. This affects both the \"corectness predictor\", where repeated fine-tuning could lead to different results, but also the LLM fine-tuning on the selected samples. Although the expected differences might be small, the differences in results across different approaches are not that high so it would be interesting to so how the method is affected by randomness.\n\n\n**(minor) Writing changes**\n\nIn Table 2 (and also other tables) some numbers are bolded or underlines, but there is no explanation why. It is also done only for specific datasets. What does this mean? I would expect either bolding the best performing models (and underlining the runner-up) or removing it altogether.\n\nThere are cases on data where the DDCF method does not outperform other methods, although you claim it happends -- e.g., Qwen-2.5 on Minerva and STEM datasets is outperformed by Perplexity and/or s1.1-1K; or Qwen-3 on STEM for multiple approaches. I would suggest updating lines 356-359 as it is a bit misleading in terms of results (and not really necessary as I would not expect the method to be the best across all settings); same with Appendix D\n\n**References**\n1. Datamodels: Predicting Predictions from Training Data\n2. Data Curation Alone Can Stabilize In-context Learning\n3. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\n4. Cartography Active Learning\n5. Finding Support Examples for In-Context Learning\n6. Automatic Combination of Sample Selection Strategies for Few-Shot Learning\n7. EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning\n8. Sample Efficient Demonstration Selection for In-Context Learning"}, "questions": {"value": "See weaknesses for more details:\n\nHow does the proposed DDCF method compared with different baslines for selecting samples in terms of performance-cost trade-off (including random selection)?\n\nWhat is the impact of dataset quality on the sucess of sample selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJcl3fthCz", "forum": "n9mXlqD2SJ", "replyto": "n9mXlqD2SJ", "signatures": ["ICLR.cc/2026/Conference/Submission15954/Reviewer_b2jG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15954/Reviewer_b2jG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760606893066, "cdate": 1760606893066, "tmdate": 1762926164598, "mdate": 1762926164598, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new strategy for selecting a set of high-quality samples that can be used to improve fine-tuning of LLMs on different tasks. The method is designed to work with unlabelled and limited data by first training a simple linear model for predicting the difficulty of the samples based on the behaviour of different LLMs on the questions. Afterwards, the difficulty is balanced with diversity of the samples to select the set of samples. Through extensive experiments the paper shows such selection leads to set of samples that can result in better accuracy after fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is simple, easy to understand and builds on the previous advances or other methods that were shown to work well. As such, it is not overly complicated and is easy to use which may allow its widespread use.\n\nThe method is evaluated through multiple experiments, ablation studies and multiple datasets and models.\n\nWriting is very clean and paper is easy to read and understand."}, "weaknesses": {"value": "**Missing related approaches for sample selection**\n\nAlthough the paper compares with multiple baselines, there are 2 approaches that are very similar to the DDCF method, which are completely missing from the paper. This includes the \"Datamodels\" selection [1, 2], which also trains a linear model to predict the benefit of each sample to the model (where the DDCF can be viewed as an extension of this approach that utilises multiple models at the same time when calculating the difficulty); and \"Dataset Cartography\" [3, 4] which determines the difficulty of the samples based on their training dynamics and shows that combination of easy and ambiguous samples provides benefit for training. In both cases, these methods are more related to determining the difficulty of the samples and so the comparison with them would strengthen the paper -- also including them in the related work section. I acknowledge that they are mostly used for selecting in-context examples, but they are not constrained by the typical approaches there (that select for specific test sample) and so could be used here, especially considering how similar they are to the proposed method.\n\nThere are also other methods for selecting in-context examples that balance multiple properties (including diversity) that could be included as part of the related work [5, 6, 7, 8].\n\n**Missing cost comparison across different approaches**\n\nBased on the experimental setup, for the DDCF method to work, you run ~19.5k inferences on using 23 different models. One of the main claims of the paper is that this method is less expensive than other approaches. As such, I would expect there to be a comparison in cost with different methods across different settings -- for example, if I am interested in selecting for samples for one specific model, running the extensive inferences from the DDCF method may not be the best solution. I would suggest adding a more in-depth discussion for the cost-performance trade-off. I acknowledge that there in Appendix C, there is an analysis how the DDCF difficulty evaluation performs when changing the number of models and questions that are requires, but I would expect to evaluate this not only based on the \"correctness\" of the difficulty predictor but based on the samples chosen in these data-contrained settings instead. \n\nSimilarly, based on the results, the random selection achieves strong accuracy already -- it is close to the proposed DDCF method and the best-performing approaches. I would suggest adding a discussion on this in regards to the trade-offs, as random selection is quite cheaper (and does not require labels). It might be an effect of the data, i.e., some datasets that have high-quality samples already do not benefit as much from more sophisticated selection, while on others (like AIME24) that may be more complicated or noisy, it might produce significantly better results. As such, I would suggest doing a more detailed analysis for these characteristics.\n\nA little side note: I would also suggest better references to the Appendix B and C that to a certain extent deal with this problem, but it is not as obvious from the text that it is included.\n\n\n**Results presented withouth consideration for LLM randomness**\n\nAll the results in the paper are presented from a single run, although the LLMs (and fine-tuning) are affected by different effects of randomness. This affects both the \"corectness predictor\", where repeated fine-tuning could lead to different results, but also the LLM fine-tuning on the selected samples. Although the expected differences might be small, the differences in results across different approaches are not that high so it would be interesting to so how the method is affected by randomness.\n\n\n**(minor) Writing changes**\n\nIn Table 2 (and also other tables) some numbers are bolded or underlines, but there is no explanation why. It is also done only for specific datasets. What does this mean? I would expect either bolding the best performing models (and underlining the runner-up) or removing it altogether.\n\nThere are cases on data where the DDCF method does not outperform other methods, although you claim it happends -- e.g., Qwen-2.5 on Minerva and STEM datasets is outperformed by Perplexity and/or s1.1-1K; or Qwen-3 on STEM for multiple approaches. I would suggest updating lines 356-359 as it is a bit misleading in terms of results (and not really necessary as I would not expect the method to be the best across all settings); same with Appendix D\n\n**References**\n1. Datamodels: Predicting Predictions from Training Data\n2. Data Curation Alone Can Stabilize In-context Learning\n3. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\n4. Cartography Active Learning\n5. Finding Support Examples for In-Context Learning\n6. Automatic Combination of Sample Selection Strategies for Few-Shot Learning\n7. EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning\n8. Sample Efficient Demonstration Selection for In-Context Learning\n\n\n**UPDATE:** During the rebuttal process, the authors have addressed all of my concerns, and the extensive additional ablations and details (when included in the paper) will improve it considerably, strengthen the contribution and allow for interesting discussion. As a result, I am increasing my score."}, "questions": {"value": "See weaknesses for more details:\n\nHow does the proposed DDCF method compared with different baslines for selecting samples in terms of performance-cost trade-off (including random selection)?\n\nWhat is the impact of dataset quality on the sucess of sample selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJcl3fthCz", "forum": "n9mXlqD2SJ", "replyto": "n9mXlqD2SJ", "signatures": ["ICLR.cc/2026/Conference/Submission15954/Reviewer_b2jG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15954/Reviewer_b2jG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760606893066, "cdate": 1760606893066, "tmdate": 1763712757839, "mdate": 1763712757839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors addressed the inefficiency of data selection from large scale datasets for LLM supervised fine-tuning: high annotation costs, catastrophic forgetting, and redundant examples, by proposing Difficulty-Diversity Collaborative Filtering (DDCF). DDCF is a data selection framework that curates compact, high-quality subsets from large (often unannotated corpora), it leverages collaborative filtering to balance two criteria in data selection: question difficulty and semantic diversity, to enable SFT with minimal annotation overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow;\n\n2. DDCF can leverage unannotated data in SFT;\n\n3. DDCF can balance difficulty and diversity in data selection."}, "weaknesses": {"value": "1. Considering that $h_m$ in the model encoder does not change the dimension of the embeddings, why should the authors use $h_m$ in the formulation of the model encoder, please provide some evidence to demonstrate the necessity of $h_m$.\n\n2. For the experimental setting, [1] has demonstrated that most data selection algorithms can not outperform random selection in the experiments on datasets with more than 1 million data points regarding both effectiveness and efficiency. Data selection algorithms are expected to be applied in real-world industry scenarios, which often need to deal with datasets containing more than 1 million data points. Therefore, empirical results only on OpenR1-Math-220K can not provide convincing support to demonstrate the effectiveness of DDCF for real-world scenarios.\n\n3. The provided empirical results are not that promising, some results can not even outperform the base model, like GSM8k in Table. 2, \n\n4. Considering that the proposed DDCF needs several LLMs to conduct annotation before conducting data selection, and it will obviously add computational cost, the authors should provide computational anaylsis in the paper to demonstrate the efficiency of the proposed method.\n\n[1] Xia, T., Yu, B., Dang, K., Yang, A., Wu, Y., Tian, Y., ... & Lin, J. (2024). Rethinking data selection at scale: Random selection is almost all you need. arXiv preprint arXiv:2410.09335."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8PZlvGpTVK", "forum": "n9mXlqD2SJ", "replyto": "n9mXlqD2SJ", "signatures": ["ICLR.cc/2026/Conference/Submission15954/Reviewer_ksTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15954/Reviewer_ksTm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582514539, "cdate": 1761582514539, "tmdate": 1762926163807, "mdate": 1762926163807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Difficulty–Diversity Collaborative Filtering (DDCF) as an algorithm for model-specialized data filtering, which can be applied for LLM finetuning and active learning for CoT trace annotation. From the collaborative filtering, the learned parametrized model embedding and problem encoder can predict the problem's correctness; then the final data selection scores combine the difficulty and diversity of the selected subset, using the k-greedy strategy. \nEmpirically, the proposed method achieves a SOTA performance, while with 100x less annotation costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed DDCF algorithm is neat and effective, which provides an efficient and reliable method to quantify the model-specialized difficulty and diversity.\n\n2. The DDCF method demonstrates a significant improvements on the data efficiency on both instruction tuning on annotated and unannotated data.\n\n3. The paper is well-written and easy to follow, with clear illustration and extensive experimental results."}, "weaknesses": {"value": "1. The proposed collaborative filtering algorithm might have a cold start problem with insuffient data samples or biased data distributions. The performance can also greatly depend on the seed coreset.\n\n2. Lack of ablations on the number of models and questions used to train the CF model."}, "questions": {"value": "1. Have you encountered any cold start issues when training the collaborative filtering framework? How do you collect the seed dataset used to train the CF system?\n\n2. According to Figure 5, the data distribution selected by different models seems quite close. How much could the collaborative filtering from multiple models improves the performance comparing to train the question encoder with a classifier head upon a single model's correctness?\n\n3. Are there any performance differences when using models from different families/scales? Would the obtained encoder also be able to transferrable between different model scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ECyfRymeup", "forum": "n9mXlqD2SJ", "replyto": "n9mXlqD2SJ", "signatures": ["ICLR.cc/2026/Conference/Submission15954/Reviewer_KfMD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15954/Reviewer_KfMD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15954/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959410218, "cdate": 1761959410218, "tmdate": 1762926163401, "mdate": 1762926163401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}