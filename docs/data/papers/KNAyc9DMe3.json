{"id": "KNAyc9DMe3", "number": 2125, "cdate": 1756994146712, "mdate": 1763500779385, "content": {"title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "abstract": "The extension of diffusion models to language tasks has shown promising results, but their post-training methods remain largely unexplored. We highlight the importance of aligning a diffusion language model‚Äôs preference-inference trajectory with its post-training objective. To this end, we propose TraceRL, a trajectory-aware reinforcement learning framework for DLMs that incorporates information from inference trajectories into post-training and is applicable to both full-attention and block-attention diffusion models. We also introduce a diffusion-based value model that enhances training stability and naturally accommodates process rewards. We demonstrate TraceRL‚Äôs superiority in enhancing a model‚Äôs reasoning ability on complex math and coding tasks, as well as its applicability in scaling block diffusion models to larger block sizes. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than Qwen2.5-7B-Instruct, TraDo-4B-Instruct consistently outperforms it on complex math reasoning tasks. TraDo-8B-Instruct achieves 4.5% higher accuracy on MATH500 than Qwen2.5-7B-Instruct and 6.6% higher accuracy on LiveCodeBench-V2 than Llama3.1-8B-Instruct. Through curriculum learning, we also develop the first 8B-scale long-CoT diffusion language model.", "tldr": "", "keywords": ["Diffusion Language Models", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bad0cbeca92b1e46ef3105a78e4c48d89aa73664.pdf", "supplementary_material": "/attachment/c14d05e495fbf0df8f77d98f722af0a9d102d2cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies reinforcement learning for diffusion language models. During the learning of diffusion models, one needs to sample noised intermediate samples to learn reconstructions. The authors highlight the importance of using noisy samples from diffusion models' sampling trajectories in training. They also introduce using value model to enhance training stability. The effects are validated on math and coding benchmarks, which surpasses strong autoregressive instruct models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Enabling the use of reinforcement learning is an crucial steps to advance diffusion language models. This paper provides timely and systematic investigation into this topics.\n2. This paper validates the effect of several intuitive techniques in RL for diffusion language models, which is comprehensive and helpful for following research, including shrinkage steps, aligning training and sampling trajectories, introducing value models, and sliced training.\n3. The results is superior, matching strong autoregressive language models like Qwen2.5-7B-Instruct on representative math and coding benchmark. As far as I know, this is the first RL diffusion language models to achieve such results on these convincing benchmarks. \n4. To the best of my knowledge, this paper also implement the first diffusion language models capable of long-cot thinking."}, "weaknesses": {"value": "1. The comparison to autoregressive models is not strictly fair. Strong results are only demonstrated through SDAR, which is a block diffusion language models with extremely small block sizes. This does not convince me on the potential of improving a diffusion language models. And the long-cot models are not compared to long-cot autoregressive models.\n2. The results do not support the claims about benefits of introducing value models. In figure 3, the results with value are almost identical to the ones without value model."}, "questions": {"value": "1. Could you show more results on full attention diffusion language models without blocks (i.e., LLaDA, Dream instead of SDAR) ? How about the effect of RL on diffusion language models given different block sizes?\n\n2. How long is the responses of TarDo-Instruct compared to Qwen2.5-Instruct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nmGQX8BRKa", "forum": "KNAyc9DMe3", "replyto": "KNAyc9DMe3", "signatures": ["ICLR.cc/2026/Conference/Submission2125/Reviewer_bxFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2125/Reviewer_bxFJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655499190, "cdate": 1761655499190, "tmdate": 1762916042698, "mdate": 1762916042698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TraceRL, a trajectory-aware RL framework for diffusion language models (DLMs). It argues that standard post-training objectives for DLMs (fully random masking; semi-autoregressive SFT) are misaligned with the left-to-right inference trajectories used at decoding time, and proposes to (i) aggregate decoding steps into short ‚Äútrace‚Äù segments via a shrinkage parameter , (ii) optimize a PPO-style policy objective over these segments, and (iii) pair the policy with a diffusion-style value model that provides variance-reduced advantages and supports process rewards. The approach improves math/coding benchmarks and can even adapt models to larger block sizes"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper carefully motivates why training with random masking or generic semi-AR losses can be misaligned with the actual iterative unmasking/decoding procedure (static vs dynamic sampling), and shows a simple analysis/experiment to support this. \n\n2. The algorithm aggregates ùë† consecutive decoding steps into segments, computes advantages/returns at the segment (and token) level, and updates a PPO-style objective. This aligns learning signals with how the model is actually used at inference. The paper gives explicit value loss (clipped regression) and describes sliced training under block diffusion to keep training efficient. \n\n3. Extending variance-reduction baselines to DLMs is novel here, with the verifiable reward assigned at the final sampling step and zeros elsewhere; the same machinery accommodates mid-trajectory ‚Äúprocess‚Äù rewards. Empirically this stabilizes training and accelerates optimization. \n\n4. On math/coding, TraceRL improves over strong DLM baselines; the TraDo models achieve the best results among compared DLMs, with detailed tables."}, "weaknesses": {"value": "1. For the value model, the entire verifiable reward is assigned to tokens in the last sampling step, with zeros elsewhere. This is simple but may bias learning toward late segments and under-credit earlier reasoning„ÄÇ\n\n2. Process rewards come from Qwen3-4B judging 200-token segments with a fallback to the verifiable reward. The paper should quantify judge agreement rates, false-positive penalties (reward hacking) and robustness to prompt variations. \n\n3. While the paper compares to several full- and block-attention DLMs, it would help to include stronger autoregressive SOTA (e.g., top open 7‚Äì8B instruct models) under matched decoding/training budgets. \n\n4. The paper provides explicit forms for token-wise advantages and a clipped value loss, but there‚Äôs no convergence or bias/variance analysis for the ‚Äútrace shrinkage‚Äù estimator or the ‚Äúsliced‚Äù block training."}, "questions": {"value": "1. The authors are expected to provide an ablation on reward assignment (last-step vs. proportional attribution) and an analysis of how attribution interacts with acceleration ratios/response lengths. \n\n2. It would be better if the authors could give judge prompts/configs and inter-rater stats; run sanity checks (shuffled/adversarial segments) to quantify false positives.\n\n3. The authors are expected to include strong AR instruction-tuned models under matched decoding/compute"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "05YGlSaprT", "forum": "KNAyc9DMe3", "replyto": "KNAyc9DMe3", "signatures": ["ICLR.cc/2026/Conference/Submission2125/Reviewer_hx5e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2125/Reviewer_hx5e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664464971, "cdate": 1761664464971, "tmdate": 1762916041612, "mdate": 1762916041612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies RL methods to reinforce the whole multi-step generation trajectories in DLLM -- this can be applied to both full-attention version and block-attention version. To accelerate training, the actual denoising steps are grouped into fewer actions to reinforce. Moreover, the paper introduce a step-wise value model to estimate an intermediate result to reduce the variance during. The experiments are conducted on TraDo, and achieve higher performance on math and code benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a method to shrink the trajectory length to reinforce, accelerating training\n2. Introducing value model in DLM RL algorithm is novel, and it is reasonable to evaluate the quality of an intermediate steps.\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Why do we only shrink the steps after sampling trajectory? Actually, we can also just sample shorter trajectories by allowing more tokens to be predicted in each step.\n2. The paper does not discuss the effect of shrinking $s$ consecutive steps, both on the final performance and the training efficiency. [1] reinforces the whole trajectory without shrinking. It worth a comparison.\n3. The pretrained model in L178 is actually Qwen as in Appendix D.1, right? It is quite confusing and misleading, since usually such a phrase means the model before post training. Why not sampled from the DLM itself, just like the coldstart phase in Deepseek-R1?\n\n\n[1] Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models"}, "questions": {"value": "1. How to calculate advantage in the \"w/o value model\" setting?\n2. If the preference inference traces are all left-to-right ones from Qwen, what is the final generation order preference after our RL training?\n3. Since intermediate steps may contain mask tokens, how can Qwen3-4B be used as a process reward model to evaluate such intermediate outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bPvcL20Ofd", "forum": "KNAyc9DMe3", "replyto": "KNAyc9DMe3", "signatures": ["ICLR.cc/2026/Conference/Submission2125/Reviewer_YmJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2125/Reviewer_YmJb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911868248, "cdate": 1761911868248, "tmdate": 1762916039643, "mdate": 1762916039643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TraceRL, a reinforcement learning framework designed to improve the performance of Diffusion Language Models (DLMs) by aligning their post-training objective with their inference trajectory. The authors argue that existing post-training methods, which often rely on random masking, create a mismatch with the more structured, confidence-based sampling process used during inference. TraceRL is a trajectory-aware framework that uses information from the entire sampling process to optimize the model. The paper also proposes a diffusion-based value model to stabilize training and incorporate process rewards. Using TraceRL, the authors develop a series of models called TraDo, which they show achieve state-of-the-art results on several complex reasoning benchmarks for math and coding, reportedly outperforming strong autoregressive models like Qwen2.5-7B and Llama3.1-8B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The mismatch between the training objective and the inference process in DLM RL is a well-motivated issue. Aligning the RL training process with the model's actual inference trajectory is logical and well-supported.\n- The authors have conducted comprehensive ablation studies that analyze the contribution of different components of their method, such as the value model, the choice of RL optimization strategy, and the shrinkage parameter."}, "weaknesses": {"value": "- Marginal Performance Improvement and Extra Value Model Compared to Autoregressive Models. TraceRL improves previous DLM RL methods, but the performance gain through TraceRL is still marginal (4%\\~5% on very simple benchmarks like MATH500). This largely lags behind common autoregressive LM RL recipes like GRPO/DAPO, which could easily bring 10%\\~20% gains for a 7B-scale model on MATH500 without any process reward. Post-training is critical to LLMs, while the significance of DLM RL remains questionable given the marginal improvement.\n- Incremental Novelty. The paper positions TraceRL as a novel framework that, unlike prior work, uses information \"embedded in the sampling trajectory.\" However, most policy gradient RL methods are inherently \"trajectory-aware.\", especially the diffusion RL methods like FlowGRPO/DanceGRPO. The objective function in Equation 3 appears to be a standard PPO-style implementation applied to the steps of a diffusion process, which has already been done in FlowGRPO/DanceGRPO. The novelty seems to lie in the specific application to DLMs rather than a fundamental innovation in RL methodology.\n- Overclaiming in the Title and Narrative. The title uses the word \"Revolutionizing,\" which sets an exceptionally high bar that the contributions, while solid, do not appear to meet. The work does not fundamentally change the paradigm of diffusion RL.\n- Confounding Effect of Long-CoT Fine-Tuning. The TraDo-8B-Thinking model achieves the most impressive results (e.g., 87.4% on MATH500). However, this model is the result of first applying TraceRL and then performing an additional semi-autoregressive fine-tuning step on the OpenThoughts dataset. A significant portion of the performance lift is likely attributable to the long-CoT data, and it is unclear how much TraceRL contributes. Even without TraceRL, the high-quality data of OpenThoughts itself could bring huge performance improvement. Notably, the OpenThinker-v3 7B autoregressive model tuned with the OpenThoughts dataset achieves an AIME2024 score of 69, significantly higher than TraDo-8B-Thinking's 35.5. This further undermines the necessity of using DLM."}, "questions": {"value": "- Could the authors reevaluate the baselines by applying the same in-domain RL fine-tuning to the autoregressive models? What is the advantage of DLM RL compared to autoregressive RL, given that DLM RL is more demanding in implementation and expensive in training?\n- What is the inference speed of TraDo-8B-Thinking? Does it use static or dynamic sampling? Does it have any advantages in speed or performance, compared to OpenThinker models, which are also tuned with the OpenThoughts dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JG1fBzssQF", "forum": "KNAyc9DMe3", "replyto": "KNAyc9DMe3", "signatures": ["ICLR.cc/2026/Conference/Submission2125/Reviewer_Nrqh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2125/Reviewer_Nrqh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2125/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970084521, "cdate": 1761970084521, "tmdate": 1762916039404, "mdate": 1762916039404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all the reviewers for their thorough reviews and valuable feedback. We are glad to hear that the proposed step-wise value model and acceleration framework are considered novel (reviewers YmJb, hx5e), that the trajectory alignment and evaluation on intermediate steps are well-motivated (reviewers YmJb, hx5e, bxFJ), that the results are strong and superior (reviewers hx5e, bxFJ), that the paper is well written and easy to follow (all reviewers), and that the experiments and techniques are comprehensive (reviewers Nrqh, bxFJ, hx5e). **We have revised the manuscript according to the reviewers‚Äô main suggestions, with all changes marked in blue.**\n\nWe have addressed all reviewers' concerns through experiments and clarified previous misunderstandings and sources of confusion in this rebuttal. We summarize our main responses to each reviewer below and leave the detailed explanations to the individual responses that follow.\n\n1. We have conducted three ablation studies on the robustness of process reward modeling and clarified the reward assignment approach (reviewer hx5e).\n2. We have added quantitative analyses to demonstrate the significance of the diffusion value model (avoiding ambiguity in the curves) and conducted five additional experiments to make the RL results more convincing (reviewer bxFJ).\n3. We have clarified the misunderstandings regarding the shrinking approach and the preference-trajectory collection process, and added a comparative discussion with proposed paper (reviewer YmJb).\n4. We have conducted DAPO/GRPO on similar scale LLM using the same training set. Our DLM requires only half the number of rollouts to achieve comparable or better results, demonstrating the significance of our method (reviewer Nrqh).\n\nWe reply to each reviewer's questions in detail below their reviews. Please kindly check out them. Thank you and please feel free to ask any further questions."}}, "id": "YwOJLGqidP", "forum": "KNAyc9DMe3", "replyto": "KNAyc9DMe3", "signatures": ["ICLR.cc/2026/Conference/Submission2125/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2125/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission2125/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763498282689, "cdate": 1763498282689, "tmdate": 1763499099931, "mdate": 1763499099931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}