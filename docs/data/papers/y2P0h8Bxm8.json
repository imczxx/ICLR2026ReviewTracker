{"id": "y2P0h8Bxm8", "number": 10220, "cdate": 1758164328190, "mdate": 1759897665553, "content": {"title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning", "abstract": "Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose \\textbf{Critique-Post-Edit}, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a \\textbf{Personalized Generative Reward Model (GRM)} that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a \\textbf{Critique-Post-Edit} mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.", "tldr": "We propose a method to enable faithful and controllable personalization of LLMs.", "keywords": ["Personalization", "Reinforcement Learning", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/baed376a3a3529864c3ff576511cca3b7986cd34.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Critique-Post-Edit RL, a method for improving faithful and controllable personalization of LLMs by integrating a Generative Reward Model and an edit-based feedback loop as data source. First, the authors show that replacing BT reward model by a GRM helps against reward hacking and length bias, achieving overall better performance. Second, the authors take advantage of the GRM textual critiques to have the model revises its outputs using these critiques, creating  an additional source of data for training."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The authors present comprehensive experiments across multiple datasets and scales, showing consistent gains and strong performance even compared to GPT-4.1.\n\n- The length-controlled evaluation and human validation strengthen the credibility of the reported results.\n\n- Ablation and sampling-strategy studies are detailed and help disentangle contributions between the GRM and data collection."}, "weaknesses": {"value": "- The reward aggregation uses fixed weights, but no sensitivity or robustness analysis is provided. Since these weights directly control the reward, understanding how performance varies with them is essential.\n\n- The paper insufficiently situates itself within existing work:\n\n  - From the methodological side, training models on post-critique edits has been extensively explored before (See [1] and many follow ups), yet this lineage is not discussed.\n\n  - From the application side, no comparison is made to prior personalization methods such as [2] or the line of literature about modeling users as a mixture of attributes [3,4,5]. Positioning the proposed method within these would clarify whether it contributes a new algorithmic insight or merely a variant of previous work (given that each one of the components in the pipeline has been proposed before).\n\n[1] Scheurer, J., et al. Training Language Models with Language Feedback at Scale. arXiv:2303.16755 (2023).\n\n[2] Zhao, W., et al. Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment. arXiv:2505.15456 (2025).\n\n[3] Jang, J., et al. Personalized Soups: Personalized LLM Alignment via Post-hoc Parameter Merging. arXiv:2310.11564 (2023).\n\n[4] Poddar, S., et al. Personalizing RLHF with Variational Preference Learning. NeurIPS (2024).\n\n[5] Shenfeld, I., et al. Language Model Personalization via Reward Factorization (PReF). arXiv:2503.06358 (2025)."}, "questions": {"value": "- Can you please elaborate on how AlpacaEval was transformed into a personalization setting? the description in section 5.1 is too short to be of use.\n\n- It seems to me like your usage of GRM is just as a distillation of GPT-4o for the specific task of scoring and critiquing responses. Why not just use LLM-as-a-Judge? What is the advantages for the distillation to a local model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EcBNNbi9vU", "forum": "y2P0h8Bxm8", "replyto": "y2P0h8Bxm8", "signatures": ["ICLR.cc/2026/Conference/Submission10220/Reviewer_vcdg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10220/Reviewer_vcdg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760982104972, "cdate": 1760982104972, "tmdate": 1762921578833, "mdate": 1762921578833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a RL framework called Critique-Post-Edit for personalizing LLMs. The approach motivates from limitations in training paradigms - SFT, offline DPO, PPO, which often plateau or suffer from reward hacking. The key components in the proposed framework include: (1) a Personalized Generative Reward Model (GRM) that outputs multi-dimensional scores (helpfulness, personalization, naturalness) along with textual critiques to provide robust feedback, and (2) a post-edit mechanism where the policy model refines its initial outputs based on these critiques, creating a mix of on-policy and off-policy samples for more targeted training. Experiments on benchmarks demonstrate significant gains, with a Qwen2.5-14B model achieving a 76.8% length-controlled win rate against GPT-4.1, outperforming PPO baselines by an average of 11%. The framework emphasizes faithfulness, controllability, and resistance to hacking, validated through ablations and human correlation studies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The integration of a GRM with textual critiques is persuasive, providing nuanced, multi-faceted feedback that mitigates common RL pitfalls like verbosity or superficial personalization. This builds effectively on the prior concept of generative verifiers but tailors it to personalization, showing clear empirical benefits in length-controlled evaluations.\n- The use of length-debiased metrics (Dubois et al., 2024) and multiple benchmarks ensures fair comparisons, avoiding common biases in LLM-as-judge setups. Achieving performance surpassing GPT-4.1 with open-source Qwen models is impressive and scalable, with ablations confirming the value of each component (e.g., GRM + post-edit yields 64% win rate vs. 52% for PPO).\n- By extending ideas from HelpSteer3 to personalization, it advances controllable alignment, with potential applications beyond personalization (e.g., tool-integrated RL)."}, "weaknesses": {"value": "- Overall, the missing details in many parts of the paper make it hard to follow the context. For example, Section 3 that builds core motivation of the proposed approach, lacks any details about the specific experimental setup; it abruptly starts from \"... we train with 18k samples\" without any provenance or what the goal of this training is in the first place. In Section 4.1, despite the section being details about GRM, it only mentions the construction of training data but does not provide any detail on how the GRM has actually been trained (was it just SFT'd on the gpt-4o-mini data? or did it take hybrid generative - BT formulation based RM training?)\n- While the framework is well-executed, it appears incremental over prior works. For instance, generative reward models draw heavily from existing literature (e.g.  critic-out-loud reward models - 2408.11791 & generative verifiers - 2408.15240) and the critique-post-edit paradigm closely mirrors various RLAIF methods. Personalization-specific adaptations like LoRe (low-rank reward modeling) or reinforced prompt personalization (2407.17115v2) already explore similar user-adaptive RL, potentially overshadowing the claimed novelty. The paper could better differentiate from these by discussing how it uniquely handles \"meta understanding\" of personas.\n- While the training data construction pipeline is sensible, it lacks novel component besides distilling the trajectories from gpt-4o-mini for generative training (especially given its similarity to RM training in PersonaFeedback).\n- Broadly speaking, while the goal of the work is to make LLMs more personalizable, I feel that the proposed approach does not specifically handle the challenges / pecularity of personalization; rather, it could be adopted to any general RLHF objectives, with its key novelty being the post-edit mechanism on the generated rollouts during RL. Perhaps the authors want to recalibrate their approach towards adaptive RL framework with the application being system-prompt personalization."}, "questions": {"value": "- Given the reliance on HelpSteer3 and generative verifiers, what specific modifications make this approach uniquely suited for personalization over general alignment tasks?\n- In ablations, why does random sampling outperform reward-ranked strategies? Is this due to negative sample value, or an artifact of the GRM's scoring? Could you provide more analysis on sample diversity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wUPgnta4sN", "forum": "y2P0h8Bxm8", "replyto": "y2P0h8Bxm8", "signatures": ["ICLR.cc/2026/Conference/Submission10220/Reviewer_rG5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10220/Reviewer_rG5f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415103872, "cdate": 1761415103872, "tmdate": 1762921578325, "mdate": 1762921578325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework called Critique-Post-Edit that uses a generative reward model to edit responses, score both the original and edited responses, and then use the responses/scores/rationales to perform RL to align LLMs for personalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The problem of dealing with reward hacking in personalization is relevant and the idea presented is practically useful and scalable."}, "weaknesses": {"value": "1. While the idea of using generative reward models instead of regular reward models to deal with reward hacking seems interesting and of practical relevance, the novelty is a bit weak. I recommend improving the paper with more significant contributions, one idea is coming up with a theoretical guarantee, another is to stress test and understand when the method works well and when it doesn't.\n\n2. The benchmarks with the naive DPO and RLHF are too simplistic. I recommend adding more benchmarks, for example what would happen if we just used an LLM critique in place of the generative reward model?"}, "questions": {"value": "1. The writing of the paper needs more work. It is not clear to me what personalization data is available? Is it observed actions of the user in terms of binary or scalar preferences over time? Or is it the user profile like \"X works at A doing B job. Hobbies include C and D\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F4KW53pdTV", "forum": "y2P0h8Bxm8", "replyto": "y2P0h8Bxm8", "signatures": ["ICLR.cc/2026/Conference/Submission10220/Reviewer_w4Ru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10220/Reviewer_w4Ru"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868412552, "cdate": 1761868412552, "tmdate": 1762921577911, "mdate": 1762921577911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Critique-Post-Edit RL, a personalization framework where a personalized generative reward model (GRM) supplies both multi-attribute scores and a textual critique. The policy first drafts a response, receives the GRM’s critique, then self-edits and learns from both the original and edited outputs via a hybrid on-/off-policy objective. The outperforms PPO baselines on PersonaFeedback, AlpacaEval, and PersonaMem; a Qwen2.5-14B variant is reported to surpass GPT-4.1 on average."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using a GRM that outputs both multi-dimensional scores and textual rationales, serving as a verifier that explains what to improve and why, which is more informative than single-scalar BT reward is novel.\n2. The post-edit stage yields a diverse, targeted learning signal is a novel idea to mitigate reward hacking in the standard PPO.\n3. Clear, easy-to-follow visual illustration of the framework and training loop.\n4. Addresses a limitation “one-size-fits-all” personas and shallow personalization signals (SFT/DPO) that struggle to capture meta understanding."}, "weaknesses": {"value": "1. Performance hinges on (1) the model’s ability to follow critiques, (2) the quality of GRM critiques, and (3) the consistency/calibration of generated scores; these are potential fragility points.\n2. Although the GRM outputs multiple dimensions, optimization ultimately reduces them to a single scalar signal, potentially discarding nuance and only three dimensions are considered.\n3. The GRM is trained with GPT-4o; behaviors could overfit to that judge, making the system hackable against its own verifier and less reliable with other evaluators.\n4. I am not intuitively understanding how the method outperfroms GPT-4.1, if the model is trained using GRM which was trained on GPT-4o.\n5. The evaluation set is small, and personalization appears largely synthetic/templated.\n6. Related work coverage. The related work section is thin given adjacent lines of work (self-critique/edit without RL, verifier-only RLAIF, etc/)\n7. Minor writing issues (e.g., line 35: missing space after a period; line 292.)"}, "questions": {"value": "My main question is that if the code is available and the results are reproducible.\n\nSee the weaknesses for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3odYSrQbrr", "forum": "y2P0h8Bxm8", "replyto": "y2P0h8Bxm8", "signatures": ["ICLR.cc/2026/Conference/Submission10220/Reviewer_2yAw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10220/Reviewer_2yAw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079544941, "cdate": 1762079544941, "tmdate": 1762921577537, "mdate": 1762921577537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}