{"id": "LNilmuJmF0", "number": 22556, "cdate": 1758332814168, "mdate": 1759896859799, "content": {"title": "HEART-ViT: HESSIAN-GUIDED EFFICIENT DYNAMIC ATTENTION AND TOKEN PRUNING IN VISION TRANSFORMERS", "abstract": "Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic at- tention cost and redundant computations severely hinder deployment on latency- and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART- ViT, a Hessian-guided efficient dynamic attention and token pruning for vision transformers, which to the best of our knowledge, is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian–vector products, enabling principled pruning decisions under explicit loss budgets. This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4% FLOPs reduction, 36% lower latency, and 46% higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning (e.g., +4.7% recovery at 40% token pruning). Beyond theoretical benchmarks,  we deploy HEART-ViT on different edge devices, like- AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficienc. HEART-VIT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.", "tldr": "", "keywords": ["Vision Transformers (ViTs)", "Dynamic pruning", "Hessian-based sensitivity", "Token and head pruning", "Edge-efficient inference"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b36ca241347e0c7a425bba3e0b6d1f56c1f5b04d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a hessian-guided ViT pruning technique, which is applied on both tokens and attention heads. The biggest strength of the paper is its mathematical rigor, as the motivation, formulation and solution of the problem all followed a clear math process, providing a strong insight into the method's working mechanism. However, there are fundamental flaws in the paper's experiments, for lack of comprehensive comparison, and poor presentation, which renders the paper below the standard for an ICLR paper."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Very clear math behind the proposed method, all the way through the motivation, the formulation and the solution \n- Unified token and attention head pruning provides a novel angle for ViT pruning \n- Experimental results are competitive against vanilla baselines \n- The method supports both finetuned and non-finetuned modes"}, "weaknesses": {"value": "- Overall, the quality of presentation is not great – this includes inconsistent spacing (eg. Ln226,231), lack of proper explanation of symbols (ln178), poor quality of figures (e.g. fig 1,2,3). The authors are encouraged to work on the improvement of presentation vigorously to match top-tier conference standards. \n- The biggest weakness of the paper is the experiment section. It generally lacks the comprehensive evaluations required by a solid study for a new efficient ViT method. The range of SotA baseline methods is lacking, so is the variety in model size is lacking, and the diversity in the downstream tasks (OD, seg, etc), and the coverage for datasets is limited (only ImageNet)"}, "questions": {"value": "- I recommend removing Table 1 for something that provides more information that is useful to the audience from this area. Table 1 is too verbose and contains many items that are very technique-specific.\n- Figure 1 and Figure 2 are good for teasers but poor choices for presenting as main results - table with plain numbers are easier to read for the main results. Comparisons are clearer too with tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "K8HuIBoW1m", "forum": "LNilmuJmF0", "replyto": "LNilmuJmF0", "signatures": ["ICLR.cc/2026/Conference/Submission22556/Reviewer_hz49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22556/Reviewer_hz49"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736037056, "cdate": 1761736037056, "tmdate": 1762942275413, "mdate": 1762942275413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HEART-ViT, a joint token and head pruning method for efficient ViTs. HEART-ViT employs a second-order Taylor expansion on a converged ViT model to approximate the loss change induced by pruning. The resulting scoring criterion is simplified to rely solely on the curvature term, enabling efficient computation. The proposed method is evaluated on ViT-B and DeiT-B backbones, demonstrating good performance and efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The mathematical motivation is solid. The proposed scoring strategy is sound.\n\n2. The experiments on backbone model are comprehensive.\n\n3. This work includes evaluation on edge devices, further demonstrating its practical effectiveness."}, "weaknesses": {"value": "1. __Insufficient empirical results:__ This paper lacks a bunch of experiments:\n\n   * Comparisons to prior token and/or structural pruning methods;\n   * Performance on different ViT architectures (e.g., Swin Transformer) and sizes (e.g., ViT/DeiT-Small)\n   * Performance on downstream tasks after pruning\n\n   Although the analytical studies on latency, pruning effectiveness, and layerwise similarity are informative, the absence of these fundamental experiments significantly undermines the empirical strength and overall significance of the work.\n\n2. __Massive finetuning demands:__ HEART-ViT requires 100-epoch finetuning to be effective, which is an essential drawback compared to state-of-the-art token pruning/merging methods that usually require a few finetune epochs or none at all. This substantially reduces the practical efficiency and applicability of HEART-ViT.\n\n3. __Poor presentation:__ The presentation quality is low:\n\n   * Citation format does not align with the ICLR format\n   * Overlapping between Figure 6 and Figure 5's caption\n   * Overlapping between some formulae and surrounding texts\n   * Some formulae have equation numbers while some others do not\n   * Figure 1 is never referred to\n   * Inconsistent reference formats (btw, ToMe should be published on ICLR but labelled CVPR)"}, "questions": {"value": "Please refer to the weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UB72mR4lKr", "forum": "LNilmuJmF0", "replyto": "LNilmuJmF0", "signatures": ["ICLR.cc/2026/Conference/Submission22556/Reviewer_grLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22556/Reviewer_grLo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743253364, "cdate": 1761743253364, "tmdate": 1762942273643, "mdate": 1762942273643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified token and head pruning algorithm for ViTs called HEART-ViT. HEART-ViT measures the loss perturbation caused by removing certain tokens or attention heads to identify redundant components. It also proposes a simplified formulation based on the second-order Taylor expansion of the converged model for a efficient measurement implementation. Experiments on different ViT backbones demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The unified form of token and head pruning for ViT with mathematical proof is novel and interesting.\n\n* The visualizations effectively illustrate the pruning behavior and provide valuable interpretability.\n\n* Broad studies on model performance with different pruning ratios."}, "weaknesses": {"value": "* No comparisons are provided with state-of-the-art efficient ViT methods, particularly recent token or structural pruning and merging approaches [1,2,3,4].\n\n* The experiments are limited to ViT-B and DeiT-B, which share nearly identical architectures, leaving the method’s generalization to other ViT variants unverified.\n\n* The comparison with the backbone in Appendix Tables 3&4 is unfair, as the baseline is not finetuned for the same 100 epochs used by the proposed method.\n\n* The contribution appears marginal compared to AdaViT [5], which also unifies token, head, and block pruning while employing a simpler and more practical estimation strategy.\n\n[1] Bolya, Daniel, et al. \"Token merging: Your vit but faster.\" ICLR, 2023.\n\n[2] Yang, Huanrui, et al. \"Global vision transformer pruning with hessian-aware saliency.\" CVPR, 2023.\n\n[3] Kim, Minchul, et al. \"Token fusion: Bridging the gap between token pruning and token merging.\" WACV, 2024.\n\n[4] Wang, Hongjie, Bhishma Dedhia, and Niraj K. Jha. \"Zero-TPrune: Zero-shot token pruning through leveraging of the attention graph in pre-trained transformers.\" CVPR, 2024.\n\n[5] Meng, Lingchen, et al. \"Adavit: Adaptive vision transformers for efficient image recognition.\" CVPR, 2022."}, "questions": {"value": "* How do you explain the performance different between symmertic and asymmertic pruning? Asymmetric pruning seems to be more flexible but introduces worse trade-off bwteen efficiency and performance in most cases. Does ViT architecture prefer a certain type of pruning?\n\n* How do you explain the significant performance degradation after pruning pre-finetuning? Notably, many state-of-the-art methods are finetuning-free.\n\n* Why do you specifically choose second-order Taylor expansion rather than more accurate k-th Taylor expansion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "niPLzC0XZU", "forum": "LNilmuJmF0", "replyto": "LNilmuJmF0", "signatures": ["ICLR.cc/2026/Conference/Submission22556/Reviewer_oymU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22556/Reviewer_oymU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790870583, "cdate": 1761790870583, "tmdate": 1762942272760, "mdate": 1762942272760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a Hessian-guided pruning framework for ViTs, which jointly pruning both tokens and attention heads based on second-order sensitivity. The second order information is gathered using Hessian–vector products (HVPs) method, for each token and head. \nEmpirical evaluations on ImageNet on ViT and DeiT show up to 49% FLOPs reduction, 36–43% latency improvements. Hardware benchmarking also shows realistic efficiency improvement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The second-order based pruning criterion is well motivated.\nThe authors also made endeavors to test on real world edge device to evaluate the hardware efficiency."}, "weaknesses": {"value": "Although authors mentioned the complexity analysis of the HVP calculation, the level of empirical overhead is still concerning as it requires 2 backward passes.\nAlso the paper lacks discussion on their differences and advances against existing hessian-based ViT pruning papers, e.g. LPViT [1], NViT [2]."}, "questions": {"value": "1. There are too little comparisons with SOTA methods, especially missing more recent papers that also adopts hessian in pruning, e.g. LPViT [1] (ECCV'24), NViT [2] (CVPR'23).\n2. Although authors provided hardware benchmarking results, i wonder how is it compared with SOTA methods.\n\nMinor problems:\nFigure 1 there are some visual clarity issue.\n\n\nReference:\n[1] Xu, Kaixin, et al. \"Lpvit: Low-power semi-structured pruning for vision transformers.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n[2] Yang, Huanrui, et al. \"Global vision transformer pruning with hessian-aware saliency.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d3WNWJJmSh", "forum": "LNilmuJmF0", "replyto": "LNilmuJmF0", "signatures": ["ICLR.cc/2026/Conference/Submission22556/Reviewer_8ymd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22556/Reviewer_8ymd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22556/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762672474191, "cdate": 1762672474191, "tmdate": 1762942272580, "mdate": 1762942272580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}