{"id": "ogMxCjdCCq", "number": 22727, "cdate": 1758334823676, "mdate": 1759896850194, "content": {"title": "Latent Fourier Transform", "abstract": "We introduce the Latent Fourier Transform (LatentFT), a framework that provides novel frequency-domain controls for generative audio models. LatentFT combines a diffusion autoencoder with a latent-space Fourier transform to separate musical patterns by timescale. By masking in the latent frequency domain during training, our method yields representations that can be manipulated coherently at inference. This allows us to generate musical variations and blends from reference examples while preserving characteristics at user-specified timescales. LatentFT parallels the role of the equalizer in audio production: while traditional equalizers operates on audible frequencies to shape timbre, LatentFT operates on latent frequencies to shape musical structure. Experiments and listening tests show that LatentFT improves condition adherence and quality compared to baselines. We also present a technique for hearing latent frequencies in isolation, and show different musical attributes reside in different regions of the latent spectrum. Our results show how frequency-domain control in latent space provides an intuitive, continuous frequency axis for conditioning and blending, advancing us toward more interpretable and interactive generative audio models.", "tldr": "We introduce novel frequency-domain controls for generative music models by applying the Fourier transform to the latent space of a diffusion autoencoder.", "keywords": ["Music Generation", "Signal Processing", "Diffusion Models", "Audio", "Music", "Audio Generation", "Controllable Generation", "Fourier Transform", "Diffusion Autoencoders"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/425b90b90e8098c2c804f335fd7df1b3d1371ac2.pdf", "supplementary_material": "/attachment/da15f87868bb66fa94b174bb87022954bbd69426.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Latent Fourier Transform (LATENTFT), a framework for controllable music generation that has a capability of latent frequency‑domain controls. LATENTFT is based on a diffusion autoencoder with a latent‑space discrete Fourier transform (DFT), and uses randomized frequency masking during training so that latent spectra can be manipulated coherently at inference. \nLATENTFT uniquely supports conditional generation and music blending by conditioning on user‑selected latent frequency filtering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Unique controllabilty along with timescale and frequency on latent representation.\nLATENTFT offers a continuous, interpretable frequency axis in latent space, enabling users to select which timescales to preserve or blend. This is a crisp and distinctive contribution to controllable music generation and muisc blending.\n\n- The overall design choice is simple yet effective and well-motivated based on classical audio signal processing, e.g., DFT/iDFT with masking strategy on the basis of insights on frequency characteristics of audio.\n- Empirical evidence that latent‑space frequency filtering has interesting characteristic that could pave the way to new control axis on music generation. Spesifically, int the demo page, the authors demonstrate clear difference of dynamics between singal-based and latent-frequencey-based filtering, which makes the model possible to conduct novel music blending."}, "weaknesses": {"value": "Although other representative autoencoder-based audio syhtesis methods such as RAVE [1] and DDSP [2] do not perform explicit latent frequency filtering, the lack of disucssion and comparison with these methods remains a minor weakness. Clarifying how these approaches relate to and are positioned against LATENTFT in the main body or the appendix would make the paper stronger.\n\n[1] Caillon, Antoine, et al. \"RAVE: A variational autoencoder for fast and high-quality neural audio synthesis.\" arXiv preprint arXiv:2111.05011 (2021).\n\n[2] Engel, Jesse, et al. \"DDSP: Differentiable digital signal processing.\" ICLR 2020."}, "questions": {"value": "- Would it be possible to benchmark RAVE and/or DDSP with the evaluation setups conducted on this paper? Alternatively, if these models are considered unsuitable for benchmarking, could the authors please elaborate on the reasons?\n\n[Comments]\n- The reviewer noice that there would be a high-level conceptual connection between the training strategy on LATENTFT and AudioMAE [3] (which is done in Mel-spectrogram domain). This would suggest that learned latent space might have strong semantic meanings not only for controllable music generation but also for audio classification and understanding tasks. Further investigation of the latent representation on such downstream tasks could provide interesting insights and would further strengthen the controbution.\n\n[3] Huang, Po-Yao, et al. \"Masked autoencoders that listen.\" NeurIPS 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6BeMKjYIK1", "forum": "ogMxCjdCCq", "replyto": "ogMxCjdCCq", "signatures": ["ICLR.cc/2026/Conference/Submission22727/Reviewer_eE9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22727/Reviewer_eE9f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591579599, "cdate": 1761591579599, "tmdate": 1762942360502, "mdate": 1762942360502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes controllable music generation using a Diffusion Autoencoder framework. The method involves applying a DFT over time to encoded latents (interpreted as “timescales” or “latent frequency”), random masking of selected bands, inverting back to latents via an inverse DFT, and decoding with a diffusion model. The core idea of “latent frequency” is promising and appears to offer control over musical structure including song sections, harmony, all the way down to short transient content. \nThe paper, however, has weaknesses in multiple fronts. The writing is unclear on core concepts and methods (e.g., what “frequency” means in latent space, what masking actually teaches the network), and lacks explanation on how to interpret fundamental quantities like latent space sampling rate. Several claims lack experimental validation and theoretical justification, and are not evaluated against adequate baselines. Although this idea is worth exploring, in its current state I unfortunately cannot recommend this paper for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of utilizing the Fourier spectrum of encoded music waveforms is interesting.\n\n2. Controllable music generation from a structural standpoint as opposed to just spectral is a useful direction to pursue. \n\n3. Audio demos are plentiful."}, "weaknesses": {"value": "Timescales vs. audio frequencies: The method manipulates “frequency” in latent space, and not audible frequency bins in the waveform domain. This distinction (lines 84-89) is crucial but is not emphasized clearly enough in the methods section. Please clearly define latent frequency, latent frame rate (f_r), the mapping (f_k = k f_r/T'), and how “Hz” must be interpreted in latent space. Consider moving the (f_r) explanation from the appendix into Section 3. \n\nIt is unclear as to how well latent frequency bands align with musical structure. Please provide analyses or experiments showing whether latent bands correlate with sectional changes (say chorus/verse), beat changes, chord changes etc. In addition, it would help to show whether this mapping is consistent across diverse tracks from different styles of music. Otherwise the claims are perhaps somewhat vague.\n\nSection 4 does not include a basic experiment that removes latent DFT (e.g., by conditioning simply on raw latents) to show why latent DFT along with random masking is necessary or what fundamental gains it offers. \n\nMissing model architecture. Please provide the encoder/decoder architectures and key hyperparameters in the appendix. In addition, consider reducing the section on DFTs and instead add more explanations on Diffusion Autoencoders, how they are trained, loss functions and the basic motivation for choosing this architecture over say, Latent Diffusion. \n\nAblations section in (Appendix B.1): The text references ablations (without masking/correlation/log-scale/encoder”), but only a table is provided. No qualitative plots or thorough analyses. The authors may have inadvertently forgotten to add plots and results in this sedition. Kindly add figures and more metrics to support the takeaways.\n\nDataset is too narrow: All core claims are validated on short Jamendo-style clips. Add at least one other domain e.g., other genres of music, piano (MAESTRO dataset) to highlight the “latent frequency” concept and demonstrate generality. It would be interesting to see the idea extended to speech signals. \n\nTerminology inconsistency: If authors prefer to use “timescales,” kindly define once and stick to either “timescales (latent Hz)” or just “latent frequencies.” Please do not alternate between terms as it leads to confusion.\n\nMINOR:\n\nIn Fig. 4, please mark the vertical axis as frequency in Hz instead of bin-index. If you are logarithmically scaling the latent frequency axis, please show this clearly as well. For clarity, add a section on how to convert log-spaced spectrum back to the temporal domain via an IDFT. \n\nInput representation ambiguity: Fig. 2 suggests STFT/mel input, while text says waveform input. Please clarify the exact input representation and update the figure accordingly.."}, "questions": {"value": "The authors provide an analogy of EQing, however I believe this can be misleading. I encourage the authors to consider other examples that illustrate “latent frequencies” from a more compositional angle i.e, emphasizing structural as opposed to spectral attributes. \nHow well do different latent spectrum regions map to musical structures?\n\nPlease provide experiments (beat/downbeat F-scores vs. latent frequency bands, chroma change rate vs. latent frequency bands, onset density vs. bands, ground truth chord changes) and show cross-track consistency.\n\nWhat is masking teaching the network? Explain the training signal and why random contiguous band masking encourages the encoder/decoder to organize information by temporal rate, and how this yields controllable edits at test time. Can you provide experiments that show the effect of masking vs no-masking? \n\nCan you include more audio demos on other styles of music? It is hard to guage how successful the method is given only one style of music."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1iZNyMHfFD", "forum": "ogMxCjdCCq", "replyto": "ogMxCjdCCq", "signatures": ["ICLR.cc/2026/Conference/Submission22727/Reviewer_1GMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22727/Reviewer_1GMu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947517102, "cdate": 1761947517102, "tmdate": 1763007709534, "mdate": 1763007709534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This authors introduce LATENTFT, which applies Fourier transforms to latent time series from a diffusion autoencoder and trains with random, correlated log-frequency masking to enable frequency-domain control over music generation. The approach allows users to manipulate latent frequency bands at inference to generate variations preserving specific timescales or to blend references by combining disjoint frequency regions. Experiments on MTG-Jamendo demonstrate improvements over baselines in adherence metrics (loudness, rhythm, timbre, harmony) and quality (FAD), supported by a 29-musician listening study. The method offers interpretable control via \"isolation\" experiments linking latent Hz to musical attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors' decision to operate in latent frequency space rather than audio frequency is well-motivated, and the correlated log-frequency masking scheme aligns naturally with musical structure and audio engineering intuitions. The proposed algorithms are clearly specified, the training procedure is straightforward, and the mask-conditioning interface provides an interpretable control mechanism. Quantitative improvements over multiple baselines and positive user study results support the core claims for the tested domain. Preservation curves and isolation experiments provide insight into what different latent frequency bands capture, and can potentially offer practitioners guidance for control."}, "weaknesses": {"value": "(1) All experiments use 5.9-second clips from a single dataset (MTG-Jamendo) with mel-spectrogram front-ends. This raises serious questions about generalization to: (a) longer musical forms where phrase/section structure matters, (b) other audio domains (speech, non-Western music, sound design), and (c) different representation choices (raw waveform, neural codec latents). The paper's claims about \"music generation\" are stronger than what this limited scope can support.\n\n(2) The most competitive baselines (ILVR, codec/spectrogram filtering) operate on spectrogram rather than latent frequency, creating an uneven comparison. I suggest the authors to train the same architecture with random band-drop augmentation in latent time but without explicit DFT structure as a baseline/ablation. This would isolate whether gains come from Fourier properties specifically or just from any structured frequency-aware regularization.\n\n(3) While Table 3 shows that correlated/log masking matters for quality, the ablations don't clearly separate: (a) the value of training with any latent-space augmentation, (b) the specific benefit of DFT orthogonality, and (c) the role of the log-frequency parameterization. The large FAD changes suggest the masking strategy is critical, but mechanistic understanding remains limited.\n\n(4) With only 29 participants and no reported effect sizes, confidence intervals, or inter-rater reliability, it's difficult to assess the robustness of the preference results. The study design (direct comparison on 20 pairs) is appropriate but underpowered for such strong claims."}, "questions": {"value": "(1) Can the authors provide failure case analysis? When does latent frequency control break down (e.g., very low-Hz bands dominating, cross-band dependencies)?\n\n(2) How does the method perform with longer contexts (30+ seconds, ideally ~3 mins) where hierarchical structure becomes important? Does the flat latent time series model capture phrase/section boundaries?\n\n(3) What happens with alternative encoders (e.g., raw-audio encoders like Encodec)? The appendix hints that encoder architecture affects behavior; to support the broader general claim, this deserves fuller investigation.\n\n(4) Could you add a latent-augmentation baseline mentioned above without DFT to clarify what the Fourier structure specifically contributes versus generic frequency-aware training?\n\n(5) What are the computational costs of the increased spectral resolution (zero-padding factor L) and correlated masking during training/inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fXQ5Xj6ycS", "forum": "ogMxCjdCCq", "replyto": "ogMxCjdCCq", "signatures": ["ICLR.cc/2026/Conference/Submission22727/Reviewer_YTEd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22727/Reviewer_YTEd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957947452, "cdate": 1761957947452, "tmdate": 1762942359806, "mdate": 1762942359806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a generative audio framework that enables a novel frequency-domain control by manipulating musical patterns in the latent space according to their timescales. Their method:\n- combines a diffusion autoencoder with a latent-space Fourier transform, trained end-to-end to decompose and control musical structures -- timescale directly in the latent space.\n- Allows user-specified timescale conditioning, analogous to how equalizers shape timbre in audible frequencies. They use their model for generating music variation, blending, and separating patterns by timescale directly in the latent space with musically coherent results.\n- Introduces new evaluation baselines tailored to assess this novel control task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors present a new method for guiding music generation by adjusting timescales in the latent space, offering an alternative to global, time varying, or token-based conditioning approaches.\n- In the absence of prior studies on this type of control, they design thoughtful and relevant baselines, setting the stage for future developments in the field.\n- The proposed method consistently outperforms existing baselines, demonstrating stronger conditioning behavior and clearer interpretability in blending and timescale separation tasks. Their analysis of the latent spectrum further reveals meaningful correlations with musical traits such as tempo, and pitch.\n- The audio demonstrations show that their method is promising, and the upcoming code release will make the work easily reproducible and enhanced.\n- Altogether, the paper lays a strong foundation for a new direction in generative audio research, opening the door to further refinements and creative extensions."}, "weaknesses": {"value": "- The notion of timescale conditioning offers an interesting angle, but it provides a somewhat narrow view of musical structure. Many musical genres do not rely on clear, repeating patterns, which may limit the method’s generality beyond pattern-based music like pop or rock.\n- It remains unclear whether the model can control acoustic attributes such as timbre or warmth, which reside in the actual frequency domain rather than in the latent frequency space. For instance, the diffusion model seems to receive no extra guidance, such as text or acoustic features, to generate the missing frequency content, resulting in more random variations and reduced controllability.\n- Although the model can isolate latent frequencies, it struggles to disentangle higher-level musical concepts such as instruments or timbral components. This raises concerns about scalability, for instance, how would it handle evolving patterns or instruments in a full-length piece, given that training and evaluation rely on short (≈5.9 s) audio clips?\n- The audio quality of the generated samples is still limited, with noticeable metallic artifacts that affect realism and listening comfort."}, "questions": {"value": "- Line 352: The author state that in the Guidance and ILVR baselines, the diffusion process is guided using Mel spectrograms rather than the latent spectrum. Why was this approach chosen, and wouldn’t applying the DFT directly to the latent space make for a more comparable baseline?\n- Could the authors clarify the influence of framerate T’ on the results? How should it be chosen to effectively capture musical patterns?\n- Minor note: in line 830, there appears to be an extra space that should be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NamAEkhAln", "forum": "ogMxCjdCCq", "replyto": "ogMxCjdCCq", "signatures": ["ICLR.cc/2026/Conference/Submission22727/Reviewer_jWrm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22727/Reviewer_jWrm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991151242, "cdate": 1761991151242, "tmdate": 1762942359420, "mdate": 1762942359420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}