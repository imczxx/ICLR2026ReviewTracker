{"id": "oka09rPYGR", "number": 18723, "cdate": 1758290419411, "mdate": 1759897084912, "content": {"title": "M3DOnline: Foundation-Prior Guided Monocular 3D Motion Learning for Autonomous Driving in Novel Scenes", "abstract": "We propose M3DOnline, a learning framework for normalized scene flow (NSF). NSF represents the dense 3D motion of pixels between two frames and plays a critical role in various monocular 3D vision tasks. Existing self-supervised NSF methods heavily rely on strong visual cues, which limits their performance on non-Lambertian surfaces and around motion boundaries.\nOur key insight is to leverage useful priors from foundation models to overcome the inherent limitations of texture-based matching in traditional self-supervised methods. Specifically, we design a pseudo-label generation pipeline using semantic and depth foundation models. Based on rigid motion assumptions, we divide real-world scenes into semantic segments and generate per-segment 3D motion pseudo-labels. \nTo handle inevitable non-rigid regions and reduce the impact of inaccurate predictions from foundation models, we introduce a loss-based adaptive learning strategy, which filters out obvious non-rigid areas and dynamically adjusts the learning weight and region based on label quality.\nExperiments show that M3DOnline significantly improves motion boundary estimation and the handling of reflective and transparent surfaces. This demonstrates the advantage of integrating foundation model priors into self-supervised scene flow learning. Code will be available.", "tldr": "Leverage the beneficial prior knowledge from large models to enhance the weaknesses of existing self-supervised methods.", "keywords": ["Normalized Scene Flow; Autonomous driving; 3D Vision"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83a09fbca6c5c1d6be468cee2a5e77bb076a6d34.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The method first partitions the scene into semantic blocks using SAM and estimates depth with DAM. Assuming rigid motion for these blocks, it calculates 3D motion pseudo-labels. Then it incorporates an adaptive learning strategy that dynamically adjusts weights for different regions during training."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Combines multiple pre-trained models to achieve self-supervised motion estimation.\n- The motivation is potentially interesting, though not clearly articulated or convincingly demonstrated."}, "weaknesses": {"value": "1. Weak experimental validation. The abstract claims significant improvements “on non-Lambertian surfaces and around motion boundaries,” but there is no targeted quantitative or qualitative experiment demonstrating this. A few final results do not constitute proof of these claims.\n2. Weak motivation and limited insight. The paper’s motivation is mostly engineering-driven (“use SAM/DAM to generate pseudo labels”) and lacks conceptual depth or new insights. Moreover, since both SAM and DAM also depend heavily on strong visual cues, it remains unclear how this approach fundamentally overcomes the claimed texture dependence.\n3. Writing and structure issues. The manuscript suffers from vague transitions and unclear logical flow. Overall, it is difficult to follow and feels unfinished, as if not carefully proofread. This is particularly evident in the method section (see Question 1 for example).\n4. Lack of error analysis. There is no discussion of how pseudo-label noise or inaccuracies propagate through the pipeline.\n5. Contribution feels more engineering than scientific. While the work leverages recent pretrained models, it primarily represents a pipeline integration effort rather than a contribution offering new conceptual or algorithmic insights to the community."}, "questions": {"value": "1. Unclear methodological definitions. The methodological descriptions are unclear and require substantial revision. A few examples are listed below; please review the rest of the paper carefully to ensure all definitions and procedures are clearly explained and easily understandable to readers.\n- Section 3.1.1: The notations (M_s), (M_b), and the later term “scene blocks” are not properly defined or connected. Their relationships and roles in the overall pipeline should be explicitly stated to ensure readability and reproducibility.\n- Section 3.1.2: i) The explanation of the “rigid optimization” process is unclear and lacks logical justification. The claimed “two benefits”: (1) optimizing other parts based on a small amount of correct optical flow, and (2) estimating challenging regions such as occlusion or textureless areas — are not well supported by the described mechanism. It is unclear why or how the proposed optimization method enables these advantages. Please clarify the reasoning and explicitly connect each claimed benefit to the corresponding computational step. ii) It is unclear what the “mask of the block to be optimized M_o ​ ” refers to or how these blocks are chosen. \n- Eq. (2): There is no guarantee that a point (p_1) and a point (p_2) from their respective point clouds correspond to the same physical location on the moving object. Without establishing valid correspondences, applying rigid transformation optimization in this form is fundamentally incorrect. Please clarify or provide supporting justification.\n- The term “internal parameter” should be replaced with **“camera intrinsics”** to align with standard computer vision terminology.\n\n2. How do pseudo-label errors from SAM and DAM affect training stability? Is there any filtering or confidence-based weighting besides the loss-based adaptation?\n3. What is the performance difference between SAM/DAM variants (large/base/small) in terms of actual accuracy rather than size?\n4. Please add more detail and analysis in experiment:\n- Explain clearly what “large,” “small,” and “base” refer to. Include detailed information (e.g., parameter count, model size, accuracy) in the appendix.\n- Consider including results with the latest available versions of the foundation models (SAM2, DAM2) to ensure the latest results.\n5. Formatting: Please carefully revise the paper for consistency and clarity, for example:\n- Please ensure all acronyms (e.g., TTC on L319) are defined upon first use.\n- Please ensure consistent formatting in tables (e.g., decimal places in Tab. 4) and clearly indicate whether metrics are \"lower is better\" or \"higher is better\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ntleCUj7VU", "forum": "oka09rPYGR", "replyto": "oka09rPYGR", "signatures": ["ICLR.cc/2026/Conference/Submission18723/Reviewer_UFHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18723/Reviewer_UFHx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410949040, "cdate": 1761410949040, "tmdate": 1762928429589, "mdate": 1762928429589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents M3DOnline, a self-supervised monocular 3D motion learning framework that integrates strong structural and semantic priors from Segment Anything (SAM) and Depth Anything (DAM). The method generates pseudo-labels through rigid optimization on segmented scene chunks and employs a quality-aware dynamic loss weighting mechanism. This aims to reduce the common weaknesses of photometric self-supervision, particularly around non-Lambertian surfaces and motion boundaries. The resulting model shows competitive performance on standard benchmarks, outperforming other self-supervised methods and approaching supervised performance in some metrics. The technical design is solid, although the main contribution lies in the integration of existing priors rather than a fundamentally new algorithmic framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The problem is clearly motivated, and the paper targets a well-known limitation of photometric self-supervision. Integrating SAM and DAM offline is a practical design choice that avoids computational overhead at inference while providing strong priors for training. The pseudo-label generation based on rigid motion priors is simple, conceptually clear, and empirically effective. The dynamic loss weighting is well justified and improves robustness. The experiments are clean and show consistent improvements over standard self-supervised baselines. The paper is also structured in a way that makes the approach easy to follow and likely reproducible."}, "weaknesses": {"value": "Similar integration patterns have recently appeared in optical flow and depth estimation, so the conceptual novelty is moderate. The method relies heavily on rigid-motion assumptions, which may limit generalization in nonrigid scenarios. The external validity of the proposed FCP-like assessment mechanism is not fully established—particularly its correlation with standard downstream planning or reconstruction metrics. Additionally, the paper lacks a systematic analysis of robustness to segmentation or depth noise, which makes it harder to assess the stability of the proposed approach in real-world settings."}, "questions": {"value": "The approach leans on rigid optimization and foundation-model priors. How does it behave in scenes with pronounced nonrigid motion, or when SAM/DAM provide imperfect masks/depth (e.g., fragmentation, low-texture, reflectance)? What failure modes are most characteristic in these cases?\n\nThe training signal combines pseudo-labels and quality-aware weighting. How sensitive are the reported gains to key choices (e.g., partition granularity, flow horizon, weighting thresholds/temperatures) and to moderate segmentation/depth noise?\n\nBeyond the driving benchmarks, how well does the method carry over to out-of-domain conditions (e.g., low-texture scenes, adverse weather, or non-driving datasets)? Are there conditions under which the advantages diminish markedly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A2wZGgK6hF", "forum": "oka09rPYGR", "replyto": "oka09rPYGR", "signatures": ["ICLR.cc/2026/Conference/Submission18723/Reviewer_7ybE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18723/Reviewer_7ybE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761537006658, "cdate": 1761537006658, "tmdate": 1762928428800, "mdate": 1762928428800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a monocular scene flow estimation pipeline using SAM and DAM as priors. The generation os pesudo labels are evaluated and adjusted by designed loss functions. Extensive experiments prove the competitive performance of proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The injection of depth (scene structure) and semantic information is reasonable, offering better 3D motion learning ability.\n\n2. Authors clearly illustrate the method details and novelty.\n\n3. The overall writing is good and easy to follow.\n\n4. Experiment results are sufficient to prove the effectiveness of the proposed design."}, "weaknesses": {"value": "1. Leveraging foundation models as prior is not so novel. There are some existing methods in optical flow approaches, which utilize foundation models to strengthen the performance. These works share very similar insight.\n\n2. Since VGGT can already predict accurate 3D scene information (point tracking, depth) from monocular images, what is the meaning of designing so many detailed tricks for a better 3D motion learning?\n\n3. In Section 3.1.2, why the input of the network has optical flow? How is it obtained?\n\n4. An evaluation about runtime and efficiency of this proposed method is expected.\n\n5. There are some grammar issues in the paper writing. For example, the inconsistent tenses in the first graph of Section 3."}, "questions": {"value": "Please refer to the weakness sections. Authors should give more motivations why they design the complicated information injection method from SAM and DAM, rather than using VGGT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jHIlmVLSjY", "forum": "oka09rPYGR", "replyto": "oka09rPYGR", "signatures": ["ICLR.cc/2026/Conference/Submission18723/Reviewer_XWn3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18723/Reviewer_XWn3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583705454, "cdate": 1761583705454, "tmdate": 1762928428318, "mdate": 1762928428318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes M3DOnline, a monocular self-supervised 3D scene flow estimation framework trained with foundation (namely, SAM and DAM) models' assistance. This framework applies foundation model outputs to generate pseudo-labels, used to train the model. A pseudo-label pipeline is designed to rectify current estimates based on texture and occlusion regions. A credibility score is also designed to measure the accuracy of pseudo-labels, so they can be used with different weights in training. Experiments show promising results for this method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper is clearly stated, and the proposed method generally makes sense.\n2. The figures are well-crafted, which helps understanding.\n3. The authors state that the code will be available, representing good reproducibility."}, "weaknesses": {"value": "1. The presentation of the paper needs to be improved, including more formal academic writing for better readability and clarity. Some examples include but are not limited to:\n    - Many of the citations in the paper should be cited in the `\\citep` format, but the authors used `\\citet`, which interrupts the flow of the text.\n    - Many typo or grammatical errors:\n      - Line 049: \"makes\" -> \"make\".\n      - Line 178-180: Why is mask $\\mathbf{M}_s$ bolded, but the mask $M_b$ not?\n      - Line 241: \"warp\" -> \"warped\".\n      - Line 367-368: \"supervision\" -> \"supervised\", \"self-supervision\" -> \"self-supervised\".\n\n2. The method design generally needs more context or explanation. For example, Eq (9)-(15): How is these metrics derived? Why cubed in Eq (9), to the power 0.1 in Eq (10)? Why multiplication in Eq (13), not addition? Which metric is scalar and which is vector/map? Any references? The paper does not show any context or explanation on these designs, and they currently just appear out of nowhere.\n\n3. Line 210-211: \"Assuming the camera is stationary relative to the world coordinate system\". Is this a general assumption for this whole model? I do not believe this assumption holds for autonomous driving.\n\n4. Experiment results:\n   - Most of the previous models in Table 3 did not use scene flow datasets other than KITTI to train, so that may not be a very fair comparison. The paper should at least note that in the caption.\n   - Table 4: It would be better to highlight the results of your final model, so we can easily tell apart from other ablation experiments."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OtRscjIMjb", "forum": "oka09rPYGR", "replyto": "oka09rPYGR", "signatures": ["ICLR.cc/2026/Conference/Submission18723/Reviewer_s2uE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18723/Reviewer_s2uE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951576430, "cdate": 1761951576430, "tmdate": 1762928427673, "mdate": 1762928427673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}