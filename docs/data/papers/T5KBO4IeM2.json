{"id": "T5KBO4IeM2", "number": 22390, "cdate": 1758330464116, "mdate": 1759896868777, "content": {"title": "Parallel Prompting: Fast LLM Inference for Shared-Context, Short-to-Moderate Output", "abstract": "We introduce Parallel Prompting, a novel method for efficiently decoding multiple queries that share a common prefix in large language models (LLMs). This scenario occurs naturally in tasks such as document question answering, few-shot learning, and chatbot systems, where many prompts have substantial overlap. Our approach overcomes shortcomings of prior methods, which either leads to the degraded output quality or inefficient cache management. Crucially, we identify that maximizing inference throughput requires a careful balance between attention parallelism and batch size. The theoretical maximum throughput lies at a point determined by the hardware and model specifics, and cannot be achieved by solely increasing batch size or attention parallelism. In contrast to related methods that forbid hybrid batching or require pre-allocated memory for the entire generation, our approach supports flexible batching across multiple sharing groups and enables dynamic, on-demand memory usage. By decoding all queries in parallel with efficient matrix-matrix operations, our method significantly improves throughput and memory utilization without compromising result quality. Experimental results demonstrate that our method can improve end-to-end Llama3-8B latency by up to 4× against competitive baselines on popular datasets, without compromising output quality or accuracy.", "tldr": "", "keywords": ["Attention", "GPUs", "Inference", "parallelization", "large language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/727f2dbd31850eab50242d1175214c2bf686d05e.pdf", "supplementary_material": "/attachment/86fdf906e380cdc193e86ed2ad68c427ce8d2fd4.zip"}, "replies": [{"content": {"summary": {"value": "This work focuses a practical question: how to efficiently decode multiple queries that share a common prefix. The Parallel Prompting finds that the inference throughput requires a careful balance between attention parallelism and batch size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work try to improve the efficiency of generation that shares a common prefix in large language models (LLMs). For example, the current LLM usually has a system prompt.\n* The Figure 1 presents that there is an optimal point between the parallel size and batch size. This is the strength of the motivation of this work:.\n* This work significantly improves the generation thoughtoutput, as presented in Figure 1.\n* This work sufficient discuss the effect of query number and query length in Figure 3\n* In Table 1, this work almost reduce half time cost than the vllm."}, "weaknesses": {"value": "* Is the Parallel Size P the number of sub batch that are process? In Algorithm 1, why logits ← outputs[:, −P :], as the P should be at the batch size dimension?\n* In Figure 1 middle, why the increase of log(parallel/batch_size) will lead to the memory cost decrease?\n* Does this work propose a method to estimate the optimal P for the processing? How to determine the Parallel Size P?\n* The method performance may not be good with long context."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5RiLcdbSYW", "forum": "T5KBO4IeM2", "replyto": "T5KBO4IeM2", "signatures": ["ICLR.cc/2026/Conference/Submission22390/Reviewer_Buei"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22390/Reviewer_Buei"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491584942, "cdate": 1761491584942, "tmdate": 1762942197175, "mdate": 1762942197175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Parallel Prompting to efficiently decode multiple queries that share a common prefix in LLMs. This method introduces a method for efficiently generating answers to multiple questions in parallel by independently encoding prompts and leveraging shared context in large language models. Experimental results show that Parallel can improve end-to-end Llama3-8B latency by up to 4× against competitive baselines, without compromising output quality."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Experimental results show its effectiveness."}, "weaknesses": {"value": "1. The writing is poor in Lines 110 - 151. This makes the paper very difficult to understand. \n\n2. The paper is very difficult to understand in terms of what was actually done and why the method is effective. Its algorithm is highly non-intuitive.\n\n3. The second and third paragraphs of the introduction are written very poorly. The author hasn’t even figured out how to organize their own work.\n\n4. This approach requires a longer context. Wouldn't that lead to increased computational costs?\n\n5. Why does SeqBatch take less time on the QuAC dataset?\n\n6. The placement of figures and tables in the experimental section of this paper needs to be rearranged.\n\n7. In Line 110, Suppose we have a context C and N sentence queries -> Suppose we have a context C and n sentence queries7"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sf2c5GGVyd", "forum": "T5KBO4IeM2", "replyto": "T5KBO4IeM2", "signatures": ["ICLR.cc/2026/Conference/Submission22390/Reviewer_aDJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22390/Reviewer_aDJT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812336303, "cdate": 1761812336303, "tmdate": 1762942196965, "mdate": 1762942196965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Parallel Prompting, a novel method for efficiently decoding multiple queries that share a common prefix in large language models. By leveraging parallel processing and matrix-matrix operations, the method significantly improves inference throughput while maintaining output quality. The authors provide theoretical grounding and extensive experiments demonstrating its effectiveness, particularly for short-to-moderate output lengths with high prefix overlap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty: The proposed method effectively addresses inference inefficiencies in shared-prefix scenarios.\n\n2. Theoretical Foundation: A solid analysis based on Amdahl’s Law and hardware constraints explains the trade-off between parallel size and batch size.\n\n3. Comprehensive Experiments: Systematic evaluations across multiple models and datasets show improvements in throughput, memory usage, and output quality."}, "weaknesses": {"value": "1. Advantage Diminishes and Reverses with Longer Outputs: This is the most significant limitation. As shown in this paper, the generation time of the proposed method begins to exceed that of vLLM when the output length per query exceeds approximately 200 tokens. Therefore, this method is applicable to a quite limited range of tasks.\n\n2. System Complexity and Scheduling Overhead: To achieve optimal performance, the method necessitates a sophisticated scheduler to dynamically balance parallel size (P) and batch size (B). This scheduler must make real-time decisions based on hardware specs, model size, prefix length, output length, etc. The computational and logical complexity of this scheduling itself, compared to the relatively \"dumb\" but simple scheduler in vLLM, represents an additional engineering and runtime cost that the paper does not evaluate."}, "questions": {"value": "1. In extreme cases, such as when the prefix length reaches tens of thousands of tokens and the number of queries is also large, could the initial prefill stage and memory usage of the method become a new bottleneck? How would it perform compared to other methods?\n\n2. Given the core weakness identified, the paper suggests a hybrid scheduling policy in production. Could you elaborate on the design principles of such a hybrid scheduler? For instance, would the decision to use Parallel Prompting or fall back to a dynamic batching method (like vLLM's default) be based on the estimated output length or the real-time observed generation length?\n\n3. The paper identifies that maximizing throughput requires balancing the parallel size (P) and batch size (B), and finds the optimal (P, B) through experimental search. However, for production systems, the cost of this search itself can be high. Could the authors propose a low-cost method or a rule of thumb (e.g., building a predictive model based on key factors like model size, prefix length, available memory, etc.) to efficiently determine or dynamically adapt near-optimal P and B values, rather than relying on expensive grid search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xRwjVO5mS3", "forum": "T5KBO4IeM2", "replyto": "T5KBO4IeM2", "signatures": ["ICLR.cc/2026/Conference/Submission22390/Reviewer_KAvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22390/Reviewer_KAvc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888708360, "cdate": 1761888708360, "tmdate": 1762942196630, "mdate": 1762942196630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ParallelPrompting for efficiently serving many document - many questions scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "ParallelPrompt can speed up overall serving throughput"}, "weaknesses": {"value": "I have much confusion about the figures and the main texts. Please review my questions."}, "questions": {"value": "### Questions\nWhat is the fundamental difference between RelayAttention + Prefix cached batched decode?\n\nFigure 1 Left Center. X Label should be formatted formally. What do you mean by Parallel Size / Batch Size exactly?\n\nFigure 1 Left Center. The legend does not appear to be formally formatted. What is Length(Doc)? Context length? Length of shared tokens? Number of documents in request?\n\nFigure 1 Right. What is 8 x 64, 8x 128, 8x 256? Does it mean (document count ==) batch size and parallel question sizes? X label looks confusing.\n\nFigure 2. Did you use huggingface's static cache? Why is it OOM?\n\nFigure 4. Why no SGlang?\n\nFigures should be PDF or SVG exported\n\nTable 1. What is lossless, or not (vLLM and SGlang must be lossless, right?)\n\nTable 3. num_q -> change to formal naming\n\n### Formattings\nAlgorithm 1: Why are local variables not plain text? Font formatting should be textt or text\n\nTypo line 320: inference.CodeLlama -> inference. [ ] CodeLlama"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eVmed4f5NC", "forum": "T5KBO4IeM2", "replyto": "T5KBO4IeM2", "signatures": ["ICLR.cc/2026/Conference/Submission22390/Reviewer_XfJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22390/Reviewer_XfJt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929523503, "cdate": 1761929523503, "tmdate": 1762942196388, "mdate": 1762942196388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}