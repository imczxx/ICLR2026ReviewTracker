{"id": "doR25jEjbj", "number": 17534, "cdate": 1758277264306, "mdate": 1759897168721, "content": {"title": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning", "abstract": "We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they’ve gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.", "tldr": "Use Shannon-entropy from token logprobs to stop CoT early, cutting 25–50% tokens/latency with no accuracy loss. Few examples set thresholds; harder items get extra budget, transferring across reasoning LLMs.", "keywords": ["adaptive compute", "entropy-based early stopping", "token efficiency", "confidence calibration", "chain-of-thought", "few-shot thresholding", "budget reallocation", "reasoning LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37eefffa8ca24d3f8798e7dfbd839e0dfc643f6f.pdf", "supplementary_material": "/attachment/52e9cc51cb52540915f9abedf91122be39d760c6.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a training-free framework for improving the computational efficiency of LLM reasoning. The core idea is to use the shannon entropy of token-level log-probabilities as a confidence signal for early stopping. If the entropy of the initial reasoning step is below a model-specific threshold, the model is considered confident, and further reasoning is halted, saving tokens. The authors claim this method can achieve 25-50% computational savings without any loss in task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Insight on emergent confidence calibration**: The paper empirically finds that entropy-based confidence is an emergent property of advanced post-trained models. It's a good insight for the community.\n\n* **Simplicity and applicability**: The proposed method is training-free, model-agnostic, and straightforward to implement.\n\n* **Comprehensive evaluation**: The paper proposes four distinct, mathematically-derived thresholding methods and validates them with appropriate statistical measures (Cohen's d, t-tests, confidence intervals), demonstrating robust performance on challenging reasoning benchmarks (AIME, GPQA)."}, "weaknesses": {"value": "* **Incremental novelty**: While the emergent property analysis is novel, the core idea of using token entropy as a confidence signal for adaptive computation is not new. Works like HALT-CoT [1] and AdaDec [2] have explored similar concepts.\n\n* **Simple modeling**: The paper defines \"extended reasoning\" as a fixed, 4-step sequential process. Modern advanced reasoning often involves more complex structures, such as iterative self-correction loops. The paper fails to investigate or discuss how to apply to these more practical frameworks.\n\n* **Lack of other baselines**: The evaluation only compares the early-stopping performance against the full 4-step baseline of the same model. It does not compare against other potential confidence heuristics or other methods for improving inference efficiency.\n\n[1] HALT-CoT: Model-Agnostic Early Stopping for Chain-of-Thought Reasoning via Answer Entropy. ICML 2025 workshop\n\n[2] AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation. Arxiv 2506"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cvrZbftS5c", "forum": "doR25jEjbj", "replyto": "doR25jEjbj", "signatures": ["ICLR.cc/2026/Conference/Submission17534/Reviewer_fyzz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17534/Reviewer_fyzz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760764978805, "cdate": 1760764978805, "tmdate": 1762927410466, "mdate": 1762927410466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles test-time overcomputation in multi-step reasoning LLMs by asking if confidence after the first reasoning step is already sufficient to stop. They compute sequence-level entropy over the top-k (k=20) token probabilities for that step, average over all tokens, and compare to a calibrated threshold: low entropy means halt, high entropy means run the full schedule. They introduce four thresholding schemes (mean, “information-theoretic,” Bayesian, scale-invariant) and claim each can be calibrated from a small labeled set. On AIME-2024/2025 (30 problems each) and GPQA-Diamond (~200), and on the post-trained/reasoning models they use (GPT-OSS, Qwen3), correct first-step traces show lower entropy than incorrect ones, yielding ~25–50% token savings with little or no accuracy loss. They also report a negative result on Llama-3.3-70B, where this separation vanishes, and read it as evidence that “entropy as confidence” is tied to post-trained reasoning models.\n\nHowever, the contribution is incremental: it stays within the standard entropy/confidence halting paradigm and mostly shifts the probe to “step-1 entropy + τ,” without head-to-head comparisons against the most natural existing halting methods. The evaluation is narrow and curated (tiny, clean, math/science datasets; no messy agent/tool/code/multi-hop settings), and most comparisons are intra-paper plus “full 4-step” rather than against strong prior baselines. Claims like “first comprehensive study … across mathematical and scientific reasoning” should be toned down given the small tests used."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, well-motivated problem\n- Very simple, training-free mechanism, computing sequence-level entropy after the first reasoning step and gate with a single threshold\n- Nice empirical observation: on reasoning-tuned models (DeepSeek/Qwen-style) correct step-1 traces have noticeably lower entropy than incorrect ones.\n- Honest negative result on vanilla Llama 3.3 70B, showing the signal isn’t magic and seems tied to post-trained “thinking” models."}, "weaknesses": {"value": "- Incremental beause it is within the entropy/confidence-based halting paradigm; mainly shifts the probe to “step-1 sequence entropy + calibrated τ” but does not compare to the closest existing methods.\n- Evaluation is narrow and curated: tiny, clean, math/science datasets (AIME-24/25: 30 items each; GPQA-Diamond: ~200) and no tests on messier traces (tool-augmented agents, codegen, multi-hop QA, chattier models), so the “universal/model-agnostic” claim isn’t supported. Work in this area contains at least one messy dataset as I mentioned in the summary.\n- Empirical comparisons are weak: mostly their own threshold variants vs vanilla full 4-step decoding, with no strong baselines from prior halting/entropy work, so real progress is hard to judge.\n- They present a general budgeted scheme (A.2) and prove total calls = α, but never ablate α/δ/γ to show actual reallocations. Albations are important to assess the robustness.\n- The paper says “information-theoretic / Bayesian / scale-invariant” thresholds, but the appendix relies on hand clamps (e.g. max(0, 1 − σc/µc), log(1 + |d|)), which is less principled than the main-text tone and should be toned down."}, "questions": {"value": "Refer to weaknesses please"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LZxOnn1zZu", "forum": "doR25jEjbj", "replyto": "doR25jEjbj", "signatures": ["ICLR.cc/2026/Conference/Submission17534/Reviewer_d5Ba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17534/Reviewer_d5Ba"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921692028, "cdate": 1761921692028, "tmdate": 1762927409750, "mdate": 1762927409750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a simple method using Shannon entropy to measure the confidence of reasoning trajectory, and perform early stopping based on the the confidence measure. The evaluation shows 25-50% token saving compared to basic baselines while preserving accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper explores the Shannon-entropy based method to measure sequence-level entropy as a confidence measure. The method is relatively sound. \n2. The writing is easy to follow."}, "weaknesses": {"value": "1. Novelty. Entropy-based early-exit methods has been quite extensively studied in prior works such as the following. The paper needs to make a clearer distinction between the method and the prior works (in the related work). Just listing a few as addition to the current related work section:\n- https://arxiv.org/abs/2502.12067\n- https://arxiv.org/abs/2412.21187\n- https://arxiv.org/abs/2504.01296\n- https://arxiv.org/abs/2508.15260\n- https://arxiv.org/abs/2412.20993\n- https://arxiv.org/abs/2412.18547\n- https://arxiv.org/abs/2207.05221\n\n2. Lack of baseline. The paper doesn't seem to compare against the state of the arts to show the token saving compare to these methods. Therefore, the evaluation is considered not as convincing."}, "questions": {"value": "1. Related works as stated in the weakness.\n2. Lack of baseline as stated in the weakness.\n3. Confidence threshold. In section 3.5 the authors mentioned the hyperparameter threshold to choose from. How to choose this value? Any ablation to support your claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c5UcVP1M26", "forum": "doR25jEjbj", "replyto": "doR25jEjbj", "signatures": ["ICLR.cc/2026/Conference/Submission17534/Reviewer_QCkV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17534/Reviewer_QCkV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929791944, "cdate": 1761929791944, "tmdate": 1762927408954, "mdate": 1762927408954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an Shannon Entropy based reasoning model early exiting and budget control. It computes the entropy with few shot reasoning sequence, and stop the thinking process once the entropy is higher than a threshold. The author tested four different types of threshold and show that with few shot examples (at most 20 data point), the framework can reduce 25-50% tokens to be generated, while maintaining the same accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using entropy to control the reasoning budget is clean and effective.\n- The authors design comprehensive experiments to show the power of using entropy to reduce computation cost."}, "weaknesses": {"value": "- What is the source of validation dataset? The threshold's universality among different dataset is unclear. Based on Table 1, even the same model on different math problem dataset can have varied entropy range. Adding more details on the threshold value for each dataset and the the validation data can help improve the paper's contribution.\n- The core concept \"reasoning step\" is not well defined, making the paper's soundness not satisfying. What is a reasoning step, how to define the start and end of such a step, and how to find the start and the end during the runtime?\n- The experiment lacks comparison with the latest research on the same topic, such as [1][2][3].\n\n\n[1] Chen, Xingyu, et al. \"Do not think that much for 2+ 3=? on the overthinking of o1-like llms.\"\n\n[2] Fu, Yichao, et al. \"Reasoning without self-doubt: More efficient chain-of-thought through certainty probing.\"\n\n[3] Zhang, Anqi, et al. \"Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification.\""}, "questions": {"value": "- Why is the number in Figure 4(a) exactly the same for the first two bars, and the same for the last two bars"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SOc0FRCrEY", "forum": "doR25jEjbj", "replyto": "doR25jEjbj", "signatures": ["ICLR.cc/2026/Conference/Submission17534/Reviewer_KHhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17534/Reviewer_KHhP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197469439, "cdate": 1762197469439, "tmdate": 1762927408396, "mdate": 1762927408396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}