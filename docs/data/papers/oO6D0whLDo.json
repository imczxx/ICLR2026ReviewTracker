{"id": "oO6D0whLDo", "number": 198, "cdate": 1756730835327, "mdate": 1759898271973, "content": {"title": "CLARC: C/C++ Benchmark for Robust Code Search", "abstract": "Effective retrieval of code snippets from natural language queries is essential for code reuse and developer productivity. However, current benchmarks are limited: they predominantly focus on Python, lack support for industry-focused languages like C/C++, miss structured categorization, and are susceptible to models that exploit superficial lexical features instead of code semantics. To address these limitations, we introduce CLARC (C/C++ LAnguage Retrieval with Anonymized Code), a benchmark of 1,245 C/C++ query-code pairs that is fully compilable, configurable, and extensible. CLARC systematically categorizes snippets into three groups based on dependency complexity, allowing for a nuanced evaluation of retrieval performance under varying levels of code complexity. CLARC also provides configurable settings, including anonymized identifiers and low-level representations, to evaluate model robustness across different levels of code context and abstraction. Evaluation of six state-of-the-art code search methods shows significant performance drops under identifier anonymization, exposing existing modelsâ€™ persistent reliance on superficial cues. Their poor performance on low-level languages such as Assembly and WebAssembly further reveals limited effectiveness beyond high-level programming languages. We also introduce an automated pipeline for scalable benchmark generation, validated through hypothesis tests, enabling the efficient creation of high-quality code search datasets that can be reused by other dataset builders. Our dataset is publicly available at https://huggingface.co/datasets/ClarcTeam/CLARC.", "tldr": "", "keywords": ["Code Search", "Benchmark", "Robustness"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0fb26e3e108e998a44c545c47a5836cf11859da6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "CLARC introduces a comprehensive C/C++ code search benchmark addressing the limitations in existing benchmarks. The benchmark contains 1,245 fully compilable query-code pairs sourced from popular GitHub repositories, categorized into three groups based on dependency complexity. Unlike previous benchmarks that focus primarily on Python, CLARC systematically evaluates model C/C++ code search through multiple settings: standard code, neutralized identifiers (generic placeholders), randomized identifiers, and low-level representations (Assembly/WebAssembly). The authors developed an automated pipeline using LLMs to generate natural language queries, validated through hypothesis testing against human expert annotations. The results demonstrate that current models perform poorly on low-level languages and lack robust understanding of code functionality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Fills gap for C/C++ retrieval**: First comprehensive C/C++ robustness benchmark addressing the field's Python bias with real-world compilable code\n2. **Systematic robustness evaluation**: Structured testing across identifier anonymization and compilation settings isolates semantic understanding from lexical pattern matching\n3. **Scalable automated methodology**: LLM-based query generation with statistical validation enables cost-effective benchmark expansion while reducing knowledge contamination"}, "weaknesses": {"value": "1. **Questionable prevalence of target language**\nThe paper claims C/C++ represents \"industrially prevalent languages,\" but this assertion lacks supporting evidence. While C/C++ has importance in systems programming, languages like Java, JavaScript, Python, and C# arguably have broader industrial adoption across web development, enterprise applications, and data science. The authors should justify why C/C++ specifically addresses an industrial need, or broaden their claim to acknowledge the more diverse landscape of industrially relevant languages.\n\n2. **Questionable use cases of benchmark setting**\nThe practical necessity of code retrieval across different abstraction levels is questionable. The anonymized identifier setting (func_a, var_b, etc.) represents an artificial scenario, which is contrasted by most professional developers that will follow naming conventions and write meaningful identifiers. Similarly, searching Assembly or WebAssembly code has limited real-world utility, as most developers work exclusively in high-level languages and rely on compilers for low-level translation.\n\n3. **Limited Dataset Scale and Missing Training Components**\nDespite introducing an automated pipeline, the benchmark contains only 1,245 query-code pairs, which is relatively small for modern machine learning evaluation. More critically, while the authors demonstrate automated dataset construction capabilities, they provide no training set. Given the automated nature of their pipeline, supplying training data would significantly enhance the benchmark's practical value for model development.\n\n4. **Insufficient Validation of LLM-Generated Queries**\nThe quality assessment of LLM-generated descriptions relies on only 125 samples per category, which may not be representative of the full dataset's quality. More fundamentally, LLM-generated queries tend toward stylistic homogeneity, lacking the diversity found in real-world project descriptions and developer queries. This limitation reduces the benchmark's ecological validity and may not adequately test model robustness against natural query variation.\n\n5. **Poor Results Presentation**\nThe experimental results are poorly organized across Tables 3-5, requiring excessive cross-referencing to compare model performance across different settings. A consolidated presentation showing all experimental conditions for each model/group combination would significantly improve readability and facilitate comparative analysis."}, "questions": {"value": "While the work addresses the gap (lacking C/C++) in code search evaluation, the authors should reconsider the practical relevance of their chosen scenarios and significantly expand both the dataset size and validation methodology."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vh4cv7kGIW", "forum": "oO6D0whLDo", "replyto": "oO6D0whLDo", "signatures": ["ICLR.cc/2026/Conference/Submission198/Reviewer_eTqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission198/Reviewer_eTqq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630261278, "cdate": 1761630261278, "tmdate": 1762915468136, "mdate": 1762915468136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CLARC, a new benchmark for code search focused on C/C++. CLARC includes 1,245 compilable query-code pairs divided into groups of varying dependency complexity. It also provides several robustness settings, such as anonymized identifiers, randomized names, and compilation to Assembly or WebAssembly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is solid and relevant. Focusing on C/C++ fills a clear gap in existing benchmarks. The dataset being fully compilable enhances reproducibility. The robustness settings are well-designed and informative. The automated data generation pipeline with statistical validation is technically sound. Experimental coverage is broad and clearly demonstrates the weakness of current models."}, "weaknesses": {"value": "1. The dataset is small compared to existing large benchmarks (e.g., CodeSearchNet). \n2. The contribution is mainly engineering rather than conceptual. \n3. The analysis of why performance drops is shallow, with little insight into model behavior or representation. \n4. The LLM-generated query validation focuses on surface quality rather than semantic fidelity. \n5. The paper reads more like a dataset report than a research study, and the novelty is limited. Writing is clear but somewhat lengthy and repetitive."}, "questions": {"value": "1. Have the authors tried fine-tuning models on CLARC to test whether robustness can be learned?\n2. Can the anonymization and randomization pipelines be extended to other programming languages?\n3. Is there any observed correlation between code complexity (e.g., cyclomatic complexity) and robustness degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LCG0I63mTh", "forum": "oO6D0whLDo", "replyto": "oO6D0whLDo", "signatures": ["ICLR.cc/2026/Conference/Submission198/Reviewer_mRpN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission198/Reviewer_mRpN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917085034, "cdate": 1761917085034, "tmdate": 1762915467958, "mdate": 1762915467958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLARC (C/C++ LAnguage Retrieval with Anonymized Code), a benchmark designed to evaluate the robustness of code search models on C/C++ code. The dataset consists of 1,245 query-code pairs and is categorized based on the complexity of the code dependencies. CLARC includes configurable settings like anonymized identifiers and low-level code representations (Assembly and WebAssembly) to test models across varying abstraction levels. The authors evaluate six state-of-the-art code search methods and demonstrate significant performance degradation when identifiers are anonymized or code is compiled into lower-level languages. Furthermore, the paper introduces an automated pipeline for scalable benchmark generation, making the dataset reusable and extensible for future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CLARC is a **novel benchmark** specifically for C/C++ code search, with unique settings for anonymized identifiers and low-level languages.\n2. The experimental design is thorough, with a wide range of models tested across different evaluation settings. The inclusion of low-level language scenarios is a valuable addition.\n3. The paper is mostly clear and well-structured, with good use of figures and tables to explain the methodology and results.\n4. The work addresses a gap in code search research, particularly for industrial programming languages like C/C++, and the automated pipeline for benchmark generation has broad potential for future research."}, "weaknesses": {"value": "1. While CLARC is comprehensive, the authors could explore **more complex real-world codebases** to further ensure the dataset's robustness.\n2. While the automated benchmark generation pipeline is promising, the paper could offer a more in-depth discussion of how well this approach can scale to other programming languages or larger codebases.\n3. The evaluation of low-level languages (Assembly, WebAssembly) is insightful but could benefit from a deeper analysis of why models struggle with such code (e.g., complexity of instruction sets, abstraction loss)."}, "questions": {"value": "1. Could the authors provide more details on the **scalability** of the automated benchmark generation pipeline, particularly when extended to other languages beyond C/C++?\n2. The paper discusses performance degradation when identifiers are anonymized. Would the authors consider testing **semantic-based anonymization** (e.g., anonymizing function names based on their role) to better assess the models' understanding of code semantics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bpDBrXCnNb", "forum": "oO6D0whLDo", "replyto": "oO6D0whLDo", "signatures": ["ICLR.cc/2026/Conference/Submission198/Reviewer_abmH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission198/Reviewer_abmH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328917071, "cdate": 1762328917071, "tmdate": 1762915467839, "mdate": 1762915467839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}