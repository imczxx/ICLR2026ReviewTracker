{"id": "eJ8p42r755", "number": 17566, "cdate": 1758277632528, "mdate": 1759897167377, "content": {"title": "Cross Lingual Long-tailed Entity Alignment in Knowledge Graphs", "abstract": "Entity alignment (EA) models rely mostly on triples and structural information of Knowledge Graphs (KGs), but underperform on sparsely connected long-tailed entities. We address this gap by proposing a model, \\textbf{ContrastEA}, that leverages pre-trained Language Models (LM), e.g. me5, to generate entity representations, followed by a novel contrastive learning approach that incorporates hard-negative mining strategies with \\textit{top-k} negatives per entity, alongside NT-Xent loss to separate challenging entity pairs. In addition, to address the under-representation of long-tailed entities in benchmark datasets, we curate a new dataset from DBpedia comprising long-tailed entities per language -- Arabic, German, Portuguese, Italian, Hindi, Russian, and Japanese, each aligned to English (a total of 154,296 cross-lingual entity pairs). Our results demonstrate that ContrastEA outperforms the classic EA models on three benchmark datasets, improving Hits@1 by 6--20 percentage points, and achieves SOTA on the curated dataset over the long-tailed EA models.", "tldr": "Contrastive learning based cross lingual entity alignment", "keywords": ["entity alignment", "knowledge graphs", "long-tailed entities", "contrastive learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3782fc6a3af61a4cf98d0c0fee7f129ec20c1d5d.pdf", "supplementary_material": "/attachment/e746959fb01e9c41fd4e9c00b00a33a94442fdc6.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the long-tailed entity alignment (EA) problem. The authors propose ContrastEA, a novel approach that leverages pre-trained language models (e.g., mBERT) to generate entity representations, combined with a contrastive learning framework incorporating hard negative mining by selecting top-k negatives per entity and using the NT-Xent loss to better distinguish difficult entity pairs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a novel dataset, which could be highly beneficial for the advancement of the field.\n\n2. The writing is generally clear and coherent.\n\n3. The paper addresses a novel problem and provides a preliminary solution."}, "weaknesses": {"value": "1. The key techniques presented in the paper, like contrastive learning and hard sampling, have already been proposed in previous works like BootEA, Dual-AMN, and Lambda. \n\n2. Aligning sparsely connected nodes is inherently challenging since there are no reference pre-aligned nodes to rely on; therefore, the alignment largely depends on leveraging the general knowledge encoded in large language models to enhance the initial entity representations. This approach is quite conventional and lacks novelty.\n\n3. This paper can be regarded primarily as an engineering implementation, with its sole contribution being the introduction of a new dataset. Moreover, as the field evolves, knowledge graph technologies are increasingly being supplanted by large language models, which limits the practical applicability of the authors’ work.\n\n4. The experimental results in the paper are disorganized and lack proper consolidation, indicating that the current version is far from being ready for publication."}, "questions": {"value": "Refer to weaknesses. The authors are encouraged to provide effective responses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4XxUpcb4P0", "forum": "eJ8p42r755", "replyto": "eJ8p42r755", "signatures": ["ICLR.cc/2026/Conference/Submission17566/Reviewer_a7nD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17566/Reviewer_a7nD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760945382561, "cdate": 1760945382561, "tmdate": 1762927428166, "mdate": 1762927428166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ContrastEA, a novel contrastive learning-based model for cross-lingual entity alignment (EA) in knowledge graphs (KGs), with a particular focus on the long-tailed entities—those with sparse or no structural connections. The authors also construct a new multilingual benchmark dataset, LT-EA-25K, to better reflect real-world KG sparsity across seven languages. Experiments on several datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed LT-EA-25K dataset is a significant contribution, covering Arabic, German, Portuguese, Italian, Hindi, Russian, and Japanese aligned to English. It includes both long-tailed and dangling entities, which are often ignored in existing benchmarks."}, "weaknesses": {"value": "- From my perspective, this paper does not present significant technical innovation. The use of negative sampling and contrastive learning frameworks has already been explored in prior work.\n\n- Regarding the motivation, the paper focuses on long-tailed entities, yet the proposed method does not seem to directly address the core challenge, i.e., the lack of relational triples for long-tailed entities. Although the method incorporates name-based features, in my experience, name alone can resolve most entity alignment cases. In fact, the experimental results show strong performance on long-tailed datasets, but it remains unclear how much of that success is attributable to the name feature. Overall, there is a disconnect between the method and its stated motivation, and the key contributing module behind the experimental gains is not clearly identified."}, "questions": {"value": "- What about the performance of the method that only uses name embeddings for entity alignment on your dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "46S3RQvN2P", "forum": "eJ8p42r755", "replyto": "eJ8p42r755", "signatures": ["ICLR.cc/2026/Conference/Submission17566/Reviewer_oU9k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17566/Reviewer_oU9k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536511144, "cdate": 1761536511144, "tmdate": 1762927427793, "mdate": 1762927427793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ContrastEA, a contrastive-learning-based approach for cross-lingual long-tailed entity alignment that relies purely on entity names encoded by a multilingual sentence encoder. The method uses top-k hard negative mining and the NT-Xent loss to enhance discrimination, aiming to tackle alignment under long-tailed and dangling-entity conditions where structural or relational signals are sparse.\nIn addition, the authors construct LT-EA-25K, a new benchmark covering 7 language pairs with realistic long-tailed degree distributions and dangling entities. Experiments show that ContrastEA achieves strong or state-of-the-art results over a structure-based baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments and analysis demonstrate that relying solely on entity names circumvents structural sparsity in long-tailed KGs while still delivering competitive alignment accuracy.\n\n2. The top-k strategy under NT-Xent is intuitive and empirically useful for emphasizing informative negatives.\n\n3. LT-EA-25K provides a much-needed long-tailed, multilingual dataset including dangling entities, making it valuable to the community."}, "weaknesses": {"value": "1. As ContrastEA just utilizes the entity name for EA, the paper needs to deeply analyze failure modes such as homonyms, transliteration mismatches, or domain-specific ambiguities like same names referring to different entities. It would be better if you could quantify the limitations and provide results for same-name collisions and one-sentence description augmentation.\n\n2. Lack of sensitivity to k, τ, and batch size is not explored. All negatives are selected within the mini-batch. This can lead to false negatives and heavy dependence on batch size.\n\n3. Reported numbers lack standard deviations or significance testing. Without error bars or paired t-tests, it is unclear whether improvements are statistically meaningful.\n\n4. It seems this paper misses preprocessing details. Name normalization strongly affects name-only alignment but is not described. It would be better if you could provide them.\n\n5. Although the paper has claimed efficiency, the paper reports no throughput or GPU memory statistics. Top-k hard-negative mining scales as O(batch²). I suggest adding runtime and memory benchmarks."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vncg7M8l5I", "forum": "eJ8p42r755", "replyto": "eJ8p42r755", "signatures": ["ICLR.cc/2026/Conference/Submission17566/Reviewer_qm2m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17566/Reviewer_qm2m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834836163, "cdate": 1761834836163, "tmdate": 1762927427069, "mdate": 1762927427069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of aligning sparsely connected long-tailed entities in cross-lingual knowledge graphs (KGs), where traditional entity alignment (EA) models underperform due to insufficient structural and neighbourhood information. The authors propose ContrastEA, a novel model that leverages pre-trained multilingual language models (e.g., mE5) to generate entity representations and incorporates a contrastive learning framework with hard-negative mining (top-k negatives per entity) and NT-Xent loss to distinguish similar entities. Additionally, to address the under-representation of long-tailed entities in existing benchmarks, the authors curate a new dataset LT-EA-25K from DBpedia, covering seven languages (Arabic, German, Portuguese, Italian, Hindi, Russian, Japanese) aligned to English, including 154,296 cross-lingual entity pairs and dangling entities (zero connections). Experimental results show that ContrastEA outperforms classic and long-tailed EA models on three benchmark datasets (DBP15K, SRPRS, DBP5L) by improving Hits@1 by 6–20 percentage points and achieves state-of-the-art (SOTA) performance on LT-EA-25K. The paper’s key contributions include the ContrastEA model, the LT-EA-25K dataset, and empirical validation of robustness across entities with varying connectivity and diverse language pairs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work identifies a critical gap in existing EA research—neglect of long-tailed and dangling entities in real-world KGs—and proposes a targeted solution that abandons reliance on structural/relational information, a bold and effective design choice given the sparsity of such entities.\n2. ContrastEA demonstrates strong originality by combining pre-trained multilingual LMs with hard-negative mining and NT-Xent loss; the top-k hard negative strategy addresses the limitation of uninformative negative samples in prior contrastive learning for EA, enhancing the model’s ability to distinguish deceptive similar entities.\n3. The curation of LT-EA-25K fills a crucial gap in benchmark datasets, as it reflects real-world KG sparsity (including entities with degree ≤3 and dangling entities) and covers low-resource languages (e.g., Hindi, Arabic), making it a valuable resource for future long-tailed EA research."}, "weaknesses": {"value": "1. The model relies solely on entity names for representation, ignoring other potentially useful textual features (e.g., short entity descriptions or attributes) that may exist for some long-tailed entities. For entities with ambiguous names (e.g., homonyms across languages), integrating such supplementary information could further improve alignment accuracy, especially for low-resource languages with sparse name data.\n2. The paper does not explore the impact of different pre-trained LMs on model performance.\n3. The LT-EA-25K dataset’s construction process, while detailed, does not include a quantitative analysis of label quality. For example, how were the owl:sameAs links verified for low-resource languages with limited annotation resources? Additionally, the dataset’s generalizability to non-DBpedia KGs (e.g., Wikidata, YAGO) is not evaluated, as experiments are limited to DBpedia-derived benchmarks."}, "questions": {"value": "1. How does the model handle cases where the top-k hard negatives include entities from the same semantic category as the anchor (e.g., two different \"Paris\" entities in different languages)? Does the current framework effectively distinguish such fine-grained semantic differences, or could a category-aware negative sampling approach further improve performance?\n2. What proportion of entities in each language pair are true long-tailed (degree ≤3) versus dangling (degree=0)? How does ContrastEA’s performance vary specifically on dangling entities compared to entities with 1–3 connections? Providing disaggregated results would clarify the model’s ability to handle the most extreme sparsity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iF6rgPcc25", "forum": "eJ8p42r755", "replyto": "eJ8p42r755", "signatures": ["ICLR.cc/2026/Conference/Submission17566/Reviewer_oYhE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17566/Reviewer_oYhE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903093968, "cdate": 1761903093968, "tmdate": 1762927426662, "mdate": 1762927426662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}