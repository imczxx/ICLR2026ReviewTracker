{"id": "4pS3L3wtHp", "number": 3676, "cdate": 1757495633807, "mdate": 1759898075544, "content": {"title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "abstract": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain (E$^2$C), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT)—augmented by a novel data generation algorithm enforcing strict plan adherence—with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME’2024, E$^2$C Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution.", "tldr": "We propose E$^2$C, a two-phase reasoning paradigm that decouples exploration from execution, delivering higher efficiency, stronger generalization, and improved interpretability.", "keywords": ["Chain of Thought", "Test Time Scaling", "LLM Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8a437e2066d40fe2d3506c221d18043d1d5f5e3d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the Explore-Execute Chain (E2C), a reasoning framework that separates planning from execution in large language models. In the exploration phase, the model generates concise, high-level plans; in the execution phase, it deterministically follows the chosen plan. This decoupling improves efficiency, interpretability, and generalization. E2C is trained using a two-stage process combining supervised fine-tuning with a specialized causal data generation method and reinforcement learning to enforce plan adherence and reduce execution variance. Experiments on math and medical benchmarks show that E2C outperforms baselines while using significantly fewer tokens, particularly excelling in test-time scaling and cross-domain adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is technically rigorous, combining a well-designed causal data generation process, SFT, RL with token-level control to ensure plan adherence and deterministic reasoning. \n- The experiments are also comprehensive, spanning mathematical and medical benchmarks, and convincingly demonstrate efficiency gains and cross-domain adaptability.\n- The writing is clear and well-structured, with intuitive motivation, formalism, and experimental validation."}, "weaknesses": {"value": "- I think the biggest issue is the novelty and significance. In fact, searching for high-level plans before acting has been extensively explored recently, e.g., PlanSearch [1]. The use of RL does not make significant addition to this test-time computation scaling paradigm.\n- The proposed framework is not flexible enough. The authors argue that the monolithic, auto-regressive generation process (CoT) \"leads to critical inefficiencies\". In fact, CoT uses the intermediate steps as a sketch pad to improve the reasoning performance. The paper completely decouple the high-level planning and low-level execution, which prohibits many established methods built upon CoT, e.g., ReAct, Reflexion, etc.\n- A further concern is that the paper introduces a relatively heavy framework to achieve what might be modest empirical gains. The explicit exploration–execution separation, along with the additional training stages and reward mechanisms, seems to add considerable complexity to both training and inference.\n\n\n[1] Planning In Natural Language Improves LLM Search For Code Generation"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lrkvgegj0v", "forum": "4pS3L3wtHp", "replyto": "4pS3L3wtHp", "signatures": ["ICLR.cc/2026/Conference/Submission3676/Reviewer_qDxm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3676/Reviewer_qDxm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636997410, "cdate": 1761636997410, "tmdate": 1762916913124, "mdate": 1762916913124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Explore–Execute Chain (E2C) framework, which separates reasoning into an exploratory phase for generating high-level plans and an execution phase for detailed reasoning. Using a two-stage training process with supervised fine-tuning and reinforcement learning, E2C aims to improve efficiency and interpretability. Experiments on math and medical reasoning benchmarks show improvements in accuracy with reduced computational cost and efficient domain adaptation through the Exploration-Focused SFT method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly presented and well organized. \n\n- The experiments are comprehensive, and the authors provide implementation details and intermediate results. \n\n- The advantage in improving efficiency is clear."}, "weaknesses": {"value": "- The proposed plan–then–execute paradigm raises concerns regarding its effectiveness. In our experience, prompting a model to first produce a plan and then execute it often yields worse results than directly applying CoT prompting. This limitation stems from the fact that the model must generate a plan based solely on the query, without access to any intermediate reasoning results, which may hinder its ability to form accurate or adaptive strategies.\n\n- According to the official report [1] (Table 18), simply extending the CoT length allows Qwen3-8B to achieve 76.0 on AIME’24 and 67.3 on AIME’25, substantially higher than the 40.6 and 33.8 reported by the proposed method despite its added complexity, including additional training stages, hyperparameter tuning, and structural modifications. This comparison raises questions about the significance of the proposed approach relative to simpler alternatives.\n\n- The proposed E2C-RL algorithm is quite similar to Hierarchy-Aware Credit Assignment [2], differing mainly in placing all planning tokens at the beginning, which limits its novelty.\n\n- The domain adaptation experiment setup seems unfair. The standard SFT baseline is fine-tuned solely on medical data, whereas the proposed method uses both math and medical data. A fairer comparison is to involve math data in standard SFT baseline as well.\n\n- Code and data are not provided for review.\n\n\n## Reference:\n[1] Yang, An, et al. \"Qwen3 technical report.\" arXiv preprint arXiv:2505.09388 (2025).\n\n[2] Wang, Haozhe, et al. \"Emergent hierarchical reasoning in llms through reinforcement learning.\" arXiv preprint arXiv:2509.03646 (2025)."}, "questions": {"value": "- Could the authors include results for standard SFT baselines trained on both math and medical data to enable a fairer comparison with the proposed EF-SFT method?\n\n- Could the authors clarify the notations used in Section 3.2.2? For example, what do the indices $i$ and $t$ represent, how are $T_{\\text{exp}}$ and $T_{\\text{exe}}$ determined, and what does $H$ denote in Equation (7)?\n\n- Could the authors provide the code and data for review?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UZSuCMT8ZU", "forum": "4pS3L3wtHp", "replyto": "4pS3L3wtHp", "signatures": ["ICLR.cc/2026/Conference/Submission3676/Reviewer_AJeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3676/Reviewer_AJeZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931206125, "cdate": 1761931206125, "tmdate": 1762916912924, "mdate": 1762916912924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Explore–Execute Chain (E2C) framework, which decomposes reasoning into two phases: an exploratory stage that generates high-level plans and an execution stage that deterministically follows these plans. The method is trained through a two-stage process combining supervised fine-tuning and reinforcement learning. Experiments on mathematical and medical reasoning benchmarks show that E2C improves efficiency and maintains competitive accuracy compared to existing structured reasoning approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear and organized framework that separates reasoning into exploration and execution phases. The experiments are reasonably comprehensive and the writing is clear. The methodology is well presented."}, "weaknesses": {"value": "The core idea of strictly separating planning and execution lacks sufficient justification, as it prevents the model from adapting its plan mid-way, which is often crucial for complex reasoning tasks. In addition, generating effective plans without preliminary computation remains challenging. The proposed GRPO-based training closely resembles existing work [1], which limits the novelty of the methodology.\n \n[1] Wang, Haozhe, et al. \"Emergent hierarchical reasoning in llms through reinforcement learning.\" arXiv preprint arXiv:2509.03646 (2025)."}, "questions": {"value": "• The proposed framework enforces a strict separation between planning and execution, which prevents the model from adjusting its plan mid-way. This could potentially degrade performance on complex tasks where adaptive planning is necessary. How do the authors address or mitigate this limitation?\n\n• How effective are the generated plans in practice? For challenging mathematical problems, forming a correct plan often requires partial calculations or intermediate reasoning. How does the model ensure that plans remain useful without preliminary computation?\n\n• Since the plan omits details and merely serves as part of the context, there is no guarantee that the execution phase will fully adhere to it. How do the authors ensure consistency and faithful plan adherence during execution?\n\n• Could the authors clarify what are the exploration and exploitation tokens in their RL formulation? Are these sets equivalent to the plan tokens $\\pi$ and execution tokens $e$, respectively?\n\n• How robust is the clustering-based selection method at test time? Given that small semantic differences may not significantly affect embedding similarity, how do the authors ensure that clustering reliably distinguishes meaningfully different plans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVBvdGuspz", "forum": "4pS3L3wtHp", "replyto": "4pS3L3wtHp", "signatures": ["ICLR.cc/2026/Conference/Submission3676/Reviewer_A4xv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3676/Reviewer_A4xv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957633836, "cdate": 1761957633836, "tmdate": 1762916911514, "mdate": 1762916911514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Explore–Execute Chain (E2C), a reasoning framework that separates planning (exploration) from execution in large language models. With a two-stage SFT + RL training method, E2C achieves higher efficiency and interpretability, reaching 58.1% accuracy on AIME’2024 with under 10% of the tokens and enabling data-efficient domain adaptation across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces the Explore–Execute Chain (E2C), a novel framework that separates high-level planning from detailed execution to enhance reasoning efficiency and interpretability. Through a carefully designed two-stage SFT+RL training pipeline and a data-efficient adaptation method (EF-SFT), E2C achieves state-of-the-art results with far fewer tokens, demonstrating strong generalization, scalability, and clear methodological innovation."}, "weaknesses": {"value": "1. The idea of separating reasoning into “plan” and “execute” phases is not very novel, as similar frameworks have been explored in prior works such as Planning in Natural Language Improves LLM Search for Code Generation. The main contribution lies in applying RL to this paradigm, which feels incremental.\n\n2. The performance improvements over strong baselines (e.g., GRPO) are relatively small—around 1–2% in Tables 1 and 2—making it hard to clearly demonstrate the benefits of the proposed approach.\n\n3. The baseline comparison is limited. Important RL-based methods such as DAPO and RLOO are missing, which weakens the empirical validation.\n\n4. While the paper claims efficiency, the RL training pipeline appears complex and resource-intensive. The actual training cost versus GRPO or standard SFT is not quantified."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ge9DHt9JMz", "forum": "4pS3L3wtHp", "replyto": "4pS3L3wtHp", "signatures": ["ICLR.cc/2026/Conference/Submission3676/Reviewer_Zd2Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3676/Reviewer_Zd2Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3676/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989184873, "cdate": 1761989184873, "tmdate": 1762916911083, "mdate": 1762916911083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}