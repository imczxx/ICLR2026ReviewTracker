{"id": "BigoLH4PsK", "number": 11361, "cdate": 1758197357519, "mdate": 1759897579964, "content": {"title": "Vision Hopfield Memory Networks", "abstract": "Recent vision and multimodal foundation backbones, such as Transformer families and state-space models like Mamba, have achieved remarkable progress, enabling unified modeling across images, text, and beyond. Despite their empirical success, these architectures remain far from the computational principles of the human brain, often demanding enormous amounts of training data while\noffering limited interpretability. In this work, we propose the Vision Hopfield Memory Network (V-HMN), a brain-inspired foundation backbone that integrates hierarchical memory mechanisms with iterative refinement updates. Specifically, V-HMN incorporates local Hopfield modules that provide associative memory dynamics at the image patch level, global Hopfield modules that function as episodic\nmemory for contextual modulation, and a predictive-coding–inspired refinement rule for iterative error correction. By organizing these memory-based modules hierarchically, V-HMN captures both local and global dynamics in a unified framework. Memory retrieval exposes the relationship between inputs and stored patterns, making decisions more interpretable, while the reuse of stored patterns improves data efficiency. This brain-inspired design therefore enhances interpretability and data efficiency beyond existing self-attention- or state-space–based approaches. We conducted extensive experiments on public computer vision benchmarks, and V-HMN achieved competitive results against widely adopted backbone architectures, while offering better interpretability, higher data efficency, and stronger biological plausibility. These findings highlight the potential of V-HMN to serve as a next-generation vision foundation model, while also providing a generalizable blueprint for multimodal backbones in domains such as text and audio, thereby bridging brain-inspired computation with large-scale machine learning.", "tldr": "", "keywords": ["Associative Memory", "Hopfield Networks", "Image Classification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74dbfb346fb092ac61978e15c6f9bda7bf165a1c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Vision Hopfield Memory Networks, a biologically inspired vision backbone that replaces standard token-mixing operations (like convolution or self-attention) with hierarchical Hopfield memory mechanisms. The model integrates two complementary modules: 1) Local Hopfield memory for patch-level associative retrieval and denoising, and 2) Global Hopfield memory for scene-level context and episodic modulation. Both modules interact through a predictive-coding–inspired iterative refinement rule, allowing representations to be gradually corrected based on stored prototypes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation is very well defined and strong. I think this paper proposes an approach than can deal with fundamental problems in modern AI architectures such as equipping models with associative retrieval and predictive coding capabilities while both being neuro-inspired.\n* Retrieved prototypes expose which stored patterns influence decisions which increases interpretability, a rare feature in vision backbones.\n* Achieves strong results with as little as 10–30% of labeled data.\n* The paper presents ablation studies.\n* Results are strong even though on toy datasets.\n* The paper presents qualitative results."}, "weaknesses": {"value": "* While the predictive-coding analogy is conceptually appealing it remains lightweight where the connection could be deepened theoretically or experimentally.\n* While the paper presents an appealing biologically inspired narrative, the underlying model is fairly simple, essentially a combination of local/global prototype retrieval and residual refinement. The connection to predictive coding and Hopfield dynamics is more analogical than formal, and the theoretical depth is limited. Nevertheless, the simplicity is also a strength: it demonstrates that interpretable, memory-based inductive biases can yield competitive results without architectural complexity even though on \"toy\" datasets.\n* The experiments are too much toy. Empirical results on real world datasets or tasks could be done.\n* The gains from increasing the number of refinement steps are minimal which can mean that the predictive coding strategy is not bringing much value.\n\nMinor comments:\n* Too many acronyms\n* ViT acronym is repeated"}, "questions": {"value": "* Why didn't you use learnable projection layers in the HMN block to increase expressiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not identify any significant ethical issues in this paper. The method operates on publicly available video datasets commonly used in the community, and there is no indication of privacy violations, harmful content generation, or misuse potential beyond standard concerns in visual understanding research. Therefore, I do not see any ethical concerns requiring further attention."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WpP4y7iwzn", "forum": "BigoLH4PsK", "replyto": "BigoLH4PsK", "signatures": ["ICLR.cc/2026/Conference/Submission11361/Reviewer_qwXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11361/Reviewer_qwXq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039546664, "cdate": 1761039546664, "tmdate": 1762922493192, "mdate": 1762922493192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new type of vision foundational model using Hopfield associative memory mechanism as a building block. The basic idea is to have local patch neighborhoods encoded and stored in memory banks at the local level and to cover global characteristics another encodings derived from pooling across all the patches in a global memory bank. As new training images are added, the Hopfield update operations are performed in both banks to update the prototypical encodings stored in the respective memory bank. Each such Hopfield memory-laden block can be stacked as layers followed by an attentional pooling step to give the final encoded representation which is then fed to a classifier. \n\nExperiments are conducted on a few benchmark datasets and comparison is performed to alternative vision foundational model paradigms based on ViT, VisionMamba and other architectures, and adequate ablation studies are done to expose the features. Overall, the model seems to have comparable number of parameters to the SOTA models, and the main advantage appears to be having biological motivation and fairly large improvement in classification performance with lesser training data."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The use of Hopfield retrieval circuits inside a vision foundational model is novel and very interesting. Effectively the operation is akin to local  and global clustering of patterns in encoding spaces derived originally from the image. The results are impressive as well."}, "weaknesses": {"value": "It would be good to discuss the biological plausibility of the proposed mechanism, since the Hopfield networks are found later in the pathway within the Hippocampal system and take input from the entorhinal cortex by which time the image was already analyzed in the visual pathway and via the parahippocampal and perihinal cortex, the object location and identities have already been completed. So while this is a new and interesting vision model,  I am not convinced is reflective of what is going in the brain. \n\nThe hierarchical aspects of the model need further elaboration as is the rotational and other invariances in the model where the associative recall may not address those aspects."}, "questions": {"value": "Hopfield networks are notorious for entering the metastable states. How does that effect the recognition capacity of the model. It would also be good to see if it is able to discriminate between very similar patterns using this mechanism. \n\nIf the number of parameters are about the same, is the main advantage of the model the improved accuracy (for which we should probably look at more datasets) or the amount of training data required. If more data is diluting the system due to the metastable states, then this may pose a limitation for this model which should be discussed.\n\nIs the code for this being made available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HopYGKumyt", "forum": "BigoLH4PsK", "replyto": "BigoLH4PsK", "signatures": ["ICLR.cc/2026/Conference/Submission11361/Reviewer_7PFy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11361/Reviewer_7PFy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798179782, "cdate": 1761798179782, "tmdate": 1762922492495, "mdate": 1762922492495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an application of modern Hopfield networks (MHNs) to the domain of vision and classification.\nParticularly, the application of MHNs to this domain enhances the known advantages of MHNs, like interpretability, data efficiency and biological plausibility. The proposed V-MHN architecture is extensively compared against existing architectures. \nEmpirically, V-HMN achieves strong results on data efficiency and demonstrates that memory-centric design can be a viable alternative to self-attention–based architectures in low data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel architecture that is memory centric, which is a very interesting design philosophy that should be studied more in the design of interpretable neural networks.\n\nEmpirically, this architecture is competitive with strong prior baselines. Specifically, it outperforms prior work on data efficiency. The paper is well motivated. \n\nInterpretability through explicit memory retrieval. The visualizations in Figure 2 are insightful and show that retrieved prototypes align well with semantic structure."}, "weaknesses": {"value": "While the empirical results look strong for data efficiency, it is unclear why this is the case. An analysis or a dicsussion section on why this architecture is more data efficient than prior work would be very helpful.\n\nTables 3 and 4 are steps in the right direction to aid this understanding, but it appears that neither memory size nor number of steps on iterative refinement have an effect on the performance. The natural question that arises in that case is why is this architecture better than prior work.\n\nThe predictive-coding analogy is appealing but somewhat superficial. The update rule is effectively a weighted average with a learned coefficient. It is also unclear why the number of iterations is relevant if the coefficient is learned. The connection to biological predictive coding should be tempered or better substantiated.\n\nThe interpretability claim could be better quantified, although this is a minor weakness, as this is a direct consequence of the hopfield architecture.\n\nMinor comments:  efficiency is misspelled as efficency in the abstract."}, "questions": {"value": "On table 3:\n1) Does zero iterations mean that iterative refinement is not done? In this case, is the memory bank unused? Why does the lack of a iterative refinement only marginally hurt the results?\n2) Do you fix $\\beta$ for the purpose of this part? if not, for iterations 1,2,3, wont the learning process of $\\beta$ affect the results?\n3) Would it be informative to repeat Table 3 using a small data fraction (e.g., 10\\%) to highlight whether iterative refinement contributes most under data scarcity?\n\nOn table 4:\n4. The results seem largely insensitive to memory size. Could you report retrieval hit-rate or memory-slot utilization to show that the memory is actively used?\n5. I wonder how performance would change due to class imbalance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iBXCI4E3Y0", "forum": "BigoLH4PsK", "replyto": "BigoLH4PsK", "signatures": ["ICLR.cc/2026/Conference/Submission11361/Reviewer_e5Z8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11361/Reviewer_e5Z8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803965776, "cdate": 1761803965776, "tmdate": 1762922491906, "mdate": 1762922491906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Vision Hopfield Memory Network (V-HMN), a biologically inspired foundation model that replaces self-attention and convolutional mixing with local and global Hopfield memory modules. Each block retrieves prototypes from memory banks (class-balanced, written during training) and performs iterative refinement akin to predictive-coding error correction. V-HMN achieves competitive results on CIFAR-10/100, SVHN, and Fashion-MNIST, with claimed advantages in data efficiency and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work demonstrates the benefit of a memory based system in perceptual learning, bridging the gap between memory / prototype based system and distributed system. This shows a better alignment with brain theory and practically improves the data efficiency and interpretability in perceptual models.\n- The work shows comprehensive ablation studying in various functions and roles of hyperparameters. \n- Presentation is clear and concise."}, "weaknesses": {"value": "- All benchmarks are small-scale (≤ 32×32 images). Claims about “foundation backbone” or “multimodal generalizability” are not validated on large datasets (ImageNet, ADE20K, etc.). Maybe add a small data with slightly large data resolution. \n- The idea of using memory banks to improve the data efficiency is not new (E.g. [1] in few shot image generation – to test the limits of data efficiency). The author should consider a more comprehensive background review in terms of the computational benefits of the prototype memory.\n- The paper only demonstrates the benefits of the memory based system on classification but it would be more beneficial to see if the method would generalize to other types of tasks. But it’s a promising direction that should be shared with the community. \n[1] Li, Tianqin, et al. \"Prototype memory and attention mechanisms for few shot image generation.\" International Conference on Learning Representations. 2022."}, "questions": {"value": "For the iterative refinement, does it actually change different retrieved prototype in each iteration? Whether it will actually help to retrieve more related prototypes (i.e. broader sense of pattern completion to bring stronger prior to it)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I3HIRsei2p", "forum": "BigoLH4PsK", "replyto": "BigoLH4PsK", "signatures": ["ICLR.cc/2026/Conference/Submission11361/Reviewer_FEDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11361/Reviewer_FEDf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051422506, "cdate": 1762051422506, "tmdate": 1762922491334, "mdate": 1762922491334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}