{"id": "rnrENwgDsn", "number": 15052, "cdate": 1758247232161, "mdate": 1763623486174, "content": {"title": "Toward Neural Streaming Scheduling: A Memory-Augmented Reinforcement Learning Model with Critical Structure Encoding", "abstract": "Many large-scale data analytics and AI systems execute jobs structured as Directed Acyclic Graphs (DAGs), which encode precedence constraints among interdependent stages. Efficient DAG scheduling is crucial for maximizing system throughput, especially in streaming settings where diverse jobs arrive continuously and require real-time decisions. Despite progress by heuristic and learning-based scheduling methods, capturing execution-critical structures and leveraging historical scheduling context remain key challenges. Building on this motivation, we propose MACE, a Memory-Augmented reinforcement learning model with Critical structure Encoding, which implements a scheduling policy for streaming jobs by sequentially selecting runnable stages and assigning parallelism based on cluster state. The policy is trained to minimize average job completion time using defined rewards. Specifically, MACE consists of two core components: (i) CSformer builds hierarchical embeddings that integrate stage-job-global information, capturing execution-critical structures through critical-path-aware positional encodings and a tunable attention field. This design guides the policy toward latency-sensitive and structurally related stages. (ii) A memory-augmented scheduler then uses the learned embeddings and a job memory to exploit historical contexts for the final stage and parallelism selection. Extensive experiments on Spark using the TPC-H benchmark demonstrate that MACE outperforms state-of-the-art baselines by up to 9.38% under diverse workload conditions.", "tldr": "", "keywords": ["streaming scheduling", "dynamical systems", "sequential decision making", "critical structure encoding", "historical context modeling", "graph neural networks"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ab2b6ce96cc0c8404171411320b9947ae85ce07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MACE, a new reinforcement learning model designed to optimize streaming Directed Acyclic Graph (DAG) scheduling in systems like Spark by minimizing average job completion time. MACE's architecture features two core components: a CSformer encoder that uses \"critical-path-aware positional encodings\" to identify latency-sensitive stages, and a memory-augmented scheduler that queries a job memory to incorporate historical scheduling decisions and outcomes. Evaluated on the TPC-H benchmark, MACE demonstrated its effectiveness by outperforming state-of-the-art baselines by up to 9.38% across diverse workloads."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper clearly identifies two significant weaknesses in existing learning-based schedulers: the failure to explicitly model execution-critical structures like the critical path, and the inability to leverage historical scheduling contexts. It proposes MACE, a new architecture specifically designed to address both of these shortcomings.\n\n* The core components of MACE are well-motivated and their contributions are validated through ablation studies. The CSformer's use of critical-path-aware positional encodings directly targets the critical bottleneck issue, and removing this component causes the largest performance degradation. Similarly, the memory-augmented scheduler provides a concrete mechanism to learn from past decisions, and its removal also substantially hurts performance."}, "weaknesses": {"value": "* The paper's novelty is limited, as it does not introduce a new learning paradigm. It frames DAG scheduling as a reinforcement learning problem, a well-established approach from prior work like Decima. The core contributions are the CSformer and the memory module, which are essentially incremental architectural additions (graph positional encodings and a memory-augmented agent) rather than fundamental advances in either RL or scheduling.\n\n* A critical metric for a \"streaming\" scheduler, the decision-making latency, is relegated to the appendix. The main paper (pages 1-9) focuses exclusively on job completion time (JCT). Given that the MACE model is demonstrably more complex than its baselines, this overhead is a crucial part of the performance trade-off and its omission from the main experimental results is not acceptable. Please note that the reviewers are not required to read the appendix.\n\n* The empirical evaluation is not well-aligned with the ICLR audience. The experiments are conducted exclusively on Spark using the TPC-H benchmark, which is a traditional data analytics/database workload. The paper fails to evaluate its scheduler on any workloads more central to the modern AI community, such as scheduling for large-scale model training, complex training/data-processing pipelines, or large-batch inference.\n\n* The comparison against other learning-based schedulers is insufficient, as Decima is the only learning-based baseline included in the evaluation. While Decima is a seminal work in this area, the authors explicitly exclude other relevant and more recent learning-based schedulers, such as LACHESIS and DeepWeave. Justifying this exclusion by claiming \"incompatible problem formulation\"  is a weak defense and prevents a meaningful comparison of MACE against the current state-of-the-art in learning-based DAG scheduling.\n\n* The performance gains are modest and may not justify the added complexity. The paper claims \"up to 9.38%\" improvement, but in the main \"Default\" scenario in Table 1, the improvement over the primary learning baseline (Decima) is smaller (58.6 vs 63.1). This marginal gain is achieved by introducing a significantly more complex architecture involving a custom Transformer, a memory matrix, and complex read/write operations."}, "questions": {"value": "* The paper introduces significant architectural complexity (CSformer, a query-based memory matrix) for what appears to be a modest improvement over Decima in the default case (58.6s vs 63.1s). How do you justify this complexity-performance trade-off, especially given that the scheduling latency is a critical metric?\n\n* Speaking of latency, a crucial metric for any \"streaming\" scheduler is the per-decision overhead. Why was this analysis not included in the main results (e.g., in Table 1) but instead moved to the appendix?\n\n* The evaluation is performed exclusively on the TPC-H benchmark, a traditional data analytics/database workload. Given the ICLR audience, why was the MACE scheduler not evaluated on workloads more representative of modern AI systems, such as scheduling complex ML training pipelines or data-flow graphs for large model inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cAHIiXuXHf", "forum": "rnrENwgDsn", "replyto": "rnrENwgDsn", "signatures": ["ICLR.cc/2026/Conference/Submission15052/Reviewer_PpAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15052/Reviewer_PpAN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161203727, "cdate": 1761161203727, "tmdate": 1762925373581, "mdate": 1762925373581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MACE, a new reinforcement learning model designed to optimize streaming Directed Acyclic Graph (DAG) scheduling in systems like Spark by minimizing average job completion time. MACE's architecture features two core components: a CSformer encoder that uses \"critical-path-aware positional encodings\" to identify latency-sensitive stages, and a memory-augmented scheduler that queries a job memory to incorporate historical scheduling decisions and outcomes. Evaluated on the TPC-H benchmark, MACE demonstrated its effectiveness by outperforming state-of-the-art baselines by up to 9.38% across diverse workloads."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper clearly identifies two significant weaknesses in existing learning-based schedulers: the failure to explicitly model execution-critical structures like the critical path, and the inability to leverage historical scheduling contexts. It proposes MACE, a new architecture specifically designed to address both of these shortcomings.\n\n* The core components of MACE are well-motivated and their contributions are validated through ablation studies. The CSformer's use of critical-path-aware positional encodings directly targets the critical bottleneck issue, and removing this component causes the largest performance degradation. Similarly, the memory-augmented scheduler provides a concrete mechanism to learn from past decisions, and its removal also substantially hurts performance."}, "weaknesses": {"value": "* The paper's novelty is limited, as it does not introduce a new learning paradigm. It frames DAG scheduling as a reinforcement learning problem, a well-established approach from prior work like Decima. The core contributions are the CSformer and the memory module, which are essentially incremental architectural additions (graph positional encodings and a memory-augmented agent) rather than fundamental advances in either RL or scheduling.\n\n* A critical metric for a \"streaming\" scheduler, the decision-making latency, is relegated to the appendix. The main paper (pages 1-9) focuses exclusively on job completion time (JCT). Given that the MACE model is demonstrably more complex than its baselines, this overhead is a crucial part of the performance trade-off and its omission from the main experimental results is not acceptable. Please note that the reviewers are not required to read the appendix.\n\n* The empirical evaluation is not well-aligned with the ICLR audience. The experiments are conducted exclusively on Spark using the TPC-H benchmark, which is a traditional data analytics/database workload. The paper fails to evaluate its scheduler on any workloads more central to the modern AI community, such as scheduling for large-scale model training, complex training/data-processing pipelines, or large-batch inference.\n\n* The comparison against other learning-based schedulers is insufficient, as Decima is the only learning-based baseline included in the evaluation. While Decima is a seminal work in this area, the authors explicitly exclude other relevant and more recent learning-based schedulers, such as LACHESIS and DeepWeave. Justifying this exclusion by claiming \"incompatible problem formulation\"  is a weak defense and prevents a meaningful comparison of MACE against the current state-of-the-art in learning-based DAG scheduling.\n\n* The performance gains are modest and may not justify the added complexity. The paper claims \"up to 9.38%\" improvement, but in the main \"Default\" scenario in Table 1, the improvement over the primary learning baseline (Decima) is smaller (58.6 vs 63.1). This marginal gain is achieved by introducing a significantly more complex architecture involving a custom Transformer, a memory matrix, and complex read/write operations."}, "questions": {"value": "* The paper introduces significant architectural complexity (CSformer, a query-based memory matrix) for what appears to be a modest improvement over Decima in the default case (58.6s vs 63.1s). How do you justify this complexity-performance trade-off, especially given that the scheduling latency is a critical metric?\n\n* Speaking of latency, a crucial metric for any \"streaming\" scheduler is the per-decision overhead. Why was this analysis not included in the main results (e.g., in Table 1) but instead moved to the appendix?\n\n* The evaluation is performed exclusively on the TPC-H benchmark, a traditional data analytics/database workload. Given the ICLR audience, why was the MACE scheduler not evaluated on workloads more representative of modern AI systems, such as scheduling complex ML training pipelines or data-flow graphs for large model inference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cAHIiXuXHf", "forum": "rnrENwgDsn", "replyto": "rnrENwgDsn", "signatures": ["ICLR.cc/2026/Conference/Submission15052/Reviewer_PpAN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15052/Reviewer_PpAN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761161203727, "cdate": 1761161203727, "tmdate": 1763655393826, "mdate": 1763655393826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MACE, a novel reinforcement learning (RL) model designed to schedule streaming Directed Acyclic Graph (DAG) jobs in a cluster environment. The primary objective is to learn a scheduling policy that minimizes the average Job Completion Time (JCT). The authors identify two main limitations in prior work: (1) a failure to explicitly model execution-critical structures like critical paths, and (2) an inability to leverage historical scheduling contexts and outcomes.\n\nMACE addresses these gaps with two key architectural components:\n1.  **CSformer:** A graph encoder that builds hierarchical embeddings (stage-job-global). It uniquely incorporates **critical-path-aware positional encodings (CP-aware PE)** to prioritize latency-sensitive stages and uses a **tunable attention field** to capture long-range dependencies.\n2.  **Memory-Augmented Scheduler:** This component selects a runnable stage and its parallelism. It uses the embeddings from CSformer and queries a **job memory** that stores historical scheduling decisions and rewards, allowing the policy to make more context-aware decisions.\n\nThe model is trained end-to-end with policy gradients. Experiments conducted in a Spark simulation using the TPC-H benchmark demonstrate that MACE outperforms several heuristic and learning-based baselines, including Decima, by up to 9.38% on average JCT under diverse workloads."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Well-Motivated Problem and Approach:** The paper tackles a practical and challenging problem in large-scale systems: streaming DAG scheduling. The two motivations (capturing critical structures and using historical context) are clear, intuitive, and directly address significant weaknesses in existing learning-based schedulers.\n2.  **Novel Architectural Components:** The core ideas of MACE are technically sound and novel for this domain.\n    * The **CSformer**'s use of critical-path-aware positional encodings is a clever way to inject crucial domain knowledge directly into the graph representation, guiding the model's attention toward bottleneck stages. The ablation study strongly supports this, showing that removing the CP-aware PE causes the most significant performance degradation.\n    * The **memory-augmented scheduler** provides a principled mechanism to address the short-sightedness of typical RL agents by integrating past experience (decisions and outcomes) into the current decision-making process. This is also well-supported by the ablation study.\n3.  **Comprehensive Experimental Evaluation:** The experimental setup is thorough and convincing.\n    * The authors compare MACE against a strong set of seven baselines, including both widely-used heuristics (like HEFT, SRPT) and a state-of-the-art learning-based method (Decima).\n    * The \"stress tests\" in Table 1, which vary backlog, stream length, load, and cluster capacity, provide a robust evaluation of MACE's performance and generalization under diverse conditions.\n    * The ablation studies (Table 2) are effective in demonstrating the individual contributions of the memory, CP-aware PE, and receptive field, confirming that the components are complementary.\n    * The qualitative trace analysis (Figure 3) and job-level dynamics (Figure 4) provide valuable insights into *why* MACE's policies are more efficient, showing denser resource packing and faster queue clearance."}, "weaknesses": {"value": "1.  **Clarity of the Memory Mechanism:** The formulation of the job memory is somewhat confusing. Equation 11 defines the memory $M_{t-1}$ as a transformation $f_m$ over the *entire* set of previous decision trajectories. This implies a potentially unbounded and computationally expensive construction at each step. However, Equations 16 and 17 describe an *update* mechanism ($M_t = M_{t-1} \\odot C + U$), which is more typical of a recurrent state. It is unclear how these two formulations reconcile. Is the memory a fixed-size buffer, a growing list, or a recurrent hidden state? This lack of clarity makes it difficult to assess the true scalability and implementation of the memory module.\n2.  **Misleading Terminology:** The paper repeatedly refers to a \"**tunable** attention field\" or \"tunable Receptive Field (RF)\". However, the description of the method and the hyperparameter analysis (Appendix D, Figure 5(b)) suggest that the field size $k$ is a fixed hyperparameter selected via grid search, not a parameter that is \"tuned\" dynamically by the model itself. This terminology is misleading. \"Configurable\" or \"extended\" receptive field would be more accurate.\n3.  **Limited Scalability Analysis:** While Table 3 shows that MACE's scheduling latency per decision (5.529 ms) is acceptable and comparable to Decima's (5.470 ms) for the default setting, this analysis is limited. The paper does not investigate how this latency scales as the cluster state grows—for example, with a much larger number of concurrent jobs (e.g., $N_{init}^{test} \\gg 30$) or with significantly more complex DAGs (many more nodes/stages). Both the CSformer's attention mechanism and the memory retrieval step could become bottlenecks in larger, more complex scenarios.\n4.  **Baseline Exclusions:** The paper explicitly excludes comparisons to other learning-based schedulers like LACHESIS and DeepWeave, stating their problem formulations"}, "questions": {"value": "1) Could you please clarify the exact mechanism of the job memory? How is the memory M_t−1 from Equation 11 constructed and maintained, and how does it relate to the update rule in Equation 16? What is the computational complexity of the memory read (Eq. 13 ) and write (Eq. 16 ) operations, and how do they scale with the episode length?\n\n2) Regarding the \"tunable receptive field\": Is the radius k a fixed hyperparameter for the entire model, or is it learned or otherwise adapted during execution? If it is a fixed hyperparameter, please consider revising the terminology to \"configurable\" or \"extended\" to avoid confusion.\n\n3) The reward function in Equation 18, is an interesting proxy for minimizing average JCT. While cited, could you provide a brief intuition or justification (e.g., via Little's Law) for why minimizing the cumulative sum of this reward (the integral of the number of jobs over time) is equivalent to minimizing the average JCT?\n\n\n4) Can you comment on the expected scalability of MACE's decision latency (as reported in Table 3 ) with respect to the number of concurrent jobs and the average number of stages per DAG? At what point might the CSformer or memory-augmented scheduler become a practical bottleneck?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jI9KDMuNll", "forum": "rnrENwgDsn", "replyto": "rnrENwgDsn", "signatures": ["ICLR.cc/2026/Conference/Submission15052/Reviewer_za5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15052/Reviewer_za5G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560663693, "cdate": 1761560663693, "tmdate": 1762925373282, "mdate": 1762925373282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an RL scheduler for streaming DAG jobs, built on the Decima framework but extending the action space to jointly choose the next runnable stage and its parallelism. It encodes whole-DAG structure with a critical-path–aware transformer and conditions decisions on a compact memory of past scheduling outcomes. The policy is trained end-to-end to minimise average job completion time (JCT) and is evaluated through simulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Dealing with critical paths in DAG jobs and long historical patterns are well motivated in the scheduling context;\n\n2. Encoding the scheduling stage as a position in the critical path of the DAG is novel."}, "weaknesses": {"value": "1. The evaluation is done using simulation only compared to Decima.\n\n2. As there are only 22 DAG templates used in evaluation and the memory augmentation mechanism remembers many combinations, the generalisability of the scheduling policy can be limited to different DAGs.\n\n3. Even though the critical path encoding is a new approach for characterising the job DAG, it is questionable whether critical paths need to be explicitly modelled in scheduling as the GNN graph summary in Decima already implicitly captures the representation of a DAG through GNN. Critical paths can be used as a post-hoc feature to derive interpretable scheduling policies, but the graph summary itself might be more effective in training a optimised scheduling policy."}, "questions": {"value": "The results in Fig.3 shows SJF outperforms Decima. This is contrary to the batch job results in the Decima paper. Any explanation to the results?  How is the Decima model trained in the evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3tAuGAjdee", "forum": "rnrENwgDsn", "replyto": "rnrENwgDsn", "signatures": ["ICLR.cc/2026/Conference/Submission15052/Reviewer_4nDf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15052/Reviewer_4nDf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819036007, "cdate": 1761819036007, "tmdate": 1762925372950, "mdate": 1762925372950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses streaming DAG scheduling and proposes **MACE**, which consists of two main modules:  \n- The **CSformer** encoder enriches node embeddings on the critical path (CP) by injecting CP-related information (Equations 2 and 4). The rest of the structure—concatenating stage, job, and global embeddings—is similar to  Mao et al. [1], except that the authors adopt a Transformer-based architecture instead of GNNs.  \n- The **Memory-Augmented Scheduler** performs stage selection in the same way as [1], while in parallelism selection, the authors use historical decision information (previous job embeddings, parallelism choices, and rewards) as additional embeddings to enhance the current state.\n\nOverall, the work focuses on *input feature and embedding-level design* while relying on a standard REINFORCE algorithm for training. The introduction of Transformer-style encoding into scheduling is interesting and extends architectural diversity in this domain, but the dataset and baselines are limited and outdated.\n\n----\n\n[1] Mao et al., *Learning Scheduling Algorithms for Data Processing Clusters*, SIGCOMM 2019.  \n[2] Jeon et al., *Neural DAG Scheduling via One-Shot Priority Sampling*, ICLR 2023.  \n[3] Qi et al., *Reinforcement Learning for One-Shot DAG Scheduling with Comparability Identification and Dense Reward*, NeurIPS 2025."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a Transformer-based encoder for scheduling problems.\n* The integration of critical-path encoding is meaningful, enabling better structural awareness in scheduling decisions.\n* Incorporating memory into the parallelism selection process provides a novel perspective on leveraging historical scheduling decisions."}, "weaknesses": {"value": "* The paper mainly improves feature and embedding design without methodological novelty in the RL formulation or training procedure.\n* Baselines are very old — only compared with Decima [1], omitting many recent RL-based and learning-based schedulers from 2021–2025.\n* The dataset is restricted to TPC-H, lacking standard benchmarks such as Pegasus or job shop datasets, limiting the generality of the conclusions..\n* Related work is outdated, missing key references on DAG/workflow scheduling from recent top-tier conferences."}, "questions": {"value": "1. The terminology throughout the paper (e.g., “stage”) does not align with standard DAG-scheduling terminology (“task”). Please ensure consistent and conventional use of terms.\n1. In the *Learning-based Scheduling* subsection of Related Work, most cited papers are from 2019–2022. Please update with recent literature (e.g., ICLR, NeurIPS, ICML, TPDS, TSC 2023–2025) that addresses workflow or streaming scheduling.\n1. Could you clarify the role of Equation (3)? How is it used in practice, and what impact does it have on training or inference?\n1. The **dataset** should include more established benchmarks such as Pegasus workflows or Job Shop Scheduling (JSP) instances, as used in recent DAG-scheduling papers [2][3].\n1. **Baseline** selection is limited to [1]; please compare with more recent RL-based scheduling methods to demonstrate improvements in both performance and inference time.\n1. Regarding Equations (11)–(14): since these include historical stage information of a job, does this violate the Markov property required by the MDP formulation? Please clarify how your model maintains or approximates the MDP assumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K7rwoskOyx", "forum": "rnrENwgDsn", "replyto": "rnrENwgDsn", "signatures": ["ICLR.cc/2026/Conference/Submission15052/Reviewer_riU2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15052/Reviewer_riU2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990667794, "cdate": 1761990667794, "tmdate": 1762925372480, "mdate": 1762925372480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}