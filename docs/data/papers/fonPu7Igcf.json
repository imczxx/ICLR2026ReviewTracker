{"id": "fonPu7Igcf", "number": 1915, "cdate": 1756965355432, "mdate": 1759898178915, "content": {"title": "Reconciling In-Context and In-Weight Learning: A Dual-Space Modeling Perspective", "abstract": "In-context learning (ICL) is a valuable capability exhibited by Transformers pretrained on diverse sequence tasks. However, prior studies have observed that ICL often exhibits a conflict with the model’s inherent in-weight learning (IWL) capability. In this work, we aim to reconcile ICL and IWL by disentangling the model’s encoding spaces for context and input samples. To do so, we first propose a dual-space modeling framework, explicitly modeling a task representation space via the dual space of the sample representation space. Such a dual-space structure can be derived from the linear representation hypothesis and, as we theoretically prove, is conducive to ICL by representation learning. Furthermore, we show that the standard Transformer architecture with softmax self-attention is inherently limited in realizing this structure. Building on this insight, we introduce CoQE, a Transformer architecture with separate context-query encoding, to realize the disentanglement between context and sample representations. Through experiments on both regression and classification tasks, we demonstrate that CoQE not only achieves lower ICL error compared to the standard Transformers, but also successfully reconciles ICL and IWL under diverse data distributions.", "tldr": "We propose CoQE, a Transformer with dual-space context-query encoding, reconciling ICL and IWL, and achieving lower ICL error than standard Transformers across diverse tasks and data distributions.", "keywords": ["in-context learning; in-weight learning; dual-space; model architecture"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a43d6957334da42536a355049a29910d8f452eb8.pdf", "supplementary_material": "/attachment/2ba2aa4dba53fa066f73dd44d254dbc1020c0c6e.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to reconcile the discrepancy between In-Context Learning (ICL) and In-Weight Learning (IWL) by disentangling the model’s encoding spaces for context and input samples. To achieve this, the authors propose a dual-space modeling framework that explicitly constructs a task representation space as the dual of the sample representation space. They demonstrate that such a structure facilitates ICL through improved representation learning. Building upon this insight, the authors introduce CoQE, a Transformer-based architecture with separate context and query encoders, effectively achieving the desired disentanglement between context and sample representations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and meaningful research question—the conflict between ICL and IWL.\n2. The proposed CoQE architecture introduces a novel design. Beyond empirical validation, the authors also provide a detailed theoretical analysis to support their claims."}, "weaknesses": {"value": "The theoretical presentation lacks clarity and organization:\n\n1. The purpose and implication of **Theorem 3.7** are unclear. It is not evident whether the dual-space formulation achieves a tighter bound than the one presented in this theorem.\n2. Although the paper claims to reconcile the conflict between ICL and IWL, **Theorem 3.10** only establishes their entanglement, rather than addressing the nature of their conflict. In fact, such entanglement might allow mutual reinforcement instead of opposition.\n\nAdditionally, the computational efficiency of the proposed method is not discussed. The paper omits key details such as the parameter scale and the computational cost associated with the new operations.\n\nFinally, the empirical evaluation only compares CoQE against standard Transformers. It remains uncertain whether the proposed structure outperforms other advanced Transformer variants."}, "questions": {"value": "How to assure that the task representation actually learn the representation of tasks instead of others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lxrIQutJwh", "forum": "fonPu7Igcf", "replyto": "fonPu7Igcf", "signatures": ["ICLR.cc/2026/Conference/Submission1915/Reviewer_ekpH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1915/Reviewer_ekpH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785891646, "cdate": 1761785891646, "tmdate": 1762915949406, "mdate": 1762915949406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the important challenge of designing autoregressive models that support both in-context learning (ICL) and in-weight learning (IWL). The paper proposes a dual-space modeling framework to allow both ICL and IWL capabilities, providing a theoretical support using linear representation hypothesis. They explicitly model task representation space via the dual space of the sample representation space. To implement this, the paper proposes a new architecture that encodes context and query separately to resolve the representation entanglement, which is identified as the main cause of conflict. The proposed architecture attempts to implement the dual-space theory two spaces interact through inner products. The proposed method is evaluated on a regression task using synthetic data and classification task with Omniglot data. Results show improved results on both ICL and IWL performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The conflict between ICL and IWL in autoregressive transformer models is a relevant topic of research. The idea that samples and tasks can operate in separate but dual representation spaces and the connection with Riesz representation theorem is conceptually novel. The dual-space theoretical framework is well defined covering required formal definitions and assumptions.\n\n2. Results show clear improvements with improved ICL performance without hurting IWL. To support the proposed theory, they further demonstrate empirically that separating context and query stabilizes ICL in both in-distribution and OOD cases. This is shown for both regression and classification tasks.\n\n3. Paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. Authors proposed CoQE architecture to structurally  separate the spaces to resolve the ICL and IWL conflict. However, the dependence on gaussian noise for the classification is very strong to avoid collapse, where performance is close to chance level without the noise injection. The dependence of gaussian noise regularizer parallels with the usage of l2-regularization in [Chan et al. 2022] to balance ICL and IWL, implying that the CoQE benefits step more from regularization than from its theoretical design. This weakens the theoretical claim and advantage of CoQE. \n\n2. Theorem 3.10 equates non-linearity of softmax with the non-existence of factorized dual-space. Non-linearity does not rule out the existence of an equivalent linear form under a suitable feature mapping. The reasoning is limited to a simple setup such as a single softmax-attention layer and doesn’t include a mathematical proof. Empirical studies in the literature such as Han et al. 2025 [a], show that attention-based transformers can exhibit linearly separable task vectors for distinct in-context tasks. This weakens the theorem. \n- [a] Emergence and Effectiveness of Task Vectors in In-Context Learning : An Encoder Decoder Perspective, Han et al. 2025.\n\n3. The assumption of a shared linear sample representation across tasks is conceptually good but too strong for real-world multi-task settings. Many real-world tasks in natural language and vision exhibit nonlinear mapping between input and output spaces. This limits the practical applicability of the proposed framework.\n\nPlease feel free to clarify if I misunderstood anything mentioned above."}, "questions": {"value": "1. I suggest that the authors demonstrate the advantage of the CoQE architecture with more ICL-IWL tasks which are not dependent on such strong regularizations. This would clarify whether the observed improvements come from the architecture or the regularization.\n\n2. Abstract claims that transformer with softmax self-attention is limiting for the dual-space structure. However, this limitation is not demonstrated clearly in the main text. Authors should clarify what specific theoretical or empirical evidence supports this claim.\n\n3. The paper would benefit from a better connection to previous works in the literature. A comparative discussion with recent studies mentioned below should be included to better position this work. (a) Toward Understanding In-context vs. In-weight Learning (Chan et al. 2025), (b) What Matters for In-Context Learning: A Balancing Act of Look-up and In-Weight Learning (Bratulic et al. 2025), (c) Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting (Anand et al. 2025). \n\n4. The paper lacks discussion about certain points such as implications of oversimplified assumptions like shared linear sample space across tasks and limitations of currently used empirical tasks.\n\nMinor remark:\n- Figure and table captions should be self-contained clearly conveying the necessary information to understand them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hzhMd5wY2i", "forum": "fonPu7Igcf", "replyto": "fonPu7Igcf", "signatures": ["ICLR.cc/2026/Conference/Submission1915/Reviewer_BgzZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1915/Reviewer_BgzZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833039445, "cdate": 1761833039445, "tmdate": 1762915948841, "mdate": 1762915948841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the observed conflict between in-context learning and in-weight learning . The authors propose that this tension arises because standard Transformers entangle the encoding of context and samples. To address this, they introduce a dual-space modeling framework, where the task representation space is the dual space of the sample representation space. Building on this, they propose CoQE, an architecture that explicitly separates context and query encoding: Predictions are computed via an inner product between task and sample representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper presents a novel theoretical framework to explain the underlying conflict between in-context learning (ICL) and in-weight learning (IWL).\n2.\tBuilding on this theory, the authors propose a new architectural design (CoQE) that improves performance and reconciles the trade-off between ICL and IWL.\n3.\tThe figures and visualizations are clear and well-designed."}, "weaknesses": {"value": "1.\tThe CoQE architecture resembles a dual-tower model, which may compromise the model’s capability for open-ended generation. This limitation reduces its applicability to a broader range of tasks, and additional training would likely be required for different task types.\n2.\tThe evaluation of CoQE is limited to relatively simple regression and classification tasks, which may not be sufficient to demonstrate its effectiveness on more complex or real-world tasks.\n3.\tThe proposed approach relies heavily on a strong linear representation hypothesis, assuming that the task and sample representation spaces form dual linear spaces. The paper lacks sufficient empirical evidence or justification to support this assumption."}, "questions": {"value": "1.\tIn Section 3.1, the authors state that “each basis often corresponds to an independent attribute or concept.” However, bases in a linear space can be chosen arbitrarily. It is unclear why each basis necessarily corresponds to a specific attribute. Could the authors provide concrete examples to clarify this point?\n2.\tCould the authors include experiments evaluating CoQE on more complex tasks, such as standard NLP benchmarks, to demonstrate its scalability and general applicability?\n3.\tCould the authors further explain why a task function can be regarded as a linear function of the sample representation, given that many real-world tasks—especially reasoning tasks requiring chain-of-thought (CoT)—exhibit strong nonlinearity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tQpBT6rjpb", "forum": "fonPu7Igcf", "replyto": "fonPu7Igcf", "signatures": ["ICLR.cc/2026/Conference/Submission1915/Reviewer_cPUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1915/Reviewer_cPUL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895999938, "cdate": 1761895999938, "tmdate": 1762915948086, "mdate": 1762915948086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the interplay between In-Context Learning (ICL) and In-Weight Learning (IWL) in Transformers.  \nIt proposes a dual-space modeling framework that represents these two processes in a task representation space and a sample representation space, linked through the Riesz representation theorem.  \nThe authors argue that the conflict between ICL and IWL originates from the entanglement between context and query encoding in standard self-attention.  \nTo address this issue, they introduce CoQE (Context–Query Encoding Transformer), which explicitly separates context and query encoding pathways.  \nExperiments on regression and few-shot classification tasks show consistent improvements across both ICL and IWL metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a mathematically grounded view of ICL and IWL, introducing a dual-space theoretical framework that distinguishes the task space from the weight (sample) space. This perspective is conceptually novel.  \n- Theoretical reasoning directly motivates an architectural design (CoQE), forming a coherent pipeline from theory → architecture → experiment."}, "weaknesses": {"value": "- The concept of In-Weight Learning (IWL) should be more explicitly defined within the paper, instead of relying mainly on external references.  \n- The proposed CoQE structure appears to apply an additional embedding to the context input. From Equation (11), it is unclear how this achieves a true *separation* between context and query encodings, since both still pass through a shared encoder.  \n- The mapping between ID/OOD performance and IWL/ICL capability is indirect. It is unclear why the authors did not include a no-context baseline (weight-only inference) to explicitly test IWL."}, "questions": {"value": "1. Nonlinear Sample Spaces  \n   The theoretical assumptions are quite strong and rely on the linearity of the sample space \\( $M_F$ \\).  \n   How would the proposed dual-space framework extend to nonlinear or non-convex representation spaces?\n\n2. Memory Interpretation  \n\n   I am wondering whether it is appropriate to interpret the interaction between ICL and IWL as analogous to short-term and long-term memory mechanisms.\n    Can CoQE be understood as a framework that aims to enhance both types of memory simultaneously, or rather as one that mitigates interference between them?\n\n3. Regression Function Families  \n   In Section 4.1, the authors write:  \n   *“Specifically, we use the following four classes of functions F: linear functions, sparse linear functions, two-layer ReLU networks, and combination functions.”*  \n   Are these referring to target functions used for data generation or to model architectures?  \n   Figure 3(a) suggests they are target functions, but this should be stated explicitly.\n\n4. Theory–Experiment Gap  \n   The theoretical part mainly analyzes the entanglement between ICL and IWL and argues that CoQE supports both within the dual-space framework.  \n   However, the experiments (especially regression and few-shot classification) primarily test ICL generalization, particularly under OOD settings.  \n   - Are the experiments intended to verify ICL generalization rather than true ICL–IWL coexistence?  \n   - If claiming “simultaneous improvement,” how is IWL concretely evaluated?  \n   - Would adding a no-context control (i.e., weight-only inference) better demonstrate CoQE’s preservation of IWL capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tJBjDhZQ75", "forum": "fonPu7Igcf", "replyto": "fonPu7Igcf", "signatures": ["ICLR.cc/2026/Conference/Submission1915/Reviewer_SVff"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1915/Reviewer_SVff"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925645971, "cdate": 1761925645971, "tmdate": 1762915947773, "mdate": 1762915947773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}