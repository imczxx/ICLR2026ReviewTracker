{"id": "66v0c2oOHK", "number": 3911, "cdate": 1757566724135, "mdate": 1759898063126, "content": {"title": "KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering In Multi-Turn Dialogues", "abstract": "Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application paradigm of Large Language Models (LLMs) in knowledge-intensive domains. However, existing benchmarks are limited to single-turn dialogue, while multi-turn dialogue benchmarks typically assess other orthogonal capabilities rather than knowledge-intensive factuality. To bridge this critical gap, we introduce **KnowMT-Bench**, the first-ever benchmark designed to systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields, including medicine, finance, and law. To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories given logically progressive question sequences. The factual capability and information delivery efficiency of the final-turn answer are then evaluated using a human-validated automated pipeline. Our experiments reveal that multi-turn contexts degrade performance: factual capability declines due to the contextual noise from self-generated histories, while information efficiency drops as models become more verbose with increasing dialogue length. We then investigate mitigation strategies, demonstrating that retrieval-augmented generation (RAG) can effectively alleviate and even reverse this factual degradation. These findings underscore the importance of our benchmark in evaluating and enhancing the conversational factual capabilities of LLMs in real-world knowledge-intensive applications.", "tldr": "A benchmark to evaluate knowledge-intensive long-form question answering for LLMs in multi-turn dialogues", "keywords": ["Benchmark", "Multi-turn dialogues", "Long Form Question Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08fdab544ef564434ed7b649ba130b847af90b48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces KnowMT-Bench , the first multi-turn long-form question answering (MT-LFQA) benchmark for knowledge-intensive domains (medicine, finance, law).\nThe research makes three main findings and contributions:\n1. Performance Degradation: LLMs exhibit a significant drop in factual capability and information delivery efficiency when transitioning from single-turn to multi-turn dialogue.\n2. Root Cause Identification: The decline in factual capability is primarily due to contextual noise generated by the model itself in previous turns, not merely dialogue length.\n3. Mitigation Strategy: Retrieval-Augmented Generation (RAG) is an effective strategy that can alleviate and even reverse this factual degradation by grounding each\nconversational step with external evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality**\n\n- **First-of-its-kind benchmark**: KnowMT-Bench establishes the first benchmark specifically for Multi-Turn Long-Form Question Answering (MT-LFQA) in knowledge-intensive domains (medicine, finance, and law)\n\n- **Novel evaluation setting**: Introduces a highly realistic dynamic evaluation framework where models must generate their own multi-turn conversational history from progressive question sequences\n\n- **Real-world simulation**: Effectively replicates actual consultation scenarios and captures how contextual errors accumulate over conversation turns\n\n**Methodology Quality**\n\n- **Rigorous evaluation pipeline**: Develops a custom, fully automated, two-stage Natural Language Inference (NLI)-based evaluation system that ensures high-fidelity assessment of factual consistency\n\n- **Overcomes prior limitations**: Addresses the shortcomings of superficial metrics used in previous work\n\n- **Human validation**: Extensively validates the system's reliability through human expert annotations, demonstrating excellent agreement at both the answer decomposition and factual judgment stages\n\n**Clarity**\n\n- **Precise diagnosis**: Excels in identifying the specific causes of performance degradation through clear empirical analysis\n\n- **Distinct factor separation**: Empirically distinguishes between:\n  - **Dialogue length effects**: Drives verbosity and efficiency loss\n  - **Contextual noise effects**: Drives factual accuracy decline\n\n- **Enhanced interpretability**: This clear attribution enables targeted mitigation strategies and improves result interpretability\n\n**Significance**\n\n- **Critical guidance for LLM development**: Provides essential insights for advancing future large language models\n\n- **Reveals evaluation gaps**: Fundamentally demonstrates that single-turn LFQA evaluations are inadequate for assessing real-world performance\n\n- **RAG as powerful solution**: Establishes that Retrieval-Augmented Generation (RAG) is not merely a superficial fix, but a robust solution that can:\n  - Mitigate factual degradation caused by conversational noise\n  - Potentially reverse accuracy decline\n  - Provide a clear pathway toward building more reliable conversational LLMs"}, "weaknesses": {"value": "**Limited Dialogue Complexity**\n\n- **Rigid question structure**: Uses logically progressive question sequences (PQS) guided by fixed templates, limiting conversational diversity\n\n- **Excludes critical phenomena**: Does not evaluate important real-world conversational dynamics, including:\n  - Spontaneous topic switching\n  - User correction of model errors\n  - Handling of contradictory contextual information\n\n- **Artificial noise filtering**: Actively screens out common forms of conversational noise\n\n- **Overestimation risk**: May lead to inflated assessments of LLM robustness in less structured, dynamic real-world dialogues\n\n- **Reduced real-world fidelity**: The restricted scope limits how well the benchmark represents actual consultation scenarios\n\n**High Dependence on LLM-Based Evaluation**\n\n- **Single model family dependency**: The entire quality assessment pipeline (both decomposition and judgment stages) relies exclusively on specific Qwen family LLMs\n\n- **Potential evaluation bias**: This narrow dependency raises concerns about systemic biases inherent to the chosen model architecture\n\n- **Limited generalizability testing**: Lacks validation across diverse LLM architectures\n\n- **Need for cross-model validation**: Future work should incorporate alternative high-performing LLMs (e.g., GPT-4o) in the decomposition component to:\n  - Verify consistency of factual scores across different models\n  - Ensure impartiality of the evaluation framework\n  - Confirm robustness of the assessment methodology\n\n**Insufficient Mitigation Analysis**\n\n- **Shallow prompt intervention exploration**: Section 5.3's analysis of prompt-based interventions lacks depth\n\n- **Harsh trade-off identified**: The simple noise-disregard prompt yielded problematic results:\n  - Improved factuality\n  - Increased hallucination rates\n\n- **Missed opportunities**: Failed to explore more sophisticated prompt engineering techniques, such as:\n  - Self-correction mechanisms\n  - Guided context selection strategies\n  - Multi-stage reasoning prompts\n  - Uncertainty acknowledgment frameworks\n\n- **Lack of actionable guidance**: Does not provide practical, implementable solutions for practitioners seeking to mitigate multi-turn degradation"}, "questions": {"value": "1. Dialogue Realism and Scope: The benchmark uses pre-defined logically progressive question sequences (PQS) to ensure intent preservation. Do you believe this highly constrained format adequately captures real-world conversational challenges like user topic drift or the need for error correction, or does it potentially overstate the model's factual robustness?\n2. For future work, consider including a Challenge Subset of dialogues that explicitly test robustness against user correction of model errors and contradictory contextual information.\n3. Evaluation Pipeline Robustness (Decomposition Bias): The entire evaluation's quality hinges on the LLM Decomposer (Qwen2.5-32B-Instruct); Did you investigate the cross-architectural robustness of the decomposition step? Would using a different strong model (e.g., GPT-4o) as the decomposer yield the same atomic facts and resulting metrics,\nor could there be an architectural bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2i7Y1tx03J", "forum": "66v0c2oOHK", "replyto": "66v0c2oOHK", "signatures": ["ICLR.cc/2026/Conference/Submission3911/Reviewer_jhxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3911/Reviewer_jhxT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761016621584, "cdate": 1761016621584, "tmdate": 1762917094705, "mdate": 1762917094705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KnowMT-Bench, the first benchmark specifically designed to evaluate multi-turn long-form question answering (MT-LFQA) in knowledge-intensive domains (medicine, finance, and law)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty. This work explicitly targets multi-turn, knowledge-intensive long-form QA, which prior benchmarks only partially address.\n2. Robust evaluation design. The NLI-based, human-validated pipeline is fine-grained into two parts and both automated, addressing weaknesses of LLM-as-a-judge methods.\n3. Comprehensive ablation studies by dialogue length, domain, finetuning, and retrieval strategy.\n4. Brings empirical insights by identifying contextual noise as the primary failure mode and empirically validating RAG as a strong mitigation.\n5. Covers three specialized, high-stakes areas, showing generalization."}, "weaknesses": {"value": "1. Although partially human-validated, using Qwen models as both decomposer and NLI judge + also evaluating Qwen models on the benchmark introduces potential bias.\n2. Only RAG is tested, and other context-filtering or history-truncation approaches (e.g., summarization, re-encoding) are not.\n3. Potential domain imbalance: Most data (≈ 579/801) come from finance; medical and legal subsets are smaller.\n4. Bias in synthetic dialogs: Multi-turn sequences are partly LLM-generated, which could bake in stylistic artifacts or coherence patterns not reflective of real users.\n5. No comparison to existing multi-turn factual QA datasets (e.g., ConvFinQA, Doc2Dial) under unified metrics to show cross-benchmark consistency."}, "questions": {"value": "1. “Factual F1” and “Hallucination F1” are intuitive but may double-count correlated errors. One non-factual claim will both reduce S_f (by lowering P_f) and raise S_h (by increasing P_fc), so you are double-penalizing models with concise answer (less coverage of ground truths) but one error claim. On the other hand, models with verbose answers will give lots of facts where some are true and some are false, and will inflate both S_f and S_h. Evaluating information factuality vs. verbosity is difficult when both scores co-vary due to shared entailment/contradiction judgements. A future refinement might include separating coverage, error density, and truth-to-length ratio to better distinguish these behaviors.\n2. When we evaluate one model, how should we holistically interpret the three dimensional metrics you introduced -- factuality, information delivery efficiency and hallucination metrics? What composition of these metrics indicate a good model?\n3. What exact prompt was used in prompt-based mitigation (Table 4)? Why did hallucination rise here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xFuli6b61c", "forum": "66v0c2oOHK", "replyto": "66v0c2oOHK", "signatures": ["ICLR.cc/2026/Conference/Submission3911/Reviewer_J9J3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3911/Reviewer_J9J3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073257979, "cdate": 1762073257979, "tmdate": 1762917094356, "mdate": 1762917094356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KnowMT-Bench, presented as the first benchmark for Multi-Turn Long-Form Question Answering (MT-LFQA) in knowledge-intensive domains. The authors construct 801 evidence-grounded QA instances across medicine, finance, and law, with dialogues spanning 2-5 turns. Question sequences are generated using 18 predefined templates (8 for 2-turn, 10 for 3-turn) and expanded via LLM generation with human refinement.\nThe evaluation employs a two-stage NLI-based pipeline: (1) decomposing answers into atomic facts using Qwen2.5-32B, and (2) judging factual consistency using Qwen2.5-14B, achieving 83.6% F1 agreement with human annotations. The authors evaluate 13 LLMs where models generate their own dialogue history, and assess performance across three dimensions: factuality (precision/recall of facts), hallucination (contradiction rates), and information delivery efficiency (tokens per fact)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Paper presents a well-motivated problem and addresses a clear gap at the intersection of multi-turn dialogue, long-form answers, and knowledge-intensive domains. Table 1 effectively demonstrates no existing benchmark covers all three dimensions.\n\n- Rigorous evaluation framework: Two-stage NLI-based pipeline with atomic fact decomposition achieves 83.6% F1 agreement with human judgments (Cohen's κ=0.80). The symmetric evaluation approach is efficient and interpretable.\n\n- Dynamic evaluation setting: Models generate their own dialogue history, more faithfully capturing real deployment scenarios than static benchmarks.\n- Comprehensive experiments: 13 models evaluated with systematic investigation of mitigation strategies. The \"replace\" experiment (Figure 6) provides evidence for the contextual noise hypothesis.\n- High-quality data construction: Multi-stage human validation with authoritative sources and proper inter-annotator agreement metrics."}, "weaknesses": {"value": "- Model-dependent evaluation: Paper relies on Qwen-2.5-32B for decomposition (18.1% SMAPE, 5.9% omission rate) and Qwen-2.5-14B for judgment. While Table 7 rules out self-preference, robustness to evaluator choice is not explored in detail. Further, only 100 dialogues used for human validation.\n\n- Limited scale of eval data: Only 801 instances with severe domain imbalance (72% finance, 26% medicine). Average 2.98 turns (max 5) may not capture degradation in longer conversations. No significance tests or confidence intervals reported for main results.\n\n- Artificial question sequences leads to generalizability concerns: Paper's approach of template-based generation (8-10 templates) with \"no leakage\" constraint creates unnatural conversation patterns. Real dialogues involve iterative refinement and reference to prior context. There is no analysis comparing template vs. natural conversations. \n\n- Template benchmarking vs true multi-turn reasoning: Since templates create unnatural conversation history, it could mean that benchmark is only helping model better identify the template patterns vs true multi-turn reasoning. Benchmark uses only 8 templates for 2-turn and 10 templates for 3-turn dialogues, creating serious risk that models could likely exploit template patterns rather than demonstrate genuine multi-turn reasoning.\n\n- Weak causal evidence: The claim that \"contextual noise\" causes degradation conflates history quality with model capability. \"Replace\" experiment only tests two weaker models with GPT-4o history (not vice versa). Alternative explanations (context length, position bias, different information) not ruled out.\n\n- Shallow analysis: Missing error pattern analysis, failure mode characterization, and domain-specific insights. Table 2 shows domain results but minimal discussion. No qualitative examples of where/why RAG helps or fails."}, "questions": {"value": "- Causality: Can you provide a synthetic/controlled experiment where model knowledge is verified in single-turn, then test multi-turn with systematic noise injection? This would establish causal evidence for \"contextual noise.\"\n\n- Multi-turn vs. long-context: Can you add baselines comparing (a) single-turn with concatenated history, (b) multi-turn with human-written history, and (c) multi-turn with ground-truth history? This would distinguish dialogue-structure effects from long-context effects.\n\n- SFT: Why is supervised fine-tuning not explored given that (a) domain-FT helps (Table 2), (b) prompting shows promise but limitations, and (c) this is the standard approach for teaching models new behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tiFyFc3f6Z", "forum": "66v0c2oOHK", "replyto": "66v0c2oOHK", "signatures": ["ICLR.cc/2026/Conference/Submission3911/Reviewer_RzT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3911/Reviewer_RzT5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237161187, "cdate": 1762237161187, "tmdate": 1762917094127, "mdate": 1762917094127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce the new benchmark in multi-turn long-form question answering (MT-LFQA). They claim that it is the first benchmark in this field and its questions are spread across various domains - medicine, finance and law. The authors proposed a new data construction pipeline and using that, they were able to generate around 800 samples of data. Various LLMs were then evaluated on this dataset and they showed that LLMs fail to perform well on MT-LFQA tasks. Finally, they showed that the problem can be mitigated by using RAG, while testing other approaches such as domain-specific finetuning and prompt interventions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The curation of the dataset was mostly automated except for the part where human evaluation was being used. \n2. The authors tested various mitigation strategies to resolve the problem of poor performance of LLM on their dataset and concluded that RAG is the best approach."}, "weaknesses": {"value": "1. KnowMT-Bench is the first benchmark - In introduction (L96,L97), the authors claim that KnowMT-Bench is the first benchmark to study multi-turn long-form question answering. However, there are some studies that have created very similar datasets as curated by the authors ref [a,b,c,d,e]. The authors have also mentioned one of these works along with some others in literature review (See supplementary) that have done MT-LFQA. Can authors point out how exactly their work is different from these works?\n\n\n2. Low number of turns - The average number of turns in the curated dataset is less than 3 with the highest being 5. This is shallow and does not truly capture the long-horizon conversational dependencies found in real multi-turn expert consultations and they are often longer than 5 turns. Other datasets such as DialogBench [Ref. a and c] have an average number of turns higher than 7. So the practical applicability of dataset is still needed to be justified. \n\n3.  Domain coverage - As mentioned in L180 and L181, the author's proposed data curation process yields 801 high-quality single-turn LFQA instances spanning three domains: finance (579), law (278), and medicine (209). One can note that the data composition is skewed i.e. 579 of 801 QA pairs are finance-related (which is over 70%). This represents a slightly biased evaluation as more questions are for finance.\nFurthermore, the fields are limited to just 3 categories and the number of questions are relatively less. Existing benchmark [Ref. a] has more than 15 categories and has more than 10K questions.\n\n4. Weak human alignment - In L249 and 250, the authors mentioned that the agreement between their NLI-based evaluator and the resulting gold annotations from majority voting among three annotators reached an F1-score of 83.6%. I feel that this is quite low and concerning also. A F1 score should be higher than 90% in order to show human alignment. \n\n5. Lack of statistical rigor - Performance comparisons (e.g. in figs. 4,6) rely on descriptive statistics (scatter plots, correlations) without confidence intervals. Moreover, there are no tests across random seeds or variations in decomposition/evaluator models.\n\n\n\nReferences\na) Ou et. al. - https://arxiv.org/abs/2311.01677, \nb) Zheng et. al. - https://arxiv.org/pdf/2306.05685,\nc) Li et. al. - https://arxiv.org/abs/1710.03957,\nd) Zhao et. al. - https://arxiv.org/pdf/2304.09582\ne) Feng et. al. - https://arxiv.org/abs/2109.12595"}, "questions": {"value": "There are few more comparisons that needs to be made as mentioned in weakness; also practicability and length of corpus seems low as well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "112VAAfV6H", "forum": "66v0c2oOHK", "replyto": "66v0c2oOHK", "signatures": ["ICLR.cc/2026/Conference/Submission3911/Reviewer_m7UK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3911/Reviewer_m7UK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762282400531, "cdate": 1762282400531, "tmdate": 1762917093730, "mdate": 1762917093730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KnowMT-Bench, a benchmark designed to evaluate Multi-Turn Long-Form Question Answering (MT-LFQA) in domains such as medicine, finance, and law. It aims to address the gap between existing single-turn LFQA datasets and the complex multi-turn nature of real-world professional consultations. The benchmark consists of 801 multi-turn, evidence-grounded QA instances derived from authoritative sources. The work represents a solid benchmark built to simulate a realistic human-LLMs interaction. While it heavily relies on generated data with LLM, human annotation was used to validate the agreement, showing positive results. Findings show that in a multi-turn setup, there is a degradation of performances for all the models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work represents a solid benchmark to test models in an artificial setup, which represents a good proxy also for real-world use cases. The analysis provides enough in-depth information, and findings (although not novel) represent an interesting starting point to develop further research and models to solve the multi-turn setup."}, "weaknesses": {"value": "The paper has its merits and overall it is a meaningful contribution for the research community. I have mainly two concerns: 1/ the size is limited. While this is a benchmark, it is not clear to me if the dataset contains a representative set of queries where to test generalization of models. 2/ The topic distribution seems to be skewed. Majority of questions are from finance. \n\nI would like authors to provide more details on these choices."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NvgFo0mNeL", "forum": "66v0c2oOHK", "replyto": "66v0c2oOHK", "signatures": ["ICLR.cc/2026/Conference/Submission3911/Reviewer_7n4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3911/Reviewer_7n4w"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762370937272, "cdate": 1762370937272, "tmdate": 1762917093471, "mdate": 1762917093471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}