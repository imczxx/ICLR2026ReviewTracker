{"id": "psUjNnLhl9", "number": 15615, "cdate": 1758253209851, "mdate": 1759897295157, "content": {"title": "MaskCO: Masked Generation Drives Effective Representation Learning and Exploiting for Combinatorial Optimization", "abstract": "Neural combinatorial optimization (NCO) has long been anchored in paradigms like solution construction or improvement that treat the solution as a monolithic reference, squandering the rich local decision patterns embedded in high-quality solutions.  Inspired by self-supervised pretraining breakthroughs in language and vision, where simple yet powerful paradigms like next-token prediction enable scalable learning, we ask: \\textit{Can combinatorial optimization adopt such a fundamental training paradigm to enable effective and scalable representation learning?} We introduce MaskCO, a masked generation approach that reframes learning to optimize as solution-level self-supervised learning on given reference solutions. By strategically masking portions of optimal solutions and training models to recover the missing content,  MaskCO turns a single (instance, solution) pair into hundreds of (instance, partial solution) pairs, encouraging the model to internalize fine-grained, localized decision patterns. For inference, we propose a mask-and-reconstruct procedure that naturally leverages the training objective to implement a local-search-like refinement: each iteration masks certain variables and reconstructs through masked generation, progressively improving the current solution. We also find that the learned representations readily transfer to alternative inference routines and facilitate effective fine-tuning in other models. Experimental results demonstrate that masked generation serves as a universal learning objective across multiple CO problems, redefining how solutions are learned, refined, and scaled. Compared to previous state-of-the-art neural solvers, MaskCO achieves remarkable performance improvements, exceeding 99\\% in optimality gap reduction, along with a 10x speedup on the Travelling Salesman Problem (TSP).", "tldr": "", "keywords": ["Neural Combinatorial Optimization", "Masked Generation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/217b02999206fd821f57ad85ed6522d4f79b78eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MaskCO, a masked generation paradigm that formulates the learning process of neural combinatorial optimization (NCO) as a solution-level self-supervised learning framework. Specifically, it masks part of an optimal solution and reconstructs it to learn fine-grained, localized decision patterns. During inference, the model constructs a complete solution through a mask-and-reconstruct procedure, resembling a local-search-like refinement: in each iteration, certain variables are masked and regenerated, progressively improving the current solution. Extensive experiments on TSP, CVRP, and MIS demonstrate its superiority over previous baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The topic of this paper is exciting and challenging, exploring a foundational training paradigm that enables effective and scalable representation learning for CO.\n* Self-supervised training is an appealing and promising paradigm for NCO.\n* The reported training overhead appears lightweight, as shown in Table 23.\n* The empirical results are strong."}, "weaknesses": {"value": "* The authors claim that the proposed masked generation serves as a `foundational paradigm` for NCO. Does “foundational” here imply the goal of developing a foundation model for CO? If so, why not consider the multi-task training setting? Moreover, the paper does not discuss recent efforts toward multi-task or foundation models for CO, such as [1–4].\n* The generality of the proposed approach is unclear. This work addresses TSP, CVRP, and MIS, which do not involve complex constraints. Could the proposed method handle more complex constrained VRPs [5] or other CO problems as studied in [1]?\n* The training process still requires high-quality solutions as labels. Have the authors explored self-improvement learning as studied in [6]?\n* The paper emphasizes representation learning. Could the authors provide a deeper analysis of the learned representations?\n* The writing of this paper could be improved:\n  * Parts of the introduction appear overly generated (possibly by LLMs). I would expect more direct and concrete opinions from the authors, rather than an overly abstract and ambitious presentation.\n  * The descriptions in Sections 3 and 4 are somewhat verbose and make the approach seem more complicated than it is. A clear figure illustrating the overall process would improve readability.\n  * It would be helpful to fully elaborate the model architecture in mathematical form.\n  * Visualizing how the (partial) solution evolves through the decoding process would make the approach more intuitive.\n* Minor comments:\n  * Line 79: “instancesolution” → missing space.\n  * Line 80: “scalabilityparticularly” → missing space.\n  * Line 84: Clarify “BETR and ?”.\n\n[1] GOAL: A Generalist Combinatorial Optimization Agent Learner. ICLR 2025.  \n[2] MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts. ICML 2024.  \n[3] RouteFinder: Towards Foundation Models for Vehicle Routing Problems. TMLR 2025.  \n[4] UniCO: On Unified Combinatorial Optimization via Problem Reduction to Matrix-Encoded General TSP. ICLR 2025.  \n[5] Learning to Handle Complex Constraints for Vehicle Routing Problems. NeurIPS 2024.  \n[6] Boosting neural combinatorial optimization for large-scale vehicle routing problems. ICLR 2025.\n\n----\n\nOverall, the studied topic of this paper is exciting and challenging. I believe it makes a valuable contribution to the NCO community, and therefore I recommend acceptance."}, "questions": {"value": "* Can the proposed method ensure 100% solution feasibility? If so, please explain how. If not, the feasibility rate should be reported in the main experimental table.\n* For the TSP case in Section 4.2 (MultiStepDecoding), is the $|U(G)|=m^2$? The proposed approach seems to generate multiple dynamic heatmaps rather than a single static one as in previous methods. If so, why is the inference time of MaskCO significantly lower than that of DIFUSCO and Fast T2T? Moreover, is the decoding process conceptually similar to that used in diffusion-based LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NY0XUcAYIn", "forum": "psUjNnLhl9", "replyto": "psUjNnLhl9", "signatures": ["ICLR.cc/2026/Conference/Submission15615/Reviewer_34zW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15615/Reviewer_34zW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485364952, "cdate": 1761485364952, "tmdate": 1762925884715, "mdate": 1762925884715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MaskCO, a method that masks parts of optimal solutions and trains policies to reconstruct them. Through experiments on the TSP, CVRP, and MIS problems, the authors show the efficacy of their approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to read.\n- The proposed approach, MaskCO, tackles neural CO with a novel approach: mask portions of the solutions to generate more data."}, "weaknesses": {"value": "- My primary concern with the paper is that it requires expert solutions. In general, the community is moving away from supervised learning based methods towards RL.\n- The paper does not consider several SOTA baselines [1, 2].\n\n[1] Grinsztajn et al. Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization, NeurIPS 2023.\n\n[2] Hottung et al. PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization, ICLR 2025."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BUei0bc9uu", "forum": "psUjNnLhl9", "replyto": "psUjNnLhl9", "signatures": ["ICLR.cc/2026/Conference/Submission15615/Reviewer_vnPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15615/Reviewer_vnPK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940846363, "cdate": 1761940846363, "tmdate": 1762925884206, "mdate": 1762925884206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MaskCO, a new method that uses a masked generation approach, similar to techniques in NLP and computer vision. Instead of generating a whole solution at once, the model is trained to fill in missing parts of known good solutions, which helps it learn the important local patterns within those solutions. The experiments show that MaskCO works well on three different COP tasks:  the Traveling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), and Maximum Independent Set (MIS). During inference, it employs the 2-opt local search to further enhance the performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The self-supervised paradigm of masked generation in CO is interesting, although it was first noted in [1].\n2. The performance, easpecially on large-scale CVRP, is impressive.\n\n\n[1] Solving Diverse Combinatorial Optimization Problems with a Unified Model."}, "weaknesses": {"value": "1. The idea of masked generation for CO is similar to that in [1].\n\n2. Some equations and definitions in Sections 3 and 4 could be simplified; currently, they are unnecessarily complicated, which reduces the paper’s readability.\n\n3. The implementation details for the 2-opt heuristic are unclear. How does it, with the use of penalty terms, enforce constraint satisfaction?\n\n4. The effect of adding 2-opt appears minor for TSP but significantly different for CVRP, as shown in Table 11. Most importantly, on CVRP-500 and CVRP-1000, without 2-opt, MaskCO fails to generate feasible solutions. This raises concerns about its adaptability to more complex problems.\n\n5. The hyperparameters (e.g., $K$ and $p$) vary across problem types and sizes, as shown in Tables 12–15. It would be helpful to provide the rationale behind these choices. The results in Figures 2–9 demonstrate that model performance is highly sensitive to these hyperparameters."}, "questions": {"value": "How is MaskCO scalable? Any designs and empirical results to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kp00xJK34M", "forum": "psUjNnLhl9", "replyto": "psUjNnLhl9", "signatures": ["ICLR.cc/2026/Conference/Submission15615/Reviewer_VktH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15615/Reviewer_VktH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951736762, "cdate": 1761951736762, "tmdate": 1762925883757, "mdate": 1762925883757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MaskCO, a novel and compelling paradigm for Neural Combinatorial Optimization (NCO) inspired by the success of self-supervised masked auto-encoding in natural language processing and computer vision. The paper identifies a key limitation in\nexisting NCO methods: they typically treat solutions as monolithic objects during construction or improvement, which is data-inefficient and fails to utilise the local substructures embedded within high quality solutions.\n\nTo address this, MaskCO reframes the learning problem as solution-level self-supervised\nlearning. The core of the training methodology involves strategically masking portions of\nknown optimal or near-optimal solutions and training a model to reconstruct the missing\ncomponents. This approach improves data efficiency by transforming a single (instance,\nsolution) pair into a vast number of (instance, partial solution) training examples.This process compels the model to internalize fine-grained, localized decision patterns.\n\nFor inference, the paper proposes a \"mask-and-reconstruct\" iterative refinement procedure.\nThis algorithm begins with an initial solution and progressively improves it by repeatedly\nmasking a random subset of decision variables and using the trained model to regenerate\nthem in a single forward pass. This process effectively mimics a highly efficient, parallelized\nlocal search.\n\n\nThe authors validate their approach through extensive experiments on three CO problems: the\nTraveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), and the\nMaximum Independent Set (MIS). The results demonstrate that MaskCO achieves new\nstate-of-the-art performance, significantly outperforming prior neural solvers. The paper also shows that the learned representations are highly versatile, transferring effectively to alternative decoding methods and enabling a powerful self-training paradigm that works even without access to optimal solutions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Novel & Data-Efficient Method: It introduces a new learning paradigm by reframing optimization as a \"masked reconstruction\" task. This is highly data-efficient, as one optimal solution can be used to create a large number of training examples, forcing the model to learn robust local patterns.\n\nState-of-the-Art Performance & Speed: The model achieves high quality results on TSP, CVRP, and MIS. For example, on TSP-1000, it shows its 9x faster than the previous best neural solver, making its \"mask-and-reconstruct\" inference a highly efficient, parallelized local search.\n\nHigh-Quality, Versatile Representations: The learned representations are strong that the model can outperform other methods even when using their decoders. Furthermore, it enables a powerful \"optimal-solution-free\" mode where the model can teach itself, bootstrapping from weak solutions to high performance.\n\nSignificant quality improvements are shown on benchmark datasets such as TSPLIB,"}, "weaknesses": {"value": "Check questions"}, "questions": {"value": "1. What were the parameters used for the baselines? Were they default parameters from the existing papers or tuned for the target task, Eg:- for BQ-NCO Drakulic et al.  Request the authors to clarify this to ensure fairness of the setup. Were the number of training samples and the training samples used same for baselines and proposed method. A discussion on this would clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zkCfAnVuRI", "forum": "psUjNnLhl9", "replyto": "psUjNnLhl9", "signatures": ["ICLR.cc/2026/Conference/Submission15615/Reviewer_Sw1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15615/Reviewer_Sw1x"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174612268, "cdate": 1762174612268, "tmdate": 1762925883399, "mdate": 1762925883399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}