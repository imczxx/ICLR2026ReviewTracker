{"id": "RTTYGeC2Io", "number": 4027, "cdate": 1757585944704, "mdate": 1759898057197, "content": {"title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer", "abstract": "We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces a streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments.", "tldr": "Feedforward 4D reconstruction from causal videos.", "keywords": ["Monocular and Video 3D reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a771e65cfb92663c6d504d6230e8a9a3df92be7c.pdf", "supplementary_material": "/attachment/0623618f8e6e7b628a01191fd5af3b6ff26b0c51.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a streaming version of VGGT, which can process image sequences efficiently using causal attention.\nThe authors conduct extensive experiments to show the effectiveness and advantages of their method on various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow. It also provides a nice demo to show their performance.\n2. The authors conduct extensive experiments to show the effectiveness of their method."}, "weaknesses": {"value": "1. The backbone is highly related to that of VGGT, and the usage of causal attention do not have enough contribution considering the high computational cost when the input sequences are very long (same with VGGT).\n2. I think that since the authors highlighted the advantage of Stream3R-W in Table 4 when analyzing memory usage, its results should also be included in other performance experiments (like reconstruction) for comparison. After all, the memory usage of Stream3R without the window mechanism gradually exceeds that of CUT3R (also a streaming-based method). I believe that a comprehensive analysis of Stream3R and Stream3R-W would help provide a better evaluation of this paper.\n3. Reconstruction results on longer sequences. The sequences used in Table 3 are too short, which is not enough to evaluate a streaming-based method. Please provide more comparison with Spann3R, SLAM3R, and CUT3R on sequences at the intervals of 2, 10, 20, 40 (7-scenes and NRGBD).\n4. Please provide reconstruction results comparison on ETH3D datasets, like VGGT.\n\nPlease answer the above questions carefully and provide more thorough discussion and comparison. I will consider raising the score."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dEgmQWzK0O", "forum": "RTTYGeC2Io", "replyto": "RTTYGeC2Io", "signatures": ["ICLR.cc/2026/Conference/Submission4027/Reviewer_shrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4027/Reviewer_shrJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569305193, "cdate": 1761569305193, "tmdate": 1762917141179, "mdate": 1762917141179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Conducting feed-forward 3D reconstruction from monocular videos is nowadays a very hot topic and a series of works followed dust3R and VGGT. This work is an extension of VGGT to make reconstruction be able to support streaming input. The goal is reached by reformulating the traditional global attention of traditional vggt framework into a sequential attention mechanism. Experiments are sufficient to verify the superior performance of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The target problem to support feed-forward and streaming 3D reconstruction is timely, the proposed framework also makes sense. Assume the review does not need to consider concurrent works, I think the proposed framework is compatable with existing concurrent works in terms of both performance and novelty."}, "weaknesses": {"value": "I only have some minor concerns:\n- As shown in table 7, StreamVGGT is attending the comparison which is a concurrent work with similiar key idea. From the results, it seems the proposed method outperforms StreamVGGT, but with no any explanations. I understand this work is a concurrent work. However, the explanation about the differences bettween streamVGGT and the proposed method and why the results of the proposed method are better is helpful for understanding. \n\n- In the contribution list, there are 2 points are highlighted: 1) compatible with LLM-style training, allowding efficient and scalable context accumulation across frames; 2) supports both world- and local- pointmap and natually generallizes to large-scale novel view synthesis. However, there is no any experiments and discussions to support this."}, "questions": {"value": "No more."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ipU2RGDEPf", "forum": "RTTYGeC2Io", "replyto": "RTTYGeC2Io", "signatures": ["ICLR.cc/2026/Conference/Submission4027/Reviewer_vTS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4027/Reviewer_vTS9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879952045, "cdate": 1761879952045, "tmdate": 1762917140964, "mdate": 1762917140964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STREAM3R, a novel framework that reformulates sequential 3D reconstruction as a decoder-only causal transformer problem, inspired by autoregrssive-based LLMs. The core contribution is an efficient streaming architecture that processes frames sequentially, using causal attention to integrate geometric context from a growing KVCache of previously observed features. This design enables high scalability for long sequences and efficient inference by leveraging modern LLM-style techniques like windowed attention. The method achieves state-of-the-art or highly competitive performance on numerous 3D reconstruction and video depth benchmarks, outperforming prior streaming and even full-attention-based approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core architectural proposal to use a decoder-only causal transformer with a KVCache is a novel and highly effective paradigm for streaming 3D reconstruction. It offers a more scalable and powerful alternative to prior methods based on RNNs-like structure (like CUT3R) or expensive global optimization (like VGG-T or DUSt3R-GA).\n\n2. The method demonstrates sota competitive performance across a comprehensive suite of benchmarks, including video depth estimation, 3D reconstruction, and camera pose estimation. It consistently and significantly outperforms its most direct streaming competitor, CUT3R.\n\n3. The paper provides a thorough analysis of computational trade-offs. The STREAM3R-V-W[5] (windowed) variant is particularly noteworthy, achieving the fastest inference speed (32.9 FPS) among all streaming methods while simultaneously delivering top-tier reconstruction accuracy, demonstrating the practical viability of the approach. The memory usage analysis in Table 4 clearly validates the scalability claims."}, "weaknesses": {"value": "please refer to the weakness part."}, "questions": {"value": "1. The primary technical weakness is the inherent risk of error accumulation and drift common to online, sequential methods. The paper does not propose or evaluate explicit mechanisms to combat this (e.g., loop closure, keyframe-based optimization) and it is unclear how the system would perform on extremely long sequences (e.g., thousands of frames) beyond what was tested.\n\n2. The global coordinate system is anchored to the very first frame using a learnable [reg] token. The robustness of this simple mechanism is not ablated or discussed. It is unclear how the system would perform if the initial frame is of poor quality (e.g., blurry, featureless, or heavily occluded), which could compromise the entire reconstruction.\n\n3. A key claim is the compatibility with \"LLM-style training infrastructure,\" but the experiments primarily validate LLM-style inference techniques (KVCache, windowed attention). The paper does not include experiments applying advanced LLM-style training optimizations (e.g., specific curriculum strategies, advanced data parallelism, or training-focused attention mechanisms, which the authors themselves allude to in the limitations). This makes the claim about training benefits less substantiated than the clear inference benefits, and complementary experiments would strengthen this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Eg5UIBRgSZ", "forum": "RTTYGeC2Io", "replyto": "RTTYGeC2Io", "signatures": ["ICLR.cc/2026/Conference/Submission4027/Reviewer_JPV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4027/Reviewer_JPV9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993707573, "cdate": 1761993707573, "tmdate": 1762917140644, "mdate": 1762917140644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles sequential, feed-forward 3D reconstruction, whereas most existing feed-forward models require processing all images at once. The core idea is a causal Transformer in which intra-frame self-attention is followed by inter-frame cross-attention to cached tokens from past frames, enabling online processing with KV-cache efficiency. A windowed streaming mode retains the first frame (to preserve a canonical world frame) plus the most recent context, bounding memory while maintaining global consistency. The model outputs local/global pointmaps with confidences and camera pose, and the authors fine-tune DUSt3R and VGGT backbones on public datasets. Experiments span video depth estimation and 3D reconstruction, comparing against feed-forward baselines and their pose-graph/global-alignment (GA) extensions. Results show that the proposed method delivers higher accuracy, markedly higher throughput, and linear (or constant, in windowed mode) memory in streaming/online settings without test-time global alignment, with gains especially clear on long sequences and under tight memory constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### Originality\n\n* To the best of my knowledge, this is among the first works to extend feed-forward 3D reconstruction to sequential processing via a causal transformer, rather than relying on pose-graph/global alignment.\n* While it could use more elaboration, the [reg] token is an interesting mechanism to make the model explicitly aware of the anchor frame. In addition, omitting view embeddings is a novel choice that encourages order-agnostic generalization to different input image orders.\n\n### Quality\n\n* The comparisons against both feed-forward baselines and their global-alignment (GA) extensions are generally comprehensive, covering video depth and 3D reconstruction on short and long sequences.\n* The generality of the approach is validated by combining it with both DUSt3R and VGGT baselines.\n\n### Clarity\n\n* The paper is well organized and motivated. The method is clearly introduced, and the evaluations are easy to follow.\n\n### Significance\n\n* The solution enables sequential reconstruction without pose-graph alignment, making it potentially complementary to pose-graph methods under conditions such as small overlaps.\n* It imports ideas from LLMs into feed-forward 3D reconstruction, which can inspire follow-up research that leverages recent advances beyond just causal transformers.\n* The method is general and composable with other transformer-based feed-forward models, and its efficiency could enable deployment in compute-limited applications such as robotics."}, "weaknesses": {"value": "- Contribution may be limited: The gains appear to stem primarily from a causal transformer framework rather than components specific to 3D reconstruction. The paper should clarify what is fundamentally new beyond causal masking and standard transformer design, and what is uniquely tailored to 3D reconstruction.\n\n- Anchor design needs more elaboration: The proposed [reg] token is interesting, but there is no controlled comparison to simpler anchors such as a global CLS token or relative view positional embeddings. This makes it hard to judge when [reg] is necessary and sufficient. The paper also lacks experiments on unordered image collections that could demonstrate the hypothesized advantage.\n\n- Experimental scale and fairness: Most experiments rely on small-scale data (e.g., 7-scenes), which weakens claimsâ€”especially against pose-graph alignment methods. 7-scenes has strong inter-image connectivity and may not expose long-sequence drift. Since attention couples camera poses and depths, it is unclear whether bundle-adjustment refinement is applied, which affects fairness.\n\n- Missing robustness and failure analysis: The paper offers limited analysis of failure cases to clarify pros/cons relative to global alignment approaches."}, "questions": {"value": "- Please refere to the weakness\n- What happens if the canonical first frame is occluded or does not have overlap with latest frames? Can the model re-anchor and attentive to other frames?\n- How the performance is compared with VGGT-SLAM, which is a pose graph alignment based method built upon VGGT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1h3h5raa6M", "forum": "RTTYGeC2Io", "replyto": "RTTYGeC2Io", "signatures": ["ICLR.cc/2026/Conference/Submission4027/Reviewer_P6AK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4027/Reviewer_P6AK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4027/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002462401, "cdate": 1762002462401, "tmdate": 1762917140040, "mdate": 1762917140040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}