{"id": "9JjKTp8Jmy", "number": 12385, "cdate": 1758207475082, "mdate": 1759897513371, "content": {"title": "Soft Tokens, Hard Truths", "abstract": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.\n\nThis is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@$1$ and surpass them for pass@$32$, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.", "tldr": "We present the first scalable method to learn continuous CoTs via RL, matching discrete tokens at pass@1 and outperforming them at pass@32.", "keywords": ["reinforcement learning", "large language models", "math reasoning", "latent reasoning", "soft thinking", "continuous tokens", "reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6b34e0c12bc6b1e780220563522a478cfa2c7e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a simple, scalable way to train continuous Chain‑of‑Thought (CoT) with reinforcement learning by injecting Gaussian noise into probability‑mixture (“soft”/“fuzzy”) token embeddings during the CoT phase, making the thought trajectory stochastic and thus RL‑amenable. Using RLOO (leave‑one‑out REINFORCE) with Llama‑3 (3B/8B) and Qwen‑2.5 (3B) on math datasets (GSM8K, MATH, DeepScaleR), the authors report (i) pass@1 parity with discrete‑token CoT and (ii) consistent pass@32 gains, especially when training with soft/fuzzy CoT but performing hard (discrete) inference. They further show improved out‑of‑domain calibration (lower NLL on HellaSwag, ARC, MMLU) and an entropy analysis suggesting soft/fuzzy RL better preserves base‑model uncertainty than hard‑token RL. Figures 1–4 and Tables 1–2 (pp. 2, 7–9) summarize the core method and results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is clearly written, well organized, and reproducibility details are ample (prompts, verifier, hyperparameters, etc). Figure 1 crisply contrasts hard/fuzzy/soft generation; Figures 2–4 and Tables 1–2 present results cleanly. Authors use newline a lot of time even in the abstract. Sometimes, it is useful, but it is abundant, it might be distracting. I respect authors' decision in general, but I wanted to add a comment on this.\n\n* Conceptually, replacing discrete sampling with noisy mixtures to unlock RL for continuous CoT is neat and practical, requiring only noise at the embedding layer and storing $p_t$. Empirical gains are consistent in pass@32 and calibration, and the “train soft → infer hard” takeaway is actionable.\n\n* Adding Gaussian noise to $pE$ (soft) or near‑one‑hot $p$ at tiny temperature (fuzzy) yields a tractable REINFORCE signal with minimal overhead."}, "weaknesses": {"value": "* The paper says overhead is “minimal,” yet training uses G=32 samples per prompt and up to 512 CoT tokens, on 8×H100/H200 for 48–96 hours. A FLOPs comparison vs hard RL would strengthen the practicality claim.\n\n* The related work is good but incomplete. I did not see any citation regarding adding noise to enable exploration, which has a decent literature. The authors can take a look at these papers: Jain et al., NEFTune. Yadav & Singh, SymNoise. Fortunato et al., Noisy Networks for Exploration. For latent thinking, the literature is growing very quickly. The authors can take a look at these papers as well: Gozeten et al., Continuous Chain of Thought Enables Parallel Exploration and Reasoning and Yue et al., Hybrid Latent Reasoning via Reinforcement Learning.\n\n* For Llama‑3B on MATH, soft is ~4–5 points below hard. This means that pass@1 parity is not universal. The paper should nuance the claim and analyze when parity fails."}, "questions": {"value": "* Do soft/fuzzy CoTs exhibit interpretable superpositions (e.g., linearly separable sub‑paths in $\\tilde{h}_t^0)? Probing classifiers or SVCCA across diverse problems could show additional evidence about the mechanism. I am asking this question out of curiosity, and I wanted to know whether the authors tried probing before.\n\n* Soft/fuzzy are behaving in parallel to each other. Do the authors find any cases where soft/fuzzy behaves slightly different?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWLaasIOon", "forum": "9JjKTp8Jmy", "replyto": "9JjKTp8Jmy", "signatures": ["ICLR.cc/2026/Conference/Submission12385/Reviewer_kauv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12385/Reviewer_kauv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978866630, "cdate": 1761978866630, "tmdate": 1762923288024, "mdate": 1762923288024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a reinforcement learning approach to train CoT with \"soft\" tokens—probabilistic mixtures of token embeddings with added noise for exploration without requiring distillation from reference answers. The study validates that models trained this way match standard discrete CoT models on pass@1 accuracy and consistently outperform at pass@32 (diversity), while also providing insights into entropy profiles and robustness on both in-domain mathematical reasoning tasks and out-of-domain tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The soft CoT method proposed in the paper modestly improves pass@1 accuracy and significantly boosts pass@32 accuracy across multiple benchmarks. Meanwhile, both the soft and fuzzy approaches maintain pass@k curves that are close to the no-finetuning baseline when the value of k is large, preserving the model's foundational capabilities.\n\n2. This paper propose a practical, memory-efficient method to train continuous CoT for hundreds of reasoning steps.The RL-based training approach eliminates the dependency on reference discrete tokens, thereby removing the constraints on CoT length that plagued previous work.\n\n3. While achieving superior performance on the original datasets, soft and fuzzy methods have minimal impact on the base model's capabilities in domains outside mathematical reasoning, as evidenced by lower negative log-likelihood (NLL) on out-of-distribution tasks.\n\n4. This paper conducts a detailed analysis of entropy, figure 4 demonstrates a notable distinction between entropy behavior under hard CoT and soft CoT RL, connecting lower entropy overconfidence to potential gains in reasoning diversity and generalization."}, "weaknesses": {"value": "1. The performance improvement of soft CoT under pass@1 is not that significant. In some datasets and benchmarks, for example, when using Llama-3B-Instruct with DeepScalar on OlympiadBench—hard CoT seems to perform better. In this case, the pass@32 score of soft and fuzzy is higher, but it remains questionable whether this trade-off is worthwhile as both hard, soft, and fuzzy appear to have lower pass@32 scores than the base model.\n\n2. The paper does not analyse group size, which is a crucial hyper parameter of RL training, which affects model entropy and pass@k performance. Soft CoT can be seen as considering multiple possibilities simultaneously during the CoT process, imposing less restriction on entropy compared to hard CoT, thus achieving better pass@k performance. However, could increasing the rollout group size of hard CoT also improve its pass@k?\n\n3. Wang et al. (2022) (Self-Consistency for CoT) propose sampling multiple reasoning paths to improve  diversity. As pass@32 is a core claim, not including self-consistency as a baseline or in related work weakens the empirical case for the diversity benefit of soft CoT. While the paper discusses diversity and entropy, it's not clear whether the improvement is due to the token mixture, RL formulation, or just more stochasticity.\n\n4. The main innovation is the practical integration of noise for RL compatibility,  rather than a fundamentally new architecture. However, the paper lacks direct  experimental comparison with distillation-based soft CoT methods to demonstrate  the advantages of the RL approach."}, "questions": {"value": "1. Numerous related works have achieved considerable improvements in the field of RL + Math. Can fuzzy or soft CoT— which are orthogonal to RL methods— be applied to some of the current SOTA methods? Is the observed gain unique to continuous CoT RL, or mainly due to more diverse entropy? Would entropy  regularized hard CoT RL do as well?\n2. Why were Llama-8b-Instruct and Qwen-3b-Instruct only trained on the GSM8K or Math datasets, instead of using all three training datasets (as was done for Llama-3b-Instruct)? Is the training method of soft CoT sensitive to the choice of dataset?\n3. Have the authors explored generalization to non-math reasoning domains (e.g., code generation, commonsense multi-hop QA or other verifiable questions), do the claimed gains vanish as the underlying structure of reasoning becomes less sequential?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6a0SUTzzsa", "forum": "9JjKTp8Jmy", "replyto": "9JjKTp8Jmy", "signatures": ["ICLR.cc/2026/Conference/Submission12385/Reviewer_PsKn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12385/Reviewer_PsKn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995953529, "cdate": 1761995953529, "tmdate": 1762923287551, "mdate": 1762923287551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an algorithm for LM post-training that leverages the continuous space of hidden states to encourage exploration. This approach achieves performance comparable to or better than standard token-based methods in pass@1, and attains the best results on pass@k for larger values of k (compared to the hard baseline), suggesting greater diversity in the model’s outputs."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The method is simple and can be easily implemented for broad adaptation.\n- The analysis is thorough, covering out-of-distribution (OOD) tasks and multiple models.\n- The presentation and illustrations are clear and effectively aid understanding of the method."}, "weaknesses": {"value": "- Nearly all reported results overlap with the hard baseline (at least for pass@1). It remains unclear why this method is preferable—for instance, one could dynamically switch between a tuned model and a “No train” model depending on the required inference budget (k).\n- The authors avoid a direct comparison with [1]. Although they cite it in the related work, it seems relevant to consider whether training with that method’s type of noise—and then inferring it as soft or hard tokens—would yield similar benefits. Notably, [1] was published in August 2025, making the works concurrent; thus, a detailed comparison may not be expected.\n- In Figure 3, I recommend adding pass@64. It would be valuable to see whether the “No Finetune” baseline eventually surpasses the Soft and Fuzzy methods at higher k.\n\n[1] LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism of Soft Thinking. Junhong Wu et al."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nQL10LgB37", "forum": "9JjKTp8Jmy", "replyto": "9JjKTp8Jmy", "signatures": ["ICLR.cc/2026/Conference/Submission12385/Reviewer_gSxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12385/Reviewer_gSxV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997557817, "cdate": 1761997557817, "tmdate": 1762923287267, "mdate": 1762923287267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on designing a scalable training approach for continuous Chain-of-Thoughs (CoT). Towards this, the paper proposes a reinforcement learning method that feeds a \"soft token\" produced by an LLM as the input to the LLM at the next step after adding random Gaussian noise to the soft token. The paper also considers a variant of this approach which feeds a noisy \"fuzzy token\" as opposed to the noisy \"soft token\" as the input. The paper claims that they propose the first continuous CoT training approach that does not require ground-truth discrete CoT. The proposed method achieves pass@1 accuracy which is on-par that of discrete CoT while realizing higher pass@K performance. Furthermore, the proposed training approach ensures that the base model's out of domain generalization is unaffected. The experiments are conducted with the Llama 3 and Qwen 3 family models on multiple datasets, including GSM8K, MATH-500, and OlympiadBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a scalable training method for continuous CoT. \n- The proposed method achieves better pass@K than discrete CoT. \n- The proposed method preserves model performance on out of domain tasks."}, "weaknesses": {"value": "- The paper does not provide an adequate discussion of prior works that propose reinforcement learning approaches for continuous CoT. E.g., see https://arxiv.org/pdf/2505.18454 and https://arxiv.org/abs/2505.23648. Could authors provide a discussion on why their approach adds significant novelty on top of such existing works?\n- The main contribution of the paper appears to be limited to adding the noise to soft/fuzzy tokens before feeding it as an input. However, the paper does not provide a detailed treatment of this approach. Why is Gaussian choice the best approach? Even within the Gaussian noise, how noise parameters affect the overall performance. \n- The paper shows that soft/fuzzy training followed by hard sampling (discrete CoT) performs the best. However, the paper does provide any meaningful analysis/discussion on why such a discrepancy between the training and test time leads to the best performance.\n- It appears that for pass@1, continuous CoT does not outperform discrete CoT. But does continuous CoT provide inference efficiency benefits such as smaller thought lengths, especially when continuous CoT is employed at the test time?"}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2rAO7zcCC9", "forum": "9JjKTp8Jmy", "replyto": "9JjKTp8Jmy", "signatures": ["ICLR.cc/2026/Conference/Submission12385/Reviewer_vwfP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12385/Reviewer_vwfP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317003559, "cdate": 1762317003559, "tmdate": 1762923286734, "mdate": 1762923286734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}