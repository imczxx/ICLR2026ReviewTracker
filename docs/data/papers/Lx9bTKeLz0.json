{"id": "Lx9bTKeLz0", "number": 1127, "cdate": 1756846035344, "mdate": 1763703144867, "content": {"title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning", "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also *think with images*: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a *think about images* paradigm, where images are regarded as static inputs. To address this gap, we introduce VisToolBench, a vision tool-use reasoning benchmark that rigorously evaluates MLLMs’ ability to perceive, transform, and reason across complex visual–textual tasks under the *think with images* paradigm. VisToolBench comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.44\\% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on *think with images*, VisToolBench offers critical insights for advancing visual intelligence in MLLMs.", "tldr": "We present VisToolBench, the first vision tool-use reasoning benchmark that requires model think with images.", "keywords": ["Multi-Modal", "MLLM", "Vision Tool Use", "Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36c9dd9f62961eed4b0b3e3aa975f4efd521e2c1.pdf", "supplementary_material": "/attachment/4faecfe3867bac71591a9a3261e7d898383bf433.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VISTOOLBENCH, a novel benchmark designed to evaluate MLLMs under the think with images paradigm. The benchmark comprises 1,204 open-ended vision tasks across five domains. Through rigorous evaluation of 16 representative MLLMs, the paper shows that current models struggle with integrating vision and tools and identifies divergent tool-use behaviors across models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and presented. The questions in VISTOOLBENCH are well-designed and carefully collected.\n2. The evaluation is both comprehensive and detailed. VISTOOLBENCH devises a nuanced, rubric-based evaluation framework that moves beyond simple binary accuracy. This method enables fine-grained assessment across multiple dimensions and successes of each model's reasoning process.\n3. The empirical results and analysis are valuable for the tool use of MLLMs. The paper benchmarks the performance of 16 SOTA models and uncovers critical tool-use behaviors between different models. These findings offer actionable insights for improving MLLMs' ability to strategically manipulate visual information and reason effectively in complex, real-world applications."}, "weaknesses": {"value": "1.\tThe paper emphasizes the importance of tool use. However, the error analysis section 3.3 identifies model weaknesses such as visual perception errors, reasoning mistakes and calculation errors that do not appear to directly stem from tool usage. These error types are generally applicable to almost all multimodal evaluation tasks, regardless of whether tools are involved or successfully utilized.\n2.\tIn Figure 7, the experiments show that Gemini 2.5 Pro actually performs 2.7% better after the tool is removed. The paper attributes this counterintuitive result to its “stronger native visual capabilities” and “possibly limited exposure to tool-use training.” This intriguing anomaly warrants deeper investigation. Did the model make incorrect tool calls, or did tool integration interfere with its otherwise strong visual reasoning process? Furthermore, since the effectiveness seems heavily influenced by the strength of the system prompt, are the experimental results sufficiently generalizable and robust?\n3.\tIt would be valuable to include results from more recent open-source models such as Qwen3-VL, as well as tool-augmented open-source models like DeepEyes [1] et al.\n4.\tThe paper selects tasks that models like o3 and o4-mini cannot handle, which may raise potential concerns about selection bias when arguing that GPT-5-think and other models exhibit lower success rates.\n\n[1] DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning, 2025."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jr3FciI7tl", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Reviewer_64fz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Reviewer_64fz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460795168, "cdate": 1761460795168, "tmdate": 1762915686933, "mdate": 1762915686933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response Part I"}, "comment": {"value": "We thank the reviewers for the thoughtful and constructive feedback. Reviewers highlighted that our work clearly identifies an important gap in current MLLM evaluation, distinguishing between **thinking about images** and **thinking with images**, and emphasized the value of enabling models to actively manipulate imperfect, real-world visual inputs [97Na, 5rAQ]. We also appreciate the recognition of the breadth and realism of VisToolBench, which contains 1,204 high-quality tasks across five domains and is supported by a careful, multi-stage human-annotated data collection and review process [97Na, 5rAQ, dBhG, 64fz]. We also appreciate the reviewers recognition of rubric-based grading that help reveal more detailed model behaviors beyond simple accuracy [97Na, 5rAQ, dBhG, 64fz]. Reviewers also found the empirical results informative, especially the consistently low pass rates of state-of-the-art models and the clear differences in tool-use patterns across systems, which point to concrete directions for improving MLLMs [97Na, 5rAQ, dBhG, 64fz]. We are grateful that the research design and experimental analysis were viewed as both innovative and practical [5rAQ]. Below we first address two main comments mentioned by multiple reviewers, and we will address each point raised by the reviewers separately. \n\n### Fine-grained vision tool-use error analysis\n\nWe thank the reviewers for encouraging a more fine-grained examination of tool-use behaviors. Our primary metrics, APR and ASR, evaluate the correctness and faithfulness of the model’s final answers, and we additionally report three high-level tool-use metrics: tool-call success rate, tool-call proactivity, and tool-call volume. We fully agree that a deeper analysis of vision tool-use failures can offer more actionable insights for model development.\n\nTo this end, we have incorporated a detailed tool-use failure analysis covering **eleven categories** of vision tool-related errors, including visual perception mistakes (after vision tool call), incorrect cropping, incorrect rotation/flip operations, OCR errors, distractions, and incorrect tool selection, etc. To support large-scale evaluation, we prompt GPT-5 with the question, the model’s tool-use chain, the golden answer, and the model’s actual answer to generate an initial failure-category prediction. Human supervision is then applied to filter out any inappropriate or incorrect categories. As shown in the table below, visual perception remains the dominant failure mode across all models (GPT-5: 34.40%, Gemini-2.5-Pro: 32.56%, Claude-Opus-4.1: 39.46%), and errors such as incorrect cropping and execution failures also occur frequently (e.g., GPT-5: 14.68% cropping errors and 15.14% execution errors).\n\n| Failure Type                 | GPT-5 (%) | Gemini-2.5-pro (%) | Claude-opus-4.1 (%) |\n|-----------------------------|-----------|----------------------|-----------------------|\n| Visual Misinterpretation    | 34.40    | 32.56              | 39.46               |\n| OCR Error                   | 11.01    | 12.21               | 7.62                |\n| Distraction                 | 8.26     | 4.07               | 10.76               |\n| Tool Execution Error        | 15.14    | 12.79               | 6.28               |\n| Incorrect Tool Selection    | 2.75     | 3.49               | 5.83                |\n| Incorrect Cropping          | 14.68    | 12.21               | 17.94              |\n| Incorrect Rotation/Flipping | 5.05     | 5.23                | 5.38                |\n| Incorrect Enhancing         | 1.83     | 0.58                | 1.35               |\n| Incorrect Editing           | 0.00     | 0.00               | 0.45                |\n| Incorrect Resizing          | 0.46     | 0.00                | 0.00                |\n| Other                       | 6.42     | 16.86               | 4.93            |\n| **Total Count**  | **218**  |**172** | **223**    | \n\nWe emphasize that reasonably good visual perception is essential for enabling effective tool use in our benchmark. For example, some tasks require the model to first localize a small object or region. However, as our qualitative examples in show, models often fail to propose accurate cropping coordinates in one shot, leading to zooming into irrelevant regions, and this makes models to perform multiple unnecessary tools calls (see examples in Appendix F.3 in our revised paper), which may propagate downstream errors.\n\nOverall, we believe that this fine-grained error analysis provides process-level insight into where current MLLMs struggle, complementing our existing metrics and strengthening the evaluation framework. We have included this analysis in Appendix B.7 in the revised paper (highlighted in blue)."}}, "id": "PC3Ql6p8sK", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697606644, "cdate": 1763697606644, "tmdate": 1763703683564, "mdate": 1763703683564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response Part I"}, "comment": {"value": "We thank the reviewers for the thoughtful and constructive feedback. Reviewers highlighted that our work clearly identifies an important gap in current MLLM evaluation, distinguishing between **thinking about images** and **thinking with images**, and emphasized the value of enabling models to actively manipulate imperfect, real-world visual inputs [97Na, 5rAQ]. We also appreciate the recognition of the breadth and realism of VisToolBench, which contains 1,204 high-quality tasks across five domains and is supported by a careful, multi-stage human-annotated data collection and review process [97Na, 5rAQ, dBhG, 64fz]. We also appreciate the reviewers recognition of rubric-based grading that help reveal more detailed model behaviors beyond simple accuracy [97Na, 5rAQ, dBhG, 64fz]. Reviewers also found the empirical results informative, especially the consistently low pass rates of state-of-the-art models and the clear differences in tool-use patterns across systems, which point to concrete directions for improving MLLMs [97Na, 5rAQ, dBhG, 64fz]. We are grateful that the research design and experimental analysis were viewed as both innovative and practical [5rAQ]. Below we first address two main comments mentioned by multiple reviewers, and we will address each point raised by the reviewers separately. \n\n### Fine-grained vision tool-use error analysis\n\nWe thank the reviewers for encouraging a more fine-grained examination of tool-use behaviors. Our primary metrics, APR and ASR, evaluate the correctness and faithfulness of the model’s final answers, and we additionally report three high-level tool-use metrics: tool-call success rate, tool-call proactivity, and tool-call volume. We fully agree that a deeper analysis of vision tool-use failures can offer more actionable insights for model development.\n\nTo this end, we have incorporated a detailed tool-use failure analysis covering **eleven categories** of vision tool-related errors, including visual perception mistakes (after vision tool call), incorrect cropping, incorrect rotation/flip operations, OCR errors, distractions, and incorrect tool selection, etc. To support large-scale evaluation, we prompt GPT-5 with the question, the model’s tool-use chain, the golden answer, and the model’s actual answer to generate an initial failure-category prediction. Human supervision is then applied to filter out any inappropriate or incorrect categories. As shown in the table below, visual perception remains the dominant failure mode across all models (GPT-5: 34.40%, Gemini-2.5-Pro: 32.56%, Claude-Opus-4.1: 39.46%), and errors such as incorrect cropping and execution failures also occur frequently (e.g., GPT-5: 14.68% cropping errors and 15.14% execution errors).\n\n| Failure Type                 | GPT-5 (%) | Gemini-2.5-pro (%) | Claude-opus-4.1 (%) |\n|-----------------------------|-----------|----------------------|-----------------------|\n| Visual Misinterpretation    | 34.40    | 32.56              | 39.46               |\n| OCR Error                   | 11.01    | 12.21               | 7.62                |\n| Distraction                 | 8.26     | 4.07               | 10.76               |\n| Tool Execution Error        | 15.14    | 12.79               | 6.28               |\n| Incorrect Tool Selection    | 2.75     | 3.49               | 5.83                |\n| Incorrect Cropping          | 14.68    | 12.21               | 17.94              |\n| Incorrect Rotation/Flipping | 5.05     | 5.23                | 5.38                |\n| Incorrect Enhancing         | 1.83     | 0.58                | 1.35               |\n| Incorrect Editing           | 0.00     | 0.00               | 0.45                |\n| Incorrect Resizing          | 0.46     | 0.00                | 0.00                |\n| Other                       | 6.42     | 16.86               | 4.93            |\n| **Total Count**  | **218**  |**172** | **223**    | \n\nWe emphasize that reasonably good visual perception is essential for enabling effective tool use in our benchmark. For example, some tasks require the model to first localize a small object or region. However, as our qualitative examples in show, models often fail to propose accurate cropping coordinates in one shot, leading to zooming into irrelevant regions, and this makes models to perform multiple unnecessary tools calls (see examples in Appendix F.3 in our revised paper), which may propagate downstream errors. We also included several model success examples, failure examples, and transformed images using vision tools in Appendix F.2 to F.4 in the revised paper.\n\nOverall, we believe that this fine-grained error analysis provides process-level insight into where current MLLMs struggle, complementing our existing metrics and strengthening the evaluation framework. We have included this analysis in Appendix B.7 in the revised paper (highlighted in blue)."}}, "id": "PC3Ql6p8sK", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697606644, "cdate": 1763697606644, "tmdate": 1763747349366, "mdate": 1763747349366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VisToolBench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in active visual reasoning in terms of perceiving, transforming, and reasoning with images rather than treating them as static inputs. The benchmark includes 1.2k complex, open-ended tasks across five domains, with detailed evaluation rubrics. Experiments show that current MLLMs struggle with such tasks, achieving low performance. The results reveal significant variation in tool-use behavior across models, highlighting the need for better integration of visual perception and tool-based reasoning in future MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a large collection of complex VQA tasks with high-quality annotations spanning diverse topics. This dataset will serve as a valuable resource for future evaluations of perception and visual reasoning models.\n\n2. The human-annotated process scores provides an excellent foundation for assessing the faithfulness of reasoning trajectories generated by multimodal LLMs.\n\n3. The introduction of the “proactive” rubric is particularly valuable, as it captures a key aspect of tool-using and agentic LLM behavior that is essential for evaluating real-world applications."}, "weaknesses": {"value": "1. The paper, \"VisToolBench,\" lacks clarity on how tools are actually called, particularly regarding the “reference-tool-use chain” described in Section 2.2. This crucial implementation detail is also missing from the accompanying Hugging Face page. Based on the system prompts shown in Appendix C.2, the tool usage appears to rely solely on plain text prompts without any explicit tool-calling format or structured invocation instructions. Furthermore, Appendix D.1 is insufficient for understanding the system’s behavior — it only presents text-based tool trajectories, without showing concrete results or demonstrations of the tool operations.\n\n2. The core motivation of the paper is to evaluate tool-using multimodal LLMs, yet the proposed dataset mainly consists of visual detail search tasks with perturbed images (flipped, etc). The paper does not sufficiently justify why tool use is necessary for these tasks. Tools like historical_weather may be essential for answering a question but there lacks a quantitative results. To strengthen the argument, it would be valuable to provide baseline comparisons where tool use is explicitly disabled (e.g., prompting closed-source MLLMs with “please do not call any tools”) or by evaluating vanilla models such as Qwen, LLaMA, or Gemma. This would better demonstrate the actual benefit and necessity of tool usage for the benchmark.\n\n3. The two main criteria, APR and ARS, primarily evaluate the model's accuracy and the faithfulness of its CoT trajectory. They do not directly assess the quality or correctness of the tool-using process itself (e.g., tool selection, argument formulation, or interpreting tool output). The evaluation is thus mismatched with the paper's stated goal of evaluating tool-using MLLMs."}, "questions": {"value": "My concerns on the main topic of tool-using is elaborated in Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BIe6cnPgJb", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Reviewer_dBhG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Reviewer_dBhG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540726605, "cdate": 1761540726605, "tmdate": 1762915686808, "mdate": 1762915686808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **VISTOOLBENCH**, a novel benchmark designed to evaluate Multimodal Large Language Models (MLLMs) under the \"think with images\" paradigm—an important shift from existing benchmarks that treat images as static inputs (\"think about images\"). The work addresses a critical gap in real-world MLLM deployment: dynamic, tool-augmented reasoning over imperfect or complex visual inputs. The benchmark’s design (task diversity, rigorous data collection, nuanced evaluation metrics) and comprehensive experimental analysis (16 models, tool-use behavior, error diagnosis) make it a valuable contribution to multimodal AI research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Most existing benchmarks are limited to \"static image input + passive reasoning\". However, in real-world scenarios, user images often have issues such as rotation and underexposure, requiring dynamic tool operations (cropping, enhancement, etc.) to extract key information. The \"think with images\" paradigm proposed in the paper transforms visual input from a \"passive context\" into an \"operable cognitive space\", addressing the core pain points in the practical application of MLLMs.\n\n2. The tasks span 5 domains: STEM, healthcare, finance, sports, and general use, encompassing both single-turn (603) and multi-turn (601) interactions. Moreover, the task design simulates real user needs (such as menu price calculation and medical image interpretation), avoiding the limitations of synthetic scenarios.\n\n3. High-quality tasks are selected through a multi-stage process of \"contributor training → initial design → model pre-evaluation (to ensure task difficulty) → two rounds of review\"."}, "weaknesses": {"value": "1. The \"specific scenario coverage\" of tasks in various domains is not clearly defined (for example, in the medical field, only \"medical-related tasks\" are mentioned, without specifying whether sub-scenarios such as medical image interpretation and medical record image analysis are included), which may affect the evaluation of the benchmark's adaptability to different scenarios.\n2. The distribution of \"image types\" in the task (such as the proportion of photos, charts, screenshots, etc.) is not specified. If a certain type of image accounts for a too high proportion, it may cause the model evaluation results to be biased towards the processing ability of that type of image rather than the general visual tool ability.\n3. The paper points out that \"visual perception errors account for the highest proportion,\" but it does not further break them down into subcategories such as \"regional positioning errors (incorrect ROI cropping),\" \"insufficient image enhancement (failure to brighten key information),\" and \"OCR text extraction errors,\" making it impossible to accurately identify the shortcomings in the model's use of visual tools.\n4. The human-LLM consistency for subjective indicators (such as \"answer clarity\") is only 73.96%-81.77%, and there is no explanation on how to handle such inconsistencies (e.g., whether to adopt multi-LLM voting or optimize prompts), which may affect the credibility of the scoring results."}, "questions": {"value": "The VISTOOLBENCH benchmark proposed in this paper fills the gap in evaluating the \"thinking with images\" capability of MLLMs. The research design is innovative and practical, and the experimental analysis is in-depth. If the transparency of tasks, evaluation metrics, and experimental fairness can be optimized according to the above suggestions, the academic value and persuasiveness of the paper will be further enhanced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5NpHfl06BF", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Reviewer_5rAQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Reviewer_5rAQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803953708, "cdate": 1761803953708, "tmdate": 1762915686681, "mdate": 1762915686681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focus on accessing the ability of Large Vision-Language Models (LVLMs) to \"think with images\"—that is, to actively manipulate images (e.g., crop, edit, enhance) and integrate other tools to solve complex problems. The authors introduce a new benchmark named VISTOOLBENCH, specifically designed to evaluate MLLMs under the \"think with images\" paradigm. Tasks are designed to necessitate the simultaneous use of \"vision tools\" (like image processing libraries) and \"general-purpose tools\" (like web search, calculators, and Python interpreters) for their solution. It employs a detailed, rubric-based evaluation system instead of simple binary (correct/incorrect) judgments to provide more nuanced diagnostics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper accurately identifies a critical gap in current MLLM evaluation. The distinction between \"thinking about\" vs. \"thinking with\" images is clear and crucial, as the latter is vital for the real-world deployment of MLLMs (handling imperfect, user-provided photos).\n2. The design of VISTOLLBENCH is comprehensive. It contains 1,204 tasks across 5 professional domains, providing sufficient volume and broad coverage.\n3. Moving beyond binary accuracy, the use of weighted rubrics (assessing understanding, truthfulness, reasoning, etc.) allows for a deeper diagnosis of where models fail.\n4. The experimental results (SOTA model at 18.44% pass rate) are striking and clearly expose the shortcomings of current models. The divergence between GPT and Gemini in tool-use benefit is a particularly interesting and thought-provoking insight."}, "weaknesses": {"value": "1. **Confation in Visual Tool Design:** The core \"vision tool\" is python_image_processing, which requires the MLLM to generate Python (PIL/OpenCV) code to perform image operations. This design conflates two different capabilities: A) The model's \"reasoning ability\" for visual operations (knowing what to do, e.g., \"crop the menu area\") and B) The model's \"programming ability\" (being able to write correct Python code, e.g., img.crop((x1, y1, x2, y2))). A model might be strong in A but weak in B, leading to task failure. A cleaner design would provide \"atomic\" tools (e.g., crop(region_name) or enhance(type)) to more purely evaluate visual reasoning, not programming.\n\n2. **Vagueness in Error Attribution:** The paper attributes 70%-80% of failures to \"Visual Perception Error,\" which is an overly broad and potentially misleading conclusion. Combined with point 1, this \"perception error\" is not fine-grained. Does it mean:\n\n- (a) The model failed to realize it needed a tool? (This is a reasoning failure.)\n\n- (b) The model realized it, but wrote incorrect code for the tool? (This is a tool-use failure.)\n\n- (c) The model successfully executed the tool (e.g., correct crop), but still couldn't see the details in the transformed image? (This is a true perception failure.)\n\nThe paper does not (or cannot) clearly decouple these scenarios, making it difficult to pinpoint the true bottleneck.\n\n3. **Lack of Comparison to Relevant SOTA Models:** The paper claims to evaluate the tool-use capabilities of MLLMs, but the main results lack an evaluation of several SOTA models that have already explored this domain (tool-calling and active perception).\n- Thyme: Think Beyond Images\n- Reinforced Visual Perception with Tools\n- Qwen3-VL\n- Seed1.6"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "juPFdSg6vw", "forum": "Lx9bTKeLz0", "replyto": "Lx9bTKeLz0", "signatures": ["ICLR.cc/2026/Conference/Submission1127/Reviewer_97Na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1127/Reviewer_97Na"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908975486, "cdate": 1761908975486, "tmdate": 1762915686568, "mdate": 1762915686568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}