{"id": "RGHxlzhQLN", "number": 16430, "cdate": 1758264423325, "mdate": 1759897241187, "content": {"title": "Adaptive Curriculum Strategies: Stabilizing Reinforcement Learning for Large Language Models", "abstract": "Curriculum learning has shown promise for enhancing Large Language Models (LLMs) through progressive difficulty management, yet existing approaches suffer from instability issues when applied to reinforcement learning paradigms. Existing curriculum-based RL training exhibits catastrophic performance collapse during difficulty transitions, particularly when models encounter samples beyond their current capabilities. This instability stems from rigid curriculum designs that fail to adapt to individual model characteristics and learning trajectories. To address these limitations, we propose Adaptive Curriculum Strategies (ACS), a framework that promotes stable and effective training throughout curriculum progression. Our approach introduces model-specific difficulty calibration that adapts to each model's capabilities, and ``Guided Prompting'' that transforms challenging samples to prevent training instability. Experiments demonstrate that ACS prevents performance collapse in traditional curriculum RL training, achieving substantial improvements across five mathematical reasoning benchmarks while enhancing training stability.", "tldr": "", "keywords": ["Mathematical Reasoning;Large Language Models;Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3d9ea697cbaf7c380287c060935aa0d5df9508c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Adaptive Curriculum Strategies (ACS) to stabilize reinforcement learning for LLM mathematical reasoning by (a) model-specific difficulty calibration via multi-sample accuracy per item and (b) Guided Prompting that transforms hard problems with partial-solution hints until the model meets a stability threshold, combined with staged GRPO training and a curriculum review data-mixing strategy to mitigate collapse and forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear identification of instability during curriculum stage transitions in RL fine-tuning, with an operational definition and visualizations illustrating catastrophic drops without stability mechanisms.\n- Concrete, implementable pipeline: per-sample multi-draw accuracy calibration, guided hinting with thresholds, and staged GRPO augmented with a curriculum review mixing policy that reduces forgetting.\n- Consistent gains across five math benchmarks on two model sizes, plus cross-model results on DeepSeek-Math-7B-Instruct; ablations indicate curriculum review outperforms naive staging."}, "weaknesses": {"value": "- Narrow domain scope: all tasks are mathematical reasoning; there is no evidence ACS generalizes to code, QA, multi-turn dialogue, or retrieval-augmented regimes, limiting external validity of stability claims.\n- Stability attribution is under-isolated: guided prompting, data partitioning, GRPO modifications, and review mixing change simultaneously; ablations do not fully disentangle which component prevents collapse under consistent compute budgets.\n- Guided Prompting may leak reference solution structure into training examples, risking distribution shift and overfitting; safeguards and analyses of hint-length sensitivity or label leakage are not provided.\n- Difficulty calibration relies on n=16 sampled generations per item with temperature 0.7; the resulting ACC is stochastic and decoding-dependent, yet robustness to n, temperature, and evaluator scripts is not quantified.\n- The stability-aware GRPO objective is presented, but theoretical guarantees on stability (e.g., monotone improvement, bounded gradient variance across curriculum transitions) are not established, leaving “stability” as an empirical observation.\n- Baseline protocols vary in ways that may advantage ACS (e.g., discarding vs retaining hard samples, or using fixed external assessors) without strong hyperparameter sweeps or compute parity evidence across methods."}, "questions": {"value": "- How robust are the results to different n and temperature choices in the calibration step, and do deterministic decoding or alternative evaluators (e.g., verifier models) change partitioning outcomes?​\n- Does Guided Prompting induce dependency on solution prefixes at inference time, and how does performance change if hints are removed post-training or restricted to schematic advice rather than literal step prefixes?​\n- Can the components be ablated under equalized compute to quantify each contribution to stability: calibration only, prompting only, review only, and GRPO stability term only?​\n- How does ACS behave on non-math tasks (e.g., GSM8K vs HotpotQA vs code benchmarks) and with retrieval-augmented inputs, where difficulty and instability arise from different factors?​\n- What are memory/latency impacts in large-scale settings: calibrating 100k–1M samples, longer max lengths, larger candidate counts, and bigger models; is <5% overhead still valid?​\n- Could a verifier-based or cost-sensitive calibration (penalizing formatting or compute) outperform raw ACC, and does mixing by calibrated uncertainty (rather than discrete tertiles) improve stability?​"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CLblNNlvcs", "forum": "RGHxlzhQLN", "replyto": "RGHxlzhQLN", "signatures": ["ICLR.cc/2026/Conference/Submission16430/Reviewer_31BR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16430/Reviewer_31BR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989979049, "cdate": 1761989979049, "tmdate": 1762926547919, "mdate": 1762926547919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies why curriculum learning combined with RL for LLM mathematical reasoning often collapses at stage transitions and proposes Adaptive Curriculum Strategies (ACS) to keep training stable and effective. The core idea is to calibrate difficulty for the current model via multi-sample accuracy estimates, transform over-difficult items with Guided Prompting (prefix hints from the reference solution) so they stay learnable. Experiments across different math benchmarks and model sizes show ACS removes the catastrophic drops seen in naïve curricula and improves average performance"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation and the analysis on the collapses at difficulty stage transitions is interesting.\n\n2. The studied problem is important.\n\n3. Different model families (both Qwen and DeepSeek Math) are involved in the experiments, demonstrating the generalizability of the proposed method across different models.\n\n4. Guided prompting method is reasonable."}, "weaknesses": {"value": "1. This paper does not discuss and compare against many existing curriculum-learning methods for reinforcement learning, despite a growing body of existing works (see references below). The reported baselines are naive heuristics, which makes it hard to compare the proposed methed and other advanced curriculum learning methods. In addition, the related-work section does not adequately position the method within prior curriculum strategies on RL. A better evaluation should includes more existing RL curriculum learning methods. It is also suggested to expand the related-work discussion to clarify what is new versus known in curriculum design for LLM RL and traditional RL. Some of the existing methods listed below are also adaptive curriculum instead of learning via fixed difficulies orders/phases.\n\n2. All experiments use GRPO; there is no evaluation against other LLM-RL algorithms such as PPO, DAPO, or Reinforcement++. This makes it unclear whether ACS is only applicable to GRPO. Adding PPO/DAPO/Reinforcement++ experiments, would make the experiments more comprehensive and verify the generalizability of the proposed method.\n\nZhang et al., Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation. EMNLP 2025.\n\nTzannetos et al., Proximal Curriculum for Reinforcement Learning Agents. TMLR 2023.\n\nShi et al., Efficient Reinforcement Finetuning via Adaptive Curriculum Learning\n\nParashar et al., Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning\n\nWang et al., DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training\n\nChen et al., Self-Evolving Curriculum for LLM Reasoning\n\nBae et al., Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hhroWUoVmJ", "forum": "RGHxlzhQLN", "replyto": "RGHxlzhQLN", "signatures": ["ICLR.cc/2026/Conference/Submission16430/Reviewer_K2EF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16430/Reviewer_K2EF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135082094, "cdate": 1762135082094, "tmdate": 1762926546776, "mdate": 1762926546776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed to improve reinforcement training using curriculum strategy. Compared to existing literature that studies curriculum assisted RL training, it adopts a more robust difficulty measurer and proposes guided prompting to mitigate instability training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The authors have conducted extensive experiments on three backbone models, namely Qwen2.5 Math 1.5B, Qwen2.5 Math 7B and Deepseek Math 7B Instruct."}, "weaknesses": {"value": "Weaknesses and Questions:\n1. For section 3.1, I think the curriculum strategy adopted in this paper is basically BabyStep[1], with the step number set to 3. Although the difficulty is calculated based on the current accuracy, it is still pre-defined. Since the authors claim to \"adapt sample assessment to each model's evolving capabilities\", I believe Self-paced Learning[2] (SPL) will be a much better choice. SPL is a variant of automatic curriculum learning[3] which adopts a dynamic scheduler. For some variant of SPL, samples may be assigned a dynamic weight based on the current capability of the base model, which may avoids the problem of training instability when advancing to the next stage.\n2. I believe a large part of the contribution lies in the curriculum strategy applied in reinforcement learning. However, some hyper-parameters for curriculum learning such as step number are not ablated. Meanwhile, only two variants of curriculum learning strategies are experimented, and for Curriculum Review, can the authors specify exactly how many easier samples are incorporated? And is the proportion static across different models? \n3. For guided prompting, the difficulty of training samples is manually reduced by providing “hints.” However, wouldn’t this approach potentially weaken the model’s ability to handle difficult samples, since it only encounters samples with hints? Is there a mechanism to gradually remove these hints during training? Moreover, the hyper-parameters — the hint ratio \\alpha and threshold \\tau — are not specified or ablated, even though they may play a critical role in the effectiveness of this strategy. Are these parameters kept static across different models?\n\nReferences:\n\n[1] Spitkovsky, V. I., Alshawi, H., & Jurafsky, D. (2010, June). From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 751-759).\n\n[2] Wang, X., Chen, Y., & Zhu, W. (2021). A survey on curriculum learning. IEEE transactions on pattern analysis and machine intelligence, 44(9), 4555-4576.\n\n[3] Tullis, J. G., & Benjamin, A. S. (2011). On the effectiveness of self-paced learning. Journal of memory and language, 64(2), 109-118."}, "questions": {"value": "Please refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6RZp1noAxb", "forum": "RGHxlzhQLN", "replyto": "RGHxlzhQLN", "signatures": ["ICLR.cc/2026/Conference/Submission16430/Reviewer_jHpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16430/Reviewer_jHpU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762226827376, "cdate": 1762226827376, "tmdate": 1762926546334, "mdate": 1762926546334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}