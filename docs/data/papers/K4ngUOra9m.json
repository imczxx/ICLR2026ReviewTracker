{"id": "K4ngUOra9m", "number": 25348, "cdate": 1758367043581, "mdate": 1759896723819, "content": {"title": "Masked Skill Token Training for Hierarchical Off-Dynamics Transfer", "abstract": "Generalizing policies across environments with altered dynamics remains a key challenge in reinforcement learning, particularly in offline settings where direct interaction or fine-tuning is impractical. We introduce Masked Skill Token Training (MSTT), a fully offline hierarchical RL framework that enables policy transfer using observation-only demonstrations. MSTT constructs a discrete skill space via unsupervised trajectory tokenization and trains a skill-conditioned value function using masked Bellman updates, which simulate dynamics shifts by selectively disabling skills. A diffusion-based trajectory generator, paired with feasibility-based filtering, enables the agent to execute valid, temporally extended actions without requiring action labels or access to the target environment. Our results in both discrete and continuous domains demonstrate the potential of mask-guided planning for robust generalization under dynamics shifts. To our knowledge, MSTT is the first work to explore masking as a mechanism for simulating and generalizing across off-dynamics environments. It marks a promising step toward scalable, structure-aware transfer and opens avenues to explore multi-goal conditioning, and extensions to more complex, real-world scenarios.", "tldr": "", "keywords": ["Tranfser Learning", "Skills", "Hierarchical RL", "Embodied AI"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9f5c6214a2e0cfdadef9431dd4cc79a24ed9296.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MSTT, a hierarchical RL framework for off-dynamics policy transfer. The agent is trained entirely offline on data from a source environment and must adapt to a target environment with structural changes (e.g., blocked paths) using only a single, observation-only demonstration. MSTT first learns a discrete skill vocabulary by tokenizing trajectory segments with a VQ-VAE. It then trains a skill-conditioned Q-function using a novel masked Bellman operator, which simulates dynamics shifts during training by randomly making subsets of skills unavailable. At test time, a feasibility mask is inferred from the skills observed in the demonstration, and the agent plans using the learned Q-function conditioned on this mask. A diffusion model generates trajectories corresponding to feasible skills. The method is shown to significantly outperform baselines on tasks with altered dynamics in Maze2D, FetchReach, and Habitat."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The primary strength is the novel formulation of the masked Bellman operator for learning a feasibility-conditioned value function. This provides an elegant way to simulate a wide range of dynamics shifts during offline training without requiring multi-environment data.\n\n- Significance: The paper tackles the critical problem of generalization in RL under a practical, low-supervision setting. The ability to adapt from a single, action-free demonstration makes the approach applicable to real-world scenarios where target environment interaction is costly or unsafe.\n\n- Clarity: The paper's exposition is good. Complex ideas are broken down into understandable components, and the writing is concise.\n\n- Empirical Quality: The experiments are thorough and convincingly demonstrate the superiority of MSTT over strong baselines in diverse and challenging environments, including continuous control and visual navigation."}, "weaknesses": {"value": "- Mask Inference: The test-time mask is inferred by assuming any skill observed in the single demonstration is feasible, and all others are not. This is a strong heuristic that may be brittle. If the demonstration is suboptimal or does not exhaustively cover all feasible and necessary skills for a given task, the agent's capability will be unnecessarily constrained.\n\n- Scalability of Mask Sampling: The training relies on random sampling of skill masks from a space of size 2^L, where L is the codebook size. For the reported L=240, this space is astronomically large. While the paper shows this works, there is no analysis on why random sampling is sufficient. It is plausible the method's effectiveness could degrade for problems requiring larger or less structured skill vocabularies where random sampling provides poor coverage.\n\n- Scope of Dynamics Shifts: The method is tailored for structural dynamics changes that render skills binary (feasible/infeasible). It is less clear how it would adapt to continuous dynamics shifts (e.g., changes in mass or friction), which is a limitation on the \"off-dynamics\" claim. The authors acknowledge this in a footnote."}, "questions": {"value": "- Robustness of Mask Inference: How sensitive is the method to the quality of the demonstration? For example, in Maze2D, what if the demonstration covers only the first half of the only feasible path? Can the agent still reach the goal, or does the incomplete mask render the problem unsolvable?\n- On the Sufficiency of Random Masking: Could you provide more intuition as to why random mask sampling during training is effective, given the combinatorial explosion of the mask space? Does the value function learn to generalize across masks with similar properties, or does the structure of the skill space make many masks irrelevant?\n- Beyond Binary Feasibility: The current model assumes skills are either feasible or not. Have you considered extending this to handle dynamics shifts that make skills more costly or stochastic, rather than impossible? For example, by learning a mask that represents a cost multiplier instead of a binary indicator.\n- Influence of VQ-VAE: The performance seems highly dependent on the quality of the learned skill codebook. Could you comment on how performance changes with the codebook size and provide any qualitative analysis of the learned skills to show they are semantically meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZX94ufwqDk", "forum": "K4ngUOra9m", "replyto": "K4ngUOra9m", "signatures": ["ICLR.cc/2026/Conference/Submission25348/Reviewer_VHqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25348/Reviewer_VHqp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608306213, "cdate": 1761608306213, "tmdate": 1762943411638, "mdate": 1762943411638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Masked Skill Token Training (MSTT), a hierarchical reinforcement learning framework for off-dynamics policy transfer in a fully offline setting. The core idea is to adapt an agent trained in a source environment to a target environment with new constraints (like a blocked passage) using only a single, observation-only demonstration for adaptation.\n\nMSTT works in three phases:\n1.  **Skill Tokenization:** It uses a VQ-VAE to learn a discrete codebook of \"skills\" from fixed-length, unsupervised trajectory segments.\n2.  **Masked Critic Training:** It trains a \"feasibility-conditioned\" $Q$-function. This is the clever part: it uses a novel masked Bellman operator and randomly samples binary skill masks during training to simulate a wide variety of potential dynamics shifts.\n3.  **Adaptation & Execution:** At test time, it infers a single, fixed feasibility mask by tokenizing the provided demonstration. A diffusion-based generative policy then proposes candidate trajectories, which are filtered by this mask before being executed.\n\nThe authors provide a theoretical analysis for their masked Bellman operator and demonstrate strong empirical performance on several control and navigation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel and Practical Problem:** The paper tackles a well-motivated and important problem: adapting offline-trained agents to new environmental constraints without costly interaction or action labels.\n2.  **Elegant Core Idea:** The \"masked Bellman operator\" is the paper's strongest asset. The insight to train a single critic conditioned on a binary feasibility mask $m$, and to generalize this critic by randomly sampling $m$ during training, is a new idea.\n3.  **Sound Theoretical Core:** The method is justified by a novel theoretical analysis. The core contribution is Lemma 1 (we have checked the correctness), which bounds the approximation error of the masking approach and links it directly to skill determinism ($p_{min} \\to 1$). While the associated proofs for Lemma 2 and Lemma 3 are based on standard Q-learning convergence analyses, they are essential for the completeness of the main theorem, which is both novel and well-motivated.\n4.  **Dominant Empirical Success:** The framework is not just a theory. It achieves near-expert performance and massively outperforms strong baselines across all tasks. The FetchReach result, where MSTT is the *only* method to both succeed and avoid the new constraint (0% failure rate), is particularly striking."}, "weaknesses": {"value": "1.  **Misleading Presentation of \"Options\":** The paper's framing around the \"options\" framework is a bit oversell. The method's theory and implementation do not address true temporal abstraction (i.e., SMDPs) but are instead restricted to a fixed-horizon, augmented action-space MDP. While the proof for the masked MDP's bounded convergence is novel, this limitation of the theoretical contribution to a standard MDP, rather than a more general SMDP, constrains the work's theoretical novelty. This critical simplifying assumption—the fixed skill horizon—might be better stated clearly upfront in the main text, not relegated to a footnote or appendix. We would encourage the authors to adopt the more rigorous and accurate term \"augmented action-space MDP\" throughout the paper; doing so would significantly improve the clarity and precision of the contribution.\n\n2.  **Simplistic Mask Inference:** The test-time mask inference is a simple \"set $m(z)=1$ if $z$ is in the demo.\" This assumes the single demonstration is both correct and comprehensive. The paper seems doesn't explore how robust this is to suboptimal or noisy demonstrations. And a soft version seems to be more plausible than hard assignment for real world application.\n\n3.  **Potential Inefficiency:** The pipeline of a VQ-VAE encoder, a diffusion model policy and the complexity of sampling mechanism seems limited to scale to large scale applications. The test-time adaptation (Algorithm 2) is a rejection sampler. In highly constrained target environments (where most $m(z)=0$), this could be extremely inefficient, requiring many samples from the diffusion model to find one valid option. So the scalability feels a bit limited.\n\n4. A closely related work with potential theoretical improvement is if the masked value theorem applied to HiT-MDP then the claim of options will hold. Also skill discovery algo will be simplified and generalized. Authors may refer to \n\nLi, Chang, Dongjin Song, and Dacheng Tao. \"The skill-action architecture: Learning abstract action embeddings for reinforcement learning.\" (2021).\n\nLi, Chang, Dongjin Song, and Dacheng Tao. \"Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings.\" The Eleventh International Conference on Learning Representations. 2023.\n\nfor an efficient and scalable option embedding discovery framework. An offline algo vmoc is derived here:\n\nLi, Chang, et al. \"Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs.\" arXiv preprint arXiv:2507.16473 (2025)."}, "questions": {"value": "1.  Given that the theory and implementation rely on fixed-horizon skills $H$, would the authors be willing to clarify this in the main text (e.g., by framing it as an \"augmented action-space MDP\") to avoid confusion with the more general SMDP \"options\" framework? \n2.  How sensitive is the mask inference to the quality of the single demonstration? What happens if the demo is suboptimal (e.g., takes a long but feasible path) or noisy (e.g., contains a skill that is, in fact, infeasible)? \n3.  Can you report the test-time sampling efficiency? Specifically, what is the average rejection rate (or number of samples) in Algorithm 2 for the different target environments? One might expect the highly-constrained Maze2D tasks to have a high rejection rate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "42Lr60i8iH", "forum": "K4ngUOra9m", "replyto": "K4ngUOra9m", "signatures": ["ICLR.cc/2026/Conference/Submission25348/Reviewer_DAdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25348/Reviewer_DAdX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718204021, "cdate": 1761718204021, "tmdate": 1762943411230, "mdate": 1762943411230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Masked Skill Token Training (MSTT), an offline hierarchical reinforcement learning framework for off-dynamics transfer using only observation-only demonstrations. MSTT learns a discrete skill space via VQ-VAE and trains a feasibility-conditioned critic through masked Bellman updates. At deployment, it infers a binary skill mask from a single demonstration and uses a diffusion-based trajectory generator to execute only feasible skills. Experiments on Maze2D, FetchReach, and Habitat environments show that MSTT achieves robust off-dynamics transfer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses the important problem of transferring policies using only observation-based demonstrations. By introducing a simple masking mechanism over discretized skills, the method enables efficient offline adaptation. The proposed framework demonstrates strong generalization and robustness for off-dynamics transfer, and further presents experiments combining MSTT with VLMs, showing its potential for low-effort policy adaptation."}, "weaknesses": {"value": "1. The masking mechanism relies on random sampling during training, which seems inefficient. Could the masking process be learned? Moreover, MSTT's scalability is limited as the skill vocabulary grows, might cause the critic to overfit to partial masking patterns and weaken generalization.\n\n2. The proposed method mainly addresses structural dynamics shifts (e.g., blocked transitions) and does not handle continuous or parametric changes in dynamics (e.g., friction, mass, damping). This narrows its applicability.\n\n3. Visualizing which discretized skills become masked in each target domain could provide strong qualitative and interpretable evidence of proper adaptation. By examining how feasibility patterns change across domains, the authors could further demonstrate the effectiveness of the proposed method.\n\n4. The baselines, such as BC and Diffuser, are not specifically designed for off-dynamics transfer, limiting the fairness of comparison. It would strengthen the evaluation to include methods such as SiMPL[1], PDT[2], MetaDiffuser[3], HDP[4], which explicitly target policy generalization or meta-adaptation across dynamics shifts. Furthermore, the hierarchical skill-based approaches are missing.\n    \n    [1] Skill-based Meta-Reinforcement Learning, ICLR 2022\n    \n    [2] Prompting Decision Transformer for Few-Shot Policy Generalization, ICML 2022 \n\n    [3] MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL, ICML 2023\n\n    [4] Hyper-Decision Transformer for Efficient Online Policy Adaptation, ICLR 2023"}, "questions": {"value": "1. In the VLM + MSTT case study, does the VLM function as a world model that generates visual observation trajectories to serve as demonstration inputs for the policy transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aZxVMI1RU5", "forum": "K4ngUOra9m", "replyto": "K4ngUOra9m", "signatures": ["ICLR.cc/2026/Conference/Submission25348/Reviewer_J5hB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25348/Reviewer_J5hB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875870579, "cdate": 1761875870579, "tmdate": 1762943410753, "mdate": 1762943410753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MSTT, an offline hierarchical learning method for off-dynamics transfer. It treats the dynamics discrepancy as blocking some learnt skills and thus masking those out  at deployment. The target skill mask is inferred based on single observation-only demonstration which makes it suitable for domains where action annotations or environment interactions are costly. Empirical results show MSTT shows higher return and less complete steps than DARC and BC in Maze2d transfer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method proposed is novel to me. Modeling dynamics discrepancy as masked out skill tokens alleviate the problem presented in specific transfer learning problems.\n\n2. Proposed MSTT shows better performance in the demonstration-only offline transfer learning setting over some imitation and offline learning baselines."}, "weaknesses": {"value": "1. The assumption that source and target domains only differ in feasibility distinction of specific skills is strong to me. This limits the generalizability of the method proposed, and raises my concern to the setting proposed that target dataset should have similar levels of accessibility compared to source dataset.\n\n2. Weak baseline comparison. For cross-domain imitation learning, [1] suggests a method which also learns from cross domain state-only demonstrations, which explicitly deals with domain discrepancy upon ID methods. Moreover, as a minor concern, DARA is not a recent offline transfer method though it doesn't share the same setting with MSTT. I would suggest replace it with some stronger baselines such as [2].\n\n[1] Cross-domain Imitation from Observations Dripta S. Raychaudhuri, Sujoy Paul, Jeroen Vanbaar, Amit K. Roy-Chowdhury Proceedings of the 38th International Conference on Machine Learning, PMLR 139:8902-8912, 2021.\n\n[2] Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning Xiaoyu Wen, Chenjia Bai, Kang Xu, Xudong Yu, Yang Zhang, Xuelong Li, Zhen Wang Proceedings of the 41st International Conference on Machine Learning, PMLR 235:52720-52743, 2024."}, "questions": {"value": "1. Related to W1, is it possible that dynamics discrepancy leads to new skills that have not been learnt with source domain dataset, in which case the application of MSTT may lead to suboptimal performance?\n\n2. How does the codebook size affect the downstream performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2AiAaIOABu", "forum": "K4ngUOra9m", "replyto": "K4ngUOra9m", "signatures": ["ICLR.cc/2026/Conference/Submission25348/Reviewer_Z4mu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25348/Reviewer_Z4mu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998393717, "cdate": 1761998393717, "tmdate": 1762943409656, "mdate": 1762943409656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}