{"id": "5RcoUe1tA1", "number": 19203, "cdate": 1758294389039, "mdate": 1759897052342, "content": {"title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation", "abstract": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding.  We present **SC-ARENA**, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a *virtual cell* abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks — cell type annotation, captioning, generation, perturbation prediction, and scientific QA — that probe core reasoning capabilities in cellular biology.  To overcome the limitations of brittle string-matching metrics, we introduce **knowledge-augmented evaluation**, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments.  Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that: (i) under the *Virtual Cell* unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. **SC-ARENA** thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.", "tldr": "", "keywords": ["Large Language Models", "Single-cell Foundation Models", "Scientific AI Benchmark", "Knowledge-augmented Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d22649770de711ad48609354edb4879c0609306b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SC-ARENA, a new benchmark to evaluate large language models (LLMs) in single-cell biology. The core ideas are:\n1. Virtual Cell abstraction\nTreats a model as if it were a “virtual cell.”\nThe “cell” has attributes (identity/state) and methods (how it responds to the environment).\nThis unifies evaluation across different biological tasks instead of testing each task in isolation. \n2. Five natural language tasks\nEach one probes something biologically meaningful:\nCell Type Annotation (CTA): Given expression → predict ontology cell type.\nCell Captioning (CC): Given expression → describe the cell in natural language.\nCell Generation (CG): Given a cell type description → generate a plausible “cell sentence” (i.e. pseudo-expression profile).\nPerturbation Prediction (PP): Given baseline + perturbation, predict changes and post-perturbation state.\nScientific QA (SQA): Answer mechanistic, literature-grounded biological questions. \nThese map both static identity and dynamic behavior (how a cell changes under interventions), which is exactly what one would expect from a “virtual cell.” \n3. Knowledge-augmented evaluation\nInstead of BLEU / ROUGE / exact match, which fail badly in biology, SC-ARENA uses an LLM-as-a-judge but grounds the judge in external biological knowledge (Cell Ontology, Gene Ontology, UniProt, CellMarker, PubMed evidence, etc.).\nThe evaluator scores answers and produces an interpretable rationale with references, not just a number. \nThey show this judgment correlates with real biological hierarchy (e.g. closer ontology terms → higher score; Spearman ρ≈0.62, p<0.001). \n4. Empirical study\nBenchmarks general LLMs (Qwen2.5/3, GPT-4o, DeepSeek-R1, Kimi-K2) and domain-specific single-cell models (C2S, scGPT, scGenePT, Cell-O1).\nFinds: no model is good at everything; general models are fluent but biologically shaky, and domain models are precise in narrow skills but weak elsewhere. \nFor example, captioning and science QA get the best scores (~60–74/100 for top models), but perturbation prediction and mechanistic reasoning remain very weak (<38/100). \nSmall domain models like C2S actually beat giant general LLMs on cell type annotation, which shows specialization can outperform sheer parameter count for grounded biology."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified evaluation via the Virtual Cell abstraction\nInstead of 5 unrelated leaderboards, SC-ARENA frames everything around one object: the Virtual Cell. The “attributes” vs “methods” split is elegant, and it matches how experimentalists think (cell identity vs response to perturbation). This is novel and, importantly, extensible. \nThis framing makes it possible to talk about “does an LLM qualify as a virtual cell model?” instead of “did it get 2% better BLEU.” That’s a conceptual contribution, not just engineering.\n2. Knowledge-augmented evaluation is thoughtful and genuinely useful\nThe work directly addresses a known failure mode of LLM evals in science: surface-similarity metrics like BLEU and ROUGE often (a) reward buzzwords, (b) fail to punish mechanistic nonsense.\nHere, the evaluator:\nretrieves ontology / marker / pathway / literature evidence, explains why it scored something, and aligns with biological hierarchy (ρ=0.6212 between ontology distance and evaluator score)."}, "weaknesses": {"value": "1. The benchmark leans heavily on LLM-as-a-judge, which is itself another model\nYes, they mitigate this with knowledge retrieval and ontologies. Yes, they validate correlation with ontology distance. But the scoring model is still an LLM (GPT-4o-mini), which raises standard questions:\nHow stable is the score across judge variants / seeds?\nCould a model “game” the judge by mimicking citation-sounding language and pathway buzzwords?\nIs there any adversarial testing (e.g., hallucinated but authoritative-sounding nonsense)?\nThey partially address alignment with expert judgments and ontology distance, but a deeper robustness audit (or inter-judge agreement across two different judges) would make the claim stronger. \n\n2.Ground truth for some tasks is underspecified\nFor perturbation prediction (PP), the model is supposed to say:\nwhich genes go up/down, and\nproduce a plausible “post-perturbation cell sentence.”\nBut evaluating free-form gene-level differential expression is biologically thorny:\nThey say they use DEGs from Norman/Adamson and external knowledge bases (GO, UniProt, NCBI) to judge plausibility. \n\nWhat isn’t fully clear is: how do you distinguish a true novel hypothesis from a hallucination? If a model proposes a plausible but previously unreported compensatory pathway, does it get penalized or rewarded?"}, "questions": {"value": "1. Knowledge-Augmented Judging\nWhat prevents the LLM-as-judge from being biased by language fluency rather than biological accuracy?\nHow reproducible are scores across judges (e.g., GPT-4o vs DeepSeek-R1) or seeds?\nHow do you ensure the judge doesn’t “reward” models that use familiar ontology terms but misstate mechanisms?\n\n2. Dataset and Coverage\nAre the 600-sample CellxGene subset and 138 perturbations enough to represent cellular diversity?\nCould SC-ARENA generalize to datasets from other species, tissues, or modalities?\nAre there biases toward well-studied cell types (e.g., immune, epithelial) due to ontology density?\n\n3. Benchmark Design\nWhy were all five tasks weighted equally in the “Total Score”?\n→ Would weighting causal tasks (e.g., perturbation prediction) more heavily yield a different ranking?\nDid the authors test whether models overfit to the language style of prompts rather than underlying biology?\nHow much variance exists across repeated evaluations (inter-run consistency)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4yPs3eN2H9", "forum": "5RcoUe1tA1", "replyto": "5RcoUe1tA1", "signatures": ["ICLR.cc/2026/Conference/Submission19203/Reviewer_6o9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19203/Reviewer_6o9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798152593, "cdate": 1761798152593, "tmdate": 1762931198859, "mdate": 1762931198859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SC-Arena, a natural language benchmark for evaluating large language models (LLMs) in single-cell biology. The authors propose a unified “Virtual Cell” abstraction and design five biologically grounded tasks: cell type annotation, captioning, generation, perturbation prediction, and scientific QA. The evaluation leverages a knowledge-augmented LLM-as-a-judge framework, integrating external ontologies and databases to ensure interpretability and biological fidelity. Experiments compare both general-purpose and domain-specialized LLMs, revealing strengths and limitations in biological reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  The Virtual Cell abstraction and multi-task natural language evaluation is an interesting idea to more objectively test different models’ capacity to understand cellular processes. \n\n- Integrating external biological knowledge (Cell Ontology, UniProt, GO, CellMarker, PubMed) into the evaluation pipeline is a major strength and a clever way to address the limitations of string-matching metrics.\n\n- The paper benchmarks a wide range of models (Qwen, GPT-4o, DeepSeek-R1, Kimi-K2, C2S-Scale, scGenePT, scGPT, Cell-O1) and analyzes performance across tasks, model scales, and domains, using only open-source datasets."}, "weaknesses": {"value": "- While the benchmark covers different tasks and the datasets are open-source, the paper does not address the risk of benchmark dataset leakage; such as, whether the datasets used to construct the SC-Arena benchmark were present in the pretraining or fine-tuning data of the evaluated models. \n\n- The rationale for model selection and the fairness of comparisons (e.g., fine-tuning protocols, input formats) could be better discussed. \n\n- The knowledge-augmented LLM-as-a-judge is promising, but its reliability and potential biases should be discussed further."}, "questions": {"value": "- While the use of open-source datasets is commendable, how do the authors plan to address the risk of benchmark dataset leakage? For example, CS2 used the CELLXGENE dataset for training. Could this explain their performance on the CTA task?  \n\n- Are there plans to expand the benchmark to include additional modalities or more challenging reasoning tasks? \n\n- It’s unclear how the architecture of the models will influence performance in these tasks. My main concern is that performance differences may reflect experimental artifacts rather than true differences in model capability. For example, a domain-specific model might outperform others simply because it was fine-tuned on data similar to the benchmark, or because its input format more closely matches the evaluation protocol. How do the authors plan to address this potential bias? Will this influence which models can or cannot be used for the benchmarking?\n \n- The assessment of the knowledge augmented evaluator is shown only for the CTA task. How do the authors plan to validate the evaluator for the other tasks? How can we be confident that the evaluator is reliable and interpretable for these other tasks, which may involve different types of reasoning, output formats, and biological knowledge?\n\n- As the evaluator is also a model, a proper evaluation of the model should be performed. For example, are there cases where the evaluator produces scores that do not align with expert human judgment, or where it fails to recognize biologically implausible or trivial answers? Does the evaluator systematically favor certain model architectures, output styles, or biological domains? Does it reward verbosity, penalize concise but correct answers, or show preference for models trained on similar data as the evaluator itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TrnAsyC6co", "forum": "5RcoUe1tA1", "replyto": "5RcoUe1tA1", "signatures": ["ICLR.cc/2026/Conference/Submission19203/Reviewer_2kiV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19203/Reviewer_2kiV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912026302, "cdate": 1761912026302, "tmdate": 1762931198363, "mdate": 1762931198363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SC-Arena, a benchmark designed to evaluate large language models (LLMs) as “virtual cells” capable of reasoning over biological knowledge and single-cell data. The benchmark includes multiple question types testing biological plausibility, reasoning consistency, and interpretability. It also introduces a knowledge-augmented evaluation strategy (Eval-RAG) that uses retrieval-augmented generation to penalize biologically implausible answers and reward semantically coherent responses beyond exact-match metrics."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tVery creative and well-motivated benchmark that treats LLMs as reasoning agents over biological cell states.\n•\tThe Eval-RAG strategy is an elegant idea that improves evaluation by incorporating biological context and semantic plausibility, moving beyond token-level correctness.\n•\tThe paper provides a valuable framework for comparing different LLMs under biologically grounded tasks.\n•\tThe paper does extensive evaluation of general and domain-specific models.\n•\tDiscussion includes relevant next steps for producing a more biologically robust evaluation benchmark."}, "weaknesses": {"value": "The paper would benefit from more detailed examples—for instance, elaborating on the process shown in Figure 2, panel B, to clearly explain how the biological plausibility scoring is computed step by step."}, "questions": {"value": "How sensitive is Eval-RAG to the retrieval source—does the choice of biological database or text corpus significantly change the evaluation outcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h7EH3W9Qtd", "forum": "5RcoUe1tA1", "replyto": "5RcoUe1tA1", "signatures": ["ICLR.cc/2026/Conference/Submission19203/Reviewer_6ZRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19203/Reviewer_6ZRo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937216112, "cdate": 1761937216112, "tmdate": 1762931197792, "mdate": 1762931197792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an evaluation framework called Virtual Cell which seeks to unify the assessment of LLMs performance on various sub tasks important for single cell analysis like cell type annotation, captioning etc. The authors then evaluated various LLMs on a dataset derived from combining publicly available single-cell databases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Combining vast amounts of single-cell data with natural language based knowledge available to gain insights into cellular function is beneficial to the biology community so this is a timely topic. \n2. Authors have benchmarked their proposed framework for evaluating LLM performance across many different LLM models or other domain specific models. \n3. Definition of the knowledge cell class is well thought out in considering multiple sources of information available for analyzing cellular dynamics."}, "weaknesses": {"value": "- The novelty/value of this framework for evaluation is unclear, many current models like Cell2Sentence already combine single cell rna data with text based information and have shown use cases for downstream tasks like cell type prediction, perturbation response prediction etc. \n- Considering existing methods that can perform some of the tasks mentioned in the multi-task benchmark like Cell2Sentence, CellReasoning etc, to fully evaluate this work, performance of existing methods on individual tasks should be included and discussed."}, "questions": {"value": "- This research area is a useful application of LLMs for biological science but while the authors mention the previous works, proper comparisons to these methods is lacking. Authors should consider expanded the related work section needs to clarify the contributions of this paper in comparing to some of the works mentioned here. \n- Authors should consider incorporating some of the ideas suggested in the discussion on modeling, evaluating and scoring into this work to improve the contribution and novelty of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7V3FDdYkOH", "forum": "5RcoUe1tA1", "replyto": "5RcoUe1tA1", "signatures": ["ICLR.cc/2026/Conference/Submission19203/Reviewer_k5M8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19203/Reviewer_k5M8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056989996, "cdate": 1762056989996, "tmdate": 1762931197448, "mdate": 1762931197448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}