{"id": "DldwXCCP25", "number": 4454, "cdate": 1757683060812, "mdate": 1759898031740, "content": {"title": "CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally", "abstract": "CLIP (Contrastive Language-Image Pretraining) has become a popular choice for various downstream tasks. However, recent studies have questioned its ability to represent compositional concepts effectively. These works suggest that CLIP often acts like a bag-of-words (BoW) model, interpreting images and text as sets of individual concepts without grasping the structural relationships. In particular, CLIP struggles to correctly bind attributes to their corresponding objects when multiple objects are present in an image or text. In this work, we investigate why CLIP exhibits this BoW-like behavior. Our key finding is that CLIP does not lack binding information. Through linear probing, robustness tests with increasing object counts, and conjunctive search experiments, we show that attribute–object bindings are already encoded within CLIP’s text and image embeddings. The weakness lies in the cross-modal alignment, which fails to preserve this information. We show it can be accessed cross-modally with a simple linear transformation to text embeddings. This improves CLIP’s attribute-object binding performance and confirms that the information was already encoded unimodally. In practice, this means CLIP-based systems can be enhanced with a lightweight linear layer trained on existing embeddings, avoiding costly encoder retraining.", "tldr": "CLIP binds concepts, but only uni-modally.", "keywords": ["vision-language models", "CLIP", "compositionality", "binding"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cccabca6d05dba633b6eac02253607fc342f7f38.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the Bag-of-Words aspect of CLIP embeddings, namely that compositional information is limited and representations are not sensitive to compositional orderings in a cross-modal way, but that this information is present in the individual modalities. The work goes on to explore various ideas aimed at uncovering what is at the core of this problem and how to fix the BoW-nature of standard CLIP embeddings in cross-modal compositional contexts.\n\nIn summary, I loved reading this paper and the simple explanation and guided tour of the hypothesis, easy-to-understand experiments that are nice and conceptually clear. I definitely think this is highly relevant to the community. However, passages such as the one in the conclusion (\"We investigated the reasons behind CLIP's bag-of-words behavior\") sort of ring a little bit hollow because I feel like a solid step was taken towards localising the nature of the issue, but I am not so sure a lot of solid ground was covered to explain the more interesting question - why this occurs. We know that a linear transformation among the spaces to align the embeddings goes some way to solving this issue, but what drives this and how simple linear transformations actually \"recover compositional information\" from a BoW kind of treatment is still a bit of a mystery to me. Having said that, given the space allowed for submissions, this is a great submission and full of detail and interesting ideas."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This is quite a well-written paper. It's informative and also understands the audience needs some examples, so the example with the orange square and blue triangle to highlight the problem in a clear example is very welcome to give a concrete example of the issue at hand (and how the permuted text also has a high match with the inconsistent image). The questions that lie at the crux of the issue are put forward in a simple, clear and easy-to-understand manner. This is refreshing to see and greatly appreciated, making it easy to follow how the setup of the experiments answers the questions that truly get to the heart of what the paper claims to be about. \n\nThere is clear attention to detail and awareness of the often-unavoidable quagmire of confounds that render any experimental results across a large dataset of real images to be impossible to declare to be fully associated with a list of simple claims. This then pushed the authors to work in a controlled setting that they are able to manipulate clearly and cleverly in order to show that conclusions and differences only arise due to the re-ordering of the components in the text encoder. This experimental awareness is a solid positive for this paper, also in the recognition that both real and synthetic datasets can provide complementary results and a holistic approach of the results is often necessary.\n\nAnother strength is the consideration and attention to splits in the linear classifier case. This work correctly identifies previous approaches being problematic and not evaluating where the lack of alignment comes from. Additionally, the presence of summary takeaway points greatly enables the reader to follow the main points being raised and to follow the authors' claims and hypotheses with regard to the presented results."}, "weaknesses": {"value": "The weaknesses here are only very minor and should be treated as small tips to increase the readability or accessibility of the paper.\n\n1. When listing the compositional datasets, it would be helpful to include the names of those benchmarks rather than just the generic theme and the citation, as not everyone will be aware of the citation but might be familiar with a name of a specific dataset and this would only increase the mental links you are raising in those lines of the paper. I know later in Table 1 this is clarified, but you might facilitate reading by including them in that section, too.\n\n2. When discussing the matching of permuted text encoders, it means something different to be picking the correct match out of a small number of possibilities than it is to picking out the correct match among a large potential batch of potential pairs, due to the fact that there is a higher chance to randomly select the correct one. I found myself curious of this when reading through section 3 and felt like additional information would be welcome in this part of the paper, even if just in brackets it mentions how many options were being selected from in the batch. This would then stop me from wondering if the batches were small enough that random correct selection might be a possibility to explain some results.\n\n3. The paper's goals aimed to find the cause of the BoW phenomenon and did a great job of locating it, but the abstract and conclusion lead one to believe that more of a causative finding has been presented. I am impressed by the paper and the results found, but after finishing the paper, I'm not sure I've been enlightened too much as to what is causing this phenomenon. The authors have certainly made a scientific advance by locating where the issue lies, but without tying it to other relevant concepts like the modality gap (which the authors show increases in some cases in one of the appendices for a dataset) then I'm left even more confused as to what brought about the cross-modal lack of alignment (I had previously suspected it was more tightly connected to and explained by the modality gap issue). Additionally, I feel some of that work and potential explanatory notes should be made in the relevant literature section."}, "questions": {"value": "1. Have you considered the cases that extend beyond image-text modalities? \n\n2. What potential pitfalls did you consider in using a synthetically generated benchmark?\n\n3. Why do you think PUG:SPARE in CLIP (ft) achieved higher than PUG:SPAR in the same case (i.e. 1.00 > 0.98) when the PUG:SPARE model was supposed to be more difficult. This is not a hugely important question for the validity of the experiments, but I did find myself having this question when considering the results and I wasn't confident in my hypothesis. Perhaps the authors can share some of their thoughts on this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lshnH2aSNw", "forum": "DldwXCCP25", "replyto": "DldwXCCP25", "signatures": ["ICLR.cc/2026/Conference/Submission4454/Reviewer_CZn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4454/Reviewer_CZn4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502976846, "cdate": 1761502976846, "tmdate": 1762917374020, "mdate": 1762917374020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CLIP models often has Bag-of-Words (BoW) property where it treats images and texts as sets of individual concepts and struggles to correctly bind each object and its corresponding attribute pairs. This paper questions the reasons of such BoW property with a series of controlled experiments and proposes a simple and efficient fix to this problem. Specifically, several controlled experiments with linear probing on individual visual and text embeddings from object-attribute binding datasets indicate that CLIP models already encode attribute-object relationships in each modality. In addition, it is shown that such attribute-object concepts are linearly separable on the CLIP's representation space. Based on the results, the problem lies on the cross-modal alignment, and a simple solution is proposed by linearly projecting one embedding during the alignment training. In this way, the performance recovers close to the fully fine-tuned CLIP models while achieving several practical advantages such as computational efficiency and backward compatibility of already-deployed systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper investigates the reason behind the CLIP models' BoW property which is often unexplored from the previous compositional reasoning literature. The controlled experiments and analyses made could be a valuable reference for guiding a better solution for the following works. From the methodological perspective, LABCLIP for appending a linear projection matrix on either image or text embedding is effective to remedy the CLIP models' attribute-object binding capabilities, while it does not need the full fine-tuning of encoders parameters. This can benefit both on the training and inference procedure."}, "weaknesses": {"value": "One major concern lies in the scope of the work with respective to compositional reasoning tasks in several aspects. While the paper mainly focused on a relatively simple settings, attribute-object binding, but it is not yet discussed whether the observations and solutions made in the paper can generalize to a more complex and realistic compositional reasoning benchmarks such as Winoground. In addition, since the oracle method to the proposed solution is NegCLIP, which is trained on both original and hard negative captions, the proposed solution might share the NegCLIP's limitations such as lower performance on challenging compositional reasoning benchmarks such as Winoground and original CLIP's downstream tasks including zero-shot image classification and image-text retrieval. Overall, the method appears to focus on a specific area of compositional reasoning and requires further justifications to a complex and realistic settings."}, "questions": {"value": "Regarding the weaknesses stated above, it is expected to show that the proposed findings and methods can generalize beyond the simple attribute-object binding setting to more complex compositional reasoning tasks, such as Winoground, where the oracle method NegCLIP also show limited performance. In addition, it would be important to show how the proposed LABCLIP method affect the general downstream tasks such as zero-shot image classification and image-text retrieval."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ecWyh0Ruof", "forum": "DldwXCCP25", "replyto": "DldwXCCP25", "signatures": ["ICLR.cc/2026/Conference/Submission4454/Reviewer_kJCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4454/Reviewer_kJCN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928546494, "cdate": 1761928546494, "tmdate": 1762917373670, "mdate": 1762917373670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing work has questioned CLIP’s ability to bind concepts, suggesting it behaves like a bag-of-words representation. In this paper, the authors find that CLIP can bind attributes to their objects, but only unimodally. They present linear-probing experiments, robustness tests with increasing object counts, and conjunctive-search experiments to show that CLIP’s text and visual representations bind concepts independently. Finally, they train a lightweight layer to patch existing CLIP embeddings and teach the model to bind concepts."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written. The introduction clearly motivates an important problem and explains how prior work does not fully investigate the source of bag-of-wordness. The related-work section is thorough and effectively positions the paper within the literature.\n\nLarge-scale experiments: The paper shows that concept binding can be improved with a simple linear probe on large-scale datasets such as ARO and SugarCrepe."}, "weaknesses": {"value": "**Differences in results in prior work.**\nWhile the paper’s results are encouraging, I would like to understand the differences from existing works that show that CLIP, even after fine-tuning, does not bind concepts [a, b].\n\nIn [a], the authors claim that the representations are not expressive enough to bind concepts. They introduce the Concept Binding Benchmark and show that fine-tuning CLIP and linear probes (the type-logical model in their experiments) do not generalize to held-out compositions that require binding. However, the results in this manuscript suggest that binding can be remedied with a linear probe.\n\nFurthermore, the results in this paper contradict those of Kamath et al. (2024) [b]. In Table 2 (Section 4.3), [b] fine-tune CLIP and evaluate performance on held-out compositions for spatial reasoning (which can be viewed as binding objects to their relations).\n\nIt would be helpful if the authors could explain why the experiments in this paper contradict prior work. The paper would be strengthened by running evaluations on datasets [a] and [b] and demonstrating improved binding.\n\n\n**Minor fixes**\n\nLine 309: use \\citet for Campbell et al., 2024. \n\nFigure 5: Could you draw a box to highlight the incongruency? \n\n**References**\n\n[a] Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. Findings of EACL 2024. \n\n[b] What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning. EMNLP 2024."}, "questions": {"value": "Do you suspect that the BOWness arises from the contrastive loss or the dual encoder in CLIP? It would be insightful to the reader if you could include this discussion in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RTtwApk9NL", "forum": "DldwXCCP25", "replyto": "DldwXCCP25", "signatures": ["ICLR.cc/2026/Conference/Submission4454/Reviewer_84zT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4454/Reviewer_84zT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961319593, "cdate": 1761961319593, "tmdate": 1762917373173, "mdate": 1762917373173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the root cause of CLIP's well-documented \"bag-of-words\" (BoW) behavior in compositional reasoning tasks, where it fails to correctly bind attributes to their corresponding objects in multi-object scenes. The authors make a key distinction between uni-modal and cross-modal binding. Through a series of linear probing, robustness, and conjunctive search experiments, they convincingly demonstrate that CLIP's individual image and text encoders do encode attribute-object binding information in a linearly separable manner. The core problem, they argue, lies not in a lack of information but in a cross-modal alignment failure that prevents this information from being effectively utilized during retrieval. To validate this hypothesis, they propose LABCLIP, a simple method that applies a lightweight linear transformation to the text embeddings. This approach significantly improves performance on compositional benchmarks (ARO, SugarCrepe, COCO) without retraining the frozen CLIP encoders, offering a practical and efficient solution for enhancing existing CLIP-based systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The use of multiple, complementary experimental paradigms (linear probing, robustness to object count, conjunctive search) provides multi-faceted evidence for the uni-modal binding claim.\n- The paper is well-written and clear. The core insight is presented early and reinforced throughout. Figures 1, 3, and 5 are particularly effective in illustrating the key concepts."}, "weaknesses": {"value": "1. The conclusion that “the problem lies in cross-modal alignment” remains descriptive. The paper lacks a deeper theoretical or mathematical explanation of why the alignment loss fails to preserve binding (e.g., analyzing the contrastive objective’s geometry or the modality gap quantitatively). \n2. The paper strongly asserts that the problem is only alignment. However, linear probing showing high accuracy only proves the information is linearly recoverable, not that it is represented in a way that is semantically robust or generalizable. High-dimensional embeddings can sometimes allow linear separation of concepts without the model having a true understanding of their binding.\n3. The work focuses exclusively on attribute-object binding. While this is a fundamental primitive, true compositionality also involves relations, quantifiers, and negation. The paper does not demonstrate that the alignment fix would generalize to these more complex structures, which may require non-linear solutions.\n4. The method for generating hard negatives on COCO relies on the heuristic from NegCLIP, which shuffles adjectives and nouns. This can create grammatically or semantically implausible captions (e.g., \"a helmet man\"). This risks teaching the model spurious correlations based on surface-level word co-occurrence rather than true structural binding, potentially inflating the reported gains.\n5. The paper convincingly shows a linear layer is sufficient, but it does not compare against other lightweight, non-linear adapter methods (e.g., LoRA, small MLPs). Such a comparison would strengthen the claim that linearity is indeed the optimal and simplest solution. The argument that \"linear is enough\" is practical but not theoretically definitive.\n6. As shown in Table 10 (Appendix), LABCLIP reduces zero-shot accuracy on standard single-object classification datasets (CIFAR, ImageNet). While expected, this trade-off is significant for practitioners and should be discussed more prominently in the main text, as it highlights the specialization of the method."}, "questions": {"value": "My major concerns are outlined in the \"Weaknesses\" part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Vfm9TUxkNo", "forum": "DldwXCCP25", "replyto": "DldwXCCP25", "signatures": ["ICLR.cc/2026/Conference/Submission4454/Reviewer_dUnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4454/Reviewer_dUnx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997198977, "cdate": 1761997198977, "tmdate": 1762917372486, "mdate": 1762917372486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}