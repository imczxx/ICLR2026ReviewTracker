{"id": "DrETNoeqS5", "number": 9804, "cdate": 1758141402310, "mdate": 1763759068272, "content": {"title": "Lazy-Attention: Efficient Retrieval-Augmented Generation with Deferred Positional Encoding", "abstract": "Key-value (KV) caching accelerates inference in large language models (LLMs) by reusing computations from previously generated tokens. Its importance becomes even greater in long-context applications such as retrieval-augmented generation (RAG) and in-context learning (ICL). However, conventional KV caching embeds positional information directly into the cache, limiting its reusability.\nWe introduce \\ourmethod, a position-free KV caching mechanism that decouples positional information from cached keys and values, applying positional encoding dynamically during inference. This design reduces memory footprint and prefill overhead while preserving generation quality, with only negligible additional cost during decoding. Leveraging two optimized kernels tailored for prefilling and decoding, Lazy-Attention achieves significant efficiency improvements: under skewed document distributions, it reduces time-to-first-token (TTFT) by 1.37$\\times$ and increases inference throughput by 1.40$\\times$ compared to the state-of-the-art Block Attention, while maintaining comparable output quality.", "tldr": "", "keywords": ["Retrieval-augemented generation", "Positional encoding", "KV Cache Reuse"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1749a740e68e6954478d28cf8a7ef2ab11144ad8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address the low reusability and high memory overhead of conventional Key-Value (KV) caching in retrieval-augmented generation (RAG) tasks caused by embedding positional information directly into the cache, limiting reuse to identical document positions, the paper proposes Lazy-Attention, a position-agnostic KV caching mechanism. It decouples positional encoding from cached keys/values, deferring dynamic positional adjustment to the attention computation stage instead of pre-embedding it. Lazy-Attention implements two optimized Triton kernels to minimize overhead, achieving lower time-to-first-token (TTFT) and  higher inference throughput than SOTA Block Attention under skewed document distributions. Experiments on Tulu3-Block-FT  across 4 RAG benchmarks show it maintains comparable generation quality while reducing VRAM cache hit ratio gaps."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike conventional caching (requiring O(N!) space for 100% hit rate of N documents due to position-dependent duplicates) or Block Attention (duplicating KVs for different positions), Lazy-Attention stores position-agnostic KV entries. A single document’s KV cache can be reused across any prompt position,  thereby reducing the space requirement.\n2. Lazy-Attention eliminates position-coupled KV duplication, outperforming baselines across cache budgets. At 1GB KV cache (low skewness), hit ratio is 7.40%; at 10GB (high skewness), it reaches 20.69%. This avoids memory pressure from duplicate KVs (Block Attention) or recomputation (CacheBlend)."}, "weaknesses": {"value": "1. Lazy-Attention is only tested on 4,096-token documents (2,048-token chunk budget). For 8K/16K sequences (e.g., legal contracts), fixed tile sizes (M/N) become suboptimal: larger N increases prefill K rotation cost (6DN FLOPs), while smaller M amplifies decoding Q rotation overhead. Additionally, relative offset calculation for offsets >1000 may introduce cumulative RoPE rotation errors, but this is untested.\n2. Kernels use fixed M (e.g., 128 for prefill) and N (e.g., 64 for decoding) regardless of document length. For short documents (256-token product descriptions), M=128 leads to underutilized tiles (half-empty), increasing per-token overhead. For long documents (>4K), N=64 requires more tile splits, adding kernel launch latency. Unlike FlexAttention (dynamic tiles), this reduces efficiency in mixed-length RAG workloads.Is it better to make the proposed method adaptive to tile sizing for dynamic document lengths?\n3. Lazy-Attention excels with hot documents but struggles with cold starts (no cached documents). In uniform distributions (low reuse), its prefill (0.59%) and decoding (0.023%) overheads make it only slightly better than Block Attention (TTFT difference <5%), with no advantage over Prefix Caching for fully cold requests."}, "questions": {"value": "1. How does Lazy-Attention perform on 8K/16K-token documents (e.g., extended NarrativeQA novels)? Do cumulative RoPE rotation errors (offset >1000) degrade EM scores? Can dynamic tile sizing (e.g., M=256 for 8K, N=128 for 16K) reduce per-tile overhead? \n2. Can combining Lazy-Attention (hot docs) + Prefix Caching (cold docs) + Block Attention (semi-hot docs) cover all RAG scenarios? What TTFT/throughput gains does this hybrid strategy achieve vs. standalone Lazy-Attention method?\n3. How does batch size (16/32/64) affect Lazy-Attention’s latency/throughput? Do concurrent relative rotations for shared KV caches cause register contention? It is better report latency variance across batches and compare with Block Attention’s batch performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t91WuT3NoS", "forum": "DrETNoeqS5", "replyto": "DrETNoeqS5", "signatures": ["ICLR.cc/2026/Conference/Submission9804/Reviewer_Qrx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9804/Reviewer_Qrx9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760953402026, "cdate": 1760953402026, "tmdate": 1762921289457, "mdate": 1762921289457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mechanism that defers positional encoding in Transformers to enable position-agnostic KV cache reuse in RAG systems. The key idea is to apply RoPE rotations during attention computation rather than before caching, allowing document caches to be shared across different prompt positions without duplication. The presentation is very nice, but the experiments are limited to a single model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a limitation in existing KV caching, that is position-dependence leads to O(N!) space complexity for N documents, which is impractical. The presentation of the problem is clear and easy to follow.\n- Custom Triton kernels for both prefill and decoding with measured overheads.\n- Multiple and accurate choice for the metrics."}, "weaknesses": {"value": "- The core insight of deferring RoPE application is from Block-Attention. This work primarily optimizes where rotation happens in the kernel. The contribution is more engineering (still very valid) than algorithmic. \n- Evaluation uses only one model, which is specifically fine-tuned for Block-Attention. Generalization to other architectures unclear.\n- Table 1 shows hit ratios at 1/5/10GB, but no analysis of when memory constraints actually matter. With H100, why is 10GB limiting?\n- The method is tailored to RoPE-based attention. The claim that it “can easily extend to other positional encodings” is unsubstantiated. However, this is minor given that most LLMs use RoPE or RoPE-inspired PE."}, "questions": {"value": "- What happens with longer documents (>4096 tokens)? \n- What happens with other models, given that only one is tested ? \n- Does it work with larger KV Cache sizes, larger than 10 GB ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OrOcrSJbRH", "forum": "DrETNoeqS5", "replyto": "DrETNoeqS5", "signatures": ["ICLR.cc/2026/Conference/Submission9804/Reviewer_Qma1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9804/Reviewer_Qma1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918755621, "cdate": 1761918755621, "tmdate": 1762921289045, "mdate": 1762921289045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Retrieval-augmented generation (RAG) faces efficiency bottlenecks due to conventional key-value (KV) caches, which embed positional encoding directly into keys/values—limiting reuse to identical document positions and requiring $O(N!)$ space for $N$ documents—while existing solutions like Block-Attention still incur KV duplication. To solve this, Lazy-Attention proposes deferred positional encoding: it stores position-agnostic KV caches (only $O(N)$ space per document) and dynamically applies relative rotation (leveraging RoPE’s relative position property) via custom Triton kernels—adding 0.59% compute overhead in prefill (rotating KV tiles once) and 0.023% average overhead in decoding (rotating queries selectively). Evaluated on NVIDIA H100 with Tulu3-Block-FT (8B) across RAG benchmarks, it reduces TTFT by up to 1.37× vs. Block-Attention (skewed distributions), boosts cache hit rates by 44% (20.69% vs. 14.38% at 10GB VRAM), maintains comparable generation quality (avg. EM 69.2%), and has only ~0.2% total overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ **Ultra-high memory efficiency and cache reuse rate**: Conventional KV caches require $O(N!)$ space for $N$ documents due to position dependence, while Lazy-Attention decouples positional encoding from KV caches via deferred encoding, making KV caches position-agnostic—each document only needs one copy of KV cache (occupying $O(N)$ space). In scenarios with 10GB VRAM and high document skewness, its cache hit rate reaches 20.69%, 44% higher than Block-Attention (14.38%), significantly improving the reuse efficiency of hot documents in RAG scenarios.  \n+ **Low overhead and easy engineering deployment**: It optimizes the prefill and decoding phases with custom Triton kernels—only 0.59% compute overhead in prefill and 0.023% average overhead in decoding, with a total additional overhead of ~0.2%. No model training is required; it is implemented based on PyTorch/CUDA (5,000 lines of code) and can be seamlessly integrated into existing LLM serving frameworks like vLLM, compatible with optimizations such as continuous batching, resulting in low engineering deployment costs."}, "weaknesses": {"value": "+ **Limited experimental scope in model scale and task types**: The evaluation only uses the mid-sized Tulu3-Block-FT model (8B parameters) and focuses on RAG-oriented QA tasks (e.g., 2WikiMQA, HotpotQA), lacking validation on ultra-large LLMs (70B+ parameters) where KV cache dynamics and memory constraints may differ. It also fails to test non-technical scenarios like dialogue generation or creative writing, making it hard to confirm versatility across diverse LLM applications.  \n+ **Reliance on manual hyperparameter tuning**: Core parameters (e.g., KV tile size N=64, prefill query tile size M=128, decoding rotation trigger ratio r) are set manually without an adaptive adjustment mechanism. For extreme sequence lengths (e.g., <32 tokens or >1M tokens) or varying task characteristics, fixed parameters may cause redundant overhead (e.g., unnecessary rotations for short sequences) or insufficient error control (e.g., imprecise relative offsets for ultra-long sequences), increasing practical tuning costs.  \n+ **Unverified performance in extremely long-sequence scenarios**: While the paper validates long contexts (e.g., 4K-token documents), it does not test ultra-long sequences (e.g., millions of tokens). As sequence length scales to extreme sizes, cumulative overhead from deferred rotation may rise (e.g., more KV tiles requiring rotation), and the accuracy of relative offset calculations could degrade due to dispersed context information—issues not analyzed in the evaluation ."}, "questions": {"value": "I would be happy to increase my rating if my views are given a thorough discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "egZ1wyjDNk", "forum": "DrETNoeqS5", "replyto": "DrETNoeqS5", "signatures": ["ICLR.cc/2026/Conference/Submission9804/Reviewer_J9c8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9804/Reviewer_J9c8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990109989, "cdate": 1761990109989, "tmdate": 1762921288595, "mdate": 1762921288595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lazy or deferred attention scheme: store position-free K/V in the cache and apply RoPE (or more generally, the position transform) inside the attention kernel at lookup time, using the query–key relative offset. This lets different requests reuse the same per-document KV even when the document appears at different positions. The paper further gives two Triton kernels (prefill vs. decode) and integrates it into a vLLM-style runtime to show better TTFT and higher KV hit ratio under skewed RAG workloads."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a very real, very current bottleneck in RAG-serving: repeated passages at different offsets.\n\n- Moves positional handling into the kernel in a way that is compatible with existing vLLM runtimes and shows low overhead on real hardware.\n\n- Experimental evidence broadly matches the theoretical overhead story (small prefill cost, tiny decode cost) and shows higher KV hit ratio under skewed RAG.\n\n- Engineering is neat: two kernels, offset packing, and a clear explanation of where the extra FLOPs go."}, "weaknesses": {"value": "- Novelty is narrower than claimed. There are already RAG-oriented KV reusers (RAGCache) and KV-centric serving systems (Mooncake). There are already attention mechanisms that explicitly decouple RoPE from KV (MLA / TransMLA / DeepSeek-V2/V3). There is already block-wise RAG reuse that solves the “position doesn’t match” problem, albeit by re-encoding and fine-tuning (Block-Attn). Your paper’s real addition is to push that decoupling into a production kernel and make it work without retraining. That’s incremental. \n\n- Baselines omit some of the most relevant contemporary systems, so the empirical advantage is not yet airtight.\n\n- Scope is a bit tailored: H100 + vLLM + RAG with hot docs. It’s not yet shown this stays cheap on weaker GPUs or less skewed workloads.\n\n- Related work section needs to be rewritten to explicitly place the method next to RAGCache (tree-structured reuse), Mooncake (disaggregated KV), vLLM PagedAttention (logical–physical decoupling), and MLA/TransMLA (decoupled RoPE). Right now it overstates novelty."}, "questions": {"value": "Your idea becomes much more convincing when framed as \"kernelizing the known RoPE-decoupling trick for production RAG engines\" rather than \"we are the first to let multiple prompts share KV at different positions\". The former is true; the latter is not."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GtZFiRly84", "forum": "DrETNoeqS5", "replyto": "DrETNoeqS5", "signatures": ["ICLR.cc/2026/Conference/Submission9804/Reviewer_4N3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9804/Reviewer_4N3e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061271358, "cdate": 1762061271358, "tmdate": 1762921288258, "mdate": 1762921288258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}