{"id": "6J6yU6n5Bp", "number": 6966, "cdate": 1758003803109, "mdate": 1759897881266, "content": {"title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization", "abstract": "Group Relative Policy Optimization (GRPO) introduces a new paradigm for reinforcement learning in Large Language Models (LLMs), modifying PPO by eliminating the value model for efficient post-training. However, vanilla GRPO assigns equal weight to all prompts during policy updates, overlooking that some prompts lie beyond the LLMs’ current knowledge and can harm performance. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs’ uncertainty and uses it to modulate the learning process. This enables conservative updates for high-uncertainty prompts (e.g., beyond model knowledge) while preserving relatively higher signals for confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization. The code, implementation details, and pre-trained models will be publicly released.", "tldr": "", "keywords": ["LLM", "Post-training", "GRPO"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b28618bff2e766722d0eadf35cd7c6013b122a9.pdf", "supplementary_material": "/attachment/fa0f72821ee8e80e0f99263e9ff3e79815c33929.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the importance of different prompt in GRPO training. The paper first argues that training on some prompts on which the model is uncertain harms the training procedure. To mitigate this problem, the paper propose to use semantic entropy as an additional importance factor to reweight each prompt. The experiments on three models shows that proposed method outperforms a variety of baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper is shown as follows\n\n1. This paper propose SEED-GRPO, which mitigate the issue of training on potential harmful prompt in a simple way.\n\n2. The experiments are conducted on three models and tested on five datasets, and the results look promising.\n\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "The weaknesses of this paper are listed as follows\n\n1. The configuration of the baselines are unclear. It looks like that the paper simply integrate a bunch of off-the-shelf models trained with diferent algorithms as the baselines. However, it is unclear whether these models are trained from the same base model and with a same dataset. Given this, it is hard to conclude whether SEED-GRPO really outperform the baselines\n\n2. In the experiment setup, the maximum output is set to 3000 tokens. However, this might not be enough for hard datasets like AIME24. How would the perforamnce of SEED-GRPO compared to baselines if we allows more output tokens (e.g., 8k)? \n\n3. The argument that prompts inducing high uncertainty harms training lacks an empirical justification. Could the authors conduct a simple experiment, where the model is only trained on those prompts that the model is uncertain and report the performance (we would probably see a performance drop compared to the baseline)?"}, "questions": {"value": "See weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NAubPefykE", "forum": "6J6yU6n5Bp", "replyto": "6J6yU6n5Bp", "signatures": ["ICLR.cc/2026/Conference/Submission6966/Reviewer_q1DN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6966/Reviewer_q1DN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677140716, "cdate": 1761677140716, "tmdate": 1762919188998, "mdate": 1762919188998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SEED-GRPO, an enhancement to Group Relative Policy Optimization (GRPO) algorithm, which incorporates uncertainty-aware prompt reweighting during reinforcement learning. The key novelty lies in introducing semantic entropy, a measure of response diversity across multiple rollouts, as a prompt-level uncertainty signal. Intuitively, the more uncertain the response is, the fewer advantages it should have, and the less magnitude the policy update should be. This enables conservative learning on high-uncertainty prompts and encourages learning on confident cases.\n\nExtensive experiments on five mathematical reasoning benchmarks (AIME24, AMC, MATH, Minerva, and OlympiadBench) show that SEED-GRPO achieves state-of-the-art results, outperforming strong baselines like Dr.GRPO and DisCO even with smaller model sizes (7B vs. 32B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Quality: SEED-GRPO achieves state-of-the-art performance on average performance in five mathematical reasoning benchmarks with the Qwen2.5-Math backbone model. Over 15 baselines have been included for comparison.\n\n2. Clarity: The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Significance: The paper focuses exclusively on mathematical reasoning, where uncertainty and correctness are easy to define. It remains unclear how semantic entropy performs in open-ended or multimodal domains, where \"semantic clusters\" may not be easily defined. This introduces challenges for the algorithm to extend to more general scenarios.\n\n2. Novelty: Although the paper claims it is the first paper to incorporate uncertainty into GRPO, the actual implementation is essentially reweighting prompts based on the final answer's self-consistency, which is not new in GRPO [1]. It would be nice if the authors could tone down this claim.\n\n3. Quality: It would be nice if there were case studies to show that the interpretation in lines 273–300 is true in actual samples.\n\n4. Clarity: The background color of Figure 2 can be improved by using a consistent pure color to improve readability.\n\n### Reference\n[1]: Chen, Yi, et al. \"GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning.\" arXiv preprint arXiv:2506.16141 (2025)."}, "questions": {"value": "* It appears the rigorous form of f is not given. May I ask what the specific form of $f$ is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "elQb7S5mJb", "forum": "6J6yU6n5Bp", "replyto": "6J6yU6n5Bp", "signatures": ["ICLR.cc/2026/Conference/Submission6966/Reviewer_ogpC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6966/Reviewer_ogpC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785657056, "cdate": 1761785657056, "tmdate": 1762919188567, "mdate": 1762919188567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SEED-GRPO, an uncertainty-aware extension of Group Relative Policy Optimization for large language models. The key idea is to use semantic entropy to measure the model’s uncertainty about each prompt and adjust the policy update magnitude accordingly. Prompts with consistent responses receive stronger updates, while those with diverse or conflicting answers are updated more conservatively. The method requires no extra sampling cost since it reuses GRPO rollouts for entropy estimation. Experiments on five mathematical reasoning benchmarks show consistent improvements over strong baselines, achieving new state-of-the-art results with a 7B model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The idea is clear and intuitive\nUsing semantic entropy to control update strength is a natural way to make GRPO uncertainty-aware. It seems make sense.\n\n2. The method is simple and clean.  \nWith no extra sampling cost and limited training cost, making it easy to integrate.\n\n3. Experiment results.\nExperiments are strong and consistent across five math reasoning benchmarks; improvements over Dr.GRPO and other large baselines are convincing. Ablations are systematic and well presented, showing stable trends across α, weighting functions, and rollout numbers."}, "weaknesses": {"value": "1. Lacks details. \nsemantic grouping is not clear, will it affect the final performance?\n\n2. The entropy calculation\nThe semantic entropy is computed only from final answers, will it be better if we also consider the entropy for the thinking process?\n\n3. More benchmark results.\nIs it possible to extend the results on more benchmarks, not limited to math?"}, "questions": {"value": "Please make response to the points listed in the weaknesses. \nGive more details and analysis of the semantic grouping"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3IZQRe7JTc", "forum": "6J6yU6n5Bp", "replyto": "6J6yU6n5Bp", "signatures": ["ICLR.cc/2026/Conference/Submission6966/Reviewer_UZRX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6966/Reviewer_UZRX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864923895, "cdate": 1761864923895, "tmdate": 1762919187883, "mdate": 1762919187883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}