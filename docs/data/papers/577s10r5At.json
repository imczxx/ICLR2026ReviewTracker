{"id": "577s10r5At", "number": 14721, "cdate": 1758242421576, "mdate": 1759897352929, "content": {"title": "CTM-AI: A Blueprint for General AI Inspired by Consciousness", "abstract": "Despite remarkable advances, today's AI systems remain narrow in scope, falling short of the flexible, adaptive, and multisensory intelligence that characterizes humans. This gap has fueled longstanding debates about whether AI might one day achieve human-like generality or even consciousness, and whether principles of consciousness can inspire new architectures for general AI. This paper presents an early blueprint for implementing a general AI system based on the Conscious Turing Machine (CTM), a formal machine model of consciousness. The CTM has an enormous number of powerful processors ranging from specialized experts (e.g., vision–language models, search engines, APIs) to unspecialized general-purpose learners poised to develop their own expertise. Crucially, for whatever problem must be dealt with, the system need not know in advance which processors hold the relevant expertise; instead, multimodal machine learning methods enable the system to select, integrate, and fuse information across processors. We extend the CTM into a practical framework, the CTM-AI, and demonstrate its utility on diverse tasks including multimodal perception, tool learning with multiple APIs, and multi-turn web agent tasks. Together, this work offers a principled and testable blueprint for general AI inspired by computational models of consciousness.", "tldr": "", "keywords": ["conscious turing machine", "multimodal machine learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fcf94c07611857b6d2540fd7c228bdda5c2259a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a flexible multi-modal agentic framework inspired by Blum&Blum's Conscious Turing Machine (CTM). Many parallel processors (e.g., LLMs, VLMs, ALMs, ...) run in parallel. At every step, one output (or 'gist') is chosen based on a self-assigned weight, written into a global workspace, and broadcast to all processors. In addition, 'subconscious' links can be formed between processors, which allows them to answer each others queries. A supervisor model decides the output from the information in the global workspace. Experiments show that such agents are competitive with baselines, or even outperform them. All components (parallel sub-processes, global broadcasting, competitive gist selection, link-forming) contribute to the overall performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The framework is interesting and may have the potential to be a general setup for agentic systems.\n- Taking inspiration from the Conscious Turing Machine opens up a fascinating perspective on the parallels (and differences) between the human mind and AIs.\n- In the experiments, the CTM-AI performs competitively with, or even outperforms, several baselines on a range of multi-modal tasks.\n- An ablation shows that all components contribute to the CTM-AI's performance."}, "weaknesses": {"value": "The most important weak point is lack of detail in the implementation and experimental protocol. Agentic systems are difficult to reliably evaluate. It's all the more important to be very clear and explicit about the exact setup of the system and the experiments. The paper gives barely any detail on either, which is the main reason why I vote reject at the moment. Another concern is the relationship between the proposed framework and the CTM, as well as consciousness in general."}, "questions": {"value": "Below I list important points that, when addressed in a satisfactory way, would improve my score:\n- What are the exact prompts used for the different processors? How many different prompts were tried before settling on the ones for used in the experiments? How does this compare to the baselines? The paper claims that only very superficial changes are necessary to adapt it to different tasks. So were the same prompts (with only minor adaptations) were used in all experiments? \n- How many, and which kind of LTM processors (what exact inputs, which modalities, and which models) were used in the different experiments?\n- How exactly are the weights assigned to the gists? How exactly are links between LTM processors established?\n- How much compute (how many tokens, how many FLOPs) is used for each of the experiments? How does this compare to the baselines?\n\nOther things I would wished to be addressed are the following:\n- CMT-AI's placement in the literature seems to me a bit superficial. It is surely not the first work proposing a self-organized agentic system that uses natural language as a common medium of communication between specialized agents. As one example, \"Mindstorms in Natural Language-Based Societies of Mind\" (Zhuge et al., 2023) proposes related concepts.\n- The algorithm in the appendix at the moment contains many functions which are not clearly defined, which strongly limits is usefulness. How do Fuse(), UpTree(), Act() and IsHelpful() work? I know that it is pseudocode, but the paper does not give sufficient detail to implement the algorithm.\n- How is 'brainish' different from natural language? If I understand it correctly, in the CMT, brainish is supposed to be inherently multimodal, not explicitly symbolic or language like. Can gists contain anything other than language, if so, how can all models understand them (if there's an image in a gist, an ALM won't be able to process it).\n- What was process like of translating the original CTM to the LM-based CTM-AI? How did the authors decide which aspects to include? For example, in the original CTM, the 'Model of the World' LTM plays quite a crucial role, yet it does not appear in the CTM-AI. Did the authors not deem it important? Or did they try it and it didn't work?\n- More generally, what is the author's perspective on how the CMT-AI relates to the CMT framework? Was the CMT simply an inspiration? Does the fact that CMT-AI works support the validity of CMT as a model of consciousness? Would this then mean that CMT-AI is conscious? I think dealing with these concepts, even if they just appear in the name, requires a careful discussion of such questions.\n- The title overpromises; it is very unclear to me to what extent the proposed framework is a blueprint for general AI, and also how exactly it inspired by consciousness (as opposed to simply by some aspects of the CMT model).\n\nSome additional small comments and corrections:\n\n- P.8: Figure 3 is mentioned twice, but it doesn't exist. I assume the authors mean Table 3?\n- P.9 line 467 \"and explicitly asking for more surrounding context\" is grammatically incorrect in this context.\n- Appendix E: Please reference Table 7, it might not be immediately obvious that the table belongs to the section. A more detailed analysis could be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BOniGCXVhp", "forum": "577s10r5At", "replyto": "577s10r5At", "signatures": ["ICLR.cc/2026/Conference/Submission14721/Reviewer_NwUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14721/Reviewer_NwUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659508921, "cdate": 1761659508921, "tmdate": 1762925083820, "mdate": 1762925083820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents blueprint for implementing a general AI system based on the Conscious Turing Machine (CTM), a formal machine model of consciousness. Unlike other cognitive architectures, CTM employs a global workspace and distributed competition and collects parallel independent cognitive, sensory, motor, and extended processors."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n- Practical application of CTM model by CTM-AI\n- Empirical demonstration of CTM-AI as a general and multi-action AI, including language modeling, multimodal understanding and human behavior understanding"}, "weaknesses": {"value": "- The proposed algorithm is dependent on finite lifetime T and height h of binary tree, Up-Tree. Can the author provide an empirical evaluation of how these parameters impact model’s performance? An example of such analysis could the the answer of RQ1 of the following paper [1]\n\n-Can the author compare their proposed methods alternative approaches (e.g., ensemble voting, query augmentation, or multi-agent debates?)\n\n\n\nReferences\nSaha, Swarnadeep, Peter Hase, and Mohit Bansal. \"Can language models teach? teacher explanations improve student performance via personalization.\" Advances in Neural Information Processing Systems 36 (2023): 62869-62891."}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ESw4tOiXSt", "forum": "577s10r5At", "replyto": "577s10r5At", "signatures": ["ICLR.cc/2026/Conference/Submission14721/Reviewer_JtxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14721/Reviewer_JtxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957011250, "cdate": 1761957011250, "tmdate": 1762925083360, "mdate": 1762925083360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CTM-AI, a multi-processor architecture inspired by the Conscious Turing Machine. Specialized and general processors compete to write into a global workspace (STM), whose content is then broadcast; processors can also form dynamic links. The system is evaluated on multimodal perception, tool use, and small web-agent tasks and shows improvements over reported baselines. Ablation studies convincingly validate each component. However, baseline comparisons all use 2021-2023 models (vs. 2024 Gemini-2.0),  and offers limited proof of cross-modal balancing. These are addressable issues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear presentation with solid ablation studies validating each component's contribution. \n\n2. The closed-loop mechanism of module competition, STM storage, and global broadcasting is well-motivated and modular, facilitating transfer to other tasks.\n3. Demonstrates results on multimodal understanding, tool use, and small web-agent tasks, suggesting the approach is not task-specific."}, "weaknesses": {"value": "1. CTM-AI uses Gemini-2.0-flash-lite (2024) while baselines are from 2021-2023 (MMoE, BLIP2, etc.), making it difficult to isolate architectural contributions from model improvements. \n\n2. Efficiency reporting (suggested). It would strengthen the paper to include average end-to-end latency, API calls per task, and cost relative to the base model to contextualize practical overheads."}, "questions": {"value": "1. How are rare or weak modalities weighted or gated so they are not overshadowed by dominant, feature-rich modalities (e.g., vision/text)?\n\n2. Could you share systematic failure modes (e.g., vision-only misleads, unhandled tool errors) and the mitigations you found effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0utuwKfLwL", "forum": "577s10r5At", "replyto": "577s10r5At", "signatures": ["ICLR.cc/2026/Conference/Submission14721/Reviewer_dnVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14721/Reviewer_dnVs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990427169, "cdate": 1761990427169, "tmdate": 1762925082850, "mdate": 1762925082850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical method for enhancing integration of multi-modal perceptions for abstract decision making and tool using. This applies the mechanism of the “Conscious Turing Machine” to current multimodal pre-trained large models. The experiments somehow show improvements on classification tasks, long term decision tasks, and tools using tasks with multi-modal models without reinforcement post-raining. However, some concern arises about the implementation methods, and discussions about the reliability are missing for some of the concepts defined. And the reinforcement learning models are not compared in terms of both theoretical and empirical justification of absolute improvement orthogonal to the advantages introduced by post-training, which makes the contributions hard to identify."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic discussed is new, which tries to build an agentic framework that is biomimetic.\n2. The paper is easy to follow in terms of what the authors intend to show.\n3. The authors try hard to provide empirical evidence that the framework has superiority over “monolithic” models such as GPT-4o and Gemini-Flash-Lite."}, "weaknesses": {"value": "1. The generation procedure of “utility weight”, defined as an element of the chunks, is not introduced. Moreover, the definition as “certainty or utility” makes it hard to buy it.\n2. Authors should add more discussion to justify in what circumstances the advantages can be achieved compared to other advanced techniques, such as post-training. To achieve this, rigorous theoretical discussions or a more serious demonstration of the ablation study should be provided.\n3. The efficiency of the provided method and how to optimize it are not provided in the manuscript.\n4. The computation consumption & parameter number of the baselines is not clearly introduced and compared in the manuscript, which leads to fairness concerns."}, "questions": {"value": "See weaknesses.\nFurther question as following:\nHow the proposed method processes nested complex structure in each perception that is chunked to a swollen chunk?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mXUtv0n1OA", "forum": "577s10r5At", "replyto": "577s10r5At", "signatures": ["ICLR.cc/2026/Conference/Submission14721/Reviewer_Wsj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14721/Reviewer_Wsj6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993304964, "cdate": 1761993304964, "tmdate": 1762925082490, "mdate": 1762925082490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CTM-AI, a practical framework that operationalizes the theoretical Conscious Turing Machine (CTM) model. The architecture is designed to mimic principles of consciousness by using a large number of distributed processors that operate in parallel. These processors are not limited to modalities but include specialized experts for sensory input, cognitive reasoning, and extended tool/API use. Instead of a central executive, the system uses a \"global workspace\" (Short-Term Memory) where processors compete via an \"up-tree\" mechanism to select a winning \"chunk\" of information. This winning chunk is then broadcast via a \"down-tree\" to all other processors, enabling dynamic information fusion and iterative reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1) The core contribution of bridging the gap between the abstract Conscious Turing Machine (CTM) theory and a concrete, implementable AI system is novel and interesting6.\n\nS2) The paper provides a clear explanation of its dynamics, supported by strong ablation studies (Table 3, 5) and an intuitive case study (Figure 2) that effectively demonstrates the iterative reasoning process.\n\nS3) The experimental evaluation is rigorous, testing the framework's versatility across a diverse set of tasks, including multimodal perception (MUSTARD), tool use (StableToolBench), and agentic web tasks (WebArena)."}, "weaknesses": {"value": "O1)  Distinction with multi-agent systems: The paper's primary claim of differentiation from multi-agent systems rests on CTM-AI being \"free of a central executive\" and not using \"fixed role assignments\". However, this distinction feels underspecified. The \"Up-Tree competition\" mechanism, which selects the chunk with the highest weight, and the \"STM processor\" (a stateless LLM that produces the final answer)  functionally serve as a form of selection and orchestration. The paper would be stronger if it more directly contrasted its dynamic, competition-based mechanism against a modern multi-agent orchestrator on a complex task.\n\nO2) Discrepancy Between Theory and Implementation: The paper introduces the formal CTM with an \"enormous number of powerful processors\". It then abandons the CTM's core hierarchical, local competition mechanism in favor of a simple \"global competition\" (argmax). The justification is that \"typically only a few (<10) LTM processors are active\". This simplification, motivated by a small-scale implementation, undermines the scalability and theoretical grounding the CTM framework is supposed to provide.\n\nO3 ) Ambiguous Learning Mechanism: The paper's definition of \"learning\" is weak and underdeveloped. Section 3.3 describes learning as \"in-context learning with memory updates\", which consists of broadcasting the winning chunk to all processors' memories and updating the link matrix. It is unclear if the processor parameters (theta_i) themselves are ever updated. If all \"learning\" is purely modifying the LTM (prompt memory) and processor graph, this is a significant limitation and may not support the deep adaptation or skill acquisition implied by the \"general AI\" goal."}, "questions": {"value": "Please address weaknesses O1-O3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Prrt9cFoc4", "forum": "577s10r5At", "replyto": "577s10r5At", "signatures": ["ICLR.cc/2026/Conference/Submission14721/Reviewer_i9GU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14721/Reviewer_i9GU"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14721/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007608524, "cdate": 1762007608524, "tmdate": 1762925082118, "mdate": 1762925082118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}