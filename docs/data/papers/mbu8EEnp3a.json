{"id": "mbu8EEnp3a", "number": 25050, "cdate": 1758363507462, "mdate": 1759896736353, "content": {"title": "Do LLMs Signal When They’re Right?  Evidence from Neuron Agreement", "abstract": "Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders (e.g., majority voting), achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can be poorly calibrated after post training. We instead analyze internal behavior based on neuron activations and uncover three findings: (1) external signals are low dimensional projections of richer internal dynamics; (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and (3) activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge. Motivated by these observations, we propose Neuron Agreement Decoding (NAD), an unsupervised best of N method that selects candidates using activation sparsity and cross sample neuron agreement, operating solely on internal signals and without requiring comparable textual outputs. NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping. Across math and science benchmarks with verifiable answers, NAD matches majority voting; on open ended coding benchmarks where majority voting is inapplicable, NAD consistently outperforms Avg@64. By pruning unpromising trajectories early, NAD reduces token usage by 99\\% with minimal loss in generation quality, showing that internal signals provide reliable, scalable, and efficient guidance for label free ensemble decoding.", "tldr": "", "keywords": ["Neuron-Agreement Decoding (NAD); Neuron activation patterns; Unsupervised answer selection; Chain-of-thought ensembling; Token efficiency"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4188ed87d8238e926d1ecc665363ba8ac4330b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Neuron Agreement Decoding (NAD), an unsupervised best-of-N method that selects candidates using activation sparsity and cross sample neuron agreement. NAD is motivated by three observations:\n\n- External signals are low dimensional projections of richer internal dynamics;\n- Correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and\n- Activations from correct responses exhibit stronger cross sample agreement, whereas\nincorrect ones diverge\n\nThe authors claim that:\n\n- NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping\n- NAD matches the accuracy of majority voting in math and science benchmarks and outperforms Average@64 in open-ended coding benchmarks.\n- NAD reduces token usage by 99% with minimal loss in generation quality"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a promising method which uses mechanistic interpretability for selecting best reasoning traces in Best-of-N sampling\n- The early stopping analysis may be of interest to the efficient inference community."}, "weaknesses": {"value": "## Major Weaknesses\n\n- Preliminary claims are poorly justified\n    - Section 3.2:  The authors claim that neuron activation patterns capture structure beyond what entropy represents, citing that samples within clusters have varying entropy values. However, this conclusion is poorly justified.\n        - First, they have shown that the *number* of activated neurons correlates with entropy, suggesting the clustering is partially driven by a scalar feature that entropy already captures.\n        - Second, any high-dimensional representation will trivially contain structure that a single scalar cannot fully represent, which is not evidence of meaningful structure. The variation in entropy within clusters could simply reflect noise, measurement artifacts, or the fact that t-SNE on Jaccard distances emphasizes pattern overlap rather than distributional properties. At the moment, the more likely conclusion from Figure 3 is that one metric does not perfectly predict another.\n    - The preliminary experiments are done with only one model (Qwen3-4B) which may not be generalizable.\n- Lack of motivation on the experimental setup\n    - It is unclear why the models are selected (Qwen3-4B-thinking-0527, Qwen3-4B-Instruct-0527 and DeepSeek-R1-0528-Qwen3-8B). Is it because of the different reasoning training regime? or are there specific reasons?\n    - Lack of baselines.\n        - This is very critical especially because the authors claim that the method captures structure beyond what the “external behaviors” can, thus it is natural to expect that NAD would outperform prior works which are based on these external behaviors:\n            - Majority-based selection: Universal Self Consistency [1]\n            - Confidence-based selection: Self-Certainty[2], DeepConf [3], PiCSAR [4]\n            - Length-based selection: short-1@k [5]\n- Lack of statistical rigor\n    - As the paper is dealing with sampling, the authors should try to run the experiments with multiple random seeds to account for stochasticity.\n\n## Additional Suggestions\n\n- L56: Please cite the GPT-4 reports\n- Figure 3: Update the colorbar label (”Average Entropy”)\n- Section 3.2: I believe the Jaccard index is calculated pairwise among all responses across all questions. Please add that explanation in the paragraph\n- Figure 6, 7, and 8 are ordered awkwardly. Figures 7 and 8 are mentioned in the text earlier than Figure 6.\n- The model name Qwen3-4B-thinking-0527 is perhaps a typo? it should have been Qwen3-4B-thinking-**2507**\n\n## References\n\n- [1] Universal Self-Consistency for Large Language Model Generation\n- [2] Scalable Best-of-N Selection for Large Language Models via Self-Certainty\n- [3] Deep think with confidence\n- [4] PiCSAR: Probabilistic Confidence Selection And Ranking for Reasoning Chains\n- [5] Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning"}, "questions": {"value": "- Figure 2:\n    - Have you tried separating the correct vs incorrect instances in Figure 2? The trend may differ between the two categories.\n    - Have you tried plotting Figure 2 in log-log scale? I suspect that there is a power-law relation there, which may be interesting.\n- Why are the AIME24 and AIME25 reported as one task?\n- Why is GPQA under Math Reasoning? Which subset of GPQA did you use?\n- Table 2: It is rather awkward to report the total token consumption. Any particular reason why you choose to report that? I believe we are more interested in the average number of tokens saved per question (with confidence interval).\n- In Section 5.3 analysis of the top-k method, what is the metric used to decide the separation? You should consider using statistical test to quantify it.\n- Figure 8:\n    - Is this averaged across questions? If yes, please provide the confidence interval bars.\n    - Am I understanding it incorrectly? Because the B=16k seems to achieve the highest accuracy, which contradicts the conclusion mentioned in the text.\n    - What should I interpret from the token consumption line in the plot?\n- Is there a way to automatically decide the early stopping position? If not, it seems like a difficult hyperparameter to tune."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kDwXNnnqSH", "forum": "mbu8EEnp3a", "replyto": "mbu8EEnp3a", "signatures": ["ICLR.cc/2026/Conference/Submission25050/Reviewer_NEts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25050/Reviewer_NEts"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604940256, "cdate": 1761604940256, "tmdate": 1762943301886, "mdate": 1762943301886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether an LLM’s neuron activations can be used to determine if its generated response is correct. The authors contrast this with prevailing methods that rely on \"external\" signals like token probabilities, output entropy, or model self-evaluations, which the paper argues can be poorly calibrated.\n\nThe paper shows that these external signals are effectively low-dimensional projections of richer, high-dimensional internal dynamics. The authors' analysis uncovers two key findings. Sparsity: Correct responses activate \"substantially fewer unique neurons\" than incorrect responses during generation. Agreement: The activation patterns from correct responses exhibit \"stronger cross-sample agreement,\" while incorrect responses tend to diverge.\n\nMotivated by these observations, the paper proposes a novel unsupervised method called Neuron Agreement Decoding (NAD). NAD selects the best response from a batch of $N$ samples by identifying the candidate with the highest activation agreement with its peers or, alternatively, the one with the fewest activated neurons (activation sparsity)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength is its novel investigation that successfully links an LLM's internal neuron activation patterns to the external correctness of its reasoning. Specifically, looking at the number of activated neurons and how they overlap between different inputs can provide a signal for whether the answer is correct. This is pretty cool and I haven’t seen such an exploration before.\n\nOne strength of NAD is its ability to operate without requiring comparable textual outputs, unlike majority voting. This makes it applicable to open-ended tasks like code generation, where majority voting is often inapplicable. This is an important direction for research these days.\n\nNAD matches or outperforms the performance of majority voting on math and science benchmarks and open-ended coding benchmarks. This is pretty strong evidence that NAD can work well, and without using too much inference time (or even reducing compared to Majority voting over many samples)."}, "weaknesses": {"value": "* One area I am quite skeptical about is whether this method works when the base model has relatively high or relatively lower accuracy on the task in the first place. The experiments right now show that NAD works in the “middle ground” regime, with about 50-70% accuracy. However, for high accuracy (>90%) then NAD seems to degrade performance. Similarly if the original performance is low, I can imagine that the neuron activations could be more “random” so that NAD method doesn’t work. \n\n* Also the value of Avg@64 on these tasks is surprisingly high (since you are averaging over 64 outputs), which means the model is inherently very confident on these tasks. It could very well be the case that NAD only improves performance if Avg@64 is similar to Pass@1 or something. Basically the model is not very creative and only tries the same types of solutions. \n\n* There are no baselines in this paper. The paper only compares its own variants and the base models. I generally am skeptical about a paper without any other methods in the experiments. I understand that NAD is kind of a unique method, but there are a lot of test-time methods for improving reasoning these days. For example, TTRL (https://arxiv.org/abs/2504.16084) or the authors already discuss DeepConf (https://arxiv.org/abs/2508.15260). I am less interested in the 3 different clustering methods (which are basically an ablation for NAD)."}, "questions": {"value": "* For the evaluations, although the datasets are varied, the performance is somewhat clustered. What happens when the base model has high accuracy (e.g., GSM8k)? Or low accuracy, on some harder benchmarks (e.g., Humanities Last Exam or some of the newer benchmarks with search like SealQA https://arxiv.org/abs/2506.01062). Do we see any benefit or does it also degrade? \n\n* What happens if you perform the analyses with different numbers of samples? The analysis right now is very focused on 64. However, it is not clear if this is kind of a local maximum for NAD performance or whether 32 and 128 also exhibit good performance.\n\n* Similarly, what if Avg@64 is low because the model can output a lot of wrong answers if you keep sampling. This seems like the dominant regime if we are looking forward to AGI and harder tasks. Can you say something about if NAD will work then?\n\n* How does NAD compare to other method’s performance on the same models/benchmarks? The paper cites a few majority voting variants, or compare against TTRL or DeepConf or any of the methods the paper mentions about “external” rewards. These are still very valid approaches for the task at hand.\n\n* This is more minor, but there is a lot of work on decoding methods for improving model outputs. I would say it is worth citing these, and perhaps comparing against them. For example, Factuality Decoding methods also use internal signals (internal layers) to improve the output, e.g., DoLA (https://arxiv.org/abs/2309.03883) and SLED (https://arxiv.org/abs/2411.02433). I would take a look at TTRL (https://arxiv.org/abs/2504.16084) and the forward citations as well. I think currently there are only **4 papers cited in the related work** section, which is quite limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D99DklozeF", "forum": "mbu8EEnp3a", "replyto": "mbu8EEnp3a", "signatures": ["ICLR.cc/2026/Conference/Submission25050/Reviewer_nzaY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25050/Reviewer_nzaY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759402964, "cdate": 1761759402964, "tmdate": 1762943301673, "mdate": 1762943301673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- **Core idea:** Use internal neuron activations (not output logits/entropies) to score and select reasoning traces. Two signals: (i) activation sparsity (correct traces activate fewer unique neurons) and (ii) cross-sample neuron agreement (correct traces share more similar activation sets).  \n- **Method:** Neuron Agreement Decoding (NAD): build a Jaccard-similarity matrix over activated-neuron sets across sampled traces and select via kNN/medoid/DBSCAN; a MinAct variant selects the fewest-unique-neuron trajectory. Early stopping uses the same signals after the first 32 tokens (B=32) to prune low-quality paths.  \n- **Findings:** On AIME24/25 & GPQA, NAD matches majority vote while enabling aggressive early stop; on code (HumanEval, MBPP, LiveCodeBench), where voting is hard, NAD beats Avg@64. Reported token reductions up to ~99% with small accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Insightful internal analysis:** shows entropy/self-certainty are low-dimensional projections of richer activation dynamics; correct traces are sparser and more aligned across samples.  \n- **Simple selection rules:** kNN/medoid/DBSCAN over Jaccard of activated-neuron sets; unsupervised and label-free at test time.  \n- **Early-stop lever:** practical chunked early-stop at 32 tokens with large token savings in parallel sampling."}, "weaknesses": {"value": "- **Positioning vs token-confidence baselines:** Conceptually close to self-consistency / DeepConf (token-level confidence/entropy) but at the neuron level; however, there is no apples-to-apples comparison against DeepConf under the same sampling regime (accuracy + compute).  \n- **“Early correctness within 32 tokens” needs clarification:** Paper sets early stop at B=32 and infers quality from internal signals—not ground-truth correctness mid-generation. Clarify how “NAD enables early correctness prediction within the first 32 generated tokens” is quantified and whether OOD checks were made to avoid overfitting to seen patterns.  \n- **Scope & generality:** Signals are shown strongly on AIME-style math; for open-ended tasks (code), MinAct can underperform, and neuron-agreement advantages shrink—casting doubt on broad generality (e.g., free-form scientific discovery).  \n- **Cost reporting is incomplete:** Paper emphasizes token savings, but wall-clock, activation extraction overhead, pairwise Jaccard construction, and memory/storage (noted in Limitations) aren’t benchmarked vs strong external baselines.  \n- **Baselines:** Mainly Avg@64 and Cons@64; missing self-evaluate before ensemble and confidence-based (e.g., DeepConf) under matched budgets."}, "questions": {"value": "1. **Meaning of “early correctness”:** When you say “enables early correctness prediction within the first 32 tokens”, do you mean ranking traces by internal signals at 32 tokens and later verifying with ground truth, or a calibrated correctness probability? How is this measured, and did you test OOD prompts to check robustness?  \n2. **DeepConf comparison:** Please provide matched-budget comparisons to DeepConf (token-level entropy pruning): final accuracy, token count, wall-clock, and memory. This will isolate the incremental value of neuron-level signals over token-level confidence.  \n3. **Generalisation beyond math:** Your Figure 6 suggests weaker or minimal gains (even reversals) for code. Can you evaluate on open-form science benchmarks to test whether neuron sparsity/agreement remains predictive when answers are not short-form/numeric?  \n4. **Computation & storage:** Please report the per-token/trace overhead of computing activation sets, building the n×n Jaccard matrix, and memory footprint (with/without bitset compression), compared to token-confidence baselines.  \n5. **Ablations:** How sensitive are results to the activation thresholding (top-k per token), chunk size B=32, and the choice among kNN/medoid/DBSCAN? Could later chunks introduce noise (as hinted by Figure 8), and how does this vary by task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1zn4ZXUZN7", "forum": "mbu8EEnp3a", "replyto": "mbu8EEnp3a", "signatures": ["ICLR.cc/2026/Conference/Submission25050/Reviewer_jKnt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25050/Reviewer_jKnt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948794512, "cdate": 1761948794512, "tmdate": 1762943301286, "mdate": 1762943301286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies internal activation patterns in LLMs and reports two empirical regularities: (i) correct generations activate fewer unique neurons than incorrect ones; and (ii) correct generations show higher cross-sample neuron-set agreement. Building on these observations, the authors introduce Neuron Agreement Decoding (NAD)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel internal-signal criterion: Selecting candidates via Jaccard agreement of activated-neuron sets rather than output-space agreement is intellectually novel\n\n2. Computational savings: Early pruning at 32 tokens yields two orders of magnitude fewer tokens with modest accuracy impact\n\n3. Method simplicity: NAD relies on inexpensive set operations over FFN activations; the MinAct variant is parameter-light, aiding adoption."}, "weaknesses": {"value": "1. External validity to large/closed models: All results are on small/medium, open models. It is unclear whether the “fewer-neurons-when-correct” regularity and NAD’s gains hold for frontier models (70B–>100B)\n\n2. Definition and sensitivity of “activated neuron set”: The operational definition depends on thresholds/top-k within layers and across chunks. Although ablations exist, a more systematic sensitivity analysis (varying k, chunk size B, layer subsets, and gating functions) would strengthen your claims. \n\n3. Sampling hyperparameters: Results are reported for T=0.6, top-p=0.9; robustness to temperature/top-p and to different N would help verify the effectiveness."}, "questions": {"value": "1. How does NAD’s advantage change with larger models and larger N?\n2. Could you provide a more solid theoretical analysis for your arguments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q2QF9xWDWT", "forum": "mbu8EEnp3a", "replyto": "mbu8EEnp3a", "signatures": ["ICLR.cc/2026/Conference/Submission25050/Reviewer_gDoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25050/Reviewer_gDoM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981191794, "cdate": 1761981191794, "tmdate": 1762943301052, "mdate": 1762943301052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}