{"id": "SjAKLyCvmC", "number": 20593, "cdate": 1758308059863, "mdate": 1759896969031, "content": {"title": "Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model", "abstract": "To foster social bonding, humans naturally develop the ability to select appropriate conversational skills (e.g., empathy) based on situational context—a cognitive process we term skill-of-mind. However, LLMs often struggle to generate human-like responses in complex social dialogues. To address this, we propose a 100K skill-of-mind-annotated conversation dataset, Multifaceted Skill-of-Mind, which includes 38 conversational skills across various interactive scenarios (e.g., chitchat), grounded in diverse social contexts (e.g., demographics). Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, Thanos, with model sizes of 1B, 3B, and 8B parameters. We also introduce a comprehensive benchmark suit, ThanosBench, for assessing both capabilities of skill-of-mind and response generation in LLMs. Through extensive experiments evaluating 12 LLMs, Thanos demonstrates performance comparable to Claude-3.5-Sonnet, even outperforming LLaMA-3.1-405B. Specifically, Thanos enhances LLM-generated responses, making them more human-favorable and empathetic communication. Because we find out that recent high-performing LLMs still struggle to exhibit superior skill-of-mind capabilities, we believe it is invaluable to highlight the inherent challenges in this area.", "tldr": "We introduce a new skill-of-mind-infused LLM families with model sizes of 1B, 3B, and 8B and demonstrate its effectiveness on ThanosBench.", "keywords": ["Skill-of-Mind", "Large Language Model", "Social Interaction", "Conversational Skill"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8c8cc0f88c288a2010994ffead4043e0ca159e8.pdf", "supplementary_material": "/attachment/2b43aee7d6eebeb2959bcf6840a98b959e667848.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Skill-of-Mind (SoM), a new idea for social dialogue that models how humans choose the right conversational skill—like empathy, persuasion, or humor—in conversation.\nThe authors build three parts:  1. a GPT-4–labeled dataset of 100k dialogues with 38 skills and explanations from 12 public sources. 2. THANOS models (1B/3B/8B): fine-tuned from LLaMA-3 to generate explanations and select skills. THANOSBENCH: a benchmark testing skill reasoning and response generation. Results show THANOS-8B outperforms LLaMA-3.1-70B/405B and nears Claude-3.5-Sonnet, while THANOS-3B enhances GPT-4o and Gemini responses without extra training."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- SoM is a fresh idea, different from chain-of-thought and commonsense reasoning. It focuses on strategy in social context.\n- 100k labeled items covering 38 skills, with explanations + skills, which is rare and valuable for social context modeling.\n- Fine-grained breakdown across abilities (Perspective-Taking, Semantic Similarity, Mentalizing, etc.) helps reveal current bottlenecks in social intelligence."}, "weaknesses": {"value": "- The entire \"skill-of-mind\" annotation process for the dataset is based on automatic generation by GPT-4. Although the authors validated the annotation quality through human evaluation (Krippendorff's α = 0.62), this method may introduce GPT-4's own biases and limitations. The model's performance ceiling is, to some extent, limited by the capabilities of GPT-4 acting as the \"teacher.\"\n- SOM groundtruth are auto-generated by a single model. Even with some human checks, there’s a risk of closed-loop bias (GPT-4 labels → GPT-4 judges). It would be better to use independent judges (e.g., Claude) and more human evaluations.\n- The paper lacks qualitative error analysis. It would benefit from showing representative failure cases—where the predicted skill mismatches the context or the generated explanation fails to justify the response—to clarify the limitations of the SoM reasoning process."}, "questions": {"value": "- Have you performed ablation studies to quantify the individual contributions of the “explanation” and the “skill label”? It would also be helpful to clarify how much Skill-of-Mind improves the interpretability of the generated responses.\n- I’m curious what the results would be if, instead of using the SoM skill as the intermediate explanation, GPT-4 directly generated other empathy-related reasoning strategies.\n- How much did it cost to generate these datasets using GPT-4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O1Tqe7cHMv", "forum": "SjAKLyCvmC", "replyto": "SjAKLyCvmC", "signatures": ["ICLR.cc/2026/Conference/Submission20593/Reviewer_TvgS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20593/Reviewer_TvgS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732598422, "cdate": 1761732598422, "tmdate": 1762934002859, "mdate": 1762934002859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel concept called \"skill-of-mind\" to improve the social intelligence of LLMs in conversational settings. The core idea is to explicitly model the cognitive process of selecting an appropriate conversational skill by first generating a rationale (\"explanation\") and then choosing a skill from a predefined set. The authors create a large-scale dataset and use it to fine-tune a family of LLaMA-based models called THANOS for this task . The paper shows that using THANOS to guide other LLMs can significantly improve the quality of their generated responses, making them more natural and human-favorable. The work also contributes THANOSBENCH, a comprehensive benchmark for evaluating these capabilities ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed \"skill-of-mind\" method is conceptually appealing and novel. It provides a structured and interpretable way to model a complex cognitive process by breaking it down into an explanation and a skill selection, moving beyond simple end-to-end response generation.\n2. The experiments are extensive, consisting of fine-tuning and evaluation across 12 different base models from various families (GPT, Claude, Gemini, LLaMA, Gemma, Qwen) , which demonstrates the general applicability of the proposed method.\n3. The topic is very interesting."}, "weaknesses": {"value": "There is one main concern I have: the central claim of generating more \"human-like\" or \"human-favorable\" responses is not adequately supported by human evaluation, creating a methodological loop. The core evaluation of response quality (Table 5, 8) relies on another LLM (GPT-4o-mini) as the judge. The entire pipeline involves a dataset generated by GPT-4 (gpt-4-turbo) used to train a model (THANOS), which is then checklist-based  evaluated by another GPT model(GPT-4o-2024-11-20). This closed loop can only demonstrate that THANOS is effective at mimicking GPT-4's preferences, not that it is genuinely more human-like. The human evaluation presented (Table 6) is far too small (N=70 dialogues ) and limited to small models (Gemma-2-2B and LLaMA-3.1-8B ) to be considered conclusive evidence for the paper's primary claim."}, "questions": {"value": "1. I’m finding it difficult to reconcile the paper's central claim of improving \"human-like\" responses with the heavy reliance on LLM-based evaluation. It seems that the pipeline, training on GPT-4's synthetic data and then evaluating with another GPT model, creates a methodological loop. I was wondering if the authors could provide a large-scale human A/B test, particularly for the stronger models like THANOS-guided GPT-4o, to more conclusively support the claims made in Table 5 and Table 8?\n2. I'm still a bit unconvinced that the \" explanation-skill-response \" framework is fundamentally different from a well-crafted Chain-of-Thought (CoT) prompt. This  pipeline seems structurally analogous to CoT, and the comparison to DOCTOR in Table 8 doesn't quite address this, as it's a specialized model. I am very curious to know if the authors have results comparing their method to a generic CoT baseline (e.g., \"think step-by-step before responding\")? It would be interesting to see if the explicit \"skill\" component is truly necessary.\n3. The list of 38 skills is quite extensive, but I found its structure a bit arbitrary, mixing high-level concepts like \"Empathy\" with simple behavioral descriptions like \"Immediate Response.\" While I understand from Appendix D.2 that proposing a new taxonomy wasn't the goal, I was wondering if the authors have thought about the implications of this ad-hoc design? For instance, have you considered analyzing skill co-occurrence or grouping them into a more coherent hierarchy to provide a more principled justification for the list?\n4. Could you clarify the multi-turn inference process? It appears the skill-of-mind reasoning is generated fresh each turn and then discarded, rather than being added to the conversational history. I am curious about the rationale for this design and whether an alternative, preserving a 'reasoning history', was considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yNOyFqajPj", "forum": "SjAKLyCvmC", "replyto": "SjAKLyCvmC", "signatures": ["ICLR.cc/2026/Conference/Submission20593/Reviewer_z354"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20593/Reviewer_z354"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843796493, "cdate": 1761843796493, "tmdate": 1762934002461, "mdate": 1762934002461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces skill-of-mind, a novel framework inspired by how people talk and think in social settings. The main idea is that before replying, an AI should first decide what kind of skill is needed, like empathy, clarification, or ethics. and after that generate its answer. They curate a dataset with 100k samples. Each sample contains explanations and 38 conversation skills taken from 12 sources. These also include info like who is speaking, their relationship, and memories. They then used this dataset to train a group of models, ranging in size, to guess both the reasoning and the skill behind what someone says. They tested their models on a test set with 8 datasets to check how well models do in skill reasoning and how good their responses sound, using both humans and LLMs as judges. The main finding is that even the best models like GPT-4o and Claude-3.5 perform poorly on skill-of-mind tasks. But adding their trained 3B model, makes LLMs sound more natural, empathetic, and human-like."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The concept of skill-of-mind, which connects human social thinking, theory of mind, and real conversation strategy, is interesting.\n- The proposed dataset is strong as it also contains explanations with the skill labels.\n- THANOSBENCH tests not only if a model can guess the right skill, but also if it helps make better replies. The checklist-style evaluation makes results more clear and fair.\n- The author's proposed system worked better than existing SOTA methods to produce more natural replies.\n- The paper is well written and is easy to follow."}, "weaknesses": {"value": "- The training and testing methodology is creating a loop in terms of LLM dependance. That is, the skills and explanations come from GPT-4, and then THANOS is trained and tested on that same GPT-4 data. This way, the benchmark might just reward models that copy GPT-4's style instead of showing real human-like thinking.\n- The 38 skills mix different kinds of things like memory, social behavior, and how people move topics. Although this mix is practical, but it is also confusing. It’s hard to tell what exactly the model is learning, like is it learning a mental skill, a social act, or a speaking trick.\n- The results show broad (coarse) skills work better than fine-grained ones, but the paper doesn’t explain why. Maybe small skill differences are too subjective or noisy. Not exploring this further feels like a missed chance."}, "questions": {"value": "- Real talk often needs more than one skill at once. Can we model uncertainty or mixing like \"empathy + clarification\" instead of forcing one skill per turn?\n- Why does Immediate Response make things worse (Figure 5)? Maybe it shows thinking before speaking is more important than speed in social talk.\n- Can we personalize skill-of-mind? For example, pick different skills for introverts vs extroverts, or adapt over time for each user."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TIXHQy99x1", "forum": "SjAKLyCvmC", "replyto": "SjAKLyCvmC", "signatures": ["ICLR.cc/2026/Conference/Submission20593/Reviewer_W2NB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20593/Reviewer_W2NB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927960470, "cdate": 1761927960470, "tmdate": 1762934002142, "mdate": 1762934002142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of Skill-of-Mind (SoM), to enhance the human-likeness and naturalness of LLM responses in complex social dialogues. The central hypothesis is that providing LLMs with appropriate social skill guidance can effectively improve their response quality.\nThe primary contributions of this work are:\n1. The MULTIFACETED SKILL-OF-MIND dataset, a corpus of nearly 100k samples annotating the conversational skills (from a summarized set of 38) and the explaination.\n2. THANOS, a series of models trained on this dataset, designed to provide social skill decisions.\n3. The THANOSBENCH benchmark, developed to evaluate both SoM capabilities and response generation quality.\nThe authors' experiments demonstrate that: (1) The THANOS models outperform baseline models of equivalent and even larger scales on SoM tasks. (2) Using the SoM (including explanations and skills) generated by THANOS to guide other LLMs significantly enhances the naturalness and engagingness of their responses ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Large-scale, comprehensive dataset**: The paper constructs a large-scale SoM-annotated dataset with 100k samples and 38 skill categories. This is one of the largest and most skill-comprehensive datasets in this domain, making it a valuable resource for research on social dialogue reasoning.\n2. **Holistic benchmark**: THANOSBENCH provides a comprehensive evaluation framework that assesses not only the final response quality but also the intermediate \"Skill-of-Mind\" reasoning process, including explanation generation (ExG) and skill classification (SC) . This is a crucial step toward evaluating interpretable and controllable dialogue systems.\n3. **Relatively strong empirical results**: The experiments show that the THANOS models, specifically infused with SoM capabilities, perform robustly. Moreover, THANOS-3B, when used as a guidance mechanism, significantly improves the social response quality of various SOTA LLMs, including GPT-4o , demonstrating the method's practicality and effectiveness."}, "weaknesses": {"value": "1. **Heavy reliance on LLM-as-a-judge with inadequate justification**: The evaluation in THANOSBENCH relies heavily on an LLM-as-a-judge. In Appendix I, the authors attempt to validate the consistency between the LLM Evaluator and humans in other multi-turn social dialogue settings, but the correlation is not high (e.g., a Spearman correlation of 0.489 ). It is widely recognized (e.g., in work on the SOTOPIA benchmark) that LLMs as judges can exhibit various biases, especially when directly comparing two responses in a complex, subjective setting (like the RESPONSE GENERATION TASK). The small-scale human validation in the main paper , conducted only on two smaller models (Gemma-2-2B and LLaMA-3.1-8B), is insufficient to substantiate the broad claims made about performance improvements on SOTA models like GPT-4o.\n2. **Questionable inter-rater agreement**: When assessing the dataset's annotation quality, the authors report an inter-rater agreement using Krippendorff’s α of 0.62, claiming this \"indicates a substantial level of agreement\". However, this value is often considered moderate at best. According to established guidelines (e.g., doi:10.1016/j.mex.2023.102545), an alpha < 0.67 may suggest poor agreement among raters. This raises concerns about the reliability of human annotation. \n3. **Lack of ablation studies**: A key argument of the paper is that accurate SoM reasoning and classification prompts improve response quality. However, the authors do not sufficiently verify the inverse: whether providing an incorrect or suboptimal skill and its explaination would relatively degrade response quality. For instance, an ablation could involve using the SoM outputs from weaker models (from Table 3) to guide the SOTA models (in Table 5). The experiment in Appendix H.6, while related, is insufficient as it only injects the skill label without the corresponding explaination , making the setup inconsistent with the main experiments. This is a critical omission, as the presence of a longer, chain-of-thought-style explanation—regardless of its accuracy—may in itself contribute to improved response quality."}, "questions": {"value": "1. Can the authors provide more evidence for the reliability of the LLM-judge, particularly for the RESPONSE GENERATION TASK, where subjective comparison is required?\n2. In the human evaluation presented in Table 6, why were only these two smaller models (Gemma-2-2B and LLaMA-3.1-8B) selected for assessment? Why not include larger, SOTA models to provide a more comprehensive and compelling validation of the THANOS mechanism's effectiveness?\n3. Have the authors considered ablation studies to verify the effect of different strategies for providing the SoM (skill + explanation) on response quality? Specifically, why not use the better-performing models from Table 3 (e.g., GPT-4o or Qwen-2.5-72B) to provide the SoM guidance in the response generation experiments, rather than THANOS-3B?\n4. Have the authors considered testing the latest LLMs or LRMs (Large Reasoning Models) on these tasks? It would be valuable to see if explicit SoM prompting still holds an advantage over, for instance, a general long reasoning strategy.\n5. A minor presentation issue: In the prompt templates in Appendix N, there are several instances of text like \"¡think¿\" and \"¡Yes or No¿\" . This might be a rendering error."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gsHKNoHdVc", "forum": "SjAKLyCvmC", "replyto": "SjAKLyCvmC", "signatures": ["ICLR.cc/2026/Conference/Submission20593/Reviewer_wNvi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20593/Reviewer_wNvi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001247835, "cdate": 1762001247835, "tmdate": 1762934001647, "mdate": 1762934001647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}