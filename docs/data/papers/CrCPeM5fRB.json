{"id": "CrCPeM5fRB", "number": 6764, "cdate": 1757994820845, "mdate": 1763616176631, "content": {"title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "abstract": "We propose the Signal Dice Similarity Coefficient (SDSC), a novel structure-aware metric for time series self-supervised representation learning. Most existing methods rely on distance-based objectives such as mean squared error, which are sensitive to amplitude, often under-penalize waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. The SDSC quantifies structural agreement between temporal signals based on signed amplitude intersections, extending the Dice Similarity Coefficient from segmentation to continuous signals. Although originally defined as a metric, SDSC can also be used as a loss by subtracting from 1 and applying a differentiable Heaviside approximation. A hybrid loss that combines SDSC with MSE further improves stability and preserves amplitude when necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance relative to MSE, particularly in in-domain and low-resource scenarios. These results suggest that enforcing structural fidelity enhances semantic representation quality and motivates the reconsideration of structure-aware objectives as alternatives to conventional distance-based losses.", "tldr": "We identify limitations of MSE for time-series SSL and introduce SDSC, a structure-sensitive metric that captures signal semantics.", "keywords": ["self-supervised learning", "representation learning", "time series", "structure-aware metric", "Signal Dice Similarity Coefficient (SDSC)"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/837f1f03d52fb035013cadfcc7bc98e3654b197e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric for time-series self-supervised learning. SDSC extends the Dice–Sørensen coefficient, widely used in image segmentation, to signed continuous signals by (1) gating overlaps with a (smoothed) Heaviside on sign agreement between prediction and target, and (2) accumulating pointwise intersections via a $\\min(\\|E\\|,\\|R\\|)$ operation normalized to $[0,1]$. The paper further proposes a hybrid loss that combines SDSC with MSE, weighted by uncertainty-based coefficients, and evaluates both as reconstruction objectives within a SimMTM pretraining framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Conceptual originality:**: Extending Dice coefficient to continuous, signed time-series is intuitive yet non-trivial. The proposed formulation yields a bounded, symmetric, and interpretable metric within the range $[0,1]$.\n- **Design rationale**: Using $\\min(\\|E\\|,\\|R\\|)$ with a Heaviside gating directly targets polarity issues and reduces pure amplitude bias, which are key limitations of conventional MSE/MAE.\n- **Efficiency and simplicity**: SDSC avoids explicit temporal alignment or complex dynamic programming, making it lightweight and easy to implement compared to SoftDTW or DILATE.\n- **Hybrid loss formulation**: The uncertainty-weighted combination of SDSC and MSE provides a balanced approach that couples structure-awareness with amplitude precision."}, "weaknesses": {"value": "1. **Narrow definition of \"structure\"**: SDSC captures only pointwise magnitude overlap under sign gating, overlooking broader structural properties such as local waveform shape and phase alignment. Consequently, it lacks time-shift/warping tolerance—small temporal lags can significantly reduce the score, contradicting the intended “structure-aware” characterization.\n2. **Offset bias**: When both signals share the same polarity or a strong DC offset, $H(E\\cdot R)\\approx1$ holds broadly, leading to inflated similarity even when shapes differ.\n3. **Zero-crossing noise and stability**: Around near-zero amplitudes, the Heaviside gating becomes highly noise-sensitive, causing unstable or vanishing gradients. Although the authors acknowledge gradient vanishing and describe it as robustness, this more likely indicates blindness to misalignment rather than genuine robustness.\n4. **Evaluation mismatch**: If MSE is said to fail at capturing structure, the evaluation should include structure-aware metrics such as Pearson correlation, spectral coherence, or STFT/Mel-cosine similarity. Relying solely on MSE/MAE weakens the empirical claim of structural fidelity.\n5. **Limited baselines**: The work omits direct comparisons with established structure-aware objectives. Simply stating these are \"computationally heavy\" is insufficient. An explicit time-vs-accuracy trade-off or short-sequence comparison would strengthen the argument.\n6. **Gradient analysis limitations**: Reporting only gradient norms does not characterize optimization behavior. The analysis should also consider gradient direction alignment, variance, or loss-landscape smoothness to assess stability."}, "questions": {"value": "1. Could the authors include experiments with timing shifts or mild time warping, and compare SDSC against MSE, SoftDTW, and DILATE?\n2. Beyond pointwise overlap, could you report additional structure-sensitive metrics to substantiate the \"structure-aware\" claim both at pretraining and downstream evaluation stages?\n3. Have you explored mean-removal preprocessing or frequency-domain SDSC to mitigate offset bias?\n4. Could you provide a practical guideline summarizing when SDSC, MSE, or the hybrid loss is preferred (i.e., amplitude-critical vs. structure-critical regimes)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TsYTh5o80R", "forum": "CrCPeM5fRB", "replyto": "CrCPeM5fRB", "signatures": ["ICLR.cc/2026/Conference/Submission6764/Reviewer_tXhx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6764/Reviewer_tXhx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761040957214, "cdate": 1761040957214, "tmdate": 1762919042935, "mdate": 1762919042935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment – Revision Version 1"}, "comment": {"value": "Dear Reviewers,\n\nI sincerely apologize for the delay in submitting my response. I greatly appreciate all of your valuable and thoughtful feedback. Many of your comments pointed to aspects of the experiments, so I initially intended to complete those experiments before posting this revision comment. However, as some of them are taking longer than expected, I have first addressed the sections that could be revised without additional results.\n\nI will respond carefully to each question and weakness you raised, and I will include the experimental updates as soon as they are finalized. If any further revisions or clarifications arise during this process, I will actively update my comments.\n\nThank you very much for your time and constructive insights.\n\nSincerely,"}}, "id": "iZtglmDp8f", "forum": "CrCPeM5fRB", "replyto": "CrCPeM5fRB", "signatures": ["ICLR.cc/2026/Conference/Submission6764/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6764/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6764/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763611545889, "cdate": 1763611545889, "tmdate": 1763611545889, "mdate": 1763611545889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric for time series self-supervised representation learning that addresses limitations of distance-based objectives like MSE.  SDSC extends the Dice Similarity Coefficient from image segmentation to continuous temporal signals by quantifying structural agreement through signed amplitude intersections."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is relatively simple and easy to understand.\n\n2. Experiments span multiple tasks (forecasting, in-domain classification, cross-domain classification), settings (frozen encoders, fine-tuning), and datasets, demonstrating broad applicability and providing nuanced insights into when each approach works best.\n\n3. The paper acknowledges that SDSC models achieve higher structural alignment at the cost of increased MSE, and that dataset characteristics influence which approach works better, showing intellectual honesty."}, "weaknesses": {"value": "1.  The paper uses only SimMTM as the backbone \"for architectural simplicity,\" which severely limits generalizability claims. Without validation on diverse architectures (Transformers, CNNs, RNNs, recent foundation models), it's unclear if SDSC benefits are architecture-specific or truly general.\n\n2. The paper only compares against MSE, PCC, and SI-SNR. Recent structure-aware losses for time series (e.g., shape-based losses, spectral losses, contrastive losses) are not included, making it difficult to assess whether SDSC represents state-of-the-art for structure-aware objectives.\n\n3. Despite strong motivation, the actual improvements are modest: hybrid loss achieves 0.4783 vs. 0.4852 MSE in forecasting (1.4% improvement), and Table 6 shows MSE sometimes outperforms SDSC in fine-tuning scenarios. The gains don't match the strength of the conceptual contribution.\n\n4.  Computing SDSC requires element-wise min operations, Heaviside evaluations, and additional summations compared to MSE. No analysis of training time overhead or memory consumption is provided, which is critical for practical adoption."}, "questions": {"value": "1. There seems to be an incorrect line break at the title.\n\n2. Why does SDSC underperform MSE in cross-domain settings (Tables 5, 6)? If SDSC provides better \"semantic representations,\" shouldn't it generalize better across domains? What explains this contradiction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1rPABoqCQM", "forum": "CrCPeM5fRB", "replyto": "CrCPeM5fRB", "signatures": ["ICLR.cc/2026/Conference/Submission6764/Reviewer_QVmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6764/Reviewer_QVmd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746790558, "cdate": 1761746790558, "tmdate": 1762919042623, "mdate": 1762919042623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Signal Dice Similarity Coefficient (SDSC), a novel, structure-aware metric for self-supervised learning (SSL) of time-series representations. Existing methods such as MSE is overly sensitive to signal amplitude and scale, while being insensitive to waveform structure, phase, and polarity. DTW and other metrics are also having their weakness. To overcome teh limitations, SDSC is adapted from the Dice Similarity Coefficient (DSC), a metric widely used for overlap in image segmentation. SDSC compares two signals at each time step. It calculates a score based on the minimum amplitude of the two signals, but only if both signals have the same sign (e.g., both are positive or both are negative). If the signs are different, that time step is heavily penalized (it contributes zero to the similarity score).Properties: This approach makes SDSC robust to amplitude scaling while being highly sensitive to polarity mismatches. The resulting metric is bounded between [0, 1] (making it interpretable) and is computationally efficient (linear $O(T)$ complexity), unlike other alignment-based metrics (like SoftDTW, which is $O(T^2)$). The author also uses a smooth sigmoid approximation to make the function differentiable. In experiments, they also propose to combines MSE and SDSC, to capture both structural and amplitude information.Experiments on forecasting and classification tasks show that pre-training with SDSC or the hybrid loss is competitive or superior to models pre-trained with MSE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is efficient, linear time complexity, much better than other methods like DTW, which performs similar structure-awareness measurement.\n2. The metric is bounded from 0 to 1, which provides better interpretability.\n3. The paper is clean written, with illustrative examples to show numbers using different metric, under different structure changes."}, "weaknesses": {"value": "1. No clear definition of \"structure\", still related to alignment or warping.\n2. The backbone model is not widely tested. With more powerful models, we don't know if the advantage of SDSC still exists.\n3. The imrpovement on various tasks, are very marginal. For example, in the fine-tuned classification task, the SDSC approach is not showing better results in either in-domain or out-domain experiments."}, "questions": {"value": "If switching to other transformer-based backbone model, would the proposed method performs consistenly better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HqVC0xKH4J", "forum": "CrCPeM5fRB", "replyto": "CrCPeM5fRB", "signatures": ["ICLR.cc/2026/Conference/Submission6764/Reviewer_cQD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6764/Reviewer_cQD8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856480756, "cdate": 1761856480756, "tmdate": 1762919042339, "mdate": 1762919042339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of conventional distance-based metrics (e.g., MSE) in time-series SSL by introducing a novel structure-aware metric, the Signal Dice Similarity Coefficient (SDSC). The method reframes the signal reconstruction problem as measuring the overlap of the areas under the respective curves. Through the introduction of a signed amplitude intersection term, it ensures that overlap is computed only when signal polarities align, thereby effectively addressing the noted deficiency of MSE in being insensitive to phase inversions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tAdapting the DSC from the segmentation domain to time-series signals is a novel perspective. Using the signed amplitude intersection as a proxy for waveform structure similarity is an interesting idea.\n2.\tThe O(T) linear complexity of SDSC is computationally efficient, which is a practical advantage.\n3.\tThe mathematical definition is intuitive, and the experimental design is well-structured."}, "weaknesses": {"value": "1.\tA motivation for the paper is that SDSC serves as a lightweight alternative to O(T^2) metrics (e.g., SoftDTW, DILATE). However, a direct comparison against them is missing. Currently, we only know that SDSC is faster, but we do not know how much performance is lost (or gained) compared to SoftDTW.\n2.\tThe α parameter in the Sigmoid function significantly influences the gradient shape. The paper lacks a sensitivity analysis on how α affects the performance of downstream tasks.\n3.\tThe \"alignment-free\" description may be somewhat misleading. While SDSC does not perform explicit temporal warping like DTW, it does enforce strict temporal and polarity alignment through its point-wise comparison."}, "questions": {"value": "I will reconsider my score during the rebuttal phase based on the authors' response to following issues.\n\n1.\tCould the authors include an experiment in the appendix that compares SDSC with SoftDTW (as a loss function) on a downstream task (e.g., forecasting), using at least one small-scale dataset?\n2.\tRegarding the hybrid loss in Equation (8), how is the uncertainty-based tuning strategy specifically implemented to determine the weights λsdsc and λmse? How does this adaptive strategy compare to using simple fixed weights (e.g., λsdsc = λmse = 0.5)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hmDXFiQaFX", "forum": "CrCPeM5fRB", "replyto": "CrCPeM5fRB", "signatures": ["ICLR.cc/2026/Conference/Submission6764/Reviewer_5F1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6764/Reviewer_5F1V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6764/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926823379, "cdate": 1761926823379, "tmdate": 1762919041916, "mdate": 1762919041916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}