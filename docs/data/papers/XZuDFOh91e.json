{"id": "XZuDFOh91e", "number": 18862, "cdate": 1758291582070, "mdate": 1759897076971, "content": {"title": "Reward-Guided Flow Merging via Implicit Density Operators", "abstract": "Unprecedented progress in large-scale flow and diffusion modeling for scientific discovery recently raised two fundamental challenges: $(i)$ reward-guided adaptation of pre-trained flows, and $(ii)$ integration of multiple models, i.e., model merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging. This captures generative optimization tasks requiring information from multiple pre-trained flows, as well as task-aware flow merging (e.g., for maximization of drug-discovery utilities). Our formulation renders possible to express a rich family of implicit operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models) and interpolation (e.g., for discovery in data-scarce regions). Moreover, it allows to compute complex logic expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a theory-backed mirror-descent scheme that reduces reward-guided flow merging to a sequential fine-tuning problem that can be tackled via scalable, established methods. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and on a high-dimensional drug design task generating low-energy molecular conformers.", "tldr": "", "keywords": ["flow models", "diffusion models", "model merging", "reward-guided fine-tuning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce6a605ca970236b4b228babc1137f1c0f3e2e60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "My expertise is in AI for Science, not core AI theory. Therefore, this review primarily assesses the paper's practical application, experimental validation, and its claimed contribution to scientific tasks. I have focused less on the fundamental methodological novelty in the pure AI domain, deferring that judgment to other reviewers.\n\nThis paper presents a novel, theoretically-grounded framework for unifying reward-guided optimization with flow model merging. The concept is well-motivated and clearly demonstrated on multiple well-structured toy examples.\n\nHowever, the empirical validation on the high-dimensional task is limited. There is only one application task. Moreover, the core reward-based optimization (RFM-RB) fails, performing significantly worse than a simpler baseline in both key metrics (energy and validity).\nThese results restrict the method's practical utility.\n\nIn summary, while the paper introduces a promising and theoretically sound framework, its practical effectiveness is not yet thoroughly validated due to limited experimental results on high-dimensional tasks.\n\n---\n**The usage of LLM**: I wrote the entire review myself and only used the LLM to correct the grammar and improve readability."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Unified Framework:** A primary strength is the novel formulation that unifies reward-guided optimization and flow merging into a single objective function. This framework is supported by a solid theoretical background and demonstrated with diverse operators and toy examples (e.g., intersection, union, interpolation).\n2. **Clear Motivation and Methodology:** The paper presents a clear motivation and a well-articulated methodology. This is effectively explained in Sections 3 and 4 and well-illustrated through several intuitive toy examples.\n3. **Demonstrated Feasibility on High-Dimensional Data:** The authors successfully demonstrate the feasibility of their flow merging approach on a high-dimensional task (molecular generation) that involves both discrete (molecular graph) and continuous (3D coordinates) data."}, "weaknesses": {"value": "The paper effectively demonstrates its motivation and capabilities through several well-designed toy examples. However, its validation on high-dimensional tasks appears insufficient.\n\n**1. Significant performance degradation on optimization setting**\n\nA core claim of the paper is to _simultaneously_ optimize a reward function while merging multiple flows.\n> **Page 1, Line 46**: *Can we fine-tune a pretrained flow model to optimize a given reward function while integrating information from (i.e., merge) multiple pre-trained flows.*\n\nHowever, the empirical evidence for the reward-guided optimization component is weak and concerning.\n- The primary optimization experiment (RFM-RB) is not presented in the main manuscript and is only available in Appendix F.2.\n- The results in Table 1 show that RFM-RB fails to outperform the PRE-2 baseline (which was trained with standard Adjoint Matching). In fact, RFM-RB performs significantly worse: it achieves a worse mean total energy (−12.47 Ha) compared to the baseline while suffering a significant drop in molecular validity (from 76% to 33%).\n\nThe authors attribute this performance degradation to the multi-objective nature of molecular design.\nHowever, this explanation is insufficient, as the adjoint matching (PRE-2) demonstrates a better trade-off, improving energy significantly (−14.76 Ha) with a minor validity drop (from 76% to 68%).\n\nThis poor performance undermines the paper's central claim.\nThe authors must provide additional evidence to validate that RFM can practically solve reward-based optimization problems beyond its theoretical guarantees.\n\n**2. Limited Application Study**\n\nThe paper is limited to a single application: a molecular discovery task.\nThis contrasts with other RL-based finetuning methods, which are often validated on well-established tasks (e.g., image generation) with diverse baselines[1] or across multiple domains[2,3].\n\n\n**3. Ambiguity of experiment name**\n\nThere is a significant ambiguity in the naming of the high-dimensional experiment that is likely to mislead domain experts.\nThe authors repeatedly describe the task as \"conformer generation\" including in the abstract.\n\nHowever, this task is **3D molecular generation** (i.e., jointly generating the molecular graph and 3D coordinates).\nThis is fundamentally different from **molecular conformer generation**, which involves generating 3D coordinates for _given_ molecule(s).\n\nThis misnaming caused significant confusion. I strongly recommend the authors correct this terminology throughout the manuscript to accurately reflect the experimental task.\n- **3D molecular generation:** Joint design of molecular graph (categorical) and its 3D conformer (continuous). (e.g., FlowMol[4], SemlaFlow[5], CGFlow[6])\n- **Molecular conformer generation:** 3D conformer generation of a _given_ molecule. (e.g., Torsional Diffusion[7], ETFlow[8])\n\n---\n**Reference**\n1. Domingo-Enrich, Carles, et al. \"Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control.\" _arXiv preprint arXiv:2409.08861_(2024).\n2. Venkatraman, Siddarth, et al. \"Amortizing intractable inference in diffusion models for vision, language, and control.\" _Advances in neural information processing systems_ 37 (2024): 76080-76114.\n3. Venkatraman, Siddarth, et al. \"Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models.\" _arXiv preprint arXiv:2502.06999_ (2025).\n4. Dunn, Ian, and David Ryan Koes. \"Mixed continuous and categorical flow matching for 3d de novo molecule generation.\" _ArXiv_ (2024): arXiv-2404.\n5. Irwin, Ross, et al. \"SemlaFlow--Efficient 3D Molecular Generation with Latent Attention and Equivariant Flow Matching.\" _arXiv preprint arXiv:2406.07266_ (2024).\n6. Shen, Tony, et al. \"Compositional Flows for 3D Molecule and Synthesis Pathway Co-design.\" _arXiv preprint arXiv:2504.08051_ (2025).\n7. Jing, Bowen, et al. \"Torsional diffusion for molecular conformer generation.\" _Advances in neural information processing systems_ 35 (2022): 24240-24253.\n8. Hassan, Majdi, et al. \"Et-flow: Equivariant flow-matching for molecular conformer generation.\" _Advances in Neural Information Processing Systems_ 37 (2024): 128798-128824."}, "questions": {"value": "- What do the authors perceive as the primary limitations of the proposed RFM approach?\n    \n- Questions related to the molecular design task:\n    - **Figure 3 and Table 1**: What do `RFM-B` and `RFM-UB` stand for? I assume those mean \"balanced\" and \"unbalanced,\" which are mentioned in the text, but this is not explicitly defined in the figure/table captions.\n    - **Page 23 Line 1231**: What reward function was used to obtain the PRE-2 model via AM? Was the exact same reward function used for the RFM-RB experiment?\n    - What was the initial model ($\\pi_\\text{init}$) used for the RFM-B, RFM-UB, and RFM-RB experiments? Was it PRE-1 or PRE-2?\n        \n- Minor Typos/Formatting:\n\t- **Page 2, Line 82**: Could you change the citation format from \"$p_{data}\\text{Lipman et al. ...}$\" to \"$p_{data}~(\\text{Lipman et al. ...})$\"?\n\t- **Page 5, Line 236**: The hyperlink for the equation 8 appears to be missing.\n    - **Page 22, Line 1163 (in Algorithm 3, step 7)**: The objective function is referred to as '??'. MOreover, the equation number is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nagZ98ds7Z", "forum": "XZuDFOh91e", "replyto": "XZuDFOh91e", "signatures": ["ICLR.cc/2026/Conference/Submission18862/Reviewer_vWQC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18862/Reviewer_vWQC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948665442, "cdate": 1760948665442, "tmdate": 1762930829859, "mdate": 1762930829859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reward-Guided Flow Merging (RFM), a unified framework that jointly addresses reward-guided adaptation of pre-trained flows and integration of multiple models. The method formulates merging as optimization over diverse implicit density operators, such as intersection, union, and interpolation. RFM employs a mirror-descent scheme that converts complex merging tasks into sequential fine-tuning problems. The paper presents theoretical proofs as well as experiments on the drug design task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a unified theoretical framework that generalizes two difficult problems in AI for science, reward-guided fine-tuning and model merging. This framework will benefit the design of many scientific models. The paper also provides rigorous proofs of theoretical guarantees for the proposed Reward-Guided Flow Merging algorithm, providing a reliable foundation for the framework."}, "weaknesses": {"value": "The paper only evaluates its framework on the molecular design task. This limits the demonstration of the claimed wide real-world applications. More experiments in future works would strengthen the generality and practical impact of the algorithm."}, "questions": {"value": "The paper provides an effective framework for merging pre-trained models. Could the authors discuss more about how model performance and efficiency scale as the number of merged models increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P46w4RYWok", "forum": "XZuDFOh91e", "replyto": "XZuDFOh91e", "signatures": ["ICLR.cc/2026/Conference/Submission18862/Reviewer_sG1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18862/Reviewer_sG1a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982220770, "cdate": 1761982220770, "tmdate": 1762930828947, "mdate": 1762930828947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a generative optimisation framework that advocates task-aware reward-guided adaptation of multiple pretrained flow models. The proposed formulation entails implicit density operators (union, intersection, interpolation, and their combinations) over generative model densities. The authors also implement a mirror-descent scheme to approximate the objective in terms of a sequence of reward-guided fine-tuning problems. \n\nnprecedented progress in large-scale flow and diffusion modeling for scientific discovery recently raised two fundamental challenges:  reward-guided adaptation of pre-trained flows, and integration of multiple models, i.e., model merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging. This captures generative optimization tasks requiring information from multiple pre-trained flows, as well as task-aware flow merging (e.g., for maximization of drug-discovery utilities). Our formulation renders possible to express a rich family of implicit operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models) and interpolation (e.g., for discovery in data-scarce regions). Moreover, it allows to compute complex logic expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a theory-backed mirror-descent scheme that reduces reward-guided flow merging to a sequential fine-tuning problem that can be tackled via scalable, established methods. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and on a high-dimensional drug design task generating low-energy molecular conformers."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "--- The manuscript is generally easy-to-read. \n\n--- Model composition and fine-tuning are clearly, and justifiably, extremely active areas of research, so the work deals with a topical subject."}, "weaknesses": {"value": "I apologise in advance for what might come across as a rather disappointing/critical review for the authors, but I put in extensive effort on reviewing this paper to the best of my ability and knowledge of the field. \n\n— What the authors call 'merging' is typically referred to as composition in the literature. An entire body of work has been dedicated to composition  (including under constraints and/or using density operators including based on logical And-Or-Not Operators, Ito operators, Feynman-Kac etc.) that has been completely sidestepped in the current work. See, e.g., references [1, 2, 4, 5, 6, 7, 8, 9]. Not only should the work have been contextualised in terms of similarities and differences with this prior work, but comprehensive empirical benefits over them should have been shown. \n\nThere are other works such as [3] which being contemporaneous are not subject to this criticism.  \n\n-- For a technical/optimisation perspective, \"reward guidance\" is not central to the work and comes across as rather peripheral/contrived since the solution to the formulation only cares about the convexity/concavity of the problem and availability of the function gradient (including, or without, the reward). Similarly, the generality of \"merging\" in the current context is overstated. As the formulation is restricted to an affine/convex combination of divergences, it cannot implement important operations such as negation and contrast. As soon as the overall concavity/convexity is violated, no global convergence guarantee holds as the classical stochastic approximation theory (Robbins-Monro style updates) can only guarantee a local solution. \n\n--- The entire framework is essentially a straightforward amalgamation of existing ideas, and it's unclear how this work advances the field at all. In particular, heavily derives from/relies on key notions and machinery already tackled in the literature on RL under general utilities and stochastic optimal control [10, 11, 12, 13].  \n\n--- Experiments are also underwhelming, with essentially no meaningful comparisons included against state-of-the-art baselines on composition and fine-tuning. \n\n[1] Khalafi et al. Constrained Diffusion Models via Dual Training. NeurIPS 2024.\n\n[2] Giannone et al. Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation. NeurIPS 2023.\n\n[3] Khalafi et al. Composition and Alignment of Diffusion Models using Constrained Learning. arXiv 2025.\n\n[4] Garipov et al. Compositional Sculpting of Iterative Generative Processes. NeurIPS 2023. \n\n[5] Thornton et al.  Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo. AISTATS 2025. \n\n[6] Skreta et al. The superposition of Diffusion Models using the Ito Density Estimator. ICLR 2025. \n\n[7] Skreta et al. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts. ICML 2025. \n\n[8] Karczewski et al. Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models. ICML 2025. \n\n[9] Shih et al. Long Horizon Temperature Scaling. ICML 2023. \n\n[10] Zhang et al. Variational Policy Gradient Method for Reinforcement Learning with General Utilities. NeurIPS 2020. \n\n[11] Domingo-Enrich et al.  Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control. arXiv 2024.\n\n[12] Han et al. Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence. ICML 2025.\n\n[13] Uehara et al. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv 2024."}, "questions": {"value": "Could you please address my concerns detailed in the weaknesses section? In addition, wondering \n\n(1) what the computational cost of the entire procedure is (noting that a sequence of fine-tuning steps needs to be solved)? \n\n(2) what the effect of inexact updates is in practice in terms of discrepancy from the solution arrived through exact updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uMxoyhCJix", "forum": "XZuDFOh91e", "replyto": "XZuDFOh91e", "signatures": ["ICLR.cc/2026/Conference/Submission18862/Reviewer_6Mtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18862/Reviewer_6Mtg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992642199, "cdate": 1761992642199, "tmdate": 1762930828313, "mdate": 1762930828313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a generative optimisation framework that advocates task-aware reward-guided adaptation of multiple pretrained flow models. The proposed formulation entails implicit density operators (union, intersection, interpolation, and their combinations) over generative model densities. The authors also implement a mirror-descent scheme to approximate the objective in terms of a sequence of reward-guided fine-tuning problems."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "--- The manuscript is generally easy-to-read. \n\n--- Model composition and fine-tuning are clearly, and justifiably, extremely active areas of research, so the work deals with a topical subject."}, "weaknesses": {"value": "I apologise in advance for what might come across as a rather disappointing/critical review for the authors, but I put in extensive effort on reviewing this paper to the best of my ability and knowledge of the field. \n\n— What the authors call 'merging' is typically referred to as composition in the literature. An entire body of work has been dedicated to composition  (including under constraints and/or using density operators including based on logical And-Or-Not Operators, Ito operators, Feynman-Kac etc.) that has been completely sidestepped in the current work. See, e.g., references [1, 2, 4, 5, 6, 7, 8, 9]. Not only should the work have been contextualised in terms of similarities and differences with this prior work, but comprehensive empirical benefits over them should have been shown. \n\nThere are other works such as [3] which being contemporaneous are not subject to this criticism.  \n\n-- For a technical/optimisation perspective, \"reward guidance\" is not central to the work and comes across as rather peripheral/contrived since the solution to the formulation only cares about the convexity/concavity of the problem and availability of the function gradient (including, or without, the reward). Similarly, the generality of \"merging\" in the current context is overstated. As the formulation is restricted to an affine/convex combination of divergences, it cannot implement important operations such as negation and contrast. As soon as the overall concavity/convexity is violated, no global convergence guarantee holds as the classical stochastic approximation theory (Robbins-Monro style updates) can only guarantee a local solution. \n\n--- The entire framework is essentially a straightforward amalgamation of existing ideas, and it's unclear how this work advances the field at all. In particular, heavily derives from/relies on key notions and machinery already tackled in the literature on RL under general utilities and stochastic optimal control [10, 11, 12, 13].  \n\n--- Experiments are also underwhelming, with essentially no meaningful comparisons included against state-of-the-art baselines on composition and fine-tuning. \n\n[1] Khalafi et al. Constrained Diffusion Models via Dual Training. NeurIPS 2024.\n\n[2] Giannone et al. Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation. NeurIPS 2023.\n\n[3] Khalafi et al. Composition and Alignment of Diffusion Models using Constrained Learning. arXiv 2025.\n\n[4] Garipov et al. Compositional Sculpting of Iterative Generative Processes. NeurIPS 2023. \n\n[5] Thornton et al.  Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo. AISTATS 2025. \n\n[6] Skreta et al. The superposition of Diffusion Models using the Ito Density Estimator. ICLR 2025. \n\n[7] Skreta et al. Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts. ICML 2025. \n\n[8] Karczewski et al. Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models. ICML 2025. \n\n[9] Shih et al. Long Horizon Temperature Scaling. ICML 2023. \n\n[10] Zhang et al. Variational Policy Gradient Method for Reinforcement Learning with General Utilities. NeurIPS 2020. \n\n[11] Domingo-Enrich et al.  Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control. arXiv 2024.\n\n[12] Han et al. Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence. ICML 2025.\n\n[13] Uehara et al. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv 2024."}, "questions": {"value": "Could you please address my concerns detailed in the weaknesses section? In addition, wondering \n\n(1) what the computational cost of the entire procedure is (noting that a sequence of fine-tuning steps needs to be solved)? \n\n(2) what the effect of inexact updates is in practice in terms of discrepancy from the solution arrived through exact updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uMxoyhCJix", "forum": "XZuDFOh91e", "replyto": "XZuDFOh91e", "signatures": ["ICLR.cc/2026/Conference/Submission18862/Reviewer_6Mtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18862/Reviewer_6Mtg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992642199, "cdate": 1761992642199, "tmdate": 1763329486593, "mdate": 1763329486593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}