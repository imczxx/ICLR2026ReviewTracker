{"id": "WIXohR7mEo", "number": 20990, "cdate": 1758312439456, "mdate": 1763154401148, "content": {"title": "ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning", "abstract": "We introduce ACPBench Hard, a dataset of generative, open-ended questions which LLM models needs to answer in order to plan. Models that perform well on these tasks could in principle be integrated into a planner or be used directly as a policy. We discuss the complexity of these tasks as well as the complexity of validating the correctness of their answers and present validation algorithms for each task. Equipped with these validators, we test the performance of a variety of models on our tasks and find that for most of these tasks, the performance of even the largest models is still subpar.  The models do not possess even the most basic capability of identifying which actions can be performed in a given state.   No model outperforms any other on our proposed tasks and, with a few exceptions, all tested language models score below 65\\%, indicating that even the current frontier language models as well as so-called reasoning models have a long way to go before they can reliably reason about planning.", "tldr": "", "keywords": ["Planning", "Dataset and Benchmark", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b6a69da53e2e365b656c4817b18c72ba19b984ff.pdf", "supplementary_material": "/attachment/1e7a9dbbba73aac58ca9bf0bf0a0f7aebfeecb9d.zip"}, "replies": [{"content": {"summary": {"value": "The paper describes and novel dataset on planning tasks which are supposed to be harder. The paper describes the creation and details the included task. It contains 13 planning task with each 8 examples if I read the paper correctly. They describe that none of the datasets can be solved by the best models and reach about up to 60% . The task are in natural language translated from pddl."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. New and harder planning dataset. \n2. Detailed evaluation on a larger set of models for small to large LLMs.\n3. Detailed analysis of results."}, "weaknesses": {"value": "The datasets is seems too small per task to obtain good confidence intervals. I would prefer per task much more examples, via the automatic process they describe, you could produce per planning problems thoustands of samples automatically. I would strongly suggetest to provide more. These seem expensive to run but per tokens prices fall and for small models they run the costs a very small. You could define subsets. \n\nThe task should be provide as both NL and PDDL. The models understand PDDL well. NL is not anymore needed. The translation is per template and needs backtranslation for testing.\n\nValmeekam et al. used thinking models and solved many of these task. Blocksworld with 20+ blocks can be now solved by these models. Even Valmeekam used not the latest models of the thinking models now a year ago at Neurisp and the newer once might solve all the provided problems, I guess. The authors could have tried these with GPT-5 or Gemini 2.5 thinking models. These experiments seem missing."}, "questions": {"value": "1. Could you provide per planning problem 1000 samples? Or is this too much effort in case can you explain why? I think this would be ideal.\n2. It would be nice to try with the latest thinking models using NL and PDDL and evaluate both. Would strengthen the claim that these models can not solve those for some time. \n3. Could you provide the statistics about the models in a table, e.g. how much planning steps in average, how distributed, how many samples, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Kvr5dyvREG", "forum": "WIXohR7mEo", "replyto": "WIXohR7mEo", "signatures": ["ICLR.cc/2026/Conference/Submission20990/Reviewer_Zfe9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20990/Reviewer_Zfe9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689066361, "cdate": 1761689066361, "tmdate": 1762999991989, "mdate": 1762999991989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ACPBench Hard, a benchmark designed to evaluate LLMs on open-ended questions about planning. It builds upon the prior ACPBench work which only included boolean and multiple-choice questions, with the difference being that this benchmark requires models to generate complete answers that automated planners would produce during the planning process. Specifically, the benchmark includes eight reasoning tasks across thirteen planning domains: applicability, progression, reachability, action reachability, validation, justification, landmarks, and next action. The authors develop symbolic validators for each task and evaluate fifteen models ranging from 8B to 405B parameters, including reasoning models like o1. Their central finding is that even frontier language models perform poorly, with most scoring below 65% on most tasks, indicating substantial gaps in planning reasoning capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes an important advance in evaluating the planning/reasoning abilities of LLMs and LRMs by shifting from discriminative to generative evaluation, which more faithfully represents the actual decision-making processes that (symbolic) planners perform. This transition is non-trivial because generative tasks require models to produce answers from large action spaces rather than selecting from pre-curated options, making the benchmark more realistic for assessing practical planning capabilities.\n\nUsing symbolic validators is also a substantial strength of the benchmark. Unlike other approaches that use other LLMs for evaluation (which introduces noise and potential biases), these validators provide deterministic and theoretically grounded correctness verification. The authors thoughtfully discuss computational complexity for both the tasks themselves and their validation procedures, with several tasks being PSPACE-complete.\n\nThe empirical evaluation is comprehensive, testing fifteen models including small (8B), medium (33-34B), large (70B-405B), and reasoning models (o1, DeepSeek R1), providing good coverage of the current model landscape. The inclusion of recent reasoning models is particularly valuable for understanding whether extended inference computation translates to improved planning reasoning.\n\nThe qualitative analysis also provides insights into specific failure modes, such as models recognizing blocks cannot be stacked on themselves but struggling with more subtle applicability constraints, or o1-preview's systematic off-by-one errors in the validation task. These concrete observations are more informative than aggregate statistics alone.\n\nThe investigation of representation formats, i.e., combinations of NL and PDDL descriptions, is a good additional experiment. Maybe not surprisingly, the PDDL+NL representation substantially improves performance, which may have some implications for how planning problems should be presented to LLMs."}, "weaknesses": {"value": "One of the immediate weaknesses I notice in the paper is on the evaluation side. Namely:\n\n- The main paper lacks details about exact prompts used, model hyperparameters (temperature, top-p,), random seeds, and how many independent runs were conducted. The paper states a maximum of 1,000 tokens was used (4,000 for GPT-OSS models) but does not specify whether models used greedy decoding or sampling with what parameters. Given the stochastic nature of LLMs, these details are essential for reproducibility.\n\n- The paper employs only 2-shot prompting without exploring alternative prompting strategies. Given the complexity of these tasks, this seems limiting. The paper does not investigate whether chain-of-thought prompting, task decomposition, or additional examples might improve performance. The reasoning models presumably use some form of extended reasoning internally, but the regular LLMs were not given explicit instructions to reason step-by-step in the prompts. At least, that's what can be inferred from the main paper. Additionally, the \"lenient grammar-based parser\" that discards non-conforming tokens may reject valid answers expressed in unexpected formats. The paper acknowledges evaluating only the first answer when models might produce multiple, but does not quantify how often parsing failures occur versus genuine incorrect answers.\n\n- The representation experiment reveals that PDDL+NL substantially improves performance (at least for DeepSeek V3). But why does providing PDDL help? Is it because the structured representation reduces ambiguity? Do models pattern-match on PDDL syntax? Does this suggest LLMs are better at parsing structured representations than natural language descriptions? This result might have implications for prompt engineering and practical deployment, meaning that a deeper analysis including ablations and attention visualization will be beneficial (however, one can argue this is beyond the scope of this paper)."}, "questions": {"value": "1. Concerning natural language generation, how exactly were PDDL problems translated to natural language? I understand you're extending the previous ACPBench, but was this process automated using templates, manually created, or some combination?\n\n2. What hyperparameter values, i.e., temperature and top-p, were used in the experiments? How sensitive are results to different hyperparameter values? Were multiple independent runs conducted to assess variance?\n\n3. Did you experiment with alternative prompting strategies such as explicit CoT reasoning, task decomposition, or providing worked examples for multi-step reasoning?\n\n4. Can you quantify how often the grammar-based parser failed to extract answers versus models providing incorrect answers? Did you manually inspect a sample of parsing failures to ensure the parser is not rejecting valid answers? \n\n5. Can you provide more detailed analysis of why providing PDDL helps? Does it reduce ambiguity, provide structure that models can pattern-match, or serve a different function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7xQzWfxCbg", "forum": "WIXohR7mEo", "replyto": "WIXohR7mEo", "signatures": ["ICLR.cc/2026/Conference/Submission20990/Reviewer_wXMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20990/Reviewer_wXMk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869633881, "cdate": 1761869633881, "tmdate": 1762999992215, "mdate": 1762999992215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ACPBench Hard, a benchmark specifically designed for evaluating the Reasoning about Actions and Change (RAC) capabilities of Large Language Models (LLMs). Compared to existing RAC benchmarks (such as TRAC, ActionReasoningBench, and ACPBench), ACPBench Hard features a richer set of tasks and transforms reasoning tasks into generative questions. Furthermore, the authors designed dedicated symbolic validation mechanisms for each tasks to avoid the uncertainties associated with using LLMs for evaluation. Then, the authors conducted a comprehensive evaluation of 15 LLMs and reasoning models of varying scales using this benchmark. The experimental results clearly demonstrate that even SOTA models perform poorly on these fundamental RAC tasks, indicating that current LLMs are not yet capable of being reliably used as components in symbolic planners or as standalone planners."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Compared to other RAC datasets, the ACPBench Hard dataset features more challenging tasks and provides rigorous formal definitions for each task.\n\n2. The use of symbolic solvers for answer evaluation avoids potential information loss that might occur when relying solely on LLMs for assessment.\n\n3. The paper goes beyond evaluating end-to-end planning capabilities and delves into underlying, atomic-level reasoning. The authors accurately analyze and pinpoint specific reasons for LLM failures in these tasks (for example, LLMs cannot even reliably list all executable actions in a given state).\n\n4. The evaluation is comprehensive, covering models of different sizes, both LLMs and LRMs, as well as open-source and proprietary models. The domains used in the benchmark are also extensive."}, "weaknesses": {"value": "1. The evaluation employed a 2-shot prompting strategy. It remains unclear whether this prompting approach is optimal for eliciting planning and reasoning capabilities in models. More sophisticated strategies (such as ToT and GoT, SC, etc) might yield different outcomes. Besides, it is not clear whether using examples from the target domain would help.\n\n2. The generative vs. mcq comparison is limited. Figure 4 only compares the GPT-4o model, making it uncertain to determine whether the increased difficulty is a universal phenomenon or specific to certain models."}, "questions": {"value": "Q1: There is an inconsistency in task names. Line 381 mentions \"atom reachability\" - does this refer to Task 3 \"reachability\"?\n\nQ2: Using symbolic solvers to verify generative answers usually requires a formalization process. I am curious whether using only a grammar parser is sufficient to accomplish this translation, given the diversity of LLM responses. Do parsing failures occur during evaluation? If so, have you considered adopting LLM-based evaluation as a backup strategy when parsing fails?\n\nQ3: During evaluation, did you set specific temperature parameters for the LLMs, or were default values used throughout?\n\nQ4: NextA is an interesting task. How exactly did you compute the heuristic value h*(s) for states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X7E41CxwF9", "forum": "WIXohR7mEo", "replyto": "WIXohR7mEo", "signatures": ["ICLR.cc/2026/Conference/Submission20990/Reviewer_UptW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20990/Reviewer_UptW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896670798, "cdate": 1761896670798, "tmdate": 1762939834105, "mdate": 1762939834105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ACPBench Hard, a benchmark dataset for evaluating LLMs ability to perform generative reasoning tasks related to automated planning. Building upon the existing ACPBench, the authors create open-ended, generative versions of 7 existing tasks plus one new task. The benchmark spans 13 PDDL planning domains and includes symbolic validators for each task. The authors conduct an extensive empirical evaluation of 15 LLMs and reasoning models, revealing that even state-of-the-art models perform poorly on most tasks, with many scores below 65%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a significant problem by requiring generative responses rather than classification, which better reflects real-world planner requirements.\n* The paper tests 15 models across different scales and types, providing empirical coverage\n* The description of different tasks in the paper is clear and structured. The discussion of the computational complexity for each task is thorough."}, "weaknesses": {"value": "* The evaluation only considers the first answer provided by the model. This might underestimate a model's capabilities if it generates a correct answer later in its output.\n* The model needs to produce all actionsâ€™ effects correctly to be scored. This evaluation is strict. Partial credits may be considered as other evaluation metric. \n* The baselines of the evaluation are not comprehensive enough. Only 2-shot prompting is included. No CoT prompting or hybrid approaches with solvers are included. \n* The paper is text heavy. More illustrations can be added to aid understanding. The high level workflow looks simple and clear but is not in the main texts."}, "questions": {"value": "* What would results look like under less strict metrics? \n* What is the dataset size per task? For each task, can you include more information such as some data examples, example input/output, failure cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tNmvRnvFa0", "forum": "WIXohR7mEo", "replyto": "WIXohR7mEo", "signatures": ["ICLR.cc/2026/Conference/Submission20990/Reviewer_2qTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20990/Reviewer_2qTa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987923693, "cdate": 1761987923693, "tmdate": 1762939810419, "mdate": 1762939810419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}