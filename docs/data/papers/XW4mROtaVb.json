{"id": "XW4mROtaVb", "number": 16985, "cdate": 1758270938173, "mdate": 1759897206227, "content": {"title": "Judo: A Juxtaposed Domain-oriented Multimodal Reasoner for Industrial Anomaly QA", "abstract": "Industrial anomaly detection has been significantly advanced by large multimodal models (LMMs), enabling diverse human instructions beyond detection, particularly through visual-grounded reasoning for better image understanding. However, the lack of domain-specific knowledge of LMMs limits the accurate generation of responses in complex industrial scenarios. In this work, we present\nJUDO, Juxtaposed Domain-Oriented Multi-modal Reasoner, a framework that efficiently incorporates domain knowledge and context in visual and text reasoning. The visual reasoning provides detailed inspection by segmenting the defect region in the query image by juxtaposing it with the normal image as visual domain context, enabling a fine-grained visual comparative analysis. Furthermore, we inject domain knowledge through supervised fine-tuning (SFT) to enhance context understanding and subsequently guide domain reasoning through reinforcement learning (GRPO) with three tailored rewards. Experimental results demonstrate that JUDO achieves superior performance on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and GPT4o. These results highlight the importance of enhancing domain knowledge and context for effective reasoning in anomaly understanding. The implementation of JUDO can be found in\nhttps://anonymous.4open.science/r/JUDO-9C8B.", "tldr": "We introduce JUDO, a novel framework that enhances industrial anomaly detection by juxtaposing defective and normal images for domain-oriented visual reasoning while injecting domain knowledge through reinforcement learning.", "keywords": ["Industrial Anomaly QA", "Large Multimodal models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce068be5ec453024a3e429be2340a898d048758e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on multi-modal anomaly reasoning via large-scale multi-modal language model. Specifically, it considers how to make the multi-modal language model be more aware of the domain-specific anomaly. To achieve this, it proposes a three-stage model training/post-training pipeline by considering the anomaly discrimination of normal and abnormal cases, the supervised instruction tuning and reinforcement-learning-based post training. The proposed method acgieves state of the art perfroance of anomaly reasoning on the MMAD benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is well-motivated, seeking to enhance domain-specific anomaly understanding by injecting fine-grained anomaly knowledge throughout the training stages of a large multi-modal language model.\n\n- The proposed three-stage training pipeline, which integrates both visual and linguistic anomaly data, is logically sound and effectively develops strong anomaly reasoning capabilities.\n\n- The reward engineering implemented in the reinforcement-based post-training stage is well-designed and appropriate for the task.\n\n- The paper is well-written and easy to follow, presenting a clear and logical flow of ideas."}, "weaknesses": {"value": "- The methodological contribution appears limited, as the proposed framework primarily involves a sequential application of standard techniques (instruction-based fine-tuning and reinforcement learning-based post-training). While effective, this approach offers fewer novel insights into the fundamental challenges of injecting anomaly knowledge into multi-modal language models.\n\n- The proposed method demonstrates a significant performance degradation in anomaly discrimination compared to the Qwen-2.5 baseline. The paper offers several hypotheses for this outcome in the analysis section, but these are not experimentally validated, which undermines their credibility. A more rigorous analysis, such as an ablation study, is needed to clarify which training stage introduces the apparent trade-off between anomaly detection and anomaly reasoning capabilities.\n\n- The manuscript lacks a detailed discussion of the training data division and statistics, which are critical for evaluating the effectiveness of the post-training stage. Furthermore, the paper's central claim—that injecting domain knowledge during training is superior to doing so at test-time—is not substantiated with experimental evidence. A direct comparison is needed to validate this assertion.\n\n- The comparison to general-purpose LLMs could be strengthened. While the inclusion of GPT-4o is a good start, this baseline is rapidly becoming dated. To provide a more current and comprehensive understanding of the performance landscape, it would be beneficial to include comparisons with more recent state-of-the-art commercial models (e.g., Claude 3, Gemini 1.5)."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "edtKcKEPg1", "forum": "XW4mROtaVb", "replyto": "XW4mROtaVb", "signatures": ["ICLR.cc/2026/Conference/Submission16985/Reviewer_EjVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16985/Reviewer_EjVp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760823489598, "cdate": 1760823489598, "tmdate": 1762927001146, "mdate": 1762927001146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JUDO, a multimodal reasoning framework for industrial anomaly understanding. The method integrates visual comparative segmentation, domain knowledge injection via supervised QA fine-tuning, and domain-oriented GRPO alignment to improve both defect localization and explanation quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well-motivated, addressing the domain knowledge gap that limits existing VLMs in industrial anomaly analysis. \n2. The three-stage training pipeline is clear, systematic, and practically meaningful. The incorporation of juxtaposed segmentation grounding enhances fine-grained visual reasoning, while domain QA fine-tuning contributes to more reliable textual analysis. \n3. The paper is clearly structured and the experimental results are presented in a well-organized manner."}, "weaknesses": {"value": "1. The proposed framework is mainly an integration of existing components—comparative segmentation, supervised domain knowledge tuning, and GRPO-based post-training—and thus reads more as pipeline engineering rather than introducing a fundamentally new reasoning mechanism or architectural innovation. The methodological novelty therefore appears limited.\n2. The model depends on GPT-4o–generated reasoning chains as pseudo ground truth, yet the paper does not evaluate whether these rationales are reliable, consistent, or susceptible to hallucination. This creates a conceptual gap, since the paper also claims that general LMMs (including GPT models) lack domain reasoning capability.\n3. The paper attributes the performance degradation in anomaly discrimination to catastrophic forgetting but provides no empirical analysis or mitigation. Without verification or intervention, this explanation remains speculative and raises concerns about the robustness of the multi-stage training pipeline."}, "questions": {"value": "1. My interpretation is that the method essentially fine-tunes an existing VLM on domain-specific multimodal data, achieving better performance due to domain alignment. Could the authors clarify what they consider the core novelty beyond a well-designed training pipeline?\n2. The paper argues that GPT-based multimodal models lack sufficient industrial anomaly reasoning capability. However, GPT-4o is still used as the source of \"golden reasoning\" in the training pipeline. How do the authors justify the reliability and consistency of GPT-generated reasoning as ground truth in a domain where GPT is claimed to be insufficient? More specifically, how do you ensure the reasoning produced is correct and not hallucinated?\n\nIf I have misunderstood the intended contribution or positioning, I would appreciate clarification from the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M3HQ60nzuN", "forum": "XW4mROtaVb", "replyto": "XW4mROtaVb", "signatures": ["ICLR.cc/2026/Conference/Submission16985/Reviewer_mhqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16985/Reviewer_mhqc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454897156, "cdate": 1761454897156, "tmdate": 1762927000179, "mdate": 1762927000179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents JUDO, a Juxtaposed Domain-Oriented Multimodal Reasoner, to address the current weaknesses in industrial anomaly detection using LMMs (Large Multimodal Models). While the GRPO-based LMMs are optimized for tasks grounded in general knowledge, pinpointing the defects in industrial setups requires superior domain knowledge and visual context through a three-stage progressive framework. The framework uses juxtaposed segmentation learning, supervised fine-tuning (SFT) that injects domain-specific knowledge, and reinforcement learning (GRPO) with tailored multi-component rewards for domain-aware reasoning, resulting in an average accuracy of 80.73% and improvement in explainability through domain-aligned, visually grounded reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The three-stage learning framework proposed in the paper is logical and helps advance the integration of domain knowledge into multimodal reasoning. The three-stage progressive learning pipeline (SegJux, DomInj, GRPOdom) effectively combines visual segmentation, textual domain knowledge, and reinforcement learning. This hierarchical design is both conceptually strong and technically sound, marking a meaningful step beyond conventional GRPO-based post-training methods.\n\n2. The visual grounding via segmentation and the domain-aligned textual reasoning make outputs transparent and practical for industrial use. By segmenting the input images into semantically meaningful regions (e.g., defect areas, product components, the model anchors its subsequent textual explanations to specific visual evidence. This ensures that every reasoning step is traceable to an observable region within the image, reducing errors and improving trustworthiness.\n\n3. This paper addresses a significant industrial challenge of creating an automated, explainable anomaly detection system. The findings have potential applicability across various industry segments like manufacturing, logistics, etc.\n\n4. State-of-the-art average on MMAD versus both general LMMs and AnomalyR1; ablations demonstrate additive benefits of each stage."}, "weaknesses": {"value": "1. Compared with other general-purpose models, JUDO lags in binary anomaly detection tasks. JUDO’s score (64.51%) in simple anomaly-vs-normal classification tasks is lower than that of general-purpose VLMs such as Kimi-VL (72.93%). Although the paper claims this can be attributed to the advanced vision encoder of Kimi-VL, this could also suggest that the model’s complex reasoning optimization may not translate well to simpler binary tasks.\n\n2. Reasoning mode not systematically evaluated: The paper argues that free-form CoT can hurt MMAD accuracy, thus opting for a constrained `<seg>/<think>/<answer>`  format with reward shaping. However, the trade-offs among no reasoning, free CoT, and structured reasoning are not empirically studied. A controlled comparison would clarify why structured reasoning helps and when CoT hurts, strengthening claims.\n\n3. The paper does acknowledge that sequential training across three stages can cause performance degradation in earlier-learned tasks. Although addressed to some extent, this issue remains a known weakness of multi-phase fine-tuning and may affect scalability to more diverse industrial domains."}, "questions": {"value": "1. Could the authors provide an ablation comparing different reasoning styles – (a) direct answer (no reasoning), (b) free-form Chain-of-Thought, and the proposed structured `<seg>/<think>/<answer>` reasoning – to quantify the trade-offs between interpretability and task accuracy?\n\n2. JUDO underperforms compared to general-purpose models in binary anomaly discrimination. Could the authors clarify whether it is due to catastrophic forgetting, visual encoder limitations, or reinforcement objectives prioritizing reasoning quality over detection precision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0k2xRezC0N", "forum": "XW4mROtaVb", "replyto": "XW4mROtaVb", "signatures": ["ICLR.cc/2026/Conference/Submission16985/Reviewer_ixi3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16985/Reviewer_ixi3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794194983, "cdate": 1761794194983, "tmdate": 1762926999198, "mdate": 1762926999198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper presents JUDO (Juxtaposed Domain-Oriented Multi-modal Reasoner), a framework designed to enhance industrial anomaly detection by integrating domain-specific knowledge and contextual reasoning into large multimodal models (LMMs).\n2. JUDO performs visual comparative reasoning by juxtaposing defect and normal images for fine-grained inspection, and injects domain expertise through supervised fine-tuning followed by reinforcement learning (GRPO) with tailored rewards.\n3. Experiments on the MMAD benchmark show that JUDO outperforms strong baselines such as Qwen2.5-VL-7B and GPT-4o, highlighting the value of domain-oriented reasoning for anomaly understanding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Multistage Training Framework:\nThe paper introduces a well-structured two-stage pipeline combining supervised fine-tuning (SFT) and reinforcement learning (GRPO), effectively aligning domain-specific knowledge with multimodal reasoning objectives.\n\n2. Conceptual Contribution in Knowledge Integration:\nIt provides a meaningful conceptual advancement in how to inject out-of-domain or domain-specific knowledge into large multimodal language models, enabling more accurate and interpretable reasoning for industrial anomaly understanding."}, "weaknesses": {"value": "1. Clarity and Presentation:\nThe paper’s presentation is somewhat difficult to follow, particularly in the implementation and dataset construction sections.\nFor example, in Lines 216–224, it would be helpful to include a concrete example comparing a QA pair derived directly from the domain snippet versus one generated for general inspection knowledge, to clearly illustrate their differences.\n\n2. Incomplete Baseline Comparison:\nThe experimental section lacks evaluation against recent state-of-the-art vision–language models (VLMs) such as Gemini 2.5 Pro, which would provide a stronger and more convincing benchmark for JUDO’s effectiveness."}, "questions": {"value": "1. Dataset Scale and Composition:\nHow many QA pairs were generated for the “general inspection knowledge” subset? It would be useful to know the relative proportion of these conceptual QA pairs compared to those derived directly from domain snippets in MMAD.\n\n2. Use of External Knowledge in Closed-Source Models:\nGiven that open-source VLMs still lag slightly behind but demonstrate strong reasoning capabilities, could the proposed “general inspection knowledge” QA corpus be used as external prompt knowledge for closed-source models such as GPT-4o, GPT-5, or Gemini 2.5 Pro?\nHow would this prompt-based knowledge injection compare to the performance of the proposed fine-tuned JUDO model in terms of reasoning accuracy and interpretability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sb0APqwSJX", "forum": "XW4mROtaVb", "replyto": "XW4mROtaVb", "signatures": ["ICLR.cc/2026/Conference/Submission16985/Reviewer_wrJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16985/Reviewer_wrJU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026098464, "cdate": 1762026098464, "tmdate": 1762926998542, "mdate": 1762926998542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}