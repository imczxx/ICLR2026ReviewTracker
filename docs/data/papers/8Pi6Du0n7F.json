{"id": "8Pi6Du0n7F", "number": 17241, "cdate": 1758273780658, "mdate": 1763727344715, "content": {"title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors", "abstract": "Context compression presents a promising approach for accelerating large language model (LLM) inference by compressing long contexts into compact representations.Current context compression methods predominantly rely on autoencoding tasks to train context-agnostic compression tokens to compress contextual semantics.While autoencoding tasks enable compression tokens to acquire compression capabilities, compression via autoencoding tasks creates a fundamental mismatch: the models are optimized for reconstruction that diverge from actual downstream tasks, thereby weakening the features more beneficial for real-world usage.We propose Semantic-Anchor Compression (SAC), a novel method that shifts from autoencoding task based compression to an architecture that is equipped with this compression capability $\\textit{a priori}$. Instead of training models to compress contexts through autoencoding tasks, SAC directly selects so-called anchor tokens from the original context and aggregates contextual information into their key-value (KV) representations. By deriving representations directly from the contextual tokens, SAC eliminates the need for autoencoding training. To ensure compression performance while directly leveraging anchor tokens directly, SAC incorporates two key designs: (1) anchor embeddings that enable the compressor to identify critical tokens, and (2) bidirectional attention modification that allows anchor tokens to capture information from the entire context.Experimental results demonstrate that SAC consistently outperforms existing context compression methods across various compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves 1 EM improvement at 5x compression over strong baselines,with increasing advantages at higher compression ratios.", "tldr": "SAC achieves efficient context compression without autoencoder training by using contextual semantic anchors and bidirectional attention.", "keywords": ["Context Compression", "Long Context LLMs", "LLM Memory"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba2685f3f41810072aa60fb0b4719a69448f72bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Semantic-Anchor Compression (SAC), which directly selects anchor tokens from the original context and aggregates contextual information into KV representations. It consists of two key designs---anchor embeddings and bidirectional attention to ensure the compression performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed solution is simple but effective. The author shows good results on many downstream tasks.\n2. The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "1. The intuition behind the proposed method is confusing. The authors argue that \"SAC is distinct to previous compression-based methods\" because \"previous methods compress contextual information into context-agnostic special tokens\". However, in a auto-regressive setting, the compressed token is actually context-dependent since it can see all tokens behind in attention computation.\n2. SAC add lora to Llama-3.2-1B with attention map modification, from causal to bidirection attention. This sounds quite confusing and seems not to be a normal routine. Why does not use encoder models directly?\n3. The idea of context compression has been well-studied in many previous works, such as [1]-[5]. These works impair the novelty of proposed method.\n\n[1] Yen, Howard. Long-context language modeling with parallel context encoding. MS thesis. Princeton University, 2024.\n\n[2] Li, Yuhong, et al. \"Snapkv: Llm knows what you are looking for before generation.\" Advances in Neural Information Processing Systems 37 (2024): 22947-22970.\n\n[3] Zhang, Zhenyu, et al. \"H2o: Heavy-hitter oracle for efficient generative inference of large language models.\" Advances in Neural Information Processing Systems 36 (2023): 34661-34710.\n\n[4] Han, Wei, et al. \"Two are better than one: Context window extension with multi-grained self-injection.\" arXiv preprint arXiv:2410.19318 (2024).\n\n[5] Zhang, Peitian, et al. \"Long context compression with activation beacon.\" arXiv preprint arXiv:2401.03462 (2024)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9wHVGHYeIk", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Reviewer_4eQ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Reviewer_4eQ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760971642756, "cdate": 1760971642756, "tmdate": 1762927196777, "mdate": 1762927196777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Semantic-Anchor Compression (SAC)**, a new training-based method for LLM context compression. Its generation pipeline follows the prevailing setup:\n\n- one language model acts as the compression model to produce compressed tokens from a given context, and\n- a (frozen) generation model consumes those compressed tokens to generate outputs for downstream tasks.\n\nPrior approaches----ICAE, 500xCompressor, and EPL ----first **pretrain with an autoencoding (AE) objective** to reconstruct the original context from compressed tokens, then fine-tune by passing the compressed tokens to a frozen generator, optimizing the LM objective and, finally, the downstream (e.g., QA) objective. The authors argue these methods require new, randomly initialized compression tokens*whose embeddings must be trained----on top of LoRA fine-tuning of the compression model----which \"lack inherent semantic information\" and thus demand extensive AE pretraining.  \n\nSAC addresses this by selecting anchor tokens directly from the original context, removing the need for AE pretraining and retaining only LM and task-level fine-tuning. Concretely, the context is partitioned into equal-length chunks, and the **center token** of each chunk is chosen as the **anchor** (following EPL). A fixed **anchor embedding** is then added to these tokens to highlight their special role. Finally, **bidirectional attention** is enabled **only in the compression model**, allowing each anchor to aggregate information from the **entire** context; the resulting **compressed KV states**----one per anchor----serve as the compact representation passed to the generation model. In experiments, the authors use Llama-3.2-1B for both compression and generation; during training, the compression model is fine-tuned with LoRA (r=128) while the generation model remains **frozen**.\n\nTo recap, SAC's contributions are twofold:\n1. **Eliminates AE pretraining**, relying only on **LM** and **downstream (QA) fine-tuning**.  \n2. **Replaces learned compression tokens** with **semantically grounded anchor tokens** taken **from the original context**.\n\nTwo simple architectural tweaks make this effective: (i) add a dedicated **anchor embedding** to mark anchors, and (ii) apply **bidirectional attention** (in the compression model) so anchors can gather information from both preceding and succeeding tokens.\n\nEmpirically on MRQA, SAC consistently outperforms strong AE-based baselines (500xCompressor, EPL) on both in-domain and out-of-domain datasets, with the gap widening at higher compression ratios (e.g., 15× and 51×). The method also shows better training efficiency due to removing AE pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The critique of the AE's inefficiency is valid.\n\n2. The core idea of using in-context \"anchor tokens\" is reasonable. It simplifies the compression training process and, as the t-SNE analysis (Fig. 5) suggests, produces compressed representations that are more aligned with the original token space. \n\n3. SAC consistently outperforms baselines on MRQA, as well as during training (Appendix C trade-off curves). The fact that its performance advantage *increases* at higher compression ratios (15x, 51x) makes it more compelling. The experimental setups and compression ratios are consistent with prior works.\n\n4. Table 5 shows that adding an AE task to the SAC architecture (SAC w/ AE+LM) actually *hurts* performance compared to the LM-only (SAC) model. This provides powerful evidence for the \"AE mismatch\" theory that the authors argue for, which is quite different from what the literature used to believe.\n\n5. By removing the costly AE pretraining step, the method is inherently simpler and faster to train (as shown in Appendix B.1, Table 7)."}, "weaknesses": {"value": "1. The paper's default strategy (uniform chunking) is simple and effective. However, the ablation in Table 4 shows it performs similarly to a more complex method (Lingua-2). This suggests the selection strategy is important, but the paper stops short of exploring it deeply. \n\n2. The baselines compared in the paper seem to be a bit outdated. The paper is missing an important baseline, DAST [1], which is also chunked compression but with a dynamic token budget, achieving decent performance at a high compression ratio. \n\n3. The evaluation is performed exclusively on MRQA, a question-answering benchmark. This is a good choice, as QA is a key application. However, the paper's central critique is that the AE task is misaligned with *downstream tasks*. For a task like long-document summarization, an AE-style reconstruction objective might be *more* aligned than a QA objective. It is unclear if SAC's superiority would hold on tasks that require a more holistic understanding of the full context, rather than just extracting specific facts.\n\n4. The paper states it \"modifies the standard causal attention to a bidirectional attention mechanism.\" For clarity, it would be beneficial to explicitly state whether this bidirectional mask is applied *only* to the anchor tokens (i.e., their queries can see all keys/values) or if the entire compressor LLM (with LoRA) is run in a fully non-causal (BERT-style) mode during the compression pass.\n\n5. All experiments are conducted on a single model at a single parameter size. It does not guarantee that the performance is generalizable to other models or models with much larger scales.\n\n**Reference**:\nShaoshen Chen, Yangning Li, Zishan Xu, Yongqin Zeng, Shunlong Wu, Xinshuo Hu, Zifei Shan, Xin Su, Jiwei Tang, Yinghui Li, and Hai-Tao Zheng. 2025. DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens. In Findings of the Association for Computational Linguistics: ACL 2025, pages 20544–20552, Vienna, Austria. Association for Computational Linguistics."}, "questions": {"value": "1. (Regarding Weakness 1) Have you experimented with or considered a *learned* selection mechanism? It seems plausible that a small, trained module could identify more semantically salient tokens to serve as anchors, potentially improving performance further, especially at very high compression ratios. In addition, attention/entropy-based KV selection (i.e., from the KV eviction literature) could also be worth a try.\n\n2. (Regarding Weakness 2) Can you add DAST[1] for comparison? Better if PCC [2] can also be included. \n\n3. (Regarding Weakness 3) How do you anticipate SAC would perform on non-extractive, non-QA tasks like long-document summarization? The AE objective, while mismatched for QA, does force a holistic reconstruction. Does the LM-only pretraining capture enough global information for tasks that require a broader, generative understanding of the entire context? \n\n3. (Regarding Weakness 4) Could you please clarify the precise implementation of the bidirectional attention? Is the attention mask modified only for the anchor token queries, or is the entire encoder pass non-causal for all tokens?\n\n4. It would be better if more compression ratios could be experimented with, producing a trade-off curve between performance and compression ratio to further validate the robustness of SAC.\n\n5. Can you clarify how you initialize the anchor embeddings?\n\n**Minor Issues (does not affect evaluation)**:\n\n1. At line 198, \"While using original tokens … limiting its representation power.\" should be 1 sentence instead of 2.\n2. At line 429, there should be a space within \"Analysis.In\".\n\n**Reference**:\n1. Shaoshen Chen, Yangning Li, Zishan Xu, Yongqin Zeng, Shunlong Wu, Xinshuo Hu, Zifei Shan, Xin Su, Jiwei Tang, Yinghui Li, and Hai-Tao Zheng. 2025. DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens. In Findings of the Association for Computational Linguistics: ACL 2025, pages 20544–20552, Vienna, Austria. Association for Computational Linguistics.\n\n2. Liao. 2025. Pretraining Context Compressor for Large Language Models with Embedding-Based Memory. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 28715–28732, Vienna, Austria. Association for Computational Linguistics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eoq0qdxKs5", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Reviewer_wACd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Reviewer_wACd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771288116, "cdate": 1761771288116, "tmdate": 1762927196447, "mdate": 1762927196447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Semantic-Anchor Compression, a novel framework designed to improve the efficiency and performance of Large Language Models on long-context tasks. The method introduces an autoencoding-free approach to context compression. The core novelty lies in three architectural designs: 1) It selects 'anchor tokens' directly from the original context to act as information carriers. 2) It introduces a trainable 'anchor embedding' to semantically distinguish these selected tokens. 3) It enables 'bidirectional attention' specifically for these anchor tokens, allowing them to effectively aggregate global context. This design allows the compression mechanism to be optimized directly for downstream tasks. Experimental results on a benchmark dataset demonstrate that the method achieves strong performance across various compression ratios and shows favorable generalization capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel and effective information compression method based on anchor tokens. This \"autoencoding-free\" paradigm successfully moves beyond traditional context-agnostic compression tokens.\n\n2. The proposed SAC method is simple, architecturally concise, and elegant in its design.\n\n3. The paper is exceptionally well-written and well-structured, clearly articulating the core problem, the proposed solution, and its advantages."}, "weaknesses": {"value": "1. The paper's default strategy is to select the \"middle token\" of a chunk. However, the ablations (Table 4) are missing a critical and intuitive baseline: selecting the \"last token\" of each chunk as the anchor. In a causal model, the last token has naturally seen the entire chunk. This is a significant omission that prevents a full assessment of the \"middle token\" strategy and makes it difficult to judge if the performance gain comes from a truly better mechanism or just a better-positioned token.\n2. All experiments are conducted only on QA tasks. As the authors state, the AE task deviates from \"real-world usage,\" but using only QA to represent \"real-world usage\" is far from sufficient. SAC is a lossy compression (as it discards the AE objective), making it highly likely to have overfitted to the specific patterns of QA tasks. Its performance on other long-context tasks (e.g., summarization, story generation) is unknown.\n\n3. The paper's core argument is that AE-free is better. However, the authors never test the SAC + AE combination. That is, using SAC's anchor architecture (with bidirectional attention) to simultaneously perform the downstream task and the AE reconstruction task. It is plausible that SAC + AE would have slightly lower QA performance but be more robust and general-purpose (by retaining reconstruction). The authors claim AE is harmful but do not prove this experimentally (i.e., by comparing SAC vs. SAC + AE)."}, "questions": {"value": "1. Can you please provide a baseline experiment using the \"last token\" of each chunk as the anchor, while still applying your anchor embedding and bidirectional attention? I am very curious how this intuitive baseline compares to your \"middle token\" strategy.\n\n2. Do you have plans to validate SAC on other long-context tasks besides QA (e.g., summarization, multi-turn dialogue)? The current setup makes me highly concerned that the model is overfitting to QA while sacrificing general compression capabilities.\n\n3. Did you consider testing a SAC + AE setup (i.e., using your anchor architecture but retaining the AE loss)? What are your thoughts on the trade-off between QA performance and general-purpose robustness that this combination might offer?\n\n4. Could you provide a detailed analysis of the inference overhead? While SAC saves on AE training, what is the specific extra compute (FLOPs) or latency cost of enabling bidirectional attention for anchor tokens (either over the full sequence or within chunks) during inference, compared to standard KV caching or older compression methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wwY8sAIX3h", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Reviewer_NhSy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Reviewer_NhSy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902014323, "cdate": 1761902014323, "tmdate": 1762927196137, "mdate": 1762927196137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a context compression method that does not relying on autoencoding training. The proposed method, SAC, selects anchor tokens directly from the original tokens and aggregates contextual information into their key-value tokens using bidrectional attention. Experiments show that SAC outperforms existing context compression methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Context compression is an important problem worthy of investigation. \n2. The proposed method is simple. It combines token selection, common in KV cache compression but probably not in context compression, and bidirectional attention. \n3. The proposed method outperfoms existing context compression methods."}, "weaknesses": {"value": "1. The method is not clearly described and sometimes very confusing. \n2. The contexts in the experiments are in general short. It is of more interest to compress long contexts. \n3. The experiments use a single model, Llama-3.2-1B. It is unclear whether the proposed method generalizes well to larger models and other model families. \n4. The paper has an observation that challenges previous work, but does not provide any explanation, so it is hard to see if is a bug or something new."}, "questions": {"value": "1. Can you give a clear description of the method? For example, what are the anchor embeddings? How does anchor embeddings enable the compressor to identify critical tokens? The anchor tokens are just sampled evenly, so why are they more important? Why do you choose the output of anchor tokens from the LLM's final layers or the KV pairs from each layer? Any justifications? \n2. Can you demonstrate the effectiveness and performance improvement of the proposed method for long contexts? \n3. Can you provide evidence for the generalizability of the proposed model to larger models and other model families? \n4. What do you think might have caused the discrepancy in the observed AE effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dRvZ5nvfDv", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Reviewer_rbBK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Reviewer_rbBK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970284257, "cdate": 1761970284257, "tmdate": 1762927195826, "mdate": 1762927195826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Supplementary Results - part 1"}, "comment": {"value": "We sincerely thank all the reviewers for their constructive suggestions. In response, we have carefully analyzed their concerns and added the following experimental results. Except for the larger-scale experiments, all our experiments are conducted using Llama-3.2-1B with a fixed 15 times compression rate, and the evaluation metric is ROUGE-1 F1.To ensure the fairness of the comparison, all our comparative experiments maintained the same training data and hyperparameter settings.\n\n# Long-Context Task Evaluation Results (Summarization & QA)\n\nWe are grateful to the reviewers for their concern about the performance of SAC in long text tasks. \nFor the summarization tasks, we trained SAC and EPL [1] on QMSum [2] and GovReport [3] with a maximum input length of 32K tokens, and their test results are shown in Table 1. Meanwhile, We also perfrom the test on a 24k token-long context QA benchmark, using the model checkpoint following the settings in the paper trained with a 15x compression ratio in a 2k token context, and the results are shown in Table 2.\n\nTable 1: Model Performance on 32K Long-Text Summarization Tasks.\n||QMSum|GovReport|Summ Avg|\n|:-:|:-:|:-:|:-:|\n|SAC|14.95|**22.03**|**18.49**|\n|SAC(ae+lm)|**15.32**|20.15|17.74|\n|EPL|14.82|20.40|17.61|\n\nTable 2: Model Performance on 24K Long-Text QA Tasks.\n||MultifieldQA|NarrativeQA|Qasper|Sigle-doc QA Avg|2Wikimqa|Musique|HotpotQA|Multi-doc QA Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**17.75**|**11.33**|**11.67**|**13.58**|**24.78**|**12.36**|**26.03**|**21.06**|\n|SAC(ae+lm)|17.38|10.49|9.05|12.31|22.47|7.76|23.88|18.04|\n|EPL|14.34|10.50|7.58|10.81|21.99|8.43|23.72|18.05|\n\n# Scale-up Evaluation Results\n\nWe thank the reviewers for their concern regarding larger-scale experiments. We trained Llama3.2-3B and Llama-3.1-8B on 8 A100 GPUs using the same data and training settings as described in the paper.\n\nTable 3: Experimental Results of the 1B/3B/8B Models on the In-Domain QA Tasks.\n\n||SQuAD|NewsQA|TriviaQA|SearchQA|HotpotQA|NQ|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC(1B)|**47.43**|**36.55**|**61.13**|**68.97**|**58.83**|**56.79**|**54.95**|\n|EPL(1B)|44.58|33.34|56.16|66.36|54.88|53.80|51.52|\n|SAC(3B)|**63.11**|**49.23**|**69.65**|**73.88**|**68.87**|**65.98**|**65.12**|\n|EPL(3B)|62.43|46.72|68.83|73.61|68.50|63.59|63.95|\n|SAC(8B)|64.92|49.77|72.37|**76.92**|70.80|**67.77**|**67.09**|\n|EPL(8B)|**65.79**|**51.18**|**72.4**|74.86|**70.98**|67.24|67.08|\n\nTable 4: Experimental Results of the 1B/3B/8B Models on the Out-of-Domain QA Tasks.\n\n||BioASQ|DROP|DuoRC|RACE|RE|TextbookQA|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC(1B)|**41.31**|**36.72**|**28.94**|**23.35**|**61.04**|**44.21**|**39.26**|\n|EPL(1B)|40.52|32.16|25.70|20.97|59.75|41.31|36.74|\n|SAC(3B)|**47.96**|**48.46**|**42.21**|33.61|**75.11**|**55.53**|**50.48**|\n|EPL(3B)|46.76|46.20|36.81|**33.80**|65.73|55.46|47.46|\n|SAC(8B)|48.54|**51.55**|**41.44**|**35.52**|**78.91**|**57.87**|**52.31**|\n|EPL(8B)|**52.52**|49.98|40.09|35.22|69.77|57.36|50.82|\n\n# Anchor Selection Strategy Comparison for SAC\n\nWe thank the reviewer for this insightful suggestion. We have experimented with a learned selection mechanism as follows. To enable end-to-end training with discrete Top-K selection, we employ a differentiable approximation: the input is first mapped to logits via a linear layer, then relaxed into a continuous probability distribution using Gumbel-Softmax. During the forward pass, we perform hard Top-K selection to identify anchor tokens, while during backpropagation, we apply the straight-through (ST) estimator—gradients bypass the non-differentiable Top-K operation and flow through the soft distribution to update the linear layer. This allows the module to automatically learn which tokens best preserve sequence semantics and minimize the final LM loss.\n\nTable 5: Performance of Anchor Selection Strategies on In-Domain QA Tasks.\n\n||SQuAD|NewsQA|TriviaQA|SearchQA|HotpotQA|NQ|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**47.43**|**36.55**|**61.13**|**68.97**|**58.83**|**56.79**|**54.95**|\n|SAC(chunk last token)|46.59|36.03|60.80|67.20|58.51|55.41|54.09|\n|SAC(learned selection)|42.32|33.69|59.04|68.27|55.52|53.76|52.10|\n\nTable 6: Performance of Anchor Selection Strategies on Out-of-Domain QA Tasks\n\n||BioASQ|DROP|DuoRC|RACE|RE|TextbookQA|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|41.31|**36.72**|28.94|23.35|**61.04**|44.21|**39.26**|\n|SAC(chunk last token)|**42.87**|35.61|**29.11**|**23.39**|56.11| **45.17**|38.71|\n|SAC(learned selection)|41.93|34.12|28.64|21.07|58.24|44.38|38.06|"}}, "id": "ZcLlm0Cuny", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724318853, "cdate": 1763724318853, "tmdate": 1763724318853, "mdate": 1763724318853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Supplementary Results - part 2"}, "comment": {"value": "# Comparison of SAC with Other Baseline Methods\n\nWe thank the reviewers for suggesting the comparative baseline, which has enriched our paper. Since the DAST [4] codebase is empty, we reproduced it according to the descriptions in the paper. For a fair comparison, all hyperparameter settings and training data are identical to those used for SAC.\n\nFor Activation Beacon [6], we employed the same training data and only modified some hyperparameter settings to ensure alignment with SAC, specifically setting beacon_window=510 and beacon_ratio=[2,4,8,15], while keeping the remaining settings consistent with the examples in the codebase.\n\nTable 7: Baseline Performance Comparison on In-Domain QA Tasks.\n\n||SQuAD|NewsQA|TriviaQA|SearchQA|HotpotQA|NQ|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**47.43**|**36.55**|**61.13**|**68.97**|**58.83**|**56.79**|**54.95**|\n|DAST|36.33|31.55|56.92|68.07|54.02|52.10|49.83|\n|Activation Beacon|37.53|31.10|48.85|39.06|45.01|40.29|40.31|\n\nTable 8: Baseline Performance Comparison on Out-of-Domain QA Tasks.\n\n||BioASQ|DROP|DuoRC|RACE|RE|TextbookQA|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**41.31**|**36.72**|28.94|23.35|**61.04**|**44.21**|**39.26**|\n|DAST|36.57|31.90|21.56|16.31|48.54|36.69|31.83|\n|Activation Beacon|36.07|34.39|**33.78**|**26.68**|53.28|37.27|36.91|\n\n# Effect of Different Compressors\n\nWe thank the reviewer for the insightful suggestion.  Since ALBERT's [5] KV layers cannot align with the decoder, we chose to use the hidden state as the compressed representation and compare with ICAE which also uses hidden state as the compressed representation.  For fair comparison, we maintained identical training data and hyperparameter settings, with the results shown in the table below.\n\nTable 9: Performance of Different Compressors on In-Domain Tasks.\n\n||SQuAD|NewsQA|TriviaQA|SearchQA|HotpotQA|NQ|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**47.43**|**36.55**|**61.13**|**68.97**|**58.83**|**56.79**|**54.95**|\n|ICAE-Llama|31.90|25.25|51.78|64.81|45.22|48.01|44.50|\n|ICAE-ALBert|13.16|6.47|19.13|30.93|12.09|13.06|15.81|\n\nTable 10: Performance of Different Compressors on Out-of-Domain Tasks.\n\n||BioASQ|DROP|DuoRC|RACE|RE|TextbookQA|Avg|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|SAC|**41.31**|**36.72**|**28.94**|**23.35**|**61.04**|**44.21**|**39.26**|\n|ICAE-Llama|35.51|30.39|13.78|15.21|55.24|34.75|30.81|\n|ICAE-ALBert|23.73|21.62|3.37|5.56|15.62|25.15|15.84|\n\n# Reference\n\n[1] Runsong Zhao, et al. Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models. EMNLP Findings, 2025.\n\n[2] Ming Zhong, et al. QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization. NAACL, 2021.\n\n[3] Luyang Huang, et al. Efficient Attentions for Long Document Summarization. NAACL, 2021.\n\n[4] Shaoshen Chen, et al. DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens. ACL Findings, 2025.\n\n[5] Zhenzhong Lan, et al. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. ICLR, 2020.\n\n[6] Zhang, Peitian, et al. \"Long Context Compression with Activation Beacon.\" arXiv preprint arXiv:2401.03462 (2024)."}}, "id": "mO7Cr1hvSF", "forum": "8Pi6Du0n7F", "replyto": "8Pi6Du0n7F", "signatures": ["ICLR.cc/2026/Conference/Submission17241/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17241/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission17241/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724464053, "cdate": 1763724464053, "tmdate": 1763724464053, "mdate": 1763724464053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}