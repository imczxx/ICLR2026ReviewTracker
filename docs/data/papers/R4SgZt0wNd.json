{"id": "R4SgZt0wNd", "number": 18166, "cdate": 1758284598421, "mdate": 1763267680181, "content": {"title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing", "abstract": "Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have begun supporting pointing capabilities, existing benchmarks typically focus only on referential object localization. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset of approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive web-based arena facilitating blind, pairwise model comparisons, which has collected over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate model pointing in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary models. Results indicate that Molmo-72B consistently outperforms others, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training targeting pointing tasks significantly improves performance. Across our multi-stage evaluation pipeline, we observe strong correlations, underscoring the critical role of precise pointing in enabling multimodal models to bridge abstract reasoning with real-world actions.", "tldr": "An Open Platform for Evaluating Multimodal Large Language Models on Visually Grounded Reasoning via Pointing Across Multiple Evaluation Stages", "keywords": ["Multimodal Large Language Models (MLLMs)", "Pointing", "Open Evaluation", "Visual Grounding", "Language-guided pointing", "MLLM Arena"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/657c2d9211029801ed13739f6f26bc2c638aa315.pdf", "supplementary_material": "/attachment/14155c18556b23737ad5ff75709c41383197edc9.zip"}, "replies": [{"content": {"summary": {"value": "Authors introduced a benchmark for evaluating the image-langue-pointing capabilities of multi-model large language models. The benchmark devided the tasks into five categories based on a survey. This paper also evaluated various LLMs, providing valuable information for downstream users. Nevertheless, the benchmark seems to be an oversimplification of language condition segmentation, and the benchmark missed to evaluate \"segment anything\" baseline."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposed a benchmark on language-image-poinintng with clear definition. The benchmark extensively evaluated various LLMs. Authors also discovered that chain-of-thought decreases the performance on the point task."}, "weaknesses": {"value": "My main concern is that the language-image-pointing task seems to be a simplification of language-image-segmentation task, where the LLM is prompted to segment out the object(s) of interest. One can subsample a pixel from the segmentation mask as the final output “point”. If my understanding is correct, then why do we need the pointing benchmark? Furthermore, why not include segment anything as a baseline? Authors can prompt the language question and subsample a pixel as a point."}, "questions": {"value": "In addition to the main concern on the weakness section, I have following questions:\n\na. What are the implementation details of the point-act? How is the robot controlled?\nb. In Figure 3, why “human”’s performance is not approaching 100% but is around 90%?\nc. In Figure 4, in addition to colors, could authors use different markers for each LLMs for better interpretability?\nd. Authors claim that “Pointing supervision significantly boosts performance.”, could you provide evidence that these model used “Point supervison”, and ideally provide an ablation study?\ne. Authors claim that “Molmo excels on Point-Act evaluation.”, could you explain why Molmo outperformed other baselines? Is it due to Molmo's architecture or training pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ThJvtCsKU8", "forum": "R4SgZt0wNd", "replyto": "R4SgZt0wNd", "signatures": ["ICLR.cc/2026/Conference/Submission18166/Reviewer_FkoL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18166/Reviewer_FkoL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447397766, "cdate": 1761447397766, "tmdate": 1762927926557, "mdate": 1762927926557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GhVce88WN7", "forum": "R4SgZt0wNd", "replyto": "R4SgZt0wNd", "signatures": ["ICLR.cc/2026/Conference/Submission18166/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18166/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763267679390, "cdate": 1763267679390, "tmdate": 1763267679390, "mdate": 1763267679390, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes Point-Bench, a curated dataset comprising approximately 1,000 pointing tasks across five reasoning categories, and evaluates both closed-source and open-source models on it. It further introduces Point-Battle, a framework for assessing VLMs’ pointing capabilities based on human preferences. Finally, the study demonstrates that Point-Bench accuracy strongly correlates with real-world manipulation task success."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The overall writing is clear and easy to follow.\n\nThe paper presents a well-motivated study that focuses on benchmarking the pointing ability of VLMs, with a clearly articulated purpose.\n\nIn addition to reporting standard success rates, it introduces Point-Battle and Point-Act as complementary evaluation metrics, providing a more comprehensive assessment."}, "weaknesses": {"value": "The paper lacks detailed statistics of the benchmark, such as the types of scenarios and object categories included, which are essential for understanding its coverage and diversity.\n\nAlthough Point-Act is presented as a key contribution, the paper provides insufficient details about the specific tasks performed by the robot and how the pointing ability translates into or supports manipulation performance.\n\nThe inference time of different models is not reported. Moreover, as a benchmarking paper, it would be beneficial to include information on the cost and feasibility of running the benchmark to facilitate future evaluations by other researchers."}, "questions": {"value": "What types of tasks are evaluated in Point-Act, and how are manipulation actions derived from the model’s pointing outputs? I would assume the use of a depth image or 3D information, but this part remains unclear.\n\nIs there a way to develop a more precise metric for measuring pointing ability? Using “point within the mask” seems to be only a rough criterion.\n\nHow does the model perform in terms of cross-view pointing ability i.e., consistency of pointing performance across different camera perspectives?\n\nIf different forms of prompts are used to query the MLLMs, would the evaluation results on this benchmark vary? Is it possible to provide additional experimental results to further verify the consistency and robustness of the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oYczl2d7VP", "forum": "R4SgZt0wNd", "replyto": "R4SgZt0wNd", "signatures": ["ICLR.cc/2026/Conference/Submission18166/Reviewer_deBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18166/Reviewer_deBG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946215937, "cdate": 1761946215937, "tmdate": 1762927925982, "mdate": 1762927925982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed PointArena, a comprehensive evaluation platform for multimodal pointing evaluation. \nIt consists of a dataset with 5 reasoning categories (Point-Bench), an interactive web-based arena (Point-Battle), and a real-world robot manipulation system (Point-Act). \nThe authors also conducted experiments with existing SoTA models and discussed insights about the role of pointing in real world reasoning-based tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The dataset consists of 5 representative and challenging reasoning/localization tasks, making this dataset able to evaluate various aspects of multimodal point comprehension. Point-Battle is also a good approach to address the lack of referral image-text pairs and accurate evaluators."}, "weaknesses": {"value": "1. Point-Act is under-explained. There's too little information about how the robot and experiments are set up. Evaluation metrics, examples, results, and how to access this platform, are missing. As robot pick-and-place is not only related to pointing itself, it's essential to clarify the rest of the setting, such as how the grasp motion and controller are implemented, and therefore how this platform contributes to the entire PointArena system.\n2. Sec 5 seems unfinished. Discussion, limitation, and conclusion look like summarizations of Sec 3, rather than drawing any insights from the experiments.\n3. Appendix is messy. Please make them more organized and clearly explained, rather than listing prompts and unscaled figures randomly, especially when you have some of your main results in the Appendix."}, "questions": {"value": "Point-Bench consists of 982 image-question pairs. Considering the diversity of open-world combinations of objects and relationships, is this scale sufficient to draw conclusions about model performance, especially when divided into 5 categories? As you said, it is totally possible to have leakage when evaluating up-to-date VLMs. Is integrating data from Point-Battle in the future your solution to this problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mjLRlgxFi8", "forum": "R4SgZt0wNd", "replyto": "R4SgZt0wNd", "signatures": ["ICLR.cc/2026/Conference/Submission18166/Reviewer_cWoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18166/Reviewer_cWoh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971246097, "cdate": 1761971246097, "tmdate": 1762927924496, "mdate": 1762927924496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PointArena, a three-stage evaluation suite for language-guided pointing with multimodal models: Point-Bench (a curated static benchmark of 982 image–query pairs across five reasoning categories), Point-Battle (a live, blinded, head-to-head arena that has collected >4.5k votes from around 100 participants), and Point-Act (a real-world robotic setup where models' predicted points drive an xArm to execute pick/place). The task is formalized as predicting one or more image-space coordinates that must land inside ground-truth masks, whose success is measured via a binary mask-coverage criterion with standardized [x, y] outputs. Evaluations of both proprietary and open-source MLLMs indicate Molmo-72B attains the highest Point-Bench accuracy, though some proprietary models are close; pointing-specific supervision yields large gains; and Point-Bench accuracy correlates strongly with Point-Battle preferences and Point-Act success. Ablations suggest targeted prompts beat chain-of-thought for pointing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. Treating pointing as a precise grounding interface is timely for robotics, assistive tech, and interactive systems. \n3. Point-Bench covers five complementary categories (Spatial, Affordance, Counting, Steerable, Reasoning), enabling diagnostic analyses beyond simple referring localization. Point-Battle brings scalable preference data with blinded pairwise comparisons and anonymized voting. Point-Act grounds claims in physical manipulation on an xArm 6 Lite, directly testing whether pointing precision transfers to real tasks.\n4. Strong cross-stage correlations between Point-Bench and Point-Battle (R²≈0.85), and Bench and Act (R²≈0.92), strengthen external validity."}, "weaknesses": {"value": "1. For non-counting tasks only the first returned point is scored, which may bias against models returning ranked or structured outputs. Consider top-k or confidence-weighted evaluation.\n2. For counting or multi-target cases, the success criterion is set coverage without precision/recall trade-offs. A bipartite matching metric (Hungarian assignment with distance thresholds) plus F1 would better reflect partial correctness and over-/under-pointing behavior.\n3. The Arena uses Elo with K=2 for stability. However, there is little discussion of voter quality control, duplicate participant mitigation, regional/task mixture balance, or defenses against strategic voting.\n4. Point-Act reports relative statements (e.g., substantial margin) but omits granular success definitions (reach/pick/place), latency, and failure modes (depth errors, gripper slip, calibration)."}, "questions": {"value": "1. I recommend to include a consolidated results table: per-category and average accuracy for all 16 models, with mean±SD over 3 runs and 95% CIs and annotate statistically indistinguishable groups.\n2. For the R²=0.85/0.92 findings, it would be better to provide scatter plots, model counts, bootstrapped CIs, and sensitivity checks (e.g., exclude models trained on pointing data; remove one family at a time).\n3. It would be better to report inter-annotator agreement on masks (e.g., mean IoU across verifiers) and a boundary-sensitivity study (success vs. distance to mask edges). Given SAM dependence, can you estimate error rates on thin structures? A small expert-reannotated subset could quantify bias.\n4. Did you run near-duplicate checks vs. PixMo/RoboPoint/RefCOCO family? Any overlap screening against common pretraining sets (e.g., LAION)? Since you argue pointing supervision drives gains, leakage audits will make the causal story more persuasive.\n5. Why CoT Hurts Pointing? Can you analyze failure modes (e.g., verbose reasoning truncating coordinates, distractor attention, incorrect normalization)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UZQJk7s2qc", "forum": "R4SgZt0wNd", "replyto": "R4SgZt0wNd", "signatures": ["ICLR.cc/2026/Conference/Submission18166/Reviewer_CNEY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18166/Reviewer_CNEY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972083487, "cdate": 1761972083487, "tmdate": 1762927922828, "mdate": 1762927922828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}