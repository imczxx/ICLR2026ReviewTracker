{"id": "xjoVqCJ9G4", "number": 4911, "cdate": 1757801924679, "mdate": 1759898005623, "content": {"title": "Towards Efficient Unroll Generalization in Learned Optimizers", "abstract": "Recent works have demonstrated that learned optimizers (LOs) can be competitive and sometimes even outperform hand-designed counterparts, highlighting their potential as a pathway toward developing better optimization algorithms. Yet, despite this promise, meta-generalization remains a major challenge for LOs. In particular, they often struggle to maintain stable convergence over long unrolls, as they are typically meta-trained only on short horizons. While extending the unroll length during meta-training may seem like a natural remedy, in practice it substantially increases computational cost (at least linearly) and frequently leads to divergence or collapse due to compounding errors. To improve the long unroll generalization of LOs, we propose a novel meta-training scheme called Efficient Long-horizon Learning (ELO), which leverages a replay buffer to efficiently extend unroll length during meta-training without adding extra meta-training cost. In addition, it integrates online behavior cloning to stabilize meta-training and potentially inherit the generalization benefits of hand-designed optimizers. We evaluate ELO on a variety of vision and language tasks, showing its success in achieving long-unroll generalization in practical scenarios.", "tldr": "", "keywords": ["deep learning", "meta-learning", "learned optimizers/learned optimization", "unroll generalization", "optimization", "replay buffer", "imitation learning", "computer vision", "language models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b43d5845b0df4a5e8e933c9e690130bd34065e54.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Learned optimizers have shown promise, but due to computational constraints and error compounding, they are meta-trained on short horizons and thus struggle on long horizons that may appear during meta-testing. This paper proposes ELO, a method to improve the generalization ability of learned optimizers to horizon lengths longer than what is usually seen during meta-training. It combines two ideas from the reinforcement learning literature, a replay buffer and behavior cloning, to stabilize meta-training and improve generalization. The replay buffer allows restarting from previously trained meta-optimizers, effectively increasing the horizon length without increasing the computational burden. Behavior cloning enforces that the learned optimizer is similar to Adam at the beginning of meta-training, thereby improving stability. Experiments are done on ImageNet-1K (32x32) and FineWeb-10B illustrating the efficacy of the method over AdamW and the original learned optimizer method, and ablations on the main components are also done."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and easy to understand. The motivation and the algorithm are explained clearly, and the figures are illustrative.\n- The algorithm itself is straightforward to understand, combining two well-established techniques from reinforcement learning.\n- The topic of learned optimizers is an important one, having the potential to improve training of a wide range of models."}, "weaknesses": {"value": "I believe that the main weakness of this paper lies in its experimental set-up. \n- First, ELO is quite similar to the algorithm in Chen et al. (2020). Chen et al. (2020) uses a curriculum of horizon lengths (which is a baseline in the experiments) along with off-policy imitation learning instead of behavior cloning. Therefore, I think that it should be included as a baseline in the experiments as well.\n- Only one meta-optimizer architecture and size were used in the experiments. It would be helpful to do experiments on a range of architectures or sizes, in order to see the applicability and scability of ELO.\n\nA second minor weakness is novelty. ELO provides a novel combination of experience replay and behavior cloning to the learned optimizer literature, but it is quite similar to the algorithm in Chen et al. (2020) as stated above. Thus, it would be helpful if the paper further discussed the relation of ELO to the algorithm in Chen et al. (2020). The appendix is also missing."}, "questions": {"value": "- Why was the size of the buffer chosen to be 4? Isn't that a bit small?\n- Wouldn't it be more fair to have Adam as a baseline instead of AdamW, since Adam is used as the expert in ELO?\n- Why is the accuracy so low in Table 1? Is it because of the size of the optimizer architecture?\n- Can ELO handle OOD tasks with architectures with different numbers of parameters, or do they have the same number of parameters (lines 361-369)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "adqC1jq3uc", "forum": "xjoVqCJ9G4", "replyto": "xjoVqCJ9G4", "signatures": ["ICLR.cc/2026/Conference/Submission4911/Reviewer_3mNj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4911/Reviewer_3mNj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760641180765, "cdate": 1760641180765, "tmdate": 1762917757214, "mdate": 1762917757214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a critical and well-known challenge in the field of learned optimizers (LOs): their poor generalization to long optimization horizons (\"unroll generalization\"). The authors propose ELO (Efficient Long-horizon Learning), a meta-training scheme designed to address this issue through two key components:\n\n1. ELO uses a replay buffer to store and sample intermediate training checkpoints. This allows new unrolls to start from an advanced state (e.g., step K) instead of always from step 0. This mechanism cleverly reallocates computation from repeatedly re-training the \"easy\" early steps to exploring longer, more challenging horizons, thereby achieving efficient long-unroll training without extra linear cost.\n\n2. The authors astutely observe that naively jumping into long unrolls (as \"Buffer_only\" in Fig 2) is unstable due to compounding errors. To solve this, ELO introduces an online behavior cloning (BC) curriculum. The meta-loss is a scheduled blend of a task loss and a BC loss, which forces the LO to imitate a stable expert (Adam). This stabilizes the volatile early stages of meta-training.\n\nThe empirical results, while limited by the experimental setup, demonstrate that ELO-LO outperforms naive LOs, curriculum-based LOs, and AdamW on the specific tasks chosen, particularly in the long-horizon regime (10,000 steps)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core contribution—using a replay buffer not for its traditional sample efficiency but to efficiently reallocate compute to longer unroll horizons—is a insightful concept.\n\n2. Unroll generalization is arguably one of the biggest hurdles preventing the practical, widespread adoption of LOs. This paper directly attacks this fundamental problem.\n\n3. The solution is a necessary combination of two components. The ablation studies (Fig 2 and Fig 7) convincingly show that both the Replay Buffer and the Behavior Cloning are essential. The buffer-only approach fails, proving that the BC component is a crucial stabilizer."}, "weaknesses": {"value": "1. The credibility of the paper's claims is severely undermined by the choice of meta-training and evaluation datasets. Using ImageNet-1K at a 32x32 resolution (Sec 5.1) is unreasonable. This low resolution discards the fine-grained details that make ImageNet a challenging task. While resource constraints are understandable, conclusions about generalization drawn from this setup are not convincing for practical, large-scale scenarios (which typically use 224x224 or 256x256 resolutions). \n\n2. The paper cites relevant, state-of-the-art LOs like VeLO (Metz et al., 2022), which also claim generalization to 10k+ steps. However, the paper fails to include these in the experimental comparison. The baselines are limited to AdamW and two simplistic LO variants (naive, curriculum), which is insufficient to demonstrate that ELO is a competitive, state-of-the-art approach to unroll generalization.\n\n3. The method's stability seems heavily tied to the quality of the expert (Adam). This reliance is problematic, as the authors admit (Sec 6) that using \"more advanced\" experts like AdamW was \"challenging\" and \"did not yet observe improvements.\" Furthermore, using an expert optimizer for behavior cloning is not a new contribution in itself, such as VeLO use the value output of hand-crafted optimizer as part of the input for guidance, making the novelty of this component limited.\n\n4. The replay buffer is central to the \"Efficient\" claim, yet it seems underdeveloped. In Sec 4.1, the default size is set to just 4. This is an exceptionally small number for a buffer and is not well-justified. The impact of buffer size and sampling strategy is not explored.\n\n5. The paper placeholder \"A APPENDIX - To be added!\" is a major omission."}, "questions": {"value": "see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nafq7UcKNv", "forum": "xjoVqCJ9G4", "replyto": "xjoVqCJ9G4", "signatures": ["ICLR.cc/2026/Conference/Submission4911/Reviewer_mhUx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4911/Reviewer_mhUx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761481218634, "cdate": 1761481218634, "tmdate": 1762917756766, "mdate": 1762917756766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELO, a novel learned optimizer (LO) meta-training algorithm that mitigates the out-of-distribution generalization issue of LOs when applied to optimize for a much longer horizon compared to the training distribution. The is claimed that ELO integrates behavior cloning and replay buffer. Experiments show ELO generalizes well on out-of-distribution optimization horizons compared to baselines, and consistently outperforms AdamW. A detailed ablation study is provided."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm proposed is novel and inspiring.\n\nThe central claim is well supported by the experiments."}, "weaknesses": {"value": "Need references to support the limitations of existing LOs and many other statements.\n\nThere should be space before citations.\n\nShould use \\citet for Chen in line 088-089\n\nLine 135-136 needs to spell rms out.\n\nShould make it clear that the parameters of the optimizee are randomly initialized in lines 158-160.\n\nLine 202-203 needs a citation or evidence.\n\nNeed to state what assumptions equation (7) requires.\n\nI believe the loss defined in equation (9) does not match the standard behavior cloning, which optimizes the difference between policy actions and expert actions, not the difference between states.\n\nOverall, the writing needs to be significantly improved.\n\nIf I'm correct, ELO-LO is meta-trained on unroll length beyond 1000 steps or even 10000 steps, although each meta-training iteration only unrolls 1000 steps. Therefore, the claim that Section 5.4 tests on unrolls beyond the meta-training horizon is wrong.\n\nSimilarly, I disagree that the memory buffer used in ELO is a replay buffer in the sense of RL."}, "questions": {"value": "Why is Figure 2 an example of unroll sampling?\n\nIs the \\alpha_t in equation (8) the same as that in equation (10)?\n\nIs there a relation between \\zeta and \\pi_\\zeta?\n\nHow is LoL-LO different from ELO-LO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4KpUoPFJp2", "forum": "xjoVqCJ9G4", "replyto": "xjoVqCJ9G4", "signatures": ["ICLR.cc/2026/Conference/Submission4911/Reviewer_P3hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4911/Reviewer_P3hF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616828595, "cdate": 1761616828595, "tmdate": 1762917755022, "mdate": 1762917755022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Learned optimizers (LOs) can outperform hand-designed ones but struggle to generalize over long unrolls. Simply extending unrolls is costly and unstable. The proposed Efficient Long-horizon Learning (ELO) uses a replay buffer and online behavior cloning to improve long-unroll stability and efficiency."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and significant challenge, ‘improving unroll generalization’ in the LO field. Solving this problem is a key bottleneck for making LOs practical for real-world tasks.\n\n2. The proposed ELO scheme is a novel combination of two complementary ideas.\n\n3. The replay buffer is a wise idea to recycle computation and expose the LO to longer effective horizons without the additional cost of back-propagation. The online Behavior Cloning (BC) component is a reasonable way to stabilize training."}, "weaknesses": {"value": "1. [Lack of Evidence on Efficiency]\n  The paper's central claims of ‘efficiency’ and operating ‘without adding extra meta-training cost’ are unsubstantiated and appear to be incorrect. \n  ELO requires calculating both the LO's update and the expert's (Adam) update at every inner step, plus the BC loss. This is a significant computational overhead compared to a naive LO. \n  The replay buffer in the paper must store four full copies of the model parameters (theta) and the expert's optimizer states. This means a massive VRAM overhead, combined with the need for the expert's optimizer states for the BC.\n  Also, the paper provides no empirical data (e.g., wall-clock time, peak VRAM usage) to support its efficiency claims against baselines.\n\n2. [Questionable Practical Value]\n  The empirical gains over the simple and strong AdamW baseline are quite marginal. Given the significant compute and memory overheads (from W1), the paper does not make a compelling case for why a user would use this complex method over AdamW for such a small benefit.\n\n3. [Unclear Contribution of Proposed Methods]\n  The paper's narrative focuses on the replay buffer as the main contribution. However, the ablation in Figure 7 shows that Behavior Cloning (LO-BC) is responsible for the vast majority of the performance gain. The step from LO-BC to ELO-LO (adding the buffer) provides very little extra benefit. This suggests the paper's main contribution is actually the online BC technique, which is framed as just a ‘stabilizer’.\n\n4. [Lack of Analysis on Expert (Adam) Choices]\n  The method's success depends on using Adam as the expert. The authors briefly state that AdamW was not ‘reliable’, but provide no analysis as to why. Why does the AdamW fail as an expert, while Adam succeeds?\n  It also creates an inconsistency by using Adam as a teacher but AdamW as the main baseline in Figure 3-5.\n\n5. [Limited Scale of Experiments]\n  All experiments use very small-scale models and low-resolution data. The authors acknowledge this limitation. It is completely unproven whether these results will scale to modern, large-scale models (e.g., LLMs) or hold for truly long-term (e.g., 1M) training."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6T712T1TVJ", "forum": "xjoVqCJ9G4", "replyto": "xjoVqCJ9G4", "signatures": ["ICLR.cc/2026/Conference/Submission4911/Reviewer_xJF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4911/Reviewer_xJF6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4911/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829039848, "cdate": 1761829039848, "tmdate": 1762917754660, "mdate": 1762917754660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}