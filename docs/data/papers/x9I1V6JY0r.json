{"id": "x9I1V6JY0r", "number": 17117, "cdate": 1758272414544, "mdate": 1759897195890, "content": {"title": "GenMark: An Embedded Watermarking Scheme for Generative Audio Synthesis", "abstract": "Audio watermarking provides an effective approach for tracing and protecting synthetic audio content. Traditional methods often apply watermarking as a post-processing step, which makes the watermark vulnerable to removal or degradation through signal processing or code editing. To address these issues, our paper introduces GenMark, a novel approach that embeds watermarks directly into the decoder of neural audio generation models during training. Our approach combines time-frequency perceptual losses, a mask-based localization model, and adversarial training to ensure high audio quality and watermark robustness. Experimental results on speech and music generation tasks demonstrate superior detection accuracy (TPR: 99.9\\% for speech, 100.0\\% for music). GenMark also preserves perceptual quality with less than 2\\% degradation in MUSHRA scores, establishing it as a strong candidate for practical and secure watermarking in generative audio systems.", "tldr": "GenMark is a training-time audio watermarking method that embeds watermarks directly into generative decoders, improving robustness against removal while preserving perceptual quality.", "keywords": ["Neural Audio Generation", "Audio Watermarking", "Robust Watermark Embedding", "Speech Synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65274eec132248aea66765ade95ac598ef2c1fee.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The submission proposes to root the watermark functionality inside the audio generative AI. To do so, the authors fine-tune the decoder so that the generator produces audio samples \"natively\" watermarked. This finetuning is driven by several losses ensuring robustness and imperceptibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The performance, in terms of robustness and quality, is very good. Robustness is measured against strong attacks (stronger than typical attacks in practice, I would say), and quality is gauged by metrics but also 20 audio experts.\n\n- The Figure 3 is key (unfortunately not outlined in the submission). It explains why the proposed watermarking scheme is so robust: the spectrum of the watermark signal mimics the spectrum of the generated audio. Therefore, stripping it out without damaging the audio is extremely difficult."}, "weaknesses": {"value": "### W1 - Bibliography\n\nThe problem with the modern literature on watermarking is that all papers look the same. The embedder and decoder are NN trained under a bunch of losses. Here, 6 losses are defined. Are they all necessary? are they sufficient?\n\nAlthough not cited, the approach transposes the Stable Signature scheme to audio GenAI, ie. rooting the watermark functionality in the last  brick of the GenAI, ie. the decoder. One difference, though, is that here the decoder is fine-tuned jointly while the watermark decoder is trained. Also, while not cited in Sect. 3.4, the masking strategy and the localization borrow a lot to AudioSeal.\n\nNow, there are other missing references which targets the same objective but with different approaches:\n- GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis, Liu et al., ACM MM 2024.\nThis paper follows the same objective but by crafting the latent vector seeding the diffusion model. It has the advantage of embedding any message without retrainaing.\n- Latent Watermarking of Audio Generative Models, San Roman et al., ICASSP 2025. They learn watermarked tokens robust to the Encodec quantization.\n\nThis latter reference is essential as the authors outline that it is easy to replace/fine-tune/learn the GenAI decoder (See Sect. V.A). Therefore, rooting the watermarking inside this decoder might not be very secure.\n\n### W2 -  Approximations\n- Line 39: post-hoc watermarking is fine, except when the audio GenAI is open-sourced. This paragraph should make this point clear.\n- Line 63: FAD and KLD do not measure perceptual distortion, but distributional distortion.\n- Eq (1) does not show any moving average, contrary to algo. 4 in appendix. Index $i$ is missing in the definition of $g_i$\n- Line 132: Spread-spectrum is not a transform domain. It is an embedding technique. References are missing in Section 2.3\n- Line 139: What is \"*gradient steganography*\"? A reference to Stable Signature is missing in this paragraph.\n- Maks -> Mask (several times in the submission).\n- The watermark detector gives 16 decoded bits ($m_t$) + 1 score ($y_t$)  for pure detection. Why 18 outputs?\n- Line 267: While I love the reference to Kirovski & Malvar, I do not understand the sentence. Which transformation? What enhancement?\n\n### W3 - Benchmark\n- It is not clear whether the proposed scheme has one unique or two versions (one for speech, one for music). In any case, AudioSeal was dedicated to speech, so it is kind of unfair to benchmark it on music. It is easy to fine-tune AudioSeal for music.\n- What is the Detection Error Rate exactly? Is this based on $\\hat{m}_t\\neq m_t$ or on $\\hat{y}_t\\neq y_t$?\n- The original paper of EncoDec uses OVL and REL to measure the quality of the generated audio. Why do you use different metrics?"}, "questions": {"value": "See all the questions above, plus:\n- What is the duration of a time frame? This indicates the granularity of the localization.\n- Table 3 shows a \"white-box\" and \"black-box\" column without explaining these attacks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a7HYUuBgEb", "forum": "x9I1V6JY0r", "replyto": "x9I1V6JY0r", "signatures": ["ICLR.cc/2026/Conference/Submission17117/Reviewer_Zcr4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17117/Reviewer_Zcr4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761066953307, "cdate": 1761066953307, "tmdate": 1762927117575, "mdate": 1762927117575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GenMark, a new way to watermark generative audio models. Instead of adding a watermark after the audio is created (which can be easily skipped), GenMark embeds the watermark directly into the model's decoder during training. This means any audio the model produces is inherently watermarked. It uses a combination of perceptual losses, adversarial training, and a special \"Mask Model\" to make the watermark imperceptible but extremely robust to attacks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is its novel formulation of the audio watermarking problem. By embedding the watermark directly into the generative model's decoder (\"in-process injection\"), Genmark represents a shift toward a more robust security paradigm for generative audio."}, "weaknesses": {"value": "I feel the work is not ready given the significant performance drop when facing high-pass filter and encodec. The author's explanation that \"The drop under Highpass is expected: to preserve perceptual quality we intentionally concentrate watermark energy in midâ€“low frequencies, so aggressive high-pass filtering removes a larger fraction of the embedded signal.\" does not make sense to me. This is basically saying the proposed method can not achieve good perpectuality and robustness simultaneously."}, "questions": {"value": "Please provide a solution for the high decoding error rate under high-pass filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oEqfPwrScP", "forum": "x9I1V6JY0r", "replyto": "x9I1V6JY0r", "signatures": ["ICLR.cc/2026/Conference/Submission17117/Reviewer_YE7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17117/Reviewer_YE7S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805019721, "cdate": 1761805019721, "tmdate": 1762927117280, "mdate": 1762927117280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a watermark scheme that directly embeds watermark at the decoding stage of audio generation. Several losses are considered and automatically balanced by their scale of gradient. These losses are selected from previous works and have been proved their usefulness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "GenMark works on both speech and music, and have been demonstrated on 2 models (Bark, MusicGen) with consistent performance. The design of the training pipeline incorporated great designs such as the hybrid loss with auto balance, and the mask model for augmentation and robustness. It's relatively lightweight because there's no need to fine-tune the generative model itself, just the decoding part of audio tokenizer. The manuscript is easy to follow, and many useful details can be found in appendices."}, "weaknesses": {"value": "Although the problem setup of watermarking generated content is well-motivated, it's still unclear about why in-process watermark is harder to be attacked. For example, post-processing module can also be encapsulated within the model and being released as a black-box system. In contrast, if there's no proper encapsulation, attacker can figure out the structure and still replace \\hat{C} with C.\n\nThe watermark message of GenMark is fixed for every audio example (ref. Algorithm 1, Appendix B). However, most the conventional watermarks (WavMark, AudioSeal, SilentCipher) allow free binding of watermark messages at inference time. Considering GenMark is a scheme for watermarking generated content, this could be a feasible setup. However, it is still a huge drawback for actual application. Also, it's hard to guarantee \\hat{C} will work properly for every unseen generated content at inference time.\n\nThere are also some issues regarding the evaluation result. In table 1, according to Appendix E, the CLAP score is based on cosine similarity, it's impossible to have values larger than 1. Moreover, it has been pointed out by several papers (see below) that, FAD based on Vggish (the standard FAD) is not a good indicator for perceptual quality. FAD_clap, KAD, or other MMD-based metrics can be better indicators of perceptual quality (still, none of them are guaranteed to correlate with human perception).\nhttps://arxiv.org/abs/2311.01616\nhttps://arxiv.org/abs/2502.15602\n\nAbout robustness evaluation, most of the parameter selections are reasonable, but the parameter selection on lowpass (cf 500hz) and high pass (cf 1500hz) seem to be too aggressive. Under such condition, neither speech nor audio retain minimal intelligibility. Robustness should be tested (and only meaningful) such that the audio content is still retaining minimum acceptable quality. In addition, it's a bit concerning that AudioSeal has a very high DER under EnCodec attack. However, EnCodec is a part of AudioSeal and at least it should be more robust than WavMark. Please see also the original AudioSeal paper and a paper from another group of researchers (https://arxiv.org/pdf/2401.17264, https://arxiv.org/pdf/2505.19663)\n\nMinor issues:\n- L181: \"Robustness Enhancement Module module.\" looks to be unnatural.\n- L392: \"Use\" -> \"Usability\" ?"}, "questions": {"value": "- Figure 1: There's no injection point of the 16-bit watermark message, I guess it should be injected into \\hat{C} directly?\n- L184: Why 2 bits are required for existence detection (D_det(\\hat{w}))? How do these bits aggregated to the binary result (0~3 -> Yes/No)?\n- L196: Considering Eq.3 is a hybrid L1-L2 loss and the explanation at L209, why only L1 loss is included in Eq.2? \n- L226: Is \"G\" referring to \\hat{C} in Figure 1 ?\n- L258-L259: When a watermarked segment is replaced, how does the segment with another watermark is created? Are there multiple SampleMsg functions?\n- L303: Why SilentCipher is not included in MUSHRA test?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2UqamXX5a5", "forum": "x9I1V6JY0r", "replyto": "x9I1V6JY0r", "signatures": ["ICLR.cc/2026/Conference/Submission17117/Reviewer_ij4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17117/Reviewer_ij4E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840329242, "cdate": 1761840329242, "tmdate": 1762927116968, "mdate": 1762927116968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GenMark, an in-process watermarking scheme for generative audio models. The method involves fine-tuning the model's decoder to embed a watermark signal directly into the generated audio output. The training process is optimized with a combination of time-frequency perceptual losses, an adversarial loss, and a mask-based localization model to ensure both high audio fidelity and robust watermark detection. It is compared to and beats other strong baslines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1. Domain: The paper addresses the important but under-explored domain of in-model watermarking for generative audio synthesis, providing a more secure alternative to post-processing methods.\n\nS2. Clarity: The paper is very well-written, and the methodology is explained clearly.\n\nS3. Thorough evaluation: The experimental validation is comprehensive. It includes comparisons against strong, state-of-the-art audio watermarking baselines (AudioSeal, WavMark, SilentCipher), a wide range of objective metrics (FAD, KLD, CLAP, SI-SNR, VISQOL) , an extensive robustness evaluation against 12 different transformations and a subjective MUSHRA user study.\n\nS4. Strong results: The method achieves excellent results, demonstrating near-perfect detection (99.9% TPR for Bark, 100.0% for MusicGen), superior robustness (lowest average error rate across 12 attacks), and high perceptual quality (a MUSHRA score of less than 2% degradation from the clean reference)."}, "weaknesses": {"value": "W1. Missing citations and credit: The paper fails to properly credit prior work that established its core methodology. The central idea: fine-tuning a generative model's decoder using a combination of a message-decoding loss and a perceptual/adversarial loss against the original frozen decoder, is identical to Stable Signature (Fernandez et al., 2023) method from the image domain. Furthermore, the pipeline's use of detection logits and a masking model is taken directly from AudioSeal (San Roman et al. 2024), which should be acknowledged as a primary methodological source, not just a baseline for comparison.\n\nW2. Robustness to fine-tuning: The method's security relies on the fine-tuned decoder. In practice, a common attack vector would be to simply re-fine-tune this decoder, either benignly on a new dataset (which happens a lot in audio with the change of vocoders) or adversarially to remove the watermark. This significant threat is not evaluated."}, "questions": {"value": "Q1 (W2): How robust is the GenMark watermark to removal attacks via fine-tuning? For instance, if a user takes the GenMark-watermarked decoder and fine-tunes it (e.g., on a new speech dataset), does the watermark survive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A4iKa24CdI", "forum": "x9I1V6JY0r", "replyto": "x9I1V6JY0r", "signatures": ["ICLR.cc/2026/Conference/Submission17117/Reviewer_tMam"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17117/Reviewer_tMam"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17117/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939166044, "cdate": 1761939166044, "tmdate": 1762927116653, "mdate": 1762927116653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}