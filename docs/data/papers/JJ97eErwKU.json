{"id": "JJ97eErwKU", "number": 18256, "cdate": 1758285689102, "mdate": 1759897116127, "content": {"title": "Thinking with Time Series: Interleaved Deep Thinking for Enhanced Time Series Reasoning", "abstract": "Understanding and reasoning with time series is an important yet unsolved challenge for multimodal large language models (MLLMs). Current time series MLLMs (TS-MLLMs) often struggle with complex tasks due to their overly simplified reasoning process. In this work, we argue that deep thinking is essential for comprehensively understanding and effectively reasoning over time series. We present ThinkTime, the first TS-MLLM that supports Interleaved Time series Chain-of-Thought (iTCoT) with integrated tool calls. In iTCoT, the reasoning process is interleaved with tool calls, allowing the model to dynamically incorporate information from time series slices into its thought process. To enable comprehensive analysis, the model introduces two fundamental operations, slice and compare, which are designed to observe detailed and correlation features. To achieve this, we design a two-stage training process and propose a task-specific training data construction method based on synthetic data. In the supervised fine-tuning stage, we use an iTCoT dataset to teach the model how to integrate tool responses with reasoning processes. Then, in the reinforcement learning stage, we implement an RL training framework for TS-MLLMs that supports iTCoT, improving the model's reasoning and tool-use abilities. Experiments conducted on a wide range of real-world time series demonstrate that ThinkTime achieves substantial improvements in reasoning tasks while maintaining high alignment between time series and text descriptions.", "tldr": "This work introduces ThinkTime, the first time series multimodal LLM with interleaved CoT and tool calls, trained via a two-stage pipeline, which significantly improves time series reasoning while preserving alignment between time series and text.", "keywords": ["Time Series", "Multimodal Large Language Models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb1790f1b40817ea833a24fdf26854d1847c224b.pdf", "supplementary_material": "/attachment/ea29ee472786440b60cfe6bdb5decafb2873a47f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces ThinkTime, a framework for time-series reasoning via interleaved Chain-of-Thought (iTCoT) using simple slice and compare tools. Trained on synthetic ChatTS data with RLVR, it shows structured reasoning but limited real deep thinking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are well-organized and cover multiple benchmarks, showing consistent improvements over baseline time-series LLMs. The evaluation includes both alignment and reasoning tasks, demonstrating that the proposed iTCoT approach."}, "weaknesses": {"value": "1. The tool design is too simple\nOnly two operations, `slice` and `compare`, are provided, which makes it difficult to capture complex relationships. There is a lack of deeper analytical tool design, and I think there is a large gap between this and what is claimed as *DEEP THINKING*.\n\n2. The reasoning is more like “description”\nThe model mostly observes the figures and gives language summaries, rather than performing real statistical or causal reasoning.\nFurthermore, in the DAPO training there is no supervision information for complex reasoning chains, and most prompts are just template-like instructions.\nThe RLVR reward function also only considers format, accuracy, and length, without taking reasoning quality or the rationality of tool usage into account.\n\n3. The training data come from ChatTS\nAlthough an additional CoT part was added, after checking Appendix D, the process is overly regularized and lacks diversity of real thinking. Most are rather general instructions. I doubt whether the current model can truly follow these instructions, or is merely producing hallucinatory outputs."}, "questions": {"value": "Please see the weaknesses discussed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TVD0HdHf1G", "forum": "JJ97eErwKU", "replyto": "JJ97eErwKU", "signatures": ["ICLR.cc/2026/Conference/Submission18256/Reviewer_it4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18256/Reviewer_it4P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668297818, "cdate": 1761668297818, "tmdate": 1762927982182, "mdate": 1762927982182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ThinkTime, a time-series multimodal LLM (TS-MLLM) that introduces interleaved Time-series Chain-of-Thought (iTCoT) with two tool operations—slice and compare—to enable deep, step-wise reasoning over multivariate time series. The model uses a two-stage training pipeline: Warm-Up SFT on iTCoT alignment data (largely synthetic) and RL with verifiable rewards (RLVR) using DAPO/TRL to improve tool use and reasoning trajectories. The authors curate evaluation suites spanning five reasoning categories (pattern, numerical, calculation, causal, comparison) and six alignment subtasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a clear, original step toward time-series deep reasoning by operationalizing interleaved CoT with slice/compare tools and by delivering a complete training recipe. The empirical coverage across reasoning and alignment tasks is broad and convincing, with informative ablations and robustness analyses. ThinkTime shows substantial, consistent improvements over strong baselines and retains good alignment, suggesting the approach is both effective and practical."}, "weaknesses": {"value": "1. The slice and compare operators are intuitive but remain informally defined.\n\n2. there is no analysis of when/why iTCoT should reduce reasoning error vs. text-only CoT, nor bounds on over-slicing or mis-alignment risks in multivariate settings, consider add sensitivity analyses to slicing granularity and normalization choices.\n\n3. Warm-Up SFT and much of RLVR rely on synthetic data. While results on real benchmarks are positive, the work lacks systematic transfer diagnostics, e.g., feature distribution distances, task-wise error taxonomy, and negative cases where synthetic priors mislead real-world reasoning. Provide dataset shift measurements and per-category failure analyses.\n\n4. Little analysis of failure modes, such as false causal attributions, period misestimation under nonstationarity. Given iTCoT’s autonomy to call tools, a section on unsafe or privacy-sensitive tool uses would be valuable. Please add fine-grained error taxonomy and safeguards.\n\n5. The approach focuses on regular numeric time series; extensions to event and irregular sampling, missingness, or exogenous interventions are mentioned implicitly via tools but not validated. Please consider add tests on missing data and irregular timestamps."}, "questions": {"value": "Please see the above weaknesses, and the following:\n\n1. How do you impose or learn a tool-call budget to prevent over-slicing? Is there a learned stopping policy or hard cap per turn/task?\n\n2. Do you quantify distribution shift between synthetic and real data?\n\n3. Beyond series length/number, have you tested robustness under noise, distribution shifts, or missingness?\n\n4. The “causal” category is intriguing. Are these counterfactual or correlational diagnostics? What guarantees exist against spurious causal claims? \n\n5. Please add ablations on reward weights, introduce penalties for degenerate tool use, and report compute and latency-accuracy trade-offs. \n\n6. Please share ablations for $w_{\\mathrm{acc}},\\, w_{\\mathrm{format}},\\, w_{\\mathrm{len}}$ and whether an efficiency/latency term or redundancy penalty improves behavior."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VTAmoKfS1n", "forum": "JJ97eErwKU", "replyto": "JJ97eErwKU", "signatures": ["ICLR.cc/2026/Conference/Submission18256/Reviewer_L55B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18256/Reviewer_L55B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749455697, "cdate": 1761749455697, "tmdate": 1762927981831, "mdate": 1762927981831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ThinkTime, a multimodal large language model (TS-MLLM) for time-series reasoning that introduces Interleaved Time-series Chain-of-Thought (iTCoT) — a mechanism where reasoning steps are alternated with external “tool calls” such as slice and compare. Inspired by “Thinking with Images” (OpenAI, 2025), the model allows dynamic inspection of local temporal regions during reasoning. The training pipeline consists of two stages: (1) warm-up supervised fine-tuning (SFT) with synthetic iTCoT data, and (2) reinforcement learning with verifiable rewards (RLVR) under the DAPO framework. Experiments on 11 datasets show large gains over text, vision, agent, and previous TS-MLLM baselines (e.g., ChatTS-14B)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel conceptual extension: Introducing interleaved tool-based CoT for time-series reasoning is original and clearly differentiates from prior TS-MLLMs that only employ static CoT or fixed-window representations.\n\n2. Well-structured system design: The paper provides an end-to-end pipeline (data construction → SFT → RL → evaluation) with a clear operational flow.\n\n3. Comprehensive evaluation: Results include both reasoning and alignment benchmarks, with ablations on tool usage, RL, and workflow variants.\n\n4. Readable presentation: Figures (1–4) and examples of tool calls make the paradigm understandable and reproducible."}, "weaknesses": {"value": "1. The core assumption—that a reasoning paradigm proven effective for images (cropped regions and visual focus) directly generalizes to time-series—is not theoretically supported.\n\n2. The contribution is largely an architectural composition (existing LLM + tool-call loop + RL). No new algorithmic component (e.g., reward shaping, CoT optimization, or reasoning trace modeling) is introduced. The reinforcement learning setup merely adapts DAPO with task-specific rewards but lacks theoretical or empirical insights into why RL improves reasoning in the temporal domain.\n\n3. The experimental analysis is insufficient to substantiate the paper’s core claims. There is no ablation isolating the effects of the key slice and compare operations, baseline comparisons are potentially unfair due to inconsistent fine-tuning settings, and the tool-use evaluation lacks quantitative interpretability (e.g., precision, redundancy, or effectiveness). As a result, the reported performance gains cannot be confidently attributed to the proposed iTCoT mechanism."}, "questions": {"value": "1. Logical Coherence and Transitions\nThe transition from the problem statement (“TS-MLLMs struggle with complex tasks”) to the core claim (“deep thinking is essential”) feels abrupt. The authors should explicitly articulate why “deep thinking” is necessary for time-series reasoning—e.g., because it allows iterative observation and temporal abstraction that single-pass reasoning cannot achieve.\n2. Method and Contribution Clarity\nThe abstract presents too many methodological details at once, making it difficult to distinguish the core innovation. The authors should streamline the description, ensure consistent terminology, and clearly highlight the main contributions beyond prior TS-MLLMs.\n3. Cross-Modal Generalization Assumption\nThe assumption that methods effective for image reasoning will directly apply to time-series reasoning lacks justification. The authors should clarify what aspects are transferable and provide supporting evidence.\n4. Experimental Validation\nHow can the authors ensure that the reported improvements genuinely stem from the proposed iTCoT mechanism?\nThere is no ablation isolating the roles of slice and compare operations.\nThe fairness of baseline comparisons (fine-tuned vs. zero-shot models) is unclear.\nThe quantitative effectiveness of tool use (e.g., precision, redundancy, or correlation with accuracy) is not analyzed.\nProviding these analyses or additional experiments could significantly strengthen the credibility of the results.\n5. Minor Issue \nThe paper exhibits inconsistencies in capitalization and formatting, such as mixed usage of “Interleaved Time series Chain-of-Thought” vs. “Interleaved Time-Series Chain-of-Thought,” inconsistent reference styles (e.g., “OpenAI, c.” vs. “OpenAI (b)”), occasional mismatches between figure numbers and citations, and irregular quotation marks in JSON examples. These should be standardized for clarity and professionalism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "svyV825YJN", "forum": "JJ97eErwKU", "replyto": "JJ97eErwKU", "signatures": ["ICLR.cc/2026/Conference/Submission18256/Reviewer_BP6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18256/Reviewer_BP6D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797154965, "cdate": 1761797154965, "tmdate": 1762927981504, "mdate": 1762927981504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper introduces ThinkTime, a time series multimodal large language model (TS-MLLM) developed to address the challenges of complex reasoning tasks. The model incorporates a method called Interleaved Time series Chain-of-Thought (iTCoT), which allows the reasoning process to be combined with tool calls.\n\nThe framework uses two main operations, slice and compare, to let the model examine specific segments of a time series and analyze correlations between different parts. ThinkTime is developed through a two-stage training process that begins with Supervised Fine-Tuning (SFT) to learn the iTCoT process, followed by Reinforcement Learning (RL) to enhance its reasoning and tool-use capabilities. The authors report that this approach leads to noticeable improvements in the model's performance on various reasoning and alignment tasks involving real-world time series data"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* proposed a methodology that can use integrated \"tools\" that explore specific components  and comparisons between time-series. This \"tool\" based method is novel for the time-series space, and has the potential to more deeply understand and probe a given time-series. Due to this, a DAPO approach for agent RL learning is also new and interesting for this time-series MLLM space. \n* code is publicly available upon submission"}, "weaknesses": {"value": "* the core focus of this paper is on \"slice\" and \"compare\" operations and yet, in my opinion, the paper does not sufficiently explain why these two operations are fundamental for reasoning for time-series. I agree that they are important, yes, but the only two things that are necessary? I am not sure. The ablations also do not break down the tool use between the two. the paper would benefit from further discussion as to why these two exact tools ideas are used.\n* Further explanation on the agent based models are needed in order to understand what capabilities they have available to them to make sure this is a fair comparison, especially with them having significantly worse performance than the TS methods. \n* A key contribution is \"propose a comprehensive data pipeline to support iTCoT\" but it seems that most of these details are relegated to the appendix, so I am struggling to understand the core technical contribution of this, other than specific prompting techniques.\n* I am unclear on the evaluation. How the evaluation datasets are constructed is also relegated into the appendix, so it is difficult to understand how they evaluate reasoning + alignment specifically. Even after I look into the appendix \"Based on this, we used the LLM to construct a series of reasoning questions and answers. We manually verified the correctness of each question to ensure\nthe quality of the evaluation data. \" -> this is very unclear as to how exactly reasoning is tested among other ambiguity.\n* workflow-based model is suddenly introduced in the ablation study, and it is not clearly explained, so it is difficult to contextualize the comparison and understand exactly how ICoT improves upon it."}, "questions": {"value": "* When constructing the RLVR data and LLM-as-a-judge is used to verify QA quality. How do we know that the LLM-as-a-judge was sufficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WgF2VzeQIJ", "forum": "JJ97eErwKU", "replyto": "JJ97eErwKU", "signatures": ["ICLR.cc/2026/Conference/Submission18256/Reviewer_aYSq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18256/Reviewer_aYSq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889167613, "cdate": 1761889167613, "tmdate": 1762927981084, "mdate": 1762927981084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}