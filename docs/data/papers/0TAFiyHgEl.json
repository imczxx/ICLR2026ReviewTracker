{"id": "0TAFiyHgEl", "number": 9571, "cdate": 1758128212482, "mdate": 1763744982870, "content": {"title": "Unlocking the Value of Text: Event-Driven Reasoning and Multi-Level Alignment for Time Series Forecasting", "abstract": "Existing time series forecasting methods primarily rely on the numerical data itself. However, real-world time series exhibit complex patterns associated with multimodal information, making them difficult to predict with numerical data alone. While several multimodal time series forecasting methods have emerged, they either utilize text with limited supplementary information or focus merely on representation extraction, extracting minimal textual information for forecasting. To unlock the Value of Text, we propose VoT, a method with Event-driven Reasoning and Multi-level Alignment. Event-driven Reasoning combines the rich information in exogenous text with the powerful reasoning capabilities of LLMs for time series forecasting. To guide the LLMs in effective reasoning, we propose the Historical In-context Learning that retrieves and applies historical examples as in-context guidance. To maximize the utilization of text, we propose Multi-level Alignment. At the representation level, we utilize the Endogenous Text Alignment to integrate the endogenous text information with the time series. At the prediction level, we design the Adaptive Frequency Fusion to fuse the frequency components of event-driven prediction and numerical prediction to achieve complementary advantages. Experiments on real-world datasets across 10 domains demonstrate significant improvements over existing methods, validating the effectiveness of our approach in the utilization of text. The code is made available at https://anonymous.4open.science/r/VoT-465C.", "tldr": "", "keywords": ["time series forecasting", "multimodal"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f4bca33b9aa46a007f57fbbfa5304c48e6a5b16.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents VoT, a dual-branch framework for multimodal time-series forecasting. One branch performs event-driven reasoning over exogenous text through a three-stage pipeline enhanced by Historical In-Context Learning (HIC). The second branch conducts Endogenous Text Alignment (ETA) by converting statistics of the target series into textual descriptors and aligning them with numeric representations via contrastive learning. The two branches are combined using Adaptive Frequency Fusion (AFF), which learns frequency-band weights so that low-frequency, event-level effects and high-frequency fluctuations are fused coherently. Experiments on multiple datasets report consistent gains over strong baselines, with ablations attributing benefits to HIC, ETA, and AFF."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Separating event-reasoning over exogenous text from numerical modelling, then fusing by learned frequency bands, is well motivated and clearly presented.\n- The HIC retrieval strategy is a reasonable way to reuse corrected reasoning traces without fine-tuning.\n- Strong empirical results span numerous datasets and include ablations that remove or modify key components."}, "weaknesses": {"value": "- The generative pipeline plus HIC retrieval likely increases latency and cost; wall-clock measurements and throughput are not reported.\n- The evaluation omits parameter-matched alternatives such as a gated residual or a lightweight cross-attention/FiLM plug-in, making it difficult to isolate architectural merit.\n- Split protocols are not described in sufficient detail to exclude future-revealing or outcome-summarising language at time t.\n- The stability of learned frequency weights under distribution shift and the sensitivity to band partitioning remain unclear."}, "questions": {"value": "- What are wall-clock training and inference costs for the event pipeline and HIC retrieval, and how do they scale with text volume?\n- How does VoT compare with (a) a small gated residual that can down-weight text channels and (b) a light cross-attention/FiLM block with similar parameter counts?\n- How sensitive are results to the choice of frequency bands, and do learned fusion weights transfer across datasets or shifts?\n- What is the behaviour when exogenous text is noisy, or sparse, and is there a fallback to ETA-only or TS-only prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VbDbsXu0nJ", "forum": "0TAFiyHgEl", "replyto": "0TAFiyHgEl", "signatures": ["ICLR.cc/2026/Conference/Submission9571/Reviewer_UX4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9571/Reviewer_UX4Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654251390, "cdate": 1761654251390, "tmdate": 1762921125352, "mdate": 1762921125352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VoT (Unlocking the Value of Text), a framework that injects real-world textual context (e.g., news, reports, policies) into time-series forecasting through event-driven reasoning and multi-level alignment. VoT runs two prediction branches in parallel: (1) an event-driven LLM branch that converts external text into numeric forecasts via a three-stage pipeline (template → summary → reasoning) and a Historical In-Context (HIC) mechanism that retrieves past correction examples to reduce recurring errors; and (2) a numerical branch that aligns time-series embeddings with endogenous text describing trend/seasonality using ETA (representation-level alignment). Final predictions are fused in the frequency domain with AFF, which learns band-specific weights (low/mid/high) to exploit each branch’s strengths.\nContributions\n- A principled event-to-forecast pipeline that turns text reasoning into numeric predictions, regularized by HIC.\n- ETA: fine-grained representation alignment between TS embeddings and textual descriptions (trend/seasonality).\n- AFF: prediction-level alignment via frequency-band fusion of event and numerical branches.\n- Extensive benchmarks (10 domains) showing consistent gains over strong time-series and multimodal baselines, especially for event-sensitive regimes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Proposes a dual-branch framework that combines Event-driven Reasoning from exogenous text with Multi-level Alignment (ETA for representation-level, AFF for prediction-level). The Historical In-Context Learning (HIC) idea—retrieving corrected past reasoning to guide current predictions—is a creative reuse of prior errors as in-context examples."}, "weaknesses": {"value": "- The endogenous text appears to restate information already present in the series; the paper does not clearly justify why ETA (textualizing trend/seasonal components and aligning them) adds value beyond standard time–series decomposition and representation learning.\n-  The computation of $Y_{\\text{num}}$ is under-specified. It is unclear whether the forward pass at inference consumes only $H_{\\text{ts}}$ or also the aligned textual features (e.g., $Z_{\\mathrm{tr}}, Z_{\\mathrm{se}}$).\n- The construction and maintenance of the Knowledge Base $K$ are not operationalized: if an entry is stored per training window, $|K|$ may become very large. The paper does not detail pruning, refresh policies, or retrieval complexity/latency."}, "questions": {"value": "- Could you add a baseline that performs ETA via simple TS–Text contrastive learning without trend/seasonal decomposition? Showing a consistent gap in favor of your decomposition-based ETA would strengthen the core claim.\n- Why text-driven decomposition? Compared to standard time-series-based TS decomposition [1,2] applied to (H_{\\text{ts}}), what concrete advantage does using endogenous text with TS–Text attention to produce (TS_Text_trend) and (TS_Text_seasonal) provide? Please quantify where text-guided components outperform classic decompositions.\n- In Figure 6, the differences between the TS-only (pink) and event-driven (green) branches are not very clear across high- and low-frequency ranges. To make their roles more distinct, it might help to report a quantitative evaluation or show a case with a pronounced regime shift where the two branches diverge more visibly.\n\n[1] Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting, NeurIPS21.\n[2] Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling, ICLR24."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZG6kc1QYz9", "forum": "0TAFiyHgEl", "replyto": "0TAFiyHgEl", "signatures": ["ICLR.cc/2026/Conference/Submission9571/Reviewer_jn6c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9571/Reviewer_jn6c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894912188, "cdate": 1761894912188, "tmdate": 1762921124597, "mdate": 1762921124597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VoT (Value of Text), a multimodal time-series forecasting framework that integrates exogenous and endogenous text using a dual-branch architecture. The event-driven branch performs generative reasoning over textual events via Historical In-Context Learning (HIC), while the numerical branch aligns representations through Endogenous Text Alignment (ETA) and fuses predictions with Adaptive Frequency Fusion (AFF)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation that textual signals carry event-driven information complementary to numerical dynamics.\n- Technically coherent pipeline combining LLM-based reasoning and representation alignment."}, "weaknesses": {"value": "- The Event-Driven Reasoning and Historical In-Context Learning (HIC) components are interesting, but it remains somewhat unclear how much of the improvement truly comes from the reasoning process itself. Could the authors clarify whether HIC’s benefit persists when compared to simpler retrieval or embedding-based baselines? Also, since the reasoning pipeline involves curated summaries and correction prompts, how robust is it under domain shift or when applied without instruction-tuned LLMs?\n- The averaged results suggest strong overall performance, but they do not reveal when text contributes most. Have the authors examined per-event or per-regime behavior—for instance, during shocks, sudden changes, or stable intervals? Such an analysis might clarify whether the textual reasoning module adapts dynamically to event intensity or merely improves overall fit.\n- The Endogenous Text Alignment (ETA) and Adaptive Frequency Fusion (AFF) appear to rely on manually chosen decompositions (trend/seasonal, low/mid/high frequency). Are these partitions fixed, or are they optimized jointly with the forecasting backbone? If they are heuristic, have the authors tested sensitivity to these design choices or considered end-to-end learning variants?\n- Given that the system depends on multiple LLMs and retrieval modules, can the authors discuss its computational footprint and inference latency? Is the framework feasible for continuous or real-time forecasting, or is it primarily an offline research prototype?"}, "questions": {"value": "- Can the authors provide a breakdown of performance during event vs. non-event intervals to demonstrate that “event-driven reasoning” indeed contributes selectively?\n- How does HIC perform when no close historical correction exists (e.g., unseen event types)? Is retrieval robust to domain shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fDAfguZpgj", "forum": "0TAFiyHgEl", "replyto": "0TAFiyHgEl", "signatures": ["ICLR.cc/2026/Conference/Submission9571/Reviewer_Se43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9571/Reviewer_Se43"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981772756, "cdate": 1761981772756, "tmdate": 1762921124347, "mdate": 1762921124347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VoT (Value of Text) a multimodal model to process text + numerical data for TSF. It uses an event-driven prediction branch and a numerical prediction branch to obtain branch wise predictions. event-driven prediction branch processes exogenous text data and uses retrieval to get relevant time-series data for historical in context learning (HIC). The numerical prediction branch aligns the time series with endogenous text that describes the time series. These branch wise predictions are then separated by 3 frequency bands (low, middle and high). The adaptive frequency fusion module (AFF) performs a linear combination at each band to then obtain to obtain the final forecast of the time series. The authors perform ablations of each module as well as perform experiments across 10 domains (from the MM-TSFLib benchmark + a new weather dataset) and 8 baselines to show superiority of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. I like that the authors performed ablation on every module (ETA, AFF, HIC, Event branch), and appreciate the authors giving some insights rather than just presenting numbers. For instance, in section 4.3.1, the authors note that \"removing HIC results in worse performance than removing the entire event-driven branch (w/o Event), suggesting that unguided LLM reasoning can be more detrimental than no reasoning at all.\", which I think is a useful insight.\n\n2. The pipeline of using LLMs to process exogenous text data to create summaries and time series, correcting the reasoning process to keep the correct samples and using a retrieval mechanism for historical in context learning seems a bit over-engineered. However, this is still an interesting idea, is novel (to the best of my knowledge), and helps improve performance.\n\n3. The model achieves SOTA performance over the unimodal and multimodal baselines on all tasks."}, "weaknesses": {"value": "1. The main weakness of the paper stems from the risk of temporal leakage in HIC module: It is crucial that the knowledge base never indexes summaries/corrections from future windows relative to the test horizon. The paper should spell out strict time-based splits for KB construction and retrieval, but HIC description lacks an explicit leakage guarantee.\n\n2. The final prediction of the model is a linear combination of band-specific frequency components $\\mathcal{F}^b_{*}$ between the exogenous (event-drive) and endogenous (numerical) branches. Why is this so? It seems quite reasonable to believe that the representation of these branches would have complex non-linear interactions. The paper would benefit from an ablation that models the final prediction as a nonlinear function instead (for example, MLP or cross attention).\n\n\nMinor: The authors have provided a link to their code, but most files are not viewable (I get an error: \"The requested file is not found\")."}, "questions": {"value": "- How do the authors ensure in equation 7 that $E^{tr}$ and  $E^{se}$ extract trend and seasonality? What is enforcing this behaviour (the loss, prompt or something else)? This should be made clear in section 3.4.1. To me it does not seem like this is being enforced, in which case $Q^{tr}$ and $Q^{se}$ are essentially different query heads, capturing different information?\n\n- Table 6 is quite large and bolding numbers would be beneficial to the reader.\n\n - Exactly how is the HIC knowledge base constructed per split? During inference at time t, are you guaranteed to retrieve only corrections derived from training windows strictly before t? \n\n- How are the initial band boundaries chosen per dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "suaVJ1uk7m", "forum": "0TAFiyHgEl", "replyto": "0TAFiyHgEl", "signatures": ["ICLR.cc/2026/Conference/Submission9571/Reviewer_Wk2i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9571/Reviewer_Wk2i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119888597, "cdate": 1762119888597, "tmdate": 1762921124012, "mdate": 1762921124012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}