{"id": "n5dkjiplv4", "number": 7569, "cdate": 1758027979134, "mdate": 1759897845841, "content": {"title": "Reinforcement Learning with Inverse Rewards for World Model Post-training", "abstract": "World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains underexplored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose **Reinforcement Learning with Inverse Rewards (RLIR)**, a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5–10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.", "tldr": "We propose RLIR as a post-training framework for world models, aiming to enhance their action-following capability.", "keywords": ["world model", "reinforcement learning", "post-training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad1bca0213edec3313ac27a4b7d1098ffb94aa51.pdf", "supplementary_material": "/attachment/7a43e66537db4898d22b74d088bea8d27c7e5c9f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RLIR (Reinforcement Learning with Inverse Rewards), a post-training framework for action-conditioned world model designed to improve action-following ability using an inverse dynamics model (IDM). Rather than relying on human annotation or hand-crafted video verifiers, RLIR maps generated videos to the action space by inferring actions from the predicted future states via IDM, comparing against ground-truth actions to create a verifiable reward for GRPO-based post-training. The method is evaluated on both autoregressive and diffusion-based world models. Experimental results demonstrate consistent improvements over baselines in action-following accuracy, visual quality and human preference metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-Efficient Action-Consistent Rewarding: The core idea, using an IDM to verify whether the world model outputs actually encode the given actions, is a practical, scalable alternative to expensive or brittle human/hand-crafted reward schemes.\n-Comprehensive Experimentation: Experimental results span autoregressive (MineWorld) and diffusion-based (NFD) architectures across model sizes; Tables 1–2 show consistent gains (~5–10% action-following) alongside improved visual quality.\n-Rigorous Baseline and Ablation: The paper rigorously benchmarks against alternate reward shaping baselines (VideoAlign, RLVR-World pixel-level rewards), with ablation on reward function properties, hyperparameters (Table 8-10), and action/verifiability structure.\n-Substantive Qualitative Demonstration: Figure 3 (Page 7) provides explicit, step-by-step visual evidence: RLIR-corrected models address fine-grained failures in the baseline, such as action misalignment and localized visual artifacts, supporting key claims on both metrics.\n-Plug-and-play compatibility: RLIR is architecture-agnostic and integrates with GRPO without modifying the backbone, suggesting easy transfer across model families."}, "weaknesses": {"value": "-Data leakage risk: What worries me most is if the IDM and world model share training distributions, the reward may encode dataset priors rather than transferable action understanding.\n-Training details and reproducibility gaps: The proposed method relies on an Inverse Dynamics Model but provides no reproducible training details—data/labeling, architecture, objective, hyperparameters, or validation. This undermines reproducibility and confidence in the reward/credit assignment.\n-Dependence on IDM Fidelity: As noted in the limitations (p. 9), IDM errors cap reward quality. The paper does not quantify boundary conditions or adversarial/ambiguous cases where IDM mistakes could mis-reward or mis-penalize the model; robustness analysis is missing.\n-Generality Across Domains: Validation is limited to Minecraft-style environments with a strong pre-trained IDM (VPT). It is unclear how RLIR/IDM extend to open-ended, multimodal, or real-world domains with continuous or ambiguous actions.\n-Interpretability and Failure Cases: While qualitative improvements are highlighted (Figure 3), there is little diagnosis of RLIR’s failure modes. For example, are there patterns in which RLIR models overfit the IDM or fail to improve when IDM is already near-saturated? Do there exist situations where the world model “hacks” the IDM in a manner akin to classic reward hacking phenomena?\n-Scalability Limitations: The largest model is 1.2B parameters (Table 1). Costs, verifier throughput, and latency trade-offs at larger scales remain unclear. \n-Unproven planning benefit: Although RLIR post-trains world models, there is no controlled evidence that the resulting models improve planning or RL (closed-loop task performance, sample efficiency, or long-horizon generalization)."}, "questions": {"value": "-Data availability: Can you introduce the IDM training dataset (or a reproducible subset) and the RLIR post-training dataset/splits (including hashes, preprocessing, and licensing) to enable replication.\n-IDM Robustness and Generalization: Can you provide a detailed evaluation of IDM failure cases, especially in adversarial or ambiguous visual/action states? How do you prevent the RLIR-trained world model from hacking IDM?\n-Out-of-Domain application/scalability: What modifications are required for continuous or multimodal action spaces, or partially observable settings? Any pilot evidence? \n-Planning/RL evidence: Can you provide closed-loop planning/MBRL results (task success, sample efficiency, long-horizon returns) with ablations against a no-RLIR baseline?\n-Human preference details: Please report rater instructions, sampling, and inter-rater agreement to ensure robustness.\n-Potential action Imbalance: Does the IDM training dataset exhibit action-distribution imbalance, and if so, how is it addressed (e.g., reweighting, resampling, per-class metrics)?\nRating: 6, marginally above the acceptance threshold. But would not mind if paper is rejected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7G5rqimGR6", "forum": "n5dkjiplv4", "replyto": "n5dkjiplv4", "signatures": ["ICLR.cc/2026/Conference/Submission7569/Reviewer_ZYxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7569/Reviewer_ZYxS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720519810, "cdate": 1761720519810, "tmdate": 1762919664467, "mdate": 1762919664467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a verifiable reward for world model post-training via a pretrained IDM. This reward makes it verifiable whether frames generated actually carry out the action inputs. GRPO with this reward shows better generation compared to human preference reward and pixel-level L1 reward, and both diffusion and autoregressive models can benefit from this reward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The subject being studied is interesting. Whether the action is correctly carried out is an angle from which the understanding capability of the world model could be strengthen.\n\n2. Both autoregressive and diffusion based world models can benefit from the method proposed. Empirical results beat two baseline reward functions."}, "weaknesses": {"value": "1. The definition of \"Action Following\" is vague. Literally, it's hard to distinguish it from temporal consistency where both of them refer to the generated frames contain unwilling behaviors, based on which I would say action following is a subproblem of temporal consistency. This makes it hard for me to understand the visualization in Fig.3 where authors claim that action following is improved.\n\n2. The method proposed lacks novelty to me. It seems like simply apply VPT to serve as supervision, despite RL regime it uses such as GRPO on both autoregressive and diffusion models.\n\n3. This method is heavily dependent on the quality of pretrained IDM, which poses a limitation on those tasks without a large-scale pretrain dataset from which a precise IDM can be trained."}, "questions": {"value": "1. Related to W3,  is there any cases of reward hacking observed due to an unprecise estimate by IDM?\n\n2. Suppose IDM provides us precise action-label estimates, what are the results if we SFT finetune the WM on that IDM-annotated dataset similarly using action following distance as the SFT loss? Is RL necessary for improving action following?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0ienRDHyr3", "forum": "n5dkjiplv4", "replyto": "n5dkjiplv4", "signatures": ["ICLR.cc/2026/Conference/Submission7569/Reviewer_pmFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7569/Reviewer_pmFp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842667130, "cdate": 1761842667130, "tmdate": 1762919663847, "mdate": 1762919663847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses inaccurate action-following in video world models. It proposes RLIR, a post-training framework that uses an Inverse Dynamics Model (IDM) to recover actions from generated videos, converting high-dimensional video outputs into low-dimensional, verifiable rewards. The GRPO algorithm optimizes the world model. Experiments on MineWorld and NFD models with VPT videos show 5–10% improvements in action-following, visual quality, and higher human preference scores. The paper introduces RLIR for post-training, validates it across autoregressive and diffusion paradigms, and demonstrates consistent gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a key gap: While prior world models focus on visual quality, this paper targets action-following, which is crucial for controllable world models. The IDM-based rewards avoid costly human annotations or rule-based verifiers.\n2. The paper clearly describes IDM training, GRPO optimization, and adaptations for autoregressive and diffusion models, demonstrating a coherent and well-structured framework."}, "weaknesses": {"value": "1. Leveraging an IDM for reward construction is conceptually similar to curiosity-driven RL approaches that use inverse models to guide exploration. While RLIR applies this concept for post-training world models with an action-alignment reward, the paper does not discuss these prior connections, leaving the core insight only moderately novel.\n2. The paper lacks a clear definition of the MDP used in RL optimization. Key components—state space (e.g., historical frames, latent representations), action space (mapping generative steps to RL actions), and transition dynamics—are not explicitly described. This ambiguity makes it difficult to fully understand how RL interacts with world model generation.\n3. Experiments are conducted solely on the VPT dataset (Minecraft gameplay), which restricts conclusions about RLIR’s generalization. Validation on other domains (e.g., robotic manipulation, autonomous driving, real-world scene simulation) with different action types or visual scenarios would strengthen claims of general applicability."}, "questions": {"value": "1. GRPO is claimed to be more memory-efficient than PPO by removing the value network. However, large-scale world models (1B+ parameters) have high computational demands. Could you provide quantitative data on training time and memory overhead of RLIR post-training compared to baseline fine-tuning methods (e.g., supervised action-label fine-tuning)? This would clarify RLIR’s practicality at scale.\n2. RL is proposed as necessary for optimizing world models with IDM-derived rewards, but why not use direct gradient propagation through IDM? For example, treating IDM’s action prediction loss as a supervision signal for end-to-end fine-tuning. Could you include a comparative experiment between RLIR and direct end-to-end fine-tuning, or explain why the latter is infeasible (e.g., instability, conflicting objectives)?\n3. The IDM’s accuracy bounds RLIR’s performance, yet only a pre-trained VPT IDM is used. Have you investigated how IDM accuracy impacts RLIR results? For instance, ablating IDM with different prediction accuracies or fine-tuning IDM on subsets of VPT to reduce accuracy. This would clarify RLIR’s sensitivity to IDM quality and the potential gains from improving IDM further."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ueNI9s8o1B", "forum": "n5dkjiplv4", "replyto": "n5dkjiplv4", "signatures": ["ICLR.cc/2026/Conference/Submission7569/Reviewer_3FNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7569/Reviewer_3FNX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977715193, "cdate": 1761977715193, "tmdate": 1762919662353, "mdate": 1762919662353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present post-training method that improves how well video world models follow input actions by using an Inverse Dynamics Model to turn generated videos back into predicted actions and reward the model based on their agreement. This avoids costly human preferences or hand-crafted video verifiers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- new reward design avoids need for human preference labels or video checkers.\n- re-uses pre-trained IDMs\n- strong empirical gains"}, "weaknesses": {"value": "- limited evaluation outside of minecraft.\n- better description of the minecraft action space is required. unclear if authors are using the full keyboard + mouse action space or the simplified action space."}, "questions": {"value": "- do the authors have alternative datasets they have evaluated their method on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kSJEXwIv9n", "forum": "n5dkjiplv4", "replyto": "n5dkjiplv4", "signatures": ["ICLR.cc/2026/Conference/Submission7569/Reviewer_xznu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7569/Reviewer_xznu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7569/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762486529428, "cdate": 1762486529428, "tmdate": 1762919660054, "mdate": 1762919660054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}