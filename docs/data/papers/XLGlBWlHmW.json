{"id": "XLGlBWlHmW", "number": 7374, "cdate": 1758018302818, "mdate": 1759897856511, "content": {"title": "ChronosCore: Context-Aware Scheduling via Slack-Driven Temporal Reasoning", "abstract": "Modern real-time systems demand schedulers that can autonomously manage complex temporal interactions without manual tuning. ChronosCore is a value-based agent that integrates a compact Transformer encoder into a Deep Q-Network. Temporal slack is discretized into learnable slack tokens and fed to an attention module that models global inter-task relations in parallel while remaining computationally lightweight. Extensive experiments on single-core benchmarks, industrial mixed-criticality traces, and large-scale multiprocessor workloads show that ChronosCore consistently surpasses classical fixed-priority algorithms and feedforward reinforcement learning baselines in deadline adherence and response time, while preserving sub-millisecond inference latency. Ablation studies on encoder depth, head count, and embedding size indicate that modest architectures attain the best trade-off between decision quality and runtime cost. Finally, attention visualizations reveal that high-attention regions align with critical deadline interactions, improving model transparency and aiding post-hoc analysis.", "tldr": "ChronosCore learns context-aware scheduling policies using slack-driven reasoning, improving deadline adherence and scalability in real-time systems.", "keywords": ["Reinforcement Learning", "Real-Time Systems", "Transformer Models", "Attention Mechanisms", "Scheduling under Uncertainty", "Resource Allocation", "Embedded AI"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/269983a38c804c3cf10fc74bed57117243cd9433.pdf", "supplementary_material": "/attachment/1a0467ea252b841aad848b18e359ff54864cc9ac.zip"}, "replies": [{"content": {"summary": {"value": "The paper is about ChronosCore, a reinforcement learning (RL) framework for task scheduling that uses a Transformer encoder with a Deep Q-Network (DQN). The method discretizes temporal slack into learnable “slack tokens,” enabling permutation-invariant attention across variable-sized task sets. ChronosCore is designed to maintain sub-millisecond inference latency while modeling global inter-task dependencies. That is important for real-time embedded and multiprocessor systems. Experiments on demonstrate substantial gains compared to various methods. The paper also includes interpretability analyses using attention maps and provides theoretical and concentration guarantees."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper strengths are:\n\nThe paper shows slack-token embeddings is a contribution that encodes temporal urgency in a compact, order-agnostic way. The work bridges scheduling theory and learning-based adaptability, addressing gaps in permutation invariance and inference latency.\n\nAblation studies on encoder depth, attention heads, and embedding dimension provide convincing evidence that the design is balanced and not over-engineered. And interpretability analyses help clarify model behavior and link learned policies to scheduling intuition."}, "weaknesses": {"value": "The paper weaknesses are:\n\nWhile industrial traces are shown, it is unclear whether ChronosCore has been evaluated in an actual hardware-in-the-loop or production real-time system. Adding validation on embedded hardware or real cloud scheduling scenarios would strengthen the paper claims\n\nThe experiments are strong across task-set variations, but results under non-stationary workloads or unseen task distributions are not explicitly shown. RL is known for having generalization and stability challenges, this could be explored more in-depth."}, "questions": {"value": "A few questions for the authors:\n\nHow sensitive is performance to the choice of quantization levels (Q) and bin width (Δ)? \n\nThe paper focus on DQN-style value learning. Adding actor-critic or offline RL extensions could improve sample efficiency further?\n\nCan a trained ChronosCore model generalize to workloads with different utilization profiles or core counts without retraining? Adding discussion or few-shot adaptation results would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I2HaVxfNfi", "forum": "XLGlBWlHmW", "replyto": "XLGlBWlHmW", "signatures": ["ICLR.cc/2026/Conference/Submission7374/Reviewer_maHy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7374/Reviewer_maHy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761355690115, "cdate": 1761355690115, "tmdate": 1762919503676, "mdate": 1762919503676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### **Review Summary**\n\nThis paper introduces ChronosCore, a novel value-based reinforcement learning (RL) agent for real-time task scheduling. The authors identify that existing RL schedulers often suffer from order-dependence (e.g., using RNNs/FFNs) or high computational cost (e.g., using standard Transformers), making them ill-suited for low-latency, real-time environments.\n\nThe core contribution is a new architecture that integrates a compact Transformer encoder into a Deep Q-Network (DQN). [cite_start]To make this feasible, the paper introduces \"slack-tokens\": the agent's state representation is not a simple vector of task features, but a permutation-invariant set of learnable embeddings derived from the *quantised temporal slack* of each task [cite: 3379, 3476, 3479-3483]. This allows the Transformer to perform parallel, global reasoning on the relative urgency of all functions.\n\nThe authors demonstrate through extensive experiments on uniprocessor, industrial mixed-criticality, and large-scale multiprocessor benchmarks that ChronosCore significantly outperforms classical schedulers (EDF, RM) and feedforward RL baselines. [cite_start]Crucially, it achieves this while remaining \"computationally lightweight,\" maintaining sub-millisecond inference latency and demonstrating near-linear empirical scaling [cite: 3221, 3848, 3900-3901, 5045-5047]."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### **Strengths**\n\n1.  **Novel and Elegant State Representation:** The paper's primary contribution, the \"slack-token\" embedding, is a powerful and intuitive idea [cite: 3476, 3479-3483]. By discretising temporal slack (the most critical feature for real-time scheduling) and using it as the input to an embedding layer, the authors create a fixed-size, permutation-invariant *set* of tokens. [cite_start]This directly solves the order-dependence problem of sequence-based models [cite: 3232, 3313] and is a natural fit for the self-attention mechanism.\n\n2.  **Strong Empirical Validation:** The experiments are comprehensive and cover a wide range of relevant scenarios, from standard single-core benchmarks [cite: 3730] to industrial mixed-criticality workloads [cite: 3794] and large-scale (600-task) multiprocessor systems[cite: 3702, 3867]. ChronosCore consistently and significantly outperforms all baselines, including classical schedulers (EDF, RM) and various RL/heuristic methods[cite: 3739, 3761, 3800, 3867].\n\n3.  **Excellent Efficiency and Scaling:** A key claim of the paper is its low latency, which is critical for a \"real-time\" scheduler. The authors back this up convincingly. [cite_start]They report sub-millisecond inference latency [cite: 3703] and show excellent empirical scaling of $\\mathcal{O}(N^{1.1})$, which is far superior to the $\\mathcal{O}(N^{2.2})$ of a baseline like MHQISSO [cite: 3848, 3901, 5045-5047]. This is achieved by using a compact encoder and sparse attention, demonstrating a practical and well-engineered solution [cite: 3375, 3560-3563].\n\n4.  **Thorough Ablation and Interpretability:** The paper includes a strong set of ablation studies in the appendix (A.3) that justify the architectural choices (layers, heads, embedding dimension) [cite: 4305-4312, 4363, 4381]. These studies reinforce the \"lightweight\" claim by showing that a \"modest\" architecture (2 layers, 4 heads, $d=128$) provides the best trade-off. Furthermore, the attention visualisation (Fig. 6, Appendix), which shows a very high correlation ($r=0.98$) between attention weights and task deadline proximity, provides excellent interpretability and builds trust that the model is learning the correct temporal relationships [cite: 3223-3224, 4227, 4339-4340, 4357]."}, "weaknesses": {"value": "### **Weaknesses and Questions**\n\n1.  **\"Slack-Token\" Phrasing:** The paper heavily emphasises the novelty of \"slack-tokens.\" However, the method described is a two-stage process: (1) quantizing the continuous slack $s_i(t)$ into a discrete index $\\tilde{s}_i(t)$ [cite: 3479-3483], and (2) feeding this index into a standard learnable embedding matrix $E$ [cite: 3501-3503]. This is a standard and well-established technique (discretisation followed by embedding). The novelty lies in using *quantised temporal slack* as the specific input for a scheduling Transformer, not in the embedding mechanism itself. The terminology feels slightly overstated.\n\n2.  **Multicore Mapping Ablation:** The paper formalizes two practical strategies for multicore action mapping: (A) iterative masked-greedy selection and (B) a bipartite assignment layer [cite: 3597-3605]. However, the paper then states that only Option A is used in the experiments[cite: 3598]. This is a missed opportunity. Option B (bipartite matching) seems more principled, and an ablation comparing the performance and latency of these two strategies would have been a valuable addition. Why was Option A chosen over Option B?\n\n3.  **Reward Function Simplicity:** The reward function provides a +1 for any job completion and a -1 for any deadline miss [cite: 3398-3401]. This binary signal does not differentiate between a \"near miss\" (e.g., 1ms late) and a \"catastrophic miss\" (e.g., 100ms late). It also does not incentivise finishing tasks *well before* their deadline to build up slack for future, potentially bursty workloads. Could a shaped reward function, incorporating the *amount* of slack remaining (or negative slack incurred), lead to even more robust performance?"}, "questions": {"value": "no further questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7GJByVYOIJ", "forum": "XLGlBWlHmW", "replyto": "XLGlBWlHmW", "signatures": ["ICLR.cc/2026/Conference/Submission7374/Reviewer_XBTH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7374/Reviewer_XBTH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415869430, "cdate": 1761415869430, "tmdate": 1762919503304, "mdate": 1762919503304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ChronosCore, a real-time scheduler trained with value-based RL that embeds a lightweight Transformer encoder inside a DQN. It discretizes each task’s temporal slack into “slack tokens,” enabling policy learning over variable-sized task sets. The method is evaluated on single-core, multi-core, and multi-processor settings, with baselines including classic schedulers (Rate Monotonic, Earliest Deadline First) and RL-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed slack-quantised embedding method is new and effective in formulating the scheduling problem as a RL problem."}, "weaknesses": {"value": "1. Although attention weights can offer local interpretability, the Transformer-based scheduler lacks a clear, global description of its policy compared to traditional methods, which may hinder analysis and practical adoption.\n\n2. The fixed quantisation depends on job deadlines. If the deadline distribution is heavy-tailed (e.g., many short vs. few very long), can a meaningful scheduling policy still be learned? does it bias towards a specific type of tasks?\n\n3. Slack based scheduling share similarity to shortest-remaining-processing-time scheduling policy  (SRPT). The paper does not have a direct comparison to it and the task execution time distribution appears to be uniform."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nLSnzFVWY5", "forum": "XLGlBWlHmW", "replyto": "XLGlBWlHmW", "signatures": ["ICLR.cc/2026/Conference/Submission7374/Reviewer_TarM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7374/Reviewer_TarM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761780608985, "cdate": 1761780608985, "tmdate": 1762919502779, "mdate": 1762919502779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ChronosCore, an attention-driven RL framework that leverages slack-token embeddings and a Transformer-based Q-network to achieve efficient and scalable real-time scheduling on both single-core and multi-core systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- One strength of this paper is its modular architecture such that the proposed Q-network structure is generic, allowing it to be extended to multi-core scheduling without architectural modification. Because the per-task Q-scores are computed independently through attention and the mapping layer is decoupled from the network itself, the same base model can be directly applied to different numbers of cores, demonstrating scalability and structural simplicity.\n\n- The paper provides experimental validation across three representative cases, uniprocessor, industrial mixed-criticality, and large-scale multiprocessor setups, demonstrating the generality and practical effectiveness of the proposed approach."}, "weaknesses": {"value": "1. Slack is a continuous feature, and discretizing it to create one-hot or embedding tokens is essentially a continuous-to-categorical transformation, a common preprocessing technique in categorical encoding or quantized feature engineering. As such, this design appears more like feature engineering than an algorithmic innovation. The paper should clearly explain why this transformation is technically innovative and what unique role it plays in RL-based scheduling compared to other standard preprocessing or representation methods.\n\n2. The paper claims that discretization improves generalization, but provides no clear theoretical or empirical justification. The ablation lacks direct comparison with continuous slack and other methods, leaving the generalization claim unsupported.\n\n3. The paper reports an empirical complexity but lacks a theoretical derivation or analytical justification of this result. The explanation seems to remain implementation-level, based on observed runtime rather than a formal complexity analysis of the proposed scheme.\n\n4. The network optimization technique proposed in this paper appears quite similar to existing lightweight Transformer approaches, such as Explicit Sparse Transformer [1] and Efficient Sparse Attention [2]. It would strengthen the paper to clearly describe how the proposed optimization differs from these prior methods.\n\n[1] Zhao, Guangxiang, et al. \"Explicit sparse transformer: Concentrated attention through explicit selection.\" arXiv preprint arXiv:1912.11637 (2019).\n\n[2] Lou, Chao, et al. \"Sparser is faster and less is more: Efficient sparse attention for long-range transformers.\" arXiv preprint arXiv:2406.16747 (2024).\n\n5. The idea of integrating a Transformer encoder within a DQN is interesting but not new, as several prior studies have explored similar approaches. The paper would benefit from a clearer explanation of what distinguishes its proposed attention-driven Q-network: for example, whether the novelty lies in the lightweight architecture, the permutation-invariant design, or the latency optimization. An explicit comparison with existing Transformer-RL approaches would help clarify the technical contribution.\n\n6. The results demonstrate consistent improvements over baselines, but the absence of comparative discussion limits understanding of why the proposed approach performs better or under what conditions its advantages hold, especially compared to other RL-based baselines. \n\n7. The two proposed mapping strategies, iterative masked-greedy and bipartite assignment, demonstrate the model’s scalability to multi-core scheduling. Yet, it is difficult to regard either strategy as algorithmically novel, since similar greedy and matching-based methods have been explored in prior works. If there is specific novelty in how these strategies are formulated or integrated, it should be clearly explained in the paper. \n\n8. It would improve the paper’s clarity if the ablation results for the proposed technical components were summarized in the main text."}, "questions": {"value": "It appears that none of the RL-based baselines include slack as an explicit input feature. Are there truly no prior RL methods that use slack? Including or re-implementing RL baselines with explicit slack features and comparing them against the proposed slack-token embeddings would enable a more informative empirical analysis of representation choices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1SfwpTpx69", "forum": "XLGlBWlHmW", "replyto": "XLGlBWlHmW", "signatures": ["ICLR.cc/2026/Conference/Submission7374/Reviewer_TAn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7374/Reviewer_TAn4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995067725, "cdate": 1761995067725, "tmdate": 1762919502438, "mdate": 1762919502438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}