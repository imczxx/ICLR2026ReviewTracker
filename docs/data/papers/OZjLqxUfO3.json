{"id": "OZjLqxUfO3", "number": 13882, "cdate": 1758224371078, "mdate": 1759897407046, "content": {"title": "LiteHall: A Three-Stage, Modular and Lightweight Pipeline for End-to-End Hallucination Detection", "abstract": "Large Language Models (LLMs) are increasingly applied in high-stakes domains such as medicine and law, where hallucinations can have serious consequences. Existing detection approaches either depend on costly proprietary LLMs with limited adaptability, or on monolithic open-source models that require full retraining, struggle with long evidence contexts, and lack transparency. We introduce LiteHall, a lightweight, fully open-source, three-stage hallucination detection pipeline designed for modularity, domain adaptability, and interpretability. Each stage leverages a 1.7B-parameter Small Language Model (SLM) trained independently with stage-specific Reinforcement Learning with Verifiable Rewards (RLVR) over a high-quality synthetic corpus of 120K+ examples, enabling efficient specialization without reliance on large monolithic models. To advance rigorous evaluation, we present HaFin500, a fine-grained benchmark of 500 long-form QA pairs spanning 30 fact-seeking domains, annotated with 6K claims and 3.5M evidence tokens. Extensive experiments show that LiteHall consistently surpasses both open-source and proprietary detectors. On out-of-domain benchmarks, LiteHall achieves substantial gains over strong baselines, including +6.4% / +10.0% (Accuracy/F1) against MiniCheck-7B, +6.1% / +4.8% over SAFE (GPT-3.5-turbo), +11.5% / +13.0% over AlignScore, and +9.8% / +15.2% over FAVA. Even compared to GPT-4o, LiteHall delivers +4.7% / +3.0% improvements in zero-shot mode, while retaining an additional +2.0% / +0.9% advantage when GPT-4o is integrated as a backbone. These results demonstrate that LiteHall not only matches or exceeds in-domain performance but also generalizes robustly out-of-domain, establishing it as a practical, transparent, and reproducible solution for trustworthy LLM deployments.", "tldr": "", "keywords": ["Hallucination Detection", "Large Language Models (LLMs)", "Small Language Models (SLMs)", "RLVR", "LiteHall", "HaFin500", "Modularity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f369f9466edbdeb2dd16871ef67ca7f31aa84ee.pdf", "supplementary_material": "/attachment/220b6a0208a237ac134d9c9d13614caa0f09e8a0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces LiteHall, a three-stage, open-source pipeline for hallucination detection that includes claim extraction, sentence-level evidence retrieval, and claim verification. Each stage is powered by a 1.7B small language model trained with supervised fine-tuning and optimized using reinforcement learning with verifiable rewards. The authors also present HaFin500, a benchmark with 500 QA pairs and detailed annotations for claims, evidence, and hallucination spans. The experiments show strong results on both in-domain and out-of-domain datasets compared to existing baselines like AlignScore, MiniCheck, and SAFE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and interpretable design: The pipeline’s modular approach with sentence-indexed evidence allows precise traceability and interpretability.\n2. Strong evaluation: The experiments span multiple datasets and domains, demonstrating consistent performance improvements.\n3. New benchmark contribution: HaFin500 fills a gap in fine-grained hallucination evaluation with high-quality annotations and diverse topics.\n4. Efficient and open-source: The use of small models and open data increases accessibility for the research community.\n5. Empirical performance: The results show consistent gains over several baselines and even outperform GPT-4-based methods on some tasks."}, "weaknesses": {"value": "1. Dependence on closed models: Although the pipeline itself is open-source, the data generation process relies heavily on GPT-4-based annotations, which limits full reproducibility.\n2. Limited real-world validation: Most experiments are done on curated datasets. It’s unclear how the model performs on naturally occurring hallucinations from real-world sources.\n3. Efficiency claims not quantified: The paper states the approach is lightweight but lacks concrete numbers on latency and throughput compared to single-model detectors.\n4. Threshold tuning: The decision boundary for hallucination detection is manually tuned, and its generalization across domains is not analyzed.\n5. Potential overfitting to synthetic styles: Since much of the training data is generated synthetically, there’s a risk that the model learns to detect artificial patterns rather than genuine factual errors."}, "questions": {"value": "1. What are the actual latency and resource requirements for the end-to-end pipeline compared to larger models like MiniCheck-7B?\n2. How much performance improvement comes from reinforcement learning compared to fine-tuning alone?\n3. How robust is the system to retrieval noise or outdated evidence?\n4. Can LiteHall be extended to multilingual hallucination detection?\n5. How sensitive is performance to the chosen threshold for FactScore or hallucination classification?\n6. Have you conducted any human evaluation to measure interpretability benefits in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gqsvcygG1Q", "forum": "OZjLqxUfO3", "replyto": "OZjLqxUfO3", "signatures": ["ICLR.cc/2026/Conference/Submission13882/Reviewer_kUun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13882/Reviewer_kUun"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611434553, "cdate": 1761611434553, "tmdate": 1762924393970, "mdate": 1762924393970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LiteHall, a modular, lightweight, open-source pipeline for detecting hallucinations in LLMs using three stages—claim extraction, evidence retrieval, and claim verification—each powered by a 1.7B-parameter SLM trained via SFT and RLVR on a 120K+ synthetic dataset across 30 domains. It emphasizes transparency, efficiency, and adaptability without proprietary models. The authors introduce HaFin500, a benchmark with 500 long-form QA pairs, 6K claims, and 3.5M evidence tokens for fine-grained evaluation. \n\nExperiments show LiteHall outperforms baselines like MiniCheck-7B (+6.4% Acc/+10.0% F1), SAFE (+6.1%/+4.8%), AlignScore (+11.5%/+13.0%), FAVA (+9.8%/+15.2%), and even GPT-4o zero-shot (+4.7%/+3.0%), demonstrating strong in-domain and out-of-domain generalization. Key contributions include the modular RLVR design, HaFin500 dataset, synthetic corpus, and empirical validations proving SLMs can surpass larger models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.Efficient, transparent pipeline that outperforms larger baselines while using fewer resources.\n\n2.Comprehensive benchmark (HaFin500) fills a gap in fine-grained hallucination evaluation.\n\n3.Strong empirical results with clear gains in accuracy/F1 across diverse datasets."}, "weaknesses": {"value": "1.Reliance on GPT-4o for synthetic data generation may propagate biases into training.\n\n2.RLVR is central to the paper’s contribution, but there is no ablation comparing RLVR vs. SFT-only versions of the three modules.\n\n3.Although the paper emphasizes that LiteHall is \"lightweight and efficient,\" it does not report inference time, memory consumption, or computational cost."}, "questions": {"value": "1.You could provide the results of LiteHall trained without RLVR (i.e., using SFT only) to better quantify the exact contribution of verifiable reinforcement learning to each module.\n\n2.You could include the inference times (or FLOPs) per module and compare them with MiniCheck-7B and GPT-4o (LiteHall) to provide a clearer understanding of the computational efficiency and scalability of your approach.\n\n3.How did you ensure that HaFin500’s examples do not overlap with or resemble your 120K synthetic training corpus?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F5cZDr4nnq", "forum": "OZjLqxUfO3", "replyto": "OZjLqxUfO3", "signatures": ["ICLR.cc/2026/Conference/Submission13882/Reviewer_LYiF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13882/Reviewer_LYiF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896684037, "cdate": 1761896684037, "tmdate": 1762924393429, "mdate": 1762924393429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a modular hallucination detection pipeline, LiteHall, decomposing the task into three different stages: claim extraction, evidence retrieval, and claim verification. Each module is implemented using a small language model trained on a large synthetic corpus. According to the evaluation results on both in-domain and out-of-domain benchmarks, LiteHall outperforms strong baselines while using smaller models. In addition, the paper introduces a fine-grained benchmark, HaFin500, covering more than 30 topics and three sub-stage tasks."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Solid and Comprehensive experiments**: The authors conducts solid evaluations across both in-domain and out-of-domain benchmarks, reporting results at both the end-to-end and module levels. It further provides ablation studies to verify the contribution of each module in the pipeline.\n2. **Strong Performance**: LiteHall outperforms many strong baselines, and even surpasses the pipeline versions with GPT-4o as the backbone model for each module, demonstrating the effectiveness of its modular design and training strategy.\n3. **New Resources**: The paper introduces HaFin500, a new benchmark with long-form responses covering a wide range of domains. It includes detailed annotations, enabling fine-grained evaluation of hallucination detection models."}, "weaknesses": {"value": "1. The pipeline design is not particularly novel, as similar multi-stage hallucination detection approaches have appeared in prior work [1]. The main contributions of this paper lie more in the engineering implementation and the creation of a new hallucination detection benchmark (HaFin500).\n\n2. I have some concerns regarding potential error accumulation across stages in the proposed pipeline, as no analysis of this issue is provided.\n\n3. The use of a threshold for hallucination detection (Line 242-244) raises my concerns about the reliability of the verifier’s predictions. In general, any claim that is unverified or unsupported should be considered a hallucination, making the need for a threshold somewhat questionable.\n\n4. HaFin500 is one of the main contributions of the paper, yet its description in the main text is insufficient. More details and statistics should be provided.\n\n5. The paper claims that the proposed method is efficient, but it lacks evaluation or comparison of efficiency, such as inference latency, between end-to-end hallucination detection models and the proposed pipeline.\n\n[1] Long-form factuality in large language models, NeuIPS 2024."}, "questions": {"value": "## Questions:\n1. In the section about Span-Level Evaluation, I am not sure whether using O3 for error classification is appropriate and fair. Reporting results on other span-level benchmarks, such as RAGTruth, would be helpful.\n\n## Suggestions\n1. Captions should clarify the meaning of bold text and asterisks in tables and figures to avoid ambiguity.\n2. Important aspects of the training procedure, such as algorithm choices and reward design, should be described in the main text rather than deferred to the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RB6mh3aCLK", "forum": "OZjLqxUfO3", "replyto": "OZjLqxUfO3", "signatures": ["ICLR.cc/2026/Conference/Submission13882/Reviewer_ZiiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13882/Reviewer_ZiiX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976015794, "cdate": 1761976015794, "tmdate": 1762924393042, "mdate": 1762924393042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, they address the problem of LLM's illusions in long text generation by proposing an interpretable automated detection scheme aimed at replacing black-box evaluation that relies on closed-source models. Specifically, this paper proposes a three-stage detection scheme called LiteHall. The method trains a 1.7B model through supervised fine-tuning training with verifiable reward reinforcement learning for verifiable claim extraction, relevant sentence retrieval, and hallucination judgments. In addition, the authors propose a fine-grained benchmark HaFin500 for end-to-end reviews.The authors demonstrate the effectiveness of the proposed method on multiple datasets and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a new fine-grained annotated benchmark to evaluate the capability of LLMs at all stages, which is based on relatively long contexts.\n2. Experimental validation is comprehensive. The authors report results on multiple in-domain as well as out-of-domain benchmarks."}, "weaknesses": {"value": "1. Over-reliance on synthetic corpora with closed-source model annotation. The paper relies heavily on synthetic data generated or labeled by closed-source models such as GPT-4o in both training and testing phases. The training samples for the three-stage models are mainly generated automatically by LLMs, and the HaFin500 benchmark also produces responses with GPT-4o and confirms them via votes from multiple closed-source models, with only limited manual inspection. This practice may lead to distributional coupling between training and test data, potentially giving the model a pseudo-advantage during evaluation. In addition, manual validation ratios and consistency metrics (e.g., Cohen’s Kappa) are not included, making it difficult to ensure labeling quality.\n2. Insufficient justification for the model parameter scale. The authors repeatedly emphasize that each sub-model of LiteHall has 1.7B parameters, presenting this as the main selling point for lightweight efficiency. However, the paper lacks systematic scaling experiments to illustrate how performance varies with model size, and does not verify whether larger models would yield further improvements. Reporting only a single-point performance without a performance–cost scaling curve makes the claim that a small model is optimal seem unconvincing.\n3. Missing trade-off analysis between performance and inference overhead. LiteHall adopts a three-stage pipeline, which inevitably introduces additional inference overhead along with improving accuracy. Although the authors claim low latency, the paper does not report specific end-to-end latency, memory consumption, or computational cost, nor does it compare efficiency with single-model detectors under consistent hardware settings. Since hallucination detection often needs to operate in real-time or batch settings, the lack of a latency–performance trade-off analysis undermines the framework’s practical deployability.\n4. Domain-specific adaptation of the FactScore threshold reduces usability. The paper uses a 0.75 threshold to determine whether a response is hallucinated and mentions that this threshold can be adjusted across domains for better results. However, in real-world out-of-domain applications, annotated data for threshold recalibration are typically unavailable, and such domain-specific tuning weakens the plug-and-play generalization of the system. If the model requires re-thresholding for each new domain, its out-of-domain performance will be difficult to evaluate and less practical for deployment."}, "questions": {"value": "Please examine the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Riki7V4JU", "forum": "OZjLqxUfO3", "replyto": "OZjLqxUfO3", "signatures": ["ICLR.cc/2026/Conference/Submission13882/Reviewer_EY22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13882/Reviewer_EY22"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980315425, "cdate": 1761980315425, "tmdate": 1762924392556, "mdate": 1762924392556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}