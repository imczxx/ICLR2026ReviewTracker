{"id": "u6DbnUgBDV", "number": 22648, "cdate": 1758333940064, "mdate": 1759896854895, "content": {"title": "EvalRes: Evaluating VLMs' Sensitivity to Image Resolution and Relative Detail Size", "abstract": "Visual Language Models (VLMs) have achieved remarkable success across a wide range of Visual Question Answering (VQA) tasks. Yet, they still struggle with high-resolution visual inputs where regions providing key information are relatively small or scenes are highly detailed and cluttered. This limitation stems from the architectural bottlenecks of current vision encoders, which often fail to preserve fine-granular details necessary for precise reasoning. While several approaches have been proposed to address this issue, a systematic evaluation of a model's capacity to process high-resolution content and small-scale visual cues has been lacking. In this work, we introduce a versatile framework to extend benchmarks and propose two novel metrics designed to assess VLMs' scalability across varying image resolutions and aspect ratios. Unlike evaluation with existing benchmarks, which lack consistency in image properties and fail to isolate resolution and aspect ratio effects, our method enables controlled experimentation to disentangle resolution sensitivity from the overall task performance. Our framework not only enables more robust and fair VLM evaluation, but also paves the way for future research into high-fidelity visual understanding. We evaluate several widely used VLMs with the proposed framework, revealing that even state-of-the-art models struggle with higher resolution and non-standard aspect ratios, and that processing small details remains a major challenge.", "tldr": "A flexible framework for evaluating VLMs sensitivity and robustness to resolution and aspect ratio related image transformations", "keywords": ["vision language model", "multimodal", "evaluation", "benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a32ed0658d6c04bf946eb20faa1910b28a42490.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the scalability and robustness of VLMs with respect to image resolution and the relative size of important visual details. The authors propose a controlled experimental setting that augments images with neutral margins to systematically test resolution and aspect ratio sensitivity, introducing two metrics — AUSC and RBS— as quantitative measures. Several popular VLMs are evaluated under different margin sizes, positions, and aspect ratios."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Controlled Benchmark Extension — The paper provides a simple yet reproducible way to alter benchmark datasets to isolate resolution and aspect ratio effects, which can be applied across existing VQA benchmarks.\n\n2. New Metrics — AUSC and RBS are well-defined, intuitive metrics that allow decoupling resolution scalability from model size or overall accuracy.\n\n3. Practical Implications — The results highlight non-trivial weaknesses in current VLM vision encoders, emphasizing the importance of resolution invariance in applications like OCR, mathematical reasoning, and diagram understanding."}, "weaknesses": {"value": "1. Presentation & Naming Issues — There are multiple inconsistencies in formatting and nomenclature (e.g., table headings, capitalisation of “LLaVA”; missing model size specifications; incorrect naming such as “LlaVa-1.5” instead of “LLaVA-v1.5-XB”). These hinder clarity and professionalism.\n\n2. Motivation May Be Weak — The proposed margin-based “resolution sensitivity” setting may not be crucial for next-generation models (e.g., o3-like multimodal LLMs) that can actively crop or resize relevant image regions on their own.\n\n3. Limited Impact of Experimental Variations — In many cases (e.g., Tables 1 & 2), model performance changes across settings are small, suggesting that the proposed setup might not challenge stronger models significantly. The main large deviation is for LLaVA-v1.5, which is likely due to its center-cropping preprocessing rather than a fundamental resolution limitation.\n\n4. Contribution Scope — The newly proposed metrics are essentially variants of existing evaluation measures (e.g., AUSC is conceptually an AUC-like metric), and the core experiments reveal only moderate performance differences.\n\n5. No Deep Dive into Underlying Causes — While the paper reports performance degradation, there is limited deeper analysis (e.g., attention maps, reasoning chain analysis) to fully explain why such degradations occur."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mBQpuaGYHK", "forum": "u6DbnUgBDV", "replyto": "u6DbnUgBDV", "signatures": ["ICLR.cc/2026/Conference/Submission22648/Reviewer_nURN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22648/Reviewer_nURN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910207218, "cdate": 1761910207218, "tmdate": 1762942320631, "mdate": 1762942320631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EvalRes, a simple, controlled framework to evaluate VLMs’ sensitivity to image resolution and aspect ratio by padding each benchmark image with neutral margins while keeping the original content unchanged. It proposes two metrics: AUSC (Area Under the Scaling Curve) and RBS (Resolution Bias Score) to quantify scaling robustness and prediction instability under such transformations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, controlled manipulation: padding preserves answer-relevant pixels while varying overall image size, which would be proper for isolating preprocessing effects from reasoning.\n2. Two concise metrics (AUSC/RBS) that separate robustness over items the model got right and the instability over items it got wrong."}, "weaknesses": {"value": "1. Paper feels rushed: very short introduction with no references, formatting inconsistencies, and some mathematical formulations that do not clearly support new insights in the main text.\n2. A fundamental concern I have is that the provided method does not actually change the signal resolution of the original scene. It simply adds pad/noise patches that alter packing/cropping and more specifically, input tokens of the vision model. This primarily would test padding/cropping sensitivity of the models rather than true resolution scalability."}, "questions": {"value": "1. Following weakness #1, the paper seems a bit rushed e.g. (1) The introduction is very short and has no citations/references for the motivations (2) In figure 5, the plots and their titles do not have the same size and are not aligned (3) The metrics are relatively simple and therefore, the mathematical formulations do not seem to be necessary to be presented in the main text. Overall I think a few rounds of polishing might be necessary for the paper to become submission-ready.\n2. To further elaborate on weakness #2: With the proposed approach, in my humble opinion, the paper is mostly testing how much the model can detect and ignore full and partial noise/pad tokens from its inputs, and does not analyze how much the model can handle different resolutions which contain different amount of high-frequency signal. In ViT-style encoders, white margins create (1) fully padded patches and (2) mixed patches (part margin, part content). This is not the same as feeding a genuinely higher-resolution image with more high-frequency signal and fine-grained information. In a proper study on this problem, one would try out different resolutions of the same images (so actually having more fine-grained signals as you increase the resolution) to test this. I would be interested to see such experiments to better validate the claims of the paper.\n3. It is mentioned in section 3.1 that super-resolution (SR) models are (1) computationally expensive and (2) might add noise/irrelevant information to the image. Modern and older SR approaches such as GAN-based models like Real-ESRGAN [1] perform quite well and are publicly available on very small to large sizes. I would appreciate it if the authors can either update the claim or provide proper explanations, citations, or experiments to support it.\n4. Several relevant works have not mentioned in the paper [2, 3, 4] which to some degree address the paper's questions. I would appreciate it if you could elaborate on these works and explain how they relate to the paper.\n5. Could you elaborate on baseline selection? It seems there are additional baselines ([3, 4, 5]) that would be interesting to include or at least position against.\n\n[1] Wang, Xintao, et al. \"Real-esrgan: Training real-world blind super-resolution with pure synthetic data.\" _Proceedings of the IEEE/CVF international conference on computer vision_. 2021.\n\n[2] Chai, Lucy, et al. \"Any-resolution training for high-resolution image synthesis.\" _European conference on computer vision_. Cham: Springer Nature Switzerland, 2022.\n\n[3] Xue, Le, et al. \"xgen-mm (blip-3): A family of open large multimodal models.\" _arXiv preprint arXiv:2408.08872_ (2024).\n\n[4] Wang, Peng, et al. \"Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.\" _arXiv preprint arXiv:2409.12191_ (2024).\n\n[5] Guo, Zonghao, et al. \"Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.\" _European Conference on Computer Vision_. Cham: Springer Nature Switzerland, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gzr2iIA2D7", "forum": "u6DbnUgBDV", "replyto": "u6DbnUgBDV", "signatures": ["ICLR.cc/2026/Conference/Submission22648/Reviewer_6Hpo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22648/Reviewer_6Hpo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911955236, "cdate": 1761911955236, "tmdate": 1762942319839, "mdate": 1762942319839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to augment existing VLM benchmarks to varying image background sizes. The authors also proposes corresponding metrics and evaluates five VLMs on five benchmarks under their proposed setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* This paper demonstrates that some VLMs are sensitive to the position of images when placed on a white background."}, "weaknesses": {"value": "* The technical contribution is overly incremental and trivial, offering little novel insight into VLM evaluation. Instead of providing an ad hoc data augmentation method, the authors ought to focus on a more fundamental question: given the multitude of possible ways to augment existing benchmarks, why is this specific metric or augmentation valuable over others? The authors must dive deeper into the essential reasons for their approach.\n* Moreover, the paper does not achieve what it claims regarding increased evaluation difficulty or diversity. The proposed method simply places the original image onto white canvases of varying sizes. While this does generate more data, the underlying informative visual signals remain exactly the same and can be easily addressed by simple preprocessing steps (e.g., cropping the empty background). Consequently, the claimed benefit is not sound or robust."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6miKKiiRJW", "forum": "u6DbnUgBDV", "replyto": "u6DbnUgBDV", "signatures": ["ICLR.cc/2026/Conference/Submission22648/Reviewer_VNeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22648/Reviewer_VNeL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952496611, "cdate": 1761952496611, "tmdate": 1762942318395, "mdate": 1762942318395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark to evaluate VLMs' sensitivity to image resolution and relative finegrained detail, as well as two metrics for assesement. The authors propose a simple method to construct the benchmark by augmenting existing images with neutral backgrounds of varying sizes. Based on the benchmarking method, the authors propose two metrics, AUSC and RBS, to measure the VLMs' scalability with respect to resolution and aspect ratio, both metrics is interpretable. The authors evaluate several VLMs on the benchmark and provide analysis based on the evaluation results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is easy to follow.\n* The paper is well-motivated, as resolution is actually playing a very important role in vision-language models, yet few works have thoroughly investigated its impact.\n* The proposed metrics are interpretable and make sense.\n* The paper proposes two metrics, AUSC and RBS, to measure VLMs' scalability with respect to resolution and aspect ratio."}, "weaknesses": {"value": "* In my view, the contribution of this paper is not significant enough, only proposes two metrics and an evaluation methodology, alough I acknowledge the task is well-motivated and important.\n* The model versioning in the paper is confusing. Specifically, Qwen2.5 and Llama3.2 are text-only models, so using them to evaluate multimodal capabilities is weird, maybe what the authors mean is Qwen2.5VL?\n* The experimental evaluation is insufficient; experiments were conducted on only five models, which is not convincing enough."}, "questions": {"value": "* In L257, the recommended $|B_{correct}|$ should be greater than 75,  why, is there any explanation?\n* Does adding margins around images alter the image domain, making them differ from natural images? Could this introduce a variable, as performance differences across models may stem not only from image resolution but also from whether their training data included similar images with margins?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WxA88uGNdp", "forum": "u6DbnUgBDV", "replyto": "u6DbnUgBDV", "signatures": ["ICLR.cc/2026/Conference/Submission22648/Reviewer_mKBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22648/Reviewer_mKBi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998257464, "cdate": 1761998257464, "tmdate": 1762942318170, "mdate": 1762942318170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}