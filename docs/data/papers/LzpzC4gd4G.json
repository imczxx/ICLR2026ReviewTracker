{"id": "LzpzC4gd4G", "number": 17078, "cdate": 1758271881360, "mdate": 1759897199714, "content": {"title": "Choices Speak Louder than Questions", "abstract": "Recent findings raise concerns about whether the evaluation of Multiple-Choice Question Answering (MCQA) accurately reflects the comprehension abilities of large language models. This paper explores the concept of \\textit{choice sensitivity}, which refers to the tendency for model decisions to be more influenced by the answer options than by a genuine understanding of the question. We introduce a new scoring method called **Normalized Probability Shift by the Question (NPSQ)**, designed to isolate the impact of the question itself and provide a more reliable assessment of comprehension. Through experiments involving various input formats, including cloze, symbols, and hybrid formats, we find that traditional scoring methods — such as those based on log-likelihood or its length-normalized variant — are vulnerable to superficial characteristics of the answer choices. In contrast, NPSQ remains stable even when modifications are made to the answer options.", "tldr": "The paper shows that models are often biased by answer choices, and proposes Normalized Probability Shift by the Question (NPSQ), a new metric that isolates the influence of the question and is less affected by the composition of the answer choices.", "keywords": ["large language model", "evaluation methodologies", "multiple choice question"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b40a2d178f1d42183fc5b21042f4d7e6af70aa56.pdf", "supplementary_material": "/attachment/c697ba629cf704d5c11fcee977861a80804af581.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, authors raise the issue of LLM answers to questions with option choice being driven by some intrinsic hints in the option texts, not by understanding of the question. Authors decompose the log-probability that model assign to each answer to the given question into choice- and question-driven components and develop a method to identify questions for which the model's answers are caused by choice texts. Authors apply this method to analyze several LLMs (base and instruction-tuned) on three common MCQA benchmarks. Finally, they propose a new method to select the answer option (NPSQ) that is less sensitive to superficial characteristics of the answer choices than log-probability and provide analysis to show that it is a more reliable tool to assess the model's comprehension of the question."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper addresses an important problem; it proposes a novel approach to identify and mitigate such \"choice-induced\" biases; this method is easy to deploy and does not require training of a model or additional data.\n\n- The proposed method can be used to improve existing (and future) MCQA benchmarks by removing \"shortcuts\" presented in the formulation of choice option texts.\n\n- Experiments cover different task formulations (options only/cloze prompting/hybrid). A detailed analysis of the proposed method (NPSQ) and observed effects is provided.\n\n- Paper is well-written and easy to follow."}, "weaknesses": {"value": "- My main concern regarding the proposed method is the fact that it does not take into account effects caused by the order of the options (which is known from the previous studies in the field to be an influential factor). In several works it was shown that, given a list, LLMs often `focuses' more on the later entries from it than on earlier. It can (theoretically) affect the proposed sensitivity analysis method:\n\nChoice sensitivity inequality for one question (line 140) can be reformulated into\n$$\n2 * (Score_{choice}(Q, C, x_1) - Score_{choice}(Q, C, x_2) ) > Score(Q, C, x_1) - Score(Q, C, x_2)\n$$\nif $x_1$ is the last option (e.g., D for 4-option setup in MMLU), and $x_2$ is one of the first, left side may be very high, because without question (empty \"C\" in calculation of $Score_{choice}$) mentioned above effect of list item recall may take place.\n\nA similar issue arises in the formula for NPSQ (line 321).\n\n I would recommend to further analyze this issue and, maybe, (although it is just one of the potential ways to address it, if it is a real problem; I do not insist on using this suggestion) average probabilities from initial options order and after a certain permutation.\n\n- The performed experiments cover only relatively small models up to 8B parameters. Presented results show that larger model has smaller (but still noticeable) sensitivity bias, and it would be very interesting to see the results for a larger model (at least -14B or -32B)."}, "questions": {"value": "- Would inclusion of irrelevant (i.e., ) \"uncertainty sinks\" options (like ``I don't know`` or ``None of the above``) affect the model's sensitivity to options?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KWNSrw369z", "forum": "LzpzC4gd4G", "replyto": "LzpzC4gd4G", "signatures": ["ICLR.cc/2026/Conference/Submission17078/Reviewer_WE2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17078/Reviewer_WE2p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670390811, "cdate": 1761670390811, "tmdate": 1762927087029, "mdate": 1762927087029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates \"choice sensitivity\" in the MCQA evaluation of LLMs: the phenomenon where models exploit superficial features of the answer options rather than genuinely comprehending the question. The authors demonstrate that a significant portion of model decisions (20-60%) is driven by this sensitivity. To address this, they propose a new evaluation metric, Normalized Probability Shift by the Question (NPSQ), which is designed to isolate the impact of the question itself. Experiments, particularly those using adversarial choices, show that NPSQ is significantly more robust to option-based artifacts than traditional log-likelihood or length-normalized metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses a clear and increasingly important issue in LLM evaluation. As models achieve high scores on benchmarks, it is crucial to understand if this reflects true comprehension or artifact exploitation.\n\nThe proposed NPSQ metric is intuitive, well-motivated, and directly targets the identified problem by quantifying the \"value\" of the question.\n\nThe use of adversarial choices provides a very clear and convincing demonstration of the weaknesses of existing metrics and the robustness of NPSQ. The results in Figure 3 are particularly compelling."}, "weaknesses": {"value": "While valuable, the contribution is an incremental improvement in evaluation methodology rather than a new task, model, and with no fundamental insight into model reasoning.\n\nThe analysis of why models exhibit this sensitivity is not deeply explored, though this is not the primary focus. The experiments are solid but could be extended to a wider range of model architectures and benchmarks."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cque0dlgqj", "forum": "LzpzC4gd4G", "replyto": "LzpzC4gd4G", "signatures": ["ICLR.cc/2026/Conference/Submission17078/Reviewer_yhhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17078/Reviewer_yhhk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882130232, "cdate": 1761882130232, "tmdate": 1762927086650, "mdate": 1762927086650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that standard MCQA metrics reward models for exploiting \"option-surface cues\" rather than understanding the question. It proposes NPSQ (Normalized Probability Shift by the Question), a new metric that isolates the question's contribution by comparing an option's log-likelihood with versus without the question, then normalizing this gain. Experiments on HellaSwag, ARC-Challenge, and MMLU show that while standard accuracy is fragile to adversarial or rephrased choices, NPSQ accuracy remains stable and can reorder model rankings. The study also quantifies this \"choice sensitivity\" and explores other factors like formats and prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies and formalizes a pervasive evaluation artifact, choice sensitivity, and gives a principled, testable metric to mitigate it.\n\n2. The core construct (question-conditioned vs. question-ablated likelihood shift with normalization) is simple, auditable, and easy to slot into existing LM-eval pipelines."}, "weaknesses": {"value": "1. The normalization in NPSQ is not stress-tested against plausible alternatives (e.g., z-scores, temperature scaling, ECE), leaving ranking stability under-substantiated.\n\n2. Key stability claims (flip rates, adversarial drops) lack uncertainty quantification and significance testing, weakening the statistical support for the conclusions.\n\n3. The metric relies on token-level probabilities and a hand-crafted “no-question” template whose wording or API backend may change outcomes, reducing reproducibility.\n\n4. Computing scores with and without the question doubles evaluation cost, which is non-trivial for large suites."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3kPDCDhW7B", "forum": "LzpzC4gd4G", "replyto": "LzpzC4gd4G", "signatures": ["ICLR.cc/2026/Conference/Submission17078/Reviewer_ACim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17078/Reviewer_ACim"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929019126, "cdate": 1761929019126, "tmdate": 1762927086300, "mdate": 1762927086300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates LLM benchmarking methodology.  It begins by exploring the fact that some LLMs can answer multiple-choice benchmarking questions without actually looking at the question, implying that they are influenced directly by the content of the answers, as opposed to a true understanding of the question. This suggests some natural measures of the influence of the choice text versus the question text.  A probabilistic view on these measures further suggest the NPSQ (for \"Normalized Probability Shift for the Question\") which normalizes the vanilla measure to account for different baselines.\n\nEmpirically, the paper explores the choice sensitivity and NPSQ on a small set of models and benchmark tasks.  Results show that a surprisingly high percentage of performance is attributable to choice sensitivity, suggesting both improved measures are needed (and, I will add, improved benchmark questions, although this is not argued for in the paper)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I liked this paper. I think it's well-written, addresses an interesting issue, primes other researchers for future work in this area, and makes non-obvious contributions to the literature.\n\n* Understanding choice sensitivity seems like an important issue in benchmark design.\n\n* The method of calculating choice sensitivity is natural and intuitive.\n\n* The empirical results convincingly show a wide variety of surprising behavior of LLMs wrt choice sensitivity.\n\n* The authors (generally) do a great job of writing, and highlighting important conclusions.\n\nDespite its weaknesses, I recommend acceptance."}, "weaknesses": {"value": "I think the biggest weakness of the paper lies in the presentation.\n\nI think Section 3 was beautiful.  It flowed naturally, the experiments were clean, and the authors did a great job of pulling out crisp conclusions.\n\nSection 4 was fine; it introduces NPSQ. It would have been nice to connect the mathematical notation in Section 3 to that in Section 4 a bit more directly -- it was VERY unclear what the \"score\" function in Section 3 was -- and since it was mentioned that \"log p(x|q,c)\" was part of it, it seems like there are some unstated notational overlaps between the sections.\n\nWhere things get muddled is Section 5.  I supposed I expected to see a clean comparison of NPSQ vs. the vanilla Choice Sensitivity in Section 3, but instead, the authors *also* introduced the idea of adversarial prompting.  This came out of nowhere, and (to me, at least) derailed the narrative flow.  I kind of see how it was designed to really skew the choice information, and therefore demonstrate how NPSQ was more robust the regular CS, but it was pretty unclear to me why this idea was introduced in Section 5 -- it seems like it should have been introduced earlier, or not at all.\n\nI of course wish that the authors had tested on a wider variety of LLMs.  I understand computational limitations and all, but still - it seems that, as a benchmarking paper that is entirely empirical, more could have been done."}, "questions": {"value": "* Why wasn't adversarial prompting introduced earlier (in sec 3?) and evaluated as part of the basic CS experiments?\n\n* I was interested to see how choice sensitivity decreases as a function of model size. It seems like you could have tested on 70/80b variants of several of your models; is there a reason you didn't?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YZZMDLUJH8", "forum": "LzpzC4gd4G", "replyto": "LzpzC4gd4G", "signatures": ["ICLR.cc/2026/Conference/Submission17078/Reviewer_mbsE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17078/Reviewer_mbsE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17078/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051069761, "cdate": 1762051069761, "tmdate": 1762927085153, "mdate": 1762927085153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}