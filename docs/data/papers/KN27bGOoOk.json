{"id": "KN27bGOoOk", "number": 13253, "cdate": 1758215690489, "mdate": 1759897452506, "content": {"title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability", "abstract": "Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts\nrepresented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success,\na rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the *inverse process of machine learning*, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) **Faithfulness:** whether the identified concept faithfully represents the neuron's underlying function and (2) **Stability:** whether the identification results are consistent across probing datasets.  We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with **BE** (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.", "tldr": "", "keywords": ["Mechannistic interpretability", "concept identification"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16a23625422c5e63504e1a96f46663b9e49f5478.pdf", "supplementary_material": "/attachment/480264fe67cdebd91b5086c81c4bb9ff6a927843.pdf"}, "replies": [{"content": {"summary": {"value": "This paper tackles neuron identification through an inverse-learning lens: instead of training a model to fit data, it ‚Äúidentifies‚Äù a concept ùëê that best matches a fixed neuron (or neuron set) ùëì. The authors formalize faithfulness as population-level similarity sim(ùëì,ùëê) and prove uniform generalization bounds showing that maximizing empirical similarity on a probe set yields near-optimal population similarity. They further introduce a stability notion via a bootstrap wrapper that produces prediction sets of candidate concepts with a provable coverage lower bound. The theory is instantiated for common similarity metrics (e.g., accuracy, AUROC, IoU), yielding rate expressions that translate into practical sample-size and metric-selection guidance. Empirically, they wrap standard neuron-identification methods (e.g., NetDissect/CLIP-Dissect‚Äìstyle pipelines) to illustrate the approach."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors identifie a genuine gap‚Äîneuron identification lacked a defensible definition of faithfulness‚Äîand provides a principled formulation together with a stability construct. Treating the task as an inverse problem is a fresh angle that connects interpretability with classical generalization theory. Moreover, it reframes neuron identification as an inverse problem with explicit statistical guarantees‚Äînovel for this literature.\n\nThe theory is sound and broadly applicable: uniform convergence results are presented for several practical similarity metrics, turning into actionable guidance on sample size and metric choice (e.g., dependence on concept frequency). The stability wrapper is clean, black-box, and yields interpretable prediction sets with coverage guarantees, enabling immediate integration with existing tools.\n\nOverall, I believe the contributions of this paper exceed acceptance bar. However, due to following weakness, the authors should fix some issues to be accepted."}, "weaknesses": {"value": "Empirical breadth and depth are insufficient.\n- Experiments are concentrated on a single architecture and a narrow set of baselines; this limits external validity. Including diverse backbones (e.g., ViT/ConvNeXt, larger CNNs) and multiple concept sources would better test generality.\n- The paper promises additional results in the appendix C, but I could not find no additional results at figure 6 of appendix C\n- The authors should include qualitative results on how faithfulness difference affects the neuron identification quality.\n\nCompute/efficiency analysis is missing.\n\n- The bootstrap wrapper introduces a multiplicative factor in runtime (by ùêæ resamples) and scales with the concept vocabulary size ‚à£ùê∂‚à£. There is no systematic study of wall-clock time, memory footprint, or accuracy‚Äìefficiency trade-offs (e.g., coverage vs. ùêæ, or performance vs. ‚à£ùê∂‚à£)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MRelOed5R9", "forum": "KN27bGOoOk", "replyto": "KN27bGOoOk", "signatures": ["ICLR.cc/2026/Conference/Submission13253/Reviewer_7iEL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13253/Reviewer_7iEL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630068086, "cdate": 1761630068086, "tmdate": 1762923932474, "mdate": 1762923932474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of neuron identification for explanation, where the lack of formal theoretical analysis undermines the trustworthiness and reliability of existing neuron explanation methods. To overcome this limitation, the paper introduces a theoretical framework that views neuron identification as the inverse process of machine learning, enabling the adaptation of tools from statistical learning theory to establish theoretical guarantees for the faithfulness and stability of neuron explanations. The paper further conducts synthetic simulations and experiments on real-world datasets using existing neuron identification approaches to verify their theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a theoretical analysis of neuron explanations, establishing formal bounds for various concept‚Äìneuron similarity metrics to ensure faithfulness, and deriving concept prediction probabilistic guarantees for stability.\n2. The idea of treating neuron identification as an inverse machine learning process is a novel insight that enables the adaption of the generalization theory and helps justify the reliability of neuron explanations."}, "weaknesses": {"value": "1. Although the paper presents explicit theoretical analyses, the empirical validation is limited and does not sufficiently demonstrate the effectiveness of the proposed theorems. For faithfulness, only simple binary cases and synthetic simulations are provided; for stability, the paper includes only two visualization examples, and the additional results in the Appendix appear the same to those in the main paper.\n2. The paper would benefit from a more comprehensive empirical verification, including comparisons between theoretical and experimental results on real-world datasets, with quantitative and qualitative evaluations. Such analyses would substantially strengthen the support for the theoretical claims.\n3. Several formal definitions of the theorems are missing, such as the convergence rate function. In addition, the simulation study details for experiments 1 and 2 are not provided in the main paper or the Appendix."}, "questions": {"value": "1. How scalable are the similarity metrics listed in the paper? How would the proposed framework extend to other metrics, such as WPMI [1] and MAD [2]?\n2. Could the paper discuss how the theoretical guarantees for neuron identification will regularize or guide the development of future neuron explanation methods?\n\n[1] CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. ICLR 2023.\n\n[2] CoSy: Evaluating Textual Explanations of Neurons. NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DxVLa7m01a", "forum": "KN27bGOoOk", "replyto": "KN27bGOoOk", "signatures": ["ICLR.cc/2026/Conference/Submission13253/Reviewer_Uu2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13253/Reviewer_Uu2E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792955249, "cdate": 1761792955249, "tmdate": 1762923932124, "mdate": 1762923932124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Recent advances, such as Network Dissection and CLIP Dissection, have achieved remarkable progress in neuron identification. However, these approaches still lack a theoretical foundation for ensuring faithful and stable explanations. The authors observe that neuron identification is similar to the inverse process of machine learning. Building on this insight, authors derive theoretical guarantees for neuron-level explanations and present theoretical analyses addressing the challenges of (1)Faithfulness and (2)Stability in neuron interpretation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work presents a novel and insightful perspective by interpreting the neuron identification problem as an inverse process of machine learning. This conceptual shift provides a fresh way to reason about how neurons emerge and can be systematically explained within learned models.\n- This work strengthens the theoretical foundation of neuron interpretation by addressing both faithfulness and stability. It provides analytical support for the reliability of neuron-level explanations and offers theoretical clarity beyond empirical observation."}, "weaknesses": {"value": "- This work concludes that CLIP Dissect tends to identify more abstract concepts, whereas NetDissect captures more concrete ones. However, it remains unclear whether the probing set and concept set used for this comparison are consistent across both methods. Specifically, CLIP Dissect employs images from the model‚Äôs training data as the probing set and utilizes a concept set of approximately 2K words, while NetDissect uses the Broden dataset, which includes segmentation mask annotations but contains far fewer samples. Therefore, when applying the proposed Bootstrap Ensemble procedure under the original settings of each method, the resulting bootstrap datasets and outcomes are unlikely to be derived from identical conditions. A clarification on this point would strengthen the validity of the comparison.\n- The paper introduces five similarity metrics, accuracy, AUROC, IoU, recall, and precision, as general evaluation measures. However, the applicability of each metric highly depends on the label type of the probing set and concept set. For example, IoU typically requires pixel-level ground-truth segmentation masks, making it suitable for NetDissect (which uses the Broden dataset) but not directly applicable to CLIP-Dissect, which lacks such annotations. Therefore, it would be beneficial to clarify whether different metrics are selectively used depending on the label types of the probing and concept sets if all five metrics are claimed to be generally applicable, to provide additional explanation on how each metric is adapted to different concept identification methods."}, "questions": {"value": "- How broad is the applicability of the approach (e.g., beyond vision, beyond single neurons, beyond simple concept sets)?\n- The focus is on individual neurons and concept alignment. But neurons may be polysemantic. Or representations may be distributed. How does that affect the validity of using single-neuron explanations?\n- The derivation of generalisation bounds for metrics such as IoU and AUROC is an interesting contribution. Are the bounds tight/meaningful in practice?\n- What are the implications of the proposed method for downstream interpretability applications (e.g., model auditing, bias detection, editing networks)?\n- How many neurons/explanations were evaluated in the empirical section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RouKpjq2bl", "forum": "KN27bGOoOk", "replyto": "KN27bGOoOk", "signatures": ["ICLR.cc/2026/Conference/Submission13253/Reviewer_iGbW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13253/Reviewer_iGbW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13253/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925149531, "cdate": 1761925149531, "tmdate": 1762923931752, "mdate": 1762923931752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}