{"id": "7FNcl66rFi", "number": 14753, "cdate": 1758243077327, "mdate": 1759897351213, "content": {"title": "The challenge of hidden gifts in multi-agent reinforcement learning", "abstract": "Sometimes we benefit from actions that others have taken even when we are\nunaware that they took those actions. For example, if your neighbor chooses not\nto take a parking spot in front of your house when you are not there, you can\nbenefit, even without being aware that they took this action. These “hidden gifts”\nrepresent an interesting challenge for multi-agent reinforcement learning (MARL),\nsince assigning credit when the beneficial actions of others are hidden is non-trivial.\nHere, we study the impact of hidden gifts with a very simple MARL task. In this\ntask, agents in a grid-world environment have individual doors to unlock in order\nto obtain individual rewards. As well, if all the agents unlock their door the group\nreceives a larger collective reward. However, there is only one key for all of the\ndoors, such that the collective reward can only be obtained when the agents drop the\nkey for others after they use it. Notably, there is nothing to indicate to an agent that\nthe other agents have dropped the key, thus the act of dropping the key for others is\na “hidden gift”. We show that several different state-of-the-art MARL algorithms,\nincluding MARL specific architectures, fail to learn how to obtain the collective\nreward in this simple task. Interestingly, we find that decentralized actor-critic\npolicy gradient agents can solve the task when we provide them with information\nabout their own action history, but MARL agents still cannot solve the task with\naction history. Finally, we derive a correction term for these policy gradient agents,\ninspired by learning aware approaches, which reduces the variance in learning and\nhelps them to converge to collective success more reliably. These results show\nthat credit assignment in multi-agent settings can be particularly challenging in\nthe presence of “hidden gifts”, and demonstrate that self learning awareness in\ndecentralized agents can benefit these settings.", "tldr": "Hidden gifts (unobserved cooperation) break MARL credit assignment; in Manitokan task, SOTA methods fail, while a decentralized learning-aware policy-gradient correction with action history reduces variance and achieves collective success.", "keywords": ["Multi-agent Reinforcement Learning", "Learning Awareness", "Mixed Motive Games"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c5d220a0eaaaf43c15a2a5dc398d3f55f2d2486.pdf", "supplementary_material": "/attachment/1ca8edb71fda68567a309fa1a8e40c6e25dadb96.zip"}, "replies": [{"content": {"summary": {"value": "The paper considers the credit assignment problem in multi-agent RL by proposing a grid-world game where agents collect a key to open doors. Opening individual doors gives rewards to corresponding agents, and opening all doors gives a collective reward to all agents. Agents can drop keys so that other agents can collect the key to open their own door. The authors empirically ran a number of prior MARL baselines and found that they did not learn to drop the key. They then derive a corrective term for this game and empirically show that this term improves learning success."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Credit assignment in MARL is an important and difficult problem.\n- Proposing new games to crystallize the problem can be a valuable approach.\n- A number of MARL algorithms and ablations were run on the proposed game, establishing diverse baselines.\n- The paper attempts to give formal explanations, which is nice (but see formal errors, below).\n- The paper is well written."}, "weaknesses": {"value": "Unfortunately, the paper contains some important formal errors that invalidate several key claims:\n\n1) In Section 3, the game is formalized as a Dec-POMDP, but the formalization is not correct:\n\n1a: The state is defined as the combined observations of agents. However, observations are local 3x3 tiles, which collectively may not reveal the true state (for example, if all agents are in the same corner, their combined observations may not show the doors and key in the opposite corner).\n\n1b: The reward function in Eq(1) defines individual agent rewards, but Dec-POMDPs assume a single shared reward for all agents. Thus the appropriate game model is the Partially Observable Stochastic Game (see the MARL book by Albrecht et al.). Such games require equilibrium analysis rather than maximizing joint returns of agents. Moreover, the conditions in the definition of (1) are not mutually exclusive, meaning that both conditions may be true at the same time (so the function is ill-defined).\n\n2) The proofs for section 5 contain basic errors. In appendix P1:\n\n2a: The gradient of the collective objective for agent j is written in score-function form and then immediately factorized as\n\nE[∇Θj​​logπ~cj​(aj​∣oj​)]E[Qc​(oj​,aj​)]\n\nin Eq(14). The text justifies this by claiming the agents’ policies are “probabilistically independent.” But independence of policies across agents does not imply independence between the score ∇Θj​​logπcj​(aj​∣oj​) and the value Qc​(oj​,aj​) for the same agent and the same distribution, as Qc depends on aj and oj. Even if one (incorrectly) allowed the factorization, the first factor is the expected score, which equals zero under the sampling distribution: Ea∼π​[∇Θ​logπ(a∣o)]=0. The proof then uses this expectation as a denominator in Eq(15), making this ill-defined due to zero-division.\n\n2b: Circular “proof” that the problematic denominator is non-zero: Appendix P.3 argues by contradiction that if the collective reward is absent, the correction term must be zero. After Eq(30), it is claimed that the denominator cannot be equal to zero since it was a denominator, which is circular. It assumes that the division was valid to begin with, which is not correct."}, "questions": {"value": "Q1: The proposed correction (ignoring the formal errors; see above) seems very specific to the particular game proposed. To what extent could this generalize to other games? I think the paper could be strengthened if broader applicability and usefulness of such terms could be demonstrated.\n\nQ2: Have you tried MARL algorithms that aim to learn collaborative strategies in games such as the ones you proposed? For example, Pareto-AC (https://arxiv.org/pdf/2209.14344) can successfully learn in similar games, such as the \"Boulder game\" used in their experiments.\n\nQ3: I think the fact that all the MARL algorithms did not learn to drop they key requires a more in-depth analysis of the root causes. I have not checked the code, but I wondered whether there might be a bug where the \"key\" state is not reset after episodes? In this case, it might be rational for agents to learn to want to keep the key, since it will enable quicker rewards in subsequent episodes? In general, given the POSG setting (as opposed to Dec-POMDP; see above), I think the analysis should focus on what is individually rational for agents rather than collectively rational. It may reveal new angles on the problem analysis.\n\nQ4: Given issue 1b, how did the authors apply MARL algorithms that strictly assume identical rewards between agents, such as VDN and QMIX?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oosBld0GO3", "forum": "7FNcl66rFi", "replyto": "7FNcl66rFi", "signatures": ["ICLR.cc/2026/Conference/Submission14753/Reviewer_TeS5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14753/Reviewer_TeS5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760995251082, "cdate": 1760995251082, "tmdate": 1762925111847, "mdate": 1762925111847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new learning task that focuses specifically on the “hidden” collaboration behaviours (dubbed “hidden gifts”) in the multi-agent reinforcement learning. It evaluates several existing MARL algorithms on this task and shows that all of them failed to learn meaningful policies. The paper then proposes a new policy gradient theorem that has an extra correction term and has been shown to work well on this learning task."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes an interesting learning task for multi-agent reinforcement learning (MARL): Manitokan task. This task emphasizes specifically on the collective behaviour among agents (called the hidden gifts). The paper conducted a relatively comprehensive evaluation of mainstream MARL methods and reported their performance on this task. \n\nThe paper is well written and easy to read. All results are presented in a clear and understandable way."}, "weaknesses": {"value": "The formal analysis is not rigorous and the contributions are unclear\n### Lacks of rigour\n1. The expectations in all equations are not specified or clearly defined, which makes some equations suspiciously wrong (or with factual errors). For example, Eq. (2) is inconsistent with the definition of reward in Eq. (1), the latter of which specifies two cases but the former just sums up these two cases; Eq. (3) appears to be incorrect if the expectation involves the summation over action space; in Theorem 1, $\\Psi$ is defined as the expected value of reciprocal, which is a gradient with respect to $\\Theta$. What does it mean for a reciprocal of a vector? \n\n2. Most results relies on the (possibly incorrect) configurations of the test environments. Specifically, the Dec-POMDP might not be correctly defined in the first place. At the least the reward function in Eq. (1) shows that $\\mathcal{R}$ is not only a function of $o_t$ and $a_t$ but also a function of the status of all doors. Second, the results in Section 4.1 and Section 4.2 essentially verify that the configurations of the environment were not correct initially: that the Dec-POMDP formulation couldn’t be used to maximize desired rewards, which was later confirmed by the results in Section 4.3. However, Section 4.3 only considers a very simple improvement: adding the last action as a one-hot vector (why not using the full action history?). \n\n### Unclear contributions\n1. There is no in-depth analysis into any MARL algorithms or their poor performance Among all these results, only the success rate and cumulative rewards are reported, which can hardly be used to analyse why these algorithms failed to learn. For example, if the last action is added as extra info, the paper should at least verify whether such last action is really being used by the policy learning with a GRU unit (whether more previous actions are needed?) Also, the reward for each agent is dependent on the order when the agent picks up the key: if the first one, then reward is small; if the last one, the reward is then increased. This would affect the value estimation as well: the paper should then check if such order info has been provided in value approximation? \n \n2. The empirical results provided in Figure 5 are obtained from training with significantly more episodes than that in Figure 2, 3, and 4. This makes the results in Figure 2, 3, and 4 less convincing since the early stopping of the training can lead to completely different conclusions. For example, if the training terminates at 7500 episodes in Figure 5, then success rate and cumulative reward would be very similar to those reported in other figures."}, "questions": {"value": "1. What’s the definition of expectation in Eq. (2), (3), (4) and (5)? \n2. Why the reward in Eq.(2) is the sum of two terms? (the reward was previously defined as the piecewise function)\n3. the $\\Psi$ in Theorem 1 is not correctly defined: the reciprocal of vector makes no sense."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rv8uCSijbl", "forum": "7FNcl66rFi", "replyto": "7FNcl66rFi", "signatures": ["ICLR.cc/2026/Conference/Submission14753/Reviewer_5tWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14753/Reviewer_5tWK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752970541, "cdate": 1761752970541, "tmdate": 1762925111367, "mdate": 1762925111367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why state-of-the-art multi-agent reinforcement learning (MARL) algorithms struggle on certain cooperative tasks, focusing on a simple yet challenging benchmark called the Manitokan task. The authors argue that this task reveals failures in credit assignment—the ability of agents to attribute collective outcomes to individual actions. To study this, they introduce a minimal domain that supports both mathematical and empirical analysis. They observe that most MARL algorithms, including MAPPO, COMA, and QMIX, fail to make learning progress on Manitokan, whereas single-agent baselines and decentralized agents with action history perform better. The authors also derive a self-correction term motivated by their theoretical analysis of policy gradients in this environment, which is intended to address learning inefficiencies observed in their experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem is clearly motivated and addresses an important gap in our understanding of MARL failure modes.\n- Several empirical findings are interesting, including that most algorithms tested struggle to make learning progress on Manitokan, and that adding action history helps decentralized agents but not MARL agents.\n- I appreciate the approach of proposing a minimal, analytically tractable toy problem and performing both theoretical and empirical analysis within that setting."}, "weaknesses": {"value": "- The claim that MARL algorithms struggle on Manitokan primarily due to credit assignment issues is not convincingly substantiated. The paper does not clearly define “credit assignment,” offering only intuitive examples. Nor does it show how such difficulties mathematically impact the gradients or losses of the algorithms studied. Alternative explanations—such as biased gradients in MAPPO or the presence of local minima—were not ruled out. No empirical probes were conducted to directly verify that credit assignment was the key bottleneck.\nThere is no clear evidence that “hidden gift” type problems, in general, pose unique challenges for MARL algorithms beyond the specific Manitokan setup.\n- The evidence that state-of-the-art MARL algorithms cannot solve Manitokan is limited. In Figure 4, these algorithms experience a decrease in reward before 8K episodes, but Policy Gradient—tested out to 24K episodes—shows the same initial drop before eventually achieving a high success rate. It seems plausible that MAPPO, COMA, or QMIX might have reached similar or higher success rates if trained for longer.\n- The significance of the self-correction term is unclear. It appears to rely on assumptions specific to Manitokan, leaving open whether it generalizes to other domains. Moreover, it does not appear to substantially improve success rates. While it may slightly reduce return variance, the paper provides no confidence intervals (e.g., in Fig. 5c) to assess whether this effect is statistically meaningful.\n- The plots use colors that are difficult to distinguish, especially in Figure 5, where the lines for Vanilla and Self-Correction are nearly indistinguishable.\n- The mathematical analysis lacks intuitive interpretation. For example, what is the meaning of Equations 3–5, and how do they concretely relate to the credit assignment problem? What is the intuition behind the correction term, and why should it improve learning?\n- The theoretical section also omits key definitions and justifications. For instance, what precisely is meant by a sub-policy? This concept does not seem to be rigorously defined, either in the paper or in the cited work by Sutton. Several assumptions in the derivations are also stated without explanation or justification, making it difficult to verify correctness."}, "questions": {"value": "- Regarding the correction term: is the claim that the gradients computed by standard policy gradient are biased or otherwise incorrect? What precisely is being “corrected”?\n- How do you formally define credit assignment in your framework?\n- What is a sub-policy, and how does it differ from an agent’s standard policy?\n- What is the conceptual significance of Equation 3?\n- In your derivation, how are the expectations over the log-policy term and the Q-function separated? It may help to explicitly state what each expectation is taken with respect to.\n- In Equation 16, is there a missing equals sign or continuation marker? It’s unclear whether this is part of the preceding expression."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n5yv0F0dyl", "forum": "7FNcl66rFi", "replyto": "7FNcl66rFi", "signatures": ["ICLR.cc/2026/Conference/Submission14753/Reviewer_turr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14753/Reviewer_turr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757346678, "cdate": 1761757346678, "tmdate": 1762925110906, "mdate": 1762925110906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"Manitokan task,\" a simple cooperative grid-world environment designed to study the problem of \"hidden gifts.\" In this task, agents must share a single key to open their individual doors, but the act of one agent dropping the key (the \"gift\") for another is not directly observable. The authors show that several state-of-the-art MARL algorithms (including VDN, QMIX, COMA, and MAPPO) fail to solve this task, often collapsing to policies with zero cooperative behavior. The paper finds that a simple decentralized Policy Gradient (PG) agent, when provided with its own action history, can learn the task, albeit with high variance. The authors then derive a \"Self Correction\" term, inspired by learning-aware approaches, which is shown to reduce this variance and improve convergence to the cooperative solution."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Problem Formulation**: The paper identifies an interesting and potentially challenging problem in MARL: credit assignment for cooperative actions that are not immediately or directly observable by the beneficiaries.\n\n- **Empirical Analysis of Baseline Failures**: The demonstration that a wide range of standard MARL baselines fails on this seemingly simple task is a valuable finding. The analysis of this failure (e.g., collapse of key-dropping behavior) provides a clear motivation for the work.\n\n- **Proposed Solution**: The derived \"Self Correction\" term, while simple, is empirically shown to be effective. The results in Figure 5 demonstrate that this term reduces variance and improves the collective success rate compared to vanilla PG and a max-entropy baseline. Although this experiment by itself is not robust to make the claim."}, "weaknesses": {"value": "This paper, while presenting an interesting idea, suffers from several major weaknesses in its problem formulation, experimental setup, and theoretical grounding.\n\n- **Unclear Definition of \"Hidden Gift\"**: The central premise of the \"hidden gift\" is unclearly defined and potentially flawed. The paper states \"the act of dropping the key is not actually observable\" (line 189). However, it is not easy to know the experimental setup if:\n  - (a) Only the action $a_{\\text{drop}}$ is hidden from other agents' observations $o^j$, but the resulting state change (i.e., the key appearing on the ground at $s'$) is observable?\n  - (b) The resulting state change is also hidden (e.g., the key is invisible to agent $j$ even after agent $j$ picks it up)?\n\nIf (a), then this is not a new \"hidden gift\" problem but a standard MARL problem of non-stationarity and multi-step temporal credit assignment, where the state change is induced by another agent's policy. If (b), this is an extremely strong partial-observability assumption that needs much clearer justification.\n\n- **Extreme and Unjustified Partial Observability**: The initial experiments (Sec 4.1) appear to be conducted under extreme partial observability, where \"agents did not have an explicit signal for their door being opened or that they are holding the key\" (lines 268-270). This lack of basic proprioceptive state (knowing if one is holding the key or has completed one's own task) makes the problem artificially difficult and is a highly unusual assumption for MARL environments. This should be made far clearer and be justified.\n\n- **Missing Key Baselines and Related Work**: The paper's primary claim is that existing MARL algorithms fail at this credit assignment problem. However, it omits a large and highly relevant body of work specifically designed to address non-stationarity and credit assignment in CTDE settings.\n\n  - The paper notes issues like \"conflicting gradient updates\" and the risks of individualized rewards (lines 114) but fails to compare against methods built to solve these exact problems, such as HAPPO [1], MAT [2], A2PO [3], and Partial Reward Decoupling (PRD) [4,5].\n\n  - The problem could also be framed as one of reward redistribution, but relevant methods (e.g., AREL [6], STAS [7], TAR$^2$ [8, 9]) are not discussed or compared.\n\n  - The novelty claim regarding the \"transfer of tangible, task-critical resources\" (line 123) seems to overlook existing work in environments like Overcooked, where agents must pass items to each other.\n\n- **Overstated Conclusions and Contradictory Results**:\n\n  - The paper's conclusion (line 331) that the problem \"can be addressed by the standard policy gradient objective, but not fancier trust region mechanisms\" (i.e., PPO) is a significant overstatement based on the provided evidence. The failure of MAPPO/IPPO could be due to many factors.\n\n  - There is a direct contradiction in the results of Section 4.2. Lines 295-297 state that \"MARL agents (MAPPO, QMIX, COMA) showing total collapse.\" However, lines 298-299 state that \"only MAPPO and decentralized PG showed any learning in the task.\" These two statements cannot both be true.\n\n- **Simplicity of the Environment**: The Manitokan task is a very small and simple grid world. The failure of strong, modern baselines like MAPPO on such a trivial task is surprising. This may suggest that the failure is not due to a fundamental \"hidden gift\" challenge, but rather to the extreme POMDP assumptions (Weakness #2) or insufficient hyperparameter tuning. The paper's claims would be much stronger if demonstrated in a more complex environment.\n\n- **Invalid Theoretical Analysis**: The \"Self Correction\" term is presented as a formally derived component, but the analysis in Section 5 and Appendix P appears to be an ad-hoc justification for a heuristic. The derivation is based on a series of steps that are not easy to follow (seems flawed) and invalid assumptions.\n\n  - **Appendix P.1 (Deriving the Correction Term)**: This proof is invalid. It makes an incorrect independence assumption to split the expectation in Eq. 13-14 (the score function $\\nabla \\log \\pi$ and the Q-value $Q_c$ are not always independent. Under what conditions are they independent?). Furthermore, the derivation does not actually produce the new Hessian term; the term is simply inserted into the final objective (Eq. 22) without a valid preceding derivation.\n\n  - **Appendix P.2 (Deriving the \"Self\" Correction)**: This \"proof,\" which is critical for the algorithm's decentralization, is fundamentally flawed. The authors attempt to justify replacing the cross-agent Hessian with a self-Hessian by claiming $\\mathbb{E}[Q_c^i] \\approx \\mathbb{E}[Q_c^j]$. This is a consequence. Proving that both agents' expected collective rewards are related to the same global $r^c$ does not in any way prove that their respective gradient/Hessian terms are equivalent.\n\n  - **Confusing Claims and Typos**: The analysis is further muddied by confusing, unsupported claims (e.g., the \"inverse entropy\" relationship, line 375) and clear typos (e.g., the missing summation in Equation 8, line 1914), which makes the entire section lack rigor.\n\n[1] Kuba, Jakub Grudzien, et al. \"Trust region policy optimisation in multi-agent reinforcement learning.\" arXiv preprint arXiv:2109.11251 (2021).\n\n[2] Wen, Muning, et al. \"Multi-agent reinforcement learning is a sequence modeling problem.\" Advances in Neural Information Processing Systems 35 (2022): 16509-16521.\n\n[3] Wang, Xihuai, et al. \"Order matters: Agent-by-agent policy optimization.\" arXiv preprint arXiv:2302.06205 (2023).\n\n[4] B. Freed, A. Kapoor, I. Abraham, J. Schneider and H. Choset, \"Learning Cooperative Multi-Agent Policies With Partial Reward Decoupling,\" in IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 890-897, April 2022\n\n[5] Kapoor, Aditya, et al. \"Assigning credit with partial reward decoupling in multi-agent proximal policy optimization.\" arXiv preprint arXiv:2408.04295 (2024).\n\n[6] Xiao, Baicen, Bhaskar Ramasubramanian, and Radha Poovendran. \"Agent-temporal attention for reward redistribution in episodic multi-agent reinforcement learning.\" arXiv preprint arXiv:2201.04612 (2022).\n\n[7] Chen, Sirui, et al. \"STAS: spatial-temporal return decomposition for solving sparse rewards problems in multi-agent reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 16. 2024.\n\n[8] Kapoor, Aditya, et al. \"Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning.\" arXiv preprint arXiv:2412.14779 (2024).\n\n[9] Kapoor, Aditya, et al. \"$ TAR^ 2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning.\" arXiv preprint arXiv:2502.04864 (2025)."}, "questions": {"value": "- **Clarity on Observability (W1)**: Could you please precisely define what is \"hidden\" in the Manitokan task? When agent $i$ drops the key, is the key's new location on the ground immediately observable in agent $j$'s partial observation (if agent $j$ is looking in that direction)?\n\n- **Clarity on Value Function Inputs (W1)**: For the centralized training baselines (MAPPO, COMA, QMIX), are the centralized critics/value functions conditioned on local action-observation histories or the full global state? If they use the global state, how can the \"gift\" be considered hidden during training?\n\n- **Success Rate vs. Reward**: In Section 4.1 and Appendix E.4, you note that randomizing the policy can improve the collective success rate but reduce the cumulative reward. What does this imply about the alignment of the cumulative reward objective (which includes individual rewards) and the desired collective task?\n\n- **Basis for Claim (Sec 4.1)**: The claim that agents in MAPPO/IPPO \"were still successfully opening their individual doors\" (lines 254-2555) is used to explain their higher cumulative reward. Is this a speculation, or is it an observation from evaluating the converged policies (e.g., what percentage of the time did they open their own door)?\n\n- **Contradiction (Sec 4.2)**: Could you please resolve the contradiction in Section 4.2 regarding MAPPO's performance (lines 295-297)? Did it show \"total collapse\" or \"any learning\"?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bMvgOvDGRF", "forum": "7FNcl66rFi", "replyto": "7FNcl66rFi", "signatures": ["ICLR.cc/2026/Conference/Submission14753/Reviewer_ZmqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14753/Reviewer_ZmqS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939334759, "cdate": 1761939334759, "tmdate": 1762925109940, "mdate": 1762925109940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}