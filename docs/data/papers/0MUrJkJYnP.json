{"id": "0MUrJkJYnP", "number": 22896, "cdate": 1758336861624, "mdate": 1759896840883, "content": {"title": "A Fine-Grained Analysis of Pure Semantic Preference Alignment in Large Language Models", "abstract": "Large language models (LLMs) are typically aligned with human preferences through methods such as direct preference optimization (DPO). While empirically successful, these approaches face well-known limitations, including length bias, reward hacking, binary preference assumptions, and the aggregation of heterogeneous preferences into a single scalar signal. In this work, we take an inverse perspective: rather than attempting to resolve these issues, we investigate an idealized setting, which we call the *pure semantic preference scenario*, where such confounding factors are absent. We show that even in this idealized setting, existing alignment methods still do not fully capture the preference. Our analysis further reveals that (i) on-policy algorithms align more effectively, (ii) models trained without an explicit reference model perform better, and (iii) preference-model–based approaches consistently outperform reward-model–based approaches. Motivated by these observations, we introduce *preference matching optimization* (PMO), a DPO-type method that admits a closed-form solution and provably better approximates the true preference distribution. Experiments on both practical and idealized settings demonstrate that PMO achieves comparable performance with existing alignment methods in the practical setting, while offering stronger theoretical grounding and better performance in the pure semantic setting.", "tldr": "", "keywords": ["Large Language Models", "Reference Alignment", "Human Feedback"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2bd2ecad3dbaf5d35aaebac83de1e946fce8bd2.pdf", "supplementary_material": "/attachment/ab45568a47706a688ec1da3c9239ed1139ed95ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper looks at the behavior of preference alignment algorithms in an idealized \"pure semantic preference scenario,\" which is a synthetic dataset designed to isolate semantic choices from confounding factors like length bias, reward hacking, and binary preference assumptions. In practice, this is a synthetic dataset constructed so only one word is changed between the two options.\n\nThe authors show that even in this controlled setting, existing alignment methods (like DPO and PPO-based RLHF) do not optimally recover the ground-truth probabilistic preferences. Their analysis highlights three findings: (i) on-policy algorithms are more effective at this task, (ii) reference-free methods (like SimPO) perform better than reference-based ones (like DPO), and (iii) what they term \"preference-model-based\" approaches (e.g., DPO) outperform \"reward-model-based\" ones (e.g., PPO-RLHF).\n\nThe paper identifies a preference-accuracy trade-off, where methods that better align with the probabilistic preferences on the synthetic task show reduced accuracy on standard knowledge benchmarks (e.g., MMLU), and vice-versa. The authors attribute this trade-off to the distorting influence of the reference model (pi_ref) used in methods like DPO.\n\nTo address this, they propose Preference Matching Optimization (PMO), a new DPO-style objective. It is derived from an RL objective that combines both KL-divergence (to pi_ref) and entropy regularization. The resulting closed-form loss (Eq 5) resembles DPO but with an attenuated reference model term, which should allow PMO to balance probabilistic preference matching with accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- An initial analysis on the simple, synthetic dataset is a good scientific approach to remove confounders\n- The paper clearly identifies preference-accuracy trade-off using their synthetic task.\n- The proposed PMO method is well-motivated. It is not an ad-hoc objective but is derived (Proposition 4.2) from a clear RL objective (Eq 4) that explicitly combines entropy and KL regularization.\n- The ablation studies on the α (entropy) and β (KL) hyperparameters (Table 1 and 3) clearly show how they affect accuracy and KL metrics"}, "weaknesses": {"value": "- The paper evaluates \"accuracy\" using knowledge-based, multiple-choice benchmarks (MMLU, ARC, HellaSwag). These benchmarks are not great for evaluating the alignment performance of an algorithm. An alignment algorithm's success is measured by its ability to follow human preferences in generative, open-ended tasks. The authors should have evaluated their method on standard alignment benchmarks like AlpacaEval2 or ArenaHard. Without this, we only know that PMO doesn't degrade knowledge task performance, but we have no evidence that it leads to a better-aligned model in practice, which is the entire goal.\n\n- The core idea of using entropy regularisation is not new. SimPO, which the authors compare against, is a reference-free model, which Proposition 4.1 shows is equivalent to a maximum-entropy formulation. The authors also discuss H-DPO in the appendix (and mention it in the main text as well), which explicitly adds an entropy term to the DPO objective. The main contribution seems to be the specific αH + βKL formulation. Given the prior work, it is unclear if this specific variant is a significant enough contribution over existing methods like SimPO and H-DPO.\n\n- The finding that on-policy algorithms perform better is acknowledged by the authors as mirroring \"broader evidence\" (line 311), though this evidence is not cited.\n\n- The preference-accuracy trade-off is also discussed in papers like [1], which also investigates the role of entropy. The discussion on the role of regularisation (Section 4.2) would benefit from citing work like [2] where similar discussions are already present.\n\n[1] Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints, https://arxiv.org/abs/2309.16240\n\n[2] Meta-Learning Objectives for Preference Optimization, https://arxiv.org/pdf/2411.06568"}, "questions": {"value": "- Why did the authors choose to evaluate accuracy on knowledge-based benchmarks (MMLU, etc.) instead of established alignment benchmarks (AlpacaEval2, ArenaHard)? The latter seems far more relevant for a paper proposing a new alignment algorithm.\n\n- Distinction from H-DPO: The appendix (B.2) mentions H-DPO but claims the \"underlying principles differ.\" However, the H-DPO objective (Eq 8 in their paper) and PMO's RL objective (Eq 4) both combine a reward term, an entropy term, and a KL/cross-entropy term. Why do the principles differ?\n\n- The paper distinguishes between \"preference-model-based\" (DPO-like) and \"reward-model-based\" (PPO-RLHF) approaches. This terminology is not standard. A brief, explicit definition in the introduction would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VyGPRT3kMC", "forum": "0MUrJkJYnP", "replyto": "0MUrJkJYnP", "signatures": ["ICLR.cc/2026/Conference/Submission22896/Reviewer_1Uc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22896/Reviewer_1Uc7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771743362, "cdate": 1761771743362, "tmdate": 1762942429875, "mdate": 1762942429875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an idealized evaluation setting for preference alignment, termed the Pure Semantic Preference (PSP) scenario. The goal is to isolate semantic alignment quality from confounding factors such as response length, syntactic variations, and stylistic biases. Within this framework, the authors systematically analyze the behavior of existing alignment algorithms. To address the limitations observed, they propose a new method called Preference Matching Optimization (PMO), which integrates entropy regularization with a KL-divergence–based objective to better approximate the target preference distribution. Experimental results suggest that PMO achieves comparable performance to existing methods while offering improved probability fidelity and reduced preference collapse (PCI) in the PSP setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "By analyzing alignment behavior under a controlled “semantic-only” setup, the paper offers valuable insights into what current preference optimization methods are actually learning."}, "weaknesses": {"value": "1. All experiments are conducted on relatively small models (~1B parameters). It remains unclear whether the findings hold for larger models such as 7B or 14B.\n\n2. While PPO and NashMD are briefly mentioned, the paper focuses primarily on off-policy approaches. The implications for practical on-policy RLHF remain underexplored.\n\n3. The paper emphasizes PCI as a major issue, but it is not evident that preserving uncertainty is always desirable—certain tasks may actually benefit from more decisive behavior.\n\n4. Lack of evidence linking KL improvement to preference quality. Although PMO shows better KL/PCL metrics, it is not demonstrated that these necessarily translate to improved human-aligned outcomes. It would strengthen the paper to show in which types of semantic preference tasks PMO better captures human intent.\n\n5. On real-world benchmarks, the authors only report accuracy. Including additional measures would give a more complete view of alignment performance."}, "questions": {"value": "see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZRvUYeRl3y", "forum": "0MUrJkJYnP", "replyto": "0MUrJkJYnP", "signatures": ["ICLR.cc/2026/Conference/Submission22896/Reviewer_mVq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22896/Reviewer_mVq1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905636522, "cdate": 1761905636522, "tmdate": 1762942429573, "mdate": 1762942429573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the fundamental limits of preference-based alignment methods like DPO under an idealized pure semantic preference setting, where confounding factors such as length bias and heterogeneous rewards are removed. The authors find that even in this setting, existing methods fail to fully capture true preferences. They show that on-policy training, removing the reference model, and using preference models lead to better alignment, and propose Preference Matching Optimization (PMO). PMO is a closed-form, DPO-style algorithm that more accurately approximates the true preference distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear and easy to follow.\n\n2. PMO offers provable advantages and interpretable formulation.\n\n3. Provides new insights into key factors affecting alignment."}, "weaknesses": {"value": "1. The authors mention that their main focus is on reward hacking issues for RLHF tasks like length bias. It's unclear to me how to resolve the reward hacking issues.\n\n2. I think it’s difficult to conclude that reliance on a reference model directly leads to differences in preference alignment or accuracy from Figure 2. A more convincing approach would be to control for models with comparable accuracy and then evaluate other alignment-related metrics.\n\n3. Moreover, since entropy and KL divergence losses are standard components in reinforcement learning algorithms, the technical contribution of this work appears somewhat limited."}, "questions": {"value": "Please refer weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a9u0KXw3YS", "forum": "0MUrJkJYnP", "replyto": "0MUrJkJYnP", "signatures": ["ICLR.cc/2026/Conference/Submission22896/Reviewer_8GDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22896/Reviewer_8GDx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952247716, "cdate": 1761952247716, "tmdate": 1762942428910, "mdate": 1762942428910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of a pure semantic preference setting, aiming to analyze how preference alignment methods like RLHF and DPO behave when non-semantic confounders (e.g., length, style, and structure) are removed. The authors propose a new variant called Preference Matching Optimization (PMO), which combines entropy and KL regularization in a DPO-style objective to better approximate human preference distributions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and logically organized, and the “pure semantic preference” idea is clearly articulated. It provides a clean testbed to isolate semantic alignment factors.\n\n2. The derivation of PMO is mathematically sound and shows a solid understanding of the relationships among DPO, SimPO, and RLHF.\n\n3. The experiments demonstrate a recurring trade-off between alignment and accuracy, lending support to the authors’ theoretical analysis."}, "weaknesses": {"value": "1. The *pure semantic preference* scenario is so contrived that it provides limited insight into realistic preference alignment. Real-world human judgments involve complex, multi-dimensional signals (tone, politeness, factuality, etc.), none of which are modeled here. As a result, the practical implications of the findings are unclear.\n\n2. The paper evaluates only small models (1B–3B), using synthetic datasets rather than genuine human preference data. No large-scale or qualitative studies are presented, making it difficult to assess whether PMO offers any tangible advantages in actual alignment pipelines.\n\n3.  The claimed benefits of PMO over existing methods (e.g., DPO, SimPO, CPO) are small and often within noise range. The benchmark experiments show similar or even slightly worse results in some cases, suggesting that PMO’s advantages may be overstated.\n\n4. PMO essentially interpolates between existing regularization techniques (entropy + KL), offering an incremental extension rather than a fundamentally new approach. The paper might be better suited as an empirical note rather than a full ICLR paper."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5dbnWRhFtD", "forum": "0MUrJkJYnP", "replyto": "0MUrJkJYnP", "signatures": ["ICLR.cc/2026/Conference/Submission22896/Reviewer_HRi2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22896/Reviewer_HRi2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213756518, "cdate": 1762213756518, "tmdate": 1762942428693, "mdate": 1762942428693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}