{"id": "gHFeLx0QX7", "number": 1376, "cdate": 1756877374817, "mdate": 1759898212011, "content": {"title": "DPsurv: Dual-Prototype Evidential Fusion for Uncertainty-Aware and Interpretable Whole Slide Image Survival Prediction", "abstract": "Pathology whole-slide images (WSIs) are widely used for cancer survival analysis because of their comprehensive histopathological information at both cellular and tissue levels, enabling quantitative, large-scale, and prognostically rich tumor feature analysis. However, most existing methods in WSI survival analysis struggle with limited interpretability and often overlook predictive uncertainty in heterogeneous slide images. In this paper, we propose DPsurv, a dual-prototype whole-slide image evidential fusion network that outputs uncertainty-aware survival intervals, while enabling interpretation of predictions through patch prototype assignment maps, component prototypes, and component-wise relative risk aggregation. Experiments on five publicly available datasets achieve the highest mean concordance index and the lowest mean integrated Brier score, validating the effectiveness and reliability of DPsurv. The interpretation of prediction results provides transparency at the feature, reasoning, and decision levels, thereby enhancing the trustworthiness and interpretability of DPsurv.", "tldr": "", "keywords": ["Survival Analysis", "Evidential Deep Learning", "Whole Slide Image", "Uncertainty Quantification", "Interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/758a754af40b0fadce6552d6d360ca90f39a1f5e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DPsurv, a novel framework for Whole Slide Image (WSI) survival prediction that aims to be both interpretable and uncertainty-aware. The method employs a \"dual-prototype\" architecture. First, it uses a Gaussian Mixture Model (GMM) on patch embeddings to learn \"patch prototypes\" that represent distinct morphological phenotypes, yielding a slide-level component embedding. Second, these component embeddings are mapped into an evidence space using \"component prototypes\" and Gaussian Random Fuzzy Numbers (GRFNs) to model aleatoric and epistemic uncertainty. The final prediction is a mixture of component-level evidence, resulting in a survival curve with uncertainty bounds. The authors claim the framework provides multi-level interpretability (feature, reasoning, and decision) and demonstrate state-of-the-art performance on five TCGA datasets in terms of both discrimination (C-index) and calibration (IBS, IBLL)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "$\\textbf{Significance of the Problem: }$ The work addresses two of the most critical challenges in clinical deep learning for computational pathology: model interpretability and uncertainty quantification. A reliable solution in this space would have a significant clinical impact.\n\n$\\textbf{Novelty and Comprehensive Framework: }$ The proposed dual-prototype architecture that models feature-phenotypes and survival evidence separately is a novel concept. The framework provides a comprehensive, end-to-end solution for multi-level interpretability, from patch-level features to component-wise risk contributions, which is a major step beyond simple attention heatmaps. \n\n$\\textbf{Excellent Empirical Performance: }$ The method demonstrates state-of-the-art performance across five public TCGA cohorts, achieving the best average C-index for discrimination and, most notably, the best average IBS and IBLL for calibration.  The substantial improvement in calibration metrics is a particularly strong result.\n\n$\\textbf{Strong Qualitative Interpretability: }$ The visualizations in Figure 2, supported by pathologist review, provide compelling qualitative evidence that the learned prototypes correspond to meaningful histopathological patterns and that the model's reasoning aligns with clinical knowledge."}, "weaknesses": {"value": "$\\textbf{Extreme Complexity and Hyperparameter Sensitivity: }$ The proposed framework is exceptionally complex, combining a foundation model, GMMs, two levels of prototypes, and the evidential framework of GRFNs. The authors themselves acknowledge in the limitations section that the model's performance is sensitive to the number of patch and component prototypes. The hyperparameter table confirms that the number of component prototypes (K) was manually tuned for each dataset, which undermines the robustness and generalizability of the approach. \n\n$\\textbf{Crucial Lack of Ablation Studies: }$ Given the model's complexity, the absence of ablation studies is a major omission. It is impossible to assess the contribution of each component to the final performance. For example, how crucial is the second \"component prototype\" level? What is the performance if the GMM components are mapped directly to survival evidence? How does the complex GRFN framework compare to simpler uncertainty quantification techniques like Deep Ensembles or Monte Carlo Dropout applied to a similar backbone? Without these experiments, the necessity of the full, complex architecture is not justified.\n\n$\\textbf{Insufficient Motivation for Evidential Framework Choice: }$ The paper uses Gaussian Random Fuzzy Numbers (GRFNs), a sophisticated but relatively niche framework. While it is shown to work well, the paper does not sufficiently motivate why this specific choice is superior to more widely-used uncertainty quantification methods in deep learning. A discussion comparing its theoretical advantages and disadvantages for this specific problem would strengthen the paper."}, "questions": {"value": "$\\textbf{1.}$ Regarding the model's sensitivity to the number of prototypes: This appears to be a major barrier to practical application. Have you considered methods to automatically learn or dynamically determine the optimal number of patch prototypes (C) and component prototypes (K) for a given dataset, rather than relying on manual tuning?\n\n$\\textbf{2.}$ Could you please provide ablation studies to justify the complexity of the DPsurv architecture? Specifically, I am interested in understanding the performance impact of (a) removing the second level of component prototypes and mapping GMM outputs directly to survival evidence, and (b) replacing the GRFN-based evidence module with a more standard uncertainty quantification technique like Deep Ensembles to see if similar calibration gains can be achieved with a simpler method.\n\n$\\textbf{3.}$ Could you provide more details on the computational cost and scalability of the method, particularly the GMM fitting and the weighted K-means initialization steps? How does the training time scale as the number of WSIs in the training cohort increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VNuNc5CGL4", "forum": "gHFeLx0QX7", "replyto": "gHFeLx0QX7", "signatures": ["ICLR.cc/2026/Conference/Submission1376/Reviewer_Uv5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1376/Reviewer_Uv5K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760862192183, "cdate": 1760862192183, "tmdate": 1762915753175, "mdate": 1762915753175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a dual‑prototype, evidential fusion framework to tackle the problem that WSI-based survival analysis suffers from limited interpretability and lack of calibrated uncertainty. Specifically, for each slide component, K component prototypes are learned, and evidence is modeled with GRFNs and a mixture of GRFNs is used for the slide level. The experiments showed that the proposed method achieves SOTA and good calibration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem targeting is timely: uncertainty quantification and interpretable reasoning for WSI survival is both relevant and challenging.\n2. Empirical scope: five TCGA cancers with site‑stratified 5‑fold CV is stronger than many prior works.\n3. Reported calibration: the comparison between BPIs and PPIs is conceptually apt, and the finding that modeling epistemic uncertainty improves coverage is plausible and clinically valuable."}, "weaknesses": {"value": "1.\tUnclear component definition: It is unclear how “component c” in slide i is aligned with “component c” in slide j in order to share $p_{c,k}$ and learn $β_{c,k}$. The text mentions “patch prototypes h_c” and then a GMM, but it is not explicit whether the per‑slide GMM components are anchored to these global patch prototypes.\n2.\tChoice of $λ$: The paper turns interval‑valued survival bounds (Bel, Pl) into a single S(t) via $S = λ Bel + (1−λ) Pl$, with $λ$ fixed to 0.1. While practical for metrics requiring a point probability, the statistical meaning of this convex interpolation is unclear and may bias calibration if $λ$ is tuned on validation. It also conflates epistemic with a fixed risk tolerance rather than propagating it.\n3.\tComputational complexity and scalability: Per‑slide GMM in high‑N patch regimes can be expensive; the paper does not detail computational cost vs. alternatives.\n4.\tLack of experiments on hyperparameter studies.\n5.\tMinor Issues: Eq. (3) vs Eq. (4) leaves $φ_i(·,·)$ under‑specified, please write $φ_i$ explicitly. Clarify the final dimensionality $R^{C×(2d+1)}$."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HNn9QeXro9", "forum": "gHFeLx0QX7", "replyto": "gHFeLx0QX7", "signatures": ["ICLR.cc/2026/Conference/Submission1376/Reviewer_MZaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1376/Reviewer_MZaC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015520011, "cdate": 1761015520011, "tmdate": 1762915752980, "mdate": 1762915752980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problems of interpretability, uncertainty, and tissue heterogeneity in survival prediction with whole slide images. The authors proposed DPsurv, a dual-prototype whole-slide image evidential fusion network that outputs uncertainty-aware survival intervals, while enabling interpretation of predictions through patch prototype assignment maps, component prototypes, and component-wise relative risk aggregation. The comparison of other baseline methods shows the good performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and stated the problems logically, well motivated, and the figures are clear and easy to understand.\n2. The comparison experiment is extensive."}, "weaknesses": {"value": "1. While the paper presents an interesting framework, a major shortcoming is the absence of a systematic ablation study. DPsurv introduces several novel components, including the dual-prototype embedding, component-wise evidential fusion, and mixture evidential loss. However, the contribution of each module is not quantitatively isolated. The current comparisons against baseline models only provide indirect evidence of effectiveness.\n2. Sensitivity analyses on the number of prototypes (C, K) and the trade-off parameters (α, λ...) are missing.\n3. Limited novelty. The paper's novelty only lies in combining prototype-based representation learning with evidential reasoning."}, "questions": {"value": "1. Could the authors provide a clear ablation study isolating the effects of (a) the dual-prototype embedding, (b) evidential fusion, and (c) uncertainty modeling? Currently, their individual contributions remain unclear.\n2. Have the authors evaluated how sensitive the results are to the trade-off parameters (α, λ...) and the number of prototypes (C, K)?\n3. What is the computational overhead of the dual-prototype evidential fusion compared to simpler MIL models? Can the framework scale to larger cohorts or higher-resolution WSIs?\n4. Can you explain more about your main contribution compared to CVPR 2024 paper [1].\n\n[1] Song, Andrew H., et al. \"Morphological prototyping for unsupervised slide representation learning in computational pathology.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\nIf you can address my concern, I would like to raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PkYK9s9Ory", "forum": "gHFeLx0QX7", "replyto": "gHFeLx0QX7", "signatures": ["ICLR.cc/2026/Conference/Submission1376/Reviewer_AfEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1376/Reviewer_AfEP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411198336, "cdate": 1761411198336, "tmdate": 1762915752686, "mdate": 1762915752686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present DPsurv, a novel framework for survival prediction from whole-slide images that uniquely combines interpretability with uncertainty quantification. The core contribution is a dual-prototype architecture: it first employs unsupervised Gaussian Mixture Models (GMM) to encode morphological heterogeneity into **patch prototypes**, then maps these representations to a survival prediction using a second layer of **component prototypes** grounded in evidential theory. This approach explicitly models both aleatoric and epistemic uncertainty, enabling the generation of well-calibrated Belief Prediction Intervals (BPIs). Experiments on five TCGA cohorts demonstrate state-of-the-art performance in both discrimination (C-index) and calibration (IBS), while offering multi-level interpretability from patch features to prognostic reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes several notable contributions to the field of computational pathology, particularly in its approach to building more transparent and reliable survival prediction models.\n\n**1. A Novel Hierarchical Architecture for Interpretability:**\nThe primary strength of this work is its introduction of a novel dual-prototype architecture that mirrors a pathologist's diagnostic reasoning. By first using unsupervised patch prototypes to categorize morphological features and then employing a second layer of component prototypes to link these features to prognostic outcomes, the model creates a structured and semantically meaningful reasoning path. This hierarchical design is a creative combination of existing ideas (GMMs, prototype learning) and represents a significant conceptual advance over flatter, end-to-end deep learning models, offering a clear pathway toward feature-level and case-level interpretability.\n\n**2. Principled Integration of Uncertainty Quantification:**\nA key merit of the paper is its rigorous and principled approach to uncertainty modeling. Instead of relying on ad-hoc or purely probabilistic methods, the authors ground their framework in evidential theory, using Gaussian Random Fuzzy Numbers to explicitly disentangle aleatoric (data-inherent) and epistemic (model-based) uncertainty. This is a technically sophisticated choice that is well-suited to the problem, as it allows the model to report not just a prediction, but also its own confidence in that prediction. The experimental results, particularly the superior calibration performance measured by IBS, suggest that this approach is highly effective.\n\n**3. Strong Empirical Performance on Diverse Datasets:**\nThe quality and significance of the proposed method are substantiated by strong empirical results. The authors conduct a thorough evaluation across five different cancer types from the TCGA database, which represent a diverse set of challenges in terms of tissue morphology and data characteristics. The fact that DPsurv consistently achieves state-of-the-art or competitive performance in both discrimination (C-index) and, crucially, calibration (IBS) against a range of relevant baselines validates the effectiveness of the overall framework. This robust performance across multiple datasets underscores the potential of this approach to be a generalizable tool for survival analysis."}, "weaknesses": {"value": "Despite its promising contributions, the paper exhibits several significant weaknesses that temper its conclusions and limit the reproducibility and reliability of the proposed framework. The following points outline specific areas that require substantial improvement.\n\n**1. Lack of Ablation Studies to Justify Architectural Complexity:**\nThe central weakness of this paper is the absence of a thorough ablation study to validate its complex, multi-stage architecture. The DPsurv model combines several sophisticated components (GMMs, a dual-prototype structure, and an evidential fusion mechanism), but their individual contributions to the final performance are not disentangled. To justify this complexity, the authors should provide experiments that answer the following critical questions:\n*   **Is the dual-prototype structure essential?** What is the performance drop if the second layer of component prototypes is removed, and the GMM-derived features are directly mapped to the evidential output layer? This would clarify the true value of the evidence-fusion stage.\n*   **How does the evidential framework compare to alternatives?** The claim of superior uncertainty quantification would be much stronger if DPsurv's evidential components were replaced with more standard uncertainty techniques like Monte Carlo Dropout or Deep Ensembles, followed by a direct comparison of their IBS scores and calibration plots. This is crucial for demonstrating that the benefits stem from the evidential framework itself, not just from having an uncertainty-aware component in general.\n*   **What is the impact of the regularization terms?** The contribution of the `R1` and `R2` regularization terms in the mixture evidential loss (Eq. 15) is unclear. An analysis showing performance with and without these terms would clarify their role in stabilizing the training and improving calibration.\nWithout these ablations, it is difficult to ascertain whether the model's success is truly due to its intricate design or if a simpler model could achieve comparable results.\n\n**2. Insufficient Analysis of Hyperparameter Sensitivity:**\nThe model's reliability is questionable due to the large number of crucial hyperparameters and the lack of a sensitivity analysis. Key parameters include the number of patch prototypes (`C`), the number of component prototypes (`K`), and the loss trade-off parameter (`alpha`).\n*   **The choice of `K` lacks empirical support.** The authors provide a qualitative, knowledge-driven rationale for selecting `K` for each dataset (Appendix B.4). While insightful, this is not a substitute for empirical validation. The paper would be significantly strengthened by including a table or plot showing how performance (e.g., C-index and IBS) varies with different choices of `K` for at least one or two datasets. This would demonstrate the robustness of the model to this critical parameter.\n\n**3. Subjective and Limited Evaluation of Interpretability:**\nThe paper claims interpretability as a core contribution, yet its evaluation is purely qualitative and based on a single case study (Figure 2). This is insufficient to support a general claim of interpretability.\n*   **Lack of objective metrics.** The evaluation could be strengthened by introducing more objective, even if semi-quantitative, assessments. For instance, the authors could conduct a user study where multiple pathologists are asked to rate the clinical relevance and coherence of the prototypes and assignment maps across a larger set of randomly selected cases. Their inter-rater agreement could then be reported.\n*   **Risk of cherry-picking.** The presented case in Figure 2 may be a \"best-case\" example. To mitigate concerns of selection bias, the authors should present additional, randomly sampled case studies (perhaps in the appendix) to demonstrate the consistency of the model's interpretable outputs. A model's interpretability is only valuable if it is reliable and consistent, not just illustrative in one favorable instance.\n\n**4. Clarity of Exposition Could Be Improved for Broader Accessibility:**\nWhile technically detailed, the paper's exposition could be made more accessible to a broader audience. The reliance on dense terminology from evidential theory (e.g., Gaussian Random Fuzzy Numbers, belief/plausibility functions) without sufficient high-level intuition can make the core mechanism difficult to grasp for readers not already expert in this specific niche. Incorporating more analogies or simplified step-by-step explanations of how evidence is mathematically fused would significantly improve the paper's clarity and impact, helping to bridge the gap between the complex theory and its practical application."}, "questions": {"value": "Thank you for this interesting and thought-provoking work. Your approach of integrating a dual-prototype structure with evidential theory is highly novel. To better understand the contributions and limitations of your framework, I have a few questions and suggestions that I hope will be helpful for the discussion phase.\n\n1.  **On the Necessity of the Dual-Prototype Architecture:** The multi-stage design is intricate. To better understand the contribution of each stage, could you please provide an ablation study? Specifically, what would the performance be if the second layer of component prototypes was ablated, and the GMM-derived component summary (the output of the first stage) was fed directly into a simpler evidential regression head? This would help clarify the exact performance gain attributable to the similarity-based evidence fusion mechanism.\n\n2.  **On the Choice of the Evidential Framework:** The use of GRFNs is central to your claim of superior uncertainty quantification. To place this contribution in the context of more standard methods, could you provide a direct comparison against a strong non-evidential uncertainty baseline, such as Deep Ensembles, trained on the same architecture? A comparison of the IBS scores and calibration plots (like in Figure 3) would be particularly insightful to empirically demonstrate the advantages of the evidential approach over these well-established alternatives.\n\n3.  **On Hyperparameter Sensitivity, Particularly for `K`:** The number of component prototypes, `K`, seems to be a critical and manually tuned parameter. While the clinical rationale provided in the appendix is appreciated, the paper would be much more convincing if this choice were supported by empirical data. Could you please provide a sensitivity analysis for `K` on at least one or two of the datasets? For example, a plot showing how the C-index and IBS vary as `K` is changed (e.g., from 2 to 6) would provide valuable insight into the model's robustness and help justify the values you selected.\n\n4.  **On the Evaluation of Interpretability:** The interpretability claims are primarily supported by a single, albeit detailed, case study in Figure 2. To strengthen this claim and address potential concerns of cherry-picking:\n    *   Could you provide additional examples of the model's interpretable outputs on other, randomly selected cases, perhaps in the appendix? Seeing the model's reasoning on more average or ambiguous cases would be very helpful.\n    *   Have you considered any form of more objective evaluation? For instance, even a small-scale study where pathologists assess the clinical coherence of the discovered prototypes across a larger set of cases could provide more robust evidence than a single illustrative example.\n\n5.  **On the Intuition behind Evidence Fusion:** For readers who are not experts in evidential theory, the mathematical fusion of GRFNs in Equation 8 can be difficult to grasp intuitively. Would it be possible to provide a simplified, high-level walkthrough of this step? For example, a toy example with two component prototypes and a target component, showing how their respective `μ`, `σ²`, and `h` values are combined, would greatly improve the accessibility of your core mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jluYXaXsK4", "forum": "gHFeLx0QX7", "replyto": "gHFeLx0QX7", "signatures": ["ICLR.cc/2026/Conference/Submission1376/Reviewer_McuC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1376/Reviewer_McuC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011385393, "cdate": 1762011385393, "tmdate": 1762915752563, "mdate": 1762915752563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}