{"id": "bX0Rw2uZgV", "number": 21272, "cdate": 1758315682059, "mdate": 1759896931416, "content": {"title": "An Automated Data Engineering Pipeline for Time Series Classification Via Text Embeddings", "abstract": "Data engineering pipelines are essential - albeit costly - components of predictive analytics frameworks requiring significant engineering time and domain expertise for carrying out tasks such as data ingestion, preprocessing, feature extraction, and feature engineering. In this paper, we propose ADEPT, an automated data engineering pipeline via text embeddings. At the core of the ADEPT framework is a simple yet powerful idea that the entropy of embeddings corresponding to textually dense raw format representation of time series can be intuitively viewed as equivalent (or in many cases superior) to that of numerically dense vector representations obtained by data engineering pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text embeddings to represent the diverse data sources, and (ii) constructs a variational information bottleneck criteria to mitigate entropy variance in text embeddings of time series data. We further establish theoretical guarantees showing that our construction maximizes mutual information while controlling predictive error, ensuring both compression and preservation of the predictive signal. ADEPT provides an end-to-end automated implementation of predictive models that offers superior predictive performance despite issues such as missing data, ill-formed records, improper or corrupted data formats and irregular timestamps. Through exhaustive experiments, we show that the ADEPT outperforms the best existing benchmarks in a diverse set of datasets from large-scale applications across healthcare, finance, science and industrial internet of things. Our results show that ADEPT can potentially leapfrog many conventional data pipeline steps thereby paving the way for efficient and scalable automation pathways for diverse data science applications.", "tldr": "", "keywords": ["Automated Data Engineering Pipelines", "Text Embeddings", "Time Series Classification", "Predictive Analytics", "Information Theory"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07b06a8ef93341b49f2105f2ff8630bd84332d68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ADEPT, an Automated Data Engineering Pipeline that transforms raw time series data into embedding representations using pretrained language models. A Variational Information Bottleneck (VIB) is employed to denoise these embeddings while preserving task-relevant information. The proposed pipeline outperforms domain-specific baselines across four application domains."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The method is simple and sound.\n    \n- The paper aims for a grounded analysis of the theory supporting their approach, which is good."}, "weaknesses": {"value": "Major:\n\n- **W1 —** Experimental evaluation/evidence in the main text is limited:\n    \n    - All reported results lack an estimate of experimental uncertainty, i.e., variability across random initializations with different seeds. This is critical to ensure significance of the results and all associated claims.\n        \n    - Comparisons with other methods are conducted on a single metric only.\n        \n    - The scalability of ADEPT to low-data regimes is not evaluated., i.e., the setting where hand-crafted features can (typically) outperform deep learning models.\n        \n    - The required preprocessing time is not compared, despite being one of the paper’s key motivations.\n        \n    - Comparison with other specialized ML approaches is missing. How does ADEPT compare with recent foundation models for time series representation learning, e.g., [3]? Similarly, how does it perform relative to unsupervised representation learning methods (broad literature) such as MiniROCKET [1] or reservoir-based embeddings [2]? \n        \n    \nI think these are all requirements for ICLR standards.\n    \n- **W2** — The notation is often imprecise and unclear. A few examples below:  \n    - Section 2.1 is difficult to follow, despite covering elementary preliminaries. \n    - From l.189 onward, $R^{(j)}$ uses parentheses while $S^j$ does not. \n    - Several symbols are undefined after Eq. 4 (e.g., what is $H_b​?$ What do $S$ and $E$ without indices denote? What does lowercase $s_i$ represent? What is $f$ in line 309?). Some are defined only in the Appendix, which disrupts the flow. \n    - Some acronyms are undefined: *TFK* (l. 110), *KBs*, *MBs* (l. 193). The symbol *p* is used both for prediction error (in Eq. 4, which is quite an unusual notation) and for probability (Eq. 11).\n    - It seems to me Appendix C describes a sensitivity analysis, not an ablation study.\n    \n\n- **W3** — The paper's motivations are not convincing to me. The paper appears to overstate the claim that a single pipeline and downstream model can automate every time-series classification task. In particular:\n    \n    - The authors state that “successful preprocessing and feature generation are inherently domain-specific and demand substantial engineering time and effort.” However, I believe this is often for good reason, as such steps can inject domain knowledge. Could ADEPT incorporate domain knowledge in some way?\n    \n    - Furthermore, domain-specific preprocessing allows practitioners to control individual stages of the pipeline. Fully automated and general pipelines based on deep learning might trade interpretability and reliability for (in-sample) performance.\n  \n\n    To be clear, I am not arguing that domain-specific methods are superior to automated ones, but rather that the opposite is not self-evident. I think the benefits of domain expertise in science and engineering are substantial, and the paper should more carefully motivate its scope and position with respect to these considerations.\n\n- **W4** - If the main contribution lies in the automated embedding extraction pipeline, why devote so much detail (ll. 280–305) to the Transformer classifier? Presumably, any model could be used, and this flexibility should be demonstrated. Do the theoretical arguments in Section 3.4 apply generally, or only to the Transformer described in the same section?\n    \n\nMinor:\n\n- **W5** - In their paper, the authors empathize automation over domain-specific and engineered choices. However, in their methods, the choice of the ‘chunk‘ size M is not automated. This is an important design choices (arguably one of the most important) and could be very heterogeneous across different classification tasks even within the same domain. The authors briefly mention this in ll.198–203, claiming that “validation studies for determining M can be easily automated,” but this statement is unconvincing and does not justify why this aspect was not explored in depth.\n    \n- **W6** \\- Most theoretical results are intuitive (e.g., “better token prediction corresponds to more informative embeddings”) but I have found their presentation hard to follow.\n    \n- **W7** - Novelty is limited. The proposed method combines three known components: LLM embeddings → Variational Information Bottleneck → Transformer classifier.\n    \n\n[1] - Dempster et al., ‘A very fast (almost) deterministic transform for time series classification’, SIGKDD, 2021.\n\n[2] - Bianchi et al., ‘Reservoir computing approaches for representation and classification of multivariate time series’, TNNLS, 2020.\n\n[3] - Ansari et al., ‘Chronos: Learning the Language of Time Series’, TMLR, 2024."}, "questions": {"value": "- **Q1** (Related to W1) - Could the authors repeat their experiments using different random seeds and report the individual means and standard deviations? I understand that this may be expensive, but I believe it is a hard requirement for the significance of a scientific claim.\n    \n- **Q2** (Related to W1) - How does ADEPT compare to foundation models and other unsupervised approaches for time series representation learning?\n    \n- **Q3** (Related to W3) - Could the authors clarify how ADEPT can integrate domain knowledge? Also, how could a fully-automated and black-box approach be trusted in scientific and high-stakes domains?\n    \n\nWhile I remain open to a constructive discussion, I believe the paper requires substantial improvement. At this stage, I'm not inclined to raise my score, as the gap between the current submission and a version that would meet the bar for acceptance still feels too wide."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dapAlDdifz", "forum": "bX0Rw2uZgV", "replyto": "bX0Rw2uZgV", "signatures": ["ICLR.cc/2026/Conference/Submission21272/Reviewer_EXjK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21272/Reviewer_EXjK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734484425, "cdate": 1761734484425, "tmdate": 1762941663419, "mdate": 1762941663419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a text embedding representation for time series as an alternative to a manually defined data engineering pipeline. They also propose a variational information bottleneck (VIB)  to improve the representation. The proposed method demonstrated superior performance on selected benchmarks compared to other works. The authors argue that embeddings of textually dense RFRs can retain mutual information comparable to feature vectors from manual pipelines, and that VIB improves information retention for prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The central premise is reasonably original and, in retrospect, surprisingly simple. The idea of \"text-ifying\" raw time series to leverage powerful, pre-trained text embedders is a clever conceptual leap.\n+ The paper's claims are well-supported by strong empirical results on selected benchmarks.\n+ The paper does an excellent job demonstrating the critical importance of the VIB layer with a proper ablation study."}, "weaknesses": {"value": "+ The authors state they \"serialize\" chunks and pass them to an embedder, but they never discuss what happens inside that black box. The frozen embedder uses its own tokenizer, which was trained on natural language, not on strings of numbers. How does this tokenizer handle a \"token\" like \"0.45, -1.23, 5.8e-2\"? Is it treated as a sequence of characters and digits? Is it broken into arbitrary sub-words? This is the paper's most significant conceptual gap.\n+ The paper claims its method is \"scalable\". However, the proposed pipeline requires separate inferences from a large text-embedding model for each time series sample (one for each chunk). For long time series, this could be dramatically slower and more computationally expensive than a single pass through a purpose-built numerical model.\n+ The paper claims AutoML methods produce \"opaque 'black-box' pipelines\", but ADEPT is arguably even less interpretable. It is unclear how a practitioner could debug the model or gain insights. How does one interpret what a text embedder, trained on natural language, has \"learned\" from a string of sensor readings?"}, "questions": {"value": "+ Could you please elaborate on the actual tokenization process? Have you investigated how the frozen text tokenizer segments the serialized numerical strings? How robust is this representation to simple formatting changes (e.g., padding, precision, scientific notation) that do not change the underlying numerical data? Moving from OpenAI`s text-embedding-3-small to Google`s gemini-embedding-001 could not completely change the results?\n+ The paper's core innovation is using pre-trained language embedders on serialized time series, which implicitly relies on a language-based tokenizer not designed for numerical sequences. This contrasts with contemporary time series foundation models that use patching to directly tokenize numerical windows, a method explicitly designed to preserve local temporal patterns. Could you elaborate on this trade-off? Specifically, how does your approach ensure that the language tokenizer captures meaningful temporal dynamics rather than superficial syntactic artifacts of the text string, which a direct numerical patching approach inherently avoids?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uw7DLaRfq1", "forum": "bX0Rw2uZgV", "replyto": "bX0Rw2uZgV", "signatures": ["ICLR.cc/2026/Conference/Submission21272/Reviewer_fyKE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21272/Reviewer_fyKE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921553752, "cdate": 1761921553752, "tmdate": 1762941663103, "mdate": 1762941663103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ADEPT, an automated data engineering pipeline designed for time series classification1. The central idea is to bypass conventional, labor-intensive data engineering steps, such as imputation, normalization, and feature engineering, by treating time series data in its Raw Format Representation (RFR) as text. The ADEPT pipeline consists of four main stages: (1) automated temporal chunking of the raw data, (2) applying a pre-trained, frozen LLM-based text embedding model to these chunks, (3) using a Variational Information Bottleneck (VIB) to denoise and compress these embeddings, and (4) feeding the resulting representations into a Transformer-based classifier for prediction. The authors provide theoretical justification for the VIB's role using mutual information bounds and conduct experiments across four distinct domains (science, healthcare, finance, and IoT), claiming superior performance over existing application-specific benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The paper targets a well-known and significant issue in applied machine learning: the cost and domain expertise required for data engineering and preprocessing in time series analysis. \n\nS2. The paper is well-written, and the proposed ADEPT pipeline is presented clearly. The diagrams (Figures 1 and 2) effectively illustrate the conceptual difference between ADEPT and traditional pipelines."}, "weaknesses": {"value": "W1. The primary weakness is the paper's lack of substantial technical innovation. The proposed ADEPT framework is a composition of several well-known, existing methods. The use of pre-trained text embeddings, the Variational Information Bottleneck (VIB), and a standard Transformer-based classifier are all off-the-shelf components. While system-level contributions are valid, the paper lacks any methodological development, making its contribution insufficient for a top-tier conference.\n\nW2. The claims of state-of-the-art performance are not adequately supported. The paper compares ADEPT against a small, seemingly arbitrary selection of \"application-specific\" models. This is not a sufficient standard. The experiments fail to include comparisons against widely accepted, general-purpose time series classification SOTA models. \n\n\nW3. The paper fails to provide a crucial ablation study: one that replaces the pre-trained text embedder with a simpler, randomly initialized encoder trained from scratch on the serialized chunks. This would be necessary to prove that the pre-trained knowledge from the LLM is the active ingredient, rather than just the VIB and Transformer backend.\n\n\nW4. The paper claims to \"bypass\" and \"leapfrog\" data engineering steps. However, \"Automated Temporal Chunking\" is a form of preprocessing. The choice of chunk size $M$ is a critical hyperparameter that directly impacts the temporal dependencies the model can learn. The paper does not provide a systematic study of this parameter's sensitivity, nor does it provide a clear justification for the different values of $M$ used across experiments ."}, "questions": {"value": "Q1. Why did the authors choose not to benchmark ADEPT against a standard suite of general, high-performance time series classification models across all four datasets?\nCould the authors provide a more detailed explanation of the serialization process? How are numerical values (with varying precision), timestamps, and categorical flags precisely converted into a \"unified string\"? \n\nQ2. How was the chunk size $M$ selected for each experiment? Given its importance in defining the temporal receptive field, can the authors provide a sensitivity analysis of $M$ on at least one dataset?\n\n\nQ3. To isolate the contribution of the pre-trained text embeddings, have the authors run an ablation study where the embedding model $g$ is replaced with a randomly initialized 1D-CNN or MLP encoder, which is then trained end-to-end with the VIB and classifier? \n\nQ4. In the healthcare experiment, ADEPT v1.0 (without VIB) achieved 58.97% accuracy, which was comparable to or worse than the baselines. ADEPT v2.0 (with VIB) jumped to 73.68%. This suggests the VIB is critical. Does this not imply that the raw text embeddings are, by themselves, extremely noisy and not superior representations as claimed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D2ydKcqlV6", "forum": "bX0Rw2uZgV", "replyto": "bX0Rw2uZgV", "signatures": ["ICLR.cc/2026/Conference/Submission21272/Reviewer_iZru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21272/Reviewer_iZru"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929669273, "cdate": 1761929669273, "tmdate": 1762941662335, "mdate": 1762941662335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ADEPT, an automated data engineering pipeline for time-series classification that aims to bypass traditional preprocessing steps such as data cleaning, imputation, and feature engineering. The framework converts each raw-format representation (RFR, e.g., CSV/HDF5) of a time series into text, divides it into segments, and generates embeddings for each chunk using pre-trained text embedding models such as OpenAI’s text-embedding-3-small. These embeddings are then processed by a Variational Information Bottleneck (VIB) layer to denoise and compress the representations, and finally fed into a Transformer classifier for end-to-end prediction. Two variants are presented: ADEPT v1.0, which omits the VIB, and ADEPT v2.0, which includes it. The authors also derive information-theoretic lower bounds on mutual information and evaluate ADEPT on four domains—PLAsTiCC (Science), SelfRegulationSCP2 (EEG), Bitcoin (Finance), and Hydropower IoT—claiming that ADEPT v2.0 “matches or surpasses” existing state-of-the-art baselines"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "[S1] Practical motivation: Automating feature engineering for time series could reduce expensive domain effort.\n\n[S2] Clarity: Diagrams and algorithms effectively communicate the pipeline.\n\n[S3] Better domain-specific results: In PLAsTiCC, ADEPT v2.0 achieves 97.83%, outperforming reported 80%–84% baselines; in IoT, Top-2 accuracy reaches 97.5%."}, "weaknesses": {"value": "[W1] Limited pipeline scope and novelty: If positioned as a pipeline paper, the work lacks the breadth and flexibility expected in this category. Unlike strong pipeline studies such as Auto-sklearn [1] or AutoGluon [2], which test multiple components and tasks, ADEPT is evaluated on only four classification datasets with fixed settings. It introduces no new training, assembly, or generalization strategy; thus, the contribution appears limited in novelty compared to typical ICLR submissions.\n\n[W2] Minimal methodological contribution: Viewed as a method paper, ADEPT offers little innovation. It primarily combines two established techniques—LLM-based textualization and the Variational Information Bottleneck (VIB)—without introducing new mechanisms or insights. The former has been studied in Time-LLM [3], and the latter is a standard compression method from Alemi et al. (2017) [4].\n\n[W3] Weak and inconsistent baselines: The chosen baselines are not consistently strong or up to date. The Finance task relies on older models (LSTM, 2019; BiLSTM, 2022), whereas the IoT task is compared only with a self-built TSFEL+MHAN setup. This narrow and uneven benchmarking limits the credibility of ADEPT’s reported gains.\n\n[W4] Contradictory results: The experimental findings do not fully support the paper’s claims. On the Healthcare (EEG) dataset, ADEPT v2.0 achieves a score of 73.68%, which is below the TSEM baseline of 75.60% (Pham et al., 2023), contradicting the claim of consistently outperforming prior work.\n\n[W5] Incomplete evaluation metrics: The main results report only accuracy, even though several datasets are imbalanced. Without macro-F1 or per-class precision and recall, it’s difficult to judge the model’s overall robustness or compare fairly with other methods.\n\nReferences:\n\n[1] Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and Robust Automated Machine Learning (Auto-sklearn). Advances in Neural Information Processing Systems (NeurIPS 2015).\n\n[2] Erickson, N., Klein, T., Zhang, C., Liu, J., Mindermann, S., Winther, O., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. Proceedings of the AutoML Conference 2020.\n\n[3] Jin, M., Zha, K., Cao, D., Liu, Y., Li, Q., He, H., & Zhang, J. (2024). Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. International Conference on Learning Representations (ICLR 2024).\n\n[4] Alemi, A. A., Fischer, I., Dillon, J. V., & Murphy, K. (2017). Deep Variational Information Bottleneck. International Conference on Learning Representations (ICLR 2017)."}, "questions": {"value": "[Q1] The claim of a “general automated pipeline” would be stronger if ADEPT were evaluated against standard TSC baselines (e.g., ROCKET, InceptionTime, HIVE-COTE). Could the authors clarify why these were omitted and whether ADEPT has been, or could be, tested with alternative components or tasks to demonstrate broader generalization?\n\n[Q2] How do the authors reconcile the Healthcare Dataset result in Table 1 — TSEM (75.60%) > ADEPT (73.68%) — with the claim that ADEPT “surpasses all benchmarks”?\n\n[Q3] Can the authors provide macro-F1 and per-class metrics in Table 1 to reflect the performance under class imbalance?\n\n[Q4] Please clarify whether the Finance (Bitcoin) baselines were trained and evaluated using exactly the same 2015–2023 dataset and three-class formulation (rise / fall / stable) that ADEPT uses? If the baselines were taken from other papers, it would be helpful to adjust those baselines under the same conditions to ensure a fair comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qqs6vp9rI9", "forum": "bX0Rw2uZgV", "replyto": "bX0Rw2uZgV", "signatures": ["ICLR.cc/2026/Conference/Submission21272/Reviewer_nXRs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21272/Reviewer_nXRs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21272/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989744767, "cdate": 1761989744767, "tmdate": 1762941662090, "mdate": 1762941662090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}