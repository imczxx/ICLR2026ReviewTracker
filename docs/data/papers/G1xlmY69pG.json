{"id": "G1xlmY69pG", "number": 6241, "cdate": 1757961420050, "mdate": 1759897927413, "content": {"title": "Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design", "abstract": "Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.\n\nWe present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.", "tldr": "This study demonstrates that while modern reinforcement learning methods initially struggle with a 3D same-different visuospatial task, a curriculum learning approach, informed by human experiments, can successfully teach the model to solve it.", "keywords": ["Visuospatial Problem-Solving", "Reinforcement Learning", "Machine Learning", "Same-Different", "Curriculum Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8461d76fa6ea9532ee1455fa2e2146e53602c242.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work seems to demonstrate the effectiveness of RL when training is driven by Curriculum Learning (CL). To illustrate it, the authors compare a PPO algorithm using CL against imitation learning, a generative adversarial approach, and standard learning without curricula. The testbed is based on the Same-Different task. The paper provides marginal novelty since CL is predominant in a range of successful complex task accomplishments in RL literature."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Overall, the paper is easy to follow, with methodology clearly defined, although there are conceptual questions regarding the suitability of the methods compared, considering the task domain, which deserves attention."}, "weaknesses": {"value": "In the Introduction, one paragraph among the three is dedicated to the question **Is reward enough?**. The following paragraph briefly introduces the Same-Different task. More details could be given on how this task is suitable for the context of the work and how it is related to the question that rewards are enough signals to lead to learning (please see Q1).\n\nAlthough the problem \"Same-Different\" is very interesting, I don't see why RL would be suitable for it, and foremost, this seems a generalization problem, which is a desirable property, but not inherently the target of policy learning. Moreover, the claim that a human-informed curriculum is more effective using a single environment test does not seem grounded. How about a curriculum generated by an LLM, for instance? \n\nThe most critical point is that the curriculum learning is well known to leverage the learning in RL, although there's no principled way of designing it. Since the paper evaluates a single task, with a single curriculum derived from humans, the claims turn out to be weak."}, "questions": {"value": "**Q1.** How does the paper address the question, or bridge the gap between the evaluation performed and the question \"Is reward enough\"? Is there any correlation between the binary reward problem formulation and the effectiveness of human-informed CL?\n\n**Q2.** Wouldn't it be more suitable to propose the Same-Different task in an evaluation setting, considering policy generalization?\n\n**Q3.** If supervised learning is said to be ill-suited for the task (section 4), why compare RL+CL against behaviour cloning, which turns out to be a supervised learning derived method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "snjPV98hz9", "forum": "G1xlmY69pG", "replyto": "G1xlmY69pG", "signatures": ["ICLR.cc/2026/Conference/Submission6241/Reviewer_2ZVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6241/Reviewer_2ZVc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760966796113, "cdate": 1760966796113, "tmdate": 1762918567094, "mdate": 1762918567094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes to use RL to learn a new task, a 3D Same-Different Task, which is one of the most primitive human psychophysical tasks. The goal of a Same-Different task is to determine whether two stimuli are identical. The task require the agent to move around the spaces to observe the two stimuli and determine whether they are the same or not. \n\nThe authors experimented with various RL/IL strategies, such as BC, PPO, contrastive learning (GAIL) and curriculum learning. While simpler algorithms such as BC, PPO and GAIL fail to solve the task, the authors found that curriculum learning serves as a good strategy to tackle the 3D same-different task. The authors also showed that using the human experiment as a reference to design the curriculum can significantly help with the training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The same-different task itself is quite interesting and to my knowledge novel to solve in the realm of RL. \n- The analysis from the discussion section is quite interesting. Especially, the plot in figure 10 and 11 where the RL agent exploits different viewpoints from humans.\n- Variations to increase the dimensionality of the same-different task is experimented.     \n- The proposed method is simple, which can be seen as a strength but also as a weakness."}, "weaknesses": {"value": "- First of all, the writing of the manuscript can be improved, especially in the introduction section. For example, the relationship between this paper and reward design, as mentioned in the introduction, is unclear to me. Furthermore, some important details about the experiments and methods are not clearly described. For example, what is the loss of BC? One can assume it might be MSE, but it is not clearly written in the manuscript. Details about the curriculum learning is also not described, such as how many timesteps/episodes were trained for each lesson?\n\n- In my opinion, analysis of the results would be the main contribution of this work. To this end, the discussion section can be significantly more detailed. For example, it is not clear to me why IL based methods fail. Since the trajectories are all expert trajectories as far as I understand, shouldn’t IL methods be able to at least learn the simplest 6-cell environment?\n\n- Some minor issues:   \nRegarding quantifying the results, I’m not a fan of removing results that came from poor initialisation and not including them in calculating the metrics, and would like to see the authors include them for a fairer comparison."}, "questions": {"value": "I will repost some questions that were mentioned in the weakness section here for clarity. \n\n$$\\textbf{Suggestions}$$\nS1. Add plots the training reward/timestep across different training run seeds during training, not just the failed runs.   \nS2. Add more descriptions about the baselines methods.   \nS3. It would be good to describe the task more clearly in the RL settings.   \nS4. I wonder what the performances of some of the methods used in this paper would be for the same task but in 2D? It would be good to clarify how significant does moving to 3D change the difficulty of the task, and why using RL to solve it is an good approach.    \n\n$$\\textbf{Questions}$$\nQ1. I find it interesting that naive PPO can somewhat output 50% result but curriculum learning cannot at harder tasks. Are there some intuitive reason to this result?   \nQ2. If an random agent is outputting 50% results, it is not clear to me why IL based methods fail at all.   \nQ3. Following up on Q2, since the trajectories are all expert trajectories as far as I understand, shouldn’t IL methods be able to at least learn the simplest 6-cell environment?   \nQ4. I wonder why the curriculum agent is staying in the same top-right viewpoint? The paper mentioned that it is because at the top-right cell the agent can see both objects, allowing for better comparison. If that’s the case, are there particular reasons why the actions are not evenly distributed around top-right, top-left, bottom-left, bottom-right corners?    \nQ5. Following up on the question 4, if memory is indeed a problem, does the authors think that switching to an off-policy algorithm such as SAC or TD3 solve the problem more efficiently?   \nQ6. It is mentioned in the manuscript that the image inputs used are 512x512. These are quite large! If I recall correctly the image inputs in typical 2d-based RL agents are something like 64x64 (DreamerV2[1]) or 84x84 (DrQv2[2]). I wonder if the results for baseline agents would be different if (much) smaller image sizes are used.    \n\n---\n\n[1] Danijar Hafner et al., Mastering Atari with Discrete World Models, ICLR 2021    \n[2] Denis Yarats et al., Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning, ICLR 2021"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O6TH0qKpVu", "forum": "G1xlmY69pG", "replyto": "G1xlmY69pG", "signatures": ["ICLR.cc/2026/Conference/Submission6241/Reviewer_tQMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6241/Reviewer_tQMc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408682394, "cdate": 1761408682394, "tmdate": 1762918566724, "mdate": 1762918566724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors apply reinforcement learning (and imitation learning) to the problem of comparing complex 3D shapes, where the agent can control its viewpoint.\n\nThey show that basic RL can solve the task in constrained environments with few accessible states. Introducing a curriculum improves performance by allowing success in less constrained environments. Modifying the curriculum in a manner inspired by human experiments seems to again slightly improve performance. Imitation learning invariably fails."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experiments are interesting. To my knowledge the experimental platform is novel."}, "weaknesses": {"value": "- The results are somewhat unedifying. We see that curriculum learning does seem to help to tackle more open environment. Furthermore, it seems that for this specific task, a certain tweak in the order of the curriculum, inspired by human experiments, may have slightly improved performance on 48 cells environments. It is not obvious what we can extrapolate from this for RL in general.\n\n- More generally, as the authors acknowledge, comparisons with human behavior are extremely difficult due to the fact that the agent is memory-less, preventing working memory approaches which seem to underlie human behavior, and resulting in very different behavior between humans and agents.\n\n- A consequence of this difference is that the agent cannot use a strategy that integrates information from various viewpoints; the best it can hope for is to select an optimal viewpoint for discriminating between any two pairs. In theory, this could still be flexible and adaptable (selecting a different, optimal viewpoint for any given pair of stimuli), but in practice it seems that the agent shows extreme preference for one specific viewpoint (Figure 10a).\n\n- As a result, it is likely that the apparent small gain in performance from the \"human-inspired\" curriculum was coincidence.\n\n- Some details of the task could be explained a bit better, see below."}, "questions": {"value": "- What is the exact action space? Does the agent choose a whole position-direction pair at each time step, resulting in a Number_of_states x 2 action space? Or is there some actual locomotion from place to place?\n\n- Some tasks report 50% accuracy (i.e. chance), while others report 0% accuracy. I'm assuming the latter means \"no response given - ever\"? But if no response is ever given, no training signal is provided? If so, it should be possible to always enforce a response (e.g. whatever is the highest output at the end of the trial), resulting in a 2-Alternative Forced Choice that is typical of biological experiments - and more importantly, always providing a training signal to the agent for every trial, which might help learning. If I misunderstood the meaning of \"0% accuracy\", please clarify it.\n\n- Minor: citations are messed up, with parentheses in the wrong place, suggesting a mixup between \\citep and \\citet. The authors might find the following Latex command useful: \\renewcommand{\\cite}{\\citep}  \n\n- The authors briefly mention the work of Logothetis and colleagues on very similar stimuli. Logothetis' focus was somewhat different (stimulus representation and fast learning in the visual system) but it might constitute another direction of research for this experimental platform, perhaps with meta-learning approaches based on fast plasticity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6unyi25aXh", "forum": "G1xlmY69pG", "replyto": "G1xlmY69pG", "signatures": ["ICLR.cc/2026/Conference/Submission6241/Reviewer_1zTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6241/Reviewer_1zTK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599305317, "cdate": 1761599305317, "tmdate": 1762918566325, "mdate": 1762918566325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores whether reinforcement learning agents can solve a 3D Same–Different visuospatial reasoning task inspired by human psychophysics experiments. Using a Unity-based environment, the authors test PPO, BC, and GAIL, and find that none can learn the task directly. They then introduce curriculum learning, structuring the task into progressively harder “lessons,” and report that performance improves substantially, especially when the curriculum order is derived from human behavioral data. The human-informed curriculum enables the agent to succeed on discrete tasks with up to 48 viewpoints but still fails in larger or continuous settings. The authors conclude that curriculum learning, particularly when guided by human data, can help RL agents acquire visuospatial reasoning skills that simpler end-to-end training cannot."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper adapts a well-known visuospatial reasoning problem to the RL setting. The objective is well-defined, and the motivation for using this task to explore the limits of visual reasoning in RL agents is clear. The simple setup offers an intuitive and interpretable framework for examining how agents develop spatial understanding and active perception strategies. It’s refreshing to see a problem that feels genuine rather than over-engineered.\n2. The curriculum learning results are insightful.  The naïve setting already enabled PPO to master tasks that were otherwise unlearnable. The comparison with the human-informed curriculum further reveals that task variants difficult for humans are similarly challenging for PPO in this setting. Interestingly, the curriculum derived from human performance data provides an additional boost, suggesting that human behavioral structure has the potential to design RL curricula.\n3. The paper is well-written and easy to follow. The narrative flows logically, and the environment is clearly described and easy to understand."}, "weaknesses": {"value": "1. The paper does not clearly formalize the task as a POMDP. While the text describes the environment qualitatively, it never defines the state, observation, action, and reward functions that make this original binary classification task into a sequential decision-making problem. Without this formal grounding, it remains unclear what the agent is optimizing, how partial observability is handled, or how the final binary decision integrates into the trajectory-based reward structure.\n2. The sparse terminal reward uses +1 for correct and −1 for incorrect responses. This symmetric structure is questionable because it penalizes exploration and encourages indecision. In sparse-reward RL, a strong negative terminal signal can dominate the learning signal, making the policy overly cautious and hesitant to terminate episodes. This is consistent with the authors’ own observation: “*…performance dropped to 0%, as the agent frequently failed to provide an answer at all*.” A zero baseline for incorrect outcomes (or a smaller negative value) would likely yield more stable learning dynamics and prevent the agent from stalling while it attempts to avoid the heavy penalty.\n3. The authors explicitly exclude experimental runs that “did not train due to bad random initialization or catastrophic forgetting.” Excluding failed seeds biases the results upward and undermines reproducibility. Robust evaluation in RL typically reports statistics across all runs, not only the successful ones. I suggest reporting the IQM to mitigate this issue, instead of cherry-picking results. Moreover, I don’t see how “catastrophic forgetting” plays a role here. The setup is not continual or sequentially shifting across tasks but rather involves a single fixed environment. The observed reward collapse is more plausibly explained by policy instability or mode collapse, potentially driven by the strong −1 penalty for incorrect terminal actions.\n4. There is no clearly defined experimental protocol. The authors state that training was “stopped if the cumulative reward plateaued.” It appears this “plateau” is heuristically determined. Such subjective termination is not at all rigorous. Since the paper proposes a task for evaluating RL methods, a fixed experimental protocol is essential. The authors should report the data budget, evaluation frequency, early stopping conditions, random seed handling, etc.\n5. The authors refer to the human-collected trajectories as *expert demonstrations* for imitation learning. However, no description of their structure or justification for their “expert” status is provided. In the Same-Different task, it is unclear what constitutes expertise: an optimal (oracle) policy would solve the task immediately with a single step, whereas humans typically explore idiosyncratically before deciding. Without a formal definition of optimal behavior or a quantitative measure of demonstration quality, it is impossible to assess whether the dataset meaningfully guides imitation learning. The demonstrations likely encode heterogeneous, non-optimal exploration patterns rather than consistent expert policies, making the term “expert” misleading and potentially explaining the failure of BC and GAIL to learn effectively.\n6. The presentation of results is poor. The bars in Figures 5 & 8 are overlapping. This reduces visual clarity. Why not use the extra horizontal space to properly place the side-by-side? Figures 6 & 7 look more like screenshots from tensorboard or wandb, rather than a properly plotted figure. Figure 6 (a) and (b) could be combined to a single graph.\n7. The presentation of results is poor. In Figures 5 and 8, the overlapping bars for accuracy and viewpoints introduce clutter. This could easily be resolved by using the extra horizontal space in the paper to separate the bars. Figures 6 and 7 appear to be direct screenshots from TensorBoard or WandB rather than quality plots. Figure 6 (a) and (b) could be merged into a single panel comparing both methods. \n8. Since task success heavily relies on visual perception, the authors should put more emphasis on the visual encoder. Try different settings, and analyze where and why things go wrong. What representations do the agents learn? Are they useful?\n9. Given that task success fundamentally depends on visual perception, the paper should devote more attention to the agent’s visual encoder. The authors use a “simple CNN” (Table 3) but never justify its architecture, capacity, or input preprocessing. No ablations are presented on encoder depth, resolution, or feature representations. Without analyzing the learned embeddings or visual attention patterns, it is difficult to understand what representations the agent has learned or **why it fails beyond 48 viewpoints.\n10. The experimental comparison is narrow, considering only PPO, BC, and GAIL. This offers little insight into whether the observed difficulties stem from the algorithm, the reward design, or the environment itself. More baselines would better ground…, potentially including off-policy and model-based methods.\n11. The study only evaluates PPO, BC, and GAIL, which provides a limited perspective on the sources of failure or success. Incorporating additional baselines, optionally of-policy and model-based methods, would better ground the analysis.\n\n### Minor Points\n\n1. Vertical lines in Tables 1 & 2."}, "questions": {"value": "1. Does the curriculum agent receive a higher training budget?\n2. How are episodes initialized? Is there stochasticity involved? Where does the agent spawn? Where do the objects spawn, and how are they orientated?\n3. Since episodes vary in length depending on when the agent makes a guess, does a longer episode provide the agent with more learning data? Is the total training budget defined by a maximum number of steps or by a fixed number of episodes?\n4. What constitutes the “*badness*” of random initializations? How do you determine/quantify that a random initialization is “*bad*”?\n5. How did the authors measure the quality of the human-collected data for imitation learning?\n6. Why doesn’t a simple +1 reward for correct guesses suffice? Why is the -1 penalty for incorrect guesses necessary?\n7. What is a “*training attempt*” (lines 262, 306)? Are the authors referring to a single run with a particular seed?\n8. PPO and imitation learning methods are referred to as “*one-shot*” (lines 361-362). What do the authors mean by this? To me, they are anything but “*one-shot*”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oKj4QqLN2p", "forum": "G1xlmY69pG", "replyto": "G1xlmY69pG", "signatures": ["ICLR.cc/2026/Conference/Submission6241/Reviewer_exxm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6241/Reviewer_exxm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881583302, "cdate": 1761881583302, "tmdate": 1762918565903, "mdate": 1762918565903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}