{"id": "AEg6tomw9y", "number": 10778, "cdate": 1758181686775, "mdate": 1759897629462, "content": {"title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI", "abstract": "We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few‑shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero‑shot target NLI model. On standard benchmarks, VAULT elevates RoBERTa‑base accuracy from 88.48% to 92.60% on SNLI (+4.12%), from 75.04% to 80.95% on ANLI (+5.91%), and from 54.67% to 71.99% on MultiNLI (+17.32%). It also consistently outperforms prior in‑context adversarial methods by up to 2.0% across datasets. By automating high‑quality adversarial data curation at scale, VAULT enables rapid, human‑independent robustness improvements in NLI inference tasks.", "tldr": "VAULT automates RAG for NLI: for each premise-label pair, it retrieves balanced contexts via semantic and lexical similarity, generates adversarial hypotheses, ensemble‑validates them, and injects them back into training to harden the model.", "keywords": ["RAG", "SNLI", "LLM", "text generation", "adversarial training", "data augmentation", "few‑shot learning", "natural language inference"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44cd742460e504bc098e38f85bc7ff3b3ff4a6e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VAULT, an automated pipeline for generating and validating adversarial examples to improve natural language inference (NLI) models. \nThe approach integrates retrieval, LLM generation, automatic validation, and iterative retraining in a single loop. \nBy generating challenging examples that existing models misclassify and filtering them through multiple LLM “judges,” the method aims to produce high-quality adversarial training data without human intervention. \nExperiments on SNLI, ANLI, and MultiNLI show that VAULT consistently improves model performance, achieving notable accuracy gains while using far fewer examples than previous large-scale synthetic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and coherent framework that combines retrieval, adversarial generation, validation, and retraining in a single automated loop. The methodology is easy to follow, and the ablation studies are detailed enough to support the main claims.\n\n2. The use of both semantic and lexical retrieval provides a good balance between relevance and diversity, leading to more effective adversarial examples.\n\n3. Empirical results are strong across multiple benchmarks, showing consistent improvements while using far fewer examples than large-scale synthetic datasets."}, "weaknesses": {"value": "1. The unanimous validation rule may filter out useful but ambiguous examples, and there is no human evaluation to verify the accuracy of the validated data.\n\n2. While comparisons to large synthetic datasets are provided, stronger baselines such as recent adversarial training or contrastive learning methods are missing.\n\n3. The paper could include a deeper analysis of what types of reasoning errors are actually corrected by the proposed approach, to clarify the nature of the robustness gains.\n\n4. This idea of having LLMs generate challenging examples and gradually incorporating them into training is not new; it’s just the first time it has been applied to NLI."}, "questions": {"value": "1. Please report generation + validation cost (GPU hours) for producing the ~6–6.6k validated examples per strategy, and the end-to-end training time. How does cost scale with T iterations and k shots?\n\n2. Do the three LLM judges ever agree yet be wrong relative to human labels? Provide a human audit on a random sample of accepted items to estimate precision of the unanimous filter. Also share inter-judge agreement matrices and typical disagreement cases. \n\n3. Break down gains by challenge types (negation, comparatives, quantifiers, monotonicity, syntactic perturbations). Which phenomena benefit most from VAULT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "a6mIxWrB4a", "forum": "AEg6tomw9y", "replyto": "AEg6tomw9y", "signatures": ["ICLR.cc/2026/Conference/Submission10778/Reviewer_CXgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10778/Reviewer_CXgR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495045322, "cdate": 1761495045322, "tmdate": 1762921990662, "mdate": 1762921990662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method of LLM with RAG for natural language inference, which considers to incorporate adversarial examples for fune-tuning LLMs to improve its robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes an end-to-end automated adversarial RAG pipeline, which fully automates retrieval, adversarial generation, multi-LLM validation, and iterative retraining.\n- This paper provides a detailed procedure for the RAG pipeline, and experimental results on three NLI datasets show the proposed method achieves better performance by fune-tuning RoBERTa-base."}, "weaknesses": {"value": "- The proposed method is not innovative, since adversarial generation has been proposed by prior works.\n- As many LLMs can use RAG for implementing natural language inference, I want to see a direct comparison with these LLMs in the experiment.\n- The structure of the paper could be improved, the figures and the hyperparameter settings can be put in appropriate positions."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D5VO3I2NqN", "forum": "AEg6tomw9y", "replyto": "AEg6tomw9y", "signatures": ["ICLR.cc/2026/Conference/Submission10778/Reviewer_Lkk5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10778/Reviewer_Lkk5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790285038, "cdate": 1761790285038, "tmdate": 1762921989496, "mdate": 1762921989496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a data augmentation framework VAULT for the NLI task that includes three stages: retrieval, adversarial generation, and iterative retraining. \nThe retrieval stage uses both semantic (BGE) and lexical (BM25) similarities for few-shot sample retrieval; The adversarial generation phase employs LLM to generate hypotheses, with label accuracy validated through three LLMs; The iterative retraining stage fine-tunes NLI model by mixing the given original data and the generated adversarial data.\nExperimental results on three NLI datasets show that VAULT improves the performance of a RoBERTa-base NLI model.\nHowever, this paper still exhibits certain issues in paper writing and experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The authors employ a data augmentation method for the NLI task which generate the adversarial data for training a model.\n2) During the retrieval phase, the authors combine BGE and BM25 methods for the sample similarity assessment, effectively capturing the semantic information and the token-level feature. Experimental results further validated the effectiveness of this methodology."}, "weaknesses": {"value": "1） This method only has been evaluated on NLI tasks, which limits its practical value. \n2） The experimental comparison is insufficient. Since this paper focuses on model training with labelled data, the baseline methods should include few-shot learning methods and LLMs based contextual learning methods. However, currently the paper only compared with LLMs, lacking comparisons with the aforementioned types of methods. Besides, the authors only use Roberta-base as the baseline model for NLI tasks, which cannot convince the effectiveness of the proposed method across a broader range of NLI scenarios. In the experimental setup, there is not the essential statistical information regarding the dataset. \n3） The authors do not explain clearly the process on how large language models (LLMs) generate hypotheses based on retrieval results. This process, however, critically influences the subsequent filtering and iterative retraining steps within the LLM framework.\n4） The are some conflicts in the model section. In Section 3.2, the explanation of the formula contradicts the interpretation of the results in Figure 5. The claim that “the combined BGE+BM25 strategy consistently outperforms either alone” is inconsistent with the findings illustrated in Figure 5. In the retrieval part, there is an inconsistency in symbol usage—for instance, both the premise and the query are denoted by the symbol “p”."}, "questions": {"value": "Here are some suggestions: \n1)\tIt is recommended to validate the proposed method across multiple tasks and compare it with additional baseline methods under identical experimental settings, particularly using labeled data of comparable scale.\n2)\tMore details are desired on the method, including the adversarial generation process, the manner of organizing input information, and the prompt.\n3)\tThe authors are recommended to provide more explanations when analysing experimental results, such as the reasons that cause the performance improvements or declines, rather than merely describing the observed changes, and analyze whether these changes can be attributed to your novel method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sDbn7sV7al", "forum": "AEg6tomw9y", "replyto": "AEg6tomw9y", "signatures": ["ICLR.cc/2026/Conference/Submission10778/Reviewer_ksm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10778/Reviewer_ksm1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836059020, "cdate": 1761836059020, "tmdate": 1762921989158, "mdate": 1762921989158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VAULT, an automated adversarial RAG pipeline for improving the robustness of NLI models. The method iteratively retrieves balanced few-shot contexts, generates adversarial hypotheses with LLM, validates them through a multi LLMs ensemble, and retrains RoBERTa with the validated data. VAULT achieves substantial accuracy gains on SNLI, ANLI, and MultiNLI, while requiring no human annotation, demonstrating that fully automated, LLM-driven adversarial data generation can effectively enhance model generalization and resilience."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution of this paper is the design of an automated “retrieve-generate-validate-retrain” closed-loop system, which provides a valuable engineering framework for improving the robustness of NLI models in a scalable manner without human annotation.\n2. The paper conducts extensive ablation studies, demonstrating substantial experimental effort. The main experiments show that with a small number of adversarial samples, the method can achieve significant performance improvements across multiple NLI tasks."}, "weaknesses": {"value": "1. The primary concern about this paper lies in its novelty. Each component of VAULT (including RAG, adversarial sample selection and refinement through iterative training, and the use of an LLM as a verifier) has been explored in prior work. VAULT appears to be more of an integration and adaptation of these existing techniques rather than a fundamentally new approach.\n2. The paper lacks ablation studies across different models.\n\n    a. Using different backbone models. It would be important to see whether VAULT remains effective when applied to stronger NLI models or to decoder-only language models such as Qwen3-0.6B or SmolLM2-360M.\n\n    b. The study should also examine the effect of using generation and verification models of different scales. Evaluating stronger or weaker LLMs would help analyze how VAULT performs under different computational budgets.\n\n    c. It is necessary to control the strength of the generator and the verifier to determine whether the performance gain mainly comes from the generation process or from the verification process.\n3. [Minor] While VAULT indeed reduces the demand for large data volumes, it is model-specific. The method is tailored to address the weaknesses of Roberta-base-snli, whereas GNLI is a general-purpose data augmentation approach. Therefore, the comparison in terms of efficiency between VAULT and methods like GNLI is somewhat unfair."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m8uBBKvFWJ", "forum": "AEg6tomw9y", "replyto": "AEg6tomw9y", "signatures": ["ICLR.cc/2026/Conference/Submission10778/Reviewer_6FzM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10778/Reviewer_6FzM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10778/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153933983, "cdate": 1762153933983, "tmdate": 1762921988699, "mdate": 1762921988699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}