{"id": "wyJ4YagQqw", "number": 7551, "cdate": 1758027122934, "mdate": 1763002962737, "content": {"title": "Score Augmentation for Diffusion Models", "abstract": "Diffusion models have recently achieved remarkable advances in generative modeling, yet we show that they are still prone to overfitting, especially when trained with limited data. To address this issue, we introduce Score Augmentation (ScoreAug), a data augmentation framework tailored for training diffusion models. Unlike conventional methods that augment clean data, ScoreAug operates directly on noisy data, naturally aligning with the denoising process of diffusion models. Moreover, the denoiser is required to predict the transformed target of the original signal, establishing an equivariant learning objective. This equivariance enables learning of scores across diverse denoising spaces -- a principle we call score augmentation. We provide theoretical analysis of score consistency under general transformations, and empirically validate ScoreAug across CIFAR-10, FFHQ, AFHQv2, and ImageNet, with U-Net and DiT backbones. Results show consistent performance improvements over baselines, effective mitigation of overfitting under varying data scales and model capacities, and stable convergence. Beyond improved generalization, ScoreAug avoids potential data leakage in certain scenarios and can be seamlessly combined with standard augmentation strategies for further gains.", "tldr": "", "keywords": ["score augmentation", "diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a2071ed53f3233f210b20784483e363ecd5754df.pdf", "supplementary_material": "/attachment/82f2df58c5d8b7c9d9f4012d45a7eb75f136d2c8.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Score Augmentation (ScoreAug), a diffusion-aligned augmentation framework that operates on noisy inputs and trains the denoiser to predict transformed clean targets, yielding an equivariant learning objective. The authors provide a score–transformation analysis (incl. a general theorem) and demonstrate gains across CIFAR-10, FFHQ, AFHQv2, and ImageNet with both U-Net/EDM and DiT/SiT backbones, showing better FID/sFID/IS, improved robustness to overfitting (small data, larger models), and more stable convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Method–process alignment: Augmenting in the noisy space with an equivariant target is principled and avoids the mismatch of clean-only augmentation; it also mitigates augmentation leakage with proper conditioning. \n\n2. A clear correspondence between scores under general transformations (Theorem 1) supports the design beyond linear cases. \n\n3. Broad empirical coverage: Consistent improvements over EDM baselines (with/without non-leaky aug) and additional gains on SiT (ImageNet-256), indicating applicability beyond a single architecture/dataset. \n\n4. Overfitting mitigation evidence: Stronger performance under reduced data, larger channel sizes, and throughout training (FID curves), matching the paper’s motivation."}, "weaknesses": {"value": "1. Ablation depth: Type I vs. Type II under nonlinear transforms and the exact role/sensitivity of conditioning (and its injection method) deserve deeper, more granular analysis beyond a few tables. \n\n2. Compute reporting: Claims that resources “do not increase significantly” are not quantified (e.g., wall-clock, GPU-hours) across datasets; clearer cost–benefit and sampling-time impacts would help adoption. \n\n3. Metric/setting breadth: Heavy reliance on FID, with limited diversity metrics or human eval; text-conditional or higher-res benchmarks are only lightly touched (no large-scale T2I)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9UNGHzsMDU", "forum": "wyJ4YagQqw", "replyto": "wyJ4YagQqw", "signatures": ["ICLR.cc/2026/Conference/Submission7551/Reviewer_Ko2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7551/Reviewer_Ko2W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641115016, "cdate": 1761641115016, "tmdate": 1762919646468, "mdate": 1762919646468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "PGYwd7bEsP", "forum": "wyJ4YagQqw", "replyto": "wyJ4YagQqw", "signatures": ["ICLR.cc/2026/Conference/Submission7551/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7551/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763002960896, "cdate": 1763002960896, "tmdate": 1763002960896, "mdate": 1763002960896, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ScoreAug, a novel data augmentation framework specifically designed for training diffusion models. The goal is to alleviate overfitting and avoid artifacts caused by augmentation leakage.\n\nThe idea is to apply transformations to the noisy data rather than the clean data, and to train the denoiser to predict the transformed clean data rather than the original one, which corresponds to an equivariant learning objective. The authors provide a theoretical analysis that frames this approach as learning score functions in different transformed spaces.\n\nExperiments on CIFAR-10, FFHQ, and ImageNet with U-Net and DiT architectures show that ScoreAug mitigates overfitting and improves generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper aims to tackle an important and practical problem, providing an additional (and relatively understudied) perspective on improving the training of diffusion models.\n- Figure 1 and Table 1 provide a clear and intuitive illustration of how ScoreAug works.\n- The study presented in Figure 2 strongly supports the claim that ScoreAug effectively mitigates overfitting with limited data or excessive model capacity."}, "weaknesses": {"value": "- The experimental setups described in Table 2 are very confusing, particularly the meaning of $+$ and $\\times$.\n  - Accoring to my understanding:\n    - \"EDM w/ NLA\" = using a non-linear transformation pipeline (described in Table 6 of their paper), and conditioning the model with the 9-dim aug label vector $a$.\n    - \"ScoreAug(Linear)\" = using one randomly selected linear transformation (Appx. B), and conditioning the model with the vector $\\omega$ (Line 265).\n    - \"ScoreAug(type 1/2)\" = using the same pipeline and conditioning as \"EDM w/ NLA\", but the input and target follow the definitions in Table 1.\n  - Therefore, the \"EDM w/ NLA + ScoreAug(Linear)\" setup is particularly ambiguous. It is unclear how two distinct pipelines, conditioning vectors, and input-target definitions are combined to achieve a \"synergistic\" effect. Is it a sequential application, a random selection, or another strategy? Lines 805-806 indicate that the transformations differ from both ScoreAug(Linear)'s and EDM's.\n\n- The experiments on the latent-space DiT (SiT) are preliminary.\n  - The reported performance gain on SiT-XL is marginal, and results for smaller models like SiT-B are absent (smaller models can often show greater benefit).\n  - Is the setup class-conditional or unconditional? Will these somewhat marginal improvements be diminished when classifier-free guidance (CFG) is applied?\n  - The choice of augmentation for ImageNet appears overly simple. The appendix suggests that only translation was applied in the VAE's latent space. Such a simple transformation for a complex dataset may not fully demonstrate the potential of ScoreAug.\n  - Although the ImageNet dataset is much larger than CIFAR and not as \"data-limited\" in the generative modeling context, data augmentation is still crucial for representation learning. Recent work has illustrated the close relationship between representation learning, generation quality, and training efficiency in diffusion models. Therefore, data augmentation on latent-space ImageNet and even larger-scale datasets may be very important. I was hoping the paper would provide an effective solution in this direction, but unfortunately, it seems to only introduce a weak \"translation\" with limited improvement.\n\n- Presentation issues.\n  - The reported FID scores are inconsistent in Table 2, 4, and 5.\n    - Table 2 and Table 4 appear to be the same configuration for CIFAR-10, but the FIDs are different (2.35, 2.24, 2.11, 2.25 vs. 2.27, 2.29, 2.11, 2.06).\n    - The result in Table 5 (2.21, 2.12, 2.01, 2.08, without brightness) is better than that in Table 4 (with all four augmentations).\n  - Figure 1 and Table 1 illustrate the same idea and could be merged for simplicity.\n  - Figure 2 and Table 5 are referenced in the text on Page 4, long before they actually appear."}, "questions": {"value": "Please refer to the weaknesses regarding the \"EDM w/ NLA + ScoreAug(Linear)\" setup, and particularly the latent-space DiT experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zvtUO82Z9J", "forum": "wyJ4YagQqw", "replyto": "wyJ4YagQqw", "signatures": ["ICLR.cc/2026/Conference/Submission7551/Reviewer_33yG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7551/Reviewer_33yG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997220155, "cdate": 1761997220155, "tmdate": 1762919645828, "mdate": 1762919645828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ScoreAug, a data augmentation framework that applies transformations directly to noisy inputs in diffusion models, enforcing an equivariant objective that aligns augmentation with the denoising process. It claims improved generalization and stability across datasets (CIFAR-10, FFHQ, AFHQv2, ImageNet) with modest FID gains over EDM and EDM w/ NLA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "It's interesting to confirm that the transformation at the noisy space works as much as the original EDM training."}, "weaknesses": {"value": "### W1. Marginal empirical gains.\n\n- Table 2 already shows EDM w/ NLA ≈ 2.1 FID, while the proposed method achieves only ~0.05–0.1 improvement—well within run-to-run variance.\n- The authors describe this as a “consistent performance improvement,” but such a small delta is unlikely to be statistically meaningful, particularly given the stochasticity of diffusion model training.\n- Without repeated trials or confidence intervals, it is difficult to tell whether any real gain exists or if this reflects noise in evaluation. A more rigorous experimental protocol (multiple seeds, variance analysis) would be required to support the claimed benefit.\n\n### W2. Limited conceptual novelty.\n\n- The contribution would be stronger if the authors demonstrated clear failure modes of EDM w/ NLA and showed that ScoreAug resolves them in a measurable way (e.g., improved generalization under severe data scarcity).\n- Furthermore, the relevance of augmentation for large-scale diffusion models is questionable: models trained on hundreds of millions of images already exhibit strong data diversity, so augmentation provides little marginal value.\n- If the authors believe augmentation still matters, they should justify this by analyzing genuinely low-data domains (e.g., medical, scientific, or niche artistic datasets) where overfitting remains critical.\n\n### W3. Overstated significance.\n\n- The practical contribution seems incremental rather than a new paradigm for regularizing diffusion models. Clarifying the novelty boundary with EDM and reporting statistical significance would strengthen credibility."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not applied"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wBwopyxqiZ", "forum": "wyJ4YagQqw", "replyto": "wyJ4YagQqw", "signatures": ["ICLR.cc/2026/Conference/Submission7551/Reviewer_MqQy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7551/Reviewer_MqQy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7551/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154016883, "cdate": 1762154016883, "tmdate": 1762919645247, "mdate": 1762919645247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}