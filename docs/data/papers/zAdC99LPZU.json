{"id": "zAdC99LPZU", "number": 20696, "cdate": 1758309113578, "mdate": 1759896963510, "content": {"title": "Graph neural networks extrapolate out-of-distribution for shortest paths", "abstract": "Neural networks (NNs), despite their success and wide adoption, still struggle to extrapolate out-of-distribution (OOD), i.e., to inputs that are not well-represented by their training dataset. Addressing the OOD generalization gap is crucial when models are deployed in environments significantly different from the training set, such as applying Graph Neural Networks (GNNs) trained on small graphs to large, real-world graphs. One promising approach for achieving robust OOD generalization is the framework of neural algorithmic alignment, which incorporates ideas from classical algorithms by designing neural architectures that resemble specific algorithmic paradigms (e.g. dynamic programming). The hope is that trained models of this form would have superior OOD capabilities, in much the same way that classical algorithms work for all instances. We employ sparsity regularization as a tool for analyzing the role of algorithmic alignment in achieving OOD generalization, focusing on graph neural networks (GNNs) applied to the canonical shortest path problem.  We prove that GNNs, trained to minimize a sparsity-regularized loss over a small set of shortest path instances, are guaranteed to extrapolate to arbitrary shortest-path problems, including instances of any size. In fact, if a GNN minimizes this loss within an error of $\\epsilon$, it computes shortest path distances up to $O(\\epsilon)$ on instances. Our empirical results support our theory by showing that NNs trained by gradient descent are able to minimize this loss and extrapolate in practice.", "tldr": "This paper provides the first rigorous guarantee that graph neural networks can extrapolate out-of-distribution, establishing a novel approach for studying generalization in neural algorithmic reasoning.", "keywords": ["Shortest paths", "extrapolation", "ood generalization", "neural algorithmic alignment", "graph neural network"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7d87ffee24656ea4fad5174a71f14ee19f3dafc.pdf", "supplementary_material": "/attachment/56eeed0d3aac30ace38a34aeab987ca625f8a4cb.zip"}, "replies": [{"content": {"summary": {"value": "This work aims to bridge the gap between graph algorithms and GNN models. The paper introduces a message-passing GNN variant that can approximate the steps of the Bellman-Ford algorithm when trained with a loss function that promotes sparsity. It is shown that the model can achieve OOD generalization in computing Bellman-Ford steps across varying graph structures. The theoretical results are supported by empirical evaluations on synthetic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The problem addressed in this paper is highly relevant. Designing GNNs to mirror the steps of graph algorithms is an interesting direction. So far, the ability of these models to simulate such algorithms has been supported mainly by empirical evidence.\n\n- The main strength of the paper lies in its theoretical contribution. The theoretical results demonstrate that message-passing GNNs that use the min aggregator can learn to simulate the Bellman–Ford algorithm. Once the models are properly trained, they can provably achieve OOD generalization, which is a particularly interesting result.\n\n- Overall, the paper is well-written and easy to read. It is also well-structured, thus enhancing the reader's understanding."}, "weaknesses": {"value": "- While the experimental results support the theoretical findings, the evaluation is restricted to simplified settings where only at most two steps of the Bellman–Ford algorithm are simulated in a single forward pass of the model. Therefore, it does not capture real-world scenarios in which nodes are distant from each other and more than two steps are required.\n\n- No empirical results are provided to demonstrate that the proposed GNN model can achieve high performance on tasks where shortest path distances between nodes are important. For instance, the authors could construct a synthetic dataset where node interactions depend on the nodes' shortest path distances, and show that the proposed model can outperform other standard GNN architectures.\n\n- Theorem 2.2 appears to have limited significance, since it is restricted to a single message-passing iteration and one iteration of the Bellman–Ford algorithm. As a result, its practical relevance is somewhat limited.\n\n- It is not clear how much the loss $L_{reg}(G_{train}, A_\\theta)$ deviates from its global minimum across different $\\mathscr{G}_K$ instances. If this deviation is large, the bound can be quite loose. In the current experiments, only a single training set $\\mathscr{G}_K$ is considered. A more comprehensive analysis is required.\n\n- In my understanding, the $\\Gamma$ map defined in l.214 produces node features that are all equal to 0 since $x_u=0$ for all $u \\in V$, $x(u,v) \\geq 0$ for all $u \\in V$ and $x(v, v) = 0$. Therefore, $x_u + x(v,v) = 0$. I suppose that the node itself should not be included into its neighborhood in the definition of the $\\Gamma$ map."}, "questions": {"value": "- What is the purpose of Theorem 2.2 given that Theorem 2.3 is more general? The result of Theorem 2.2 holds only for a single message-passing iteration which has limited practical significance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e4cU5dOD6H", "forum": "zAdC99LPZU", "replyto": "zAdC99LPZU", "signatures": ["ICLR.cc/2026/Conference/Submission20696/Reviewer_7xvo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20696/Reviewer_7xvo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496488377, "cdate": 1761496488377, "tmdate": 1762934073230, "mdate": 1762934073230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that Graph Neural Networks trained with a sparsity-regularized loss can learn to implement the Bellman-Ford shortest-path algorithm, thereby provably extrapolating to graphs far larger and structurally different from those seen in training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This works provides strong theoretical contribution, a formal extrapolation guarantee for message-passing GNNs on shortest path, going beyond prior intuitive “algorithmic alignment” discussions.\n2. The setup  formalization is clear and rigorous\n3. The theoretical results are backed up by empirical validations."}, "weaknesses": {"value": "1. The scope of this work is very narrow , the results are confined to shortest-path problems and to a Bellman-Ford–aligned architecture, and extension to other algorithms or even other graph problems remains untested.\n2. IMHO the main weakness  of this work is that the practical contribution is unclear. While it is fair that theory is done in a simplified way, it should be clear how the community benefits from these theoretical insights. E.g., whether the conclusions are carried to real-tasks and real-setting, or if we can derive practical insights or conclusions to how to improve GNN generalization in such settings.\nSpecifically, there are no experiments on real-world dataset, e.g. with the sparse-regularized training.\n3. Shortest path is not a task that is important of its own, as it has an efficient algorithm to solve without ML. Therefore, it has to be clear why is it interesting to know that GNNs can implement it in some setting other than for just knowing? While this is cool, IMHO it does not suffice without some clear insight based on this discovery."}, "questions": {"value": "Please provide any practical insight or evaluation for the theoretical results, e.g. evaluate whether Sparse-regularized training is also beneficial for other OOD size generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vvSJKZpbh6", "forum": "zAdC99LPZU", "replyto": "zAdC99LPZU", "signatures": ["ICLR.cc/2026/Conference/Submission20696/Reviewer_ycWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20696/Reviewer_ycWC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851658783, "cdate": 1761851658783, "tmdate": 1762934072577, "mdate": 1762934072577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates out-of-distribution (OOD) size generalization for shortest-path computation with message-passing GNNs. The authors propose a Min-Aggregation GNN (MinAgg) designed to align with Bellman–Ford (BF) updates and train it using a sparsity-regularized objective. The key idea is that learning on a small, curated toy set can certify extrapolation to arbitrary-size graphs. Two theoretical results support the claim: (i) a $1$-layer theorem showing that near-zero loss forces the network to implement approximately one BF step; and (ii) a general theorem for $L$-layer MinAgg establishing that, under near-minimal $L_0$-regularized loss, the network correctly implements $K$ steps of BF. Empirical results corroborate the theory, demonstrating stronger size-OOD performance than unregularized baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a canonical algorithmic task (shortest paths) where alignment to BF is intuitive and verifiable, making the claims clear.\n\n2. Treating a small, carefully designed training set and a sparsity term as an OOD certificate is insightful and potentially portable to other algorithmic operators.\n\n3. The two theorems are precise, with explicit error control."}, "weaknesses": {"value": "1. The guarantees are proved for $L_0$ regularization, whereas training uses $L_1$, which leaves a theory–practice gap.\n\n2. The theory guarantees only $K$ BF steps and requires $L \\ge K$. Thus, arbitrary size extrapolation is limited to $\\le$ K hops, and scaling to larger effective diameters demands larger K (hence deeper networks) or iterative reuse of a trained K-step block.\n\n3. Graphs are restricted to undirected graphs with non-negative weights and zero self-loops, while BF handles directed graphs and negative edges (without negative cycles). It is unclear whether the method and analysis can be extended to those settings."}, "questions": {"value": "1. How large can $K$ be before $G_K$ becomes impractical? Please evaluate and report for a larger K.\n\n2. Can the proof technique and MinAgg architecture be adapted to directed graphs and negative weights without negative cycles? If not, please clarify the limitations.\n\n3. Could you report inference cost compared to running classical BF, and any compression benefits from learned sparsity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9oAujg8Ez", "forum": "zAdC99LPZU", "replyto": "zAdC99LPZU", "signatures": ["ICLR.cc/2026/Conference/Submission20696/Reviewer_MFFo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20696/Reviewer_MFFo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894011144, "cdate": 1761894011144, "tmdate": 1762934072067, "mdate": 1762934072067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the submitted manuscript, the authors study the generalisation of Graph Neural Networks (GNNs) across graphs of different sizes for the shortest path problem. To do so, they study a GNN with minimum aggregation and prove strong theoretical results on how the output of the Bellman Ford algorithm both upper and lower bounds the predictions of their minimum aggregation GNN. This theoretical progress is complemented by strong empirical results of a regularised version of their GNN variant on several synthetic graph generators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and clear. \n\n- The motivation of the work is strong and the results are interesting. This seems like a very promising research direction to me. \n\n- The analysis of the learned parameters and the observation of how closely they align with the parameters of the Bellman Ford algorithm is very nice."}, "weaknesses": {"value": "- It seems to me that the experiments are on a very limited set of rather regular, synthetic graphs. Results on real-world data or at least more challenging synthetic graph generators would add to the empirical credibility of the work (see my questions for suggestions in this direction).\n\n- I believe that some of the results are not quite correctly interpreted."}, "questions": {"value": "1] I am very happy to see that you use the minimum as the aggregation function in your GNN. It is a well-motivated use of this function and makes for a suitable GNN. But in practice, I am not sure how the gradient flow of your model works then. The derivative of the min function is not continuous, so how are you able to differentiate through it? \n\n2] In your proposed solution, you rely on the insight that the underlying algorithmic solution, i.e., the Bellman Ford (BF) algorithm, is sparse in the parameters. Your general formulation is not able to discover the BF algorithm and you need to regularise the loss to obtain it. Could you comment on the degree to which this approach generalises; are most algorithms on graphs sparsely parameterised and therefore the regularisation of GNNs is a promising direction in general to solve algorithmic problems or is the current approach unique to the shortest path problem? \n\n3] It seems to me that the experiments are insufficient to demonstrate that your proposed approach is of practical relevance to the shortest path problem. \n\n3.1] The results you display in Table 1 demonstrate results on growing graphs with a constant expected degree. This may be a rather easy generalisation scenario since the local 'topology' of each graph may stay constant if the expected degree of each node on all graphs is equal. Would the same results hold if you randomly sample $p$ from a distribution for the test set? \n\n3.2] The test set up you describe in Lines 402-10 states that the test set contains 3-cycles, 4-cycles, complete graphs and ER graphs with $p=0.5$. May this collection of test graphs not be too simple? On complete graphs all shortest paths are 1 and cycles also have a relatively regular shortest path structure. Would your results still hold if you include more complex graph generators, like for example stochastic block models or Barabási–Albert graphs? \n\n3.3] Generally, it seems to me that it would be good to train on subgraphs of real world data and observe whether you generalise to the whole graph or whether you are able to partition graph level datasets into smaller and larger graphs and to observe whether training on the smaller graphs allows you to generalise to the whole graph. \n\n4] In Line 425 you state that the results in Figure 4 (a) and (b) validate your result in Theorem 2.3. I am not sure how direct this validation is. Is it not just two disjoint pieces of evidence pointing to the same conclusion? Would you not need to analytically calculate the bound that you derive in Theorem 2.3 and show that your predictions fall within the bound to speak of real validation of the theorem (I apologise if the bound value can be deduced from Figure 4, I was not able to do so based on my understanding). \n\n5] In Lines 436-7 you state that the test error of the $L_1$-regularised model does not accumulate. I could not find this in the results. For the regularised model we observe an increase in the error both as the graph size increases and if we compare row entries of columns 2 and 4 in Table 1. Am I misreading the table or should this claim be softened?\n\n6] Minor Comments:\n\n6.1] In Line 181 you write that the \"initial conditions and final answer are therefore contained in the node embeddings\". I did not fully understand this. Which final answer are you putting in the node embeddings? The example that you provide for the shortest path problem, in which you distinguish the source node from all other nodes in the node embeddings does not seem to include an embedding component that reveals final answers. \n\n6.2] Figure 1 is not discussed in the text and I personally feel that this general visualisation of GNNs does not contribute much to the understanding of your work. I think it could be moved to the appendix at almost no cost to your paper. \n\n6.3] Your Figures in Appendix D don't seem to have captions and there is some characters in the pdf \"[t]\" suggesting that there may be an error in the tex associated with these figures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cT4WZXW6N8", "forum": "zAdC99LPZU", "replyto": "zAdC99LPZU", "signatures": ["ICLR.cc/2026/Conference/Submission20696/Reviewer_11s3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20696/Reviewer_11s3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982083101, "cdate": 1761982083101, "tmdate": 1762934071426, "mdate": 1762934071426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}