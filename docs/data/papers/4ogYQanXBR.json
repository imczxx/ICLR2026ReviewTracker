{"id": "4ogYQanXBR", "number": 19067, "cdate": 1758293244196, "mdate": 1758816875270, "content": {"title": "Strategic Tradeoffs Between Human and AI Agents in Bargaining Games", "abstract": "Coordination tasks traditionally performed by humans are increasingly being delegated to autonomous agents. As this pattern progresses, it becomes critical to evaluate not only these agents' performance but also the processes through which they negotiate in dynamic, multi-agent environments. Furthermore, different agents exhibit distinct advantages: traditional statistical agents, such as Bayesian models, may excel under well-specified conditions, whereas large language models (LLMs) can generalize across contexts. In this work, we compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in a dynamic negotiation setting that enables direct, identical-condition comparisons across populations, capturing both outcomes and behavioral dynamics. Bayesian agents extract the highest surplus through aggressive optimization, at the cost of frequent trade rejections. Humans and LLMs can achieve similar overall surplus, but through distinct behaviors: LLMs favor conservative, concessionary trades with few rejections, while humans employ more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find that performance parity -- a common benchmark in agent evaluation -- can conceal fundamental differences in process and alignment, which are critical for practical deployment in real-world coordination tasks.", "tldr": "", "keywords": ["Negotiation", "Language Model", "Agent", "Human", "strategic tradeoff"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9a077b79f34f362865543f7ac7928adfd00b55e1.pdf", "supplementary_material": "/attachment/3d36d837fca7f968f51e9ea0b2c131c1b5cfad80.zip"}, "replies": [], "withdrawn": true}