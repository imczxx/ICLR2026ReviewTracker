{"id": "HHYd4Pz5Lp", "number": 18368, "cdate": 1758286900257, "mdate": 1759897108185, "content": {"title": "DelRec: learning delays in recurrent spiking neural networks.", "abstract": "Spiking neural networks (SNNs) are a bio-inspired alternative to conventional real-valued deep learning models, with the potential for substantially higher energy efficiency. Interest in SNNs has recently exploded due to a major breakthrough: surrogate gradient learning (SGL), which allows training SNNs with backpropagation, strongly outperforming other approaches. In SNNs, each synapse is characterized not only by a weight but also by a transmission delay. While theoretical works have long suggested that trainable delays significantly enhance expressivity, practical methods for learning them have only recently emerged. Here, we introduce ``DelRec'', the first SGL-based method to train axonal or synaptic delays in recurrent spiking layers. DelRec leverages a differentiable interpolation technique to handle non-integer delays with well-defined gradients at training time. We show that trainable recurrent delays outperform feedforward ones, leading to new state-of-the-art (SOTA) on two challenging temporal datasets (Spiking Speech Command, an audio dataset, and Permuted Sequential MNIST, a vision one), and match the SOTA on the now saturated Spiking Heidelberg Digit dataset using only vanilla Leaky-Integrate-and-Fire neurons with stateless (instantaneous) synapses. Our results demonstrate that recurrent delays are critical for temporal processing in SNNs and can be effectively optimized with DelRec, paving the way for efficient deployment on neuromorphic hardware with programmable delays. Our code is available at https://anonymous.4open.science/r/Recdel-4175.", "tldr": "We introduce ``DelRec'', the first surrogate gradient learning based method to train axonal or synaptic delays in recurrent spiking layers, which improves the SOTA on several temporal datasets.", "keywords": ["Spiking neural network", "Recurrent neural network", "neuromorphic computing", "computational neuroscience"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0358c3189b07c044146e80e10cba75b6500ee04.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DelRec that allows recurrent axonal delays to be learnable parameters. Given the use of real value delays, the conventional backprop can be used together with gradual quantization of delays. The proposed method was applied to several sequential data like SHD, SSC, PS-MNIST."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Propose of novel delay quantization algorithm: the authors propose a novel algorithm for gradual delay quantization."}, "weaknesses": {"value": "Limited novelty: A very similar method was proposed back in 2018 (see SLAYER in NeurIPS 2018). SLAYER is an SNN-training framework based on error-backprop with SGL. The only difference of the proposed work with SLAYER lies in its application to recurrent-type SNNs and delay quantization.\n\nWeak evaluation of delay quantization: it is not clear the value of the delay quantization method since it was not analyzed in the present manuscript.\n\nUnclear scalability: the proposed method was applied to only small datasets.\n\nUnfair comparison: the accuracy of the proposed method on the datasets was not compared with SoTA accuracy. I found much higher accuracy than those used in the comparison. Further, DCLS’s accuracy in Table 2 should be 95.07 rather than 93.77."}, "questions": {"value": "The authors should explain why some benchmark accuracy reported in this manuscript is different from the real one."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "misVjhxRgq", "forum": "HHYd4Pz5Lp", "replyto": "HHYd4Pz5Lp", "signatures": ["ICLR.cc/2026/Conference/Submission18368/Reviewer_MFAp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18368/Reviewer_MFAp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888652196, "cdate": 1761888652196, "tmdate": 1762928077002, "mdate": 1762928077002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to improve the performance of recurrent spiking neural networks by introducing tunable axonal/synaptic delays. This work leverages the standard BP with surrogate gradients to train such networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Pros**：\n\n- The paper looks into learning delays in recurrent spiking networks, which has not be extensively explored in the past.\n\n- The paper is well written, easy to follow while some of the key ideas should be better motivated. \n\n- Only pretty modest performance gain is demonstrated."}, "weaknesses": {"value": "- In general the notion of learnable delays is well understood although this was mostly explored in feedforward networks. Extending it for recurrent networks is relatively stratforward.\n\n- The proposed technique for handling learnable delays in recurrent spiking nets doesn't seem to have much novelty. This only involves with some relatively minor technical handling.\n\n- The main point of the paper centers around (9) where a targeted real-valued delay gets mapped to a set of integer delay values with a width parameter $\\sigma$. Why is this needed? Is the main motivation just to smooth out the discreteness?"}, "questions": {"value": "- Around (12), it is mentioned, based on a real-valued delay, the recurrent inputs are \"scheduled\" within a limited range of time steps? What do you mean by \"scheduling\"? Do you approximate the real-valued delay by randomly selecting an integer delay value in the range? What's the impact of doing this on training?\n\n- It is also mentioned that in the end, the support of the integer delays reduces to two adjacent time points， and  rounding to the nearest integer delay is performed for inference. What is the accuracy impact of doing this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t8ifAmcVi1", "forum": "HHYd4Pz5Lp", "replyto": "HHYd4Pz5Lp", "signatures": ["ICLR.cc/2026/Conference/Submission18368/Reviewer_rGry"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18368/Reviewer_rGry"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976870481, "cdate": 1761976870481, "tmdate": 1762928076424, "mdate": 1762928076424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DelRec, a novel method for learning neuron-level recurrent connection delays in SNNs. Its core idea is to use differentiable interpolation techniques to transform the discrete delay optimization problem into a continuous gradient optimization problem, thereby enabling end-to-end training within the mainstream Surrogate Gradient Learning (SGL) framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core contribution of the paper is its successful integration of learnable recurrent delays into the SGL framework, achieving state-of-the-art (SOTA) performance on two recognized SNN temporal benchmarks (SSC and PS-MNIST) using only simple LIF neurons. And the ablation study on the SHD dataset (Figure 3) provides valuable insights into the performance and efficiency trade-offs of different delay mechanisms (feedforward vs. recurrent)."}, "weaknesses": {"value": "1. As shown in Table 1, the authors' reproduction results for ASRC-SNN significantly differ from the original paper (the original paper achieved 96.62% on PS-MNIST with 0.15M parameters, while this paper's reproduction is only 95.77%). Moreover, their arbitrary change of configuration for different datasets (e.g., using LIF for PS-MNIST but adding Ff. delays for SSC) has unclear motivations. This severely undermines the credibility of their SOTA claim.\n\n2. The DelRec method uses a larger $\\alpha$ value during initial training, which means a spike needs to be scheduled within a wide time window (Equation 12). This may incur significant memory and computational overhead. The paper completely lacks analysis of its method's spatial/temporal complexity in relation to maximum delay, nor does it provide any reports on training/inference time consumption. This makes it impossible to judge whether the method is feasible for processing large datasets requiring long-range dependencies (such as EEG signals) or on resource-constrained hardware.\n\n3. The experimental results of the paper show internal contradictions. On the SSC dataset (Table 1), the \"recurrent delay only\" model performs better than the \"recurrent and feedforward delay combined\" model. However, on the SHD dataset (Table 2), the authors claim that the combination of both works best. The authors do not provide any explanation for this inconsistency, which casts doubt on the reliability and generality of their conclusions.\n\n4. While learning recurrent delays within the SGL framework might be new, the idea of \"learning connection delays\" has long existed in the broader field of RNNs. The paper needs to discuss early related works (missing the related works section). Furthermore, the authors claim in the abstract that their work \"paves the way for neuromorphic hardware deployment,\" but the entire paper provides no discussion or evidence related to hardware, which overstates the contribution.\n\n5. There are multiple instances of unclear symbols and formula definitions in the paper. For example, $supp(h_\\alpha,d)$ in Equation (12) and Rec. Delays in Table 1 and 2 are undefined in the main text, seriously affecting the readability and reproducibility of the paper."}, "questions": {"value": "1.Please provide a detailed explanation of the complete hyperparameters used when reproducing ASRC-SNN. Why is there a discrepancy between your reproduction results and the original paper? Also, why are different network structure configurations used for ASRC-SNN on the SSC and PS-MNIST datasets?\n\n2.I recommend that you supplement your paper with an analysis of the time and memory complexity of the DelRec method. Specifically, how does its overhead scale with maximum delay and sequence length? \n\n3.Can you comment on the feasibility of this method for processing larger-scale temporal datasets (such as EEG)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KYNWgJ0Wgk", "forum": "HHYd4Pz5Lp", "replyto": "HHYd4Pz5Lp", "signatures": ["ICLR.cc/2026/Conference/Submission18368/Reviewer_izGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18368/Reviewer_izGg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329203325, "cdate": 1762329203325, "tmdate": 1762928075997, "mdate": 1762928075997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DelRec, a novel method for learning neuron-level recurrent connection delays in SNNs. Its core idea is to use differentiable interpolation techniques to transform the discrete delay optimization problem into a continuous gradient optimization problem, thereby enabling end-to-end training within the mainstream Surrogate Gradient Learning (SGL) framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core contribution of the paper is its successful integration of learnable recurrent delays into the SGL framework, achieving state-of-the-art (SOTA) performance on two recognized SNN temporal benchmarks (SSC and PS-MNIST) using only simple LIF neurons. And the ablation study on the SHD dataset (Figure 3) provides valuable insights into the performance and efficiency trade-offs of different delay mechanisms (feedforward vs. recurrent)."}, "weaknesses": {"value": "1. As shown in Table 1, the authors' reproduction results for ASRC-SNN significantly differ from the original paper (the original paper achieved 96.62% on PS-MNIST with 0.15M parameters, while this paper's reproduction is only 95.77%). Moreover, their arbitrary change of configuration for different datasets (e.g., using LIF for PS-MNIST but adding Ff. delays for SSC) has unclear motivations. This severely undermines the credibility of their SOTA claim.\n\n2. The DelRec method uses a larger $\\alpha$ value during initial training, which means a spike needs to be scheduled within a wide time window (Equation 12). This may incur significant memory and computational overhead. The paper completely lacks analysis of its method's spatial/temporal complexity in relation to maximum delay, nor does it provide any reports on training/inference time consumption. This makes it impossible to judge whether the method is feasible for processing large datasets requiring long-range dependencies (such as EEG signals) or on resource-constrained hardware.\n\n3. The experimental results of the paper show internal contradictions. On the SSC dataset (Table 1), the \"recurrent delay only\" model performs better than the \"recurrent and feedforward delay combined\" model. However, on the SHD dataset (Table 2), the authors claim that the combination of both works best. The authors do not provide any explanation for this inconsistency, which casts doubt on the reliability and generality of their conclusions.\n\n4. While learning recurrent delays within the SGL framework might be new, the idea of \"learning connection delays\" has long existed in the broader field of RNNs. The paper needs to discuss early related works (missing the related works section). Furthermore, the authors claim in the abstract that their work \"paves the way for neuromorphic hardware deployment,\" but the entire paper provides no discussion or evidence related to hardware, which overstates the contribution.\n\n5. There are multiple instances of unclear symbols and formula definitions in the paper. For example, $supp(h_\\alpha,d)$ in Equation (12) and Rec. Delays in Table 1 and 2 are undefined in the main text, seriously affecting the readability and reproducibility of the paper."}, "questions": {"value": "1.Please provide a detailed explanation of the complete hyperparameters used when reproducing ASRC-SNN. Why is there a discrepancy between your reproduction results and the original paper? Also, why are different network structure configurations used for ASRC-SNN on the SSC and PS-MNIST datasets?\n\n2.I recommend that you supplement your paper with an analysis of the time and memory complexity of the DelRec method. Specifically, how does its overhead scale with maximum delay and sequence length? \n\n3.Can you comment on the feasibility of this method for processing larger-scale temporal datasets (such as EEG)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KYNWgJ0Wgk", "forum": "HHYd4Pz5Lp", "replyto": "HHYd4Pz5Lp", "signatures": ["ICLR.cc/2026/Conference/Submission18368/Reviewer_izGg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18368/Reviewer_izGg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18368/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329203325, "cdate": 1762329203325, "tmdate": 1763543653844, "mdate": 1763543653844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}