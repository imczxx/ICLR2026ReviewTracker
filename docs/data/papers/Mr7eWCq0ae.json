{"id": "Mr7eWCq0ae", "number": 6084, "cdate": 1757952407113, "mdate": 1759897936097, "content": {"title": "When Two is Enough: CoT–PoT Ensembling for Efficient Self-Consistency in LLM Reasoning", "abstract": "Self-consistency (SC) is a popular technique for improving the reasoning accuracy of large language models by aggregating multiple sampled outputs, but it comes at a high computational cost due to extensive sampling. We introduce a  hybrid ensembling approach that leverages the complementary strengths of two distinct modes of reasoning: Chain-of-Thought (CoT) and Program-of-Thought (PoT). We describe a general framework for combining these two forms of reasoning in self-consistency, as well as particular strategies for both full sampling and early-stopping. We show that CoT-PoT ensembling not only improves overall accuracy, but also drastically reduces the number of samples required in comparison with the most efficient SC method. In particular, the majority of tasks can be addressed with *only two* samples, which has not been possible with any prior SC methods.", "tldr": "We propose a hybrid CoT–PoT ensembling framework that improves self-consistency accuracy while drastically reducing sampling cost—often needing only two samples per task.", "keywords": ["self consistency", "reasoning", "chain of thought", "program of thought", "large language models"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c9c2925695e16124dce5d9ff65b5d910d6d6a13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- Paper proposes **cross-modal self-consistency**: sample CoT and PoT, aggregate under full budget (CPMaj/CPMax/CPAgr) or **early-stop** using a **Bayesian agreement** model; also gives simple $a_2=1$ heuristics (CPAA/CPFA/CPFF).\n- Empirically improves over CoT-only/PoT-only self-consistency on math/tabular QA while **reducing samples** (often to two: one CoT + one PoT)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear practical goal:** leverage complementary error profiles of CoT vs PoT.\n- **Simple deployable heuristics:** $a_2{=}1$ rules are easy to implement and often efficient.\n- **Reasonable empirical coverage:** multiple datasets/models; ablations touch intra- vs cross-modal agreement."}, "weaknesses": {"value": "- **Novelty:** Section 3.1 largely amounts to ensembling CoT and PoT with voting variants; the main gain appears to come from cross-modal diversity, which is conceptually incremental.\n- **Narrow task scope:** all benchmarks reduce to numeric/program-executable answers; unclear applicability to open-ended reasoning.\n- **Decorative theory:** Section 2.2.1’s Bayesian agreement model seems ornamental; best-performing procedures reduce to ad-hoc cross-modal agreement checks (CPAA/CPFA/CPFF with $a_2=1$).\n- **Fairness:** Comparisons mix tool-using PoT vs tool-free CoT, no baseline with tool-augmented CoT under identical stopping.\n- **Sensitivity & tuning:** Early-stopping hinges on thresholds / priors (e.g., $ρ$, $a_2$); robustness is not convincingly demonstrated.\n- **Scope & metrics:** Only executable numeric tasks; no latency/token-cost reporting; limited statistical rigor (no CIs/significance)."}, "questions": {"value": "- **Necessity of Section 2.2.1:** What does the Bayesian agreement model add beyond the simple cross-modal agreement heuristics actually used (CPAA/CPFA/CPFF)? Can you show any setting where the Bayesian estimator changes decisions and improves over those heuristics?\n- **Two-sample claim robustness:** For the “two is enough” claim, provide per-dataset/model histograms of stop counts and accuracy conditioned on early stop vs continued sampling. How stable is the ~2-sample regime under domain shift or weaker models?\n- **Generality:** any results on non-executable, free-form reasoning (e.g., multi-hop QA, scientific explanations)?\n- **Fair baselines:** calculator-augmented CoT and PoT-only early-stopping under identical thresholds?\n- **Sensitivity:** curves for accuracy/efficiency vs $a_2$, $\\rho$, temperature, and max budget; robustness when $a_2<1$?\n- **Cost & latency:** token counts and wall-clock (including interpreter overhead) per dataset/model.\n- **Failure modes:** when CoT and PoT agree yet are wrong, what patterns dominate, and can agreement be qualified (e.g., sanity checks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DLo1qdLQkF", "forum": "Mr7eWCq0ae", "replyto": "Mr7eWCq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission6084/Reviewer_r1PH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6084/Reviewer_r1PH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608713161, "cdate": 1761608713161, "tmdate": 1762918454587, "mdate": 1762918454587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to improve self-consistency (SC) reasoning in large language models by combining two distinct reasoning modalities: Chain-of-Thought (CoT) and Program-of-Thought (PoT). The authors argue that CoT and PoT exhibit complementary error patterns—CoT being more flexible but error-prone in arithmetic, while PoT being computationally precise but symbolically brittle. They formalize the cross-modal agreement between these reasoning modalities through a Bayesian framework and propose both full-sampling and early-stopping strategies based on this formulation. Extensive experiments on multiple reasoning benchmarks and LLMs show that the approach achieves comparable or higher accuracy with far fewer samples, often requiring only two (one CoT and one PoT)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Intuitive complementarity between reasoning modalities:**\nCombining a natural-language stepwise reasoning modality (CoT) with a symbolic/programmatic modality (PoT) aligns with an intuitive notion of complementary error modes and increased diversity of reasoning traces. The framework captures this complementarity succinctly and leverages it for more efficient ensemble decisions.\n\n**Potentially general framework for multimodal or heterogeneous reasoning:**\nThe paper presents a coherent probabilistic (Bayesian) framework that operationalizes how cross-modal agreement can be interpreted as a confidence signal. Although the experiments focus on CoT and PoT, the proposed Bayesian agreement mechanism could, in principle, be extended to any cross-modality reasoning setting, where agreement between different reasoning forms (e.g., textual, symbolic, or formal) can serve as a confidence signal for ensemble consistency.\n\n**Empirical performance:**\n The method achieves remarkable efficiency improvements, solving the majority of tasks with only two samples while maintaining high accuracy."}, "weaknesses": {"value": "**Assumption about program decomposability / limits of PoT usage.**\nThe approach leverages PoT outputs as one reasoning modality and implicitly assumes that tasks can be represented or approximated programmatically. While the Case Study discusses weaker PoT capabilities in smaller models and introduces a self-induced PoT variant, it does not address tasks fundamentally unsuited to programmatic reasoning (e.g., tasks that do not decompose naturally into executable programs or symbolic forms), leaving the framework’s broader applicability unclear.\n\n**Missing comparison with advanced reasoning frameworks:**\nThe paper does not discuss or compare against other recent multi-step reasoning paradigms such as ReAct, Reflexion, or tool-augmented reasoning, which might also address efficiency and correctness issues at inference time.\n\n**Limited generalization analysis:**\nThe Bayesian model depends on seed probabilities $(c, a_1, a_2)$ estimated from data. For the data-independent variant, the method assumes $a_2 \\approx 1$ (i.e., agreement almost guarantees correctness). It is not demonstrated whether these parameters—or this assumption—generalize across diverse tasks or models."}, "questions": {"value": "**1. Applicability:**\nFor problem classes that are not easily representable as executable programs, what is the expected behavior of the CoT–PoT ensembling framework? Do you observe failure modes where PoT is systematically inapplicable or misleading, and how should practitioners detect or mitigate such cases?\n\n**2. Generalization of the Bayesian model:**\nThe Bayesian scheme uses seed probabilities estimated from held-out data. Can these learned parameters $(c, a_1, a_2)$ be transferred between datasets, tasks, or model families? If not, how sensitive is method performance to mismatches between training-held-out data and test distributions, and do you have practical recommendations for re-estimating or adapting these priors in low-data regimes?\n\n**3. Consistent error reduction:**\nHave you evaluated whether cross-modal agreement reduces the incidence of self-consistent but incorrect outputs (i.e., situations where multiple samples agree on a wrong answer)? In particular, can you relate your empirical findings to the phenomenon discussed in “Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs”—does cross-modal ensembling mitigate or merely shift such failure modes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XuMP1R1X1c", "forum": "Mr7eWCq0ae", "replyto": "Mr7eWCq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission6084/Reviewer_FMp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6084/Reviewer_FMp9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647185629, "cdate": 1761647185629, "tmdate": 1762918454116, "mdate": 1762918454116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method that combines CoT and PoT reasoning through majority voting. Experimental results demonstrate that this approach enhances reasoning accuracy and outperforms traditional majority voting techniques."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental setup is well-designed and yields strong results.\n\n2. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The idea presented in the paper is not particularly novel. Prior work from two years ago has already explored combining CoT and PoT [1]. Moreover, the combination of only CoT and PoT offers limited contribution, as there are several other reasoning methods, such as ToT [2], that could be considered. It would be more valuable to investigate the integration of a broader range of reasoning approaches.\n\n\n2. The PoT method is relatively outdated, and numerous recent approaches have advanced the use of code in reasoning—particularly with the emergence of reinforcement learning techniques, such as ReTool [3], as well as several deep research efforts. Given these developments, combining two older reasoning methods like CoT and PoT offers limited relevance and impact in the current landscape.\n\n[1] Automatic Model Selection with Large Language Models for Reasoning. EMNLP 2023\n\n[2] Tree of Thoughts: Deliberate Problem Solving with Large Language Models. NeurIPS 2023\n\n[3] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. Arxiv 2025"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nCZ1ZwMNtn", "forum": "Mr7eWCq0ae", "replyto": "Mr7eWCq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission6084/Reviewer_ecnh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6084/Reviewer_ecnh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833995355, "cdate": 1761833995355, "tmdate": 1762918453269, "mdate": 1762918453269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to combine chain-of-thought and program-of-thought as self-consistency measure. Experiments indicate that ensembling CoT-PoT improves accuracy, and is a more efficient approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "N/A The paper writing is clear, experiments are in general thorough."}, "weaknesses": {"value": "Unfortunately, I believe this paper is currently below the acceptance bar for the conference. Rather than continuing to refine it for submission, I would suggest that the authors consider discontinuing this project or substantially rethinking its core idea.\n\nFirst, the combination of CoT and PoT reasoning is a rather straightforward extension, and similar attempts have been explored several years ago. Conceptually, PoT does not provide a complementary signal to CoT. That is to say, if a problem cannot be solved by PoT, it is unlikely that CoT alone would succeed either.\nSecond, the experimental results show marginal improvements from combining CoT and PoT compared with existing approaches, validating what my understanding of this project."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qyJnccqph7", "forum": "Mr7eWCq0ae", "replyto": "Mr7eWCq0ae", "signatures": ["ICLR.cc/2026/Conference/Submission6084/Reviewer_bUyj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6084/Reviewer_bUyj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912289707, "cdate": 1761912289707, "tmdate": 1762918452845, "mdate": 1762918452845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}