{"id": "QKrtliKPXR", "number": 12957, "cdate": 1758212032295, "mdate": 1759897474272, "content": {"title": "CLOE: Christoffel LOss autoEncoder for anomaly detection", "abstract": "Semi-supervised anomaly detection plays a key role in diverse fields such as process monitoring, healthcare, and finance. However, lightweight methods often struggle with high-dimensional data and typically require careful tuning of multiple hyperparameters. Among existing approaches, Christoffel Function–based methods are attractive due to their simplicity, requiring at most a single hyperparameter. They also benefit from a well-established theoretical foundation that yields several interesting results for data science. Their main limitation, however, is poor scalability to high-dimensional settings. In this paper, we introduce CLOE, a new method that combines an autoencoder for dimensionality reduction with a Christoffel Function–based detector applied in the latent space. To better align representation learning with anomaly detection, we design a novel loss function that leverages the Christoffel Function to guide the autoencoder toward representations that better capture the support of the normal data distribution. We further propose a principled procedure to set the detection threshold and an efficient strategy to tune the single remaining hyperparameter. Experiments on multiple high-dimensional anomaly detection benchmarks demonstrate that CLOE achieves superior performance compared to existing methods, while preserving the lightweight and low-tuning advantages of Christoffel Function–based approaches.", "tldr": "", "keywords": ["Anomaly detection", "Christoffel function", "Joint optimization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/762c842b492e8b1f49fa5cc72615f964fc7d471f.pdf", "supplementary_material": "/attachment/74170b7f3072602224bc29eca41937c58d64f9c4.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes CLOE, a semi-supervised anomaly detection method that learns a low-dimensional latent representation with an autoencoder while regularizing the latent codes with the empirical Christoffel Function (CF). Training proceeds with pretraining (MSE), then joint training with an added CF term, and finally thresholding in latent space using the maximum CF value over the training set. The method targets CPU-friendly operation with a single main hyperparameter $n$, and is evaluated on 10 ADBench tabular datasets using AUROC and AP AUC. On average, CLOE ranks first on AUROC but not on AP AUC, and ablations suggest pretraining is important whereas joint training contributes modestly."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Neat coupling of a CF-based support estimator with AE training via a differentiable loss. Practical, lightweight emphasis (CPU-only).\n- Clear method description; principled tie-in to moment matrices and support estimation.\n- Algorithms, thresholds, and training stages are spelled out. Datasets and hyperparameters are tabulated."}, "weaknesses": {"value": "- Several influential reconstruction-based methods are missing from the comparisons, including approaches that project along reconstruction pathways, adversarial or probabilistic autoencoder variants, collaborative autoencoders, and recent reappraisals of reconstruction-based OOD detection. Including such baselines would contextualize CLOE within the broader autoencoder AD landscape.\n  - Kim, Ki Hyun, et al. \"Rapp: Novelty detection with reconstruction along projection pathway.\" International Conference on Learning Representations. 2019.\n  - Almohsen, Ranya, et al. \"Generative probabilistic novelty detection with isometric adversarial autoencoders.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n  - Pidhorskyi, Stanislav, Ranya Almohsen, and Gianfranco Doretto. \"Generative probabilistic novelty detection with adversarial autoencoders.\" Advances in neural information processing systems 31 (2018).\n  - Liu, Boyang, et al. \"RCA: A deep collaborative autoencoder approach for anomaly detection.\" IJCAI: proceedings of the conference. Vol. 2021. 2021.\n  - Zhou, Yibo. \"Rethinking reconstruction autoencoder-based out-of-distribution detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n- Recent work has highlighted empirical and theoretical limitations of autoencoder-based anomaly detection and even non-trained baselines. A brief discussion of why CLOE's design might escape these pitfalls would strengthen the framing.\n  - Ryu, Seunghyoung, Yonggyun Yu, and Hogeon Seo. \"Can untrained neural networks detect anomalies?.\" IEEE Transactions on Industrial Informatics 20.4 (2024): 6477-6488.\n  - Cai, Yu, Hao Chen, and Kwang-Ting Cheng. \"Rethinking autoencoders for medical anomaly detection from a theoretical perspective.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n- **The narrative claims broad superiority, yet the aggregate picture depends on the metric**: CLOE leads the AUROC mean ranking, while DRL leads the AP AUC mean ranking. Without decision-level analyses at the proposed threshold, it is difficult to assert superiority in operational terms. Conclusions should be tempered to reflect metric-dependent outcomes and the absence of threshold-specific evaluation.\n- **Threshold policy versus ranking metrics.** The method defines a detection threshold in the latent CF space using a training-set maximum and makes hard inlier–outlier decisions, yet almost all reported evaluation focuses on ranking metrics such as AUROC and AP AUC. This leaves the central thresholding policy empirically unvalidated with respect to operational error profiles. Threshold-dependent metrics such as FPR at fixed TPR, operating points on the precision–recall curve, or F1 at the chosen threshold are needed to substantiate the proposed decision rule. Moreover, setting the threshold as a training maximum risks an overly permissive operating point for the training distribution, with potential overfitting to noise or extreme but normal instances. A validation-quantile or cross-validation–based threshold selection would better reflect deployment.\n- **Hyperparameter selection procedure for the polynomial degree.** The rule for choosing the degree relies on a validation loss formed by a reconstruction term plus the mean CF term, accepted if it decreases after an initial burn-in. It is unclear that this surrogate correlates reliably with downstream detection quality, and the acceptance rule is heuristic. The reliance on the mean CF can obscure distributional sensitivity, and the subsequent appendix study shows insensitivity to the Christoffel weight, casting doubt on the practical influence of the CF term. A more systematic selection via grid or Bayesian search over several degrees using validation ROC or PR curves would be more defensible.\n- **Fixed latent dimension without justification.** The latent size is fixed to eight for all datasets despite large variation in input dimensionality and complexity. Without a sensitivity study across datasets, it is difficult to assess whether this bottleneck is limiting performance, particularly for very high dimensional data. Reporting results for multiple latent sizes on at least a few datasets would clarify robustness.\n- Removing joint training yields only marginal degradation on reported datasets, and the appendix shows the Christoffel weight sweeping across several orders of magnitude does not change outcomes. Together, these results suggest that the CF-guided loss may have limited or null effect in practice, potentially due to scaling, normalization, or gradient-flow issues. Diagnostics such as loss decompositions, gradient norms for the CF term, and controlled ablations across multiple datasets would be needed to confirm effective contribution."}, "questions": {"value": "- Please add comparisons against prominent autoencoder-based AD methods such as projection-path reconstruction, probabilistic adversarial autoencoders, collaborative autoencoders, and recent rethinks of reconstruction-based OOD detection, or justify their exclusion with citations and discussion.\n- Briefly explain why CLOE should remain effective under critiques that challenge the reliability of autoencoder-based AD and even untrained baselines, and indicate empirical checks that support this claim.\n- Please report confusion-matrix statistics and threshold-dependent metrics at your chosen threshold and at validation-quantile alternatives. How sensitive are deployment metrics to the threshold policy?\n- Could you show training curves that separate reconstruction and CF components, together with gradient norms of the CF term? Do latent distributions, margins, or support volumes change when enabling the CF loss compared to a plain autoencoder?\n- How well does the validation-loss heuristic predict downstream AP AUC or AUROC across seeds and datasets? Would a small grid over degrees, selected by validation ROC or PR, alter your final configurations?\n- Please include results for multiple latent sizes on at least two to three datasets, especially very high-dimensional ones, to rule out capacity bottlenecks.\n- Can you align validation-based tuning for baselines under the same protocol and enforce a shared compute and memory budget, reporting success and failure rates accordingly? This would clarify whether the observed gaps persist."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A1G7eCxv8I", "forum": "QKrtliKPXR", "replyto": "QKrtliKPXR", "signatures": ["ICLR.cc/2026/Conference/Submission12957/Reviewer_LUtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12957/Reviewer_LUtD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760845659761, "cdate": 1760845659761, "tmdate": 1762923714473, "mdate": 1762923714473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for semi-supervised anomaly detection combining auto-encoder and Christoffel function. Specially, the method first pretrains an AE using reconstruction loss. Then, the AE is trained by both reconstruction loss and empirical CF-based loss. After training, Christoffel value is used as anomaly score for each test sample."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written except some parts of background, making concepts of Christoffel function hard to understand.\n- The proposed method is effective in several datasets.\n- The concept of Christoffel function is novel in anomaly detection."}, "weaknesses": {"value": "- It seems that the proposed method largely rely on the AE architecture. But what is the motivation for using such architecture instead of a MLP? \n- The authors mentioned that the method is lightweight computational yet no theoritical and experimental time cost comparison is provided. \n- The authros mentioned that the method is evaluated on 10 high-dimensional datasets yet several datasets have less than 100 and even less than 10 faetures. I do not think such datasets are high-dimensional. \n- The baselines are not strong enough. The SOTA methods like [1], [2], [3] and several strong traditional methods like kNN, KDE are not compared.\n- Evaluation on more datasets should be included. The authors chose 10 datasets from ADbench for evaluation, yet ADbench includes 47 datasets. Evaluation on more datasets would provide a more comprehensive view of effectiveness of different methods. \n\n[1] Livernoche, Victor, et al. \"On diffusion modeling for anomaly detection.\" arXiv preprint arXiv:2305.18593 (2023).\n\n[2] Yin, Jiaxin, et al. \"MCM: Masked cell modeling for anomaly detection in tabular data.\" The Twelfth International Conference on Learning Representations. 2024.\n\n[3] Thimonier, Hugo, et al. \"Beyond individual input for deep anomaly detection on tabular data.\" arXiv preprint arXiv:2305.15121 (2023)."}, "questions": {"value": "- See weaknesses.\n- What is the motivation for introducing Christoffel function in AD?\n- In Table 3, it seems that the performance of proposed method largely relies on pretraining, what is the possible reason?\n- One of the strengths of CF might be that it could figure out a threshold $\\gamma_n$ automatically. Yet the authors provide no experiments to evaluate the effectivenss of such threshold compared to the ideal threshold since the two metrics AUROC and AUPRC requires no threshold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a3vJfoq34j", "forum": "QKrtliKPXR", "replyto": "QKrtliKPXR", "signatures": ["ICLR.cc/2026/Conference/Submission12957/Reviewer_JJwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12957/Reviewer_JJwm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540298905, "cdate": 1761540298905, "tmdate": 1762923713775, "mdate": 1762923713775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an anomaly detection framework that leverages the **Christoffel Function (CF)** as a regularization term for representation learning. An autoencoder is used for dimensionality reduction, and the training objective combines a reconstruction loss with a CF-based regularization term. The approach aims to achieve effective anomaly detection with fewer hyperparameters compared to prior works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Pros\n\n* Learning representations guided by the **Christoffel Function** provides a principled way to construct a latent space with certain **theoretical guarantees**, offering a meaningful direction for robust anomaly detection."}, "weaknesses": {"value": "## Cons\n\n* Although the paper claims to use only one hyperparameter (*n*), it actually introduces another regularization coefficient (**λ**) in the training objective, which also serves as a hyperparameter.\n* The paper lacks an **ablation study** on the impact of **λ**, which is necessary to assess sensitivity and stability.\n* The **evaluated datasets** are limited compared to strong baselines such as **DRL**, which were tested on over 40 datasets. It is also unclear whether the baselines were trained with the **same number of samples** for fair comparison.\n* The **limitations** of the proposed method are not discussed, including potential issues in scalability, representation quality, and robustness in high-dimensional or real-world data.\n* The paper lacks important **experimental details**, such as the architecture of the autoencoder and the variance of reported performance metrics in the results table."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5xcq14csW", "forum": "QKrtliKPXR", "replyto": "QKrtliKPXR", "signatures": ["ICLR.cc/2026/Conference/Submission12957/Reviewer_PEKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12957/Reviewer_PEKb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12957/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971828657, "cdate": 1761971828657, "tmdate": 1762923712998, "mdate": 1762923712998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}