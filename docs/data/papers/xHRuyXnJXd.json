{"id": "xHRuyXnJXd", "number": 13024, "cdate": 1758212783358, "mdate": 1763706488543, "content": {"title": "Traceable Black-Box Watermarks For Federated Learning", "abstract": "Due to the distributed nature of Federated Learning (FL) systems, each local client has access to the global model, posing a critical risk of model leakage. Existing works have explored injecting watermarks into local models to enable intellectual property protection. However, these methods either focus on non-traceable watermarks or traceable but white-box watermarks. We identify a gap in the literature regarding the formal definition of traceable black-box watermarking and the formulation of the problem of injecting such watermarks into FL systems. In this work, we first formalize the problem of injecting traceable black-box watermarks into FL. Based on the problem, we propose a novel server-side watermarking method, $\\mathbf{TraMark}$, which creates a traceable watermarked model for each client, enabling verification of model leakage in black-box settings. To achieve this, $\\mathbf{TraMark}$ partitions the model parameter space into two distinct regions: the main task region and the watermarking region. Subsequently, a personalized global model is constructed for each client by aggregating only the main task region while preserving the watermarking region. Each model then learns a unique watermark exclusively within the watermarking region using a distinct watermark dataset before being sent back to the local client. Extensive results across various FL systems demonstrate that $\\mathbf{TraMark}$ ensures the traceability of all watermarked models while preserving their main task performance. The code is available at https://anonymous.4open.science/r/TraMark.", "tldr": "", "keywords": ["Federated Learning", "Watermark", "Black-box watermark", "Intellectual property protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ab497727bfc316be9eb78f369d68eef7483e786.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a method to address the copyright issue in Federated Learning (FL) systems where clients might leak the shared global model. The authors introduce TraMark, a server-side watermarking framework designed to be both black-box (verifiable without parameter access) and traceable (capable of identifying the specific client who leaked the model). The core mechanism involves partitioning the model's parameter space into a 'main task region' and a 'watermarking region', identifying the latter by selecting the least important parameters after a warmup phase. Using a \"masked aggregation\" process, the server combines updates from all clients only in the main task region. It simultaneously preserves a distinct watermarking region for each individual client, into which a unique watermark is injected using a distinct dataset. This approach provides each client with a personalized model that maintains high performance on the primary task while embedding a unique identifier, crucially preventing the watermarks from being destroyed during the aggregation process. The authors' evaluation shows the method achieves high traceability with a minimal drop in main task accuracy and demonstrates robustness against removal attacks like pruning and fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper has a clear motivation to achieve black-box traceability in FL.\n- This paper provides a comprehensive evaluation of various datasets and both IID and non-IID settings.\n- This paper is generally well-written and easy to follow."}, "weaknesses": {"value": "- Insufficient ablation study. To better justify the necessity of the parameter partitioning, it may be better for the authors to include a comparison against a simpler baseline that does not use this partitioning. This would help quantify the impact of watermark collisions or main task degradation that the partitioning scheme is designed to prevent.\n- Inadequate evaluation of computational overhead. The proposed method requires the server to perform personalized watermark injection (fine-tuning) for every client in each round. This process could be much slower than white-box methods like FedTracker, especially as the number of clients increases. It may be better for the authors to evaluate the computational time cost with a varying number of clients.\n- Lack of clarity in reported results. Some reported data is imprecise. For example, in Table 1, the per-dataset Verification Rate (VR) is only represented by a checkmark (if >95%) or an 'X', with only an average VR reported. It may be better for the authors to report the specific VRs for all methods and datasets to allow for a more granular and direct comparison."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y8jBHSQu0n", "forum": "xHRuyXnJXd", "replyto": "xHRuyXnJXd", "signatures": ["ICLR.cc/2026/Conference/Submission13024/Reviewer_LhGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13024/Reviewer_LhGb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535606555, "cdate": 1761535606555, "tmdate": 1762923760411, "mdate": 1762923760411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of verifying model ownership and tracing leakage in Federated Learning under a black-box setting. The authors formalize the problem and propose a framework named TraMark, which leverages parameter space partitioning and masked aggregation. Experimental results demonstrate the framework's verification rate and its impact on main task performance, complemented by a hyperparameter analysis. Furthermore, the method exhibits robustness against attacks such as pruning and fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A clear formalization of the traceable black-box watermarking problem is provided in Sec. 3.\n\nThe experimental results demonstrate an excellent Verification Rate (VR) of approximately 99.17% with a limited drop in Main-task Accuracy (MA), especially when compared to the FedTracker method.\n\nThe evaluation is conducted across multiple datasets, and the analyses of robustness, hyperparameters, and other factors are detailed."}, "weaknesses": {"value": "As indicated in Table 8 of the appendix, the per-round computational overhead for TraMark's aggregation is over 70 times that of FedAvg. The authors rationalize this by citing the small number of clients in cross-silo scenarios, a justification that is not entirely convincing and severely limits the method's scope of application.\n\nThe paper appears to overlook the method's communication overhead. At the beginning of each training round, the server is required to send a unique, personalized model to every client. This results in a sharp increase in communication costs compared to broadcasting a single global model, which in turn restricts the method's applicability.\n\nThe method's reliance on constructing an independent, out-of-distribution watermark dataset and a unique output label space for each client is a strong assumption. In realistic Federated Learning scenarios, this requirement may not be feasible, thus limiting the method's practical scalability and applicability.\n\nThe paper uses the average accuracy across all personalized client models as the metric for MA. However, this average may conceal significant performance disparities, where some clients receive highly accurate models while others are left with poorly performing ones. This masks potential issues of unfairness in performance distribution.\n\nThe method, in its pursuit of model traceability, raises questions about its potential impact on privacy preservation—one of the core advantages of Federated Learning. A detailed discussion on this critical trade-off appears to be missing from the paper.\n\nThe paper does not discuss the method's feasibility and effectiveness in Federated Learning scenarios with asynchronous client participation. It is unclear how the proposed mechanism would perform under such conditions."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i0qWVu8dmy", "forum": "xHRuyXnJXd", "replyto": "xHRuyXnJXd", "signatures": ["ICLR.cc/2026/Conference/Submission13024/Reviewer_UwiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13024/Reviewer_UwiB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792134691, "cdate": 1761792134691, "tmdate": 1762923759388, "mdate": 1762923759388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose TraMark, a black-box model watermarking technique suitable for federated learning setups where clients might be potential model leakers. TraMark works when the central server is benign and implements the watermarking procedure. TraMark verification also enables traitor tracing. TraMark restricts watermarking to a small subset of the model parameters using binary masks and adapts the weight update procedure to produce watermarked global models per each client while preserving the existence of the watermark. I think the idea is nice and simple, but the recovery of watermarked weights and possible evasion methods are not included in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. TraMark partitions the model parameters to the main task and watermarking task regions. In this way, we can say that TraMark is model-agnostic.\n\nS2. The problem formulation and the insights in Section 2 are very clear. It is difficult to inject watermarks that satisfy black-box traceability while avoiding collusion, and I think the authors did a good job of formulating this difficulty."}, "weaknesses": {"value": "W1. Potential watermark detectability by malicious clients: \n\nWatermarked weights may be relatively easy to detect by malicious clients. They could analyze which parameters change more significantly or differently during training and identify trends in weight updates of received global models. Such differences, especially if certain parameters are updated disproportionately or the updates remain closer to zero, could reveal which weights are used to embed covert information (i.e., the watermark in this case). This issue may become even more pronounced with the warm-up phase, where clients suddenly see a change in weight updates after the warm-up. The authors should include experiments or analyses investigating this potential vulnerability.\n\nW2. Key discussions moved to the appendix:\n\nSeveral important discussion points are placed in the Appendix, without even a summary of the results in the main text. This weakens the perceived impact of the contributions. I recommend moving the experimental setup to the Appendix instead and bringing key takeaways or insights from the additional experiments currently in the appendix into the main text.\n\nW3. Other weaknesses:\n\nPlease refer to the detailed questions below for additional points of concern."}, "questions": {"value": "Q1. The authors should verify the correctness of the compared methods presented in the appendix. For example, FedTracker can perform ownership verification in a black-box manner, as the related paper explicitly states that \"the zero-bit backdoor-based watermark is feasible for ownership verification and can be verified through black-box access.\" Additionally, I do not agree with the claim that RobWe is less practical because watermarking is performed on the client side. In fact, this seems more practical: each client can have their own secret watermark, eliminating potential watermark collusion problems, and maintaining robustness even when the server is untrusted.\n    \nQ2. There is insufficient discussion of scalability. The paper only tests with 10–50 clients. How would the proposed method perform with 1000 or 10000 clients? Moreover, how does the approach scale in relation to collusion resistance and the need to maintain personalized global models for each client? \n    \nQ3. The authors consider federated averaging as the only aggregation method. How would TraMark perform with alternative aggregation strategies such as Krum or other robust aggregation methods?\n    \nQ4. The format of in-text references is wrong. The authors should also be included in parentheses.\n    \nQ5. What is the model accuracy after the warm-up phase? Could this intermediate model (which includes no watermarks) be distributed directly instead of the last trained model?\n    \nQ6. The assumption that the server has full access to all local models is too strong and contradicts one of the main motivations for federated learning: privacy preservation. This assumption should be relaxed or at least discussed in detail.\n    \nQ7. Table 1 only reports the average results. The authors should also include verification rates (VR) for each method on each dataset. The current figure does not clearly convey these proportions.\n    \nQ8. Have the authors considered the possibility of out-of-distribution (OOD) detection as an evasion strategy against the proposed watermarking method? Especially considering the fact that they are using OOD samples as watermarks. \n    \nQ9. The repository link provided in the OpenReview abstract results in an error (\"not found\"). However, the link in the PDF appears to work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1eK9SrKMSw", "forum": "xHRuyXnJXd", "replyto": "xHRuyXnJXd", "signatures": ["ICLR.cc/2026/Conference/Submission13024/Reviewer_1fMj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13024/Reviewer_1fMj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837834856, "cdate": 1761837834856, "tmdate": 1762923758826, "mdate": 1762923758826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "Dear all reviewers,\n\nWe once again thank you for taking the time to read our paper carefully and for providing insightful comments and constructive suggestions that helped improve the quality of our work. We hope that our responses have addressed your concerns and questions. All revisions in the manuscript have been marked in **blue**.\n\n*We are open to any further discussion.*\n\nRegards,\n\nAuthors of submission 13024"}}, "id": "GZ0Ig2Zvy0", "forum": "xHRuyXnJXd", "replyto": "xHRuyXnJXd", "signatures": ["ICLR.cc/2026/Conference/Submission13024/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13024/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission13024/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763706916873, "cdate": 1763706916873, "tmdate": 1763706916873, "mdate": 1763706916873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}