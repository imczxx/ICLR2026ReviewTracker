{"id": "Pxd5mjwznl", "number": 15419, "cdate": 1758251156128, "mdate": 1759897308173, "content": {"title": "Difference back propagation with inverse sigmoid function", "abstract": "Since the proposal of neural network, the derivative-based back propagation algorithm has been the default setting. However, the derivative for a non-linear function is an approximation for the difference of the function values, and it would be a more precise way to do back propagation using the difference directly instead of the derivative. While the back propagation algorithm has been the rule-of-thumb for neural networks, it becomes one of the bottleneck in modern large deep learning models. With the explosion of big data and large-scale deep learning models, a tiny change in the back propagation could lead to a huge difference. Here we propose a new back propagation algorithm based on inverse sigmoid function to calculate the difference instead of derivative, and verified the effectiveness with basic examples.", "tldr": "We propose a new back propagation algorithm that calculates the back propagatiion updates using the difference instead of the derivative from the activation function", "keywords": ["Machine Learning", "AI", "Algorithm", "Back Propagation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77b23c66f95ec1a7be3fbfba3900887d77690a8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes replacing the derivative of activation function with a difference quotient to fix an alleged inconsistency in backpropagation and claims this helps with vanishing gradients."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "The described inconsistency in the paper does not exist. Gradient descent adjusts parameters, and activations a or pre-activations z are functions of parameters, so there is no inconsistency between updated z' and a'. Therefore, the paper is trying to solve a non-existent problem by replacing true gradient of activations by its finite difference approximation in backpropagation. Also, ignoring decades of prior work on training neural networks, makes the contribution appear uninformed.\n\nThe authors are strongly encouraged to further strengthen their conceptual understanding of deep learning and conduct a more thorough literature review before attempting to develop and publish new methods."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dcj3ynTZfQ", "forum": "Pxd5mjwznl", "replyto": "Pxd5mjwznl", "signatures": ["ICLR.cc/2026/Conference/Submission15419/Reviewer_EQg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15419/Reviewer_EQg7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761352610947, "cdate": 1761352610947, "tmdate": 1762925695768, "mdate": 1762925695768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to modify the backpropagation equation in neural networks to improve training. They replace the derivative of the activation function (sigmoid here) by $(a-a')/(z-z')$. They observe improvements on neural networks with 3 and 5 neurons on a synthetic task."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The motivation is grounded as backpropagation is at the core of every model training."}, "weaknesses": {"value": "- Line 29: \"To our knowledge, no new method for performing backpropagation has been proposed.\". The authors seem to be unaware of all the literature about alternatives to backpropagation. More generally, no related work is discussed, there are only 10 references and the most recent one is from 2021. I would advise the authors to look up \"alternative to backpropagation arxiv\" on any search engine.\n- The maths are wrong. The authors assume that after a step the activation $a$ will move exactly in the direction of its gradient (Eq. 3), which is wrong. However it may be a good guess.\n- I do not understand why the authors use the inverse sigmoid function to recompute $z$ from $a$ when it has already been computed during the forward pass.\n- The experimental section is extremely lacking. A 3-neurons network on a synthetic task is not a good benchmark, we would expect at the very least larger networks (e.g. 2 hidden layers, dimension 128) on CIFAR10 for instance.\n\nThe paper is only 4.5 pages long, which leaves plenty of room to include more experiments, related works, etc."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "djNfbiKa0r", "forum": "Pxd5mjwznl", "replyto": "Pxd5mjwznl", "signatures": ["ICLR.cc/2026/Conference/Submission15419/Reviewer_Zs9A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15419/Reviewer_Zs9A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994389774, "cdate": 1761994389774, "tmdate": 1762925695031, "mdate": 1762925695031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submitted manuscript considers an alternative to the classical back-propagation (BP) for activations and proposes replacing it with a finite difference approximation. The obtained numerical results demonstrate that this approach leads to minor improvement in convergence for the considered toy models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The submission is clearly written, and equations support the proposed approach."}, "weaknesses": {"value": "I have identified many weaknesses in the submitted work and have listed the most crucial ones below.\n1. The motivation for the proposed modification of the backpropagation (BP) is confusing. The exact gradient is essential for optimizers that update the model's parameters. If one approximates the gradient with a finite difference, then optimizers may converge to the wrong quasi-optimal parameters that do not correspond to the original problem. Moreover, typically only the stochastic gradient estimate is available in BP, and it remains unclear how this factor can be combined with the proposed approach.\n2. No theoretical analysis or even intuition on how the proposed approach resolves the stated problem of \"Although the models have shown great performance, it seems we are facing a bottleneck because nowadays we need to enlarge the models to billions of parameters to improve the accuracy by only a few percentages.\"\n3. The proposed approach is empirically tested only for the toy models and toy datasets, which is insufficient to make any well-supported conclusion about its effectiveness.\n4. Although generalization ability is crucial for deep learning models, I see that such analysis is explicitly excluded from the consideration.\n5. The observed gain in cost (btw what is cost in the y-axis in Figures 2 and 4?)  looks not so large and can be explained with some random initialization. No proper statistical analysis of the significance of the presented gain is provided."}, "questions": {"value": "1. Do you have any results for medium or large-scale models and datasets? E.g., ResNet18 and CIFAR10 and/or some standard benchmark in NLP like LLaMa or GPT-like models?\n2. Backpropagation (BP) algorithms compute gradients of parameters to update them and minimize the loss function with a proper optimizer. Why have you considered the objective output of BP as an \"inconsistency\"? Inconsistency with respect to what?  \n3. What optimizers have you used to obtain the reported learning curves for BP and your approach?\n4. Cross-entropy loss function incorporated sigmoid activation and avoided the mentioned instabilities since the sigmoid function is not computed standalone. So, which cases (specific application tasks) are suitable for your method? \n5. How can you explain that there is no difference between your method and BP for the topic classification task presented in Figure 5? The zoomed plots do not convince of the stability of the gain since the noise is large. Averaging across multiple runs and plotting the standard deviation are necessary to support the conclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dI3B1VuSWi", "forum": "Pxd5mjwznl", "replyto": "Pxd5mjwznl", "signatures": ["ICLR.cc/2026/Conference/Submission15419/Reviewer_ZEzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15419/Reviewer_ZEzY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15419/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067976588, "cdate": 1762067976588, "tmdate": 1762925694400, "mdate": 1762925694400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}