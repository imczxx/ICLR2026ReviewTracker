{"id": "A3YMCRm9JJ", "number": 21568, "cdate": 1758319121430, "mdate": 1759896914782, "content": {"title": "C2AL: Cohort-Contrastive Auxiliary Learning For Large-Scale Recommendation Systems", "abstract": "Training large-scale recommendation models under a single global objective implicitly assumes homogeneity across user populations. However, real-world data are composites of heterogeneous cohorts with distinct conditional distributions. As models increase in scale and complexity and as more data is used for training, they become dominated by central distribution patterns, neglecting head and tail regions. This imbalance limits the model's learning ability and can result in inactive attention weights or dead neurons. In this paper, we reveal how the attention mechanism can play a key role in factorization machines for shared embedding selection, and propose to address this challenge by analyzing the substructures in the dataset and exposing those with strong distributional contrast through auxiliary learning. Unlike previous research, which heuristically applies weighted labels or multi-task heads to mitigate such biases, we leverage partially conflicting auxiliary labels to regularize the shared representation. This approach customizes the learning process of attention layers to preserve mutual information with minority cohorts while improving global performance. We evaluated C2AL on massive production datasets with billions of data points each for six SOTA models. Experiments show that the factorization machine is able to capture fine-grained user–ad interactions using the proposed method, achieving up to a 0.16\\% reduction in normalized entropy overall and delivering gains exceeding 0.30\\% on targeted minority cohorts.", "tldr": "We propose C2AL, an auxiliary learning method leveraging under-represented cohort contrasts to improve attention factorization machines, demonstrating global performance improvement through real-world offline and online tests.", "keywords": ["Recommendation system", "Computational advertising", "Multi-task learning", "Factorization machine"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95f768eb5b705daf77d63178f640c1ac277f88c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Cohort-Contrastive Auxiliary Learning (C2AL), a framework to mitigate representation bias in large-scale recommender systems. By identifying user cohorts with high predictive divergence and introducing contrastive auxiliary tasks during training, C2AL enhances attention diversity and improves minority cohort representation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It introduces a novel cohort-contrastive auxiliary learning approach that bridges multi-task learning and bias correction in recommendation systems.\n2. The paper provides clear gradient-based analysis linking auxiliary tasks to improved attention diversity and representation learning and offers interpretable insights through weight distribution and training dynamics visualization, enhancing model transparency.\n3. Applies easily to large-scale models with no additional inference cost, making it highly practical for industrial deployment."}, "weaknesses": {"value": "1. A key limitation of C2AL is its reliance on manually defined semantic axes for cohort segmentation (e.g., user age or advertiser size), without a clear or principled selection procedure. This manual choice introduces subjectivity and may lead to inconsistent results across datasets or platforms. Moreover, the method only leverages the two cohorts with the largest distributional divergence (“head” and “tail”), but the rationale for using exactly two is not well justified. Real-world data often exhibit more complex subgroup structures, and restricting to a binary contrast may oversimplify the heterogeneity. \n\n2. While the performance gains achieved by C2AL are statistically significant, the improvements are relatively small in absolute terms (around 0.1% in normalized entropy). \n\n3. In Figure 2, it is unclear whether the left and right plots correspond to experiments on different datasets or different model settings. While the visualization idea in Figures 2 and 4 is appealing, the baseline (blue) curves are largely obscured by the red ones, making it difficult to distinguish peak density and distribution width.\n\n4.The innovation needs further clarification. According to Equation (4), the method mainly separates head and tail data for modeling and introduces a regularization term in the latent space. This is a common approach for handling data imbalance. The authors should more explicitly articulate how C2AL differs from or improves upon existing latent-space alignment and auxiliary learning methods."}, "questions": {"value": "1. The paper does not provide a detailed analysis of how the auxiliary loss weights (λ_head and λ_tail) affect model behavior or performance. These hyperparameters likely play a crucial role in balancing the influence of primary and auxiliary gradients, especially under partially conflicting objectives. \n\n2. It remains unclear whether the observed improvements genuinely arise from using the two most divergent cohorts or simply from the introduction of auxiliary learning itself. If cohorts with smaller or random divergence were used, would the model still exhibit similar attention weight diversification? This ambiguity raises questions about whether the gains stem from cohort-contrastive design or merely from additional regularization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9K66tyjgjV", "forum": "A3YMCRm9JJ", "replyto": "A3YMCRm9JJ", "signatures": ["ICLR.cc/2026/Conference/Submission21568/Reviewer_bcT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21568/Reviewer_bcT9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761407654261, "cdate": 1761407654261, "tmdate": 1762941839374, "mdate": 1762941839374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of data heterogeneity in recommender systems leading to model bias toward the majority cohort and proposes a cohort contrastive auxiliary learning framework, C2AL. This framework first automatically discovers \"head\" and \"tail\" cohorts with high predictive distribution disparity and then constructs cohort-specific auxiliary classification tasks. The gradient signals from these auxiliary tasks serve as a functional regularizer, forcing the model's attention mechanism to learn denser and more diverse weight distributions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. C2AL addresses the problem of model bias toward the majority cohort.\n2. Gradient analysis demonstrates how the auxiliary loss injects cohort-specific gradient signals into the attention matrix.\n3. Experiments are conducted on a large-scale dataset."}, "weaknesses": {"value": "1. Although Figure 2 demonstrates the sparsity of attention weights, this does not prove that the selection of these features is driven by the majority cohort. A more detailed division of features into majority and minority cohorts should be performed.\n2. The motivation for constructing an auxiliary task on the pair of cohorts with maximal distributional disparity is unclear.\n3. The paper indicates the use of multiple distributional divergence metrics (such as KL divergence and Cosine similarity) to quantify the difference, but does not report how these metrics were chosen in the experiments.\n4. The performance improvement is not significant. Although the authors emphasize the significance of this improvement, no significance test is performed.\n5. The lack of an experimental setup makes the reproducibility of the proposed algorithm unconvincing.\n6. The lack of sensitivity analysis, for example, how the hyperparameters introduced in Equation 10 affect performance.\n7. The lack of ablation experiments, for example, to analyze the performance of using only one auxiliary task.\n8. The paper targets large-scale recommender systems, but there is no specific design for such systems; it appears to be just one application."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZgYOFsWXD", "forum": "A3YMCRm9JJ", "replyto": "A3YMCRm9JJ", "signatures": ["ICLR.cc/2026/Conference/Submission21568/Reviewer_LxWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21568/Reviewer_LxWz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761496801509, "cdate": 1761496801509, "tmdate": 1762941839066, "mdate": 1762941839066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work argues that user data in  RS exhibit heterogeneous distributions. A single learning objective may cause the model to focus mainly on the behavioral representations of majority users while neglecting long-tail users, leading to poorer accuracy for the latter.\nTo address this, the authors propose a contrastive auxiliary task during training. Experiments on industrial-scale datasets and online A/B testing demonstrate that the proposed C2AL method is effective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The research problem is valuable, as it focuses on the heterogeneous distribution of users.\n- C2AL is applied only during the training stage, resulting in minimal additional cost.\n- The effectiveness of the method is validated through online A/B testing.\n- The paper provides visualization analyses, examining the attention weight distribution to demonstrate the effectiveness of  C2AL."}, "weaknesses": {"value": "- The proposed method relies on manually defined priors to label and divide user cohorts, rather than an automated approach, which makes the manually set hyperparameters potentially influential.\n- Even among long-tail users, not all samples are truly “unique,” so simply partitioning users into groups may be an oversimplified and potentially unreasonable assumption.\n- The paper does not consider the optimization conflict between the auxiliary and primary tasks, and the hyperparameter λ still requires manual tuning, which may introduce risks.\n- Common advertising metrics such as AUC are not reported, even though I understand that NE is an important metric.\n- The work lacks validation results on public datasets."}, "questions": {"value": "1. In online advertising, multiple tasks are often trained jointly rather than optimized independently; in such cases, how to design labels for auxiliary tasks remains unclear.\n2. Is there any analysis of user embedding of long-tail users? \n3. Did you consider any multi-embedding method[1,2,3] to avoid conflict representions?\n4. Is the method still suitable for transformer-based recommendation model, e.g., [4]? \n\n[1] STEM: unleashing the power of embeddings for multi-task recommendation. AAAI 2024. \n[2] Ads Recommendation in a Collapsed and Entangled World. KDD 2024. \n[3] On the Embedding Collapse when Scaling up Recommendation Models. ICML 2024.\n[4] RankMixer: Scaling Up Ranking Models in Industrial Recommenders."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RIKfmvvRGJ", "forum": "A3YMCRm9JJ", "replyto": "A3YMCRm9JJ", "signatures": ["ICLR.cc/2026/Conference/Submission21568/Reviewer_HrEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21568/Reviewer_HrEm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734580376, "cdate": 1761734580376, "tmdate": 1762941838844, "mdate": 1762941838844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds on the DHEN architecture for multi-task recommendation and adds a cohort-level contrastive auxiliary learning idea. Basically, it introduces cohort-based auxiliary heads and a contrastive regularization term to make the shared representation more discriminative across different user or ad groups. The idea is quite straightforward — using group-level contrast instead of instance-level contrast — and it’s a reasonable extension of DHEN. The paper also gives some gradient analysis to explain how the auxiliary loss affects shared features. Overall, it’s a simple and practical modification with clear motivation, though not a big conceptual jump."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper starts from a reasonable motivation — improving the DHEN framework for multi-task recommendation — and the idea is clear.\n\nIntroducing cohort-level contrastive auxiliary learning is a natural and practically meaningful direction to enhance representation learning.\n\nThe method is simple, easy to implement, and can be smoothly integrated into existing systems.\n\nThe gradient decomposition analysis helps explain the intuition of reducing task interference."}, "weaknesses": {"value": "The novelty is limited. The idea of cohort-level contrast or group-based regularization has already appeared in prior work (e.g., fairness or group-robust learning). Here it feels more like an extension on top of DHEN rather than a new concept.\n\nThe so-called contrastive loss is closer to an auxiliary classification objective and doesn’t really exploit the core property of contrastive learning.\n\nExperiments are done on large industrial data but lack strong baselines (e.g., PLE, MMoE, GroupDRO, PCGrad, etc.), making it hard to tell whether the improvement is substantial.\n\nNo ablation studies are provided to show how different components (e.g., cohort definitions or loss weights) contribute to the result.\n\nThe reported improvements (0.05%–0.16%) are very small, and there’s no statistical significance analysis, which weakens the empirical claims.\n\nThe theoretical part is more interpretive than rigorous, without real theoretical innovation."}, "questions": {"value": "The paper motivates cohort-level contrastive learning as a way to capture heterogeneity among different user groups, but the experiments mainly focus on long-tail splits or data imbalance.\nCould the authors clarify why there is this mismatch between the motivation and the experimental design?\nWhy not explore more cohort dimensions (e.g., user interests, regions, device types) to demonstrate the method’s effectiveness under broader forms of heterogeneity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kEjOnsxX1X", "forum": "A3YMCRm9JJ", "replyto": "A3YMCRm9JJ", "signatures": ["ICLR.cc/2026/Conference/Submission21568/Reviewer_Xmm1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21568/Reviewer_Xmm1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968699889, "cdate": 1761968699889, "tmdate": 1762941838633, "mdate": 1762941838633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}