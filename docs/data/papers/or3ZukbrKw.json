{"id": "or3ZukbrKw", "number": 443, "cdate": 1756739915138, "mdate": 1763618569823, "content": {"title": "Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization", "abstract": "Multimodal large language models (MLLMs) are well suited to image aesthetic assessment, as they can capture high-level aesthetic features leveraging their cross-modal understanding capacity. However, the scarcity of multimodal aesthetic reasoning data and the inherently subjective nature of aesthetic judgment make it difficult for MLLMs to generate accurate aesthetic judgments with interpretable rationales. To this end, we propose Aes-R1, a comprehensive aesthetic reasoning framework with reinforcement learning (RL). Concretely, Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality chain-of-thought aesthetic reasoning data used for cold-start. After teaching the model to generate structured explanations prior to scoring, we then employ the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that jointly optimizes absolute score regression and relative ranking order, improving both per-image accuracy and cross-image preference judgments. Aes-R1 enables MLLMs to generate grounded explanations alongside faithful scores, thereby enhancing aesthetic scoring and reasoning in a unified framework. Extensive experiments demonstrate that Aes-R1 improves the backbone’s average PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar size. More ablation studies validate Aes-R1's robust generalization under limited supervision and in out-of-distribution scenarios.", "tldr": "", "keywords": ["Reinforcement Learning，Image Aesthetic Assessment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/494c2847e36a57330a8977a3bd304fa646cea840.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Aes-R1, a novel framework for Image Aesthetic Assessment (IAA) using multimodal large language models (MLLMs). It introduces two key components: (1) AesCoT, an automatic data pipeline that generates high-quality chain-of-thought (CoT) aesthetic critiques along five dimensions (light, composition, color, exposure, mood), and (2) Relative-Absolute Policy Optimization (RAPO), a reinforcement learning (RL) algorithm that jointly optimizes absolute score regression and relative pairwise ranking. The method is trained on only 15K samples and achieves state-of-the-art performance across multiple IAA benchmarks, significantly outperforming existing baselines in both PLCC and SRCC metrics. The paper also provides insightful ablation studies on reward design and the interplay between supervised fine-tuning (SFT) and RL, emphasizing the importance of moderate SFT for maintaining policy entropy and enabling effective RL exploration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* S1: Novel and well-motivated RL framework: RAPO elegantly combines absolute and relative rewards to address the dual nature of human aesthetic judgment—calibrated scoring and ordinal preference—resolving limitations of prior rank-only or score-only RL approaches.\n\n* S2: Aes-R1 achieves significant gains (e.g., +47.9% PLCC) over strong baselines with limited training data and demonstrates robust out-of-distribution generalization."}, "weaknesses": {"value": "* W1: AesCoT relies on GPT-4.1 for data generation, which may limit reproducibility and raise concerns about generalizability if weaker open-source models are used as the \"expert.\"\n\n* W2: It seems that the main contribution of this paper lies in the dataset, and there is not much innovation in terms of methodology."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9riGwxpdAd", "forum": "or3ZukbrKw", "replyto": "or3ZukbrKw", "signatures": ["ICLR.cc/2026/Conference/Submission443/Reviewer_FLJS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission443/Reviewer_FLJS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552678222, "cdate": 1761552678222, "tmdate": 1762915521729, "mdate": 1762915521729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Aes-R1, a two-stage framework for advanced Image Aesthetic Assessment (IAA). It first \"cold-starts\" an MLLM by fine-tuning it on AesCoT, a new dataset of high-quality aesthetic critiques, to instill reasoning patterns. It then applies a novel reinforcement learning algorithm, Relative-Absolute Policy Optimization (RAPO), which uses a unique reward function that jointly optimizes for both absolute score accuracy and relative ranking preference. This combined approach allows Aes-R1 to significantly outperform existing baselines, improving its backbone model's average PLCC/SRCC scores by 47.9%/34.8% with minimal training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Relative-Absolute Policy Optimization (RAPO) Algorithm\nThe paper introduces RAPO, a novel RL algorithm with a joint reward function that optimizes for both \"absolute scores\" and \"relative ranking.\" This design better reflects complex human aesthetic judgment.\n\n2. Effective Two-Stage \"SFT Cold-Start + RL Finetuning\" Framework\nThe authors designed a two-stage process that first uses SFT to inject necessary aesthetic reasoning knowledge, then uses RL to align the model's scoring preferences."}, "weaknesses": {"value": "1. **Reasoning Quality Is Not Quantitatively Evaluated.** \nThe paper's central claim—that SFT+RL training produces superior aesthetic reasoning compared to pure RL—is supported only by a single, qualitative case study. The work lacks any systematic, quantitative evaluation of the generated \"aesthetic analysis text.\" All metrics (PLCC/SRCC) focus exclusively on the final score's accuracy, failing to validate the \"advanced reasoning\" contribution mentioned in the title.\n2. The AesCoT reasoning data is generated by GPT-4.1, which the authors treat as a competent external source. However, GPT-4.1 performs poorly on direct aesthetic score prediction (as shown in Table 1), raising questions about its suitability as a source of high-quality aesthetic critique. While the paper does include a filtering process, a more thorough validation of AesCoT's explanation quality—either through human evaluation or comparison to expert-written analyses—would help justify the choice of GPT-4.1 as a reasoning teacher.\n3. The RAPO framework combines the relative and absolute rewards using a simple summation ($r = r_{rank} + r_{abs}$), which implies an equal 1:1 weighting. While this combined reward proves effective, the paper does not explore the relative influence of these two components. It would be valuable to include an ablation study on the weighting coefficients (e.g., $\\alpha \\cdot r_{rank} + \\beta \\cdot r_{abs}$) to better understand their interplay and to investigate whether a different balance could further optimize performance.\n4. The SFT cold-start phase is presented as critical, but its implementation lacks clarity. The authors mention both AesCoT-3K and AesCoT-10K datasets in the appendix but fail to specify which version was actually used for the 1-epoch SFT in the main experiments. This ambiguity makes it impossible to reproduce the key results or understand the precise data conditions for the model's \"cold start\"."}, "questions": {"value": "1. Regarding the SFT cold-start phase: The appendix mentions both AesCoT-3K and AesCoT-10K, but the main paper does not specify which was used for the 1-epoch SFT. To ensure the reproducibility of the key results, could the authors please clarify which dataset version (3K or 10K) was used?\n\n**Suggestion:**\nThere might be a typo on Line 142 (\"fro\" -> \"for\")."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zYG1n6n1PB", "forum": "or3ZukbrKw", "replyto": "or3ZukbrKw", "signatures": ["ICLR.cc/2026/Conference/Submission443/Reviewer_xmgr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission443/Reviewer_xmgr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918450962, "cdate": 1761918450962, "tmdate": 1762915521563, "mdate": 1762915521563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors proposed Aes-R1. It is a comprehensive framework for Image Aesthetic Assessment that combines an aesthetic reasoning data pipeline (AesCoT) with a reinforcement learning algorithm (RAPO). It jointly optimizes absolute aesthetic scores and relative ranking consistency, significantly improving both accuracy and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors build AesCoT, an automatic pipeline that generates high-quality aesthetic reasoning data efficiently.\n2. They propose Aes-R1, a two-stage framework using RAPO to jointly optimize absolute scoring and relative ranking.\n3. Extensive experiments show Aes-R1 achieves state-of-the-art performance and generalization with limited training data."}, "weaknesses": {"value": "1. The authors modify the reward design based on GRPO to fit the specific characteristics of the IAA task. However, since Q-Insight has adopted similar methods on the same task, the contribution and novelty of this paper appear somewhat diminished.\n\n2. Compared with models that directly predict scores, this reasoning-based MLLM approach requires significantly more inference time, sometimes tens of times longer or even more, which poses clear challenges for large-scale inference. How do the authors consider this inefficiency issue?\n\n3. The presentation in Figure 5 is somewhat unclear. Given that score distributions can vary greatly across datasets, is it reasonable or meaningful to compare their absolute values directly?"}, "questions": {"value": "1. Compared with models that directly predict scores, this reasoning-based MLLM approach requires significantly more inference tim, sometimes tens of times longer or even more—which poses clear challenges for large-scale inference. How do the authors consider this inefficiency issue?\n\n2. The presentation in Figure 5 is somewhat unclear. Given that score distributions can vary greatly across datasets, is it reasonable or meaningful to compare their absolute values directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "daNFDnI88A", "forum": "or3ZukbrKw", "replyto": "or3ZukbrKw", "signatures": ["ICLR.cc/2026/Conference/Submission443/Reviewer_XmWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission443/Reviewer_XmWM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938206849, "cdate": 1761938206849, "tmdate": 1762915521437, "mdate": 1762915521437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces two key contributions: the AesCoT data construction pipeline, which provides high-quality data for SFT training, and the RAPO algorithm, which features a novel dual-reward mechanism comprising a Relative Rank Reward and an Absolute Error Reward, specifically tailored for IAA. However, the overall novelty of this work is limited, and there are certain deficiencies in both the data construction pipeline and the reward design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work supplies high-quality data for aesthetic reasoning by constructing an automated pipeline.\n2. It explores the application of reinforcement learning to the IAA domain, demonstrating its viability.\n3. Through a series of comprehensive experiments, the study furnishes robust empirical validation for its claims."}, "weaknesses": {"value": "1. The five evaluation dimensions proposed in the AESCOT process have certain limitations and lack support from relevant literature. In practical IAA, the assessment of image aesthetics is more flexible and cannot be comprehensively summarized by the five dimensions suggested by the authors. \n2. The filtering strategy mentioned in Section 3.2 and Appendix E is overly simplified, which raises considerable doubts about the credibility of the dataset.\n3. The method innovation is limited. Although the study focuses on applying RL to the IAA field, the overall pipeline closely resembles that of DeepSeek-R1, similarly employing high-quality data for SFT cold-start. Moreover, the proposed RAPO and DAPO are highly similar, with modifications to the reward function. The paper leans more toward application rather than introducing substantial innovations.\n4. The Relative Rank Reward relies on a critical assumption that the aesthetic score of an image follows a Gaussian distribution. All subsequent reward-rank calculations are based on this assumption, yet no empirical evidence is provided to validate that the scores indeed conform to a Gaussian distribution. The authors should substantiate the correctness of this assumption. \n5. Equation 3 essentially models the probability of which image is superior between two images. Why not directly employ the Bradley-Terry model for preference modeling instead?"}, "questions": {"value": "Please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8is2FANhtb", "forum": "or3ZukbrKw", "replyto": "or3ZukbrKw", "signatures": ["ICLR.cc/2026/Conference/Submission443/Reviewer_zj9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission443/Reviewer_zj9p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993858630, "cdate": 1761993858630, "tmdate": 1762915521277, "mdate": 1762915521277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}