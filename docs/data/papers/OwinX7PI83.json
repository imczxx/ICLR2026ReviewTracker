{"id": "OwinX7PI83", "number": 13690, "cdate": 1758220934273, "mdate": 1763707943107, "content": {"title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning", "abstract": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs.\nHowever, such vision driven embodied agents open a new attack surface: \nvisual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then executes an attacker-specified policy.\nWe introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation.\nAcross various embodied agent benchmarks  and MLLMs, \nBEAT achieves attack success rates up to 80\\%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39\\% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.", "tldr": "We introduce the first systematic framework that injects visual backdoors into MLLM-based embodied agents.", "keywords": ["multimodal large language models", "backdoor attack", "embodied agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e5c3e271bfcce7f54b8032f28b08a7c5c4c980f.pdf", "supplementary_material": "/attachment/4239c6ce04c7a97b91b64694811e99b727284e53.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes BEAT, the framework to backdoor MLLM embodied decision-making systems (e.g., home robots). The paper proposes contrastive trigger learning (CTL) to tighten the trigger boundary. Then, it utilizes two-stage fine-tuning (SFT and CTL) to improve the benign performance on embodied tasks and inject the backdoor behaviors while ensuring a low false trigger rate. In the experiments, three models (i.e., GPT-4o, Qwen2 7B, and InternVL3-8B) are tested on two datasets (i.e., VAB-OmniGibson and EB-ALFRED), showing high benign performance, attack success rate, and low false trigger rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Interesting topic**: The paper addresses an interesting and critical topic of embodied agent backdoor attacks. It shows that MLLMs can be backdoored to perform dangerous tasks when a visual trigger is presented.\n- **Good writing quality**: The paper clearly introduces the design challenge, the threat model, and the methodology. The scientific contribution is clear, and the content is easy to follow.\n- **End-to-end evaluation**: The paper conducts an end-to-end evaluation on a simulator to demonstrate the effectiveness of the backdoor attack, where the agents perform malicious tasks when being visually triggered."}, "weaknesses": {"value": "- **Lack of a comprehensive attack robustness discussion**: The paper discussed trigger robustness under an unseen trigger context. However, other factors influence the trigger robustness. For example:\n  - *System prompt robustness*: The attacker can only specify finite system prompts, but the end-user can change to whatever they want to maximize the benign performance.\n  - *Environment robustness*: The attacker can only train the backdoor models using finite environments, while the end user might deploy the model in different scenarios (e.g., different simulators or even a physical environment).\n- **Lack of defense discussion**: The paper proposes the attack without discussing any defenses. Therefore, it’s unknown if the attack can be simply defended, which might limit the real-world usability. For example, what if the backdoor model is further fine-tuned by the end-user on their specific tasks [r1]? \n\n---\n**Reference**\n\n[r1] Ni, Zhenyang, et al. \"Physical backdoor attack can jeopardize driving with vision-large-language models.\" arXiv preprint arXiv:2404.12916 (2024)."}, "questions": {"value": "- **Need for clarification**: The author mentioned that a bounding box annotation is visualized in the image to help the MLLMs' planning process. However, this raises the doubt whether the model is truly triggered by the objects or it is triggered by the text. If the actual trigger is the textual annotation, it largely weakens the main contribution of the paper (i.e., visual backdoor attacks). Therefore, this part needs to be analyzed and put in the main paper for justification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The author already included the ethics discussion."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kCp0TwfusQ", "forum": "OwinX7PI83", "replyto": "OwinX7PI83", "signatures": ["ICLR.cc/2026/Conference/Submission13690/Reviewer_24ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13690/Reviewer_24ke"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684378599, "cdate": 1761684378599, "tmdate": 1762924246654, "mdate": 1762924246654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BEAT, an innovative method that, for the first time, systematically investigates visual backdoor attacks on MLLM-driven embodied agents. The BEAT framework follows a two-stage training process: it first uses Supervised Fine-Tuning (SFT) to teach the model both benign tasks and the backdoor behavior simultaneously. It then introduces a novel method called Contrastive Trigger Learning (CTL). CTL frames the backdoor activation problem as a preference learning task, explicitly training the model to distinguish between \"trigger-present\" and \"trigger-free\" scenarios. This approach significantly enhances the precision and robustness of the backdoor activation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **A forward-looking and promising approach**: The threat model and method in this paper are highly novel. As MLLMs are increasingly deployed in robotics and autonomous agents, this type of visual backdoor attack is highly relevant to potential real-world security challenges. This work opens up an important and timely research direction for the field of embodied AI security.\n- The paper's methodology is easy to follow, and the experimental design is rigorous with a comprehensive evaluation suite."}, "weaknesses": {"value": "- **Trigger Complexity and Semantic Level**: Currently, the attack is limited to single, predefined objects like a \"knife\" or a \"vase.\" I think the authors could consider exploring more complex trigger conditions to improve the attack's stealth. For example, the trigger could be a combination of objects (\"a knife and a blue cube on the table\") or even more abstract scene semantics. While the proposed framework has the potential to be extended to such scenarios, the current experiments do not cover them.\n\n- **Generalization to Real-World Scenarios**: I'm curious—since BEAT is trained primarily on data from simulators, can this backdoor attack transfer to physical, real-world environments and still be reliably triggered (at least to some extent)? I would be very interested to see the authors' thoughts on this sim-to-real transfer challenge, as any results or discussion on this would significantly strengthen the paper's practical impact."}, "questions": {"value": "All in all, I really enjoyed this paper and appreciate the realistic challenge it presents. Visual backdoor attacks in embodied settings should certainly be a topic of continued focus for the community, and this paper provides an excellent initial and effective exploration of the problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qDzLGWB0GN", "forum": "OwinX7PI83", "replyto": "OwinX7PI83", "signatures": ["ICLR.cc/2026/Conference/Submission13690/Reviewer_tJsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13690/Reviewer_tJsF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830727163, "cdate": 1761830727163, "tmdate": 1762924246373, "mdate": 1762924246373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BEAT, a backdoor attack framework targeting MLLM-based embodied agents, where the agent behaves normally until it sees a specific visual trigger object (e.g., a knife or vase), then executes an attacker-defined multi-step malicious policy. Unlike prior LLM or MLLM backdoors, BEAT focuses on object-triggered policy-level attacks in interactive environments. The method constructs a dataset combining benign trajectories, backdoor trajectories, and contrastive trigger-pair demonstrations, then trains via a two-stage pipeline: supervised fine-tuning followed by Contrastive Trigger Learning (CTL) to sharpen trigger-conditioned behavior boundaries. Experiments on VAB-OmniGibson and EB-ALFRED with Qwen2-VL, InternVL, and GPT-4o show improved attack success rate (~80%), strong benign performance, and low false-trigger rate, including generalization to unseen trigger placements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This study is a novel problem focus: first systematic study of object-triggered, multi-step backdoor behavior in MLLM-embodied agents, going beyond prior single-turn or textual triggers. \n2. Contrastive trigger learning is intuitive and effective for sharpened trigger activation and reduced false positives. \n\n3.Experiments across multiple simulators and models demonstrate strong ASR, preserved benign performance, and OOD generalization."}, "weaknesses": {"value": "1. While the backdoor environment under multi-agent is new and novel, the methodology is simple. The object-trigger is similar with Shadowcase (Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models), the two stage training is also standard. The CTL is intuitive but adapted from similar problem.\n\n2. Backdoor trigger remains a visible object; evaluation lacks physical-world noise defenses (e.g., blur, smoothing), which would align more with realistic robotic/security settings.\n3. Experiments focus on simulated household tasks; results on broader embodied tasks or physical-robot environments would increase impact and realism.\n4. Defense coverage is limited to clean fine-tuning; stronger backdoor detection baselines (e.g., activation clustering, policy inconsistency tests) are not evaluated."}, "questions": {"value": "1. Does BEAT still activate correctly when the trigger is distracted or partially occluded, or mixed with similar objects (e.g., multiple knives)?\n2. Can the method generalize to more subtle triggers (e.g., stickers, patterns) rather than full objects? Any early observations or comments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OEqXlIK5A2", "forum": "OwinX7PI83", "replyto": "OwinX7PI83", "signatures": ["ICLR.cc/2026/Conference/Submission13690/Reviewer_cwZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13690/Reviewer_cwZC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890886622, "cdate": 1761890886622, "tmdate": 1762924246070, "mdate": 1762924246070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a backdoor framework on multi-turn MLLM-based embodied agents, BEAT, to implant backdoors actions once the target object appears in the scene. The framework is composed of two stages, including (1) SFT on mixed benign and backdoored data and (2) Contrastive Trigger Learning (CTL) applied to a new model based on the SFT model, which uses a DPO-style loss on matched trigger-present and trigger-free pairs. Experiments are conducted on open-sourced InternVL3-8B, Qwen2-VL-7B using LoRA, and proprietary model GPT-4o using only SFT on VAB-OmniGibson and EB-ALFRED benchmarks, showing that BEAT with both SFT and CTL can successfully inject backdoors while having comparable performance on the benign task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated on a novel backdoor attack targeting embodied agents within MLLM frameworks. The focus on multi-turn behavior further expands upon prior studies.\n- The paper’s writing and organization are very clear.\n- The validation is comprehensive across models and benchmarks, showing the generalizability of this approach."}, "weaknesses": {"value": "The attack is limited to a relatively simple setup, i.e., a single static trigger per benchmark, without complex actions. The complexity of injecting triggers into the dataset is also relatively expensive and not practical in real-world robotic pipelines. \n\nThe authors should also clarify below questions:\n\nExperiments\n\n- ASR is measured based on the final output; however, given the CTL objective, it would be more aligned to see the evaluation around the trigger appearing time (exact frame, or within a small window). \n- F1_BT is helpful, but it aggregates ASR and falsely triggered performance; please explicitly report FPR/ROC.\n- Figure 4. Which stage(s) are included? Why are both SR and ASR lower when k is small—what behaviors are produced?\n- All figures related to VAB-OmniGibson in the paper show the bounding box. During training/inference, are bboxes fed to the MLLM, or used offline only to construct labels?\n\nMissing ablations\n\n- Didn’t discuss the impact of $\\beta$\n- Didn’t conduct ablation on w/o SFT stage (e.g., train using CTL directly on base model, or directly apply DPO-like loss using ground-truth actions)\n\nMissing details\n\n- How is the data from OOD experiments collected?"}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qKaUCMg00T", "forum": "OwinX7PI83", "replyto": "OwinX7PI83", "signatures": ["ICLR.cc/2026/Conference/Submission13690/Reviewer_Ftfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13690/Reviewer_Ftfk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13690/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911120521, "cdate": 1761911120521, "tmdate": 1762924245701, "mdate": 1762924245701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}