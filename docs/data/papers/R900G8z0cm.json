{"id": "R900G8z0cm", "number": 17619, "cdate": 1758278411801, "mdate": 1759897164464, "content": {"title": "Task-Constrained Optimization for Entity-Relation Extraction", "abstract": "Multi-task learning for entity-relation extraction often suffers from implicit task interference and lacks explicit mechanisms to enforce structural task prioritization. We propose Hyperbolic Barrier-based Adaptive Hierarchical Optimization, a constraint-driven optimization framework that formulates entity recognition as a dynamic hard constraint via a numerically stable hyperbolic barrier function, while adaptively reweighting relation classification through a curriculum-based thresholding strategy. This principled framework enforces strict task prioritization throughout training, yielding absolute improvements of up to $6.4\\%$ in triplet F1 across five entity-relation extraction benchmarks. Furthermore, the proposed method generalizes effectively to structurally divergent domains such as recommender systems. Our findings underscore that explicitly modeling task hierarchies through constrained optimization represents a critical yet underexplored paradigm for achieving stable and effective multi-task learning.", "tldr": "", "keywords": ["Entity-relation extraction", "Multi-Task Learning", "Priority-Aware Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75807a0b7fdf6701ff6aefd221a1918f249a7945.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Hyperbolic Barrier-based Adaptive Hierarchical Optimization (HB-AHO), a principled optimization framework for multi-task learning (MTL) that explicitly enforces task hierarchy through differentiable hard constraints. The method introduces a hyperbolic barrier function to ensure smooth prioritization between entity recognition and relation classification in entity–relation extraction (ERE) tasks. The authors also provide theoretical guarantees (monotonicity, Lipschitz continuity, and convergence) and extend the framework to multi-level task hierarchies. Empirical evaluations across five ERE benchmarks and a large-scale recommendation dataset show consistent improvements (up to +6.4% triplet F1), suggesting generalization beyond NLP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and Principled Optimization Framework**  \n   - The paper goes beyond heuristic task-weighting schemes and introduces a theoretically grounded, constraint-driven formulation of hierarchical multi-task learning.  \n   - The hyperbolic barrier function φ(x) = tanh(3x + 1) is a novel design choice that maintains smoothness and bounded gradients—addressing classical issues in constrained optimization.\n\n2. **Theoretical Rigor and Clarity**  \n   - Section B provides clear proofs for monotonicity, Lipschitz continuity, and KKT-equivalence of the barrier function.  \n   - The inclusion of convergence, Lyapunov stability, and duality analyses is commendable and significantly raises the methodological credibility.\n\n3. **Comprehensive Experimental Evaluation**  \n   - Experiments span both sentence-level (NYT, WebNLG) and document-level (DocRED, CDR, GDA) datasets, as well as cross-domain validation on KuaiRand1k for recommender systems.  \n   - Results (Table 2, page 5) demonstrate consistent triplet F1 improvements of 4–6%, especially in document-level tasks where entity errors propagate more severely.\n\n4. **Robustness and Ablation Studies**  \n   - Extensive ablations (Tables 4–8, pages 7–15) confirm robustness to threshold ε initialization and curriculum decay factor γ.  \n   - Figure 3 clearly shows that the hyperbolic barrier leads to smoother and more stable training curves compared to polynomial or sigmoid alternatives.\n\n5. **Potential Cross-Domain Applicability**  \n   - The extension to recommendation tasks (Table 6, page 9) indicates that HB-AHO is architecture-agnostic, operating as an optimization layer rather than an architectural modification."}, "weaknesses": {"value": "1. **Incremental Conceptual Novelty**  \n   - While the hyperbolic barrier mechanism is elegant, the conceptual leap from existing constraint-based or Lagrangian MTL formulations (e.g., Cheng et al., 2025; Liu et al., 2021) is modest.  \n   - The method mainly combines known elements—differentiable barriers, curriculum thresholding, and hierarchical weighting—under a unified framework.\n\n2. **Limited Discussion on Computational Overhead**  \n   - The paper does not empirically quantify the additional computational cost introduced by the dynamic constraint enforcement and barrier term.  \n   - Although Section D (pages 17–18) provides asymptotic complexity analysis (O(N log(1/ϵ)) vs. O(N² log(1/ϵ))), wall-clock training comparisons would strengthen the claim.\n\n3. **Lack of Qualitative Analysis**  \n   - The study is purely quantitative; it would benefit from case studies or error analyses showing how HB-AHO alters learning trajectories or reduces entity boundary errors.\n\n4. **Ambiguity in Generalization Claims**  \n   - The cross-domain experiment (KuaiRand1k) is convincing but not deeply analyzed.  \n   - The paper asserts HB-AHO’s “domain-agnostic” potential without explaining how constraint design translates across modalities or task structures."}, "questions": {"value": "1. How sensitive is HB-AHO to noisy entity labels in the constraint task? Would barrier enforcement amplify noise?  \n2. Could the proposed method handle soft hierarchical dependencies (e.g., tasks with partial rather than strict order)?  \n3. Is there any evidence of training slowdowns due to deferred relation updates in large-scale datasets (DocRED, GDA)?  \n4. Have you considered integrating HB-AHO with LoRA-style parameter-efficient MTL (Yang et al., 2025) to verify compatibility with modular architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jewTpKUfvd", "forum": "R900G8z0cm", "replyto": "R900G8z0cm", "signatures": ["ICLR.cc/2026/Conference/Submission17619/Reviewer_7izR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17619/Reviewer_7izR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822631487, "cdate": 1761822631487, "tmdate": 1762927481932, "mdate": 1762927481932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hyperbolic Barrier-based Adaptive Hierarchical Optimization (HB-AHO), a constrained optimization framework for multi-task learning applied to entity-relation extraction (ERE). The approach reformulates multi-task training as a hierarchical optimization problem, utilizing a hyperbolic barrier function to prioritize entity recognition before relation classification. Experimental results across several ERE benchmarks and a recommendation dataset show consistent, though modest, improvements in performance.  However, the experimental setup lacks sufficient depth in terms of robustness testing, and the task setting does not offer significant novelty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a technically sound study with clear presentation."}, "weaknesses": {"value": "- **Contribution unclear** Its contribution is incremental. The \"hyperbolic barrier\" is essentially a smooth reweighting function applied to task losses. \n- **Lack of Robustness Analysis**  The paper does not examine how HB - AHO behaves when task constraints fail or when entity recognition performs poorly (i.e., beyond the idealized “entity-first” assumption). There is no stress testing for robustness under noisy or contradictory supervision, which is crucial for validating constrained optimization approaches.\n- **Limited Impact** I don't believe it will attract much attention from readers and professionals in the field."}, "questions": {"value": "no"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FKzm7CxsUV", "forum": "R900G8z0cm", "replyto": "R900G8z0cm", "signatures": ["ICLR.cc/2026/Conference/Submission17619/Reviewer_w4uJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17619/Reviewer_w4uJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898232417, "cdate": 1761898232417, "tmdate": 1762927481384, "mdate": 1762927481384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HB-AHO (Hyperbolic Barrier-based Adaptive Hierarchical Optimization), a differentiable and constraint-driven framework for multi-task entity–relation extraction (ERE). The approach explicitly encodes task hierarchies by treating entity recognition as a hard constraint and adaptively reweighting relation classification via a numerically stable hyperbolic barrier function with curriculum-guided thresholding. Theoretical analyses (monotonicity, Lipschitz continuity, and stability) and extensive experiments on five benchmarks demonstrate consistent triplet-F1 improvements, with partial generalization to recommender systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The method formalizes the intuitive hierarchy between entity and relation tasks, surpassing flat multi-task loss formulations.\n\nS2. Theoretical rigor: the hyperbolic barrier loss is well-analyzed for smoothness, feasibility preservation, and stability.\n\nS3. Strong empirical performance across five benchmarks, particularly on document-level datasets, with stable generalization across different backbones."}, "weaknesses": {"value": "W1. The claimed cross-domain generalization is weakly supported, relying on a single recommendation experiment without in-depth analysis of domain-specific hierarchies or barrier dynamics.\n\nW2. Limited discussion of failure cases—e.g., noisy entity supervision or task imbalance—where hierarchical coupling might degrade performance.\n\nW3. Curriculum scheduling details (e.g., initialization and decay of $\\varepsilon_t$) remain under-specified, affecting reproducibility.\n\nW4. Some relation-F1 fluctuations across backbones suggest that observed gains may partly stem from architecture effects rather than the hierarchy mechanism itself."}, "questions": {"value": "Q1. How does HB-AHO perform under noisy or weak entity supervision? Does the barrier amplify or mitigate such noise?\n\nQ2. What are the practical heuristics for setting and decaying \\varepsilon_t across datasets and multi-level hierarchies (N>2)?\n\nQ3. Are there qualitative or failure analyses showing when HB-AHO overemphasizes entity accuracy at the cost of relation generalization?\n\nQ4. In the cross-domain recommendation setting, what defines the hierarchical priority among tasks, and how sensitive are results to task order specification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6WQ5kE3NO1", "forum": "R900G8z0cm", "replyto": "R900G8z0cm", "signatures": ["ICLR.cc/2026/Conference/Submission17619/Reviewer_3x1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17619/Reviewer_3x1B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152418547, "cdate": 1762152418547, "tmdate": 1762927480898, "mdate": 1762927480898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the HB-AHO framework, a constraint-driven optimization paradigm for multi-task learning that explicitly encodes task dependencies. HB-AHO treats the ERE task, as a dynamic constraint using a hyperbolic barrier and a curriculum-based thresholding strategy, ensuring task prioritization throughout training. This approach enforces an \"entity-first\" regime, leading to improvements, including up to 6.4% absolute gains in triplet F1 across five ERE benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The strengths are:\n- It is a principled approach to task hierarchy enforcement.\n- The hyperbolic barrier ensures smooth and stable optimization.\n- The paper achieves good improvements in the triplet extraction task, which is a well studied task."}, "weaknesses": {"value": "The paper lacks a survey of barrier function-based methods for machine learning in general and multitask learning in particular. All the baseline MTL methods are simple gradient-based.\n\nThe paper lacks an analysis of the loss in computational efficiency due to the barrier function.\n\nThe performance on DcoRED and CDR is quite poor. Are these the absolute state of the art across all methods?"}, "questions": {"value": "Please see the last two points in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dBAjMvBiqc", "forum": "R900G8z0cm", "replyto": "R900G8z0cm", "signatures": ["ICLR.cc/2026/Conference/Submission17619/Reviewer_va7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17619/Reviewer_va7A"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17619/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762404440613, "cdate": 1762404440613, "tmdate": 1762927480224, "mdate": 1762927480224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}