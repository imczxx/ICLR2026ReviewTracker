{"id": "PfIAcdbce2", "number": 14403, "cdate": 1758234548864, "mdate": 1759897372342, "content": {"title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure—once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20\\%, whereas baselines reach up to 38.33\\%, while preserving—and sometimes improving—task accuracy (up to +3.67\\% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.", "tldr": "a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities", "keywords": ["multi-agent system", "safety", "reinforcement learning", "multimodal large language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8478836167594515c2fe74d98e3afe47adfb97ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new framework, AdvEvo-MARL, aimed at embedding safety awareness directly within large language model (LLM)-based multi-agent systems (MAS). While the conventional methods struggle to address the cross-agent safety risks or suffer from computational overhead from scaling up the number of guards, AdvEvo-MARL simultaneously trains attackers and defenders with reward design that balances safety, task accuracy, and format compliance. Furthermore, experiments conducted on several types of adversarial scenarios show improved task performance on benchmarks like AIME, GPQA, and LiveCodeBench, demonstrating that AdvEvo-MARL offers a principled approach for achieving both safety and performance in LLM-based multi-agent systems without relying on external guard modules."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) The proposed paper focuses on safety in LLM-based multi-agent systems (MAS), a problem that has become increasingly critical as the capabilities and deployment of large language models (LLMs) continue to expand across diverse domains. The authors emphasize the inevitable vulnerabilities of individual LLM agents, which are further amplified by the interaction dynamics among agents, significantly broadening the system’s attack surface. The paper directly engages with this challenge by proposing a framework that internalizes safety within task agents, allowing them to develop intrinsic robustness rather than depending solely on external verification modules or centralized guard agents. This design effectively mitigates the limitations of prior approaches.\n\n2) The authors propose a co-evolutionary learning paradigm, integrating adversarial training within a multi-agent reinforcement learning (MARL) framework to enhance system safety. As the attackers and defenders are jointly optimized, attackers continuously generate evolving jailbreak prompts, while defenders learn to resist these attacks and maintain task performance. This adversarial co-evolution mechanism represents a significant departure from traditional safety training approaches that rely on static datasets or isolated fine-tuning, which often lead to overfitting and poor generalization against adaptive adversaries. By enabling continuous adaptation through dynamic attacker–defender interactions, the framework ensures that defenders develop robust and generalizable safety behaviors rather than memorizing fixed threat patterns. \n\n3) The authors conduct evaluations across three representative multi-agent attack scenarios, including agent manipulation, message corruption, and user instruction hijackings. Furthermore, the experiments span multiple communication topologies. This design ensures that the results generalize across a range of real-world coordination structures commonly observed in multi-agent systems. Throughout these experiments, it is shown that AdvEvo-MARL consistently achieves comparably lower attack success rates (ASR), and in some cases even improves task utility. Overall, the experimental evaluation provides strong evidence of the potential of the proposed method as a framework for building safe and capable multi-agent systems."}, "weaknesses": {"value": "1) The paper seems to lack detailed descriptions and examples of the datasets used, particularly in the attacker warm-up phase and experimental setup. While the authors mention constructing the datasets $D_{init}$ and $D_{adv}$ for initializing attackers through supervised fine-tuning and imitation learning, the paper does not provide concrete examples of prompts, jailbreak strategies, or attack–defense interactions included in these datasets. Similarly, for the main experiments, the paper references several benchmark datasets considering system safety evaluation and general task evaluation, yet it does not include illustrative examples of how these prompts are structured or how adversarial modifications are applied. Without such examples, it becomes difficult for readers to fully grasp the nature and diversity of adversarial challenges that the proposed framework aims to handle. \n\n2) Although the authors state that both attackers and defenders are jointly trained using REINFORCE++ to improve system safety and task performance, the paper only provides limited conceptual background on how REINFORCE++ differs from or improves upon standard policy gradient methods such as PPO. Moreover, while the algorithm is cited as a recent approach for aligning LLMs, the paper does not clearly explain why it is particularly suited for the adversarial multi-agent reinforcement learning MARL setting used in AdvEvo-MARL. Providing a concise rationale would enhance the reader’s understanding of how REINFORCE++ contributes to the stability and effectiveness of the proposed adversarial training process. For instance, providing a comparative experiment between REINFORCE++ and PPO-based individual training would strengthen the justification.\n\n3) As LLMs advance and their usage expands, safety considerations have also received increasing attention. For example, works such as When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search [1] and Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models [2] highlight recent developments in LLM safety. It would be beneficial to include comparisons to recent works in addition to the proposed Challenger and Inspector. Moreover, while the paper describes Challenger as a self-verification strategy applied before response generation and Inspector as external guard agents that detect and correct malicious messages, the descriptions appear insufficient. To better emphasize the advantages of AdvEvo-MARL, it would be helpful to provide more detailed explanations of the baseline methods’ processes.\n\n[1] Chen, Xuan, et al. \"When llm meets drl: Advancing jailbreaking efficiency via drl-guided search.\" Advances in Neural Information Processing Systems 37 (2024): 26814-26845.\n\n[2] Pandit, Kartik, et al. \"Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models.\" arXiv preprint arXiv:2510.03520 (2025)."}, "questions": {"value": "1) The paper mentions that a representative limitation of out-of-task agent–based safety approaches is scalability, yet all experiments consider only 3 agents. We would be grateful if the authors could provide any insights or preliminary results on how the approach might perform in larger-scale settings or clarify the rationale for limiting the experiments to 3 agents. Additionally, with 3 agents, most groups are likely divided 2:1, in which case the single-agent group would only consider single-agent rewards. Could the authors kindly comment on how their approach compares to existing algorithms under such configurations, and whether the current reward design and group assignment mechanism would generalize to scenarios with more agents or more diverse group splits? Any thoughts on potential challenges or modifications needed to maintain safety performance as the number of agents scales up would also be highly appreciated.\n\n2) The paper presents an interesting framework for handling safety in multi-agent settings. We are particularly curious about how attacks evolve over time and how the severity or effectiveness of these evolved attacks is measured. Additionally, it would be valuable to understand how defenders that initially fail to counter such evolving attacks are able to adapt. Could the authors provide any experimental results, illustrative examples, or sample prompts that demonstrate the evolution of attacks and the corresponding adaptation of the defender? Insights on these points would help clarify the practical robustness of the proposed approach against dynamically changing adversarial behaviors.\n\n3) To account for each agent’s distinct objectives while also considering their interactions, reward modeling appears to play a central role in the proposed approach. In this context, the safety reward is assigned a value of 1 if the output is considered “safe.” We would appreciate it if the authors could clarify the criteria used to determine safety. Similarly, for format compliance, the conditions for satisfaction seem to be more flexible compared to safety or task utility. Could the authors elaborate on the rationale behind placing the reasoning process between 'think' tags and the final response between 'response' tags, as described in the paper? Furthermore, since the paper mentions prioritizing safety in the first half of training and reversing the weights afterward to emphasize task performance, it would be valuable to see experimental results illustrating the impact of different reward scaling on overall performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySSLmCmzn4", "forum": "PfIAcdbce2", "replyto": "PfIAcdbce2", "signatures": ["ICLR.cc/2026/Conference/Submission14403/Reviewer_yK4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14403/Reviewer_yK4m"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786264727, "cdate": 1761786264727, "tmdate": 1762924814032, "mdate": 1762924814032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AdvEvo-MARL proposes a co-evolutionary MARL framework with the goal of improving safety in multi-agent LLM systems. The method jointly optimizes the defender and the attacker, and it uses a shared, group-level mean-return baseline to stabilize this adversarial training. The result shows a reduction in attack success rate without sacrificing task performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work proposes a method to improve the safety of the multi-agent LLM by using joint optimization of defender and attacker to reduce the attack success rate of the attack.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "- The main concern I have with this work is the novelty. The joint training of defender and attacker seems relatively standard approach, while the group-level baseline seems to be quite similar to existing methods, such as GRPO (with the difference of not sharing weights between agents)\n- This work assumes the agents don't share weight. Is there a particular reason or setting where the agents must not share weights? If not, how much better does the non-share-weight agent perform compared to the share-weight agents?\n- Can this method be applied to other MARL algorithms, or is it strictly limited to REINFORCE++?\n- Currently, only 3 agents are used for evaluation. Does this method work if we scale up the agent size, which may increase the difficulty of detecting the attacker?"}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LUmCDsiQ4N", "forum": "PfIAcdbce2", "replyto": "PfIAcdbce2", "signatures": ["ICLR.cc/2026/Conference/Submission14403/Reviewer_L3xu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14403/Reviewer_L3xu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894606684, "cdate": 1761894606684, "tmdate": 1762924813357, "mdate": 1762924813357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdvEvo-MARL, an adversarial training framework for LLM-based Multi-Agent Systems (MAS). It co-trains the attackers and the defenders via Multi-Agent Reinforcement Learning (MARL) with a newly introduced public baseline. The attackers are initialized via self-supervised learning with collected high-quality adversarial prompts. Experiments show that AdvEvo-MARL outperforms three baselines (Valinna without any defense, Challenger that self-verifies each agent's inputs, and Inspector that detects malicious messages) across various MAS structures and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. Exploring adversarial training for robust LLM-based MAS is straightforward.\n3. Experiments show AdvEvo-MARL is effective across various tasks, improving the robustness of the LLM-based MAS against NefSafe, AutoInject, and UserHijack, without sacrificing the clean task performance."}, "weaknesses": {"value": "1. The technical contribution is limited. Although AdvEvo-MARL introduces a public baseline for MARL, it is too simplistic and lacks technical value. Even the ablation study (see Section 5.4) demonstrates that the public baseline yields marginal improvements in both performance and training stability.  Moreover, the examination in Figure 5 lacks the critical visualization of the variance region (maybe due to one-seed evaluation), which makes the comparison between the public-baseline and the no-baseline setting unreliable.\n2. Figure 4 shows that the diversity score of the attacker-generated prompts only varies within a small interval $[0.71,0.73]$, which cannot support the conclusion that \"the diversity of adversarial prompts generated by the attacker, reveals a non-monotonic but ultimately increasing trend over the course of training.\", Actually, there seems to be no trend at all for the diversisty of the generated prompts.\n3. There is a lack of detailed examples to show how the MAS interacts and how the attacker/defender evolves, reducing the clarity of this method. It is better to show the reader what the attack-generated prompts look like and how the MAS has gradually been defeated.\n4. The definition of Contagion Rate (why PR for shortening it?) is unclear. It is better to formalize all metrics with clear symbols rather than just a brief textual description. The discussion on the Contagion Rate of AdvEvo-MARL is also not easy to understand. It is better to provide a case study to show the difference between Attack Success Rate and Contagion Rate."}, "questions": {"value": "Please refer to Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f2G7eDXqXY", "forum": "PfIAcdbce2", "replyto": "PfIAcdbce2", "signatures": ["ICLR.cc/2026/Conference/Submission14403/Reviewer_3ReA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14403/Reviewer_3ReA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983657315, "cdate": 1761983657315, "tmdate": 1762924812952, "mdate": 1762924812952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-agent reinforcement learning framework for training LLMs to be robust against adversarial attacks. The attackers learn to synthesize jailbreak prompts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on the LLM safety problem through an interesting method, which involves extensive experiments on a variety of multi-agent configurations and models. The generalization capability can be enhanced by building a population pool and avoiding sharing parameters in multi-agent training."}, "weaknesses": {"value": "(1) Conclusions built on preliminary experiments without statistical support. All the experiments seem to be performed with only one trial, without a statistical significance/confidence interval reported in the tables and figures (indicated by one training curve in figures and no CI reported in the main paper). \n(2) No cross-play matrix reported by matching different attacks and defenders; A common issue with MARL training is that it can easily fall into a cyclic policy issue when the game is intrinsically (like rock-scissor-paper)"}, "questions": {"value": "(1)(2) see weakness\n(3) How are the attacker and defender rewards designed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9JsPxpOUtO", "forum": "PfIAcdbce2", "replyto": "PfIAcdbce2", "signatures": ["ICLR.cc/2026/Conference/Submission14403/Reviewer_wPSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14403/Reviewer_wPSA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053077125, "cdate": 1762053077125, "tmdate": 1762924812491, "mdate": 1762924812491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}