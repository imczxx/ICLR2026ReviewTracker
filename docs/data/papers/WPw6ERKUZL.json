{"id": "WPw6ERKUZL", "number": 24079, "cdate": 1758352448746, "mdate": 1759896782691, "content": {"title": "MILA (MULTILINGUAL INDIC LANGUAGE ARCHIVE): A DATASET FOR EQUITABLE MULTILINGUAL LLMS", "abstract": "Large Language Models (LLMs) are predominantly trained on high-resource languages such as English, leaving low-resource languages marginalized. This imbalance is particularly acute in India, where most Indic languages lack clean, large-scale digitized corpora despite having hundreds of millions of speakers. Building equitable representation for these languages requires not only greater data volume, but also novel pipelines for acquisition, curation, and validation. We present ***MILA***, the largest curated Indic multilingual dataset to date, spanning 7.5 trillion tokens across 16 of India’s 22 official languages. The dataset is constructed through a multi-stage process combining large-scale crawling, OCR pipelines tailored to Indic scripts, LLM post-corrected translations, synthetic augmentation via the **Indic-Persona Hub**, data distillation, and rigorous filtering. Each stage is validated by expert linguists, ensuring both linguistic fidelity and cultural authenticity. Alongside, we release \\textbf{Indic-MMLU}, a translation and verification of MMLU into 16 Indian languages, providing the first large-scale multilingual benchmark for evaluation. We further introduce multiple *domain-specific taxonomies* (finance, Ayurveda, agriculture, law, and catering) to enable creation of targeted pre-training and post-training corpora for Indic use cases. To assess the impact of these resources, we conduct extensive experiments across translation pipelines, OCR incorporation, synthetic supervised fine-tuning (SFT) data generation, and continual pretraining analyses. Across all tasks, models trained on MILA demonstrate stronger performance on Indic-MMLU and achieve improved parity with English, underscoring the value of curated pipelines for equitable multilingual modeling. By bridging resource gaps at scale and validating through expert and synthetic strategies, MILA promises to be a foundational archive for inclusive large-scale language modeling in the Indic context.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Multilingual NLP", "Low-resource languages", "Indic languages", "Data curation", "OCR for Indic scripts", "Synthetic data generation", "Translation pipelines", "Data distillation", "Benchmarking (Indic-MMLU)", "Domain-specific corpora", "Continual pretraining", "Equitable multilingual modeling", "Inclusive language technologies"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a2cae12d50405fd30f97c77264f9daffee23f138.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors create MILA, the largest Indic multilingual dataset. They combine existing multilingual datasets and construct OCR pipelines specific to Indic scripts. The authors create MILA, the largest Indic multilingual dataset to tackle the major imbalance in LLM training data for Indic languages. The dataset is built using a multi-step process that includes combining existing datasets, OCR pipelines specific for Indic scripts (including comparison with existing VLMs, and creating  synthetic OCR benchmark), LLM-assisted translation correction (with human evaluation), synthetic data generation, data distillation, and expert linguistic validation. This ensures both high quality and cultural authenticity. In addition to the dataset, the authors release Indic-MMLU, a multilingual benchmark that translates and verifies the popular MMLU test into 16 Indian languages. Through extensive experiments, models trained on MILA show better results on Indic-language tasks, reducing the performance gap with English."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "- The authors have released the dataset publicly which will be really beneficial for the community.\n- The authors built a very thorough pipeline for performing OCR for Indic scripts. The kind of problems they discussed are very relevant to what researchers face when scanning documents. Their comparison with existing VLMs, and creating synthetic OCR benchmark further validates their approach.\n- There are not many low-level details about the translation pipeline but the high level idea sounds good to create translation based data.\n- Their instruction tuned model performs well compared to off-the-shelf models and with preprocessing and postprocessing it gets even better.\n- By embedding diverse Indian identities and reasoning patterns, it goes beyond translation, ensuring language models are culturally relevant and capable of nuanced task performance.\n- The results of parity metric and Indic MMLU look decent when compared to pretrained models."}, "weaknesses": {"value": "- A huge chunk of the paper resides in the appendix. I’d request authors to bring more things from appendix to main paper otherwise readers will find it difficult to understand what’s going on. For instance:\n  - In table 3, authors don’t clarify what do different models mean?\n  - In section 3.5, what is the teacher model for distillation, what’s the final model, how many parameters it has etc.\n  - Creation of Indic MMLU: In the abstract the authors mention we create IndicMMLU and then every mention of Indic MMLU in the paper cites MMLU paper which is super confusing. Later, I saw the pipeline of Indic MMLU is in the appendix which makes things super confusing.\n  - In Section 3.4 translation pipeline: You had 3 language evals per language? Which LLM you used for inference. Do you have inter annotator agreement scores?\n  - Details about Indic PersonaHub also lie in the appendix and I couldn’t completely understand what’s happening at a lower level.\n  - To sum up, there is a lot going on in the paper and all of it is good but I feel the main paper gives just high level ideas and leaves implementation/low level details in appendix. The authors need to do a better job at presentation. \n- In Figure 4, what is low and high and which model did you train, Mistral or NLLB? I don’t see these models having comparable parameters so I doubt if it’s a fair comparison\n- The formatting of the paper is quite weird in some places. The content beside table 1 is just independent and has no relation with preceding or succeeding paragraphs. In Line 158, after “Paullada et al…” comes “To tackle these” on Line 162 which looks confusing while reading and breaks the flow. Same problem after line 269, after “low-resource Indic..” comes “Evaluation including..” on Line 291."}, "questions": {"value": "- In figure 1, please increase the font size of legends as they are blurred.\n- The authors have cited a lot of multilingual corpora, however, I see they have skipped [1]. It contains 8T tokens covering 193 languages.\n- Please change formatting of tables 3 and 4 to that of Table 5 \n- Typos\n  - Line 197- use \\citet instead of citing Mendu et al. twice\n  - Line 257, 431- I can see et al.(2025) citation\n  - Line 330- x billion tokens?\n  - Line 348- Table ??\n  - Line 445, 446- use \\cite and \\citet properly\n\n[1] An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT) (Burchell et al., ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ADlvpKytDq", "forum": "WPw6ERKUZL", "replyto": "WPw6ERKUZL", "signatures": ["ICLR.cc/2026/Conference/Submission24079/Reviewer_Dor9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24079/Reviewer_Dor9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760886173896, "cdate": 1760886173896, "tmdate": 1762942927040, "mdate": 1762942927040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a new pre-training corpus and downstream benchmark datasets focusing on Indic languages. The dataset was curated from different sources including OCR of public domain books and expert translation of English MMLU dataset. paper then shows that performance on the MMLU benchmark is significantly improved for models pretrained on the developed corpus."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The corpus includes data from public domain books that was extracted using OCR. The books come from diverse domain that are culturally and practically significant for Indic language speakers.\n2. Personally identifiable information was removed.\n3. A synthetic persona based data generation focusing on Indic context was used to enhance the usefulness of this corpus for modern language model training."}, "weaknesses": {"value": "1. The OCR pipeline depends on VLMs that were not specifically trained for Indic OCR. The risk of poor OCR quality has not been studied extensively. There should be human evaluations to determine the quality of the OCR'd texts. This is important as this component of the dataset (the OCRed books) are a major novelty of this work.\n2. There is a translation component in the pipeline. The quality of those translations needs to be verified by human annotators. The paper mentions human evaluation but do not provide any human evaluation metrics such as number of annotators or inter-rater agreement. Existing translation models are known to be of poor quality for low resource languages. \n3. The synthetic rewriting portion of the pipeline is poorly described in the main paper. It should be described in the main paper in a concise form.\n4. There are no statistics reported for how many tokens came from translation and synthetic rewriting."}, "questions": {"value": "I see that there are some presentation inconsistencies between the main paper and the appendix. The appendix follows a more clear presentation format. Please follow that for the main paper. For example the prsentation of tables in the main paper should be similar to how tables are presented in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bXUb6MjYWA", "forum": "WPw6ERKUZL", "replyto": "WPw6ERKUZL", "signatures": ["ICLR.cc/2026/Conference/Submission24079/Reviewer_rq6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24079/Reviewer_rq6h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786988510, "cdate": 1761786988510, "tmdate": 1762942926841, "mdate": 1762942926841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MILA, a large-scale curated dataset of 7.5T tokens covering 16 Indic languages, built through web crawling, OCR of scanned books, translation pipelines, and synthetic persona-driven data generation. The paper also introduces Indic-MMLU, a multilingual benchmark, and shows that continual pretraining on MILA improves both absolute performance and fairness (parity) across Indic languages."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The dataset covers multiple Indic languages, including several with limited existing digital resources, making it a valuable resource for model pretraining.\n\n- The continual pretraining experiments demonstrate consistent performance improvements across languages relative to baseline checkpoints.\n\n- The work employs a rigorous data cleaning and processing pipeline, including filtering, normalization, and OCR post-correction."}, "weaknesses": {"value": "- The overall structure of the paper makes it difficult to clearly understand the pipeline and its distinct components. Methods and results are interleaved, which obscures the main contributions.\n\n- There are no ablations isolating the impact of individual steps in the pipeline, making it difficult to determine where the performance gains originate. This limits the scientific insight of the work.\n\n- While some artifacts (e.g., prompts) are provided, the pipeline remains hard to reproduce due to missing procedural and implementation details.\n\n- The evaluation setup could be stronger. The baselines are limited, and the results do not comprehensively demonstrate improvements over established multilingual models.\n\n- The experimental design is under-specified. For example, Table 2 presents results, but the paper does not describe the experiments beyond mentioning the dataset used.\n\n- There is noticeable repetition in the writing (e.g., lines 178–179 restate lines 114–116), suggesting the paper would benefit from further editing for clarity."}, "questions": {"value": "1. In line 118, is the intention to refer to Devanagari as a script rather than as a language?\n\n2. In line 199, could you specify which PiiModifier implementation is used (e.g., version number or repository link) to support reproducibility?\n\n3. In Table 2, what criteria distinguish “conventional” from “curated” data in your comparison?\n\n4. In line 244, which dense model is being referenced, and how is “dense” being defined in this context?\n\n5. For Table 4, since the goal is to show improvements from pre-/post-processing, can you also report results for the models in Table 4a where post-processing improved performance?\n\n6. Which table is being referred to in line 348?\n\n7. In Section 5, could you report additional evaluations beyond MMLU to demonstrate broader applicability?\n\n8. In lines 445–447, could you clarify where the referenced tables/figures are located in the appendix? I couldn't find it.\n\\\nAlso, the comparison between older multilingual models (e.g., mT5, BLOOM) and monolingual models (e.g., LLaMA-2, Gemma) against Qwen-3 is unfair; could the paper justify or adjust the comparison to account for differences in training recency and model scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "twlmNFovxO", "forum": "WPw6ERKUZL", "replyto": "WPw6ERKUZL", "signatures": ["ICLR.cc/2026/Conference/Submission24079/Reviewer_uXym"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24079/Reviewer_uXym"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789810984, "cdate": 1761789810984, "tmdate": 1762942926581, "mdate": 1762942926581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MILA, the largest curated Indic multilingual dataset covering 16 of India’s 22 official languages and totaling 7.5 trillion tokens. With an attempt to reduce existing gaps in model performance for Indic languages, the authors present a multi-stage pipeline for data curation and validation, integrating large-scale crawling, OCR tailored to Indic scripts, synthetic augmentation, and rigorous filtering. They also release Indic-MMLU, a translation of MMLU into 16 Indian languages, and conduct continual pretraining on the new dataset, demonstrating some improvements in downstream performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This dataset is a valuable contribution, providing a replicable pipeline for curating the largest Indic multilingual dataset across 16 official Indic languages.  \n- The data curation process is thorough, involving multiple stages designed to preserve both data quality and overall utility, making the dataset useful for future research and model development."}, "weaknesses": {"value": "- Since this dataset was developed for pretraining, it would have been helpful to include a discussion on potential data contamination from existing test sets on Hugging Face or other web sources. How was data contamination controlled during collection? There’s a chance that some test data could have been included during scraping. Given the relatively low MMLU scores, this may not have occurred, but it would still be valuable to see an explicit discussion of how this risk was mitigated.\n\n- Could you expand more on any data provenance strategies used to ensure that potentially copyrighted materials were not included in the collected dataset? In the past, certain datasets have had to be taken down due to the inclusion of copyrighted or improperly licensed content.\n- In the current draft of the paper, it is difficult to tell what drives the marginal improvements in model performance. The gains look somewhat positive, but it's not even clear how statistically significant they are. Even more so, one would have really expected to see more substantial improvements given the size and quality of the new data. If your newly trained model only improves on the Indic MMLU, it’s not obvious that the benefits justify the significant compute and curation effort. So your model may be better on more culturally grounded benchmarks. Is there a reason why you mostly benchmark Indic MMLU ? Aren’t there other Indic datasets that could be added to your evaluation suite?\n- Why are there no comparisons to frontier models? It is honestly not my expectation that your new models should beat these models, but such comparisons can provide valuable context, helping us understand where your work stands relative to strong existing baselines and its potential practical impact.\n- There are limited analyses or explanations on the correlations between the proportion of data for a given language in your datasets and the performance improvements observed after continual pretraining. One might expect that a language with more resources, like Hindi, would show substantial gains compared to the very low-resourced ones. However, the results suggest their performance is still fairly similar to the very low-resource languages."}, "questions": {"value": "- In Line 183, you claim to employ in-house model-based quality classifiers (fastText) for all languages in your data at the document level. Could you clarify what constitutes “high quality” for Indian languages, especially for a pretraining dataset like yours? In previous work, people have quantified quality based on formality, nonsensical, or malformed text. \n- What size of the final dataset was translated? Could you provide more detailed stats on different data sources? \n- Did you adapt the Qwen tokenizer before continuing to pretrain ? Since Indic languages are mostly non-Latin script, do you have thoughts on how this might have contributed to the poor performance of your models even after adaptation?\n- If compute wasn’t a limiting factor, an ablation that would have been really good to see is studying the impact of the different data sources on your downstream performance. Obviously, this would be compute-intensive and is not a weakness of your current paper. It could just provide insights into aspects of the data curation process that could be scaled further."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqGN9o3m4Q", "forum": "WPw6ERKUZL", "replyto": "WPw6ERKUZL", "signatures": ["ICLR.cc/2026/Conference/Submission24079/Reviewer_igJu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24079/Reviewer_igJu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015332917, "cdate": 1762015332917, "tmdate": 1762942926214, "mdate": 1762942926214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}