{"id": "CB9xkb2r2J", "number": 2464, "cdate": 1757097147905, "mdate": 1759898146289, "content": {"title": "Motion-Aware Surface Smoothing for Monocular Avatar Representations", "abstract": "3D Gaussian Splatting (3DGS) has become a popular representation for 3D avatar modeling due to its fast training and real-time rendering. However, the state-of-the-art methods struggle to generalize from sparse inputs and often fail to recover realistic geometry. We introduce a motion-aware surface smoothing framework to improve 3DGS for learning from monocular human videos. Our method regularizes the training of Gaussian parameters, modulates the Adaptive Density Control (ADC) for improving surface quality, and supervises Gaussian motions under unseen camera viewpoints. The enforcement of surface smoothness yielding superior geometry contours and higher-fidelity rendering. Across five public datasets, including MVHumanNet, DNA-Rendering, ActorsHQ and outdoor videos, our approach consistently outperforms prior methods in novel view synthesis, novel pose animation, and 3D shape reconstruction. Code will be published upon acceptance.", "tldr": "", "keywords": ["3D Gaussian Splatting", "Avatar Modeling", "Rendering", "Geometry"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1788c0062d759a512372d4383847d741db410c21.pdf", "supplementary_material": "/attachment/d968d6bcb557ac0c6dd7bdc3ee72a52b246b91d5.zip"}, "replies": [{"content": {"summary": {"value": "The focus of the paper is on human modeling from monocular videos, especially from sparse frames. \n\nThe main contribution is a regularization term to encourage the smoothness of the surface from a camera view. Since the regularization is computed in 2D space after rendering the avatar articulated by a certain pose, the smoothness is disrupted at occlusion boundaries and in regions with holes. Therefore it further weights the regularization based on a distance between the corresponding points in the canonical space.\n\nIt furthers adds a smoothness-based heuristic in adaptive density control: If a Gaussian contributes more to smooth surface, it will be more likely to be densified.\n\nIt experiments on MVHumanNet, DNA-Rendering, ActorsHQ and Youtube videos. It shows superior results in sparse-view settings compared to the baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles the important problem of human modeling from sparse inputs. While dense inputs may require cumbersome data collection, sparse views as inputs are more practical.\n* The paper is clearly written and easy to follow. The methods are well motivated, e.g., why to weight the difference of depths based on the distance of Gaussians in canonical space.\n* The idea of regularizing the surface smoothness is interesting. It identifies the cases (occlusions and holes) where smoothness regularization does not hold and proposes the fix for such scenarios. It is novel and reasonable to considering the contribution of a Gaussian to the smooth surface in adaptive density control. \n* It experiments on various dataset and challenging in-the-wild videos, showing that the method is widely applicable."}, "weaknesses": {"value": "* The paper does not compare to iHuman, which also tackles the problem of human modeling from sparse inputs. iHuman [1] can model with as few as 6 views. Therefore, it is an important baseline.\n\n[1] iHuman: Instant Animatable Digital Humans From Monocular Videos, Paudel et al., ECCV 2024.\n\n* While the contributions lie in the smooth regularization and geometry-aware adaptive density control, the biggest improvement comes from replacing 3DGS with GoF and the normal map supervision from Sapiens.\n\n* When applying the smoothness regularization, the authors choose the observation space where occlusions disrupt the smoothness assumption. However, such difficulties may not exist in the canonical space, where the avatar is carefully poses to avoid occlusions as much as possible. It is not clear to me the benefits of using observation space.\n\n* The image sizes in the supplementary videos vary continuously, making it hard to identify the potential flickering and multiview consistency.\n\n* One of the key challenges of modeling from monocular videos is the incomplete observation. The monocular videos may not depict every aspect of the avatar. The paper does not show multiview rendering, therefore it's hard to tell how it hallucinates in unseen regions."}, "questions": {"value": "My main concerns are the missing baseline and limited improvement from the contributions, as listed in the weakness. Besides, the followings are minor questions and suggestions:\n\n* When locating the nearest Gaussian along the ray, does the method also consider the value of the opacity? For example, if a Gaussian very close to the camera is almost transparent, will it be used as depth? If so, the depth may be misleading.\n\n* I suggest to be careful with the word \"sparse inputs\" as sometimes it refers to even sparser input like 3 images. The settings of the paper are monocular videos which use over 100 frames for training.\n\nPotential typos:\n* In Eq. 3, the right most $x_o$ should be $x_c$.\n* In Eq. 7, is it supposed to be $S_i=\\sum_{j}w_j \\|I_j^d - I_i^d\\|$ rather than $S_i=\\sum_{j}w_j (I_j^d - I_i^d)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rm93Eo6pWd", "forum": "CB9xkb2r2J", "replyto": "CB9xkb2r2J", "signatures": ["ICLR.cc/2026/Conference/Submission2464/Reviewer_3cMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2464/Reviewer_3cMT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760740456211, "cdate": 1760740456211, "tmdate": 1762916247872, "mdate": 1762916247872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of monocular video-based human reconstruction by introducing a motion-aware surface smoothing framework. The key contribution lies in modulating the Adaptive Density Control (ADC) and supervising Gaussian motions using unseen virtual camera viewpoints. By explicitly regularizing surface smoothness through depth-based constraints, the method enhances the geometric consistency of 3D Gaussian Splatting. Evaluations across five public datasets demonstrate improved performance over recent state-of-the-art approaches in novel view synthesis, pose animation, and 3D shape reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motion-aware surface smoothing mechanism effectively enforces spatial regularization of Gaussians using rendered depth maps. Combined with the geometry-aware Adaptive Density Control, it results in smoother and more compact surface geometry.\n- The proposed method not only improves visual fidelity but also produces plausible mesh reconstructions, outperforming previous Gaussian-based avatar models in terms of geometric coherence and normal consistency."}, "weaknesses": {"value": "- While the paper claims improvements in mesh reconstruction quality, no mesh-level evaluation metrics such as *Chamfer Distance* or *Point-to-Surface (P2S)* error are reported. Including these would quantitatively support the claim that the method improves geometric accuracy. A comparison against recent mesh-based human reconstruction works would further strengthen this aspect.\n\n- In ActorsHQ novel pose evaluation, the performance drops compared to several baselines. The authors briefly acknowledge this in the appendix but do not provide an explicit analysis. An explanation regarding the causes would be valuable.\n\n- The supplementary video exhibits minor stretching artifacts and floating points around the legs (notably at 1m 5–8s). Since the framework is designed to promote spatial smoothness and motion consistency, a discussion on why these artifacts occur would clarify the limitations.\n\n- The comparison set is limited. Several recent monocular video-based reconstruction works with released code, such as *HUGS* (Kocabas et al., 2024), and *Expressive Gaussian Avatar* (Moon et al., 2024), should be included for a fairer comparison.\n\n- Missing relevant recent references related to mesh-based / gaussian-based reconstruction:\n    - [1] Moon, Gyeongsik, et al. *“Expressive Whole-body 3D Gaussian Avatar.”* **ECCV 2024.**\n\n    - [2] Shin, Jisu, et al. *“CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images.”* **ECCV 2024.**\n\n    - [3] Shao, Zhijing, et al. *“SplattingAvatar: Realistic Real-time Human Avatars with Mesh-embedded Gaussian Splatting.”* **CVPR 2024.**\n\n    - [4] Svitov, David, et al. *“HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior.”* **ACCV 2024.**"}, "questions": {"value": "- Could the authors provide training time comparisons with other baselines in the main paper or appendix? Since 3DGS-based methods often emphasize efficiency, this comparison would contextualize the proposed improvements.\n\n- What are the common failure cases (e.g., loose garments, extreme poses, occlusions, or inaccurate SMPL initialization)? Including a small visualization of typical failure cases would help readers understand the framework’s limitation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hd2yDBn6sr", "forum": "CB9xkb2r2J", "replyto": "CB9xkb2r2J", "signatures": ["ICLR.cc/2026/Conference/Submission2464/Reviewer_qjx3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2464/Reviewer_qjx3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404096811, "cdate": 1761404096811, "tmdate": 1762916247746, "mdate": 1762916247746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MASSAR (Motion-Aware Surface Smoothing for Avatar Representations), a novel framework designed to enhance 3D Gaussian Splatting (3DGS) for human avatar modeling from monocular videos.\nThe core contribution lies in a motion-aware smoothness regularization, which promotes geometrically consistent Gaussian distributions through depth-map-based supervision.\nThe proposed regularization is applied in three complementary ways: (1) directly as a smoothness loss, (2) integrated with a geometry-aware Adaptive Density Control (ADC) strategy, and (3) extended to a self-supervised virtual-view training scheme.\nThrough this design, MASSAR achieves improved performance in both novel-pose animation and novel-view rendering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motion-aware surface smoothing term is simple yet effective in enforcing geometric regularization, addressing a known limitation of 3D Gaussian Splatting (3DGS) in sparse-view or monocular settings.\n\n2.The proposed smoothness weight computation in canonical space (Eq. 7) is elegant and interpretable, as it considers the geometric relationships of nearby pixels in both canonical and observation spaces."}, "weaknesses": {"value": "1.The main contribution—smoothness-based regularization—while effective, is a incremental improvment. compare to original depth smooth regularization, the improvement is limit.\n\n\n2.While ablations show clear improvements, it would be more convincing to include quantitative sensitivity analysis for hyperparameters such as $\\sigma_{s} $ and\n$\\sigma_{c}$\n\n3.The runtime and memory usage of MASSAR are not reported compared to baseline. \n\n4.There is no visualization result without $\\{w_{i}\\}$ and with $\\{w_{i}\\}$."}, "questions": {"value": "1.Since different scenes may exhibit varying levels of surface smoothness, how sensitive is the method to this hyperparameter? Is  $\\sigma_{s}$ fixed across all datasets, or is it adaptively tuned per case?\n\n2.Could the proposed smoothness regularization be integrated into other Gaussian-based frameworks (e.g., 2DGS), given that some of them already include surface regularization terms? Would such integration require any modification, or is it directly compatible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DXbgsxlOt1", "forum": "CB9xkb2r2J", "replyto": "CB9xkb2r2J", "signatures": ["ICLR.cc/2026/Conference/Submission2464/Reviewer_Uer2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2464/Reviewer_Uer2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900546329, "cdate": 1761900546329, "tmdate": 1762916247344, "mdate": 1762916247344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the task of avatar reconstruction from a monocular video. The paper follows the strategy of canonical space + deformation (linear blend skinning). To improve the quality, the authors propose to enhance surface smoothness. Specifically, the method encourages 3DGS that are close in the observation space to also be close in the canonical space. Further, it utilizes normal map alignment with priors from foundation models. Experiments on various datasets demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- originality-wise: the surface smoothing idea is interesting.\n- quality-wise: the final results look promising.\n- clarity-wise: the paper is generally well-written.\n- significance-wise: reconstructing a human avatar from a monocular video is important for various downstream tasks, e.g., AR/VR."}, "weaknesses": {"value": "1. I am not convinced that the surface smoothing design (Sec. 3.3) contributes most to the model. Actually, when it comes to LPIPS, from Tab. 1, the most important factor is the utilization of Sapiens to compute surface normal (L367).\n\n2. Further, qualitatively, in the ablation (Fig. 5), without the Sapiens' prior, the model produces the lowest quality results, much worse than any other ablations.\n\n3. Eq. (7) encourages close points in the observation space to be close in the canonical space, i.e., $\\hat{d}_j = (\\tilde{\\mu}_j^c - \\tilde{\\mu}_i^c)$. I am curious whether this is a good signal, as close points in observation space can be really far away in the canonical space. For example, the hand will be close to the feet in the observation space if a person places their hands on their feet. However, in a T-shape, the hand will be really far from the feet."}, "questions": {"value": "See \"weakness\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6L3lPdQ8Ws", "forum": "CB9xkb2r2J", "replyto": "CB9xkb2r2J", "signatures": ["ICLR.cc/2026/Conference/Submission2464/Reviewer_KRMV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2464/Reviewer_KRMV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978770756, "cdate": 1761978770756, "tmdate": 1762916247131, "mdate": 1762916247131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}