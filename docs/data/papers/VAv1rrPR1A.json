{"id": "VAv1rrPR1A", "number": 4191, "cdate": 1757627101226, "mdate": 1763738177815, "content": {"title": "Mechanism of Task-oriented Information Removal in In-context Learning", "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.", "tldr": "We propose a new perspective of the in-context learning mechanism as a task-oriented information reduction.", "keywords": ["Mechanistic Interpretability", "In-context Learning", "Large Language Model"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3aec3dd7cc7643f6b99935cc3211b8a4fe3a5b1d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that few-shot in-context learning (ICL) works by selectively removing task-irrelevant information from query representations, steering the model toward the intended task. In zero-shot settings, injecting a low-rank \"task-verbalization\" filter that projects onto a Task-Verbalization Subspace (TVS) dramatically improves accuracy even while preserving only ~0.7% of dimensions, while in few-shot ICL the model spontaneously moves hidden states toward this TVS, as measured by geometric metrics that rise in middle-to-late layers. The paper identifies \"Denoising Heads\" (DHs)—attention heads whose ablation disrupts this removal operation—which are largely independent of induction heads and show local re-encoding patterns over query tokens; ablating DHs significantly reduces ICL accuracy and nearly collapses performance in unseen-label settings, demonstrating the causal importance of this mechanism. The authors note this mechanism applies to clustering-style classification tasks but not bijective tasks like translation, with experiments conducted across multiple text classification datasets and models including LLaMA and Qwen variants."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Task‑oriented information removal operationalized via a low‑rank subspace and two geometric metrics that correlate with accuracy and layer depth. \n\n2. identification and causal ablations of Denoising Heads, with independence from induction heads and a sensible local re‑encoding attention pattern. \n\n3. Explicit failure case on bijective tasks (translation/fact recall) and an explanation of why low‑rank removal should not help there. \n\n4. Broad, careful experiments across six datasets and three models, with appendix replications and unseen‑label analyses that stress‑test induction‑only stories."}, "weaknesses": {"value": "1. Theory is largely heuristic. The link between covariance and “information removal” is argued but not formally proved; metric choices and DH thresholds (±3.5%) are somewhat ad hoc. \n\n2. Narrow task family. The focus is single‑label classification; reasoning, chain-of‑thought, or generation settings are not studied, and the paper itself notes the mechanism likely does not apply to bijective mappings. \n\n3. Scale and generality. Only up to 8B models are used. It remains unclear whether DH distributions and metric trends stabilize for larger base or instruction‑tuned LMs."}, "questions": {"value": "1. How sensitive are results to scanning all layers/heads? Could you provide one full‑layer run on a smaller model to confirm no hidden DH clusters?\n\n2. Do your metrics or DH patterns say anything in multi‑class generative outputs (e.g., natural language rationales)? If not, what would be needed?\n\n3. Any preliminary results on larger models (e.g., 13B/34B) or instruction‑tuned variants to test whether DHs persist or shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGbXFs9KV5", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Reviewer_sEGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Reviewer_sEGe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886484073, "cdate": 1761886484073, "tmdate": 1762917221767, "mdate": 1762917221767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the internal mechanism of In-Context Learning (ICL) in large language models (LLMs) from a new perspective — task-oriented information removal. Instead of viewing ICL as pattern imitation or label copying via Induction Heads, the authors argue that few-shot demonstrations act as filters that suppress task-irrelevant information in the hidden representations.\n\nTo support this view, they introduce two novel probing metrics: Covariance Flux through the Task-Verbalization Subspace (TVS) and Eccentricity, which quantify how hidden-state variance aligns with task-relevant directions and how anisotropic (filtered) the representations become. Using these tools, they demonstrate that (1) LLMs spontaneously recover the TVS during ICL, (2) this process can be measured as an information-removal trajectory across layers, and (3) a new class of attention heads, Denoising Heads, drive this mechanism, overcoming the well-known limitation of Induction Heads when facing unseen labels."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. **Novel probing metrics with clear interpretability.**\nThe introduction of Covariance Flux through TVS and Eccentricity provides a fresh, quantitative lens for studying internal representation dynamics of LLMs. These metrics go beyond uncovering new ICL mechanisms and identifying denoising heads, enabling a measurable understanding of how information filtering evolves within transformer layers.\n\n2. **New mechanism of ICL: task-oriented information removal.**\nThis paper shifts the focus of ICL mechanism from token-copying to information filtering, which deepens our conceptual understanding of ICL.\n\n3. **Identification of Denoising Heads addressing the Induction-Head limitation.**\nThe discovery of Denoising Heads is an insightful contribution. These heads operate locally and semantically, filtering query representations rather than copying lexical patterns, thereby explaining why LLMs can generalize to unseen labels -- something Induction Heads cannot do.\n\n4. **Strong empirical grounding.**\nThe experiments are extensive and solid."}, "weaknesses": {"value": "**Overly dense and somewhat disorganized presentation.**\nWhile the findings are rich, the paper feels overly packed. The main sections attempt to cover TVS analysis, metric design, mechanistic ablations, and head identification all at once. The narrative flow occasionally sacrifices clarity for compactness, making it harder for readers to see the conceptual through-line."}, "questions": {"value": "Suggestion:\n\n **Center the paper on the probing methodology.** \n\nIf the authors had focused the paper around the two new metrics (Covariance Flux and Eccentricity) as a general probing framework, the contribution would appear sharper and of broader relevance. The ICL mechanism and Denoising Head discovery could then be presented as compelling applications of this framework, rather than parallel findings competing for attention. \n\n**A more layered structure** -- with detailed exposition of the probing metrics in the main text and the some extended empirical findings moved to the appendix -- would make the paper both clearer and more impactful. You can organize the paper similar to [1].\n\nMinors:\n\n1. In Figure 3, I understand that the marker represents the lower bound of information removal with a value between 0 and 1. However, please include at least one reference marker (for example, the marker size corresponding to no information removal) so that readers can intuitively gauge the approximate lower bound portion of information removed.\n\n[1] Ren, Yi, and Danica J. Sutherland. \"Learning dynamics of llm finetuning.\" arXiv preprint arXiv:2407.10490 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VOP5vIevJg", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Reviewer_5eyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Reviewer_5eyE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978792515, "cdate": 1761978792515, "tmdate": 1762917221344, "mdate": 1762917221344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Note 2025/11/20 (AOE)"}, "comment": {"value": "Dear PCs, SACs, ACs, and Reviewers,\n\nWe appreciate the reviewer’s overall positive feedback, which is greatly encouraging to us. **To address the valuable comments raised by the reviewer, we have made the following adjustments to the paper.** (We have highlighted the **technical** revisions in red.)\n\n1. **[Reviewer 5eyE: Paper Writing Framework]** Thank you to reviewer 5eyE for the wonderful writing suggestion. We consider the suggested writing framework elegant and have therefore revised the narrative structure of the paper accordingly. Specifically:\n   1. We created a new Section 3 to unify the previously scattered descriptions of the experiment methodology and to establish a clearer logical progression of these methods.\n   2. All experimental results have been moved to Section 4.\n   3. In addition, to accommodate this restructuring, the previous Section 2.2 (Experiment Settings) has been moved into Section 4, rather than remaining in Section 2. Section 2 is now renamed “Background” and is used solely for related works.\n   4. We also moderated the tempo of the paper, and added brief recalls of methodological details in the experiment section, so that we can help readers understand more easily, and do not need to jump back to Section 3 while reading Section 4.\n   5. The \"contribution summary\" part has been rewritten.\n   This revision is almost entirely a replacement of paragraphs (with minor adjustments to transitional passages that are not central to the paper), and the technical content of the paper remains unchanged.\n\n2. **[Reviewer M6i4, TE1T: Application & Amplifying the DH]** We appreciate the reviewer’s excellent intuition. We have added an application prototype that enhances ICL performance by amplifying the DH outputs. This serves as a frugal and effective method prototype, especially in scenarios where labels contain noise. This addition has been included at the end of Section 4.4 (page 9), and the corresponding results are shown in Figure 14.\n\n3. **[Reviewer M6i4, sEGe: Sensitivity of the Ablation Threshold]** Appendix E. We added a discussion on the sensitivity of the ablation threshold, which confirms that any non-extreme threshold can reliably distinguish DHs from random heads. A footnote describing this result has also been added to the main text.\n\n4. **[Reviewer sEGe: Mathematical Grounding of Metrics]** Appendix G. We added a mathematical proof for using covariance as an information measure. Specifically, we show that the covariance loading on principal component directions serves as an equivalent measure of entropy.\n\n5. **[Reviewer M6i4: Statistical Significance]** Table 2. We have added the p-values of the ablation results of DH against RH. Specifically, these values represent the p-test probability of accepting the hypothesis that “the DHs are in a randomly sampled attention head set.”\n\n6. **[Reviewer sEGe: Generative Tasks]** Appendix G. We added a discussion on generative tasks. Through eccentricity-based tests, we provide prototypical evidence that information removal also occurs in the English–Chinese translation task. Also, we include an additional limitation section concerning the complexity of the information filter.\n\n7. **[Reviewer 5eyE: Numerical Results for Covariance out of Rank]** Appendix I. We added numerical results for Covariance out of Rank.\n\n8. **[Reviewer sEGe: More Models]** We added experimental results for the 3B Instruct and 13B Instruct models. These results are broadly consistent with our main findings. Please see Table 2 and the Appendix I for details.\n\n---\n\n**Other minor revisions:**\n\n- Minorly revised Fig. 1 and Fig. 16 for aesthetic and typo.\n- Revised Sec. 4.1 and Table 5 since we added a new model.\n- Section 4.4 now specifies the number of demonstrations used.\n- Table 5 includes an added note regarding the range of scanned layers.\n- Layout adjustments corresponding to the above revisions, including updating the placement and size of several figures and tables, as well as adjusting some hyperlinks and footnotes.\n- Several typos, wording issues, and grammatical errors have been corrected.\n\nAgain, thank you for the time and thoughtful reviews you devoted to this paper. Your efforts have made this paper even stronger.\n\nAuthors"}}, "id": "iSaPWnf9bw", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726575492, "cdate": 1763726575492, "tmdate": 1763728297767, "mdate": 1763728297767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Note 2025/11/20 (AOE)"}, "comment": {"value": "Dear PCs, SACs, ACs, and Reviewers,\n\nWe appreciate the reviewer’s overall positive feedback, which is greatly encouraging to us. **To address the valuable comments raised by the reviewer, we have made the following adjustments to the paper.** (We have highlighted the **technical** revisions in red.)\n\n1. **[Reviewer 5eyE: Paper Writing Framework]** Thank you to reviewer 5eyE for the wonderful writing suggestion. We consider the suggested writing framework elegant and have therefore revised the narrative structure of the paper accordingly. Specifically:\n   1. We created a new Section 3 to unify the previously scattered descriptions of the experiment methodology and to establish a clearer logical progression of these methods.\n   2. All experimental results have been moved to Section 4.\n   3. In addition, to accommodate this restructuring, the previous Section 2.2 (Experiment Settings) has been moved into Section 4, rather than remaining in Section 2. Section 2 is now renamed “Background” and is used solely for related works.\n   4. We also moderated the tempo of the paper, and added brief recalls of methodological details in the experiment section, so that we can help readers understand more easily, and do not need to jump back to Section 3 while reading Section 4.\n   5. The \"contribution summary\" part has been rewritten.\n   This revision is almost entirely a replacement of paragraphs (with minor adjustments to transitional passages that are not central to the paper), and the technical content of the paper remains unchanged.\n\n2. **[Reviewer M6i4, TE1T: Application & Amplifying the DH]** We appreciate the reviewer’s excellent intuition. We have added an application prototype that enhances ICL performance by amplifying the DH outputs. This serves as a frugal and effective method prototype, especially in scenarios where labels contain noise. This addition has been included at the end of Section 4.4 (page 9), and the corresponding results are shown in Figure 14.\n\n3. **[Reviewer M6i4, sEGe: Sensitivity of the Ablation Threshold]** (Appendix E) We added a discussion on the sensitivity of the ablation threshold, which confirms that any non-extreme threshold can reliably distinguish DHs from random heads. A footnote describing this result has also been added to the main text.\n\n4. **[Reviewer sEGe: Mathematical Grounding of Metrics]** (Appendix G) We added a mathematical proof for using covariance as an information measure. Specifically, we show that the covariance loading on principal component directions serves as an equivalent measure of entropy.\n\n5. **[Reviewer M6i4: Statistical Significance]** (Table 2) We have added the p-values of the ablation results of DH against RH. Specifically, these values represent the p-test probability of accepting the hypothesis that “the DHs are in a randomly sampled attention head set.”\n\n6. **[Reviewer sEGe: Generative Tasks]** (Appendix G) We added a discussion on generative tasks. Through eccentricity-based tests, we provide prototypical evidence that information removal also occurs in the English–Chinese translation task. Also, we include an additional limitation section concerning the complexity of the information filter.\n\n7. **[Reviewer 5eyE: Numerical Results for Covariance out of Rank]** (Appendix I) We added numerical results for Covariance out of Rank.\n\n8. **[Reviewer sEGe: More Models]** We added experimental results for the 3B Instruct and 13B Instruct models. These results are broadly consistent with our main findings. Please see Table 2 and the Appendix I for details.\n\n---\n\n**Other minor revisions:**\n\n- Minorly revised Fig. 1 and Fig. 16 for aesthetic and typo.\n- Revised Sec. 4.1 and Table 5 since we added a new model.\n- Section 4.4 now specifies the number of demonstrations used.\n- Table 2 includes an added note regarding the range of scanned layers.\n- Layout adjustments corresponding to the above revisions, including updating the placement and size of several figures and tables, as well as adjusting some hyperlinks and footnotes.\n- Several typos, wording issues, and grammatical errors have been corrected.\n- [2025/11/21 ~03:00 AOE] Fixed one figure import error.\n\nAgain, thank you for the time and thoughtful reviews you devoted to this paper. Your efforts have made this paper even stronger.\n\nAuthors"}}, "id": "iSaPWnf9bw", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763726575492, "cdate": 1763726575492, "tmdate": 1763738543994, "mdate": 1763738543994, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel interpretation of the ICL mechanism by viewing it as a process to remove redundant information in the query’s hidden states for the targeted tasks. Specifically, the paper applies a low-rank filter on the hidden states for informational removal and finds it boost the performance on the designed tasks significantly. On top of that, the paper conducts further analysis to prove that a few-shot ICL implicitly performs a similar information removal process. Finally, the paper identifies attention heads that are responsible for such removal behaviors and shows that they are crucial for the success of ICL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper includes well-designed illustrations that well support the claims.\n- The paper provides a perspective for demystifying ICL that is novel yet aligns with many previous observations.\n- To support the claim that ICL performs information removal, the paper provides decently comprehensive analyses that dives deep into the activities of the hidden states of the model using self-crafted analysis methods.\n- The paper includes experiments and discussions to compare their newly discovered Denoysing Heads against the previous induction heads."}, "weaknesses": {"value": "The paper provides many insights on the mechanism of ICL, but the potential future directions built on these findings are less clear."}, "questions": {"value": "What are possible sources of the accuracy improvement from instruction given that instructions do not seem to contribute to the information removal process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Tk9d93p9RA", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Reviewer_TE1T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Reviewer_TE1T"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146553500, "cdate": 1762146553500, "tmdate": 1762917221128, "mdate": 1762917221128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new mechanistic interpretation of in-context learning: rather than copying label tokens via induction heads, large language models perform a task-oriented information removal that compresses query hidden states toward a low-rank, task-verbalization subspace (TVS). The authors demonstrate (1) that injecting a trained low-rank filter into a layer’s residual stream converts near-zero zero-shot performance into strong task-specific outputs; (2) that few-shot demonstrations induce geometric changes in hidden states — increased eccentricity and covariance flux into the learned TVS — consistent with implicit information removal; and (3) that a subset of attention heads  causally contribute to this aligning operation: ablating them reduces covariance flux, eccentricity, and downstream accuracy. The paper thus argues DHs complement induction heads and together explain a broader range of ICL phenomena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s framing of the “information removal → task-verbalization subspace” is conceptually novel and shifts the focus from mere copying to selective suppression\n\nThe paper is well-written and easy to follow. The low-rank injection experiment is simple, interpretable, and effectively demonstrates that a small learned linear filter can steer outputs.\n\n\nThe identification of Denoising Heads (DHs) and the analysis of their interaction with induction heads provide a richer mechanistic understanding of in-context learning."}, "weaknesses": {"value": "Ablating a head is a valid intervention, but zeroing a head output changes the residual stream and thus all downstream activations; the observed drops in Covariance Flux / Eccentricity / Accuracy may partly reflect these propagated network dynamics rather than a head performing a localized denoising operation.\n\nThe method appears to identify DHs only on a subset of layers. If many DHs remain unidentified, ablation results could be underestimated or misattributed. \n\n\nThe DH identification uses fixed relative-change thresholds (e.g., −3.5% / −5%). It is unclear how sensitive results are to threshold choice and whether effects are statistically significant across random seeds / datasets / prompts."}, "questions": {"value": "Can you show that amplifying DH outputs (scale >1) increases Covariance Flux/Eccentricity and improves accuracy in settings where the model is weak (e.g., few-shot with noisy labels)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oyjwFcxNvd", "forum": "VAv1rrPR1A", "replyto": "VAv1rrPR1A", "signatures": ["ICLR.cc/2026/Conference/Submission4191/Reviewer_M6i4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4191/Reviewer_M6i4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4191/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199409052, "cdate": 1762199409052, "tmdate": 1762917220816, "mdate": 1762917220816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}