{"id": "XNNDODynCl", "number": 19993, "cdate": 1758301267080, "mdate": 1759897007673, "content": {"title": "Fair Policy Aggregation from Standard Policy Optimization", "abstract": "Currently the most powerful AI systems are aligned with human values via reinforcement learning from human feedback. Yet, reinforcement learning from human feedback models human preferences as noisy samples from a single linear ordering of shared human values and is unable to incorporate democratic AI alignment. In particular, the standard approach fails to represent and reflect diverse and conflicting perspectives of pluralistic human values. Recent research introduced the theoretically principled notion of quantile fairness for training a reinforcement learning policy in the presence of multiple, competing sets of values from different agents. Quite recent work provided an algorithm for achieving quantile fairness in the tabular setting with explicit access to the full set of states, actions and transition probabilities in the MDP. These current methods require solving linear programs with the size of the constraint set given by the number of states and actions, making it unclear how to translate this into practical training algorithms that can only take actions and observe individual transitions from the current state. In this paper, we design and prove the correctness of a new algorithm for quantile fairness that makes efficient use of standard policy optimization as a black-box without any direct dependence on the number of states or actions. We further empirically validate our theoretical results and demonstrate that our algorithm achieves competitive fairness guarantees to the prior work, while being orders of magnitude more efficient with respect to computation and the required number of samples. Our algorithm opens a new avenue for provable fairness guarantees in any setting where standard policy optimization is possible.", "tldr": "", "keywords": ["AI alignment", "reinforcement learning", "democratic AI alignment", "pluralistic AI alignment", "computational social choice"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/702e76f4e4770e444fa3ae99365baee12ce102e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies quantile fairness in the context of reinforcement learning. The authors show intrinsic scalability limitations of previous work and propose a slightly different setting where the quantile fairness is measured wrt to the set of optimal policies for different agents rather than all possible policies. This alternative formulation allows for efficient algorithms to identify max-quantile fair policies. The proposed algorithm is evaluated in a synthetic factory monitoring environment and its effectiveness is assess across several different metrics (e.g., Nash welfare, Gini coefficient)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The \"negative\" result showing that quantile fairness defined on all policies can be vacuous is very interesting. The intuition behind this result is intuitive (i.e., in a high-dimensional policy space, it is likely that many policies have poor reward, thus making the max-quantile fairness to be very close to 1.0) and sound and it is likely to hold in general. It is then supported by a theoretical derivation, which relies on the construction of a specific worst-case MDP. This result provides a very solid grounding to the need of developing alternative, more practical formulations.\n* The formulation proposed in the paper (i.e., focusing on optimal policies) is sound and it makes the overall setting much more tractable. This is supported both theoretically (Thm 5.6 / 6.1 / 6.2) and empirically (L448) and it builds on a neat result showing how sampling from the policy space which is the image of the convex hull of optimal rewards can be done efficiently.\n* The empirical results provide a good evidence of the properties of the proposed formulation and algorithm. While a direct comparison with the original quantile fairness formulation cannot be done (i.e., they optimize different objectives), the authors study a number of correlated metrics that support the theoretical claims: 1) the method is more efficient; 2) the max-quantile fairness is significantly lower, showing that the method makes a more \"significant\" trade-off between preferences of different agents; 3) Nash and Gini are better; 4) per-agent return is mostly better."}, "weaknesses": {"value": "* The paper is originally motivated by RLHF and the use of deepRL techniques. While the current paper is clearly a significant step forward in that direction (e.g., the use of an oracle policy optimization routine, instead of assuming perfect knowledge of the underlying MDP and a more efficient algorithm), it still remains \"limited\" to a more theoretical setting without any direct evidence that the proposed method can scale to deepRL and can be applied to RLHF."}, "questions": {"value": "* In the formulation of Sect.3 you assume a finite MDP, but later you only assume access to a policy optimization \"oracle\". Could you please clarify where the finite MDP assumption is needed in the algorithm and theoretical results?\n* In L448 you make a high-level comparison on the running time of your algorithm and the previous one. Would it be possible to have a more direct comparison on the same hardware?\n* When I first read the high-level description of the proposed formulation I was concerned that restricting the focus on agent-optimal policies would impose a too serious restriction. For instance, we can imagine problems where a fair policy should trade-off between the rewards of different agents resulting in a behavior that is fairly different than any of the optimal policies. While the definition of optimal occupancy distribution (Def 5.4) could be still quite rich, it is not easy to interpret. I have some questions in this respect\n** Would it be possible to create \"worst-case\" MDPs where the max-quantile fair policy for the optimal occupancy distribution has q*~=0, i.e., the opposite case than in Thm5.1, where basically you restrict the policy space so much that no fair policy is possible.\n** In the worst case of Thm5.1, can you prove that your formulation would have q*>>0?\n** Can you provide some qualitative examples where your formulation is \"meaningful\", i.e., it doesn't exclude policies that are intuitively fair? The empirical results provide some evidence of that, but having a more intuitive illustration would help.\n\nMinor\n* L175: it should be J_i(pi')<J_i(pi) - eps?\n* L300: \\mathcal K -> \\mathcal K^*\n* Alg1: I would suggest to change the letter in \"s=1 to S\" to avoid confusion with states"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qIxSKaSkIL", "forum": "XNNDODynCl", "replyto": "XNNDODynCl", "signatures": ["ICLR.cc/2026/Conference/Submission19993/Reviewer_whs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19993/Reviewer_whs3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657038948, "cdate": 1761657038948, "tmdate": 1762932896669, "mdate": 1762932896669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a fair multi-objective Markov decision process problem that aims to compute a policy that is fair with respect to the objectives. The paper first introduces a q-quantile fairness notion based on the convex hull of optimal policies for individual objectives, and the authors argue that this notion is more appropriate than the one introduced in previous work. The paper then shows that this new notion makes it possible to find fair policies efficiently using a black-box policy optimization algorithm as its subroutine. Experiments were conducted to evaluate the fairness guarantee the algorithm yields."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly and nicely written. The results appear solid, and the exposition maintains a good balance between accessibility and mathematical rigor."}, "weaknesses": {"value": "1. I don't see a strong connection between the model in the paper and RLHF. Most RLHF models are not built on MDPs. \n\n2. Some of the problem setups feel somewhat artificial, potentially constructed for the sake of deriving results. For example, instead of looking at the quantile, why not simply look for solutions that yield x fraction of the maximum attainable value for every agent, and then maximize x? This seems much more straightforward and would simplify much of the analysis, and I don't see any obvious reason why this measure would be less desirable than the one proposed in the paper. In fact, the statement in Theorem 5.1 seems to suggest that we should care more about the fraction of each agent's maximum return.\n\n3. In the paragraph at the top of Page 6, it is stated that the main drawback of existing methods is that they cannot handle large state spaces, whereas policy optimization is still feasible for large state spaces. I am not sure this is a valid argument. While it is true that there are policy optimization algorithms applicable to large state spaces, they generally do not yield exact optimal solutions. This seems critical as the main algorithm in this paper relies on a policy optimization algorithm that computes exact optimal solutions as its subroutine.\n\n4. The sampling methods in Section 6.1 largely follow trivially from existing results.\n\n5. Though the paper is overall clear, I find that it would still benefit from additional explanations and intuition in certain parts. Such as the 1/e-quantile fair guarantee in Proposition 5.5, and MWU-based algorithm (Algorithm 3)."}, "questions": {"value": "- Can you respond to the first three weaknesses I mentions?\n\n- In Defintion 3.1, should $\\rho_0(s,a)$ be $\\rho(s)$?\n\n- Line 6 of Algorithm 3, should it be $(\\sum_i w_i^t + u^t) J_i(\\pi)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PYfHxJmPdF", "forum": "XNNDODynCl", "replyto": "XNNDODynCl", "signatures": ["ICLR.cc/2026/Conference/Submission19993/Reviewer_Mguy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19993/Reviewer_Mguy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863080719, "cdate": 1761863080719, "tmdate": 1762932895876, "mdate": 1762932895876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fairness in reinforcement learning with large state-action spaces. More specifically, the work studies a setting with access to a policy optimization oracle and the goal is to obtain q-quantile fairness. The paper provides an algorithm based one multiplicative weights as well as theoretical fairness guarantees. Finally, the algorithm is experimentall evaluated on a tabular MDP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Clarity**\n* The paper is well written and easy to follow.\n\n**Motivation**\n* The paper tackles an interesting, understudied problem that is of relevance in the time of finetuning LLMs using RL.\n\n**Related Work**\n* The treatment of related work is decent.\n\n**Novelty**\n* I am largely not aware of other work tackling similar problems and I believe that the work is sufficiently novel. One paper that should be highlighted is [1] which seems quite related. They also use an oracle notion in a multi-objective setting to obtain fairness guarantees. The fairness guarantees are slightly different but the manuscript might benefit from differentiating itself from this work (see Q1).\n\n**Theoretical results**\n* I mostly followed the arguments in the paper but did not verify the details of the proofs and parameter settings in the Appendix. However, using online learning techniques in combination with binary search to solve constraint optimization problems in large spaces is quite common and the algorithms as well as corresponding results seem sound.\n\n**Experimental results**\n* The experiments provide some nice intuition into the functionality of the algorithm.\n\n[1] Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces. Eaton et al. ICML 2025."}, "weaknesses": {"value": "**Clarity**\n* One confusing point to me was the terminology “policy aggregation”. In the introduction, the text refers to rewards that belong to policies but a reward is always defined with respect to an MDP and it refers to optimal policies for each agent. In hindsight, it is obvious what this is referring to but not on a first read of the intro. One needs to read section 3 to understand this part. I think it might make sense to clarify the relationship between agents and rewards in the intro when stating the model.\n\n**Empirical Design and Evidence**\n* The fundamental motivation of the paper is to design algorithms that work in high dimensional settings with neural networks but the experiments are conducted solely on what seems to be a tabular MDP. The manuscript could be strengthened by experiments actually using the complex function approximation objects that the algorithm is designed for.\n\nOverall, the pros outweigh the cons in my eyes and I even though I would like to see some deep learning experiments, prior work seems to have accepted without them (e.g. [1]). I can't find much else to complain about and think this is a solid paper. Thus, I'm willing to err on the side of optimism an recommend acceptance."}, "questions": {"value": "Q1: Can you elaborate on how your work differs from [1].\n\nQ2: Maybe I'm missing something but what happens when 2 agent reward functions oppose each other and there exists no fair allocation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ckbn4SbQ2L", "forum": "XNNDODynCl", "replyto": "XNNDODynCl", "signatures": ["ICLR.cc/2026/Conference/Submission19993/Reviewer_maiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19993/Reviewer_maiB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872696780, "cdate": 1761872696780, "tmdate": 1762932895335, "mdate": 1762932895335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of fair policy aggregation, using a quantile notion of fairness. The goal is to find a policy that each agent prefers to a k fraction of alternatives, for the largest feasible k. They build on prior work that addresses this problem for a broader notion of alternatives (the uniform distribution over all policies), but under a much stronger assumption of access to the MDP (known transition dynamics). They give an oracle-efficient algorithm for learning a quantile fair policy with respect to the occupancy measure polytope induced by the optimal policy for each agent, without any assumption of known dynamics or generative model. \n\nThey further show that redefining the set of alternative policies from prior work is necessary in their setting, as efficient algorithms to even estimate quantiles without knowledge of transition dynamics cannot exist if the uniform distribution over all policies is taken as the reference class."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper gives efficient, provable algorithms for a reasonable fairness notion for preference aggregation. They cleverly set up the comparison class of policies such that their sampling algorithm for estimating quantiles is efficient and their main algorithm for aggregation is a simple application of multiplicative weights."}, "weaknesses": {"value": "The paper claims only $O(n)$ calls to a policy optimizer are required, but it appears that Algorithm 3 makes $T = \\log(n+1)/\\varepsilon^2$ calls to an optimization oracle, in addition to the $O(n)$ calls required to define the polytope.   \n\n“The key insight of Alamdari et al. (2024) is that one can still compute a continuous analogue of a “ranking” for each agent, by assigning to each policy a score of q if the agent receives a higher score than an q-fraction of all possible alternative policies” -- I think the second \"score\" here should be \"reward\"."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2kqKHKWpy5", "forum": "XNNDODynCl", "replyto": "XNNDODynCl", "signatures": ["ICLR.cc/2026/Conference/Submission19993/Reviewer_tKms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19993/Reviewer_tKms"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19993/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762746767389, "cdate": 1762746767389, "tmdate": 1762932894912, "mdate": 1762932894912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}