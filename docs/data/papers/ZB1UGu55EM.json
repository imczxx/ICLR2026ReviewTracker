{"id": "ZB1UGu55EM", "number": 9167, "cdate": 1758113647453, "mdate": 1759897739861, "content": {"title": "EMPIRICAL PRIORS FOR BAYESIAN NEURAL NETWORKS VIA WEIGHT PRUNING", "abstract": "Designing informative priors for Bayesian neural networks (BNNs) remains a fundamental challenge, yet it plays a crucial role in determining both model performance and robustness. While numerous studies have explored effective strategies for prior selection, the suitable choice of prior distribution is often architecture-dependent and requires extensive time to determine. To address this, we propose Sparsity-Informed priors for Bayesian neural networks, SPIN, a simple method that empirically determines both the prior mean and variance of Gaussian priors: weights that survive pruning are considered important and are assigned low-variance Gaussian priors centered at their post-pruning values, while pruned weights are treated as less informative and given high-variance, zero-mean Gaussian priors. Our empirical results demonstrate that SPIN enhances performance across diverse architectures and datasets. Furthermore, we discuss how this prior design contributes to improved performance in Bayesian neural networks.", "tldr": "We propose SPIN, a method that uses network pruning to set informative priors for Bayesian Neural Networks, improving performance across various architectures and datasets.", "keywords": ["Bayesian neural network", "variational inference", "pruning", "empirical bayes", "prior"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3354ebb29403d21c81eb4baa552c876562cd091d.pdf", "supplementary_material": "/attachment/4833cebf30412ddfc96b66b3bca0d9c374b3989a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents an intuitive method for constructing priors for Bayesian Neural Networks (BNNs). The core contribution is a two-stage process: first, a deterministic neural network is trained and pruned to identify its most salient weights. Second, this structural information is used to define a prior for a BNN, where non-salient (pruned) weights are given a standard normal prior while the independent still Gaussian priors for the non-pruned weights are centered at the respective weight with very small variance. The authors use mean-field Variational Inference to approximate the posterior and provide an empirical validation of their approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **[S1] Conceptual Simplicity:** The core idea of using pruning to inform BNN priors is intuitive and conceptually straightforward. It connects two distinct areas of research in a simple and interesting way. \n\n* **[S2] Clarity of Presentation:** The manuscript is very well-written and clearly structured. The authors do an excellent job of explaining their motivation and methodology, making the paper easy to follow and understand. \n\n* **[S3] Well-Executed Experiments:** Within their defined scope, the experiments are carried out properly and described with sufficient detail to allow for reproducibility."}, "weaknesses": {"value": "* **[W1] Significant Hyperparameter Sensitivity:** The proposed pipeline introduces a non-trivial number of sensitive hyperparameters. The choice of the prior variance for non-pruned weights, $\\sigma^2$, is critical, and the paper provides little guidance on how to set it. Moreover, as the experiments in Appendix C reveal, the optimal sparsity level is highly task-dependent. This combination significantly increases the practical overhead and tuning effort, somewhat undermining the method's apparent simplicity. \n\n* **[W2] Lack of Transparency in Empirical Evaluation:** The main paper's evaluation of the sensitivity to $\\sigma^2$ (Figure 2) is concerning. The authors show results for $\\sigma \\in \\{0.001, 1, 10, 100\\}$ but omit values like $0.1$ and $0.01$ (in the main paper). According to Appendix B, the performance for $\\sigma=0.1$ is notably poor. Burying negative results in the appendix instead of discussing them transparently in the main text casts doubt on the robustness of the findings and the subjective choice of $\\sigma=0.001$ for the main experiments. Additionally, the caption for Figure 2 incorrectly assigns colors to the different methods (this is obviously very minor - just wanted to note it).\n\n* **[W3] Limited Scope of Bayesian Inference and Missing Baselines:** The paper relies exclusively on mean-field VI, one of the simplest but also most limited approximate inference techniques. It's unclear if the empirical gains are a feature of the proposed prior or an artifact of how VI interacts with it. A more compelling evaluation would involve more expressive inference methods, such as MCMC. Critically, the analysis in Section 6.2 is missing a key baseline: a fully Bayesian treatment of the *sparse subnetwork* itself (i.e., performing inference only on the unpruned weights). This comparison is essential for understanding whether the proposed method offers any advantage over simply inferring a posterior over the pruned architecture directly, which would also be cheaper not only at training but also at inference time.\n\n* **[W4, minor] Unconvincing Qualitative Analysis:** The claims made in Section 6.1 regarding the diversity of the learned representations (Figures 3a-c) are not that convincing. The provided UMAP-based visualizations show little structural differences between the methods (potentially even due to randomness in the graph layout algorithm), and the argument that the proposed prior \"frees more of the weight space\" seems handwavy to me and is not convincingly supported by the evidence. \n\n* **[W5] Superficial Engagement with Related Concepts:** The paper overlooks several important connections. For instance, why not simply perform inference on the pruned subnetwork? This connects to a rich literature on Bayesian inference in subspaces (see e.g. [1]). The discussion could also benefit from engaging with literature on overparameterization, which suggests that larger models can have smoother, easier-to-explore loss landscapes (see e.g. [2]). This stands in contrast to the implicit assumption that pruning simplifies the inference task and could actually support the use of SPIN instead of subnetwork inference. Finally, recent work [3] has shown that the exploration patterns of advanced MCMC samplers can structurally resemble classical pruning masks out of the box, an interesting connection within the field of BDL that could be analyzed in conjunction with the pruning masks obtained in the present work. \n\n## References \n\n[1] Daxberger, E., Nalisnick, E., Allingham, J. U., Antorán, J., & Hernández-Lobato, J. M. (2021, July). Bayesian deep learning via subnetwork inference. In International Conference on Machine Learning (pp. 2510-2521). PMLR.\n\n[2] Wilson, A. G. (ICML 2025). Deep learning is not so mysterious or different.\n\n[3] Sommer, E., Wimmer, L., Papamarkou, T., Bothmann, L., Bischl, B., & Rügamer, D. (ICML 2024). Connecting the dots: is mode-connectedness the key to feasible sample-based inference in Bayesian neural networks?."}, "questions": {"value": "[Q1] Regarding the missing baseline discussed in **[W3]**, could you provide a comparison against a sparse/subnetwork BNN? Specifically, a BNN where inference (e.g., via VI and, ideally, an MCMC/diffiusion sampler) is performed *only* on the subnetwork identified by the initial pruning step. This seems like the most direct and relevant point of comparison and is crucial for understanding the proposed methodology. \n\n[Q2] Can you justify the choice of $\\sigma=0.001$ as the primary setting in your experiments? Given the poor results for $\\sigma=0.1$ reported in the appendix (**[W2]**), the current presentation feels like cherry-picking. A more transparent discussion of the method's sensitivity to this crucial hyperparameter should be included in the main text. Have you analyzed whether this effectively turns your method in subnetwork inference? Is there any meaningful variance left in the marginal posterior of the salient parameters?\n\n[Q3] Why was VI chosen as the sole inference method? I guess scalability. Have you considered whether the benefits you observe are tied specifically to the limitations of the mean-field approximation? \n\n[Q4] In your analysis, did you observe any layer-wise structural patterns in the masks produced by your pruning approach? As noted in **[W5]**, recent work has found connections between exploration patterns of samplers and classical pruning patterns. A brief analysis of this could better situate your work within the current BNN literature. \n\nI enjoyed reading the manuscript, which presents an interesting, albeit simple, idea. The empirical work is solid, but the evaluation could be made much more robust by including the requested baselines and a more transparent discussion of the method's sensitivities. If the authors can satisfactorily address the weaknesses and questions raised, I would be inclined to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EjnbarH0uN", "forum": "ZB1UGu55EM", "replyto": "ZB1UGu55EM", "signatures": ["ICLR.cc/2026/Conference/Submission9167/Reviewer_Vek4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9167/Reviewer_Vek4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760713578280, "cdate": 1760713578280, "tmdate": 1762920846587, "mdate": 1762920846587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Sparsity-Informed priors for Bayesian Neural networks (SPIN), a method to empirically design Gaussian priors for BNNs by leveraging weight pruning on a pretrained deterministic DNN. Important (unpruned) weights receive low-variance priors centered at their post-pruning values, while pruned weights get high variance, zero-mean priors. This approach aims to encode parameter importance into the prior, improving BNN performance. Evaluations on image classification datasets such as the CIFAR-10/100 and TinyImageNet, across various architectures show gains in accuracy, negative log-likelihood (NLL), and out-of-distribution (OOD) detection compared to baselines like isotropic Gaussian, Laplace, Student-t, Spike-and-Slab and MOPED. The method is positioned as an empirical Bayes strategy, with analysis on how sparsity regularizes BNN representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates moderate originality by combining network pruning, inspired by the lottery ticket hypothesis, with BNN prior design in a straightforward way. While data-driven priors already exist, e.g. MODEP, SPIN innovatively uses pruning to differentiate weight importance, creating a hybrid prior that reflects sparsity-induced structure. In terms of quality, the experimental setup is solid, covering diverse architectures and datasets, with comprehensive evaluation metrics including accuracy, NLL, ECE and OOD AUROC. Comparisons to multiple baselines are fair and the ablation study on sparsity and σ adds rigor. The analysis in Section 6, using UMAP visualization and loss landscapes, provides insightful evidence that SPIN enhances representation diversity and flattens the loss surface. The clarity of the paper is also solid, with concise writing, clear figures and organized sections. Significance is evident in addressing the prior selection BNN challenge which is often computationally expensive. SPIN’s consistent improvements, for example in accuracy, suggest practical value and highlights the pruning’s role in uncertainty modeling."}, "weaknesses": {"value": "While SPIN shows empirical gains, the paper lacks a deeper theoretical justification for why pruning-derived sparsity translates effectively to BNN priors. For instance, the choice of σ=0.001 for important weights is tuned empirically, but no analysis explores why such a tight variance outperforms alternatives, making the method feel heuristic rather than principled. The computational overhead is a notable drawback since SPIN requires pretraining a DNN, iterative pruning/fine-tuning to find maximal sparsity and then VI for the BNN. This multi-stage process could double training time compared to standard priors, yet the paper does not quantify this cost or compare it to baselines. For scalability, experiments on larger datasets, e.g. full ImageNet or models are missing, limiting generalizability. Experimentally, while predictive models improve, calibration (ECE) often worsens, suggesting overconfidence. Sparsity levels vary widely across experiments, determined by validation NLL recovery, but sensitivity to pruning method is unaddressed. Additionally, the spike-and-slab baseline fails completely in some cases, but without hyperparameter details, it is unclear if this is a fair comparison. Finally, the reliance on the initial DNN’s quality is a critical limitation. If the DNN underfits, SPIN offers little benefit. While the paper acknowledges this, it provides no mitigation strategies such as adaptive pruning or warm-starting."}, "questions": {"value": "1.\tHow robust is SPIN to different pruning techniques? The paper uses global magnitude-based-pruning, have you tested one-shot method like structured pruning, e.g. filter-level?\n2.\tGiven the multi-stage overhead, is there a way to integrate pruning into end-to-end BNN training? This could make SPIN more practical. Experiments comparing compute times to baselines would help evaluate trade-offs. \n3.\tOn harder datasets like CIFAR-100/TinyImageNet, ViT improvements are marginal. Does this stem from weaker initial DNNs and if so, could hybrid pretraining, e.g. with CNN features, strengthen the sparse backbone? Ablations here might reveal ways to extend SPIN to underfitting scenarios.\n4.\tHow does SPIN compare to other-sparsity inducing BNN methods beyond the spike-and-slab, such as horseshoe priors? If baselines like spike-and-slab were unstable due to tuning, re-running with optimized hyperparameters could strengthen claims of superiority."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IZbZ4qKfxz", "forum": "ZB1UGu55EM", "replyto": "ZB1UGu55EM", "signatures": ["ICLR.cc/2026/Conference/Submission9167/Reviewer_xbfH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9167/Reviewer_xbfH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378453666, "cdate": 1761378453666, "tmdate": 1762920846003, "mdate": 1762920846003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPIN (Sparsity-Informed Priors for Bayesian Neural Networks), an empirical Bayes framework that constructs data-driven priors based on weight importance revealed through pruning a pretrained deterministic model. The approach is simple yet effective, yielding consistent gains in predictive performance and uncertainty estimation across CNN and ViT architectures."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**[S1] Novelty and Conceptual Clarity:** The idea of using network pruning to derive empirical priors is intuitive and well-motivated.\n\n**[S2] Extensive Experiments:** The paper provides extensive experiments across multiple datasets (CIFAR-10/100, TinyImageNet) and architectures (ResNet, DenseNet, MobileNet, ViT).\n\n**[S3] Clarity:** The method is well-explained with equations, figures, and pseudocode (Algorithm 1)."}, "weaknesses": {"value": "## Major Concerns\n**[W1] Lack of Explanation about Pruning:** The discussion of pruning-related literature is overly brief. Since the proposed method hinges on pruning to determine weight importance, it would be beneficial to include a more comprehensive overview of representative pruning techniques (e.g., magnitude-based, sensitivity-based, and structured pruning) and clarify how SPIN builds upon or diverges from them.\n\n\n**[W2] Insufficient Methodological Detail on Weight Partitioning:** The methodology section provides insufficient detail about how pruning is performed to distinguish important versus redundant weights. Since this partitioning directly determines the prior structure, a deeper analysis of pruning sensitivity (e.g., across different thresholds or pruning criteria) would strengthen the methodological rigor.\n\n\n**[W3] Unclear Justification for Using Gaussian Noise on Pruned Weights:** The decision to assign pruned weights a high-variance Gaussian prior (effectively adding random Gaussian noise) lacks clear justification. The authors state that this design introduces a regularization effect, but the explanation is not entirely convincing. In conventional pruning, pruned parameters are usually zeroed out rather than replaced with noise, so it is unclear how adding stochasticity meaningfully improves generalization. A more concrete theoretical or empirical motivation for this “regularization through noise” would help the reader understand its necessity.\n\n\n**[W4] Questionable Advantage of SPIN over Sparse DNNs**: In Figure 4, the Sparse DNN often matches or even exceeds the performance of the SPIN-based BNN (especially in ViT). This raises a critical question: *if a sparse deterministic model performs comparably without Bayesian overhead, when is SPIN truly advantageous?* The authors should better justify why the added computational complexity of BNN inference is worthwhile in such cases, possibly by analyzing uncertainty metrics or robustness benefits.\n\n\n**[W5] Unrealistic Scale of $\\sigma$ in Figure 2:** The variance values tested in Figure 2 ($\\sigma$ = 0.001, 1, 10, 100) are separated by several orders of magnitude, making the conclusion somewhat trivial—naturally, very large $\\sigma$ values yield poor performance. Including intermediate settings (e.g., $\\sigma$ = 0.01 or 0.1) would provide a more realistic and convincing analysis of the prior variance’s sensitivity.\n\n\n**[W6] Lack of Computational Complexity Analysis:** The paper omits a quantitative analysis of computational overhead. Since SPIN requires iterative pruning, the total cost is likely several times that of standard BNN training. Providing wall-clock runtime or FLOP comparisons—and explicitly discussing this as a limitation—would make the contribution more transparent.\n\n-----\n## Minor Concerns\n**[W7] Low Figure Readability:** Several figures (e.g., Figures 2, 4, and 5) appear blurry or low-resolution. Improving text clarity and resolution would enhance readability and presentation quality.\n\n\n**[W8] Missing Equation Numbering:** Important equations (e.g., KL divergence decomposition, hybrid prior definition) lack numbering, which makes cross-referencing within the text cumbersome and reduces readability.\n\n**[W9] Ambiguous Notation for SPIN(0.x) in Tables:** Clarify the notation “SPIN(0.x)” in tables. Although it refers to the sparsity level of the base model used to derive the prior, this should be explicitly stated near the first table for clarity.\n\n\n**[W10] Lack of Context for Loss Landscape Analysis:** The discussion around loss landscapes (Section 6.2, Figure 5) lacks sufficient context. Readers unfamiliar with this concept may not understand why flatter minima are desirable or how they relate to generalization. A brief explanation or citation would improve accessibility.\n\n**[W11] Absence of Quantitative Flatness Evaluation:** While Figure 5 provides qualitative evidence of flatter loss surfaces, the differences are visually subtle. Quantitative evaluation (e.g., Hessian eigenvalue spectra or trace-based flatness metrics) would make the argument more convincing."}, "questions": {"value": "**[Q1] Possibility of Adding Gradient/Hessian-Based Prior Comparison:** The proposed method infers weight importance solely through pruning, but alternative importance measures—such as gradient norms or Hessian-based saliency—could provide more theoretically grounded and computationally efficient ways to construct informative priors. Although the rebuttal period is short, would it be feasible for the authors to conduct or at least discuss a small-scale comparison between SPIN and such gradient/Hessian-based prior constructions? Even a limited experiment or qualitative analysis could help clarify whether the pruning-based approach offers distinct advantages over these sensitivity-based alternatives.\n\n**[Q2] Clarification on Regularization Effect of Gaussian Priors:** Could the authors elaborate on how assigning a high-variance Gaussian prior to pruned weights concretely contributes to regularization? In particular, how does this differ from simply setting those parameters to zero, and is there empirical evidence that the injected stochasticity improves generalization or uncertainty calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fZ5KTpBh4S", "forum": "ZB1UGu55EM", "replyto": "ZB1UGu55EM", "signatures": ["ICLR.cc/2026/Conference/Submission9167/Reviewer_4rDT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9167/Reviewer_4rDT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712281709, "cdate": 1761712281709, "tmdate": 1762920844911, "mdate": 1762920844911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPIN—a pruning‐informed empirical Bayes scheme that sets a hybrid Gaussian prior for Bayesian neural networks (BNNs): unpruned (“important”) weights receive low-variance Gaussians centered at their post-pruning values, while pruned (“redundant”) weights receive high-variance, zero-mean Gaussians. The prior is built by iteratively pruning a trained deterministic model to the maximum sparsity that preserves validation performance, then transferring this mask and the surviving weights into the BNN prior (Alg. 1) . Experiments across CIFAR-10/100 and TinyImageNet with ResNet-20/18, DenseNet-30, MobileNet-V2, and a lightweight ViT report consistent gains in accuracy and NLL, often strong OOD AUROC, but not the best ECE (with temperature scaling suggested as a remedy) . The paper also analyzes performance sensitivity to ViT quality and contrasts SPIN against “sparse DNN” baselines via loss-landscape visualizations (Fig. 4–5), attributing gains to the inductive bias encoded by SPIN rather than pruning alone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A clear, simple empirical-Bayes recipe that many practitioners can reproduce: train → prune to the max “no-loss” sparsity → set hybrid Gaussian prior at mask locations (Alg. 1). The method is well-specified and easy to port across architectures\n* Extensive experiments (quant & qual) on multiple datasets/architectures, including CNNs and a ViT, with accuracy/NLL/OOD metrics; ablations on σ and sparsity; and qualitative analyses (UMAP, loss landscapes)\n* BNNs are computationally heavy and often underfit at scale; sparsity is a principled way to cut cost while potentially improving robustness/uncertainty. The broader literature documents both the system-level need for sparsity and scalable BNN parameterizations, motivating this line of work [1–3,11]."}, "weaknesses": {"value": "* Limited novelty vs. existing sparsity-inducing priors and empirical-prior ideas: Heavy-tailed and explicit sparsity priors (Laplace/Student-t, horseshoe, spike-and-slab) and empirical-prior methods like MOPED have been well studied in BNNs. SPIN’s weight-wise two-group prior resembles two-group shrinkage and empirical Bayes without theoretical comparison or guarantees over these baselines [1-5]\n* Although SPIN uses a deterministic partition (not a mixture), the resulting prior class (tight Gaussian on “kept” weights, diffuse Gaussian on “pruned”) is conceptually close to spike-and-slab. The paper does not analyze when/why this empirical partition should outperform probabilistic spike-and-slab or related two-group shrinkage [3-4]\n* Prior tail behavior is not grounded. For signal preservation under sparsity, theory often favors strong concentration at zero plus heavy tails (e.g., horseshoe, R2-D2) to retain large effects. SPIN’s Gaussian slab can be too light-tailed, potentially over-shrinking important weights; no comparison to horseshoe-BNN or R2-D2-style priors is given [6-7]\n* BNNs and stochastic training exhibit high run-to-run variance; the paper reports point estimates but lacks mean ± std over seeds (and over folds where applicable). This is increasingly expected for credible claims [8-9]\n* UMAP plots are weak evidence of representation quality: DR methods (t-SNE/UMAP) can be misleading for comparing models; weight histograms, sparsity distributions per layer, or KL terms for Wimportant/Wredundant would better demonstrate “how” SPIN sparsifies [10]\n* Horseshoe-BNN—a canonical sparsity prior—was not compared; this omission limits the empirical claim that SPIN is the best “sparsity-informed” prior [1]\n\n[1] A. Ghosh, J. Yao, F. Doshi-Velez, “Model Selection in Bayesian Neural Networks via Horseshoe Priors,” JMLR, 2018.\n[2] M. Carvalho, N.G. Polson, J.G. Scott, “The Horseshoe Estimator for Sparse Signals,” Biometrika, 2010. (Background on heavy-tailed shrinkage.)\n[3] Q. Sun, Z. Song, F. Liang, “Learning Sparse Deep Neural Networks with a Spike-and-Slab Prior,” Stat. & Prob. Letters, 2022.\n[4] R. Jantre et al., “Deep Neural Networks with Spike-and-Slab Prior,” Neural Computing and Applications, 2025.\n[5] R. Krishnan, S. Subedar, O. Tickoo, “Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes,” AAAI, 2020.\n[6] Y. Zhang, etal. “Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prio,” JASA, 2021\n[7] Chan, Tsai Hor, et al. \"Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior.\" IEEE Transactions on Pattern Analysis and Machine Intelligence (2025).\n[8] J. Pineau et al., “Improving Reproducibility in ML Research,” JMLR, 2021\n[9] O.E. Gundersen, Y. Kjensmo, “Sources of Irreproducibility in ML: A Review,” arXiv:2204.07610, 2022.\n[10] D. Kobak, P. Berens, “The Art of Using t-SNE,” JMLR, 2019."}, "questions": {"value": "* Can you formalize conditions (e.g., on pruning error or mask accuracy) under which SPIN’s empirical partition yields lower posterior risk than spike-and-slab or horseshoe? Any bounds relating σ, sparsity, and excess risk/ELBO?\n* Mask correctness. How sensitive are results to mask quality (e.g., if pruning overshoots or if we use structured pruning)? Fig. 4 suggests dependence on base-model quality—can you quantify this sensitivity (ViT vs CNN)\n* Ablations to strengthen claims. Please show layer-wise sparsity, weight histograms, and KL contributions split by Wimportant/Wredundant (Eq. KL decomposition) to evidence the intended shrinkage pattern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "osiB7jvhXG", "forum": "ZB1UGu55EM", "replyto": "ZB1UGu55EM", "signatures": ["ICLR.cc/2026/Conference/Submission9167/Reviewer_Mmqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9167/Reviewer_Mmqv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861783410, "cdate": 1761861783410, "tmdate": 1762920844462, "mdate": 1762920844462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}