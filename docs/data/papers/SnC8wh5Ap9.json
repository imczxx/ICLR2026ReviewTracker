{"id": "SnC8wh5Ap9", "number": 1256, "cdate": 1756867959317, "mdate": 1759898218700, "content": {"title": "Hyperbolic Hierarchical Clustering for Visual Representation Learning", "abstract": "We investigate the token mixer in vision backbones by revisiting clustering, one of the most classic approaches in machine learning. \nAn effective token mixer is a fundamental component of modern vision backbones like vision Transformers, facilitating information exchange between image patches. Mainstream token mixers, which rely on convolution, attention, MLP, or their hybrids, primarily focus on navigating the trade-off between accuracy and computational cost. However, a significant drawback of these methods is their black-box nature; their encoding process is opaque and lacks interpretability. Diverging from these opaque designs, we introduce ClusterMixer, a new token mixer that is interpretable by design. ClusterMixer explicitly formulates the token mixing process through a hierarchical clustering mechanism. To model the natural, tree-like relationships inherent in visual data, the clustering is performed in hyperbolic space, which is well-suited for embedding hierarchies with low distortion. Building on this innovation, we present HCFormer, a new backbone architecture that integrates ClusterMixer with a series of meticulously designed clustering strategies to ensure robust performance across tasks. Extensive experiments demonstrate that HCFormer consistently outperforms its counterparts across diverse tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Considering its transparency and efficacy, we hope HCFormer can facilitate a paradigm shift toward interpretable backbones. Our source code will be released.", "tldr": "", "keywords": ["Clustering", "representation learning", "hyperbolic"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fdceb113bf6a17d9f9575a4a618cff1c854ce74f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper builds on prior work that uses clustering for token mixing in visual tasks.  The main innovations are to use a hierarchical structure in clustering and to use a hyperbolic space for clustering at a coarser stage.  The work is supported by experiments in visual domains, identification, semantic segmentation and object detection and instance segmentation.  Ablations are included to show the value of these innovations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes interesting and well motivated modifications to clustering based token mixing.  These seem to lead to real improvements in performance.  The clustering based approach is simpler and intuitive, compared to attention based methods."}, "weaknesses": {"value": "No significant weaknesses, but there are a number of ways in which the clarity of the paper could be improved.\n•\tEq. 2 is confusing because you sum a_{i,j} from 1 to M and from 1 to N.  I think something is wrong here.\n•\tIn Eq. 3, what does Norm do?  I at first thought it took the norm of g_i, but that doesn’t seem to make much sense.  Does it normalize it?\n•\tSome small errors in grammar (eg., We first partitions)\n•\tCluster Assignment, might mention clustering approaches that use soft assignment, eg., E-M.\n•\tThis description of Center Estimation could be a little more detailed, maybe in supplementary\n•\tDescriptions of the results are slightly misleading because the new method is compared to published work that uses somewhat fewer parameters.  It does seem though that the proposed method improves performance.  I know that the number of parameters can’t be equalized, but in some cases the authors could point out that their method achieves the same performance as others with fewer parameters.  \n•\tIn ablations, it is confusing because comparisons seem to be to the full model.  It seems like shifted windows should be compared to a hierarchical method with Euclidean distances?  I’m assuming that the Euclidean method that is compared to the hyperbolic one is still hierarchical?\nI guess the other limitation of the paper is that the method is not completely original, building closely on prior work, and that the performance increase is somewhat small.  But I still find the results very interesting."}, "questions": {"value": "One of the primary differences between cluster-based token mixing and transformers is that in the cluster-based method the embeddings are compared directly, whereas in transformers they are compared through a query and key, and also combined through a value matrix.  It would be interesting to have some discussion of this issue.  Does their work imply that values, queries and keys are not necessary?  Is there literature on this issue?  Could there be ablations that show that dropping the value, query and key does not hurt performance?  Or is there some reason that they are needed in transformers but not in clustering?  I don’t really expect the authors to address this in the rebuttal, but I find it curious."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nPKrcPhsis", "forum": "SnC8wh5Ap9", "replyto": "SnC8wh5Ap9", "signatures": ["ICLR.cc/2026/Conference/Submission1256/Reviewer_rvV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1256/Reviewer_rvV3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327311111, "cdate": 1761327311111, "tmdate": 1762915718904, "mdate": 1762915718904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HCFormer (Hyperbolic Hierarchical Clustering Transformer), a novel vision backbone that replaces self-attention with a hierarchical clustering-based token mixer. The key idea is to first perform local Euclidean clustering at the patch level, followed by global hyperbolic clustering across window-level features. The authors argue that hyperbolic geometry (negative curvature) naturally captures hierarchical semantics, offering both representational compactness and computational efficiency. Experimental results on ImageNet, COCO, and ADE20K show consistent improvements over CoC and FEC under comparable parameter and FLOP budgets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Conceptual novelty**: Introducing hyperbolic geometry into a hierarchical clustering-based vision transformer is a fresh and well-motivated idea. The connection between tree-like semantic structures and negative curvature is theoretically sound and intuitively clear.\n\n- **Intuitive architecture**: The design cleanly unifies local Euclidean and global hyperbolic clustering. The structure is modular and can be plugged into standard ViT-style backbones.\n\n- **Strong experimental coverage**: Evaluations span classification, detection, and segmentation. Gains of +0.7–1.0% Top-1 on ImageNet and consistent improvements on COCO/ADE20K demonstrate robustness.\n\n- **Comprehensive ablations**: The paper includes clear ablations showing the independent and combined effects of hierarchical structure (+0.8–1.0%) and hyperbolic space (+0.5–0.7%).\n\n- **Presentation quality**: The paper is very well written, with clear figures and consistent notation. Methodology and empirical sections are easy to follow."}, "weaknesses": {"value": "* **Efficiency claim not fully supported.**\n  While the paper theoretically reduces complexity from ($O(N^2)$) (self-attention) to ($O(NM)$) via hierarchical clustering, the *measured* FLOPs and throughput remain almost identical to CoC and FEC. The hyperbolic distance computation (arcosh + mapping functions) introduces overhead that cancels out the theoretical benefit.\n\n* **Performance gains are modest.**\nThe reported gains are steady but incremental. In Table 1, HCFormer outperforms the prior FEC baseline by roughly +0.7–1.0 Top-1 % on ImageNet-1K under similar FLOPs and parameter budgets. The largest relative gap (Tiny → FEC-Small) is +2.3 points, while improvements shrink at larger scales (+1.2 points for Medium). Ablations (Table 4b–c) show that hierarchical clustering contributes about +2.3 points, whereas the hyperbolic geometry adds +1.6 points in a 6 M-parameter setting. However, the hyperbolic benefit is not shown for larger backbones or downstream tasks, and no analysis is provided on curvature sensitivity or numerical stability. Hence, while the geometric component is empirically beneficial, its effect remains secondary and under-validated\n\n* **Limited analysis of hyperbolic embedding.**\n  There is no exploration of curvature sensitivity, embedding radius, or gradient stability of the exp/log mappings. Without such analysis, the hyperbolic advantage remains somewhat qualitative.\n\n* **Sec. 3.2 Cluster Mixer Writing Clarification**.\nSection 3.2 primarily restates the CoC/FEC-style token–cluster mixing operation (soft assignment → aggregation → redistribution). The real novelty appears in Sections 3.3 and 3.4, which introduce hierarchical organization and hyperbolic distance. Not sure the claim of a new token mixer from the authors is legitimate."}, "questions": {"value": "1. How sensitive are the results to the number of clusters (M) and the curvature parameter of the hyperbolic space?\n2. Have you measured actual runtime efficiency (throughput) compared to FEC/CoC on identical hardware?\n3. Could the same hierarchical clustering structure achieve similar benefits in Euclidean space with proper scaling or depth adjustments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0b18EtSRjo", "forum": "SnC8wh5Ap9", "replyto": "SnC8wh5Ap9", "signatures": ["ICLR.cc/2026/Conference/Submission1256/Reviewer_UABQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1256/Reviewer_UABQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549076407, "cdate": 1761549076407, "tmdate": 1762915718767, "mdate": 1762915718767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HCFormer, a MetaFormer-style vision backbone implementing ClusterMixer for token mixing via hierarchical clustering across dual geometries: Euclidean space for patch-level clustering within local windows and hyperbolic (Lorentz) space for window-level global abstraction. While claiming interpretability and efficiency advantages, the paper demonstrates competitive results on ImageNet, ADE20K, and COCO benchmarks but suffers from fundamental technical inconsistencies and unsupported theoretical claims."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel architectural concept: Dual-geometry approach (Euclidean for fine-grained, hyperbolic for hierarchical) is creative\n2. Comprehensive evaluation: Experiments across ImageNet, ADE20K, COCO with consistent improvements over clustering baselines\n3. Competitive performance, comparable to attention-based models"}, "weaknesses": {"value": "Major\n1. Fundamental design flaw in dual-geometry integration: in the paper, author concatenates features from Euclidean and hyperbolic spaces directly in Eq. (10) without any alignment mechanism. This is mathematically problematic as these features exist in incompatible metric spaces with different scales and properties. The simple concatenation likely destroys the geometric structure that hyperbolic space is supposed to preserve, undermining the entire theoretical motivation. \n - scale: Euclidean cosine similarity ranges in [-1, 1] while Lorentz distance has unbounded range. Without proper normalization, one component will dominate the Softmax computation.\n - Typically, when combining features from different manifolds, you need to either: a). Project both to a common space (e.g., tangent space)\n b) Use separate processing streams with late fusion, c) Apply learned projection/alignment layers\n - Cosine similarity (higher=better) vs Lorentz distance (lower=better) both fed to Softmax without explaining sign conversion\n2. Why are window-level relationships hierarchical? No explanation or evidence provided\n3. No evidence showing what hyperbolic geometry actually learns or why it helps\n4. Only one visualization provided, no quantitative interpretability metrics (cluster purity, consistency, semantic alignment)\n5. The paper completely omits standard practices for hyperbolic neural networks: No dimension reduction before hyperbolic operations (standard practice to reduce from C to C/r for computational efficiency [Khrulkov et al. 2020]:)  \n\nMinor \n1. Eq. (10) notation error: g^W_L suggests window-Lorentz while g^P_E suggests patch-Euclidean, directly contradicting the text's claim of patch=Euclidean, window=hyperbolic\n2. Eq. (2) indexing bug: Uses c'_i but should be c'_j\n3. No exploration of cluster numbers, window sizes, or curvature parameter κ."}, "questions": {"value": "1. How is your method actually implementable? Please provide the exact dimensions at each step: C → ? (reduction?) → hyperbolic (C+1?) → ? (projection?) → concatenation → FC. Without these details, the method cannot be reproduced.\n2. What is the true parameter count? If FC reduces (2C+1) → C after concatenation, this adds significant parameters. Why isn't this in Table 1?\n3. How do you handle high-dimensional hyperbolic operations? Processing 320-512 dims in hyperbolic space without dimension reduction contradicts standard practices. How is this numerically stable?\n4. Why not test Euclidean-Euclidean dual-path? This critical control would reveal whether improvements come from hyperbolic geometry or simply dual-path processing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sqzkkJOrAY", "forum": "SnC8wh5Ap9", "replyto": "SnC8wh5Ap9", "signatures": ["ICLR.cc/2026/Conference/Submission1256/Reviewer_fjaH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1256/Reviewer_fjaH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002980143, "cdate": 1762002980143, "tmdate": 1762915718542, "mdate": 1762915718542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HCFormer, a novel vision backbone architecture that leverages interpretable clustering for token mixing, moving away from the black-box nature of convolution, attention, and MLP-based mixers. One of the main contributions is ClusterMixer, which explicitly mixes tokens via hierarchical clustering. To better capture the tree-like, hierarchical relationships in visual data, clustering is performed in hyperbolic space at the window level, while Euclidean space is used for fine-grained patch-level clustering. This dual-geometry approach enables HCFormer to efficiently and transparently aggregate information across both local and global contexts. The architecture achieves linear computational complexity, strong multi-task flexibility, and built-in interpretability. Extensive experiments show HCFormer consistently outperforms comparable models in image classification, and achieve decent results on semantic segmentation, object detection, and instance segmentation. However more results are comparison with CLIP and multi-object settings will make the paper stronger."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) HCFormer replaces opaque token mixers (convolution, attention, MLP) with ClusterMixer, which uses explicit clustering algorithms for token mixing. This design provides transparency and interpretability, allowing users to understand how information is aggregated and propagated.\n2) The model performs clustering in Euclidean space for local patch-level mixing and in hyperbolic space for global window-level mixing. This approach efficiently captures both fine-grained and abstract hierarchical relationships, improving semantic modeling and reducing distortion in feature aggregation.\n\n3) By restricting clustering to local windows and using hierarchical strategies, HCFormer reduces the quadratic complexity of traditional clustering to linear, enabling efficient processing of high-resolution images and dense prediction tasks. The architecture is flexible and adapts well to various downstream tasks.\n4) \nHCFormer demonstrates strong results across multiple benchmarks, such as  ImageNet-1K, ADE20k, COCO etc."}, "weaknesses": {"value": "1) Comparison to Hyperbolic CLIP[1]; it would be good to have CLIP based baselines in the paper, since it will show general applicability of the method.\n2) How to extend this to multi-object settings? For example, When we have more than 1 object, how will this method extend? For example, On OpenImages as discussed in [2].\n3) In line 299 it’s said “ Euclidean similarity is computed at the finer-grained and computationally demanding patch-level, while hyperbolic similarity is estimated at the abstract” ; what happens if both are done using hyperbolic loss instead of euclideian loss? It’s a known fact that hyperbolic models are hard to train, it would be good to understand why this particular setup was chosen.\n4) On object detection the gains are very incremental, any particular reason why these gains are incremental when linear probing is actually decent?\nRefrences\n[1] Hyperbolic Image-Text Representations\n[2] Hyperbolic Contrastive Learning for Visual Representations beyond Objects"}, "questions": {"value": "I think there are few more experiments that can help the paper specially in CLIP side. And some of the results seem incremental which needs to be properly discussed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2pMVoQsEc9", "forum": "SnC8wh5Ap9", "replyto": "SnC8wh5Ap9", "signatures": ["ICLR.cc/2026/Conference/Submission1256/Reviewer_iogH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1256/Reviewer_iogH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224081604, "cdate": 1762224081604, "tmdate": 1762915718411, "mdate": 1762915718411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}