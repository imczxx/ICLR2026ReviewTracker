{"id": "RDOlvzwSyF", "number": 12193, "cdate": 1758206243427, "mdate": 1759897526084, "content": {"title": "Salient Object Ranking via Cyclical Perception-Viewing Interaction Modeling", "abstract": "Salient Object Ranking (SOR) aims to predict human attention shifts across different salient objects in a scene. Although a number of methods have been proposed for the task, they typically rely on modeling the bottom-up influences of image features on attention shifts. In this work, we observe that when free-viewing an image, humans instinctively browse the objects in such a way as to maximize contextual understanding of the image. This implies a cyclical interaction between content (or story) understanding of the image and attention shift over it. Based on this observation, we propose a novel SOR approach that models this explicit top-down cognitive pathway with two novel modules: a story prediction (SP) module and a guided ranking (GR) module. By formulating content understanding as the image caption generation task, the SP module learns to generate and complete the image captions conditioned on the salient object queries of the GR module, while the GR module learns to detect salient objects and their viewing orders guided by the SP module. Extensive experiments on SOR benchmarks demonstrate that our approach outperforms state-of-the-art SOR methods.", "tldr": "", "keywords": ["Saliency Ranking", "Human Attention Shift Modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/774ae1f50b4802edbbcf861f3f7eef376599e396.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a method for saliency object ranking. A story prediction module predicts the caption of the image and a guided ranking module predicts the saliency rankings. The cyclical interaction module aligns and refines the caption and the ranking iteratively. The experimental results seemed to show the proposed method outperformed previous SOTA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The cyclical interaction uses caption to guide saliency object ranking.\n- Ablations shows the effectiveness of the SITA and CMQC in the proposed method."}, "weaknesses": {"value": "- The segmentation head is unclear. The performance increase could potentially due to using a strong pretrained segmentation model.\n- The retained QAGNet has lower scores across metrics compared to the ones reported in the original paper. This is critical since the results of the proposed method does not outperform the reported results of QAGNet."}, "questions": {"value": "- Is the segmentation head a pretrained segmentation model or else? A strong segmentation model could favor the MAE.\n- What is the impact of number of object queries on results? An ablation study will be beneficial to see the impact.\n- Would a stronger text decoder leads to better performance?\n- What is the reason of decreased performance of retrained QAGNet? Did the authors use different training details or different evaluation settings or else?\n- Intuitively, the proposed method could also improve the performance on image captioning task. I am wondering if salient object ranking could help with image captioning. It will be interesting to see results compared with SOTA image captioning methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dHaAmMQMun", "forum": "RDOlvzwSyF", "replyto": "RDOlvzwSyF", "signatures": ["ICLR.cc/2026/Conference/Submission12193/Reviewer_5n6p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12193/Reviewer_5n6p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521823151, "cdate": 1761521823151, "tmdate": 1762923141796, "mdate": 1762923141796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework that models the cyclical interaction between perception and viewing for the Salient Object Ranking (SOR) task. The method introduces two key components: a Story Prediction (SP) module that simulates the human perceptual process through image caption generation, and a Guided Ranking (GR) module that predicts saliency rankings under the guidance of the SP module."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "（1）Novel Cognitive-Inspired Framework. \nThe paper introduces a cyclical perception–viewing model inspired by human visual cognition, which is strongly supported by established cognitive and psychological theories. And the introduction is easy to follow.\n\n（2）Extensive experiments.\nThe paper conducts both qualitative and quantitative experiments, and also provides an analysis of inference time. Moreover, the visualized experimental results clearly and intuitively demonstrate the improvements achieved by the proposed method."}, "weaknesses": {"value": "（1）The paper lacks a clear comparison with the recent top-down method, Language-Guided Salient Object Ranking (CVPR 2025), and its performance remains inferior to the results reported in that study.\n\n（2）In the Method Overview section, the symbols used in the equations do not correspond to those shown in Figure 2, which makes it confusing to understand the inputs and outputs of each module.\n\n（3）The experimental section mainly provides data and setup details but offers limited analysis or discussion to explain the observed results."}, "questions": {"value": "(1) Explain the differences between the proposed method and Language-Guided Salient Object Ranking (CVPR 2025). Moreover, the performance of this method is still inferior to that of the existing work.\n\n(2) In Eq.(1), when $l=1$, what dose $Q_{l-1}$ refer to?\n\n(3) In Table 2, for Setting II (“independent caption generation”), there is a performance improvement even without interaction between caption and visual features, which is confusing. Could the authors clarify this behavior?\n\n(4) How is the ground-truth (GT) caption obtained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jcZBdRCZNl", "forum": "RDOlvzwSyF", "replyto": "RDOlvzwSyF", "signatures": ["ICLR.cc/2026/Conference/Submission12193/Reviewer_siW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12193/Reviewer_siW4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559528559, "cdate": 1761559528559, "tmdate": 1762923141283, "mdate": 1762923141283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a Salient Object Ranking (SOR) approach that consists of two modules: the Guided Ranking (GR) module and the Story Prediction (SP) module, whose interaction enhances the overall performance of SOR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The design of this model aligns well with the human cognitive system, such as predictive coding, and the English writing is good and clear."}, "weaknesses": {"value": "Some experiments and details are not clearly explained. For example, in the experimental section, how was the choice of 24,000 epochs determined, and why such a large number? Could this lead to overfitting?\n\nIn addition, it would be helpful to qualitatively present the interaction between object queries and text features, as well as the results under different values of K."}, "questions": {"value": "What training data are used for the segmentation head? Was it pre-trained on the COCO segmentation dataset? If it was trained only on the SOR dataset, would its segmentation generalization ability be affected?\n\nDoes the random selection of captions influence the results? It is recommended to include a discussion—for example, are the salient objects in the image always located in the main subject position described in the caption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9DoP0bVW2p", "forum": "RDOlvzwSyF", "replyto": "RDOlvzwSyF", "signatures": ["ICLR.cc/2026/Conference/Submission12193/Reviewer_oTJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12193/Reviewer_oTJC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068030305, "cdate": 1762068030305, "tmdate": 1762923140845, "mdate": 1762923140845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}