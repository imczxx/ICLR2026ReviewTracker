{"id": "J0eNXpnrc7", "number": 25627, "cdate": 1758369588390, "mdate": 1759896712873, "content": {"title": "All-in-One: Boosting Basic Capabilities in one Omni-MLLM to Enhance Movie Understanding", "abstract": "Movie understanding is still challenging as a movie involves many characters with complex relationships and it is edited with artistic language for appealing audiences, which are neglected in current multimodal large language models (MLLMs). Only a few previous works propose ideas to identify characters and integrate ID information in models, but they use cascaded models, or vision and scripts only while ignoring audio. To address these problems, we propose an all-in-one Omni-MLLM with built-in basic capabilities of ID identification, shot-level description, and critical sub-question answer in thinking. First, we construct identity related data consisting of 12 fine-grained character-centric tasks to improve the model's ability to identify characters from vision and audio. Second, we leverage frame and shot descriptions to alleviate the difficulty of training. Third, we explore how to enhance our model further by using Chain of Thought (CoT) data from an advanced model. Experimental results show that our proposed model achieves stable improvements on both ID-Aware Movies Understanding questions' set StoryQA and general video understanding benchmark VideoMME. Ablation studies confirm the positive contributions from all of our proposed ideas.", "tldr": "An end-to-end omni-multimodal language model for id-aware video understanding", "keywords": ["omni-multimodal large language models", "identity-aware", "video understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1c1f799d21fb090a70853b9b03f7eeb499b7468.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Omni-CharM, an all-in-one Omni-MLLM designed to improve ID-aware movie understanding, especially in long-form video contexts involving characters with complex relationships. The model is developed through a two-stage training pipeline: 1) a Basic Boosting Stage that introduces 12 types of ID-related multimodal tasks, frame and shot description data, and chain-of-thought (CoT) data, and 2) a Task Adaptability Stage where the model is further adapted to StoryQA-style question answering. Experiments on StoryQA and Video-MME benchmarks demonstrate improvements over open-source baselines and the original Qwen2.5-Omni-7B backbone, with ablation studies showing contributions from each component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, this paper addresses an important challenge in long-form & character-centric movie understanding, where many existing MLLMs operate at short clip or generic video QA levels.\n2. The model demonstrates notable improvements on StoryQA, particularly in character-related reasoning."}, "weaknesses": {"value": "1. My major concern lies in the reported performance on general video benchmarks. According to the tech report of Qwen2.5-Omni, Qwen2.5-Omni-7B achieves 64.3 (w/o sub.) and 72.4 (w/ sub.) on Video-MME, which is largely higher than the reported 61.6 in this submission. The authors should provide strong justifications regarding the performance mismatch.\n2. While the proposed 12 ID-related tasks are well described, the quality of some automatically generated annotations (e.g., emotion recognition, diarization) may significantly affect training. The paper acknowledges this limitation, but a quantitative noise analysis would strengthen the claims.\n3. The claim that the model is \"all-in-one\" with built-in basic capabilities may be slightly overstated, since these capabilities do not emerge intrinsically, but result from explicit task-specific supervised fine-tuning. The writing could moderate terms like \"built-in\" to avoid confusion and over-stating."}, "questions": {"value": "Please refer to the weakness section for my questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5j6AIYGMyp", "forum": "J0eNXpnrc7", "replyto": "J0eNXpnrc7", "signatures": ["ICLR.cc/2026/Conference/Submission25627/Reviewer_TgwS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25627/Reviewer_TgwS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410861489, "cdate": 1761410861489, "tmdate": 1762943498168, "mdate": 1762943498168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an instruction-tuning method for movie understanding. The assumption is that improving the character-ID information of existing MLLMs will improve their performance on movie tasks. Therefore, the proposed Omni-MLLM is first fine-tuned on a series of ID-related tasks (Basic Boosting Stage) and then adapted to the tasks to solve (Task Adaptability Stage). The method improves over the baseline Qwen2.5-Omni 7B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents interesting CoT experiments that confirm that CoT is not helping but show that it helps at training time. \n\n- The results over the baseline are good."}, "weaknesses": {"value": "Although the intuition behind character-ID information is interesting, there are several weak issues in this manuscript. \n1/ Clarity. \nThe overall clarity of the work, both in terms of language and writing but more importantly in terms of messages and findings is very hard. I find the paper hard to follow and every part somehow disconnected from the overall goal. \n\n2/ Lack of solid contributions. \nThe paper reads as if the main contribution is the Omni-CharM model, which is essentially a fine-tuned version of Qwen2.5-Omni 7B on character-ID information. Given that there exist already works that promote the intuition that character information is important (such as StoryTeller and IDA-VLM from Table 2) together with the fact that post-training is already well-adopted in the community makes the contribution of the work incremental and marginal. \n\n3/ Generated data. \nThe proposed dataset is generated by relying heavily on very large models (Gemini 2.5-Pro and Qwen2.5-VL-72B), which risks reproducibility and scalability. Nonetheless, the analysis of the dataset is very low and we cannot understand the impact of each of these tasks; the various design choices for each task, albeit explained, are not experimentally verified and seem arbitrary. \n\n4/ Lack of insights.\nThe method is presented as if having more data and ID information helps movie understanding but without more insights into what exactly is that existing MLLMs are missing.\n\n5/ Evaluation protocol. \nOmni-MLLM is evaluated on multiple-choice QA but open-ended or generative movie understanding tasks are not explored. \n\n6/ Missing computational efficiency report. \nTable 2 shows that the proposed method outperforms the baseline Qwen2.5-Omni 7B but without revealing the additional computational cost. I think this is an essential element missing into appreciating the impact, both during training and inference. This seems important as even within the paper we can see baselines that perform almost as well as the proposed model but require less resources: For instance, in Table 3 simply the F&S Desc (third row) performs almost as well as the full model with much less training compute and in Figure 3 using only character emotion recognition (less annotations, less data, less compute) performs also very well. \n\n7/ Not verified generalization.\nThe proposed two-stage training is applied on top of Qwen2.5-Omni 7B. It would be interesting to examine whether fine-tuning other MLLMs could also improve performances. \n\n8/ Data or structure or fine-tuning attribution.\nIt is unclear whether the improvement comes from the fact that (i) the model has seen more data; (ii) from the fact that the data are structured to include character-ID information or (iii) from the proposed fine-tuning setup.  \n\n9/ The paper is not standalone. \nSeveral parts are not clear and we are pointed to external papers to understand; even for core parts of the work, such as the second part of the method in Section 3.4."}, "questions": {"value": "For questions, it would be great if the authors could address some points from the weaknesses above. Specifically: \n\nQ1 (W2, W4) Clarify contributions and insights.\n\nQ2/ (W6) Compute. \n\nQ3 (W3) Perhaps examine how data quantity and data generation with open small models affect the performance? \n\nQ4 (W5) Examine performance on other evaluation protocols (open-ended). \n\nQ5 It would be great if the authors could provide some failure case analysis. \n\nQ6 Perhaps discuss in terms of ethics the gender bias, and representation and potential misuse of character-ID. \n\nQ7 (W8) This would be an interesting experiment to have but, of course, there will be no time to examine this; however, having an intuition of where the improvement(s) comes from would be very useful."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N7CewBpSkv", "forum": "J0eNXpnrc7", "replyto": "J0eNXpnrc7", "signatures": ["ICLR.cc/2026/Conference/Submission25627/Reviewer_pdST"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25627/Reviewer_pdST"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840317898, "cdate": 1761840317898, "tmdate": 1762943497987, "mdate": 1762943497987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper poposes Omni-MLLM for movie understanding, which can combine visual, textual, and audio information. They construct identity related data to enhance capability of ID identification, and leverage frame and shot descriptions to improve model's performance. Finally, they explore the influence of CoT in movie understanding. They achieve best performance on StoryQA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Movie understanding is a significant problem for MLLM. This paper proposes Omni-MLLM to combine all modalities to understand movies, which is essential.\n2. The constructed ID-related data is comprehensive and diverse. The exploration of CoT is interesting. Because the learning of CoT improves model's understanding ability, so the metrics get higher, but if inference with CoT, it may bring some errors in the intermediate process. Improving CoT's accuracy in inference is a challenge."}, "weaknesses": {"value": "1. The writing need be improved. The abstract and introduction should be organized better. For example, 'alleviate the difficulty of training.' in the abstract, and line 78, 'other models that focus on distractors' is not clear. \n2. Task Adaptability Stage in the dual-stage should be more clear. In this version, readers don't know how it is conducted, and its effectiveness in the experiments. Compared to the first stage, the second stage is too less detailed, maybe you need change the name of 'dual-stage'.\n3. The construction of ID-related data need more explanation in the main text. Because the data construction is this paper's main contribution."}, "questions": {"value": "1. See weakness.\n2. List differences between Storyteller and Omni-MLLM."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The copyright of movie data."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qqEvrCamTi", "forum": "J0eNXpnrc7", "replyto": "J0eNXpnrc7", "signatures": ["ICLR.cc/2026/Conference/Submission25627/Reviewer_wY84"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25627/Reviewer_wY84"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007100718, "cdate": 1762007100718, "tmdate": 1762943497800, "mdate": 1762943497800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}