{"id": "KLU4Eb2U2A", "number": 5515, "cdate": 1757917301883, "mdate": 1763724490411, "content": {"title": "Automatic Reward Shaping from Multi-Objective Human Heuristics", "abstract": "Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective  environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.", "tldr": "", "keywords": ["Reinforcement Learning", "Reward Shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3841b379819511fd74ca788213d00429d8f1e70b.pdf", "supplementary_material": "/attachment/51ba38ce3bcb743ce07d997d38a88cd6ffc2efe2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes MORSE (Multi-Objective Reward Shaping with Exploration), a bi-level optimization framework that automatically combines multiple heuristic rewards into a shaped reward for reinforcement learning. The method introduces stochastic exploration in the outer loop, guided by both task performance and a novelty metric computed via Random Network Distillation (RND). Experiments on several MuJoCo and Isaac Sim locomotion tasks show that MORSE can achieve performance comparable to manually tuned “oracle” rewards while avoiding local minima that affect conventional bi-level optimization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear and realistic: reward shaping is indeed a bottleneck in RL, and automating it is a useful and practical direction.\n\n2. The paper is well motivated and the proposed method is technically sound overall, combining bi-level optimization and exploration in a novel way.\n\n3. The approach is close to practice, since many real tasks combine a sparse goal reward with auxiliary shaping terms.\n\n4. The ablation studies are valuable, helping clarify which design elements (exploration trigger, RND novelty, policy reset) matter most.\n\n5. The writing is generally clear, and the figures illustrate the concept well."}, "weaknesses": {"value": "1.  The experiments are restricted to simple locomotion tasks with only 2–3 heuristic components. This is far from realistic use cases with many interdependent objectives. The study would be stronger if it included manipulation or visual tasks (for instance, environments from RLBench, James et al., 2020).\n\n2. While using novelty helps, relying solely on RND is not fully intuitive for reward-space exploration. Methods such as Bayesian Optimization could provide a more principled trade-off between novelty and task performance.\n\n3. Details are missing. Some important definitions are vague. For example, the novelty metric $V_novelty$ is referenced but never formally defined, and the “task criteria” normalization is unclear. This makes it harder to fully reproduce the results. How exactly the P is calculated?\n\n4. The first validation experiment (Sec 6.1.1) is essentially a synthetic numerical optimization example. While it supports intuition, it feels detached from actual RL training and resembles a simulated annealing procedure rather than a meaningful RL benchmark.\n\n5 . Since the method relies on policy resets and outer-loop exploration, it might be sensitive to the learning rate and reset frequency. No robustness analysis is given."}, "questions": {"value": "1.Would off-policy methods (e.g., SAC) benefit even more from this framework, since they can reuse past transitions for multiple reward hypotheses?\n\n2. Could the authors include more diverse environments, such as manipulation or navigation tasks, to test scalability beyond locomotion?\n\n3. How does MORSE compare to a simpler baseline that uses constant reward weights but periodically resets the policy (given that “wo/ Reset Policy” performs poorly in Fig. 4)?\n\n4. How is the oracle reward function defined—are those weights hand-tuned or optimized via grid search?\n\n5. Since learning rate choices affect convergence and local minima, could the authors test MORSE and the baselines under different rates to show robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Etb7cLTfuV", "forum": "KLU4Eb2U2A", "replyto": "KLU4Eb2U2A", "signatures": ["ICLR.cc/2026/Conference/Submission5515/Reviewer_WoB4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5515/Reviewer_WoB4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863085036, "cdate": 1761863085036, "tmdate": 1762918104105, "mdate": 1762918104105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present MORSE, a framework for reward shaping that automates reward weight tuning. The method seeks to improve exploration of the reward space by guiding the RL policy towards higher task success. Their goal is not to achieve or balance multiple objectives, but rather to achieve a higher task success rate in general.\n\nThe authors propose an inner and outer loop algorithm to automatically find reward weights that aim to maximize the task reward. The inner loop trains an RL policy given a reward weight. The outer loop assigns a novel weight to the inner loop to overcome local optima. This is achieved with an exploration-guided formulation using Random Network Distillation.\n\nOverall, I am positive about this work; the idea is simple and well-evaluated. The paper is on the theoretical side, and I would like to see it applied to more practical problems that involve many more reward terms, where the problem of reward tuning becomes truly cumbersome."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is easy to follow and provides sufficient details for reproducibility. The authors also release their code.\n- The authors address an important task in RL, finding reward weights for multiple reward terms is a difficult and time-consuming task, and at the same time, it is crucial for successful training.\n- The authors run extensive validation on synthetic examples as well as popular benchmarks, and they provide ablation studies for their design decisions."}, "weaknesses": {"value": "- The paper's primary weakness lies in its comparison to existing methods. For me, it is not fully clear what the precise differences and similarities to traditional Multi-Objective Reinforcement Learning (MORL) are. The authors claim to solve a different task, but it is plausible that a standard MORL formulation could serve as a strong baseline with minor modifications. A direct comparison is currently missing. For instance, the \"Gradient w/ Reset\" baseline is intuitively similar to some MORL training approaches; it would be helpful if the authors extended their discussion on the differences or explained why MORL is not an option for this task.\n- Another weakness is the demonstrated scalability of the method. Reward tuning becomes especially cumbersome in high-dimensional problems (e.g., 10s of reward terms), whereas finding weights between 2-3 terms, as done in the paper's evaluations, can often be done relatively quickly. This connects to the question of scalability raised later. Furthermore, this raises the question of how the method would compare to a simple baseline, such as a non-expert’s first guess.\n\n**Minor:**\n- Missing references for IsaacSim and MuJoCo"}, "questions": {"value": "- Instead of balancing potentially conflicting objectives, the authors assume their task has a clear success measurement. I am still wondering how the formulation would change or differ if the measurement for success were not clearly defined. For example, in the motion imitation literature, MORL formulations have been studied (e.g., Alegre et al., 2025, AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning). In this task, the measurement for success can be based on different preferences; for instance, sometimes tracking the motion accurately is most important, while other times reducing vibrations is prioritized. The used rewards, in this case, are dense rewards, but the weights between them reflect these different, sometimes subjective, preferences. I am wondering how this view of success, which can vary based on preferences, could be understood in the context of this work.\n- Traditional MORL learns optimal policies for any given reward term weight. This would, in theory, allow one to search in the dense auxiliary-reward space for a policy that maximizes the primary-task reward. The authors motivate their method with a modified cartpole example, where we see that setting random weights and following the gradient results in optimal performance. This makes me wonder if a traditional MORL solution would not achieve a similar effect. How do the two approaches truly compare? The MORL formulation seems to provide more flexibility (e.g., allowing a change in the task reward after training and still finding the optimal weights), whereas this method appears more rigid, providing only a task-specific solution.\n- Following a similar line of thought, but from another direction: the reward space might differ for different environments (e.g., if domain randomization is applied to mass properties). Since this method ultimately provides a somewhat rigid solution, I was wondering how large the sim-to-real gap might become, ultimately, if I want to deploy my policy to the real world, my reward shaping is usually heavily influenced by this target. While I do not expect the authors to compare simulation results with real-world deployments (though that would be highly preferred), I would be interested to understand how domain randomization influences the reported results. Does the solution remain similar, or does it diverge?\n- The number of reward objectives in the evaluated examples is relatively low (max 3 rewards). However, tasks like motion imitation learning can involve 10s of reward terms. How does this method scale to a significantly larger number of terms? The authors mention this limitation in the appendix, but can we quantify when the method starts to break?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "isWbNVEu1R", "forum": "KLU4Eb2U2A", "replyto": "KLU4Eb2U2A", "signatures": ["ICLR.cc/2026/Conference/Submission5515/Reviewer_fjsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5515/Reviewer_fjsX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943572210, "cdate": 1761943572210, "tmdate": 1762918103830, "mdate": 1762918103830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New experiment results and major revisions (Part 1)"}, "comment": {"value": "Thank you for your insightful suggestions. We have conducted additional experiments and updated the detailed analysis in the revised PDF. Below we summarize the final results corresponding to the reviewers’ requests. Each cell reports the mean performance, with standard deviation shown in parentheses.\n# Main Results (Sec 6.2.2)\nWe conduct experiments on two IsaacLab manipulation environments: Reach-Franka (Isaac-Reach-Franka-v0) and Lift-Franka (Isaac-Lift-Cube-Franka-IK-Rel-v0), which contain 4 and 5 objectives. We also evaluate Unitree-A1-9obj (Isaac-Velocity-Flat-Unitree-A1-v0), which has nine objectives. We train Reach-Franka, Lift-Franka, and Unitree-A1-9obj for 1500, 5000, and 5000 iterations, respectively, and report success rates over 8 seeds (mean and std).\n\nAcross all tasks, MORSE significantly improves success rates compared to Gradient Only.\n\n| | Reach-Franka |  Lift-Franka | Unitree-A1-9obj  |\n| :--- | :--- | :--- | :--- |\n| Oracle | 0.844 (0.041) | 0.859 (0.325) | 0.998 (0.001) |\n| MORSE | 0.721 (0.209) | 0.871 (0.329) | 0.729 (0.41) |\n| Gradient | 0.533 (0.258) | 0.471 (0.472) | 0.143 (0.323) |\n\n# Ablation Studies (Sec. 6.2.3)\nWe add a **w/o Gradient** variant that removes outer-loop gradient updates and keeps only outer-loop exploration.\n\n**w/o Gradient** performs worse than **MORSE**, suggesting that outer-loop gradient update is useful. **Gradient Only** performs worse than **MORSE**, suggesting that gradient update alone is insufficient and should be complemented with exploration.\n\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| **MORSE** | 0.874 (0.33) | 0.999 (0.003) | 0.945 (0.134) |\n| **w/o Gradient** | 0.537 (0.344) | 1.0 (0.0) | 0.74 (0.304) |\n| **Gradient Only** | 0.091 (0.159) | 0.375 (0.484) | 0.176 (0.307) |\n\n# Applying MORSE to Real-World Robotic Training (Appendix, Sec. A.2)\nTo demonstrate MORSE’s utility in realistic robotics workflows, we conduct experiments under domain randomization, which is commonly used to bridge the sim-to-real gap and substantially increases training difficulty.\n## Pipeline\nOur proposed workflow consists of two stages. In stage 1, we run MORSE in environments with no or minimal domain randomization to obtain satisfying reward weights. Then, in stage 2, using the reward weights from Stage 1, we train policies under full domain randomization using standard RL algorithms until convergence.\n## Experiments on IsaacLab Quadcopter\nFor stage 1, we randomize only the target position and run MORSE for 8 seeds, collecting 8 sets of reward weights. This corresponds to the experiments in Sec. 6.2.2 Main Results. In stage 2, we randomize the quadcopter’s mass and initial states, and train separate RL policies from scratch, using the oracle weight and the 8 weights found by MORSE. Each weight is tested 4 times for 150 iterations to convergence. \n\nUnder domain randomization, the reward weights obtained by MORSE achieve performance comparable with the oracle reward. This demonstrates that MORSE-derived reward functions generalize effectively and therefore can be applied to practical robotics settings.\n\n| Oracle | MORSE 1 | MORSE 2 | MORSE 3 | MORSE 4 | MORSE 5 | MORSE 6 | MORSE 7 | MORSE 8 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1.0 (0.0) | 0.998 (0.004) | 0.99 (0.016) | 0.997 (0.004) | 1.0 (0.0) | 0.993 (0.011) | 1.0 (0.0) | 0.995 (0.009) | 0.996 (0.006) |\n\n# Alternative Design Choices of MORSE (Appendix, Sec. C.1)\nMORSE is a modular framework, and each of its component can be instantiated differently.\n## Outer-loop gradient computation\nWe compare Implicit Function (our choice) with Meta-Gradient Learning [1] under Gradient Only setting. The two estimators yield similar performance, likely because both are trapped in local optima in the absence of exploration.\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| MORSE (Implicit Function) | 0.091 (0.159) | 0.375 (0.484) | 0.176 (0.307) |\n| Meta-Gradient Learning | 0.098 (0.253) | 0.375 (0.484) | 0.32 (0.419) |\n\n## Outer-loop exploration strategy\nWe compare Random Network Distillation (our choice), random sampling, and Bayesian Optimization[2]. Across these variants, RND achieves the strongest performance, providing more informative exploration signals for reward-weight search.\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| MORSE (RND) | 0.875 (0.331) | 1.0 (0.0) | 0.977 (0.062) |\n| Random Sampling | 0.502 (0.433) | 1.0 (0.0) | 0.85 (0.327) |\n| Bayesian Optimization | 0.459 (0.467) | 1.0 (0.0) | 0.877 (0.314) |\n\n[1] Hu, Yujing, et al. \"Learning to utilize shaping rewards: A new approach of reward shaping.\" Advances in Neural Information Processing Systems 33 (2020): 15931-15941.\n\n[2] Shahriari, Bobak, et al. \"Taking the human out of the loop: A review of Bayesian optimization.\" Proceedings of the IEEE 104.1 (2015): 148-175."}}, "id": "AuyhfVPwqG", "forum": "KLU4Eb2U2A", "replyto": "KLU4Eb2U2A", "signatures": ["ICLR.cc/2026/Conference/Submission5515/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5515/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5515/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720464495, "cdate": 1763720464495, "tmdate": 1763721677492, "mdate": 1763721677492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "New experiment results and major revisions (Part 1)"}, "comment": {"value": "Thank you for your insightful suggestions. We have conducted additional experiments and updated the detailed analysis in the revised PDF. Below we summarize the final results corresponding to the reviewers’ requests. Each cell reports the mean performance, with standard deviation shown in parentheses.\n# Main Results (Sec 6.2.2)\nWe conduct experiments on two IsaacLab manipulation environments, Reach-Franka (*Isaac-Reach-Franka-v0*) and Lift-Franka (*Isaac-Lift-Cube-Franka-IK-Rel-v0*), which contain 4 and 5 objectives. We also evaluate Unitree-A1-9obj (*Isaac-Velocity-Flat-Unitree-A1-v0*), which has 9 objectives. We train Reach-Franka, Lift-Franka, and Unitree-A1-9obj for 1500, 5000, and 5000 iterations, respectively, and report success rates over 8 seeds (mean and std).\n\nAcross all tasks, MORSE significantly improves success rates compared to Gradient.\n\n| | Reach-Franka |  Lift-Franka | Unitree-A1-9obj  |\n| :--- | :--- | :--- | :--- |\n| Oracle | 0.844 (0.041) | 0.859 (0.325) | 0.998 (0.001) |\n| MORSE | 0.721 (0.209) | 0.871 (0.329) | 0.729 (0.41) |\n| Gradient | 0.533 (0.258) | 0.471 (0.472) | 0.143 (0.323) |\n\n# Ablation Studies (Sec. 6.2.3)\nWe add a **w/o Gradient** variant that removes outer-loop gradient updates and keeps only outer-loop exploration.\n\n**w/o Gradient** performs worse than **MORSE**, suggesting that outer-loop gradient update is useful. **Gradient Only** performs worse than **MORSE**, suggesting that gradient update alone is insufficient and should be complemented with exploration.\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| **MORSE** | 0.874 (0.33) | 0.999 (0.003) | 0.945 (0.134) |\n| **w/o Gradient** | 0.537 (0.344) | 1.0 (0.0) | 0.74 (0.304) |\n| **Gradient Only** | 0.091 (0.159) | 0.375 (0.484) | 0.176 (0.307) |\n# Applying MORSE to Real-World Robotic Training (Appendix, Sec. A.2)\nTraining with domain randomization can bridge the sim-to-real gap of RL policies but substantially increases training difficulty. Therefore, we propose a 2-stage pipeline to extend MORSE to support domain randomization. \n\n## Pipeline Overview\nOur proposed pipeline consists of two stages. In stage 1, we run MORSE in environments with no or minimal domain randomization to obtain satisfying reward weights. Then, in stage 2, using the reward weights from Stage 1, we train policies under full domain randomization with standard RL algorithms to convergence.\n\n## Experiments on IsaacLab Quadcopter\nFor stage 1, we randomize only the target position and run MORSE for 8 seeds, collecting 8 sets of reward weights. This corresponds to the experiments in Sec. 6.2.2 Main Results. In stage 2, we randomize the quadcopter’s mass and initial state, and train RL policies from scratch using the oracle weight and the 8 weights found by MORSE. Each weight is tested 4 times for 150 iterations to convergence. \n\nUnder domain randomization, the reward weights obtained by MORSE achieve performance comparable with the oracle reward. This demonstrates that MORSE-derived reward functions generalize effectively and therefore can be applied to practical robotics settings.\n| Oracle | MORSE 1 | MORSE 2 | MORSE 3 | MORSE 4 | MORSE 5 | MORSE 6 | MORSE 7 | MORSE 8 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 1.0 (0.0) | 0.998 (0.004) | 0.99 (0.016) | 0.997 (0.004) | 1.0 (0.0) | 0.993 (0.011) | 1.0 (0.0) | 0.995 (0.009) | 0.996 (0.006) |\n# Alternative Design Choices of MORSE (Appendix, Sec. C.1)\nMORSE is a modular framework, and each of its component can be instantiated differently.\n## Outer-loop gradient computation\nFor outer-loop gradient approximation, we compare Implicit Function (our choice) with Meta-Gradient Learning [1] under Gradient setting. The two estimators yield similar performance, likely because both are trapped in local optima in the absence of exploration.\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| MORSE (Implicit Function) | 0.091 (0.159) | 0.375 (0.484) | 0.176 (0.307) |\n| Meta-Gradient Learning | 0.098 (0.253) | 0.375 (0.484) | 0.32 (0.419) |\n\n## Outer-loop exploration strategy\nWe compare Random Network Distillation (our choice), random sampling, and Bayesian Optimization[2] as outer-loop exploration strategy.  Across these variants, RND achieves the strongest performance, providing the most informative exploration signals for weight search.\n| | Halfcheetah-hard | Hopper-hard | Walker2d-hard |\n| :--- | :--- | :--- | :--- |\n| MORSE (RND) | 0.875 (0.331) | 1.0 (0.0) | 0.977 (0.062) |\n| Random Sampling | 0.502 (0.433) | 1.0 (0.0) | 0.85 (0.327) |\n| Bayesian Optimization | 0.459 (0.467) | 1.0 (0.0) | 0.877 (0.314) |\n\n[1] Hu, Yujing, et al. \"Learning to utilize shaping rewards: A new approach of reward shaping.\" Advances in Neural Information Processing Systems 33 (2020): 15931-15941.\n\n[2] Shahriari, Bobak, et al. \"Taking the human out of the loop: A review of Bayesian optimization.\" Proceedings of the IEEE 104.1 (2015): 148-175."}}, "id": "AuyhfVPwqG", "forum": "KLU4Eb2U2A", "replyto": "KLU4Eb2U2A", "signatures": ["ICLR.cc/2026/Conference/Submission5515/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5515/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5515/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720464495, "cdate": 1763720464495, "tmdate": 1763739010060, "mdate": 1763739010060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors propose MORSE, a framework to automatically combine a set of human-provided, unweighted heuristic reward functions into a single (shaped) reward function. The system only requires practitioners to specify these heuristics and a sparse, episodic task performance criterion\n- MORSE formulates this as a bi-level optimization problem\n  - The inner loop trains an RL policy to maximize the expected return from the current shaped reward\n  - The outer loop updates the reward weight parameters to maximize the sparse task performance\n- The paper's core premise is that standard gradient-based bi-level optimization fails in this domain because the weight-performance landscape is highly non-convex. MORSE addresses this by introducing a guided stochastic exploration mechanism into the outer loop\n- The method is evaluated on a set of simple locomotion tasks"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses a critical bottleneck in RL for robotics: the labor-intensive and error-prone process of manual reward function design\n- Framing the problem as a bi-level optimization is a good approach (also used in other works)\n- The use of synthetic 2D optimization functions to isolate and test the outer-loop exploration strategy (Sec 6.1) is a good experimental design concept and the motivating example was clear"}, "weaknesses": {"value": "- Tables 1 and 2 report means without any confidence intervals or error bars, making it impossible to validate the significance of the results\n- The paper is a methodology paper but is only tested on locomotion tasks. This is not a robotics paper. The method's generality is unproven, and it should have been tested on a broader set of environments\n- The experiments use tasks with only 2 or 3 heuristics, despite motivating the problem with a 15-heuristic example (Margolis and Agrawal). The method's performance in the high-dimensional settings where it would be most valuable is unknown\n- Can the authors provide experimental validation on manipulation as well? Can the authors provide results for more complex tasks with a higher number of heuristics?\n- The claim that RND \"discovers more novel reward weights\" than random sampling does not have much supporting evidence\n- The paper's premise that gradient-based methods are unsuitable for this problem is an oversimplification and ignores the empirical success of such methods in non-convex settings\n- The related work section (Sec 2.2) fails to cite or compare against other key papers in automatic reward shaping for robotics [1, 2]\n- Tables 1 and 2 are mislabeled, using \"Total\" instead of \"Average\" for the final column\n\nReferences\n[1] Ma, Y.J., Liang, W., Wang, G., Huang, D.A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L. and Anandkumar, A., 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.\n[2] Zhang, C.B.C., Hong, Z.W., Pacchiano, A. and Agrawal, P., 2024. ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization. arXiv preprint arXiv:2410.13837."}, "questions": {"value": "- The task formulation (Sec 3.1) assumes a simple linear combination of heuristics. Why was this restrictive form chosen? What about non-linear combinations or interaction terms (eg $R_{h_1} \\times R_{h_2}$) that might be required to model complex objective trade-offs?\n- The outer loop updates the reward weights, which can drastically change the optimization landscape for the inner-loop policy. If you take significant gradient steps on the reward weights, does the policy training not suffer from severe instability due to the non-stationary reward function? How is this handled?\n- The paper states \"the resulting landscape exhibits numerous local optima, violating the assumptions under which Gradient succeeds\". While vanilla GD might fail, modern gradient-based optimizers are designed for and show enormous empirical success in non-convex settings. Can the authors provide a more rigorous justification for why this specific problem is intractable for standard gradient-based optimization? Have the authors tried other gradient-based optimization techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ka3RDUNwhL", "forum": "KLU4Eb2U2A", "replyto": "KLU4Eb2U2A", "signatures": ["ICLR.cc/2026/Conference/Submission5515/Reviewer_gZgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5515/Reviewer_gZgz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973320388, "cdate": 1761973320388, "tmdate": 1762918103515, "mdate": 1762918103515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}