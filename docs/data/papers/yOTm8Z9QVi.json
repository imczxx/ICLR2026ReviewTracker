{"id": "yOTm8Z9QVi", "number": 23822, "cdate": 1758348915899, "mdate": 1759896795218, "content": {"title": "Expert-Integrated Active Learning for Optimizing LLM Agents", "abstract": "Recent advances in Large Language Models (LLMs) have created new opportunities for their application in interactive environments. However, these agentic tasks present significant challenges due to the complexity of long and specialized interaction trajectories that are underrepresented in standard training distributions. While Reinforcement Learning (RL) post-training offers a promising approach to mitigate the need for extensive human-annotated data, it faces fundamental limitations in exploration efficiency when applied to LLMs. In this paper, we introduce a novel framework that synergistically combines RL post-training with Active Learning (AL) for LLM agents. By choosing informative tasks with reward-based filter and diversity-based selection criteria, our approach enables models to not only refine their capabilities through autonomous exploration but also strategically request expert demonstrations for challenging scenarios, thereby extending their exploration boundaries. We demonstrate the efficacy of this method on the AppWorld benchmark, showing significant performance improvements with minimal expert demonstrations. We then further look into adapting our framework for different budget and examine the factors that affect the final performance. Our method highlights the potential of efficiently integrating human resources within RL pipelines to enhance LLM agents' capabilities in complex interactive environments.", "tldr": "", "keywords": ["LLM Agent", "Reinforcement Learning", "Active Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9757eeadcab7e1b6d12cdf137e0776af7b976c50.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "```\nI used LLM to fix the grammar of the Official Review, but all opinions are my own\n```\nCollecting expert data is challenging, which is why reinforcement learning (RL) has recently become popular. However, combining large language models (LLMs) with RL introduces several problems. One issue is that LLMs operate over a massive token vocabulary, where each token corresponds to a distinct, environment-specific action. This leads to an enormous exploration space in which only a sparse subset of tokens are actually meaningful. Another problem is that pretraining constrains the model’s exploration capability. To address this, the authors propose leveraging active learning to enhance exploration efficiency: the model first explores freely, and then the system filters for high-quality samples, which are subsequently annotated by experts for further learning. The filtering focuses on selecting challenging samples. While the idea itself is interesting, the paper’s execution appears incomplete. Many parts of the methodology are unclear, and the experiments fail to convincingly demonstrate the method’s general applicability. My current inclination is to reject, though the final decision could depend on the quality of the rebuttal."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Combining active learning with RL to guide expert labeling is interesting and potentially valuable."}, "weaknesses": {"value": "1. I am not convinced of the generalizability of the proposed approach.\n2. The paper’s exposition is too brief and lacks clarity, making it difficult to understand the key implementation details."}, "questions": {"value": "1. Your selection rule prioritizes difficult samples and those showing little progress, but does this only work for environments that provide intermediate rewards? For benchmarks like HLE or SWE-Bench that lack intermediate signals, how would your method still apply?\n\n2. The “expert” component seems crucial, but the paper’s description is very vague. Are these experts humans, or stronger models? Have you tested different types of experts for comparison? Could you provide concrete examples of expert demonstrations? Without examples, it’s hard to understand the actual procedure.\n\n3. If the experts are humans, the cost would be prohibitively high. Could the process instead leverage stronger models as experts? More generally, how could you design a multi-agent RL setup with active learning using strong model-based experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PmNmLRPJi3", "forum": "yOTm8Z9QVi", "replyto": "yOTm8Z9QVi", "signatures": ["ICLR.cc/2026/Conference/Submission23822/Reviewer_cgP2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23822/Reviewer_cgP2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760679907751, "cdate": 1760679907751, "tmdate": 1762942821023, "mdate": 1762942821023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework that integrates AL and RL for training LLM–based agents in interactive environments. The authors identify the key limitation of standard RL post-training—inefficient exploration—and address it by introducing a mechanism through which the model can actively query for expert demonstrations. Specifically, the proposed system employs a reward-based filter and diversity-based task selection to identify challenging and informative tasks for expert annotation, which are then incorporated into training through a carefully designed mixing strategy. Experiments on the AppWorld benchmark show that this approach achieves notable improvements in task success rates over RL-only baselines, while requiring significantly fewer expert demonstrations compared to full supervision. The authors also provide comprehensive ablations on budget control, task diversity, and trajectory analysis, highlighting how expert data improves exploration efficiency and consistent behavioral patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a well-motivated approach that bridges AL with RL-based post-training. The proposed framework effectively addresses the long-standing challenge of balancing exploration efficiency and annotation cost in agentic training.\n2. The framework is clearly explained, including detailed algorithmic steps for reward-based filtering, diversity-based selection, and the trajectory mixing strategy. The proposed mechanisms are conceptually sound and practically relevant.\n3. The authors provide extensive experiments on AppWorld, demonstrating consistent improvements under multiple model scales (7B and 14B). The inclusion of ablation studies on similarity thresholds, early stopping, and demonstration efficiency adds strong empirical support."}, "weaknesses": {"value": "1. The proposed method still requires non-trivial manual effort. Although active selection reduces the annotation cost, the framework's practicality in large-scale real-world deployment remains uncertain. It would be beneficial to discuss possible automation or self-improvement mechanisms to reduce expert reliance.\n2. The experiments are conducted solely on AppWorld. While this benchmark is well-suited for interactive environments, validation on additional settings (e.g., WebShop, OSWorld, or τ-bench) would strengthen the claim of general applicability.\n3. The paper uses synthetic expert data (generated by DeepSeek-V3.1) instead of real human annotations. This design choice raises questions about the robustness of the findings in true human-in-the-loop scenarios.\n4. The framework involves repeated task selection, similarity computation, and trajectory mixing. The paper does not provide detailed analysis of computational overhead or memory usage, which would be important for evaluating its scalability to larger models or environments."}, "questions": {"value": "Please check my comments in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aZK2WRbS3z", "forum": "yOTm8Z9QVi", "replyto": "yOTm8Z9QVi", "signatures": ["ICLR.cc/2026/Conference/Submission23822/Reviewer_3Xpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23822/Reviewer_3Xpm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570071888, "cdate": 1761570071888, "tmdate": 1762942820651, "mdate": 1762942820651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an expert-integrated active learning (AL) framework to optimize LLM agents through GRPO-based RL post-training. The core approach involves a two-stage task selection process: (1) reward-based filtering to identify tasks with persistent failure and stagnant returns; (2) diversity-based max-min selection to eliminate redundancy. Subsequently, expert demonstrations are requested only for these tasks. Training employs a hybrid policy that blends high-quality expert trajectories with model self-sampled trajectories at a constrained ratio, leveraging expert knowledge while preserving exploration capabilities."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "## Strengths\n\n1. **Originality**\n- The paper integrates active selection of tasks with expert demonstrations into an on-policy GRPO training loop for agents—moving beyond passive offline demo mixing; the reward-stagnation + success-rate filter is a simple, RL-appropriate proxy for uncertainty.\n- The diversity-based max-min selection with a historical buffer is a practical twist that avoids repeatedly annotating near-duplicates across steps.\n2. **Quality**\n- Under matched budgets and consistent evaluation protocols, the method demonstrates stable and statistically significant performance gains relative to baselines, reflecting its empirical quality.\n- Not only did the report show overall improvement, but it also presented detailed metrics analysis such as demonstration utilization rate and reuse frequency (efficiency), revealing the phenomenon that \"a small number of high-value demonstrations are repeatedly utilized.\" This aligns with the expectation that active learning reduces annotation overhead.\n\n3. **Clarity**\n-  The paper is well-organized, with a coherent narrative from motivation to validation, making its technical and empirical contributions easy to follow.\n\n4. **Significance**\n- The paper addresses a real bottleneck for agent RL—inefficient exploration and annotation cost—and shows measurable gains on a recognized agentic benchmark with reduced cost\n\n\n---"}, "weaknesses": {"value": "## Weaknesses\n1. Lack of side-by-side comparison with existing AppWorld proxy methods: The experiment only compared the author's own three settings (GRPO/baseline, full demo, active learning) without providing direct numerical comparisons against representative methods publicly reported on AppWorld.\n2. Figures and tables are not self-contained: beyond titles, they lack descriptive captions specifying the evaluation setup, metric definitions/units,  and significance markers. This weakens clarity and makes it hard to interpret results at a glance.\n\n\n\n---"}, "questions": {"value": "## Questions\n1. Would you add head-to-head comparisons against representative AppWorld agents reported in prior work (e.g., recent public baselines), under the same evaluation protocol? \n2. In the main results table, why are Test-set results for the expert model (DeepSeek-V3.1)—which supplies demonstrations—omitted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cs9LrMB1R2", "forum": "yOTm8Z9QVi", "replyto": "yOTm8Z9QVi", "signatures": ["ICLR.cc/2026/Conference/Submission23822/Reviewer_BP7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23822/Reviewer_BP7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23822/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983043910, "cdate": 1761983043910, "tmdate": 1762942820408, "mdate": 1762942820408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}