{"id": "IdJakw2jta", "number": 2705, "cdate": 1757213874741, "mdate": 1763181747715, "content": {"title": "Towards Long-Form Spatio-Temporal Video Grounding", "abstract": "Videos can span several minutes or even hours in real scenarios, yet current research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing target from a video of tens of seconds, typically less than one minute, limiting its applications. In this paper, we explore $\\textbf{L}$ong-$\\textbf{F}$orm $\\textbf{STVG}$ ($\\textbf{LF-STVG}$), that aims to locate the target in long-term videos. In LF-STVG, long-term videos encompass a much longer temporal span and more irrelevant information, making it challenging for current short-form STVG models that process all frames at once. Addressing these, we introduce a novel $\\textbf{A}$uto$\\textbf{R}$egressive $\\textbf{T}$ransformer framework for LF-$\\textbf{STVG}$ ($\\textbf{ART-STVG}$). Unlike current STVG methods requiring seeing the entire sequence to make a full prediction at once, our ART-STVG treats the video as a streaming input and processes its frames sequentially, making it capable of easily handling the long videos. To capture spatio-temporal context in ART-STVG, spatial and temporal memory banks are developed and applied to decoders of ART-STVG. Considering that memories at different moments are not always relevant for localizing the target in current frame, we propose simple yet effective memory selective strategies that enable more relevant information for the decoders, greatly improving performance. Moreover, rather than parallelizing spatial and temporal localization as done in existing approaches, we introduce a novel cascaded spatio-temporal design that connects spatial decoder to temporal decoder during grounding. This way, our ART-STVG leverages more fine-grained target information to assist with complicated temporal localization in complex long videos, further boosting the performance. On the newly extended datasets for LF-STVG, ART-STVG largely outperforms current state-of-the-art approaches, while showing competitive results on conventional Short-Form STVG. Our code and models will be released.", "tldr": "", "keywords": ["Spatio-Temporal Video Grounding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6a39c0b15779fd11e5a273a8299ddfd76d87326c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper first introduces a new task, i.e., long-form spatio-temporal video grounding (LF-STVG), and proposes the ART-STVG, an autoregressive, memory-augmented transformer for LF-STVG. It processes video frame-by-frame, maintains spatial and temporal memory banks with selective retrieval, and uses a cascaded design so temporal localization benefits from the spatial prediction. The authors also extend HCSTVG-v2 to 1–5-minute validation and report large gains over TubeDETR, STCAT, CG-STVG, and TA-STVG on these long-form sets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a new task setting, i.e., long-form spatio-temporal video grounding (LF-STVG).\n- The authors also extend HCSTVG-v2 to 1–5-minute validation and report large gains over TubeDETR, STCAT, CG-STVG, and TA-STVG on these long-form sets.\n- This paper is easy to understand."}, "weaknesses": {"value": "- Although the paper frames LF-STVG as long-form, the maximum evaluated duration (~5 minutes) is modest when compared to datasets like MAD, where the average video spans ~110.8 minutes [a]. Moreover, OmniSTVG [b] investigates long-form conditions and multi-object detection, thereby encompassing a more comprehensive setting.\n- ART-STVG seems to assemble known techniques (self-attention fusion, memory banks, selection mechanisms), and the specific novelty remains unclear.\n- This paper claims that previous methods generally process all frames in one time, so it treats the video as a streaming input and processes its frames sequentially. However, in feature extraction, VideSwin is used to extract the motion features with previous frames as input. That means when processing the last frame, all frames will be used for the motion feature extraction. And another question is each time the current frame is processed, are all the previous frames used again? Wouldn’t that increase the computational load? I’d like to know how the training and inference time of this method compares with previous methods.\n- In this paper, FPS is set to 3.2. How will it influence the performance?\n\n[a] Mad: A scalable dataset for language grounding in videos from movie audio descriptions, CVPR 2022.\n[b] OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding, arxiv 2025."}, "questions": {"value": "See the detailed questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y7am502wA2", "forum": "IdJakw2jta", "replyto": "IdJakw2jta", "signatures": ["ICLR.cc/2026/Conference/Submission2705/Reviewer_wm7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2705/Reviewer_wm7F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760637557069, "cdate": 1760637557069, "tmdate": 1762916337967, "mdate": 1762916337967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "VSqePI60Ju", "forum": "IdJakw2jta", "replyto": "IdJakw2jta", "signatures": ["ICLR.cc/2026/Conference/Submission2705/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2705/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763181746737, "cdate": 1763181746737, "tmdate": 1763181746737, "mdate": 1763181746737, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first studies Long-Form Spatio-Temporal Video Grounding (LF-STVG) and proposes a new framework ART-STVG that handles long-term videos. It treats the video as a streaming input and processes its frames sequentially. It proposes spatial and temporal memory banks to capture spatio-temporal context and designs memory selective strategies that enable more relevant information for the decoders. Rather than parallelizing spatial and temporal localization, it introduces a cascaded spatio-temporal design. On the newly extended datasets for LF-STVG, it outperforms current Short-Form STVG methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tIt is well-motivated to be the first to explore the LF-STVG problem and propose the first framework attempting to handle LF-STVG.\n2.\tIt achieves SOTA results on extended datasets for LF-STVG, while maintains competitive results on SF-STVG.\n3.\tThe writing is clear, and the presentation of figures and diagrams is good."}, "weaknesses": {"value": "1.\tSome ablation studies are missing. For example, what is the impact of the number of selective temporal memories? What is the impact of the order of spatial decoder and temporal decoder? \n2.\tMore detailed discussion should be complemented. Since ART-STVG trades time for space by ingesting frames one at a time, detailed discussion on the trade-off between space and time is valuable to verify the balance in practical scenes. \n3.\tMore explanation should be given. Generally, longer videos bring richer spatiotemporal information and better performance. However, it is counter-intuitive that the performance gain decreases from LF-STVG-1min to LF-STVG-5min in Table 1. Therefore, more explanations or discussions about this phenomenon are needed."}, "questions": {"value": "1.\tWhat is the performance when using different 2D backbone, 3D backbone, and text backbone?\n2.\tWhat is the performance of directly splitting long-term videos into multiple short clips, and then using current SF-STVG methods to do STVG?\n3.\tWhat is the performance of ART-STVG compared to larger models, like Multimodal LLMs used for spatio-temporal video grounding (e.g., [a])?\n\n[a] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding, arXiv 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ww5VvLL4v", "forum": "IdJakw2jta", "replyto": "IdJakw2jta", "signatures": ["ICLR.cc/2026/Conference/Submission2705/Reviewer_c8Pk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2705/Reviewer_c8Pk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761032328554, "cdate": 1761032328554, "tmdate": 1762916337666, "mdate": 1762916337666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ART-STVG, a memory-augmented framework for Long-Form STVG. It models long-term dependencies via selective memory and employs a cascaded decoder to leverage spatial cues for temporal grounding. ART-STVG significantly outperforms existing methods on extended datasets of HCSTVG-v2 and demonstrates strong generalization to short videos."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper introduces ART-STVG for long-form spatio-temporal video grounding (LF-STVG), using frame-wise autoregressive decoding to mitigate the memory burden and irrelevant distractions of parallel full-clip processing.\n2.\tIt proposes novel spatio-temporal memory selection strategies: spatially, it selects context by comparing the text query with memory slots; temporally, it segments by comparing adjacent memory slots and retrieves the most recent segment—an effective design for long-video scenarios.\n3.\tIt adopts a well-motivated spatial→temporal cascade, leveraging fine-grained spatial evidence to more accurately localize temporal boundaries.\n4.\tOn HC-STVG-v2, the method delivers consistent gains and ablations verify the independent contributions of each component."}, "weaknesses": {"value": "1.\tAll experiments are conducted on the extended HC-STVG-v2 long-video validation set, with no evaluation on other grounding benchmarks such as VidSTG [A] (short/long). The model’s generalization therefore remains to be further verified.\n2.\tThe structural novelty is moderate. The adopted mechanisms—multimodal feature extraction and fusion, selective memory, and memory banks—have been widely used in video QA, temporal localization, and long-video modeling. The paper’s main contribution lies in task-oriented integration and pruning for fine-grained STVG via an autoregressive streaming pipeline plus a spatial→temporal cascade with text-guided memory selection.\n3.\tHow does the size of the memory bank grow with video length? Is there an eviction/compression policy, and what is its quantitative impact on latency and memory usage?\n4.\tAutoregression reduces memory but increases latency; the main paper lacks scaling curves of latency/memory versus video length, so the boundary of the engineering trade-off is unclear.\n5.\tThe memory selection relies on heuristic similarity rules (text–memory and adjacent-memory); comparisons against learnable or uncertainty-based alternatives are missing."}, "questions": {"value": "See the comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCGbkoYpM5", "forum": "IdJakw2jta", "replyto": "IdJakw2jta", "signatures": ["ICLR.cc/2026/Conference/Submission2705/Reviewer_He6Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2705/Reviewer_He6Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964062782, "cdate": 1761964062782, "tmdate": 1762916336881, "mdate": 1762916336881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new benchmark for the new task of Long-Form Spatio-Temporal Video Grounding (LF-STVG) along with a new framework dubbed ART-STVG specifically designed for LF-STVG. LF-STVG extends the video duration of traditional short-form STVG from tens of seconds to several minutes and the ART-STVG is designed as a streaming video processing method incorporated by spatial and temporal memories to predict the spatio-temporal localization results frame by frame. Comparisons and experiments validate the superiority of the proposed benchmark and framework over prior ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation of the proposed task and the model design is intuitively reasonable and technically sound.\n\n2.LF-STVG is a practical and promising new direction that opens up new opportunities in localization-oriented multimodal video understanding area.\n\n3.Extensive experiments verify the drawbacks of previous SF-STVG methods in handling long video scenarios and the proposed method provides a strong baseline in long-form STVG."}, "weaknesses": {"value": "1.One of my biggest concerns regarding this work is the lack of the tailored training data for LF-STVG. As mentioned in the manuscript, the authors extended the validation set of a short-form STVG dataset HCSTVG-v2 to several minutes, and the experimental results are reported for models trained on existing short-form STVG data. This limitation decreases the contribution of this work as I think LF-STVG is a very challenging task and needs tailored training data to facilitate its development in the community.\n\n2.As the authors claimed that they proposed a new streaming video processing paradigm for LF-STVG, it would be better to compare the proposed method with some streaming video understanding methods to make the paper's contributions clearer and more convincing.\n\n3.There are some unclear expressions regarding the model details. For example, the shapes of the spatial and temporal memories are missing and the update mechanism of these memories also lack clear clarifications."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ayRFQh35eZ", "forum": "IdJakw2jta", "replyto": "IdJakw2jta", "signatures": ["ICLR.cc/2026/Conference/Submission2705/Reviewer_K7x2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2705/Reviewer_K7x2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996301209, "cdate": 1761996301209, "tmdate": 1762916336486, "mdate": 1762916336486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}