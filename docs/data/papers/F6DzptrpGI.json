{"id": "F6DzptrpGI", "number": 21952, "cdate": 1758324031206, "mdate": 1759896894513, "content": {"title": "At the Edge of Understanding: Sparse Autoencoders Trace The Limits of Transformer Generalization", "abstract": "Pre-trained transformers have demonstrated remarkable generalization abilities, at times extending beyond the scope of their training data. Yet, real-world deployments often face unexpected or adversarial data that diverges from training data distributions. Without explicit mechanisms for handling such shifts, model reliability and safety degrade, urging more disciplined  study of out-of-distribution (OOD) settings for transformers. By systematic experiments, we present a mechanistic framework for delineating the precise contours of transformer model robustness. We find that OOD inputs, including subtle typos and jailbreak prompts, drive language models to operate on an increased number of fallacious concepts in their internals. We leverage this device to quantify and understand the degree of distributional shift in prompts, enabling a mechanistically grounded fine-tuning strategy to robustify LLMs. Expanding the very notion of OOD from input data to a model’s private computational processes—a new transformer diagnostic at inference time—is a critical step toward making AI systems safe for deployment across science, business, and government.", "tldr": "", "keywords": ["robustness", "generalization", "sparse autoencoders", "transformers"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2e9b18fcb77d374fbcfb2bfe98b985697ca0e576.pdf", "supplementary_material": "/attachment/128242ead7192c96e010fa6fbc686bebc5567d83.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a framework based on SAE to analyze the generalization of LLMs to OOD inputs. The authors first analyze how OOD inputs can lead to unusual features or spurious concepts within the model, which can be observed using SAE tools. They then experimentally demonstrate that typo inputs degrade model performance and that SAE-derived indicators can capture input distribution shifts. Using SAE in the jailbreak scenario, the authors observe a similar distribution shift. Finally, they demonstrate that fine-tuning using SAE-derived indicators improves the model's robustness to typo inputs and jailbreak attacks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work is comprehensive. It includes experiments on synthetic tasks, fine-tuning, and application to more realistic jailbreak scenarios, fully demonstrating the authors' claims.\n2. At least in my opinion, using the SAE indicators to observe the input distribution shift and fine-tune the model based on that is novel.\n3. The results on models of varying sizes and different tasks make their conclusions have certain practical value."}, "weaknesses": {"value": "1. Figure references are unclear. Figure 1 and Figure 2 lack necessary references. Figure 4A has an incorrect reference.\n2. The conclusions in Sections 4.1 and 4.2 are not surprising. It seems intuitive that typo inputs affect model performance and have a positive correlation.\n3. The experimental design in Section 5 is reasonable, but the experiments are conducted on GPT2. Fine-tuning on a larger model, such as llama, can better support the conclusions."}, "questions": {"value": "1. Have you tried fine-tuning on a larger model (e.g. llama) to support the conclusions in Section 5, even using LoRA? Plotting a figure similar to Figure 2a before and after fine-tuning would be a good way to support the conclusions.\n2. The authors study character-level tokenization, which is not the mainstream tokenization granularity of current LLMs. Will it affect the conclusions of the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wm1vudgHIR", "forum": "F6DzptrpGI", "replyto": "F6DzptrpGI", "signatures": ["ICLR.cc/2026/Conference/Submission21952/Reviewer_uCpa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21952/Reviewer_uCpa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761147684758, "cdate": 1761147684758, "tmdate": 1762941994452, "mdate": 1762941994452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on using mechanistic interpretability method to detect OOD data/model jailbreak attempts. \n\n1. The study proposes using sparse autoencoders (SAEs) as a mechanistic framework to analyze transformer residual stream activations. It finds that out-of-distribution (OOD) inputs, such as subtle typos and adversarial prompts, cause the model to activate a significantly larger number of spurious internal \"concepts\" compared to in-distribution data.\n2. This internal off-manifold activation is directly correlated with degraded external performance. Introducing typos into MMLU benchmark questions caused significant accuracy drops in models ranging from Llama 3.1 8B to GPT-5-thinking-nano .\n3. The framework is applied to make models more robust: an SAE-derived \"energy score\" is used to select the most OOD samples for more efficient fine-tuning, and the method identifies successful jailbreaks as OOD (activating more exclusive features). A targeted LoRA fine-tuning subsequently aligned these internal activations, reducing the jailbreak success rate.\n\nWhile the paper presents empirically effective results, it lacks a comprehensive scientific framework that explains how these SAE-based methods can be applied by other practitioners and generalized beyond the specific models and data used in the study."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The typo-based experimental design for the toy-problem is well-crafted.\n2. SAE-based methods demonstrate their ability to detect out-of-distribution data and identify jailbreak attempts. \n3. The proposed methods for enhancing fine-tuning efficiency and mitigating jailbreaks are highly effective."}, "weaknesses": {"value": "1. The paper lacks effective guidelines for utilizing raw energy scores or the number of activated concepts to determine if data is out of domain. Instead, it relies solely on simple comparisons. Statistical methods and guidelines, such as the number of standard deviations from the mean, would be more easily applicable.\n2. The paper explores the application of SAEs at various network layers, but it lacks sufficient guidelines for selecting a specific layer when applying the method.\n3. Limitations are not explicitly mentioned."}, "questions": {"value": "1. What are the main limitations of the work?\n2. Can you provide guidelines for using the energy score and the number of activated features to identify jailbreaks?\n3. Could you please provide your default recommendation for the layer to use when applying your method? Additionally, could you explain the rationale behind this choice?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper doesn’t provide any information about the use of LLMs for writing. It’s unclear whether all papers must include such statements, but I thought it was worth mentioning. If this isn’t an issue, please disregard this concern."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cQ1pAGii1r", "forum": "F6DzptrpGI", "replyto": "F6DzptrpGI", "signatures": ["ICLR.cc/2026/Conference/Submission21952/Reviewer_mKBM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21952/Reviewer_mKBM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579688673, "cdate": 1761579688673, "tmdate": 1762941993822, "mdate": 1762941993822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is a mechanistic study on the ability of LLMs to generalise to OOD, with a specific focus on controllable generation of datapoints and an emphasis on lexical over semantic perturbations. The methodology operates under the manifold hypothesis, which allows for the definition of intuitive metrics to explain robustness. The topic and thesis are not strictly novel: work on transformer robustness, especially to typos, has existed since at least 2020 with the same takeaways. However, this type of work has (surprisingly) not been explored in LLMs. This contribution, added to the robust, systematic approach to experimentation and solutions proposed, makes this paper a strong contribution to the field."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "In my opinion, this paper is very strong. It is thorough, well-written, and has a good, robust, set of experiments. All around, save for a few comments (in 'Weaknesses'), it makes a good contribution to the study of LLM generalisation. Here are a few of the more salient things of this work:\n1. A large set of experiments examining the central thesis of the work _without_ overlaps (i.e., a main evaluation and a set of complete ablation studies). \n2. Use of open/closed models with appropriate versioning and disclosures/releases. Also use of non-pretrained and pretrained models alike.\n3. Experiments have a rigorous backing and well-defined metrics. In particular, the authors display a good understanding of the behaviours and particularities of SAEs / OOD data generation and use these to their advantage to provide more robust results.\n4. Well-written paper, with some minor comments (below). In particular, it clearly states contributions, framing, and limitations.\n5. Contributions go beyond pointing out issues with LLMs, and show the applicability of SAEs for robustness improvement, especially on jailbreaks."}, "weaknesses": {"value": "I have a few minor comments.\n\n1. Better separate results from discussion. Every experiment (and sub-experiment) has results and then discussion/conclusion attached. However, they become conflated. E.g., L369 is an interpretation of the results from Sec 5 _and_ Sec 4. But Sec 5 already is working on assumptions drawn from the interpretation of the results from Sec 4 (L306 -- 'The results in Section 4 present evidence...'). The (interpretation) of the results from Sec 4 is given in both 4.1 and 4.2. This structure could be misleading and does raise questions on the rigour (i.e., objectivity) of the interpretation of the experiments.\n    - This is _especially_ important since the approach to the study of LLMs is a mechanistic analysis. In other words, the work implicitly relies on causal relationships between the hypotheses and the data. Thus, clearly separating what it is observed from what follows (as an interpretation), is necessary to showcase sufficient rigour. \n    - As an example of this: the motivation of Sec 5 follows from Sec 4, but there are likely alternative explanations to the observations from 4.1 and 4.2 (see the questions below).\n2. Reproducibility is definitely there, but I would suggest also adding the versions (or dates) for the calls to proprietary LLMs. They tend to be updated quite frequently.\n3. Formality of the writing needs to improve in some (very minor!) areas: namely, while English morphology is quite flexible, I would argue that 'robustify' is a bit less formal than desired for a scientific publication. Another example would be L412: the '=' sign inline for text shouldn't be used here (this isn't notes, it is a final, publication-worthy work).\n4. Also very minor: it's 'GPT-2', not 'GPT2'. Watch out for the citation in L142"}, "questions": {"value": "MMLU is known to be contaminated, especially in Llama / GPT models. How much do you think this influenced the observations from 4.2, and their relationship to 4.1, where it is argued that TinyStories is not memorised? The conclusion from 4.2 is that 'reasoning' is impacted by typos, but couldn't also be simply by less effective string matching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KJIrZsMxh", "forum": "F6DzptrpGI", "replyto": "F6DzptrpGI", "signatures": ["ICLR.cc/2026/Conference/Submission21952/Reviewer_4cJN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21952/Reviewer_4cJN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649823554, "cdate": 1761649823554, "tmdate": 1762941993497, "mdate": 1762941993497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how sparse autoencoders (SAEs) can be used to identify and understand out-of-distribution (OOD) behavior in transformers. The authors showed that OOD inputs (including typos and jailbreak prompts) cause LLMs to generate spurious concepts in their internal representations, which can be detected by sparse autoencoders. They further extended this observation to achieve the following tasks: (1) quantifying distributional shift, (2) designing more efficient fine-tuning by selecting high-value OOD samples, and (3) defending against jailbreak attacks through SAE-informed alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem being investigated is critical. Effectiveness within the constraint of OOD has been a long but unresolved problem.\n\n2. The application of SAE is novel and interesting. SAE is used to diagnose OOD behavior in a transformer's interior, which is an extension to traditional OOD detecting method.\n\n3. The empirical study is extensive and convincing. Experiments span over multiple scales, modalities (vision and text), and model levels (toy models to frontier models).\n\n4. The energy score metric is novel and effectively combines reconstruction error and spurious concept activation."}, "weaknesses": {"value": "The energy score formula could be better motivated before introduction. This metric, also effectively from the empirical lens, seems ad hoc. Additional explanations on this metric would be very much appreciated."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Jur11zKeLw", "forum": "F6DzptrpGI", "replyto": "F6DzptrpGI", "signatures": ["ICLR.cc/2026/Conference/Submission21952/Reviewer_Se2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21952/Reviewer_Se2j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958552725, "cdate": 1761958552725, "tmdate": 1762941993251, "mdate": 1762941993251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors aim to analyze the OOD behaviour in language models using sparse autoencoders. For this, they train sparse autoencoders on a trained model and then corrupt the input samples using typos and test the features that gets activated in SAEs. They find that on using such perturbations, several features get activated as compared to using in-distribution samples without corruptions. They further analyze that using these corruptions the MMLU performance of llms is degraded and hypothesize the presence of large number of features getting activated in SAEs being the cause for that. They further fine-tune the models on corrupted input samples and observe that this reduces the activations in SAEs, demonstrating that the larger number of SAE activations are closely related with OOD detection. They further use this to detect jailbreaks in the Llama-3.1 8B model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Using SAE to perform OOD detection is an interesting direction. This can help in detecting some adversarial samples as well."}, "weaknesses": {"value": "* The experiments performed in this paper define corruptions in the input tokens as OOD samples. However, this is a very small subset of possible OOD samples. Therefore, authors would need to scale up their experiments and evaluations to demonstrate that SAEs can really help with OOD detection. \n* SAEs are known to be less reliable as training them on different samples can lead to extraction of different features [1,2 ]. One way to resolve this issue to some extent could be to train the SAEs on a very large number of samples. However, in these cases as well it might be possible to craft some adversarial attacks which can evade being detected by SAEs.\n\n[1] Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry (https://arxiv.org/abs/2503.01822)\n\n[2] Analyzing (In)Abilities of SAEs via Formal Languages (https://arxiv.org/abs/2410.11767)"}, "questions": {"value": "It would be great if the authors can perform more extensive evaluation to demonstrate the robustness of their claims. For example, they can analyze different kinds of OOD samples instead of mere input corruptions. They can include change in format or style of the samples as compared to the ones used for training SAEs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ymT0wZaEJ0", "forum": "F6DzptrpGI", "replyto": "F6DzptrpGI", "signatures": ["ICLR.cc/2026/Conference/Submission21952/Reviewer_Vfth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21952/Reviewer_Vfth"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21952/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762229868294, "cdate": 1762229868294, "tmdate": 1762941992916, "mdate": 1762941992916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}