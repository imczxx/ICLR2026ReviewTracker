{"id": "D0u0glT060", "number": 3159, "cdate": 1757344477315, "mdate": 1759898105337, "content": {"title": "Deconstructing Positional Information: From Attention Logits to Training Biases", "abstract": "Positional encodings, a mechanism for incorporating sequential information into the Transformer model, are central to contemporary research on neural architectures. Previous work has largely focused on understanding their function through the principle of distance attenuation, where proximity dictates influence. However, the interaction between positional and semantic information remains insufficiently explored, and the complexity of mainstream corpora hinders systematic, comparative studies of these methods. This paper addresses these challenges through a deconstruction of the attention-logit computation and a structured analysis of all mainstream positional encodings. A key focus is placed on Rotary Positional Embedding (RoPE), whose product-based structure uniquely facilitates a direct interaction between position and content. To probe this characteristic, we designed a novel synthetic task that explicitly demands a strong synthesis of positional and semantic information. As theoretically predicted, RoPE demonstrates a significant performance advantage over other encodings on this specialized task. Concurrently, this targeted evaluation uncovers an implicit training issue: a hidden bias manifesting as a distinct information aggregation phenomenon in the model's shallow layers, which we term the \"single-head deposit pattern.\" Through subsequent ablation studies, we analyze this pattern and identify a method for its mitigation. These findings highlight the need for a deeper investigation into the training dynamics of positional encodings to bridge the gap between their theoretical design and practical implementation.", "tldr": "We propose a unifying perspective on the role of positional encoding and discover that rope training exhibits implicit biases.", "keywords": ["Position Encoding; Toeplitz Matrix; Attention Logit."], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7a5d1fa493221f51dbf861c8dbe97d68dcec4d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to understand the role of positional encodings by both empirical investigations and a theoretical framework based on Toeplitz modeling. Particularly, the paper contrasts additive and multiplicative encodings, using experiments and analysis to illustrate their distinct effects on model accuracy across different synthetic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very well written and organized, which has a clear motivation.\n2. The problem studied in this paper--the role of positional encodings--is of fundamental importance in the field, and this paper does a good job of filling the gap between practical success and limited theoretical understandings. In addition, this paper leverages intuitive numerical examples to illustrate theoretical results and conjectures, which makes this paper very easy to follow.\n3. The “single-head deposit pattern” is an interesting and reproducible phenomenon, providing a plausible explanation for the gap between RoPE’s theoretical advantages and its inconsistent empirical performance."}, "weaknesses": {"value": "1. [Assumption 1] This assumption--the additive decomposition of each token representation--seems a bit strong, as it rules out any interaction between content and position components. Although there is a remark that connects this assumption to existing literature, I was wondering if this assumption can be (approximately) justified via experimental/simulated results. It would be very helpful if the authors can elaborate more on this part.\n\n\n2. Overall, the writing in Section 3.2 is a bit rushy, and similar to the previous question, the remark below Assumption 2 is also not detailed and it would be better to elaborate on how sinusoidal absolute PE meets this assumption and why it could be (approximately) representative for a certain class of PEs.\nIn addition, it would be easier to understand the role of assumptions if there could be a simplified/informal result in Appendix B on how RoPE can be converted to Toeplitz matrices under Assumptions 1-2.\n\n3. Theorem 6.1 holds under Assumptions (A1-3), and I was wondering if Theorem 6.1 is novel beyond showing exponential amplification under generic Jacobian stability assumptions. Additionally, I am curious if the desired property of RoPE under these Assumptions would suggest new designs of PEs.\n\n4. Can the deposit pattern or its variants also be observed in other tasks? Also, based on the analysis in this paper on the favorable scenarios for RoPE, are there any intuitions behind which scenarios would, instead, be favorable for additive PEs, which would be very helpful to add to the discussion of this paper."}, "questions": {"value": "Please see the section Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aLW2l6rnIT", "forum": "D0u0glT060", "replyto": "D0u0glT060", "signatures": ["ICLR.cc/2026/Conference/Submission3159/Reviewer_sR2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3159/Reviewer_sR2j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760803076489, "cdate": 1760803076489, "tmdate": 1762916579606, "mdate": 1762916579606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work offers a unified framework on how positional signals are represented and processed in transformers using different positional encoding methods. The proposed framework classifies a range of methods by the way in which positional information interact with content information and contribute to attention logits. The relative merit and failures of the methods are shown through two synthetic tasks. The authors also find that RoPE results in high head specialization in positional information , and present detailed and theoretical analysis to understand this phenomenon."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper offers a set of important intuitions on the mechanisms and relative advantages between different positional encoding methods.\n- The discovery of the strong head specialization in RoPE is novel and interesting, and the accompanied analyses on the factors leading to this phenomenon are quite illuminating."}, "weaknesses": {"value": "- As the empirical experiments are performed in a synthetic setting, whether similar results generalize to a language modeling setting remains unclear. The paper would be greatly enhanced by some ablation analysis on whether similar head specialization are observed in transformers trained under language modeling with RoPE (e.g. changes in on some position-sensitive tasks), or at least, some discussions on the implications of these results for larger models."}, "questions": {"value": "I'd be curious to hear the authors intuition on whether such strong concentration of positional processing in RoPE would show up in larger models trained to model language. Since in language modeling, supposedly models are trained under much more diverse objectives, would there be more competition for a very local representation for position?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J92SJe60pA", "forum": "D0u0glT060", "replyto": "D0u0glT060", "signatures": ["ICLR.cc/2026/Conference/Submission3159/Reviewer_qxxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3159/Reviewer_qxxu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605854068, "cdate": 1761605854068, "tmdate": 1762916579423, "mdate": 1762916579423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified Toeplitz-based framework for analyzing how positional encoding schemes influence the composition of the attention matrix in Transformers, distinguishing the distinct effects of additive and multiplicative mechanisms. It shows that multiplicative encodings, such as RoPE, modulate the QK interactions between content and positional factors of model representations through a unified multiplicative term that incorporates relative positional information, thereby inducing stronger content–position interactions. Synthetic tasks reveal that RoPE excels at position-sensitive reasoning but is outperformed by certain additive embedding methods in tasks where positional information is inconsequential. Furthermore, RoPE is found to develop a “single-head deposit pattern,” in which one attention head monopolizes positional computation. Through causal ablations and hybrid designs, the authors demonstrate that this specialization is an intrinsic property of RoPE’s multiplicative structure and relate it to redundant attention heads and new attention designs such as multi-latent attention."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides clear insights by formalizing the mechanistic distinction between additive and multiplicative approaches to incorporating and modulating interactions between positional and content information in model representations, particularly in relation to relative positional encoding. Its controlled experimental settings effectively demonstrate the various idiosyncrasies of RoPE as consequences of its multiplicative interaction mechanism, most notably the single-head deposit phenomenon. The experiments in Section 5 examining variations of this behavior under different input and structural perturbations are especially thought-provoking, aligning closely with the authors’ intuitive hypotheses and theoretical characterizations. Together, these results convincingly illustrate how RoPE’s multiplicative structure both enables strong positional reasoning and induces positional over-specialization.\n\n2. The paper further strengthens its empirical observations with a theoretically rigorous proof that explains why positional specialization emerges (at least during the early stages of training). This theoretical grounding provides valuable causal insight, transforming the observed empirical regularity into a predictable and interpretable property of RoPE’s multiplicative dynamics."}, "weaknesses": {"value": "1. The second synthetic task the authors design (trigger word counting) seems rather off the point. It essentially tests to what extent different position encoding methods can suppress themselves and perform the “no-op” behavior, which is not related to the authors' characterization of the particular attention matrix component to which the relative positional information is injected via the Toeplitz formulation. From my perspective, the second task should be content-agnostic instead of position-agnostic (or more specifically, it should be agnostic to the interplay of content and relative position), which would allow for a more interesting comparison between the additive and multiplicative PE methods.\n\n2. The setting of the paper is rather limited. While it claims to “deconstruct positional information,” it actually only focuses on how multiplicative PE methods better express the interplay between relative positional information and content and shows that training on the highly specialized relative distance classification task, which exactly captures this correlation, would lead to the single-head deposit phenomenon. It is highly debatable whether such tasks exist in the pretraining corpus of LLMs at all, and the paper also does not discuss the settings of training on multiple tasks (not even the two synthetic tasks proposed), which is the usual practice of training LLMs. Furthermore, the paper does not discuss the mechanistic significance of ROPE and other PE methods in accounting for some truly intriguing LLM phenomena that concern the joint utilization of relative positional information and semantic content in more generalized settings. Notable examples include in-context learning (particularly recency bias, which means that the last demonstration's label is more likely to be predicted for the query), chain-of-thought, and in-context information retrieval (needle-in-a-haystack and such). The lack of discussion of the implications of the experimental findings for characterizing those LLM behaviors limits the significance of the findings.\n\n3. The citation format used in the paper appears problematic in various places, where the author names and publication years of the cited papers appear side by side with the main text without being enclosed in parentheses."}, "questions": {"value": "1. Could you explain how to deduce from the Toeplitz formulation of the RoPE attention matrix in (2) that “by routing the\nlearning of positional dependencies through this multiplicative kernel, RoPE may create a strong\ninductive bias that encourages positional specialization, concentrating this logic into a small subset\nof attention heads”? For me, it seems impossible to conclude that the $G_{e}$ term will be subsumed by a subset of heads *a priori* solely from the form of (2).\n\n2. Could you explain the surprisingly strong performance of absolute PE in your two synthetic experiments? While they are all additive PE methods that should behave similarly according to your Toeplitz formulation, in Task 1 it significantly outperforms relative PE, and in Task 2 it significantly outperforms ALiBi. This appears counterintuitive to me, since absolute PE is as rigid and inflexible as ALiBi, given that both are not learnable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0oIxPJrWnL", "forum": "D0u0glT060", "replyto": "D0u0glT060", "signatures": ["ICLR.cc/2026/Conference/Submission3159/Reviewer_7AJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3159/Reviewer_7AJz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883601447, "cdate": 1761883601447, "tmdate": 1762916579027, "mdate": 1762916579027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Position Encoding (PE) plays a critical role in LLMs, different PE methods have been proposed, fixed or learnable, additive or multiplicative. However, the understanding of those PE methods are still under exploration, and their efficacies are normally tested on aggregate metrics. In this study, a new framework is proposed to analyze PE more systematically.\n\nTwo simple and isolated tasks are proposed, position-sensitive and position-agnostic tasks. These two tasks can better test if PE is needed or which PE methods can deliver a better result. Based on the tasks, this study tries to analyze the key reason behind those PE methods using Toeplitz matrices. This Toeplitz framework can group those PE methods into additive and multiplicative, showing the theory behind why multiplicative PE can better inject position information. Finally, this study also shows only a few heads focus on the multiplicative PE, denoted as the deposit pattern.\n\nGenerally, this study proposes a new framework to study and compare the previous PE methods. A new finding is presented on the multiplicative PE, RoPE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PE is used in almost all LLMs (except NoPE variants), therefore this is an important topic to analyze for downstream studies. \n\n2. The framework proposed in this study can be used to investigate those widely used PE methods. Thus the finding in this paper is applicable to a wide range of LLM studies.\n\n3. The tasks used to analyze PE are simple, and the simplicity is also very important for XAI. The proposed position-dependent and position-agnostic tasks can directly isolate the effects of each PE; Thus the tasks could be a more clear metric than those aggregated ones.\n\n4. It seems technically sound using Toeplitz matrices to distinguish the additive and the multiplicative PEs. This framework can theoretically analyze the differences between the two types of PEs.\n5. A series of ablation studies are included to further explore the new finding on the multiplicative PE, RoPE.\n\nThere exist XAI studies which combine different metric tasks and analysis steps to investigate a phenomenon. However, they often lead to less clear conclusions when stitching something without a good reason. The greatest strength of the proposed framework is the simplicity, the tasks are directly designed on position information."}, "weaknesses": {"value": "Overall this study seems novel and interesting. However, the presentation of the current draft could be improved. This study aims to organize multiple things, absolute PE, Relative PE, RoPE, Alibi, fixed PE, learnable PE, additive PE, multiplicative PE. The current presentation seems not clear enough to organize all of those, especially in Fig 1. It took me several “hops” to combine those pieces, and what Toeplitz matrices are used for, but I could be wrong or miss something. \n\nAnother weakness is the explanation behind those PEs based on the two tasks. The discussion on the reason seems less solid. I will list all of my questions in detail below.\n\nDespite the weaknesses, I still believe this study is intriguing, opening a new door to this topic. And I am not expecting one study to explore the whole universe behind the door."}, "questions": {"value": "Following the weakness of clarity, here is a list of questions regarding the paper:\n1. Assumption 3.1 and in Fig 1, x = c + p, should I consider this a conceptual idea that the input can be disentangled into content and position so that we can study separately? Or this is a more strict formula that the Topelitz analysis is based on. For the latter case, does the Gram(c+p) equation also apply to ALibi, where the merge happens after taking the matrix product? (I might miss the proof for this part in the literature.)\n2. Based on the experiment and the conclusion, the proposed Toeplitz-based framework can distinguish how additive and multiplicative PEs work. It does not matter if the additive is fixed or learned, absolute or relative, they all can be summarized by Eq(1). (This is my understanding, please correct me if I am wrong). If my understanding is correct, the introduction and Sec3.1 could be a little confusing. It seems this study tries to cover or touch every aspect of PE, but it is less clear what the proposed Toeplitz framework actually solves in the context. Based on Fig 1, it seems RoPE and Relative PE are the two components for comparison, but my understanding from the experiment is the two modes should be “Additive” vs “Multiplicative”(RoPE)? And for additive PE, the mode includes both relative and absolute. If my understanding is right, it could be better to re-organize the figure and Sec3.1 for clarity.\n\n3. Following the above question, would this Toeplitz framework categorize “AliBi” to additive or an exception? Based on Fig 1, “AliBi” seems an exception to the “Relative PE”(additive to me), (Eq1 cannot explain “AliBi”??), then how is “AliBi” processed or captured by the framework? I can find a bunch of experiments for “AliBI”, but I cannot find a detailed explanation of “AliBi” based on the proposed framework. This question could be related to Q1, handling the addition before or after Gram computation.\n\n4. A naive question for Eq(1), G(qp, kp) is always Toeplitz for all additive PEs?(did I miss border cases?), what is the main purpose of the explicit bias term B?\n\n5. The de-couping framework, Eq(1) and Eq(2), demonstrates how a PE is involved with the content. Please help me understand how Toeplitz matrices play a key role in the context?\n\n6. It is interesting to see that how “Alibi” work on the position-sensitive task, Fig 2, line 269, “Methods like ALiBi, with its fixed and data-agnostic Toeplitz bias”, Is the reason behind “AliBi” due to fixed and data agnostic? Does the “absolute PE” also inherit those properties? \n\n7. Regarding “SINGLE-HEAD DEPOSIT””, this seemed an important finding by the framework. However, this finding could be over extrapolated. Given the position-sensitive task can be inferred mainly on position information, and RoPE can better inject this discriminant feature(position information) based on Fig 2. Then the explored question in Sec 4.3 is essentially finding, if there is an attention head that can capture the injected feature(position information). Again, the injected feature is a discriminant shortcut for the model to reach the correct prediction. And it is not surprising that different heads could capture different input features, and networks tend to lazily rely on easy features. Therefore, NoPE on Task 1 does not offer that feature to capture, and the feature from RoPE becomes not discriminant for Task 2 in Fig 2. This explanation could be a bummer to the finding, or please correct me if the above understanding is wrong.\n\nAlthough many points remain unclear in the draft, I still appreciate the effort the authors have made and I have learned something new from this study."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1cTd4Na1XP", "forum": "D0u0glT060", "replyto": "D0u0glT060", "signatures": ["ICLR.cc/2026/Conference/Submission3159/Reviewer_TXbo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3159/Reviewer_TXbo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013666786, "cdate": 1762013666786, "tmdate": 1762916578709, "mdate": 1762916578709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel analytical and empirical framework for understanding positional encodings (PEs) in Transformers, with a focus on the difference between additive and multiplicative (e.g., RoPE) mechanisms. The authors use a Toeplitz-matrix formulation to characterize translation invariance in positional interactions, and both predict and empirically demonstrate that multiplicative PEs (especially RoPE) produce a distinctive single-head deposit pattern, where positional reasoning becomes localized to a single head in early layers.\n\nTo explore this, the paper introduces two synthetic tasks—a position-sensitive task (relative distance classification) and a position-agnostic task (trigger counting)—and performs causal ablation studies to confirm that RoPE’s multiplicative nature induces strong specialization. The authors further provide theoretical proofs (based on gradient dynamics and spectral analysis) showing that this phenomenon arises inevitably from multiplicative coupling, and argue that DeepSeek's MLA (Multi-Latent Attention) architecture mitigates over-specialization.\n\nThis is a very good paper that would be probably be suitable for ICLR in its current form. However, I do have some questions for the authors for my own understanding. Additionally, there are several points that could be improved to make this a strong contender for spotlight or oral presentation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Relevant topic with a clear gap in the literature, whereby the differences across different strategies for positional encoding are poorly understood, making application to new tasks ad-hoc and reliant upon hyperparameter tuning, rather than theoretical insight. This paper aims to provide theoretical insight into a very clear and directly stated question: \"how, precisely, do different PE schemes mediate the interaction between token content and position). The answers are interesting in their own right and may be useful for ML practitioners moving forward. \n- The paper is very clearly written, integrating both theoretical analysis and careful synthetic experimental analyses, along with appropriate ablations. The structure of the paper is easy to follow and guides the reader along a compelling journey. \n- The framework via toeplitz matrices is clever and appreciated. \n- The author's conclusions are strongly supported by their analyses, in nearly every case (see minor exceptions below). Preliminary answers to the motivating questions are provided. Clear follow-ups to this paper can be executed in future work."}, "weaknesses": {"value": "- It wasn't clear to me why the author's posit that \"intense structurally-induced specialization is a primary cause of the gap between RoPE's theoretical pmroise and its practical performance.\" This theme comes up many times (e.g. line 387 \"the deposit pattern, while effective, is an inefficient use of model capacity\"), and I never felt the author's fully explained their rationale. Modularity is often praised for its benefits, in contrast to this position. \n- Some aspects of prior literature were explained so coarsely to have impossible for an unfamiliar reader to follow, e.g., the mention of \"massive value\" pehenomena, rotation artifacts, T5, AliBi, etc. With the additional page in the revised version, I suggest expanding relevant descriptions.\n- Assumption 3.1 (linear separability of content and position components) is questionable. The authors motivate it only with absolute PE and NoPE, but do not discuss its validity for multiplicative mechanisms, which seems more fraught. \n- For Task 1, it wasn't clear why a classification approach was used vs. the more natural regression framing. \n- 4.2: the authors suggest that their framework predicts that RoPe will exceed in the position sensitive task. This wasn't immediately obvious to me. Can the authors formalize their predictions for different models based on the framework?\n- Figure 4, and other related plots: I don't think the violin plots are the right choice of visualization, personally. The point is not about the shape of the distribution (which is quite small for a violin plot anyways, at 16 samples), but rather, the presence of a particular outlier. I would prefer to see individual data points, possibly in addition to the violin plots already plotted. This is currently plotted for the first layer on the left. However, the recommended approach would do this for each layer and remove the need for the plot on left. In any case, the plot on left would be more effectively visualized with heads sorted by accuracy, since the x-axis is meaningless currently. \n- The authors claim that Figure 6 validates their prediction of only 1 head being necessary. Clearly, it is 2 heads. This may be because this is applied across all layers, where the single-head deposit pattern is restricted to earlier layers, but this should be explained more accurately. \n- Unclear whether the benefit for MLA is due to larger parameter count (three matrices in Q,K projections vs. two in RoPE), or the hybrid approach."}, "questions": {"value": "- Why weren't learned additive PEs analyzed? Is this because they would not meet assumption 3.2 of Toeplitz structure? \n- Relatedly, I was reading the introduction, I thought the author's might analyze the validity of the Toeplitz structure of positional interactions in models where it is both baked in (baseline), and models where it must be learned. Could the authors discuss this, and if relevant, add some additional analyses in this vein? \n- The authors state that Relative PE learns to attenuate its bias towards zero in Task 2. Can they report empirical numbers to demonstrate this? \n- Section 5.3: why is the U included in the notation for weight matrices? it seems to add nothing? \n- Are MLA and RoPE parameter-matched in 5.3? If not, can you perform an additional parameter-matched experiment to convincingly demonstrate the benefit for MLA? \n- Proposition 6.2: would it be possible to assess the initial advantage empirically and thereby predict the head that develops the deposit pattern?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KE0SbyfDt5", "forum": "D0u0glT060", "replyto": "D0u0glT060", "signatures": ["ICLR.cc/2026/Conference/Submission3159/Reviewer_hmEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3159/Reviewer_hmEf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3159/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762464720010, "cdate": 1762464720010, "tmdate": 1762916578094, "mdate": 1762916578094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}