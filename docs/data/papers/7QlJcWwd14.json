{"id": "7QlJcWwd14", "number": 2216, "cdate": 1757032468896, "mdate": 1759898162327, "content": {"title": "ReasonAudio: Semantic Reasoning and Temporal Synchrony in Video–Text-to-Audio Generation", "abstract": "The rapid advancement of video-text-to-audio (VT2A) diffusion models has enabled unprecedented audio generation conditioned on video and text, yet two major challenges remain: following complex semantic descriptions and achieving robust audio–visual synchronization. In this work, we propose ReasonAudio, an MLLM-empowered flow-matching generative model with stronger semantic and robust temporal alignment. To enhance semantic understanding, we 1) address the scarcity of semantically rich tri-modal (video–text–audio) annotations by constructing VGGSound-Think, a dataset enriched with acoustic hints and audio–visual relation descriptions, and 2) leverage MLLMs to understand multimodal conditions (video and text) by introducing learnable queries that bridge understanding and generation components. To tackle temporal alignment, we employ preference optimization (Flow-DPO, Flow-RWR) with synchronization feedback, aligning generative models with visual synchrony preferences. Extensive experiments demonstrate that ReasonAudio achieves state-of-the-art performance in VT2A generation, with substantial improvements in both semantic alignment and temporal synchronization. Moreover, evaluations on VGGSound-Think show that our model excels at reasoning over acoustic hints and following descriptions of audio–visual relations (e.g., object interactions and on-/off-screen attribution). The demo page is available at https://ReasonAudio.github.io.", "tldr": "We introduce ReasonAudio, an MLLM-empowered flow-matching model that achieves state-of-the-art VT2A generation for rich semantic reasoning and enhanced temporal alignment.", "keywords": ["Multimodal Large Language Models; Video-to-Audio Generation; Preference Optimization;"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55066588da0f0a1711c491ea8727379f7dad0f9f.pdf", "supplementary_material": "/attachment/26a23737f76f9977f4f4ab39a3c37f9348872f27.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents a new MLLM-empowered framework for video-text-to-audio (VT2A) generation, ReasonAudio. In addition, preference optimization (Flow-DPO or Flow-RWR) is adopted for enhancing temporal alignment between the video and audio modalities. The paper also introduces a dataset enriched with acoustic hints and audio-visual relation descriptions, VGGSound-Think, to address the scarcity of semantically rich tri-modal (video–text–audio) annotations in previous datasets. Experiment results show that ReasonAudio outperforms existing models in terms of semantic alignment and temporal synchronization in the VT2A generation task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Although there is room for improvement in presentation, the paper itself is written well enough to make readers understand the proposed method and the experimental results.\n2. The introduced methods sound reasonable.\n3. The experiments are comprehensive, and the quantitative results are good."}, "weaknesses": {"value": "1. There is a problem regarding the generated samples on https://ReasonAudio.github.io .\n    - [1-a] Some samples generated by ReasonAudio cannot be played (as of October 25th, 2025).\n    - [1-b] I felt the quality of the other generated samples was not as good as the quantitative result.\n2. There are some unclear points for me.\n    - [2-a] How were synchrony preferences obtained?\n    - [2-b] L.256 (Section 4.3) says, \"we randomly drop conditional features.\" Did you drop all conditional features simultaneously? Or, did you drop each conditional feature independently, as in MMAudio and ThinkSound?\n    - [2-c] How was the difference in text format between VGGSound-Think and VGGSound handled? In the VGGSound-Think dataset, which is used for training a model, text descriptions are detailed and have a structure. On the other hand, text descriptions in the VGGSound dataset, which is used for evaluations, are simple and plain. I wonder how the proposed framework deals with this difference."}, "questions": {"value": "- I would appreciate it if the authors could address Weaknesses 2-a, 2-b, and 2-c that I provided above.\n- Additional questions about RoPE.\n  - [3-a] Which type of RoPE is used for noisy audio latent and temporal latent, aligned RoPE (which [Mei et al. (2024)](https://openreview.net/forum?id=HZq8Gakf6e), MMAudio, and ThinkSound adopt) or non-aligned RoPE?\n  - [3-b] Could you elaborate on why RoPE is applied to semantic queries within MMDiT? To my knowledge, one of the most popular MMDiT implementations is not to apply RoPE to text query tokens. [FLUX](https://github.com/black-forest-labs/flux) and MMAudio adopt this implementation. Did you decide the design empirically?\n\nIf the authors' response is convincing, I will raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KOch91kWC3", "forum": "7QlJcWwd14", "replyto": "7QlJcWwd14", "signatures": ["ICLR.cc/2026/Conference/Submission2216/Reviewer_3J1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2216/Reviewer_3J1P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476772296, "cdate": 1761476772296, "tmdate": 1762916147676, "mdate": 1762916147676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a VT2A framework that (i) augments VGGSound with semantically richer, structured captions (“VGGSound-Think”: hints + AV relations), (ii) uses a frozen MLLM with learnable queries to encode video+text conditions, and (iii) applies preference-based post-training (Flow-DPO/Flow-RWR) using a synchronization feedback signal to improve temporal A/V alignment. On VGGSound and AudioCaps, it shows strong semantic alignment (CLAP/IB), competitive fidelity (FD/KL/IS), better DeSync, and improved MOS-Fit."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Semantic reasoning + audio-visual temporal alignment are the two real pinpoints in V+T->A, the proposed method is clear and targets at addressing both problems.\n\n2. Applying DPO/RWR to a flow model for temporal alignment is interesting; the β ablation and SFT/RWR/DPO comparison are useful.\n\n3. The proposed dataset are valuable for both training and evaluation, and the data curation pipeline using structured, tri-modal annotations (coarse → fine → AV relations, including on/off-screen) seems robust."}, "weaknesses": {"value": "1. Lack of important citations / comparison against prior works V2A / VT2A, including but not limited to [1] - [6]. In particular, the idea of using MLLM to provide conditional signal (text and video) for audio generation is early explored in [1], and the code is fully open-sourced.\n\n2. The architectural design is lack of novelty, basically following very similar structure to MMAudio. Also, ThinkSound already provides very strong CoT reasoning design in its VT2A, what is the key insight that the proposed methodolgy shows unique advantage over ThinkSound? This also seems not clear. So in general, the novelty of the paper remains an important concern.\n\n3. Some compared methods are video-only (e.g., Diff-Foley, Frieren), while ReasonAudio uses video+text. This can inflate CLAP/IB and DeSync. Provide matched “video-only” ablations of ReasonAudio and a “text-only” mode for apples-to-apples.\n\n4. Where is the details of human evaluation? No report of rater count, items per rater, randomization, expert-level of raters. It's important to describe these details to justify the robustness of human eval.\n\n5. Synchrony reward: Precisely how is the synchronization feedback computed? If from Synchformer DeSync, how do you prevent reward hacking and distributional overfitting? Any human-labeled preference data? If it is human-labeled preference data, how sensitive or accurate can human detect the synchronization, is the data quality guaranteed, any auditing procedure applied to ensure it?\n\n[1] Tell What You Hear From What You See - Video to Audio Generation Through Text, NeurIPS 2024.\n\n[2] From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation, ICML 2024.\n\n[3] Foleygen: Visually-guided audio generation, 2024.\n\n[4] Video-Guided Foley Sound Generation with Multimodal Controls, CVPR 2024.\n\n[5] SonicVisionLM: Playing Sound with Vision Language Models, CVPR 2024.\n\n[6] Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8GKerj1w4M", "forum": "7QlJcWwd14", "replyto": "7QlJcWwd14", "signatures": ["ICLR.cc/2026/Conference/Submission2216/Reviewer_D3zi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2216/Reviewer_D3zi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775373644, "cdate": 1761775373644, "tmdate": 1762916146738, "mdate": 1762916146738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ReasonAudio, a multimodal reasoning framework for video–text-to-audio (VT2A) generation. It integrates a frozen MLLM (Qwen-VL-2.5) with learnable semantic queries, a triple-stream MMDiT architecture (semantic, temporal, audio), and a flow-matching generation backbone. A new dataset, VGGSound-Think, is introduced, featuring structured annotations with `<HINT>` and `<TRACK>` elements to enhance semantic and temporal alignment.\n Preference-based training (Flow-DPO / Flow-RWR) is employed to improve synchrony (DeSync) without heavily sacrificing audio fidelity. The system achieves strong results on VGGSound and AudioCaps benchmarks, showing promising capabilities in semantic reasoning and synchronization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear and polished writing.**\n    The paper is well-structured and easy to follow. The motivation, methodology, and experimental setup are clearly connected, and the figures (especially Fig.1 and Fig.3) effectively communicate the conceptual framework.\n2. **Use of MLLM brings insightful capabilities.**\n    The paper demonstrates that MLLMs contribute to long-range, cross-modal understanding. The model can infer implicit sound sources (e.g., sirens from “calling 911”) and better handle multi-object, on/off-screen semantics compared to CLIP-only or text-only baselines.\n3. **Quantitative and qualitative improvements.**\n    The results on VGGSound and AudioCaps show clear gains in semantic alignment (CLAP/IB) and synchrony (DeSync), confirming the system’s overall effectiveness."}, "weaknesses": {"value": "**1. Dataset construction and contribution clarity**\n\nThe dataset creation process is insufficiently described. The authors state that captions and relationships are generated using **GPT-4o**, but no details are given about how GPT-4o was prompted, filtered, or validated.\n From practical experience, GPT-4o’s **audio and video understanding is limited**, particularly for complex, real-world VGGSound clips involving multiple overlapping sources or off-screen events. Therefore, it is unclear whether this dataset truly advances beyond existing captioning sets like *Sound-VECaps*.\n\nTo justify the claimed data contribution, the authors should:\n\n- Quantitatively verify dataset quality (e.g., inter-annotator consistency, caption diversity, and alignment accuracy);\n- Compare with existing datasets on sound-event richness and relational structure;\n- Provide ablation experiments isolating the effect of dataset vs. model design.\n   At present, improvements may come from model training rather than data innovation.\n\n**2. Choice of MLLM backbone**\n\nThe choice of **Qwen-VL-2.5** instead of **Qwen-2.5-Omni** is questionable. Qwen-VL’s instruction tuning emphasizes **image-related comprehension**, potentially weakening its textual-audio reasoning ability.\n An additional experiment using Qwen-2.5-Omni (with the same size and configuration) would clarify whether the gains arise from multimodal reasoning or just visual prior knowledge. Also, the paper inconsistently refers to both “Qwen-VL-7B-Instruct” and “Qwen-VL-8B”—this should be unified and clarified.\n\n**3. DPO as a contribution**\n\nThe use of **DPO** for preference alignment appears straightforward. The paper itself admits that DPO improves synchronization (DeSync) but slightly degrades FD. Without deeper analysis or novel adaptation, this seems more like an application of existing methodology.\n A more detailed **trade-off analysis**  or a multi-objective optimization variant would strengthen the originality claim.\n\n**4. Experimental transparency and missing baselines**\n\n- Training setup lacks detail, achieving the reported performance from 200k iterations without pretraining seems unlikely.\n- The MLLMs-for-Understanding section omits a CLIP+LLM hybrid baseline, which would clarify the specific contribution of MLLMs beyond visual-text fusion.\n- Comparisons on VGGSound-Think against *DeepSound-V1* and *ThinkSound* are not meaningful, since those baselines were never trained on this data format. Results may simply reflect dataset mismatch.\n- Input formats (<HINT> vs <TRACK>) are not explicitly described per task (VT2A, T2A, reasoning), making replication difficult.\n\n**5. Reasoning validation and robustness**\n\n- Reasoning ability is mostly supported by qualitative samples rather than quantitative verification.\n\n**6. Robustness**\n\n- Additionally, robustness under weak or missing textual inputs (e.g., pure V2A mode) should be tested, as the system’s dependency on structured prompts may limit real-world usability.\n\n**7. Novelty**\n\n- Treating understanding as a preceding stage of generation is intuitively reasonable and valid; however, this stage primarily operates at the prompt level, which does not fundamentally enhance the diffusion model’s intrinsic understanding ability, thereby limiting its originality."}, "questions": {"value": "See Weaknesses\n\nIf the authors provide clearer dataset validation, justify MLLM choice, and strengthen the DPO and reasoning analyses, I would be happy to discuss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Un0Mh4tfpN", "forum": "7QlJcWwd14", "replyto": "7QlJcWwd14", "signatures": ["ICLR.cc/2026/Conference/Submission2216/Reviewer_2MVJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2216/Reviewer_2MVJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969674970, "cdate": 1761969674970, "tmdate": 1762916146095, "mdate": 1762916146095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The core goal of ReasonAudio is to synthesize realistic ambient sounds conditioned on both video and text, achieving superior semantic alignment and temporal synchronization.\n\nKey Contributions: New Reasoning Dataset (VGGSound-Think): A tri-modal (video-text-audio) dataset augmenting $\\text{VGGSound}$ with semantically rich annotations, including acoustic hints and structured audio-visual relation descriptions (e.g., multi-object interactions, on-/off-screen attribution).MLLM-Empowered Semantic Understanding: A novel approach that leverages frozen Multimodal Large Language Models ($\\text{MLLMs}$) and learnable queries to bridge the $\\text{MLLM}$'s understanding component with the flow-matching generative component, avoiding complex multi-stage training.Temporal Alignment via Preference Optimization: Application of preference post-training techniques ($\\text{Flow-DPO}$, $\\text{Flow-RWR}$) using synchronization feedback to align the generative model with visual synchrony preferences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The two key solutions are highly original. The use of learnable queries to integrate a frozen $\\text{MLLM}$ (like $\\text{Qwen2.5-VL-8B}$) directly into the flow-matching pipeline for deep semantic reasoning is a novel simplification over prior multi-stage approaches[cite: 155, 321, 681]. Furthermore, applying preference optimization ($\\text{DPO}/\\text{RWR}$) with synchronization feedback to explicitly boost temporal alignment is a unique and effective strategy for $\\text{VT2A}$\n\nThe construction of VGGSound-Think is a high-quality contribution, focusing on semantically rich, multi-grained annotations essential for reasoning. The ablation study confirms the efficacy of the proposed components: the $\\text{MLLM}$ (vs. $\\text{CLIP}$ or $\\text{LLM}$) significantly improves semantic scores, and $\\text{DPO}$ consistently yields the best temporal alignment ($\\text{DeSync}$). The architecture, based on flow-matching and $\\text{MM-DiT}$, is state-of-the-art.\n\nThe paper clearly articulates the two major challenges ($\\text{semantic}$ and $\\text{temporal}$ gaps) and presents a targeted solution for each. The architecture (Figure 3) and the $\\text{VGGSound-Think}$ annotation steps (Figure 2) are well-illustrated and easy to follow. The mathematical formulations for $\\text{SFT}$, $\\text{RWR}$, and $\\text{DPO}$ loss are clearly presented in the context of flow-matching.\n\nReasonAudio achieves state-of-the-art $\\text{VT2A}$ performance, with notable gains in semantic alignment ($\\text{CLAP}$ and $\\text{IB}$) and a substantial improvement in temporal synchronization ($\\text{DeSync}$). The case studies demonstrate a critical capability: reasoning over acoustic hints and on-/off-screen attribution. This work sets a new benchmark for multimodal audio synthesis by moving beyond shallow text captions to deep, nuanced reasoning."}, "weaknesses": {"value": "Preference Optimization vs. Audio Fidelity Trade-off: The ablation study shows that preference post-training introduces a slight degradation in audio quality (higher $\\text{FD}$)4. Specifically, DPO with $\\beta=5$ provides the best synchrony but increasing $\\beta$ beyond this value causes a \"noticeable drop in audio quality (higher $\\text{FD}$),\" indicating overfitting to the synchrony reward. Suggestion: A deeper investigation into a principled way to balance the temporal synchrony reward against the audio fidelity loss ($\\mathcal{L}_{\\text{FM}}$) is needed, perhaps by making the fidelity term adaptive during post-traini\n\nLimited Exploration of Learnable Query Design: The paper states $\\text{N}=77$ learnable query tokens were used. While the concept is novel, the selection and impact of this hyperparameter ($N$) are not ablated. The choice of $N=77$ (the same length as the CLIP token limit mentioned earlier 6) suggests a potential link that should be justified or explored. Suggestion: The authors should include an ablation study on the number of learnable queries ($N$) to demonstrate its optimal configuration and effect on the MLLM's reasoning capacity and model complexity.\n\nDependence on GPT-4o: The creation of the rich VGGSound-Think dataset heavily relies on $\\text{GPT-4o}$ for Foley Reasoning Caption Generation7. While $\\text{GPT-4o}$ is state-of-the-art, this reliance means the quality and type of reasoning generated (e.g., acoustic hints, off-screen attribution) are fundamentally limited by the capabilities and potential biases of a closed-source model. Suggestion: A brief discussion on the generalizability of $\\text{VGGSound-Think}$ annotations, specifically how performance might change if a different MLLM (e.g., a fully open model) were used for the annotation pipeline, would strengthen the contribution."}, "questions": {"value": "Synchronization Reward Function: The paper mentions using synchronization feedback from $\\text{Synchformer}$ to define the preferences/rewards for $\\text{DPO}$ and $\\text{RWR}$. Could the authors explicitly define the reward function $r(x_0, y)$? Is the reward derived directly from the negative $\\text{DeSync}$ score, or is it a more complex function incorporating $\\text{DeSync}$ score and other metrics (e.g., $\\text{IB}$ score) to reflect overall video-audio fit?\n\nMLLM Inference Latency/Overhead: While the $\\text{MLLM}$ is frozen, generating the semantic conditions still adds computation time compared to simple fixed-feature encoders ($\\text{CLIP}$). Could the authors provide a comparison of the inference latency or computational overhead introduced by the frozen $\\text{MLLM} + \\text{learnable queries}$ pipeline versus the purely $\\text{CLIP}$-based baseline? This is crucial for evaluating the practical efficiency of ReasonAudio.\n\nGeneralization to Novel Concepts: The $\\text{VGGSound-Think}$ set focuses on refining existing $\\text{VGGSound}$ concepts. Does the $\\text{MLLM}$-empowered approach demonstrate superior zero-shot generalization to completely novel video/audio concepts that were not represented in the original $\\text{VGGSound}$ or the original $\\text{Qwen2.5}$ training data? A qualitative example or a metric on the $\\text{Movie Gen Audio Bench}$ dataset that highlights this capability would be highly valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqwe1qkX1s", "forum": "7QlJcWwd14", "replyto": "7QlJcWwd14", "signatures": ["ICLR.cc/2026/Conference/Submission2216/Reviewer_6u3X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2216/Reviewer_6u3X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103277170, "cdate": 1762103277170, "tmdate": 1762916145870, "mdate": 1762916145870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}