{"id": "I2aO05fIEG", "number": 6827, "cdate": 1757997118920, "mdate": 1759897891241, "content": {"title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "abstract": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as \"dead ends\", committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose **REX-RAG** (**R**easoning **EX**ploration with Policy Realignment in **R**etrieval-**A**ugmented **G**eneration), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two symbiotic innovations: **(1) Mixed Sampling Strategy**, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and **(2) Policy Correction Mechanism**, which is essential for correcting the distributional shifts introduced by exploration. REX-RAG demonstrates that effective exploration is only viable when paired with such a rigorous correction. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of **5.1%** on Qwen2.5-3B and **3.6%** on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. Anonymous repository is provided on https://anonymous.4open.science/r/REX-RAG.", "tldr": "We propose REX-RAG, a retrieval-augmented RL framework for LLM reasoning, featuring a Mixed Sampling Strategy to escape dead ends and a Policy Correction Mechanism to correct distribution shift and reduce gradient bias.", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "Reinforcement Learning", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba130b81a79f3c45de19d2cd5c1502f3f8e9d1ba.pdf", "supplementary_material": "/attachment/e60611cf38a24ce091442e57949ca3933c475665.pdf"}, "replies": [{"content": {"summary": {"value": "During reinforcement learning (RL) training, “dead ends” — consistently failing trajectories — often occur. Simple self-reflection does not effectively address this issue, as it tends to only slightly perturb the original path without resolving the underlying problem. To mitigate this, the proposed method introduces several design considerations. First, it employs a mixed sampling strategy, which samples from both the current policy and a probe policy. Second, it includes a policy correction mechanism to account for the distribution shift introduced by exploration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated, clearly identifying a key limitation of existing self-reflection methods and proposing a principled approach to address it.\n\n2. The paper is clearly written and easy to follow, with well-structured explanations of both intuition and methodology."}, "weaknesses": {"value": "1. It is unclear whether the prompts used in “Construction of the Probe Policy” were also used as the prompts for the self-reflection baseline shown in Figure 1. Additionally, does the self-reflection baseline involve training with self-reflected responses, or is it only evaluated with self-reflected responses without training? This distinction is important because the main difference between the proposed mixed sampling strategy and prior self-reflection approaches is not entirely clear. Some previous works also correct answers using self-reflection and incorporate these corrected responses during training. Alternatively, is this work applying self-reflection in parallel to the sampled generations within GRPO? This needs to be clarified in the paper.\n\n2. The experimental section lacks a comparison with existing self-reflection baselines, which are necessary to contextualize the improvements.\n\n3. The ablation study only investigates components within the policy correction mechanism. It should also include an ablation study evaluating the contribution of the mixed sampling strategy."}, "questions": {"value": "1. Why does this work focus solely on the RAG problem? The overall pipeline appears to be general and potentially applicable to a broader range of tasks.\n\n2. Please provide the clear distinction with existing self-reflection works (especially those using self-reflection in training).\n\n3. Please provide the ablation study results of the full pipeline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bD5ly5wRLi", "forum": "I2aO05fIEG", "replyto": "I2aO05fIEG", "signatures": ["ICLR.cc/2026/Conference/Submission6827/Reviewer_4hhY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6827/Reviewer_4hhY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661154292, "cdate": 1761661154292, "tmdate": 1762919089722, "mdate": 1762919089722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, the paper is interesting and presents a novel idea. It proposes an agentic RL framework that teaches an LLM how to reflect and improve upon its own reasoning. The method leverages a probe policy that, given a trajectory leading to a dead end, generates a new trajectory with reflective reasoning to potentially correct the original mistake. The idea is original, and the overall modeling and writing are clear and well-structured.\n\nHowever, the main issue is that the authors appear to have omitted the appendix from the submission, which is a serious oversight. This omission makes it difficult to fully understand the paper, particularly regarding several important implementation details.\n\nIn summary, I find the paper promising and would be inclined to raise my score to acceptance if the authors can provide the missing appendix and clarify the implementation details during the rebuttal period."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper presents an interesting and well-written idea.\n\n- It addresses a compelling question in agentic reinforcement learning: how can we enable LLMs to learn to reflect, or more generally, when introducing an external policy (e.g., for reflection), how can we ensure the policy we want to learn remains on-policy? The paper provides reasonable and well-motivated solutions, including (1) filtering and (2) distribution realignment.\n\n- The experimental evaluation is comprehensive and thoughtfully designed, effectively exploring several important questions related to the framework’s design choices."}, "weaknesses": {"value": "- The appendix pages are missing, which makes it difficult to fully understand several key parts of the work.\n\n- The training process is somewhat complicated and not clearly explained, leading to confusion. For example, it is unclear what the complete training pipeline looks like — whether the two policies are trained jointly or sequentially, and whether they share parameters. \n\n- The ambiguity in describing the training procedure, along with the complexity of the overall training pipeline, makes it difficult to reproduce or fully evaluate the proposed method."}, "questions": {"value": "### Questions for the Authors\n\n- Regarding the definition of *dead ends*: besides trajectories that end with `<answer> ... </answer>`, are there other types of dead-end trajectories considered?  \n\n- There is confusion between the *current policy* and the *probe policy*. Does the probe policy $\\pi_\\epsilon$ share the same parameters as $\\pi_\\theta$ (only using different prompts), or are they two separate models with distinct parameters?  \n\n- For the probe policy, after identifying a dead end, are the subsequent rollouts — including reasoning, search, and answer generation — produced by $\\pi_\\epsilon$?  \n\n- What is the exact training procedure? Are $\\pi_\\theta$ and $\\pi_\\epsilon$ trained jointly, or is $\\pi_\\theta$ first warmed up and then used to train the probe policy?  \n\n- Equation (5) defining the probe policy is difficult to interpret. Could the authors provide more explanation for this formulation, particularly why the denominator involves $z^{\\frac{1}{|o'_{\\text{origin}}|}}$ when $o'_{i,t} \\in o'_{\\text{origin}}$? What is the intuition behind this design?  \n\n- In the ablation study, what exactly is *coarse-PPD*? A one-sentence description is insufficient to understand the difference — please clarify what it looks like in detail.  \n\n- It would be helpful to include more **case studies** comparing the behaviors of Search-R1 and the proposed model, to better illustrate their differences.  \n\n- During the reflection process, how many new search actions are typically performed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BGF3XSHj05", "forum": "I2aO05fIEG", "replyto": "I2aO05fIEG", "signatures": ["ICLR.cc/2026/Conference/Submission6827/Reviewer_DgE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6827/Reviewer_DgE2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866371978, "cdate": 1761866371978, "tmdate": 1762919089229, "mdate": 1762919089229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the dead-end problem in RAG systems trained with RL, where the model often gets stuck in incorrect reasoning paths and fails to explore new directions. To overcome this, the authors propose REX-RAG, a framework that introduces a Mixed Sampling Strategy to inject exploratory prompts and guide the model toward more diverse reasoning trajectories. However, such exploration may cause distributional shifts that make RL training unstable, so a Policy Correction Mechanism is further designed to re-weight the exploratory data using trajectory filtering and multiple importance sampling, keeping the optimization process stable and unbiased. Experiments on several question-answering benchmarks show that this approach brings consistent performance gains, and ablation results confirm the importance of the correction mechanism for effective exploration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method seems sound and effective based on the reported results, which makes it convincing that this approach is useful for training policies that better interact with search engines (or tools). Additionally, the experiment setup is very comprehensive and is accompanied by a good set of ablations that clarify the effectiveness of each component in the system."}, "weaknesses": {"value": "The proposed method is significantly more expensive than the baselines, specifically the most similar baseline, search-r1. It is not clear for me the improvements observed here are from the increased number of sampling during training or because of the sampling strategy. We know that number of rollouts in the training can significantly increase the compute budget of training and improving performance. I am curios to  see if this method still performs better than search-r1 if it uses the same number of rollouts (including initial and exploratory rollouts)."}, "questions": {"value": "What happens if you assign the same exploration budget (n) to the policy model of search-r1? Would it still downperform your model? Or in other words, I would like to see how your model performs if it can only sample (including exploratory sampling) the same as search-r1? Would that affect your findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cKvrZnH8LZ", "forum": "I2aO05fIEG", "replyto": "I2aO05fIEG", "signatures": ["ICLR.cc/2026/Conference/Submission6827/Reviewer_X6TS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6827/Reviewer_X6TS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049742120, "cdate": 1762049742120, "tmdate": 1762919088863, "mdate": 1762919088863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REX-RAG, a retrieval augmented generation framework to address the dead end problem in RL-based RAG training. That is, when RL rollouts results in incorrect reasoning paths but the policy is unable to self-correct. The proposed REX-RAG consists of (1) a sixed sampling strategy that uses a so-called \"probe sampling\" to help the model increase rollout sample size to avoid dead ends; and (2) policy correction learning that introduces techniques such as trajectory filtering and multiple importance sampling to stabilize RL training with the introduced rollouts & additional corrections introduced by REX-RAG. The authors evaluate the proposed method on several QA benchmarks, where REX-RAG achieved consistent improvements over RL-based RAG baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors study an important problem of dead end in RL-based RAG settings, where the policy is often unable to generate correct reasoning paths for complex input queries.\n\n2. The authors introduces effective techniques to improve the training dynamics on the rollouts and additional correction continuations by the probe policy.\n\n3. REX-RAG shows strong performance on open-domain QA datasets, suggesting the model learns improved reasoning patterns for multi-turn search LLMs."}, "weaknesses": {"value": "1. The entire exploration and correction mechanism is training-only, as it relies on ground truth labels (rather than learning a verifier) to identify dead ends and trigger exploration. Therefore in inference, these mechanisms are deactivated and the model cannot really correct itself if it heads down to incorrect reasoning paths.\n\n2. The exploration & additional sampling introduces extra computation overhead during the training phase, which may be potentially unfair to baselines like Search-R1 which adopts fixed group size in training. Although the authors provide additional results with over-sampling using DAPO, these results are inconsistent (Search-R1 outperforms REX-RAG) and does have dataset-specific results.\n\n3. Some technical details are missing in writing. For example, although I can image how these are computed, the authors should provide a formula to show how PMF is computed in Eq. 5."}, "questions": {"value": "1. For the exploration prompt sampled from a curated prompt pool, are these tokens masked out in policy update or are they also included in training as in Eq. 5-7?\n\n2. Can you provide more dataset-specific results with DAPO on Search-R1 and REX-RAG?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jD919G5hqE", "forum": "I2aO05fIEG", "replyto": "I2aO05fIEG", "signatures": ["ICLR.cc/2026/Conference/Submission6827/Reviewer_iQQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6827/Reviewer_iQQ8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136689519, "cdate": 1762136689519, "tmdate": 1762919088429, "mdate": 1762919088429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}