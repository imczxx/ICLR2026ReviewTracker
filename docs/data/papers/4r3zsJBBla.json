{"id": "4r3zsJBBla", "number": 2485, "cdate": 1757116865010, "mdate": 1759898145280, "content": {"title": "Multimodal Generative Composition Recommendation", "abstract": "Compositing an object into a given image is a common task in image editing, requiring both creative ideation and technical precision to achieve harmony. Professionals start by brainstorming concepts, then scale and position elements to integrate seamlessly within the image. While recent diffusion models have made significant progress in pixel harmonization, suggesting suitable concepts as well as recommending compatible composition locations and scaling remains a less explored and challenging task. In this work, we leverage the advanced reasoning capabilities of Multimodal Large Language Models (MLLMs) to address these challenges. We first propose a data pipeline that automatically generates diverse, high-quality, large-scale training data from an internet-scale stock image. Using this dataset, we fine-tune MLLMs with enhanced projector designs and targeted data augmentation to achieve robust content recommendation and precise object placement, demonstrating strong performance against prior methods. Our model supports flexible input options—either image or text—alongside user-defined placement control, offering designers a new level of creative flexibility. Finally, we showcase the model’s impact in real-world editing workflows where our model achieves state-of-the-art performance consistently on image composition benchmarks, including our self-created in-the-wild evaluation dataset Composition1K. The code and the Composition1K dataset are provided at https://anonymous.4open.science/r/MGCR.", "tldr": "", "keywords": ["Multimodal LLM", "Image Composition Recommendation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57f7afcc25b08c13e6f0c7ef0a146fc828568b32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets two complementary problems for image composition that precede pixel-level editing: (1) compositional content recommendation (what foregrounds suit a given background, or vice-versa) and (2) controllable object placement (where to place, how large, and at what approximate depth). The approach fine-tunes an MLLM (LLaVA-style) with an engineered data pipeline that mines stock images, filters them, removes objects to create clean backgrounds, generates dense captions for foregrounds/backgrounds, and computes depth maps. The model uses separate visual projectors for background RGB, background depth, and the foreground input, and predicts (x, y, w, h) plus an average depth, with optional natural-language control (e.g., “left side,” “distant from the camera”). Experiments cover OPA, a 10K Stock validation set, and a new in-the-wild Composition1K set. The method reports SOTA across a range of metrics and provides ablations supporting key design choices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem decomposition  \n\n2. Model design choices are simple\n\n3. The data pipeline is pragmatic"}, "weaknesses": {"value": "1. Much of the technical lift is from data curation and existing model integration. The architectural novelty (separate projectors; depth tokenization; instruction format) is modest. The contribution may be perceived as an engineering-heavy solution with incremental modeling/algorithm ideas.\n\n1. FID/LPIPS are known to be imperfect for composition realism and placement plausibility; while CMMD is added, some community members may still question whether these metrics faithfully capture compositional quality.  \n\n1. On OPA, the training uses only positives, and evaluation compares composites to positives. This choice should be justified more deeply with sensitivity analyses (e.g., does the model overfit to scene priors rather than object–background relations?).  \n\n1. For Composition1K, human study details (sampling, rater demographics, interface, inter-rater reliability, multiple-comparisons control) could be reported more thoroughly.\n\n1. The pipeline relies on “an internet-scale stock image” source. While an ethics statement exists, more detail is needed on licensing/compliance and potential dataset biases that could affect recommendations (e.g., stereotypical insertions, object-context priors learned from stock imagery). The paper would benefit from a fuller audit and mitigation strategy.\n\n1.  The pipeline depends on several external components (segmentation, depth estimation, captioners, diffusion inpainting, text-to-image synthesis, ObjectStitch). The robustness of the end-to-end experience when any of these is wrong (e.g., erroneous depth yields bad occlusion masks) is not systematically studied. A failure taxonomy with quantitative breakdowns would strengthen the claims.\n\n1.  The data pipeline filters uniform backgrounds and repetitive objects and curates certain scene types. It is unclear how well the model handles out-of-distribution imagery (medical, satellite, stylized artworks, diagrams). A cross-domain evaluation (or at least qualitative stress tests) would be valuable.\n\n1. The model supports natural-language controls such as distant from the camera or place on the left. The mapping from such phrases to quantitative constraints could be elaborated. Are these controls composable and robust to paraphrasing? A small controlled study on control-following accuracy would help.\n\n1. While an anonymous repo is linked, reproducing the full pipeline likely requires access to the large stock corpus and multiple third-party models. Clearer guidance on how to replicate with public substitutes (and expected performance deltas) would be appreciated."}, "questions": {"value": "1. How sensitive are placement results to the depth estimator used? Have you tried a weaker/older depth model, and how much do mIoU and user satisfaction drop?  \n1. Can include a brief negative set or counterfactual analysis (e.g., wrong depth, wrong caption) to quantify robustness, and add a qualitative appendix on failure cases categorized by root cause (depth misread, segmentation leak, foreground pose mismatch).\n2. For the separate projectors, did you try partial parameter sharing (e.g., shared low-level layers with modality-specific adapters)?  \n3. Can you quantify control-following accuracy for textual constraints (left/right/near/far/behind X) across a controlled benchmark?  \n4. How does the model behave with stylized or non-photographic backgrounds (cartoons, renders, paintings)?  \n5. In Composition1K, how were foreground–background pairs curated to ensure semantic compatibility without leaking positional priors?  \n6. Could the CCR model generate multiple diverse candidates with coverage guarantees, and does diversity correlate with user satisfaction?  \n7. What is the latency profile end-to-end (CCR → SD3 → OP → segmentation → ObjectStitch) on commodity GPUs?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The stock-image corpus is described abstractly. The ethics section is brief. Since the method leverages scraped or licensed images, object removal, and derived captions, the paper should clarify licensing status, redistribution constraints, de-identification, and any geographic/cultural biases in stock photography that might affect recommendations and placements."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "utSRMGg6mO", "forum": "4r3zsJBBla", "replyto": "4r3zsJBBla", "signatures": ["ICLR.cc/2026/Conference/Submission2485/Reviewer_mUVX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2485/Reviewer_mUVX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641810338, "cdate": 1761641810338, "tmdate": 1762916252492, "mdate": 1762916252492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes using Multimodal Large Language Models (MLLMs) for image composition tasks, specifically content recommendation (suggesting compatible foreground/background elements) and object placement (predicting bounding boxes and depth). The authors develop a data pipeline to generate training data from stock images using filtering, segmentation, and object removal. They fine-tune LLaVA with separate projectors for foreground, background, and depth inputs, and evaluate on OPA, a stock image validation set, and a new Composition1K dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Thorough evaluation: Creates Composition1K, an in-the-wild test set with user studies, complementing existing benchmarks.\n2. Depth prediction: Adds depth output for occlusion-aware composition, going beyond prior 2D bounding box prediction.\n3. Detailed ablations: Includes systematic ablations on data pipeline components, architectural choices, and augmentation strategies."}, "weaknesses": {"value": "1. The core contribution is applying existing MLLM architecture (LLaVA) to composition tasks. The \"separate projectors\" design is simply using independent 2-layer MLPs for different inputs—not a significant architectural innovation. Data augmentation techniques (color jittering, brightness adjustment, downsampling) are standard methods applied to prevent overfitting.\n\n\n2. The pipeline heavily relies on multiple off-the-shelf models (InternVL, ViP-LLaVA, ObjectDrop, Depth Anything). Errors propagate through this chain—the authors acknowledge foreground captioning inaccuracy but don't quantify the impact. MLLM-based filtering may introduce dataset bias, as evidenced by failure on distant scenes with crowds.\n\n\n3. Claims \"state-of-the-art\" with incomplete comparisons. The \"scaling law\" claim (Fig. 6) overstates standard data size vs. performance analysis. Significant failure cases (distant scenes, crowds) are attributed to data rather than approach limitations.\nThe integration workflow still requires multiple external models (SD3, ObjectStitch, segmentation). The MLLM primarily adds bounding box prediction, not end-to-end composition. Depth prediction doesn't handle complex spatial relationships like object contact or physical support."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z5kVAlwS0J", "forum": "4r3zsJBBla", "replyto": "4r3zsJBBla", "signatures": ["ICLR.cc/2026/Conference/Submission2485/Reviewer_syjN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2485/Reviewer_syjN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866323506, "cdate": 1761866323506, "tmdate": 1762916252155, "mdate": 1762916252155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Multimodal Generative Composition Recommendation (MGCR) framework using Multimodal Large Language Models (MLLMs) to solve two core image compositing tasks: conditional content recommendation and controllable object placement. It builds a scalable data pipeline: MLLM-based filtering retains ~70% valid stock images (709K backgrounds, 1.3M foreground-background pairs), with mask merging, improved object removal, and auto-generated captions and depth maps; targeted data augmentation prevents overfitting. The MLLM uses independent projectors for background, foreground, and depth images. Experiments show SOTA performance: OPA (FID 15.07), Stock (mIOU 0.569). The proposed benchmark, Composition1K, achieves 35.9% user satisfaction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Innovative Scalable Data Pipeline: The paper designs an automated pipeline to generate large-scale, high-quality training data from internet-scale stock images. This addresses the critical issue of insufficient high-quality data for image compositing, laying a solid training foundation.\n- Tailored MLLM Architecture: The MLLM adopts three independent projectors (for background, background depth, foreground) to avoid information loss from shared projectors, supports depth prediction (enabling realistic occlusion handling), and accepts flexible inputs (text/image) plus user-controlled placement. This significantly enhances adaptability and spatial reasoning for compositing tasks\n- New Benchmark for : Beyond SOTA performance on classic datasets (e.g., FID 15.07 on OPA, mIOU 0.569 on Stock), the paper constructs the Composition1K dataset (195 real scenes, 1149 pairs) and conducts user/MLLM evaluations. This fills the gap of real-world compositing assessment, validating practical effectiveness"}, "weaknesses": {"value": "- Outdated Baseline Comparisons: Table 1 only compares outdated open-source MLLMs (LLaVA-1.6, InternVL-2.0). Adding recent SOTA like Qwen2.5-VL, InternVL3.0 (open-source) and Gemini 2.5 Pro (commercial) is necessary to validate the model’s competitiveness.\n- Insufficient Innovation in Scene Harmony and Object Integration. The core challenge of image compositing lies not only in content recommendation but also in ensuring the inserted object’s harmony with the scene—including shadow consistency, reflection rendering, and perspective alignment. However, the paper directly adopts the existing ObjectStitch model for this critical integration step, with no additional innovations or improvements. Given that scene harmony is a well-documented pain point in compositing, relying entirely on an off-the-shelf tool might diminish the work’s technical contribution.\n- Missing Depth Input Ablation: No ablation study to verify whether depth images are important for compositional content recommendation."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1t16D1VciO", "forum": "4r3zsJBBla", "replyto": "4r3zsJBBla", "signatures": ["ICLR.cc/2026/Conference/Submission2485/Reviewer_f7Fk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2485/Reviewer_f7Fk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901752492, "cdate": 1761901752492, "tmdate": 1762916251951, "mdate": 1762916251951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework that uses Multimodal Large Language Models to enhance image compositing by automatically recommending what to add to an image and where to place it. By combining content recommendation and object placement tasks, and training on a large, automatically generated dataset, the model predicts compatible objects, locations, scales, and depths for realistic image composition. Built on an enhanced LLaVA architecture with separate visual projectors, The proposed method shows strong performance and practical potential for design and editing applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n1.The paper broadens the conventional definition of image composition, extending it beyond simple foreground–background blending to a more comprehensive task of recommending semantically and spatially compatible elements. It supports diverse multimodal inputs, enables iterative editing, and offers enhanced flexibility in compositional reasoning.\n2.The large-scale, automated data generation and filtering pipeline (including segmentation, object removal, captioning, and depth estimation) is robust and scalable. This is a practical contribution to compositional data synthesis\n3.The manuscript is clearly written and well structured, with well-defined research questions and viewpoints. The experimental design is extensive and rigorous, providing strong empirical support for the proposed approach."}, "weaknesses": {"value": "Weaknesses:\n1.Section 3.1 is somewhat verbose and could be streamlined for conciseness; however, this does not substantially affect the overall clarity or contribution of the paper, so revision is optional.\n2.In Table 2, all compared methods are from 2023 or earlier. How do the results compare with works from 2024 and 2025 (for example, those mentioned in the Image Content Recommendation section of Related Work)? It would strengthen the paper to include some newer comparisons.\n3.For the visual content recommendation task, the proposed MLLM generates prompts for foreground or background elements that are semantically compatible and aesthetically coherent with the given image. Nevertheless, potential overfitting to frequently occurring object categories remains a concern. As described in Section 3.2, the data augmentation strategy primarily involves applying random transformations to color, brightness, and resolution for both foreground and background images, without altering the underlying object categories. Consequently, it would be valuable to examine, through quantitative analysis, whether the model’s foreground recommendations exhibit spurious correlations with specific background contexts during testing."}, "questions": {"value": "Please refer to the Weaknesses 2-3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ci1LkSZusO", "forum": "4r3zsJBBla", "replyto": "4r3zsJBBla", "signatures": ["ICLR.cc/2026/Conference/Submission2485/Reviewer_RVqE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2485/Reviewer_RVqE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908009737, "cdate": 1761908009737, "tmdate": 1762916251491, "mdate": 1762916251491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}