{"id": "ti7Lxjv3Ol", "number": 7028, "cdate": 1758005491539, "mdate": 1759897877332, "content": {"title": "Guiding Explanatory Inference through Inference Types", "abstract": "This work investigates localised, quasi-symbolic inference behaviours in distributional representation spaces by focusing on Explanation-based Natural Language Inference (NLI), where two explanations (premises) are provided to derive a single conclusion. We first establish the connection between natural language and symbolic inferences by characterising quasi-symbolic NLI behaviours, named \\textit{inference types}. Next, we establish the connection between distributional and symbolic inferences by formalising the Transformer NLI model as a rule-based neural NLI model - a \\textit{quasi-symbolic NLI framework} where different inference behaviours are encoded as functionally separated subspaces in the latent parametric space. We perform extensive experiments which reveal that inference types can enhance model training and inference dynamics, and deliver localised, symbolic inference control, and latent inference-type disentanglement. Based on these findings, future work will probe the composition and generalisation of symbolic inference behaviour in distributional representation spaces.", "tldr": "", "keywords": ["Natural Language Inference", "quasi-symbolic inference control", "neuro-symbolic", "representation learning"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b860be4e1f17320034c45a0c429599bc3dcd0479.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how to realize localized and quasi-symbolic inference behaviors in a distributed representation space. Specifically, the authors focus on the Explanation-based NLI task, which involves deriving a conclusion from two explanations (premises).\n\nThe paper's core contribution is the introduction of inference types, a set of systematically defined quasi-symbolic reasoning behaviors. Using Abstract Meaning Representation (AMR) as a formalization framework, the authors annotated 5134 samples from EntailmentBank, defining 10 symbolic transformation types.\nTheoretically, the paper proposes a quasi-symbolic NLI framework and formalizes it based on the Neural Tangent Kernel (NTK) theory. The framework hypothesizes that different inference types can be encoded as functionally separate subspaces within the model's parameter space through gradient descent."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  This paper aims to address the lack of localized and quasi-symbolic inference behavior in NLI models, which is crucial for fields like explainability, controllability, and mechanistic interpretability.\n2.  The use of AMR as an intermediate interface to define inference types is an good choice. This provides a solid linguistic and formal semantic foundation for the 10 inference types, rather than just ad-hoc labels.\n3.  The paper not only proposes a framework but also attempts to use NTK theory to explain its underlying mechanism."}, "weaknesses": {"value": "The framework relies on AMR to define inference types, and the annotations are performed manually. In a practical inference setting, how would the system determine which inference type should be applied to a new, unseen pair of premises? The paper seems to assume that the target inference type $\\pi$ is always given. This is reasonable for a control task, but for the standard NLI prediction task, this seems to be a limitation. In a standard inference scenario where $\\pi$ is unknown, how do you envision this framework working? Would it require a separate classifier to predict $\\pi$ first? Or do you believe its primary application is in controllable text generation?"}, "questions": {"value": "Please refer the weaknesses section :-)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FHDNfDYpFL", "forum": "ti7Lxjv3Ol", "replyto": "ti7Lxjv3Ol", "signatures": ["ICLR.cc/2026/Conference/Submission7028/Reviewer_qrnk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7028/Reviewer_qrnk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564121616, "cdate": 1761564121616, "tmdate": 1762919230606, "mdate": 1762919230606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates quasi symbolic reasoning within Transformer models on explanation-based nlp tasks. The task involves deriving a conclusion from two explanatory premises. The authors first introduce a formalization of this reasoning behavior by defining ten \"inference types\" grounded in abstract meaning representation. \n\nThe core technical contribution is a framework for guiding a T5 model using these explicit inference types. The authors provide a theoretical justification using neural tangent kernel theory, positing that different inference types induce distinct, separable function spaces within the model's parametric space. Empirically, they demonstrate this by feeding the inference type label as a prefix to the model's encoder. This approach improves generative accuracy on the NLI task and also enables localized control. The model can be guided by changing the prefix at inference time to produce structurally different conclusions from the same premises. Their analysis also shows that this method leads to disentanglement, with inference types forming more separable clusters in the latent and parametric spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's primary strength lies in its fusion of formal linguistic structures with standard neural architectures to achieve controllable reasoning. The introduction of AMR grounded \"inference types\" is a well structured formalism for characterizing the quasi symbolic steps in explanation based NLI. \n\nThe authors propose a method and a theoretical justification using neural tangent kernels, positing that these inference types map to distinct functional subspaces. This claim is validated through multiple experiments. The demonstration of localized control, where varying the inference type prefix systematically alters the generated conclusion from identical premises, is interesting.  It is supported by quantitative analyses showing increased separability in both the latent feature space and the parametric (gradient) space, which strengthens the theoretical hypothesis."}, "weaknesses": {"value": "While the work is well-motivated, its technical execution and claims have several limitations. The theoretical justification using NTK is tenuous. NTK theory is most precise for infinite-width networks operating in a \"lazy\" training regime where parameters do not move far from initialization. This paper, however, deals with a fine-tuned, fixed-width model that is explicitly intended to learn new functional representations (feature learning), a dynamic that often deviates significantly from the lazy regime. The paper provides empirical evidence of gradient orthogonality, but it does not bridge the conceptual gap between the idealized NTK theory and the practical, non-lazy dynamics of fine-tuning T5.\n\nThe claims of \"disentanglement\" and \"quasi-symbolic control\" also are somewhat overstated. The model is learning prefix-conditioned generation, which is a form of control, but the evidence for true latent space separation is weak. The potential instability of this control can be observed in the qualitative examples. For instance, the \"ARG-SUB\" guided model generates \"the blacktop is made of smooth surface,\" which is a semantic and grammatical error. This suggests the model is not executing a symbolic rule (substituting an entity) but is performing a soft, pattern-based substitution (replacing the text \"asphalt concrete\" with \"smooth surface\") that fails to respect the underlying semantic roles.\n\nFinally, the use of AMR is a missed opportunity. AMR is used to *define* the inference type labels, but this structural information is never provided to the model. The model only sees a low-bandwidth string prefix. This means the model learns a single representation for \"ARG-SUB,\" even though this category may contain many structurally distinct operations at the AMR graph level. A more rigorous test of quasi-symbolic grounding would involve conditioning the model on richer, graph-based structural information derived from AMR."}, "questions": {"value": "1. Can you further justify the application of NTK theory? The fine-tuning of T5 seems to operate in a feature learning regime, which deviates from the lazy training assumptions inherent to NTK. How does the theory meaningfully apply in this practical, non-lazy context?\n\n2. In the qualitative examples, the ARG-SUB operation produces \"the blacktop is made of smooth surface,\" which is semantically incorrect. Does this error indicate that the model is learning a shallow textual substitution pattern associated with the prefix, rather than a robust, semantically-grounded symbolic rule?\n\n3. The AMR graphs provide a rich structural basis for the inference types, yet the model is only conditioned on a single, coarse label like \"ARG-SUB\". Have you explored conditioning the model on a more direct structural representation of the AMR transformation itself, rather than this many-to-one textual label?\n\n4. How do you expect this framework to generalize to entirely new inference types not present in the training data? Does the current approach learn the rules themselves, or does it primarily learn to associate ten specific labels with ten learned output behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VMzrmB5r1c", "forum": "ti7Lxjv3Ol", "replyto": "ti7Lxjv3Ol", "signatures": ["ICLR.cc/2026/Conference/Submission7028/Reviewer_Vw6H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7028/Reviewer_Vw6H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817523810, "cdate": 1761817523810, "tmdate": 1762919229999, "mdate": 1762919229999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies explanation-based NLI by making reasoning controllable and interpretable. To make this happen, contributions are (1) linguistic formalization, where they annotate dataset with inference types that describe how to transition from premise to conclusion (annotated dataset is planned for public release) and (2) quasi-symbolic framework where they teach transformers (e.g., T5) to govern subspace formation in their parametric space. The method is shown to improve accuracy and controllability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using inference type to teach transformers is elegant and novel. The paper reads well and the core idea is quite interesting.\n\n2. The framework has a strong theoretical support (using AMR and NTK).\n\n3. The detailed annotation as well as the annotated dataset (from EntailmentBank) itself will help reproducibility and also the research community in general.\n\n4. Empirical analysis are detailed and show improved performance and interpretability."}, "weaknesses": {"value": "1. The method currently uses two premises which is quite limited in real-world, so I was wondering if how the approach can handle multi-hop reasoning situations with more premises involved.\n\n2. Most of the core experiments and analyses were based on specific model T5, so the validation is kind of skewed IMO (e.g., separated subspaces is only validated using T5 architecture).\n\n3. Annotating by humans is usually expensive, therefore expanding to include other domains is quite challenging."}, "questions": {"value": "1. As mentioned in weaknesses, can authors show how to handle multi-hop reasoning in their setting?\n\n2. Authors mentioned math reasoning as future work, so I'd be really interested to see how their approach (e.g., inference types) would be adapted for math?\n\n3. How could we expand the framework to more (known or unknown number) of premises?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XeApXsKoBF", "forum": "ti7Lxjv3Ol", "replyto": "ti7Lxjv3Ol", "signatures": ["ICLR.cc/2026/Conference/Submission7028/Reviewer_sW2W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7028/Reviewer_sW2W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936295774, "cdate": 1761936295774, "tmdate": 1762919229601, "mdate": 1762919229601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a quasi-symbolic framework for natural language inference. The method involves three stages: (1) the premises are encoded into neural representations, (2) those representations are transformed conditional on an inference type, and (3) the representations are transformed into a conclusion. This enables guidance during training and interpretability during inference. By controlling the inference type, the generated conclusions can be controlled \n\nStrengths:\n- The family of inference types seems reasonably broad and varied to me, and the detailed information on the neuro symbolic architecture was interesting and well presented. \n- The neuro-symbolic approach seems fascinating, and its intuitive that structuring of inferences would help interpretability. \n- The method seems to work, with natural language inference results improving, though see my question below\n\nWeaknesses:\n- Generally, the neurosymbolic approach seems like it falls prey to the \"bitter lesson\". I think this research is really cool and full of deep ideas, but I can help but think that in practice if we want a better neural NLI system we would want to context engineer chat models with these inference types. I don't think that is a reason to reject this paper, as I still think that the neuro-symbolic approach could bear fruit someday, but it does prevent me from strongly advocating for this line of work.\n- The connections to NTK were not particularly helpful for me, and I felt like my intuitions about how this training would operate wasn't helped by this. Maybe work on this connection to explain how its a useful lens for the work?\n- The qualitative analysis was not that surprising or illuminating for me. It makes sense that these inference types can be separated in space, and the structure didn't look striking, kind of seems like what would have to happen if the training was successful and the inference types were used. \n\n\nJust to double check, the performance in table 2 for the NO row is still on a model that was fine-tuned for this task correct? I just want to make sure you aren't comparing a pretrained model against your method for training a model.\n\nOverall, I think this type of work is cool and this paper is worth accepting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See summary"}, "weaknesses": {"value": "See summary"}, "questions": {"value": "See summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zC94Up1XSK", "forum": "ti7Lxjv3Ol", "replyto": "ti7Lxjv3Ol", "signatures": ["ICLR.cc/2026/Conference/Submission7028/Reviewer_eP1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7028/Reviewer_eP1G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7028/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038039273, "cdate": 1762038039273, "tmdate": 1762919229195, "mdate": 1762919229195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}