{"id": "XwHQMNUSZP", "number": 4908, "cdate": 1757797956572, "mdate": 1763589791703, "content": {"title": "Generating Human Motion Videos using a Cascaded Text-to-Video Framework", "abstract": "Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. \nDespite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. \nIn this work, we propose CAMEO, a Cascaded framework for general human Motion vidEO generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. \nSpecifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. \nFurthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. \nWe demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M–VDM combination, while highlighting its versatility across diverse use cases.", "tldr": "", "keywords": ["Diffusion", "Video Diffusion", "Tex-to-Video Generation", "Human Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84c70b7ec455a4e5a1575bfbeaf3c37ccb36bf20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the same problem as HMTV, which connects a text-to-motion module with a motion-to-video generator. The authors first disentangle the text prompt into a motion prompt and a semantic prompt. The motion prompt is then converted into a motion sequence using a text-to-motion model, which is subsequently rendered as guidance videos. A tailored conditioning strategy is developed for the motion-conditioned video diffusion model (VDM). Camera pose selection is handled by an early denoising stage of a text-to-video model. The results demonstrate both quantitative and qualitative improvements over the vanilla text-to-video baseline and the prior method, HMTV."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation that large body movements are generated earlier, while finer details appear later, is interesting. Based on this observation, the authors design a conditioning strategy that improves performance.\n\n2. The approach of using a text-to-video model to generate a reference video, and then estimating the approximate camera pose from the early frames of the generated human shapes, is also interesting.\n\n3. The paper demonstrates that the generated motion videos can be edited using the SDEdit approach, which represents a meaningful and practical application of the proposed method."}, "weaknesses": {"value": "1. The paper proposes to recaption videos and disentangle the text prompt into two complementary parts: motion caption and semantic caption. However, there is no ablation study that evaluates each part separately. What would the results be if the videos were recaptioned into a single caption instead? In Table 2, the paper compares results with and without text refinement, but the improvement is unclear, and in fact, the motion metrics drop after refinement.\n2. There is no ablation study or sufficient explanation regarding the hyperparameter choices and the intuition behind the diffusion timestep sampling. In particular, the rationale for using a truncated normal distribution, the mean reduction, and the increase of standard deviation should be further clarified.\n3. The argument in Lines 200–210 and Figure 3a needs revision. As I understand it, both the vanilla training and the proposed method use the same motion but different text prompts as inputs. If the visual conditions are the same, why does the vanilla training fail to capture the fine-grained motion? Does this imply that the generated motion is not aligned with the motion condition?\n4. The camera pose selection is based on an existing text-to-video model, rather than being predicted within the proposed pipeline. This raises doubts about the accuracy of the extracted poses. It would be more elegant and meaningful if the camera poses were predicted directly by the model.\n5. The quantitative results in Table 1 are not particularly strong, especially on the MovieGen benchmark.\n6. The paper should discuss related work that studies a related problem:\nMove-in-2D: 2D-Conditioned Human Motion Generation, CVPR 2025.\n7. There are no supplementary videos provided, making it difficult to assess the visual quality of the model."}, "questions": {"value": "1. In the motion editing experiments in Figure 6, all results appear to use the same text prompt. Are there any results showing edits using different text prompts?\n2. Why is the first-stage text-to-motion model not retrained on the proposed dataset? Would retraining it improve performance?\n3. In Figure 4, should HumanVid actually be CamAnimate? Do both HumanVid and Ours use the same motion sequences as input, while VDM is trained on different data and uses a different backbone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zaZ2flfI5H", "forum": "XwHQMNUSZP", "replyto": "XwHQMNUSZP", "signatures": ["ICLR.cc/2026/Conference/Submission4908/Reviewer_RK72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4908/Reviewer_RK72"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889462046, "cdate": 1761889462046, "tmdate": 1762917753665, "mdate": 1762917753665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their constructive, positive, and thorough reviews. We are happy that the reviewers think that our paper **presents a clear and effective pipeline** (`XEUb`, `5AhP`, `APoR`, `RK72`), offers simple yet impactful design choices, and **provides competitive and promising results supported by extensive experiments** (`XEUb`, `5AhP`). Below, we first summarize the main concerns raised in the reviews and then provide detailed point by point responses.\n\n**1. Limited Novelty** (`XEUb`, `5AhP`, `APoR`) \n\n**A.** While we acknowledge that our framework uses several off the shelf components, we would like to emphasize that our main contribution lies in **identifying and addressing fundamental design factors that are critical for reliable human centric video generation**. \nWe identify overlooked dual conditioning conflicts that arise when multiple signals jointly guide the generation process, yet have not been examined in prior work despite their central role in achieving stability, controllability, and consistency in human centric generation. By explicitly analyzing these conflicts and introducing our view selection module, we show that resolving such interactions is essential for stable and controllable behavior.\n\nAlthough our analysis focuses on visual cue and text conditioning, the underlying principles apply more broadly to systems where structured conditioning signals must be generated or orchestrated. For example, recent world models such as Cosmos Transfer2.5 [1] rely on multiple conditoning signals to control behavior, yet the process of generating and harmonizing these signals remains underexplored. In this sense, our framework provides a concrete case study and an initial proof of concept for **modular generative models where control arises from the careful design and coordination of structured signals**. We expect that our findings can inform future research in this line of work by motivating more principled approaches to creating and integrating conditioning signals.\n\n**2. Conducting User study** (`XEUb`, `APoR`, `RK72`) \n\n**A.** Several reviewers pointed out that our method does not appear clearly superior under standard metrics. While VBench is widely used, many of its scores are known to saturate and can be insensitive to nuanced differences in human centric video quality. To address this limitation and provide a more reliable assessment, we conducted an additional user study upon request.\n\n|                    | **Win** | **Lose** | **Tie** | **p-value** |\n|--------------------|---------|----------|---------|-------------|\n| **Motion Quality** | **0.631** | 0.214    | 0.145   | 0.0026    |\n| **Visual Quality** | **0.545** | 0.257    | 0.186   | 0.0410    |\n\nParticipants blindly compared paired videos from the baselines and our method, judging motion quality and visual quality using win, lose, or tie. A total of 44 participants took part in the study. **Our method achieved more wins than the baselines on both criteria, with statistically significant differences** based on a binomial test excluding ties (motion: p=0.0026, visual: p=0.0410). We elaborate on the user study in 4.4 and Appendix B.4.\n\n**3. Generalization Ability** (`5AhP`, `APoR`)\n\n**A.** Several reviewers asked about the generalization ability of our method in more challenging scenarios, such as drastic camera view changes or cases where the person becomes partially out of frame. To address these questions, we added additional qualitative results covering a wider range of extreme conditions, which are now included in Appendix Figure 8. These examples illustrate that **our pipeline remains stable and coherent even under such challenging variations**. \n\n*[1] Ali, Arslan, et al. \"World Simulation with Video Foundation Models for Physical AI.\"*"}}, "id": "Jn11G1O9nn", "forum": "XwHQMNUSZP", "replyto": "XwHQMNUSZP", "signatures": ["ICLR.cc/2026/Conference/Submission4908/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4908/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4908/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763590356473, "cdate": 1763590356473, "tmdate": 1763590356473, "mdate": 1763590356473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at human video generation from text prompts. The proposed method, named CAMEO, is a cascaded T2V generation pipeline, which consists of two parts: T2M and VDM. The T2M component utilizes an off-the-shelf model (STMC) to generate low-dimensional SMPL motion sequences, which are then rendered (conditioned on the camera-view-selection) and fed into a 2D-motion-conditioned video diffusion model to produce the final video. CAMEO is compared with recent methods (HTMV and CamAnimate) and outperforms them in most quantitative evaluation metrics. The paper also presents ablation studies regarding the choice of text refinement strategy and the importance of the view selection module."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear pipeline**. It connects a text-to-motion module and a motion-conditioned video diffusion model for more robust human motion video generation, which is technically sound.\n2. **Good caption design**. The caption re-captioning with motion/semantics split reduces conflicts during VDM training.\n3. The Camera view selection idea is simple yet effective, leveraging early denoising results in the text-to-video diffusion model to extract view changes."}, "weaknesses": {"value": "1. The approach is quite similar to HMTV and does not demonstrate a significant conceptual or methodological improvement.\n2. The procedure depends on early denoising frames and SMPL estimation; robustness to text domains (e.g., stylized, occlusions) is unclear.\n3. The proposed text refinement contributes only marginal improvements, as shown in Table 2; ablation results suggest it may not be a major factor.\n4. The base models used in comparison differ, making the claimed superiority less meaningful, as the quality of the base T2V model has a substantial influence on the resulting human motion videos.\n5. No user evaluation or human preference test is provided to validate the claimed perceptual improvements.\n6. Table 1 performance markings are incorrect -- the “best” and “second-best” indicators do not match the actual numerical values, which weakens result's credibility."}, "questions": {"value": "How robust is the camera module? What happens for stylized or animation-like prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N6Ai89nPrY", "forum": "XwHQMNUSZP", "replyto": "XwHQMNUSZP", "signatures": ["ICLR.cc/2026/Conference/Submission4908/Reviewer_APoR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4908/Reviewer_APoR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971213782, "cdate": 1761971213782, "tmdate": 1762917753294, "mdate": 1762917753294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a framework to decompose Text to Motion and Video Diffusion Models to generate videos conditioned well on Motion and view points.\n\nAuthors bring in techniques (e.g. refinement by LLMs using rendering tools) to make the pipeline continuous end to end with minimal (or no) need for human interruptions from prompts to final video.\n\nA new dataset is also proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive and Complete pipeline. Authors study the video human centric video generation as an integrated system and do not leave the bottlenecks (like view point conditioning) out of the solution.\n\nThe provided qualitative results look promising.\n\nThe new dataset brings some more novelty to this work.\n\nQuantitative results are competitive on MovieGen with state of the art, and is mostly the best on the proposed dataset."}, "weaknesses": {"value": "Although I appreciate the completeness of the approach, but there is not much novelty in each element being used and to some extend the method seems like an ad-hoc utilization of some off-the-shelf tools. The VDM controlNet is the only part that is trained by a new condition."}, "questions": {"value": "1- It seems to me that Tab 2, ablation study, is not supporting the contribution of different steps. Any clarification on this?\n\n2- Is there an analysis on visibility of different body parts? I feel in most of the qualitative results, the lower body is not visible. Can this be controlled by m_{1:k}?\n\n3- Is there any measurement on the diversity of the view points in the dataset, and the generated videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBWudqhQ4k", "forum": "XwHQMNUSZP", "replyto": "XwHQMNUSZP", "signatures": ["ICLR.cc/2026/Conference/Submission4908/Reviewer_5AhP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4908/Reviewer_5AhP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060286159, "cdate": 1762060286159, "tmdate": 1762917752826, "mdate": 1762917752826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes a text-to-motion augmented cascaded text-to-video generation framework for human motion video generation"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the paper proposes a feasible solution for reliable human motion generation in video generation models\n- the proposed framework clearly improves the correctness of generated human body structures and fidelity of generated poses and motions\n- extensive experiments demonstrate the effectiveness of the proposed modules and the effectiveness of semi-explicit 3D controls"}, "weaknesses": {"value": "- while the paper demonstrates improvements in terms of single-person motion for the video generation task, the novelty of the proposed framework seems to be limited: the proposed framework feels like a combination of existing components, which are added up together to solve a specific and narrowed-down problem. while the paper acknowledged the limitations of generalizability, it still unclear how robust the proposed framework is when handling more complicated cases for single-person scene video generation, e.g., what happens when camera distance significantly changes, or can the proposed framework handle the cases where the person is occluded or partially out of the frame or temporally missing in some frames\n- while the paper adopted VBench for quantitative comparisons, the reliability of the VBench metrics is still not well justified. based on the reviewer's experience, some scores might favor specific aspects of videos while ignoring the actual visual quality. it is highly recommended to conduct a user study to validate the effectiveness of the proposed method considering the human evaluation is still the most reliable metric for video generation tasks\n- the quality of demonstrated applications of motion editing and camera view editing seem to be not good in terms of subject consistency"}, "questions": {"value": "please refer to weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hxb6KUyHhF", "forum": "XwHQMNUSZP", "replyto": "XwHQMNUSZP", "signatures": ["ICLR.cc/2026/Conference/Submission4908/Reviewer_XEUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4908/Reviewer_XEUb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257438680, "cdate": 1762257438680, "tmdate": 1762917752071, "mdate": 1762917752071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}