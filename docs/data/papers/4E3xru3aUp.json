{"id": "4E3xru3aUp", "number": 7549, "cdate": 1758027080655, "mdate": 1759897847038, "content": {"title": "Navigating Cognitive Manifolds: Optimal Transport for Large Language Model Optimization", "abstract": "Large language models (LLMs) possess vast knowledge but face inefficiencies in task-specific knowledge organization and activation. Existing prompt engineering relies on empirical trial-and-error, lacking principled optimization frameworks. We introduce Cognitive Geometry Optimal Transport (CGOT), a framework that reframes LLM cognitive optimization as geometric navigation in high-dimensional probability spaces. Our key insight models cognitive configurations as probability measures over knowledge states, leveraging optimal transport theory to derive principled paths from initial to target configurations. CGOT employs a dual geometric guidance system: Wasserstein distances for radial metrics and Kantorovich potential gradients for directional guidance, enabling continuous optimization on cognitive manifolds. Through systematic experiments on three prominent LLMs (Qwen3-72B, Deepseek-v3-67B, LLaMA-3-70B) across four cognition-intensive benchmarks (GSM8K, HumanEval, CommonsenseQA, BigBench-Hard), we demonstrate: (1) LLM cognitive spaces exhibit low-dimensional manifold structures (intrinsic dimension ~8.7) with strong geometry-performance correlation (r = -0.76); (2) CGOT achieves consistent 4.8% average performance gains (Cohen's d > 0.7 in structured tasks), outperforming baselines like APO, OPRO, GrIPS, and BayesOpt-Prompt by 0.6% on average (p<0.05); (3) the framework generalizes across prompt strategies (Zero-shot: +5.3%, Few-shot: +4.5%, Chain-of-Thought: +4.6%) and model architectures. Ablation studies confirm the critical contributions of Wasserstein metrics (-1.3% without) and non-linear optimization (-2.2% without). This work bridges optimal transport theory with LLM optimization, transforming prompt engineering from empirical art to geometric science.", "tldr": "", "keywords": ["Large Language Models", "Optimal Transport", "Cognitive Manifolds", "Wasserstein Distance", "Prompt Optimization", "Geometric Deep Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d447af788660a3a23425ee64dca3a1ddcedab18a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel theoretical framework named Cognitive Geometric Optimal Transport (CGOT) that reformulates prompt optimization as a problem of optimal transport between probability measures. This work moves prompt engineering from empirical trial-and-error to a principled, geometry-based optimization process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides strong empirical evidence across multiple benchmarks (GSM8K, HumanEval, CommonsenseQA, BigBench-Hard) and models (Qwen3-72B, DeepSeek-v3-67B, LLaMA-3-70B).\n\n2. CGOT bridges the gap between empirical prompt engineering and theoretical cognitive optimization by grounding language model control in Wasserstein geometry and Kantorovich potential fields. This represents a major conceptual advance in linking cognitive science, geometry, and LLM optimization."}, "weaknesses": {"value": "1.  The quality of the figures is not good. For example, the font size of figures 1-3 is too small to read.\n\n2. Optimal transport computations are computationally expensive. The scalability of this work to real-time or large-scale applications remains uncertain.\n\n3. The comparison is not sufficient. For example, is the proposed method comparable to the meta-heuristic method? For example,\n\nGuo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., ... & Yang, Y. (2023). Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. ICLR 2024.\n\n4. The equation number is missing in line 273.\n\n5. The figure reference has a mistake in line 213."}, "questions": {"value": "1. How sensitive is CGOT to initialization or the choice of base prompt?\n\n2. How interpretable are the optimized prompts: do they reveal meaningful structure in how LLMs reason?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ennaVUY2Do", "forum": "4E3xru3aUp", "replyto": "4E3xru3aUp", "signatures": ["ICLR.cc/2026/Conference/Submission7549/Reviewer_Ai8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7549/Reviewer_Ai8s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203308077, "cdate": 1761203308077, "tmdate": 1762919645529, "mdate": 1762919645529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Cognitive Geometry Optimal Transport (CGOT), a theoretical and algorithmic framework that reformulates prompt optimization for large language models (LLMs) as a geometric navigation problem in cognitive space. By using optimal transport theory, it computes Wasserstein distances and Kantorovich potentials  to find efficient transformation paths from a current to a target cognitive configuration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The mathematical formalization is coherent, covering measure theory, Kantorovich duality, and manifold embeddings. The inclusion of proof sketches and algorithmic convergence guarantees adds credibility.\n2. The experimental results demonstrate consistent performance gains  across models, datasets, and prompting strategies, validated by multiple metrics.\n3. The paper’s core insight that treating prompt optimization as optimal transport on a cognitive manifold conceptually elegant."}, "weaknesses": {"value": "1.\tThe method relies on Wasserstein distance computation and ICNN-based Kantorovich potentials, which are computationally expensive.Is there any way to resolve it or address this issue?\n\n2.\tWould CGOT extend to multimodal or cross-lingual LLMs where cognitive manifolds differ substantially across modalities?\n\n3.\tHow to guarantee the convergence of this method?\n\n4.\tWhat are the time and memory overheads compared to existing prompt optimization baselines? Any trade-offs between accuracy and computational cost?\n\n5.\tThe semantic diagram is not informative and it is hard to intuitively grasp the idea.\n\n6.     Overall: while the study provides a detailed geometric formulation and interesting empirical correlations between Wasserstein distance and task performance, the proposed CGOT framework does not convincingly demonstrate conceptual or methodological novelty beyond existing manifold-based optimization or transport-theoretic approaches. The individual components like manifold representation, Wasserstein objectives, and Kantorovich potential refinement are already well-established, and the integration presented here appears incremental rather than fundamentally new. That is my main concern."}, "questions": {"value": "See wqeakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0c6OyTLqa", "forum": "4E3xru3aUp", "replyto": "4E3xru3aUp", "signatures": ["ICLR.cc/2026/Conference/Submission7549/Reviewer_4Jrw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7549/Reviewer_4Jrw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887611911, "cdate": 1761887611911, "tmdate": 1762919644940, "mdate": 1762919644940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes prompt optimization as a geometric problem in probability space rather than a discrete search over text tokens. They model each prompt-induced hidden-state distribution as a probability measure on a low-dimensional manifold called a “cognitive space.”They propose using optimal transport to move the current prompt's hidden-state distribution toward a task-optimal one. They call their approach Cognitive Geometry Optimal Transport (CGOT). CGOT integrates two geometric components that define a continuous optimization procedure:\n\n* Wasserstein distance to quantify how far the current cognitive configuration is from the target (radial metric).\n* Kantorovich potential gradients to indicate how it should move optimally through that space (directional guidance).\n\nContributions:\n\n* Uses optimal transport theory to provide a mathematically principled approach to LLM prompt optimization, \n* Develops a probabilistic manifold model of cognitive space, arguing that hidden-state activations form a consistent low-dimensional structure.\n* Proposes the CGOT algorithm, which alternates between estimating Wasserstein distances, learning Kantorovich potentials, and transporting probability mass to reach optimal \"cognitive configurations\" with convergence guarantees.\n* Uses Pearson correlation to show correlation with task performance and shows CGOT yields a modest but statistically significant performance gain over well-selected prompt optimizer baselines.\n\nIn summary, the paper recasts prompt engineering as a continuous geometric optimization problem, positioning CGOT as a bridge between cognitive science, manifold learning, and large-scale language-model control."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper introduces a framing of prompt optimization as a problem of optimal transport on cognitive manifolds that is novel in my view. The approach represents a synthesis of optimal transport theory, representation geometry, and LLM control that is theoretically ground and provides a nice contrast with more heuristic methods that dominate the space. The methodology uses established OT formalism and provides convergence guarantees. The experiments provide credible empirical validation.  The results of the experiments demonstrate incremental empirical gains, but the conceptual impact makes up for this as it provides a foundation for future research on geometry-aware prompt tuning."}, "weaknesses": {"value": "Evaluating the statistical association between W_2 distance and task performance could have been much stronger without much extra effort. Specifically, it doesn't address confounding due to the model or to task difficulty. A good alternative would be using a hierarchical or mixed-effects regression controlling for model and task. I suspect use of partial pooling across model and task in a hierarchical modeling setting might even yield stronger statistical results than was presented. \n\nIt would have been good to include LLM-as-the-optimizer techniques in the baseline comparison, as my understanding is the models do well. \n\nIt would have been good to include more information on practical costs and scalability, runtime analysis, iteration counts, the number of LLM forward passes per optimization loop, GPU hours, empirical convergence curves, impacts of caching, etc., for assessing feasibility models of various sizes."}, "questions": {"value": "* Was the correlation of –0.76 calculated by combining results from all models and tasks together? Why not account for model and task-specific variance?\n* How do you get the target distribution for each task? Is it based on runs where the model performs well, and if so, does CGOT need to collect new examples for every task?\n* How many LLM evaluations does CGOT usually need before it converges on the benchmarks you report? And roughly how long does that take? can you characterize the cost for optimizing one prompt on a model the size of LLaMA-3-70B?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Could be used for jailbreaking attacks -- this is unaddressed."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W9QHz1hJL0", "forum": "4E3xru3aUp", "replyto": "4E3xru3aUp", "signatures": ["ICLR.cc/2026/Conference/Submission7549/Reviewer_6uWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7549/Reviewer_6uWh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931378991, "cdate": 1761931378991, "tmdate": 1762919644365, "mdate": 1762919644365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cognitive Geometry Optimal Transport (CGOT), modeling LLM optimization as navigation on a cognitive manifold via optimal transport. The method applies Wasserstein geometry for prompt optimization and achieves consistent but modest (~4–5%) improvements across several large models and reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Sound theoretical framing using optimal transport and geometry.\n\nMathematically rigorous and technically detailed."}, "weaknesses": {"value": "The paper is a bit hard to follow, many sections are mathematically dense and conceptually abstract.\n\nSeveral figures and tables have fonts that are too small, and some elements overlap, making them hard to read.\n\nThe proposed “cognitive manifold” idea lacks clear empirical evidence.\n\nGains are modest relative to the method’s complexity."}, "questions": {"value": "Please reply to the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7s75xYuE1G", "forum": "4E3xru3aUp", "replyto": "4E3xru3aUp", "signatures": ["ICLR.cc/2026/Conference/Submission7549/Reviewer_Js8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7549/Reviewer_Js8s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962338404, "cdate": 1761962338404, "tmdate": 1762919643995, "mdate": 1762919643995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}