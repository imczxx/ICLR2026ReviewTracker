{"id": "5YXccEP6uc", "number": 11434, "cdate": 1758198862050, "mdate": 1759897575836, "content": {"title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "abstract": "The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences as text or convert them into images. Such approaches may be insufficient for comprehensive scientific time series understanding and generation. Existing unified time series models typically specialise in either forecasting or analysis, and their effectiveness on non-periodic, heterogeneous scientific signals remains unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12 scientific domains and 43 tasks, with over 50k+ instances, both univariate and multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz in frequency. We benchmark 17 models, including text-only LLMs, multimodal LLMs, and unified time series models, and find that general-purpose LLMs exhibit stronger generalisability than specialised time series models, while representing time series as text or images limits their performance due to excessively long sequences and loss of numerical precision, respectively. We then introduce TimeOmni, a framework that equips LLMs with the ability to understand and generate time series while remaining compatible with general-purpose LLM training. This work fills a gap in both dedicated benchmarks and modelling frameworks for scientific time series, paving the way for LLMs to understand and generate complex temporal scientific data.", "tldr": "We introduce SciTS, a comprehensive scientific time-series benchmark, and TimeOmni, an LLM-based framework for time series understanding and generation.", "keywords": ["time series", "large language model", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37e2895f543805697a132e3bdb98744be9649185.pdf", "supplementary_material": "/attachment/c1265ae90cdf81c468a1b135a3f10f8824e6f0f9.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SciTS, a large-scale benchmark for evaluating the ability of LLMs to understand and generate scientific time series across 12 domains, 43 tasks, and over 52k instances. It further proposes TimeOmni, a modular framework that integrates explicit temporal encoding, router-based patch experts, and patch reprogramming for unifying diverse scientific signals within LLM architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. SciTS fills a gap by offering a unified high-quality evaluation suite for scientific time series\n2. The benchmark includes 17 baselines across modalities with consistent metrics, which is good for cross-domain generalization and model behavior."}, "weaknesses": {"value": "1. TimeOmni uses fine-tuning while others are evaluated purely zero-shot, which would impact strict comparability. An ablation without fine-tuning would strengthen the claim.\n2. No ablation for key modules, like router, patch reprogramming, and expert families.\n3. It remains unclear how TimeOmni handles very long or high-frequency sequences beyond benchmark scales.\n4. Benchmark provenance and overlap with LLM pre-training corpora are not fully documented."}, "questions": {"value": "1. How is normalization handled across domains with very different frequency/time scales?\n2. Were all non-TimeOmni baselines strictly zero-shot? If so, could you include a variant of TimeOmni under the same condition?\n3. How stable and interpretable is the router-based patch expert mechanism during inference?\n4. Do you plan to release detailed data provenance and licensing information to ensure benchmark sustainability?\n5. Could the framework extend to spatio-temporal or higher-dimensional scientific data (e.g., radar or 3D simulations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JCaWBlp28r", "forum": "5YXccEP6uc", "replyto": "5YXccEP6uc", "signatures": ["ICLR.cc/2026/Conference/Submission11434/Reviewer_5o2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11434/Reviewer_5o2d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337277858, "cdate": 1761337277858, "tmdate": 1762922548677, "mdate": 1762922548677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper collects a comprehensive set of scientific related multimodal time series datasets, spanning various tasks including forecasting, classification, anomaly detection, etc. It benchmarks text-only LLMs, multimodal LLMs, and unified time series models and addresses the limitations of existing approaches by its proposed TimeOmni. TimeOmni fine-tunes an LLM by aligning time series representations and text representations, with separate output heads for different tasks. Extensive experiments across the benchmark show that TimeOmni achieves strong performance relative to all three categories of existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper contributes a large-scale multimodal time series benchmark covering diverse tasks and domains.\n\n2. The paper conducted broad evaluation comparing both text-only LLMs, multimodal LLMs, and unified time series models.\n\n3. The authors propose a new model that aligns time series and text, and show through extensive experiments that the model outperforms all three types of existing methods."}, "weaknesses": {"value": "1. Is TimeOmni trained and evaluated per domain, or is it trained jointly across all domains and tasks? The compared models, even for those open-source models with the same scale, are evaluated in a zero-shot setting, so it seems unfair for those models as TimeOmni has been fine-tuned on the target domains. I may have missed this part, but have the authors tried any out-of-domain testing?\n\n2. The current design does not support dynamic-length generation, which limits flexibility and scalability compared to sequence decoders that can have variable-length outputs.\n\n3. Apart from per-domain performance, are there any results on per-task performance? For example, how does TimeOmni compare with current time series models on the forecasting task? Practitioners may not care about having a general model that can handle all the tasks but more on the performance for a specific task of their interest."}, "questions": {"value": "1. Have the authors explored RL training after supervised fine-tuning?\n\n2. For multivariate time series with dimensions up to 58 in Neuroscience, would the current representations make the input length very long? \n\n3. How to compute the scores of a task if a part of samples fail this task? For example, did the authors impute zeros to compute MAPE for those failed samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F9JOYgd1Zz", "forum": "5YXccEP6uc", "replyto": "5YXccEP6uc", "signatures": ["ICLR.cc/2026/Conference/Submission11434/Reviewer_pjb9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11434/Reviewer_pjb9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807101490, "cdate": 1761807101490, "tmdate": 1762922548157, "mdate": 1762922548157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper titled “SciTS: Scientific Time Series Understanding and Generation with LLMs”  introduces SciTS, a new benchmark for scientific time series understanding and generation, spanning a number of disciplines, tasks and data with diverse signal characteristics. The authors benchmark 17 models, revealing that general-purpose LLMs often show better generalization than specialized time series models, but their performance is limited when time series are converted to text (due to long sequences) or images (due to loss of precision). To address these challenges, the paper proposes TimeOmni, an LLM-based framework that explicitly models temporal dynamics and supports both time series understanding and generation, achieving the top rank on the challenging SciTS benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark provided is pretty exhaustive for scientific time series. It consists of 52,056 instances spanning 43 domain-specific tasks across 12 scientific disciplines. It has has diversity types of data including both univariate and multivariate. It also includes various task types such as anomaly detection, classification, multiple-choice question answering (MCQ), event localization, forecasting, imputation, and synthesis\n\nThe proposed TimeOmni demonstrates its effectiveness by achieving the highest overall ranking on the challenging SciTS benchmark, underscoring the advantage of its approach."}, "weaknesses": {"value": "The paper is primarily a benchmark contribution (SciTS), with the proposed TimeOmni methodology being a secondary, and arguably incremental, focus. While the benchmark is vast, this heavy reliance on data creation means the paper's core scientific novelty may be perceived as low, as the methodological innovation is not groundbreaking.\n\nThe creation of the SciTS benchmark largely involves the collection and curation of existing open-source datasets and data from scientific domain websites, alongside some numerical simulation methods. While the combination, annotation, and unifying under a prompt-based format are novel contributions , the underlying raw time series signals themselves are largely drawn from pre-existing sources.\n\nThe proposed TimeOmni framework is an adaptation of existing LLM componentsFor instance, it uses a Patch Reprogramming module which is a concept previously presented in Time-LLM (Jin et al., 2024),  the router and patch family are basically selective resizing. \n\nThe title, \"SCITS: SCIENTIFIC TIME SERIES UNDERSTANDING AND GENERATION WITH LLMS,\" is misleading. The title should use the word benchmark because that is what it is. Potential suggestion:\n\"SCITS: A NEW BENCHMARK FOR SCIENTIFIC TIME SERIES UNDERSTANDING”"}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oe2h5VBF08", "forum": "5YXccEP6uc", "replyto": "5YXccEP6uc", "signatures": ["ICLR.cc/2026/Conference/Submission11434/Reviewer_4z7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11434/Reviewer_4z7J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880451306, "cdate": 1761880451306, "tmdate": 1762922547681, "mdate": 1762922547681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SCITS (Scientific Time Series Understanding and Generation) — a comprehensive benchmark encompassing 12 scientific disciplines, 43 tasks, and over 50,000 instances — designed to evaluate the capability of large language models (LLMs) in processing scientific time series data. The authors observe that existing multimodal LLMs either encode time series as text (leading to excessive sequence lengths) or as images (losing numerical precision). To bridge this gap, the paper proposes TimeOmni, an LLM-compatible framework that explicitly models temporal dynamics through patch experts and adaptive routing, enabling both understanding and generation of time series while remaining compatible with general LLM training. Extensive benchmarking across 17 models—including GPT-5, Gemini-2.5, Qwen3, and Moirai—demonstrates that TimeOmni consistently outperforms both general LLMs and specialised time-series models across most scientific domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The introduction of SCITS represents a major step forward for LLM-based scientific data understanding. The benchmark’s diversity—covering astronomy, neuroscience, meteorology, physiology, and more—significantly broadens the scope beyond traditional forecasting or anomaly detection benchmarks.\n2. The dual emphasis on understanding (classification, QA, anomaly detection) and generation (forecasting, imputation, synthesis) is novel, establishing SCITS as arguably the first unified evaluation for both reasoning and signal generation.\n3. The manuscript is clearly written, logically structured, and well-illustrated.\n4. TimeOmni not only achieves full task coverage but also top-1 average ranking across nearly all disciplines, demonstrating both robustness and generality."}, "weaknesses": {"value": "1. The paper briefly mentions recent works like Time-LLM and ChatTS, but deeper discussion of conceptual differences and computational efficiency trade-offs would help position TimeOmni more clearly in the landscape.\n2. The paper evaluates models in a zero-shot setting only. While this fairly tests generalisation, it leaves open the question of how much performance could improve with lightweight adaptation, e.g., LoRA or task-specific finetuning.\n3. Although the patch-expert routing and reprogramming modules are key innovations, the paper does not include ablations isolating their contributions. Quantifying improvements from each would strengthen the architectural claims."}, "questions": {"value": "1. Given that the router dynamically selects patch experts, what is the computational overhead compared to standard LLM inference? Is the training cost comparable to multimodal extensions (e.g., audio or vision encoders)?\n2. How does TimeOmni perform under few-shot or domain-specific fine-tuning? Could small-scale adaptation bridge the performance gap between open-source and closed-source LLMs?\n3. Could the authors provide quantitative ablation results on the router and patch-reprogramming components to confirm their necessity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9kEyBudvSm", "forum": "5YXccEP6uc", "replyto": "5YXccEP6uc", "signatures": ["ICLR.cc/2026/Conference/Submission11434/Reviewer_P26w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11434/Reviewer_P26w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065668092, "cdate": 1762065668092, "tmdate": 1762922547294, "mdate": 1762922547294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}