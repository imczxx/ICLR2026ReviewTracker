{"id": "pEGnJbmSUy", "number": 13137, "cdate": 1758213981260, "mdate": 1759897461638, "content": {"title": "Agent Learning via Early Experience", "abstract": "A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use).\nAs a result, most current agents rely on supervised fine-tuning on expert data, which is difficult to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios, and expose the agent to limited environment diversity.\nWe address this limitation with a middle-ground paradigm we call *early experience*: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals.\nWithin this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision making.\nWe evaluate across eight diverse environments and multiple model families.\nOur approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience.\nMoreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.", "tldr": "We propose early experience, a scalable, reward-free training paradigm where agents learn from their own rollouts. We design two methods under this paradigm-both improving success, robustness, and later RL performance across eight diverse benchmarks.", "keywords": ["Language Agents", "Data Synthesis", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ca125f56bed8da306bc0a09f238f8db4f1cff831.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out two limitations of using fixed offline datasets and collecting data (including reward signals) through interaction with the environment. It proposes two approaches for training language agents to learn and improve from their own experiences without relying on external reward signals: (1) implicitly training a world model that predicts the next state given the current state and a non-expert alternative action generated by the model, and (2) predicting the rationale for comparing the expert action with the alternative action. The authors evaluate two language models using their proposed method across eight diverse environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work introduces the concept of early experience, in which interaction data generated by the agent’s own actions provide supervision through the resulting future states, without relying on reward signals. Two simple supervision strategies are proposed, which could broaden the potential applications of language agents."}, "weaknesses": {"value": "To apply the proposed strategies, the environment dynamics need to be accessible in order to efficiently collect early experience as the behavioral agent acts in arbitrary states. This requirement may limit the range of environments to which these approaches can be applied. The paper could benefit from examples of reward-free environments where designing a dense reward function is particularly challenging. Consider moving the brief explanation of the environments to the beginning of the experiment section for improved clarity and context."}, "questions": {"value": "(1) How do the next states $s^j_i$ encode implicit feedback about action quality? Since all alternative actions differ from those in the offline dataset, how can action quality be evaluated without a reward signal, based solely on the resulting next state? Similarly, $c^j_i$ appears to represent a form of explicit feedback that functions analogously to reward signals in the Self-Reflection process.\n\n(2) In the example in Section 4.3, the reasoning appears to be partially hallucinated. Could you comment on this observation?\n\n(3) Please consider evaluating with additional baselines to better isolate the effect of each component:\n\n- Data augmentation via alternative actions without IWM. How impactful is the inclusion of IWM?\n- IWM trained with only the offline dataset.\n- Reinforcement learning with LLM-assigned numerical rewards.\n- Unsupervised RL methods.\n\n(4) Please include an evaluation using a larger model (in Section 6.2).\n\n(5) Could you provide examples of $a^j_i$ and corresponding $s^j_i$, and compare them with those of the expert?\n\n**Minors**\n\n(6) When applying your approaches, do you combine each proposed loss with the imitation learning loss?\n\n(7) Which datasets are used for training and evaluation?\n\n(8) How are the experimental results are statistically consistent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C1G7uFNzoH", "forum": "pEGnJbmSUy", "replyto": "pEGnJbmSUy", "signatures": ["ICLR.cc/2026/Conference/Submission13137/Reviewer_b21C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13137/Reviewer_b21C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760937868974, "cdate": 1760937868974, "tmdate": 1762923856393, "mdate": 1762923856393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the goal of training language agents to accomplish tasks. They identify that supervised fine-tuning is a difficult paradigm to scale in many settings. Instead, they propose what they call a middle ground between IL and RL that uses reward free data (produced by environment interaction) to improve the agent. The two methods they propose are using the data for world modeling with the LLM and \"self-reflection\" where the LLM thinks about an the actions taken and explains the rollout to generate more data and improve thinking about a task. The results show their fine tuning is able to achieve superior results to the bast models on a set of agentic LLM tasks. They also explore a set of generalization ablations on OOD tests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces an interesting paradigm for bridging supervised learning setups and reinforcement learning setups. This problem is well motivated and they provide some interesting specific ways to approach this paradigm.\n\nThe empirical results on their method for augmenting the dataset are strong.\n\nThe paper is generally well written and clear."}, "weaknesses": {"value": "Need to have CIs on all the plots and table. As well the plots at 396 could probably be a table? While the plots are pretty, I think doing it this way and shortening the y axis isn't the best way to do it.\n\nI wish you had a limitations in your discussion. In my view a large limitation here is the ability to handle longer horizon tasks. If you are just doing a single action at states that is fine, but if something takes two actions then your model will never learn it. As well, you need a model or simulation of the world right? Without that you can't reset the states for your method. As well, since we are querying an already fine-tuned model we are limited to settings in which we are already moderately performant (I think?).\n\nI'll put some questions below that I view are unclear parts of the paper that hopefully could be addressed. I'll gladly raise my score if they are addressed or I am mistaken."}, "questions": {"value": "Around 219 it is described how the dataset is created. Is only one step appended? So is this a recursive-type generation process where states are added, then we redo it on new states? Or is this a single-pass setup?\n\nAround line 251 you say you're using the same model parameters for both world modeling and policy modeling. I don't know if I believe this claim the model internalizes the dynamics implicitly. How is it done? Do we have multiple heads on the same transformer backbone? I would almost like to see just training on random states and actions as an ablation to see if the true dynamics makes any difference here or if any better initialization would work.\n\nAround 276 you're saying that you train the model to produce chain of thought thinking. I don't quite understand this setup I don't think. So is the model producing some text in \"hindsight\" and then we are training the model to predict this in a forward fashion? Would it be possible to achieve something similar to this with just prompting? Like if we just gave options and we said \"consider these options and choose the best one\"? Or would that achieve something different than the optimization here?\n\nYou need a world model for your method right? So you can easily query next states from state actions to add to your dataset?\n\nIn the 396 figure which checkpoints are these trained from (like which environment are they trained on or is it just the base llama)? To be clear here as well, this is just base GRPO from your checkpoints right? Or am I misunderstanding? \n\nHow much data did you produce for training with your early experience methods? If see you have some information in the ablation about this but for your methods it would be nice to know how much data you're producing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LQRyqlns6t", "forum": "pEGnJbmSUy", "replyto": "pEGnJbmSUy", "signatures": ["ICLR.cc/2026/Conference/Submission13137/Reviewer_LDNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13137/Reviewer_LDNN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848983541, "cdate": 1761848983541, "tmdate": 1762923856117, "mdate": 1762923856117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method called *early experience* to make language agents learn from their own exploration beyond imitating expert demonstrations.  The authors prompt the agent to predict the next state based on the current state and explorational action, and to predict the expert action as well as a sentence indicating why the action should be taken."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured. The motivation and method description are very clear.\n- The proposed method is simple and practicable."}, "weaknesses": {"value": "While the experiment results show that the method can enhance the original language agent that has not been updated during its rollouts,  there seems to be no comparison between the proposed *early experience* and other online-updating techniques. Consequently, it is unclear whether the main contribution lies in a genuinely novel approach or primarily in a combination of existing methods."}, "questions": {"value": "The effectiveness of *self-reflection* relies heavily on external LLM feedback. What will happen if we directly turn this external LLM into an agent through prompting? On the other hand, how to get proper feedback when the agent's rollout is OOD for the LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vTKuxvlH37", "forum": "pEGnJbmSUy", "replyto": "pEGnJbmSUy", "signatures": ["ICLR.cc/2026/Conference/Submission13137/Reviewer_sfTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13137/Reviewer_sfTJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905285485, "cdate": 1761905285485, "tmdate": 1762923855851, "mdate": 1762923855851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work authors improve LLM policy in an imitation learning setup, coming up with implicit world model and self-reflection modules. Important motivation of the authors work is to work without access to environment reward signal (i.e. verifiable rewards)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Perfectly reasonable idea that improves LLM policy."}, "weaknesses": {"value": "- Ultimately, I feel that lack of novelty is a most serious problem of this paper. Idea is very straight forward, even though it is perfectly reasonable. Theoretical take on limitations would improve the presentation in the novelty space.  \n- Introduction has a lot of repetition. Main points of world modeling and self-reflection were repeated many times. More tighter narrative would help the paper. \n- In 3. MDP is defined, authors should explain why in their setup MDP is a reasonable fit. MDP is not reasonable fit in much more simpler environments, such as Minecraft (see for example: Ville Tanskanen, Arto Klami and Ville Hautamäki, On the Importance of Representation in Imitating Human-like Gameplay, CoG 2025.). To me it is unreasonable to state that MDP would be suitable.  \n- Section 3.1 describes behavioral cloning (BC). It is well known work really badly when observed states were not seen in expert demos. Covariate shift will also mess it up (https://arxiv.org/abs/2102.02872). If offline IL is needed then authors should look for some form of regularized BC, such as regularized via Laplacian. \n- I dont understand that if environment is available, such as in the website booking example, then why not to utilize it to the fullest? Adversarial imitation learning schemes can be very helpful in this respect."}, "questions": {"value": "- Using (3) how long trajectories could the resulting world model plausibly predict? Authors could take a look at models such as PlaNet to see state only transitions and action conditioned state transition dynamics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "10Kucgobp7", "forum": "pEGnJbmSUy", "replyto": "pEGnJbmSUy", "signatures": ["ICLR.cc/2026/Conference/Submission13137/Reviewer_MfPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13137/Reviewer_MfPf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990009487, "cdate": 1761990009487, "tmdate": 1762923855488, "mdate": 1762923855488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}