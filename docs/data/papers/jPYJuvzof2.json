{"id": "jPYJuvzof2", "number": 7614, "cdate": 1758029377655, "mdate": 1759897843533, "content": {"title": "MemMamba: Rethinking Memory Patterns in State Space Model", "abstract": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with $O(n)$ time and $O(1)$ recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba’s long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal–vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19-PPL and Passkey Retrieval, while delivering a 48\\% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity–memory trade-off, offering a new paradigm for ultra-long sequence modeling. The code and the pre-trained models will be released upon acceptance.", "tldr": "", "keywords": ["Long-sequence modeling", "Mamba", "Memory mechanisms"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a08bb1dba997890170f10cd83e00e3c00b4695a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a new metric to evaluate the memory forgetting degree as layer and time-step going deeper and validate that their architecture with memory recalling beats baselines in long-context benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work conduct extensive experiments to show their superiority with other long-context Mamba methods."}, "weaknesses": {"value": "Experiments: model scale is limited to 100+ M. Conclusion at this scale is hard to transfer and very variable.\n\nWriting: paper writing is not clear, especially Method Sec.. and many new terminology is unnecessary (like vertical-horizontal, not see any necessity to replace the vanilla description of layer/timestep)."}, "questions": {"value": "1. can you show the effectiveness of your method in 1B+ model? (pretrain a 1B model or finetune a even larger model)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eockDiAYSI", "forum": "jPYJuvzof2", "replyto": "jPYJuvzof2", "signatures": ["ICLR.cc/2026/Conference/Submission7614/Reviewer_kXYV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7614/Reviewer_kXYV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425272370, "cdate": 1761425272370, "tmdate": 1762919697721, "mdate": 1762919697721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the memory decay problem in state-space models with a focus on the Mamba architecture and presents MemMamba, a memory-augmented variant designed to improve long-range information retention while preserving linear complexity. The authors conduct both mathematical and information-theoretic analyses to explain Mamba’s exponential memory decay and introduce two metrics, Expected Token Memory Fidelity (ETMF) and Expected Cross-Layer Memory Fidelity (ECLMF), which quantify information loss across tokens and layers. MemMamba incorporates a Note Block for dynamic state summarization together with sparse cross-token and cross-layer attention to restore salient information. Experiments on long-sequence benchmarks such as PG19 and Passkey Retrieval demonstrate improved stability and efficiency compared with Mamba and Transformer baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is timely and relevant given the growing interest in SSM-based long-sequence modeling.\n- The paper presents a clear analysis of memory decay and provides intuitive metrics (ETMF / ECLMF) to visualize horizontal and vertical information loss.\n- The proposed architecture is well-motivated and empirically improves robustness on long-context benchmarks such as PG19 and Passkey Retrieval.\n- Experimental presentation and ablations are thorough, and the writing is generally clear."}, "weaknesses": {"value": "**1. Methodological novelty is minimal within the hybrid SSM + attention family**\n\nMemMamba combines a selective state-space model with sparse cross-token and cross-layer attention, but this pattern has already been explored in Compressive Transformer, RetNet, LongMamba, and other recent variants. The Note Block functions similarly to prior compression or summarization modules, offering only minor procedural differences rather than a genuinely new architecture.\n\n**2. Theoretical analysis lacks rigor and depth**\n\nThe paper’s mathematical treatment mostly restates well-known properties of linear recurrent systems, such as exponential decay under $|A| < 1$. The proposed ETMF and ECLMF metrics are intuitive but not derived from principled information-theoretic foundations, and their empirical correlation with downstream performance remains unclear.\n\n**3. Experimental gains are not sufficiently validated against strong baselines**\n\nWhile results on PG19 and Passkey Retrieval are promising, comparisons exclude important contemporaries such as Mamba-2, RetNet, and RWKV. Parameter counts and training setups also differ, leaving uncertainty over whether improvements stem from the proposed mechanisms or from implementation choices.\n\n**4. Claimed efficiency improvements are weakly supported**\n\nThe reported 48% inference speedup is measured on a single GPU under a limited setup. No analysis is provided for scaling behavior or memory usage under multi-GPU or distributed inference conditions, making the efficiency claim difficult to generalize.\n\n**5. Writing occasionally overstates the contribution**\n\nPhrases like “breakthrough” and “new paradigm” exaggerate the paper’s significance given its incremental contribution. A more balanced presentation would strengthen credibility and highlight the genuine empirical strengths."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8m7il0VQ0X", "forum": "jPYJuvzof2", "replyto": "jPYJuvzof2", "signatures": ["ICLR.cc/2026/Conference/Submission7614/Reviewer_sU2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7614/Reviewer_sU2y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890121624, "cdate": 1761890121624, "tmdate": 1762919697270, "mdate": 1762919697270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MemMamba, a memory-augmented extension of state-space models for long sequence modeling. The authors systematically analyze memory decay patterns in Mamba with the novel horizontal–vertical memory fidelity metrics. They also introduce the state summarization with cross-layer/cross-token attention mechanisms to mitigate information loss over extended contexts. The evaluation are mainly focused on diverse tasks such as language modeling (PG19), synthetic Passkey Retrieval, and cross-document reasoning, demonstrating improvements over serveral baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a systematic analysis of memory decay in Mamba by mathematical derivations and the memory fidelity metrics (ETMF, ECLMF) in both main text and Figure 4.\n2. Paper is clearly written for the most parts, with a good contextualization within the SSM and long-sequence modeling literature."}, "weaknesses": {"value": "1. Although the paper claims to offer a fundamentally “new paradigm” for ultra-long sequence modeling, the MemMamba approach can be interpreted as a synthesis and adaptation of several established ideas (memory summarization, cross-layer attention, and sparsity in attention), rather than introduction of entirely unprecedented architectures. The degree of originality, while respectable, may be somewhat overstated in the positioning.\n2. There are a few areas for improvement in the paper presentation: for example, the clarity of Figure 3 and the font size within the figures could be further adjusted. Also, are lines 456-457 redundant with a previous paragraph? They could be removed.\n3. The description of the thresholding mechanism (e.g., for triggering note-taking and cross-attention) in Section 4 lacks a fully articulated rationale for the chosen thresholds ($\\tau_1$, $\\tau_2$). Moreover, there is no sensitivity analysis or ablation study on their values—an important omission since these could significantly influence empirical outcomes.\n4. The paper lacks a comparison of GPU memory usage between the proposed model and baseline models. Also, regarding Section 5.2 'Efficiency,' why are specific results omitted, with only a comparison between MemMamba and Transformer being presented? \n5. Potential missing related work or baseline models that should be compared in the paper:\n\n[1] Wang, Qianning, He Hu, and Yucheng Zhou. 'Memorymamba: Memory-augmented state space model for defect recognition.' arXiv preprint arXiv:2405.03673 (2024).\n\n[2] Gui, Yiyu, et al. \"EEGMamba: Bidirectional state space model with mixture of experts for EEG multi-task classification.\" _arXiv preprint arXiv:2407.20254_ (2024)."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JqSLsNTH4I", "forum": "jPYJuvzof2", "replyto": "jPYJuvzof2", "signatures": ["ICLR.cc/2026/Conference/Submission7614/Reviewer_bDYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7614/Reviewer_bDYZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968147459, "cdate": 1761968147459, "tmdate": 1762919696889, "mdate": 1762919696889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes why selective state space models like Mamba forget over distance, formalizes this with a horizontal–vertical memory fidelity framework for token-level and cross-layer information loss, and shows that Mamba’s long-range contributions decay exponentially. It then introduces an architecture that couples lightweight state summarization (“Note Block”) with cross-token and sparsely triggered cross-layer attention to retain salient signals while preserving linear time/space complexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. “Note Block” state summarization + cross-token and sparse cross-layer attention improve long-range recall while keeping O(n) time."}, "weaknesses": {"value": "1. Passkey Retrieval is a relatively simple task on in-context retrieval, and it is better to try a more difficult mutli-key-value retrival task., such as Phonebook and RULER.\n2. Missing technical details and ablations. See Questions."}, "questions": {"value": "1. The Note Block and MemMamba block rely on importance scores and dual thresholds (τ₁ for “take note,” τ₂ for cross-token attention). Are τ₁/τ₂ learned, scheduled, or fixed? Are these threshoulds sensitive for the model's performance?\n2. What exact priority metric is used for Note block? Please compare FIFO vs priority on quality/latency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ionImkQtf8", "forum": "jPYJuvzof2", "replyto": "jPYJuvzof2", "signatures": ["ICLR.cc/2026/Conference/Submission7614/Reviewer_ToNT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7614/Reviewer_ToNT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131419347, "cdate": 1762131419347, "tmdate": 1762919696454, "mdate": 1762919696454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}