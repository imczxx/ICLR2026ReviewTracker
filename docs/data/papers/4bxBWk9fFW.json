{"id": "4bxBWk9fFW", "number": 11896, "cdate": 1758204521454, "mdate": 1763033676017, "content": {"title": "Boosting Training-Free Composed Image Retrieval with Tools", "abstract": "Composed Image Retrieval (CIR) retrieves a target image that preserves the reference image’s content while applying user–specified textual edits. Training-free zero-shot CIR (ZS-CIR) has progressed by casting the task as text-to-image retrieval with pretrained vision–language models, prompting multimodal LLMs to produce target captions. However, these approaches are hindered by frozen priors and a mismatch between free-form text and the retriever’s embedding space. In this work, we introduce TaCIR, a training-free, tool-augmented agent for ZS-CIR that jointly reasons over the reference image and manipulation text, optionally consults external tools, and instantiates the inferred edit as a visual proxy. This proxy grounds implicit intent and reduces text–based retrieval misalignment by enabling also image–to-image image comparisons in the retriever. A single, tool-aware, chain-of-thought prompt emits both an initial target description and an executable tool call; when a tool is invoked, the synthesized evidence is fed back to refine the description and guide retrieval. TaCIR requires no task-specific training and remains inference-efficient. Across four benchmarks and three CLIP backbones, TaCIR yields consistent improvements over strong training-free baselines, with average gains of 2.20% to 4.16%, establishing a new state of the art for training-free ZS-CIR while providing interpretable intermediate visualizations.", "tldr": "", "keywords": ["Vision and Language", "Training-free Composed Image Retrieval", "Multimodal Large Language Model", "Multimodal Tool-used agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/98002087b5220d65fe93a7b6153679d52f621d48.pdf", "supplementary_material": "/attachment/509ffc7c40d4f420f0b51a250ff79f30ec265d93.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose a composed image retrieval framework that can leverage tools. The model uses a CoT prompt to dynamically select one tool out of two to improve the composed embedding. Experiments are conducted on four existing benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is interesting. The authors propose a new framework for composed image retrieval that can directly leverage tools.\n\n2. Experiments on sufficient datasets validate the performance of the proposed framework."}, "weaknesses": {"value": "1. The presentation is not very clear. In the main paper, the authors skip many details and defer them to the appendix, which makes the main paper read like an outline. \n\n2. It is unclear how the tool usage helps to improve the retrieval. First, whether the model can correctly select the optimal tool to be used is not clear. The authors just say they use a CoT prompt for tool selection. But I am confused whether such a prompt is the optimal. In addition, the image editing tool may change the image quality. It may produce images that are unrealistic. \n\n3. The framework only leverages two tools, which are rather limited. In addition, both tools are existing methods or functions, making the technical contribution less impressive.\n\n4. The inference time is concerning. The tool requires network access and API calls for each retrieval. The inference time can be significantly slower than existing methods that can directly be run locally,"}, "questions": {"value": "Please refer to the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IlVkPiLnGz", "forum": "4bxBWk9fFW", "replyto": "4bxBWk9fFW", "signatures": ["ICLR.cc/2026/Conference/Submission11896/Reviewer_kgDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11896/Reviewer_kgDF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911704314, "cdate": 1761911704314, "tmdate": 1762922910614, "mdate": 1762922910614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear AC and Reviewers,\n\nWe sincerely appreciate the valuable feedback and constructive comments provided by the reviewers.\n\nAfter careful consideration, we have decided to withdraw our manuscript.\n\nOnce again, we extend our gratitude to AC and the reviewers for your time and insightful suggestions.\n\nBest regards,\n\nAuthors of submission 11896"}}, "id": "MTUKbQc9Lf", "forum": "4bxBWk9fFW", "replyto": "4bxBWk9fFW", "signatures": ["ICLR.cc/2026/Conference/Submission11896/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11896/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763033674965, "cdate": 1763033674965, "tmdate": 1763033674965, "mdate": 1763033674965, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on training-free Zero-shot Composed Image Retrieval (ZS-CIR) methods that utilize Multimodal Large Language Models (MLLMs) to generate target captions during the retrieval phase. However, the authors identify two key limitations: (1) the domain knowledge of MLLMs is often insufficient to fully capture user intent, and (2) the generated target captions are not easily processed by standard retrievers. To address these issues, the authors propose TaCIR (Tool-augmented agent for training-free Composed Image Retrieval), which is a tool-augmented agent that jointly reasons over the reference image and manipulation text, optionally consults external tools, and instantiates the inferred edit as a visual proxy. The proposed method achieves consistent performance improvements across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed algorithms seem effective. It shows a large margin compared to baselines across many benchmarks.\n2. The figures are illustrated well for understanding the overview.\n3. There are many experiments and analyses that show the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The motivation of the paper is somewhat vague. For example, it would be helpful to include a simple experiment or analysis demonstrating that the target captions produced by the MLLM cannot be effectively processed by existing CLIP retrievers. \n2. The method section also seems vague, and more details should be included.\n- Regarding the domain mismatch issue, the use of knowledge acquisition and editing is reasonable. However, it is not clear how the proposed refinement procedure specifically addresses the limitation that “the target caption produced by the MLLM cannot be easily processed by the retriever.” \n- The overall procedure seems computationally intensive. However, lines 466–473 claim that the method is computationally efficient compared to previous baselines. How is this possible? The “lightweight caching strategy” mentioned there is not clearly described—where in the paper is it explained, and is this strategy applicable to the baselines as well? Clarifying these points would help assess the contribution more clearly.\n- The method also appears to be a combination of multiple API calls. The use of API calls (or reasoning step) in CIR might be first introduced in OsCIR, then the main contribution of the current paper would be the construction of a tool selection pool. A clearer distinction from prior methods is needed.\n-For the knowledge acquisition step, it is unclear which database is used. Since the performance would heavily depend on the retrieval source, further clarification is needed. If it relies on open web search, the setting may not be fair or well-controlled. \n\n3. The writing quality requires substantial improvement. Several sentences are unclear or grammatically incorrect, making the paper difficult to follow.\n- Line 154: The phrase “pixel-level hypotheses” is unclear: please elaborate or rephrase.\n- Lines 183–185: The provided example does not seem to match the figure; perhaps “manipulation text” was intended here?\n- Line 186: The sentence “while Tm is a concise, executable instruction that differs from Tm” is confusing and likely contains an error.\n- Line 186: The term editor is used but not clearly defined.\n- The terms \"implicit\" and \"explicit\" are repeated throughout the paper, but their meanings in this context remain vague"}, "questions": {"value": "Wrote above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3tyagW6630", "forum": "4bxBWk9fFW", "replyto": "4bxBWk9fFW", "signatures": ["ICLR.cc/2026/Conference/Submission11896/Reviewer_VsNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11896/Reviewer_VsNE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974780269, "cdate": 1761974780269, "tmdate": 1762922909854, "mdate": 1762922909854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a tool-augmented, training-free agent that enables multimodal large language models to use external search and image-editing tools to generate visual proxies and refined captions, thereby improving zero-shot composed image retrieval accuracy and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Motivation with Tool-use for Retrieving is Good**: The motivation for developing tool-use to improve MLLM retrieving rate is good reasonable.\n\n- **Extensive Ablations with Pipeline**: There are comprehensive ablations on different combinations and pipeline options for developing this pipeline.\n\n- **Figures and Charts are well-designed**: The explanation with figures and charts are clear and easy to follow,"}, "weaknesses": {"value": "- **Not Clear with the Challenges**: What are the challenges for developing the pipeline here? What effort do you play with the data side? post-training side? scalable training / evaluation?  If the paper only ablates a set of options with the pipeline, it is limited and hard to justify the effectiveness and hardness for this task. \n\n- **Hard to demonstrate the effectiveness beyond numbers**:  I can see some tables showing higher numbers across some benchmarks. However, this is not enough for such a practical use model. The paper should discuss more about more scalable evaluation and real-use cases and demonstrate the pipeline is transferable to the SOTA models like GPT-5 / Gemini / Claude models. From current performance improvement, I cannot see the effectiveness of the proposed pipeline."}, "questions": {"value": "What is the downstream tasks and potential use cases of this pipeline? Is this pipeline scalable? How to large scale verify? Compared to the SOTA models like Gemini 2.5 pro / GPT-5 with tools, why should we follow this pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RWUCE45vSK", "forum": "4bxBWk9fFW", "replyto": "4bxBWk9fFW", "signatures": ["ICLR.cc/2026/Conference/Submission11896/Reviewer_3So2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11896/Reviewer_3So2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762215684911, "cdate": 1762215684911, "tmdate": 1762922909313, "mdate": 1762922909313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}