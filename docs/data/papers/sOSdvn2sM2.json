{"id": "sOSdvn2sM2", "number": 5507, "cdate": 1757916602370, "mdate": 1759897970540, "content": {"title": "Diffusion Policy through Conditional Proximal Policy Optimization", "abstract": "Reinforcement learning (RL) has been extensively employed in a wide range of decision-making problems, such as games and robotics. Recently, diffusion policies have shown strong potential in modeling multi-modal behaviors, enabling more diverse and flexible action generation compared to the conventional Gaussian policy. Despite various attempts to combine RL with diffusion, a key challenge is the difficulty of computing action log-likelihood under the diffusion model. This greatly hinders the direct application of diffusion policies in on-policy reinforcement learning. Most existing methods calculate or approximate the log-likelihood through the entire denoising process in the diffusion model, which can be memory- and computationally inefficient. To overcome this challenge, we propose a novel and efficient method to train a diffusion policy in an on-policy setting that requires only evaluating a simple Gaussian probability. This is achieved by aligning the policy iteration with the diffusion process, which is a distinct paradigm compared to previous work. Moreover, our formulation can naturally handle entropy regularization, which is often difficult to incorporate into diffusion policies. Experiments demonstrate that the proposed method produces multimodal policy behaviors and achieves superior performance on a variety of benchmark tasks in both IsaacLab and MuJoCo Playground.", "tldr": "We propose a simple and efficient approach to train a diffusion policy via on-policy reinforcement learning.", "keywords": ["reinforcement learning; diffusion model; on-policy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d4e5087fff58f6ecc4351a3307a7a4f8764cb7b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Diffusion Policy through Conditional Proximal Policy Optimization (DP-CPPO), a novel on-policy reinforcement learning (RL) framework that integrates diffusion generative models with Proximal Policy Optimization (PPO). The key motivation stems from the difficulty of computing action log-likelihoods within diffusion models, which traditionally limits their use in on-policy settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The method avoids the costly recursive log-likelihood computation typical in diffusion-based RL (GenPo or DACER), replacing it with a simple Gaussian-based PPO step.\n\nUnlike prior diffusion methods (FPO), which struggle with entropy terms, DP-CPPO supports entropy regularization analytically through a Gaussian lower bound, enabling controlled exploration in policy learning."}, "weaknesses": {"value": "- The explanation at the end of 3.2 is far-fetched. EMA is a parameter smoothing but not a KL constraint.\n* Missing comparisons with recent **on-policy diffusion methods** such as GenPo and DPPO.\n* Absent results on **off-policy diffusion-RL methods** (**DIME**, **DACER**, **MaxEntDP**) evaluated on OpenAI Gym MuJoCo benchmarks.\n* Multimodal behaviors are only demonstrated qualitatively in a **Multi-Goal** environment; **no quantitative metrics** (e.g., KL-divergence, mode count, action entropy) are reported.\n* No **wall-clock performance comparison** with **FPO** under equivalent computational resources, making the claim of “high computational efficiency” unsubstantiated.\n* No **variance statistics across random seeds** (only mean and visualization curves are reported), making it difficult to assess robustness under high-variance RL training.\n\nAblations\n\n**Regularization Terms:** Independently and jointly ablate the entropy and score regularizations.\n\n**EMA Mechanism:** Evaluate performance degradation when EMA is removed.\n\n**Flow Steps:** Conduct a systematic study on how the number of flow steps affects the reward."}, "questions": {"value": "How tight is the bound of this entropy related to mutual information? How will this error affect the experimental parameter setting? Can you provide an experimental analysis?\n\nGiven that the monotonic improvement of the strategy cannot be strictly guaranteed at the end of Section 3.2, is it reasonable to regard the diffusion process as a policy iteration process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hR0FqhCKi1", "forum": "sOSdvn2sM2", "replyto": "sOSdvn2sM2", "signatures": ["ICLR.cc/2026/Conference/Submission5507/Reviewer_xrap"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5507/Reviewer_xrap"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568964050, "cdate": 1761568964050, "tmdate": 1762918100915, "mdate": 1762918100915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Comparing against Langevin Soft Actor Critic  from ICLR 2025"}, "comment": {"value": "Dear authors,\n\nThis seems like a really nice work. We wanted to point out that our ICLR 2025 paper on Langevin Soft Actor Critic [1] should be used as a baseline. In LSAC, we use Langevin Monte Carlo (LMC) based Thompson Sampling and distributional critic to achieve sample efficiency as well as multi-modal behavior through parallel tempering based LMC. Given one of the main motivations for this submission is achieving multi-modal behavior in the policy, we believe our LSAC paper is highly relevant as an alternative to diffusion policy based approaches. \n\nIn our work, we used `DSAC-T` (Duan et al., 2023), `QSM` (Psenka et al., 2024), `DIPO` (Yang et al., 2023), `SAC` (Haarnoja et al., 2018a), `TD3` (Fujimoto et al., 2018), `PPO` (Schulman et al., 2017), `TRPO` (Schulman et al., 2015), `REDQ` (Chen et al., 2021),  `DrQ-v2` (Yarats et al., 2022)) and model-based (`Dreamer` (Hafner et al., 2020)) as baselines and we got superior results than these baselines in MuJoCo tasks and DeepMind Control Suite tasks. The aforementioned baselines also include some of the requested baselines mentioned by *Reviewer wG1b*. \n\nOur codebase and all the data are publicly available here https://github.com/hmishfaq/LSAC\n\nWe would appreciate if you could provide a comparison against LSAC in your experiments.\n\nThanks!\n\n[1] Ishfaq, Haque, et al. \"Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning.\"  ICLR 2025"}}, "id": "HWlDcYMU5L", "forum": "sOSdvn2sM2", "replyto": "sOSdvn2sM2", "signatures": ["~Haque_Ishfaq1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Haque_Ishfaq1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5507/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763250823989, "cdate": 1763250823989, "tmdate": 1763250823989, "mdate": 1763250823989, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel and efficient framework to train diffusion policies in on-policy reinforcement learning. Instead of directly optimizing diffusion policies, this paper aligns the policy iteration process with the diffusion process. Specifically, at each policy improvement step,  the old diffusion policy is forzen and a Gaussian residual policy is trained via conditional PPO to optimize the combination of these two models. Then, the combination policy is distilled into a new diffusion policy using the flow matching loss. The old diffusion policy is updated utilizing the EMA technique, and the proposed policy iteration can be approximately regarded as monotonically improving. Additionally, entropy regularization and score-based regularization are incorporated to enhance exploration and training stability, respectively. Experiments on IssacLab and MuJoCo Playgroud demonstrate that the proposed method effectively learns multimodal policies and achieve superior performance across various benchmark tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a key and challenging problem—applying diffusion policies in on-policy reinforcement learning.\n2. The proposed training framework is both efficient and elegant, as it only requires optimizing the Gaussian residual policy and training diffusion models using the simple flow matching loss, thereby avoiding the intractable computation of diffusion model log-likelihoods."}, "weaknesses": {"value": "1. The paper would be strengthened by including a comparison with DPPO[1], which also employs an on-policy RL algorithm (PPO) to train diffusion policies.\n2. Moving the learning curves from the appendix to the main text would provide a clearer comparison of performance against baseline methods."}, "questions": {"value": "1. In line 268, the paper states that the score-based regularization term, which tends to let a Langevin dynamics update toward the standard Gaussian, can accelerate and stabilize training. Why such a term can accelerate and stabilize training?\n2. Is the proposed training paradigm—aligning policy iteration with the diffusion process—also applicable to off-policy reinforcement learning? Have the authors experimented with replacing PPO with off-policy algorithms when training the Gaussian residual policy? \n\n[1] Allen Z Ren, Justin Lidard, Lars L Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, and Max Simchowitz. Diffusion Policy Policy Optimization. International Conference on Learning Representation. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rMTPxgwQG1", "forum": "sOSdvn2sM2", "replyto": "sOSdvn2sM2", "signatures": ["ICLR.cc/2026/Conference/Submission5507/Reviewer_ynb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5507/Reviewer_ynb6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635784056, "cdate": 1761635784056, "tmdate": 1762918100582, "mdate": 1762918100582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel policy parameterization scheme and training algorithm. By parameterizing the new policy as a convolution of a reference policy and a conditional Gaussian kernel , the policy optimization process and the entropy regularization step are made simple and efficient. The authors demonstrate the algorithm's stability and effectiveness through extensive experiments.\n\nMain Contributions:\n\nA Novel DP-CPPO Framework: The authors propose a new reinforcement learning framework (DP-CPPO) that effectively supports the use of diffusion models in on-policy algorithms and, notably, is compatible with entropy regularization.\n\nInnovative Policy Parameterization: By parameterizing the new policy as a convolution of a reference policy and a conditional Gaussian kernel, the policy optimization process and entropy regularization are made simple and efficient.\n\nTractable Entropy Regularization: The framework naturally resolves the difficulty of computing the diffusion policy's entropy $\\mathcal{H}(\\pi_{\\theta})$. The authors achieve efficient exploration by maximizing a tractable lower bound of the entropy, namely the entropy of the Gaussian."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The main strength of this paper lies in its novel and significant methodology, which cleverly bypasses the intractable $log \\pi(a|s)$ computation in on-policy diffusion training by treating each policy iteration as a denoising step. The method is computationally efficient, with GPU memory occupation comparable to PPO while maintaining reasonable training times. Furthermore, it elegantly solves the difficult entropy regularization problem by optimizing a tractable entropy lower bound, a key feature lacking in methods like FPO. Strong empirical results, including demonstrated multi-modal capabilities, excellent benchmark performance, and ablation studies proving the necessity of all components, confirm the method's effectiveness."}, "weaknesses": {"value": "The method has several weaknesses. First, it introduces a policy fitting step (Flow Matching) after the optimization step (CPPO), which creates an approximation error whose cumulative impact on convergence is unassessed. Second, the algorithm relies heavily on an EMA approximation to ensure monotonic improvement, which is not a theoretical guarantee and may fail if policy updates are too large."}, "questions": {"value": "1. Regarding stability, how does the Flow Matching fitting error behave during training? What impact does this error have on the stability of the CPPO optimization step? Is there a risk of the fitting process lagging behind large policy updates?\n\n2. What is the effective batch size (or number of samples) used to update the flow model (Eq. 12) in each policy iteration?\n\n3. Instead of training the flow model for a fixed number of epochs in each iteration (Algorithm 1, Line 4-5), have you considered an adaptive update scheme? For instance, training the flow model until its loss (Eq. 12) converges below a specific threshold. What effect might this have on the overall algorithm's stability and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "37irkoep2J", "forum": "sOSdvn2sM2", "replyto": "sOSdvn2sM2", "signatures": ["ICLR.cc/2026/Conference/Submission5507/Reviewer_UeJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5507/Reviewer_UeJD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922315199, "cdate": 1761922315199, "tmdate": 1762918100072, "mdate": 1762918100072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a diffusion-based on-policy RL method. Concretely, given an existing policy \\(\\pi^{k}\\), a single-step Gaussian kernel is trained using the PPO objective (Equation (11)) such that \\(\\pi^{k}\\), when passed through the kernel, produces a target distribution \\(\\pi^{\\mathrm{target}}\\). Subsequently, Flow Matching is used to fit this target distribution and thereby complete training. In addition, the authors investigate score-based regularization of the Gaussian kernel to align it with a single diffusion step; ablation experiments reveal its efficacy. The work is evaluated on benchmarks such as MuJoCo Playground and IsaacLab, and demonstrates top performance compared to other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is simple yet novel: it separates the Flow Matching stage from reinforcement-learning policy improvement, by first training a Teacher policy under a familiar PPO training paradigm and then using Flow Matching to learn that Teacher policy.\n2. The ablation studies are thorough: through experiments the paper convincingly shows the importance of both the entropy regularization (which allows the method to outperform FPO) and the score-based regularization (which prevents the Teacher policy from diverging and thus makes Flow Matching feasible)."}, "weaknesses": {"value": "1. Some experimental descriptions are insufficiently clear. The meaning of “Flow” vs. “Flow + Residual” is ambiguous. The original text only uses the phrase *“diffusion-only (denotes ‘Flow’) policy and the combined policy”* (lines 390–391) without further clarifying exactly what “Residual” constitutes.\n2. Details of the training process are missing. Since the method relies on Flow Matching to fit \\(\\pi^{k}\\) after \\(p_{\\boldsymbol{\\theta}}\\), the paper should report: evidence that Flow Matching converges to the Teacher policy. Although Table 5 reports `training epochs = 15` and `mini batches = 4`, I am concerned that this is insufficient to guarantee that the Flow-Matching stage has converged to the Teacher policy.\n3. Baselines are too few. The paper uses only FPO and PPO as baselines, which limits the strength of the empirical claims. Even though the authors mention (line 382) the implementation difficulty of FPO in a Torch-based IsaacLab environment, the omission of other relevant baselines is still a drawback. For example, the following open-source diffusion-RL baselines are available:\n\n   * DACER ([https://github.com/happy-yan/DACER-Diffusion-with-Online-RL](https://github.com/happy-yan/DACER-Diffusion-with-Online-RL)) – JAX implementation\n   * QVPO ([https://github.com/wadx2019/qvpo](https://github.com/wadx2019/qvpo)) – PyTorch implementation\n   * DPPO ([https://github.com/irom-princeton/dppo](https://github.com/irom-princeton/dppo)) – PyTorch implementation\n   * DIPO ([https://github.com/BellmanTimeHut/DIPO](https://github.com/BellmanTimeHut/DIPO)) – PyTorch implementation\n     Given the availability of these implementations, it seems feasible to include them as baselines."}, "questions": {"value": "In Section 3.3 you present score-based regularization and show its empirical benefit. My question is: why is it necessary to align the one-step Gaussian kernel with a one-step diffusion update? More specifically, would a simpler regularizer such as \\(\\mathrm{KL}(p_\\theta||p_{\\theta_{old}})\\) (to prevent the kernel from making large jumps) suffice? In other words, the score-based regularizer seems somewhat analogous to enforcing a TRPO-style trust region; is the more sophisticated “score alignment” strictly required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A2eMWnND6P", "forum": "sOSdvn2sM2", "replyto": "sOSdvn2sM2", "signatures": ["ICLR.cc/2026/Conference/Submission5507/Reviewer_wG1b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5507/Reviewer_wG1b"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962194665, "cdate": 1761962194665, "tmdate": 1762918099803, "mdate": 1762918099803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}