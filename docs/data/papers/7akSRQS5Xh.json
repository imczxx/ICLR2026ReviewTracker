{"id": "7akSRQS5Xh", "number": 12337, "cdate": 1758207143106, "mdate": 1759897516037, "content": {"title": "Correlations in the Data Lead to Semantically Rich Feature Geometry Under Superposition", "abstract": "Recent advances in mechanistic interpretability have shown that many features\nrepresented by deep learning models can be captured by dictionary learning approaches such as sparse autoencoders. However, our understanding of the structures formed by these internal representations is still limited. Initial “toy-model” analyses showed that in an idealized setting features can be arranged in local structures, such as small regular polytopes, through a phenomenon known as _superposition_. However, these local structures have not been observed in real language models. In contrast, language models display rich structures like semantically clustered representations or ordered circles for the months of the year which are not predicted by current theories. In this work, we introduce Bag-of-Words Superposition (BOWS), a framework in which autoencoders (AEs) with a non-linearity are trained to compress sparse, binary bag-of-words vectors drawn from Internet-scale text.\nOur framework reveals that under restrictive bottlenecks, or when trained with weight decay, non-linear AEs linearly encode the low rank structure in the data, arranging feature representations according to their co-activation patterns. This _linear superposition_ gives rise to structures like ordered circles and semantic clusters, similar to those observed in language models. Our findings suggest that\nthe semantically meaningful structures observed in language models could arise driven by compression alone, without necessarily having a functional role beyond efficiently arranging feature representations.", "tldr": "", "keywords": ["Mechanistic Interpretability", "Superposition", "Linear Representation Hypothesis", "Feature Geometry", "Feature Manifold"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f592eec76cd30d93da457df14175e417c44b505e.pdf", "supplementary_material": "/attachment/e1c9f172d84b4a6c51d3a5d9d63025514461feb2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces BOWS, which induces the formation of complex structures of features in an autoencoder network that tries to compress and reconstruct binary bag-of-words vectors. With both a linear and nonlinear autoencoder, BOWS shows that networks do not need any goals besides compression alone in order to form these structures. The feature structures form as a result of feature correlations in the data, which are exploited by the autoencoders for compressive purposes. Experiments show the successful recovery of feature structures known to exist in LLMs (such as circular month-of-the-year features and latitude-longitude features), from the learned autoencoder representations. The authors offer a distinction between value-coding and presence-coding features, and argue that value-coding features are used to perform computation. BOWS sheds some light on the reason that LLMs form complex feature structures through superposition, arguing that feature compression, without any additional goal, is the cause."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides convincing evidence that complex structures of features can emerge purely from the task of compressing features for storage, when there are correlations in the feature distribution.\n- Figure 6 provides convincing evidence that complex structures of features form and then disappear as the latent size changes, when training a network to compress information."}, "weaknesses": {"value": "The paper is unnecessarily complex and convoluted for it's goal in helping interpretability efforts.\n- BOWS is much less a \"framework\" than an application of a pre-existing technique for demonstrating superposition, in Elhage (2022) [1], but on a custom bag-of-words dataset. Most of the \"framework\" parts of the work (techniques for analysis of results, PCA, visualization, training methodology) are inherited from Elhage (2022).\n- The framework's goal is to show that compression alone can be responsible for complex feature structures learned in LLMs, but this can already be shown by taking a PCA of the data (implicit compression), making the autoencoder analysis an unnecessarily complicated solution.\n- The distinction made between presence-coding and value-coding neurons is weak/useless. The difference as defined seems to boil down to an artifact of the way the English language treats some things as quantifiable/measurable and others as not, rather than intrinsic to an LLM's internal functioning, so it is trivially true that there exist neurons for both presence-coding and value-coding things. The \"value of the north-south coordinate\" is just \"the presence of northerliness\". While this may seem contrived, there are plenty of fuzzy edge cases to be made (\"lots of hair\" (presence) vs \"many hairs\" (value)).\n  - Altogether, I do not see why making this distinction of quantifiableness helps us understand the internal functioning of LLMs any better than before. In general, I don't see how the mathematical behavior of the autoencoder is supposed to depend on things outside of the correlation structure of the features, such as our external understanding of the feature's meaning as the presence vs value of something.\n  - If the authors want to instead define presence-coding versus value-coding based on how the features are being used, they should clarify precisely what conditions are sufficient to qualify the feature as one or the other. An example does not suffice as a definition, as in lines 343-347.\n  - The authors give no examples of how making this distinction helps us to reason about other aspects of the features. Being able to identify the features as \"value-coding\" does not help tell us how it's used. Rather, identifying how it's used allows us to label it as \"value-coding\", and there doesn't seem to be any purpose served after that.\n\n[1] Elhage, et al., \"Toy Models of Superposition\", Transformer Circuits Thread, 2022."}, "questions": {"value": "I am presuming below that as indicated in the abstract, the main goal of the paper is to show that compression alone can be responsible for complex feature structures in LLMs, when features statistics are correlated.\n- It seems like this goal is already achieved with just PCA, so why ever bother with the autoencoder?. In what case does the autoencoder reveal any structures to you that PCA/probing does not? As you mention in Section 3.1, the linear autoencoder will be recovering the top two principal components anyways.\n- It seems like you designed the autoencoder as the simplest possible example where compression can be demonstrated as the root cause for the emergence of the feature structures. Why complicate the matter with a nonlinear autoencoder? Is it to take into account the possibility that the presence of nonlinearity can disrupt the usage of the complex structures as the most efficient compressed representation?\n\nMuch of the rest of the analysis describes how the autoencoders behave, but this is not necessary in order to see that the structures are inherited from data statistics, since it is already evident from PCA and probing.\n\nFor line 397, is there converse evidence that the presence-coding features are _not_ used? Also, I find this experiment unconvincing of the importance of the difference between value-coding and presence-coding. It looks like you are defining the features that are known to be used in a particular way as the value-coding ones, and the rest as presence-coding, and then using that to argue that the value-coding ones are the useful ones, which I do not see the point of.\n\nRandom:\n- Figure 7 right: You are trying to probe one city's coordinates from an 8 dimensional vector which predicts the relative positioning between two cities. Which of the two city's coordinates are you trying to probe, and which city is plotted?\n- Figure 3 typo: (a-c) instead of (a)\n- Please try to keep the figures placed close to where they are referenced."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DRGRTJ7pVu", "forum": "7akSRQS5Xh", "replyto": "7akSRQS5Xh", "signatures": ["ICLR.cc/2026/Conference/Submission12337/Reviewer_vBLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12337/Reviewer_vBLf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760812231592, "cdate": 1760812231592, "tmdate": 1762923258835, "mdate": 1762923258835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies correlations between (input) features in a “toy models of superposition” (Elhage et al. 2022) type setting, and introduces a distinction between linear and non-linear superposition.  While Elhage et al. focused on non-linear superposition, this work argues that linear superposition is more common in real data.\n\nThe submission also introduces a Bag-of-Words Superposition (BOWS) setup, which uses co-occurrence in real documents to generate correlated binary feature vectors.  (BOWS is a bit of a misnomer since bag-of-words representations capture multiplicity, but the representation in this work does not).  The work also postulates that these co-occurrence statistics may be largely responsible for the particular geometric patterns observed in LLM representations (e.g. months being arranged in a circle), although I don’t believe this hypothesis was actually tested."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The observation that linear superposition is more common (if correct), seems important.\n\nThe BOWS setup seems like a nice, sensible approach to introducing realistic correlation structure, and has the bonus of enabling researchers to bring their knowledge of words’ semantics to the analysis of experiments."}, "weaknesses": {"value": "(major): I believe this work is in need of more rigor in establishing its central definitions and concepts (e.g. “superposition”, “linear superposition”).  For instance, I’m not sure what is the content of the statement “This explicitly shows that linear dimensionality reduction enables a form of superposition (d = 12 > m = 2) by exploiting feature correlations, without requiring any non-linearity.”  This just sounds like it’s saying “you can reconstruct an input well using a few principle components when input dimensions are highly correlated”, but that’s nothing new.  Similarly, “features in linear superposition inherit their structure from their covariance matrix” seems vacuous if linear superposition basically just means PCA.  While superposition is a more established concept, it relies (in my experience) on assuming some ground-truth set of underlying features.  In this case, I guess these are the words in the BOWS representation, but this should still be made more explicit.  I would like the discussion of superposition and linear superposition to be grounded in a rigorous mathematical exposition of the concepts.\n\n(major): The submission claims that Elhage et al. (2022) don't study linear superposition or correlated features. However, Elhage et al. do consider correlated features, and state “when there isn't enough space to represent all the correlated features, it will collapse them and represent their principal component instead”.  This is a significant mischaracterization of key related work, and makes me question the novelty of this submission.\n\n(moderate): I think some of the statements in this submission overstate the successes of mechanistic interpretability, eg:\n- “These approaches have successfully uncovered interpretable units corresponding to semantic concepts, syntactic roles, or specific input patterns.”\n- “Initial works in MI studied interpretable monosematic neurons”\nI think referring to neurons as 'interpretable' is generally accurate.  They may seem more or less interpretable, but so far I’ve not seen sufficient evidence to justify claims that we really understand what neurons are doing, except perhaps in the context of particular tasks / distributions of inputs.  This is an important distinction, as overstating the success of interpretability can provide false assurances.\n\n(minor): The related work section should do more to connect the referenced works to the submission.  \n\n\n(nit): The acronyms SDL and LRH should be introduced."}, "questions": {"value": "“However, polytope-like structures have not been observed in standard LLM activations” seems to refer specifically to Elhage et al.’s 2022 work; however, Park et al. (2025) claim to have found polytopes in the activations of Gemma.  Does this invalidate the quoted claim?  How are the polytopes discussed in Park et al. related to those from Elhage et al. (2022)?\n\nHow do the experiments in Section 5 differ from what was already done by Gurnee & Tegmark (2024)?\n\n“In contrast, the patterns in Figure 7 emerge despite inputs being uncorrelated,” Can you please make the claim about inputs being uncorrelated precise and support it?\n\nI’m not convinced that “circles and clusters resemble the global semantic organization first reported in distributional word embeddings”.  Can you elaborate on and substantiate this claim?\n\nThe abstract states: “Our findings suggest that the semantically meaningful structures observed in language models could arise driven by compression alone, without necessarily having a functional role beyond efficiently arranging feature representations.”  What would it mean for this to be the case?  Are there experiments that could falsify this as a hypothesis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ER6RQvQTYL", "forum": "7akSRQS5Xh", "replyto": "7akSRQS5Xh", "signatures": ["ICLR.cc/2026/Conference/Submission12337/Reviewer_2aH1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12337/Reviewer_2aH1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761501393485, "cdate": 1761501393485, "tmdate": 1762923258479, "mdate": 1762923258479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the geometric structure of internal features in mech interp, aiming to bridge the gap between \"toy model\" theories of superposition and observations in real-world language models. Prior work suggested superposition creates interference-managing structures (like polytopes), but empirical studies of LLMs instead find semantically-rich structures (like semantic clusters or ordered circles for months).\n\nThe authors introduce a framework called Bag-of-Words Superposition (BOWS), where a simple ReLU autoencoder is trained to compress high-dimensional, sparse, binary bag-of-words vectors derived from real text. The use of the BOW space in such analysis is interesting as it provides a proxy for the platonic, objective, or \"ideal\" embedding space for features that we cannot access.\n\nThe paper makes two key claims:\n\nIt distinguishes between non-linear superposition (as seen in toy models), which uses non-linearities like ReLU to create local structures (e.g., antipodal pairs) to manage interference between uncorrelated features, and linear superposition, which emerges when features are correlated.\n\nIn the linear superposition regime—which is induced by tight bottlenecks or, significantly, by weight decay—the non-linear AE simply learns to linearly encode the low-rank structure (i.e., the principal components) of the data's covariance matrix.\n\nThe authors demonstrate that this linear superposition mechanism is sufficient to reproduce the exact semantic clusters and circular representations seen in LLMs. This suggests these structures are a \"parsimonious\" explanation, arising merely as a byproduct of efficient compression, and may not have a specific functional, computational role."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1, The paper's primary strength is its clarity. The distinction between \"linear superposition\" (PCA on correlated data) and \"non-linear superposition\" (local, ReLU-dependent structures for uncorrelated data) is a very useful and clear conceptual framework. And the flow of the paper is natural, too.\n\n2, The BOWS framework offers a good trade-off, it's more realistic than toy models but far more controllable than a full LLM. Using it to show the emergence of semantic clusters and circles from data statistics alone is highly effective. The experiment showing that a ReLU AE transitions from a linear (PCA) regime to a non-linear (antipodal) one as the latent dimension $m$ increases is clean and convincing.\n\n3, This work provides an interesting for feature geometry. It challenges the assumption that structures like semantic clusters or \"month circles\" are necessarily functional or computationally constructed by the model. The idea that they are simply a byproduct of compression (driven by data correlations and weight decay) is a fundamental claim that the mech interp community must consider."}, "weaknesses": {"value": "1, The main limitation is the simplicity of the BOWS setup. An AE trained on static BoW vectors does not necessarily capture all the SAE variants that exist in literature today, since many of them explicitly make architectural changes to cater to the space where features live.\n\n2, Also, the paper seems to strongly implies that these structures are just byproducts and not functional. This dichotomy might be false. A model could (and likely would) exploit this emergent, PCA-driven structure for computation. Framing it as byproduct OR functional is perhaps too strong and I think the two are not mutually exclusive."}, "questions": {"value": "My question would be whether \"byproduct\" structures also be \"functional\"? Couldn't the model leverage the fact that the optimal compression scheme (PCA) naturally arranges features in a computationally convenient way (e.g., a circle for months)? Or perhaps a little bit more challenging (in future works), what happens when the low-rank structure does not facilitate (or even adverserial for) a geometry that makes underlying computation (like modular arithmetic) easier?\n\nMy question, however, does not impact my perception of this paper as a novel and solid contribution to mech interp community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BxV8AdKxLA", "forum": "7akSRQS5Xh", "replyto": "7akSRQS5Xh", "signatures": ["ICLR.cc/2026/Conference/Submission12337/Reviewer_WkYu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12337/Reviewer_WkYu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777138900, "cdate": 1761777138900, "tmdate": 1762923258123, "mdate": 1762923258123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}