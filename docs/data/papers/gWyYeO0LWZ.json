{"id": "gWyYeO0LWZ", "number": 2729, "cdate": 1757225723013, "mdate": 1759898130867, "content": {"title": "Dynamic–Static Representation Learning with Mamba-Enhanced Diffusion for Temporal Knowledge Graph Reasoning", "abstract": "Temporal Knowledge Graph (TKG) reasoning aims to predict future missing facts based on historical evidence. Prior studies on graph learning and logical rules often overlook global latent semantics and struggle with long-range dependencies, particularly under sparse or unseen facts. To address these limitations, we propose DSEE-MDiff, which frames TKG reasoning as selecting informative history and denoising future signals. Specifically, a Dynamic–Static Entity Selection encoder captures global semantic evolution alongside local structural cues, while a Mamba-based diffusion module injects and removes noise with a selective state-space model to better recover long-range dependencies and mitigate sparsity. The two outputs are fused for prediction through a ConvTransE decoder. Experiments on four public datasets demonstrate that DSEE-MDiff achieves state-of-the-art performance across multiple metrics, validating the effectiveness of the proposed approach.", "tldr": "", "keywords": ["Temporal knowledge graph", "Temporal knowledge graph reasoning", "Selective State Space Model", "Diffusion"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00bcae00b45c2be35021b53c192d4691538865ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DSEE-MDiff, a framework for temporal knowledge graph reasoning that integrates a Dynamic–Static Entity Selector (DSES) with a Mamba-based diffusion module and a ConvTransE decoder. The goal is to capture both global temporal semantics and local structural dependencies, improving extrapolation on unseen facts. Experiments on four public datasets show competitive or state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Extensive experiments on four public datasets demonstrate consistent gains.\n\nThe paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The comparison on unseen facts is weak, the baselines (RE-GCN) are old, and more baseline methods should be added.\n\n2. The definition of \"unseen facts\" is unclear.\n\n3. It is unclear why ConvTransE and RGCN are specifically chosen. Would alternative decoders significantly affect the results?\n\n4. Prior works [1][2] have already explored **global graph**, so the claim that previous methods \"face challenges in capturing global latent semantics\" seems wrong.\n\n5. Ablation studies show that removing Mamba or Diffusion causes only minor degradation.\n\n6. The paper lacks deeper justifications for why Mamba improves diffusion or how the dynamic–static selector quantitatively balances its two branches.\n\n7. The paper would benefit from an analysis of runtime and computational complexity.\n\n[1] Tirgn: Time-guided recurrent graph network with local-global historical patterns for temporal knowledge graph reasoning\n\n[2] DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0m3IS0sIP5", "forum": "gWyYeO0LWZ", "replyto": "gWyYeO0LWZ", "signatures": ["ICLR.cc/2026/Conference/Submission2729/Reviewer_WHyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2729/Reviewer_WHyA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477842532, "cdate": 1761477842532, "tmdate": 1762916350290, "mdate": 1762916350290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DSEE‑MDiff, a temporal knowledge graph reasoning model that combines a dynamic–static entity selection encoder with a Mamba‑based diffusion denoiser to capture both structural and long-range temporal semantics. Experiments on ICEWS and GDELT datasets show state‑of‑the‑art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors propose a temporal knowledge graph reasoning model that integrates a dynamic–static entity selection encoder with a Mamba‑enhanced diffusion module.\n- The proposed method improves generalization under sparse or unseen facts through a structured noise injection–denoising process.\n- Extensive experiments on four public datasets demonstrate the effectiveness and robustness of the method."}, "weaknesses": {"value": "- The novelty is somewhat limited, as it mainly combines two existing ideas—dynamic/static representation learning and diffusion-based reasoning.\n- The overall framework is complex, leading to higher computational cost and more complicated training procedures.\n- TKG reasoning benchmarks include datasets such as WIKI and YAGO. Results on these datasets would strengthen the completeness of the evaluation.\n- The paper should further clarify which component—the diffusion module or the encoder design—contributes most to performance on unseen facts.\n- In Figure 4, there is an error: MESS‑MDiff should be corrected to DSEE‑MDiff."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "r4zKUpdLAp", "forum": "gWyYeO0LWZ", "replyto": "gWyYeO0LWZ", "signatures": ["ICLR.cc/2026/Conference/Submission2729/Reviewer_thbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2729/Reviewer_thbD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621995166, "cdate": 1761621995166, "tmdate": 1762916350119, "mdate": 1762916350119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DSEE-MDiff, an encoder–decoder framework for temporal knowledge graph extrapolation. The encoder selects history via a dynamic–static entity selection module that fuses global semantic evolution with local structural cues. The decoder combines a ConvTransE scoring head with a Mamba-driven diffusion denoiser whose mean is a gated mixture of Transformer and Mamba paths. Training jointly optimizes the ConvTransE loss and the diffusion loss, and the final prediction aggregates the two scores additively. Experiments on ICEWS14, ICEWS18, ICEWS05-15 and GDELT report competitive performance with state-of-the-art results on three benchmarks. The paper includes ablations, sensitivity studies on historical window and diffusion sequence length, and an unseen-facts evaluation with a short case study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe method design is coherent. The paper specifies the diffusion reverse process and the gated combination of Transformer and Mamba, then connects these to ConvTransE decoding and a joint objective. This facilitates reproducibility. \n2.\tThe aggregation rule is explicit. The final probability is defined as the sum of ConvTransE and diffusion scores, which clarifies inference behavior. \n3.\tEvaluation breadth. Four datasets are covered with a large baseline set, and the main table uses time-aware metrics for MRR and Hits. \n4.\tInformative ablations. Removal of the dynamic selector causes a larger drop than removal of the static selector, and removing either Mamba or Transformer degrades performance to a similar degree. The decoder removal further confirms the contribution of ConvTransE. \n5.\tSensitivity and unseen-facts analysis. The paper varies history window and diffusion sequence length, and constructs unseen-facts test splits for ICEWS14 and ICEWS18 with comparisons to two representative baselines."}, "weaknesses": {"value": "1.\tFusion calibration and design space remain under-explored. The paper fixes aggregation to a simple sum of Sct and Sdiff, without reporting variants with a learned weight or confidence calibration of either head. This limits understanding of whether the improvement comes from complementary information or uncalibrated score addition. A small study that tunes a scalar weight or reports calibration errors would clarify this point. \n2.\tBehavior of the gating variable is not analyzed. The denoiser mean uses a learnable gate that mixes Transformer and Mamba outputs, yet the paper does not report statistics of the gate across diffusion steps or datasets, nor whether it saturates to one path in specific regimes. Summaries such as per-step averages or histograms would illuminate how the gate allocates responsibility. \n3.\tDefinition and protocol of time-aware metrics are not explained in the main text. The main results table states that time-aware metrics are used, but the manuscript does not define the metric computation in the main body or verify parity with baselines there. A concise definition and protocol confirmation in the main section would remove ambiguity. \n4.\tResource profile is not presented. The experiments section reports accuracy but does not include parameter counts, FLOPs, wall-clock training time, inference latency, or memory consumption per dataset. Given the additional diffusion head and dual-path denoiser, such metrics would help assess practical efficiency. \n5.\tAnalysis of the GDELT gap is brief. The text notes that performance on GDELT is slightly below a diffusion baseline, yet there is no breakdown by relation type or sequence length to explain the gap. A diagnosis connected to dataset properties such as snapshot density or sequence length would make the result more actionable. \n6.\tDiffusion dynamics are only indirectly discussed. The paper studies sequence length but does not visualize how representations evolve along diffusion steps or how the denoiser removes noise. Trajectory plots or cosine-similarity traces across steps would provide concrete insight into stability and information retention."}, "questions": {"value": "1.\tHow sensitive is performance to the fixed additive fusion. Please report results when a single scalar weight multiplies the ConvTransE score and the diffusion score, and include a short calibration analysis of both heads. \n2.\tWhat does the gating variable learn across diffusion steps and datasets. Please provide summaries of the gate values, such as per-step averages and dispersion, and discuss conditions under which one path dominates. \n3.\tCould you define the time-aware metrics in the main text and confirm protocol parity with all baselines. A brief statement on filtering, ranking direction, and temporal handling would resolve ambiguity in Table two. \n4.\tWhat is the full cost profile of DSEE-MDiff. Please report parameters, FLOPs, wall-clock training time, inference latency, and peak memory on each dataset, and compare these numbers with at least one diffusion baseline and one contrastive baseline. \n5.\tCan you analyze the GDELT gap in more detail. A breakdown by relation family and a study of performance versus sequence length or snapshot density would help determine whether the diffusion head or the selector is the bottleneck. \n6.\tWould you visualize diffusion dynamics. Plots of representation trajectories or similarity to clean targets across steps, and a comparison of Transformer-only versus Mamba-only denoisers, would clarify how the selective state-space path contributes beyond depth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "emm3nO2Jbq", "forum": "gWyYeO0LWZ", "replyto": "gWyYeO0LWZ", "signatures": ["ICLR.cc/2026/Conference/Submission2729/Reviewer_YTPf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2729/Reviewer_YTPf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833575877, "cdate": 1761833575877, "tmdate": 1762916349500, "mdate": 1762916349500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for temporal knowledge graph reasoning, aiming to address the shortcomings of existing methods in capturing global semantics and long-range dependencies, especially in the prediction tasks of sparse or unseen facts. The paper presents the DSEE-MDiff framework, which includes three core modules: Dynamic-Static Entity Selection Encoder, Mamba-Driven Diffusion Module, and ConvTransE Decoder. Their main contributions are: for the first time, combining a dynamic-static selection encoder with a Mamba-enhanced diffusion model for TKG reasoning, and designing a dynamic-static selection mechanism to adaptively fuse global semantic and local structural information. Their experimental results achieved the competitive performance on four public datasets, with significant improvements especially in the MRR and Hits@1 metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrates dynamic-static semantics with the Mamba diffusion model, leveraging the selective state space mechanism of the Mamba diffusion model to more effectively retain key information in long sequences.\n2. The Mamba model, which is inherently good at capturing dependencies in long sequences during the denoising process, is used to solve the long-distance dependency problem in TKG reasoning, making up for the defect of information attenuation in traditional RNN/Transformer when processing long historical windows.\n3. Diffusion models essentially learn the generative process of data distribution by injecting and removing noise. This mechanism can \"create\" reasonable representations of facts that have not been seen during training, greatly enhancing the ability to generalize and reason about sparse and unseen facts."}, "weaknesses": {"value": "​\t1. How does the dynamic entity selection encoder implement dynamic encoding? When selecting multi-hop neighbors and multi-hop relationships, are the number of hops and the number of relationships fixed?\n\n​\t2. The function of the dynamic entity selection encoder to highlight informative entity signals requires further explanation.\n\n​\t3. What does the frequency signal in Formula 8 refer to?\n\n​\t4. In line 203, \"d\" does not appear in the previous text.\n\n​\t5. Why were other newer baseline models not chosen for comparison? For example, \"DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node Diffusion Model with Dual-Domain Periodic Contrastive Learning\" and \"Temporal Knowledge Graph Extrapolation via Causal Subhistory Identification\".\n\n​\t6. When evaluating DSEE-MDiff's ability to handle uncertainties arising from rare or unprecedented facts, what are the statistics of the training and test sets of ICEWS14 and ICEWS18, and how were they constructed? Without the proportion of unseen factual data, it cannot be stated that \"under the higher uncertainty of sparse settings, DSEE-MDiff shows greater robustness.\"\n\n​\t7. In Section 5.5, only comparisons with DiffuTKG and RE-GCN are made. Some newer models should be selected for comparison, such as DPCL-Diff.\n\n​\t8. Lack of analysis on the time cost of model training.\n\n​\t9. The effect of DSEE-MDiff on GDELT is not obvious, and there is a lack of in-depth analysis of the experimental results.\n\n​\t10. The motivation for introducing Diffusion and Mamba to temporal knowledge graph reasoning (TKGR) is not sufficiently deep, and the distinction from existing Diffusion TKGR methods is unclear."}, "questions": {"value": "Please refer to Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZnWPE35pkV", "forum": "gWyYeO0LWZ", "replyto": "gWyYeO0LWZ", "signatures": ["ICLR.cc/2026/Conference/Submission2729/Reviewer_cYVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2729/Reviewer_cYVx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896267289, "cdate": 1761896267289, "tmdate": 1762916348818, "mdate": 1762916348818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}