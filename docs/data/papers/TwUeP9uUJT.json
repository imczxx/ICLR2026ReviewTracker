{"id": "TwUeP9uUJT", "number": 2588, "cdate": 1757157203851, "mdate": 1759898139125, "content": {"title": "HHW-Ego: DSLR-Quality Enhancement for Multi-Source Wearable Ego Imaging", "abstract": "Wearable ego-centric images are now in high demand for scenarios ranging from daily smart glass usage to embodied intelligence. But the image quality is far behind smart phone due to the lacking paired high-quality reference images and the dedicated enhancement systems. To solve this, a customized degradation pipeline is designed to generate paired samples matching wearable camera images, boosting the upper limit of enhancement performance. Besides a two-stage enhancement framework is further built: first, tuning an efficient model on the paired dataset to enhance real wearable images; second, using hyperspectral data to refine color temperature for better quality. Moreover we present HHW-Ego, a multi-source paired dataset integrating with Hyperspectral, High-dynamic range and Wearable Ego-centric data, which includes hundreds of image groups spanning indoor/outdoor and day/night scenes. Experiments show the framework effectively enhances images of varying quality, and matches DSLR camera quality in specific scenarios.", "tldr": "", "keywords": ["Wearable Ego Imaging; Multi-Source Dataset; Hyperspectral Enhancement"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b29107365b43b9fd92ce731df2587fdea2b9a05c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses wearable Ego imaging. The authors design a customized degradation pipeline to generate paired samples that match wearable camera images, with the goal of raising the upper bound of enhancement performance. In addition, the authors construct a two‑stage enhancement framework and introduce HHW‑Ego, a multi‑source paired dataset that fuses hyperspectral data, high dynamic range, and wearable first‑person imagery."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper contributes HHW‑Ego, a relatively rare multi‑source dataset. The data collection pipeline is described in detail and appears credible.\n\n2. The use of hyperspectral data for further color correction is supported by relatively in‑depth theoretical analysis.\n\n3. The presentation is nice."}, "weaknesses": {"value": "1. The approach used in the first stage is similar to existing super‑resolution models (e.g., Real-ESRGAN). The authors claim the novelty lies in adding LAB correction in the data pipeline, but the reviewer finds this insufficiently novel. The authors should clarify the uniqueness and suitability of this modification specifically for wearable imaging.\n\n2. In the last row of Figure 3, the results produced by the method appear to exhibit color shifts (see the lighting region).\n\n3. In the main Table 2, the method does not seem to improve SSIM and PIQE. On the contrary, both metrics degrade after applying the method. The authors are encouraged to analyze the reasons for this.\n\n4. The additional prior from HSI data is presented as the main innovation of the method. However, in the second row of Table 3 this leads to performance degradation; the reviewer considers the corresponding gain to be marginal.\n\nAn execllent response will rise the score."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4jfx5SETGV", "forum": "TwUeP9uUJT", "replyto": "TwUeP9uUJT", "signatures": ["ICLR.cc/2026/Conference/Submission2588/Reviewer_Fehf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2588/Reviewer_Fehf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761018955363, "cdate": 1761018955363, "tmdate": 1762916290938, "mdate": 1762916290938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A custom degradation pipeline generates matching paired samples, raising enhancement limits.\nA two-stage enhancement framework is proposed.\nHHW-Ego is proposed, a multi-source paired dataset integrating Hyperspectral, HDR, and Wearable data.\nExperiments show the framework effectively enhances images."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Customized Degradation Pipeline for Paired Data Creation.\n\n2. The paper introduces a novel two-stage enhancement framework.\n\n3. Introduction of HHW-Ego: A Comprehensive Multi-Source Dataset.\n\n4. Writing is easy to follow."}, "weaknesses": {"value": "1. It is not clearly stated whether the baseline models in Table 2 were trained on wearable camera datasets or used as official pre-trained models. \n\n2. In Table 2, the \"Ours\" results should be placed closer to the corresponding baselines for easier comparison. Better visual cues, such as bolding the best results, would also improve readability (Same in Table 3).\n\n3. The paper mentions terms like color fidelity and spectral consistency without clear definitions or explanations. These concepts should be explained, especially for readers unfamiliar with them.\n\n4. The paper claims improvements in color fidelity and spectral consistency but does not provide visual examples to support these claims. Comparative images would help validate the effectiveness of the method. The conclusion in Section 5.5 (e.g., Lines 433–436) is not strongly supported. It claims color fidelity improvements from hyperspectral integration, but only shows numerical results in Table 3 without a clear visual presentation.\n\n5. The introduction does not clearly explain why integrating hyperspectral and HDR data helps improve color fidelity, spectral consistency, and detail. The motivation feels disjointed and unclear."}, "questions": {"value": "1. In Figure 3, what baseline method is used for comparison with \"Ours\"? Please clarify.\n\n2. When computing SSIM, what does \"the original input\" refer to? Is it the low-quality input?\n\n3. Can you create some paired test data using the same degradation method as the training data to make the evaluation more reliable?\n\n4. The metrics used are insufficient. Could you add more representative no-reference metrics, such as MUSIQ?\n\n5. There is a grammatical issue in Lines 447–449. Please revise the sentence for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1u6x2euLP", "forum": "TwUeP9uUJT", "replyto": "TwUeP9uUJT", "signatures": ["ICLR.cc/2026/Conference/Submission2588/Reviewer_PtW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2588/Reviewer_PtW8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827524004, "cdate": 1761827524004, "tmdate": 1762916290700, "mdate": 1762916290700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses wearable ego-centric image enhancement by proposing a two-stage framework that leverages a customized degradation pipeline for data generation and hyperspectral-guided color refinement, supported by a novel multi-source paired dataset (HHW-Ego) of hyperspectral, HDR, and wearable data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This work tackles a problem of significant and growing practical importance: the quality gap between wearable ego-centric cameras and consumer-grade DSLR/cellphone cameras. As wearable devices become ubiquitous for first-person documentation and embodied AI, this research directly addresses a critical bottleneck in visual quality, with clear applications in augmented reality, assistive technology, and lifelogging.\n\n2. A key strength is the creation of the HHW-Ego dataset, which fills a critical gap in the community. By providing meticulously paired hyperspectral, HDR, and wearable data across diverse scenes, it offers an invaluable resource not only for supervised image enhancement but also for fundamental research in color science and illumination modeling under real-world conditions.\n\n3. The proposed two-stage framework presents a pragmatic and well-structured solution. The combination of a data degradation pipeline with a spectral-based color refinement module constitutes a solid engineering approach for this task."}, "weaknesses": {"value": "**1. Lack of Critical Implementation Details**\n\nThe manuscript omits several key technical details, which hinders reproducibility and a full understanding of the proposed method.\n\n**Unexplained Module**: The exposure correction module, which is first mentioned and evaluated in the ablation study (Section 5.5), is not described in the methodology section. Its role and implementation remain unclear to the reader.\n\n**Unspecified Image Processing Pipeline**: The description of the data processing pipeline is ambiguous. It is critical to clarify whether the proposed data preprocessing and enhancement framework operates directly on RAW data or on post-processed sRGB images generated by a separate ISP. \n\n**Incomplete Figure Annotation**: Figure 1 is cited as containing \"red solid lines\" and \"black dashed lines,\" which are not present in the provided figure, leading to confusion.\n\n**Undefined Hyperparameter**: The introduction of the \"strength\" parameter in the CCT map calculation (Line 249) is not sufficiently motivated. Its impact on the final results and the rationale for its specific value are not discussed.\n\n**2. Insufficient Evidence for Key Claims**\n\nThe experimental evidence does not yet solidly support some of the paper's central claims regarding its unique contributions.\n\n**Necessity of Color Alignment**: The authors claim that a customized degradation pipeline is a core contribution, with the unique step being color alignment in Lab space. However, the performance gain from this step appears marginal in Table 4. The authors should provide stronger ablation results or analysis to conclusively demonstrate its necessity and unique benefit over other common color jittering techniques.\n\n**Benefit of Hyperspectral Guidance**: The claim that the hyperspectral-guided stage enhances color fidelity is not fully validated.\n\nThe evaluation relies solely on general image quality metrics (e.g., PSNR, SSIM), which are not sensitive enough to isolate improvements in color accuracy.\n\nThe visual comparisons provided do not clearly demonstrate superior color reproduction compared to the baselines.\n\nTo substantiate this claim, the ablation study (Table 3) should be supplemented with color-specific metrics and targeted visual examples that unequivocally show the color improvement brought by the hyperspectral data."}, "questions": {"value": "1. What's the implement of exposure correction module ?\n2. Please clarify whether the proposed data preprocessing and enhancement framework operates directly on RAW data or on post-processed sRGB images generated by a separate ISP. \n3. What's the value and effect of the \"strength\" parameter in the CCT map calculation (Line 249) ?\n4. Any other stronger justification to validate the necessity of color alignment in degradation pipeline?\n5. Any further evidence to validate the Benefit of Hyperspectral Guidance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cgW1EPYro7", "forum": "TwUeP9uUJT", "replyto": "TwUeP9uUJT", "signatures": ["ICLR.cc/2026/Conference/Submission2588/Reviewer_AsvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2588/Reviewer_AsvG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984774174, "cdate": 1761984774174, "tmdate": 1762916290535, "mdate": 1762916290535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}