{"id": "df4mr7eQg7", "number": 14772, "cdate": 1758243370948, "mdate": 1759897350131, "content": {"title": "Class-Wise Disparity in Adversarial Training: Implicit Bias Perspective", "abstract": "Disparities in class-wise robust accuracies frequently arise in adversarial training, where certain classes suffer significantly lower robustness than others, even when trained on balanced data. This phenomenon has been identified and termed robust fairness in prior work, highlighting the challenge of ensuring equitable robustness across classes.\nIn this work, we investigate the root causes of such disparities and identify a strong correlation between the norms of head parameters (i.e., the last layer’s weights) and class-wise robust accuracies. Our theoretical and empirical analyses show that adversarial training tends to amplify these disparities by disproportionately affecting head norms, which in turn influence class-wise performance.\nTo address this, we propose a simple yet effective solution that mitigates these imbalances by directly fine-tuning the head parameters while keeping the feature extractor fixed. Unlike existing methods that rely on class reweighting or remargining strategies, our approach requires no validation set and introduces minimal computational overhead.\nExperiments across various datasets and architectures demonstrate that our method significantly reduces disparities in class-wise robust accuracies without degrading overall performance, providing a practical and principled step toward improving robust fairness in adversarial learning.", "tldr": "", "keywords": ["Robust fairness", "adversarial robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/103334811e7860aca6fd524d3d78a33960a01e70.pdf", "supplementary_material": "/attachment/1b3c01a4cc7a6354d7ae05cef7335e6fdfbb23ec.zip"}, "replies": [{"content": {"summary": {"value": "This paper systematically investigates the problem of class-wise robustness disparity that is prevalent in adversarial training, and reveal that the root cause of this disparity stems from the strong correlation between the classifier head parameter norm and class-wise robustness. To this end, the authors propose two low-cost mitigation methods, HWNwB and DecoSAM, to alleviate the head norm imbalance and enhance the worst-class robustness, respectively. The effectiveness of the proposed methods is validated on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Theoretical support. The paper explains at the theoretical level that a larger classifier head norm can lead to an increase in robustness disparity.\n2. Empirical validation. The positive correlation between the classifier head norm and class-wise robust accuracy is empirically demonstrated through statistical correlation analysis."}, "weaknesses": {"value": "1. Lack of performance stability. HWNwB and DecoSAM vary significantly in effectiveness under different attacks and experimental settings. The former is more effective for within-training PGD metrics and score equalization, while the latter performs better on AA and Worst-Class (WC). This difference raises doubts about which method to choose for practical applications.\n2. Insufficient data size. Although the authors show that the near-zero WC on ImageNet is not favorable for fairness assessment, the experimental results can be verified on a medium-sized dataset such as Tiny-ImageNet-200 / ImageNet-100.\n3. Lack of discussion on different perturbation budgets. Both training and evaluation are basically fixed at ℓ∞, ε = 8/255 (PGD and AA). The lack of experiments under different budgets (e.g., ε ∈ {4, 8, 16}/255) and different norms (ℓ₂) makes it difficult to judge the robustness of the methods under varying attack strengths and norms.\n4. Insufficient discussion of model architectures. The current experiments focus on the WRN family and lack experiments on mainstream architectures such as ViT/DeiT/ConvNeXt, which makes it difficult to assess the utility of the defense methods on the latest architectures.\n5. Some theoretical assumptions are strong. The derivation relies on assumptions such as “ψ(x_adv) ≈ ψ(x)”, which may not hold under strong attacks or distributional bias scenarios. The authors need to explicitly cite the assumptions or have an explicit analysis of the assumptions to prove that the assumptions hold.\n6. Lack of comparison with other methods for addressing AT fairness. Although the paper discusses class-wise disparity and mentions its potential correlation with robust fairness, there is a lack of comparison and discussion of existing fair adversarial training methods at the experimental and methodological level."}, "questions": {"value": "Please examine the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pr3jSmMnH2", "forum": "df4mr7eQg7", "replyto": "df4mr7eQg7", "signatures": ["ICLR.cc/2026/Conference/Submission14772/Reviewer_551Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14772/Reviewer_551Q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984745943, "cdate": 1761984745943, "tmdate": 1762925125719, "mdate": 1762925125719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates class-wise robustness disparities in adversarial training, where some classes become significantly less robust than others despite balanced data. The authors identify a strong correlation between the norms of classifier head weights and class-wise robust accuracies, showing that adversarial training implicitly amplifies these norm imbalances, leading to uneven robustness. To address this, they propose Head Weights Normalization with Bias (HWNwB) and Decoupled Sharpness-Aware Minimization (DecoSAM), which adjust only the classifier head while keeping the feature extractor fixed. Extensive experiments across multiple datasets and adversarial training algorithms demonstrate that these methods substantially reduce class-wise robustness gaps with minimal computational cost and without degrading overall robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper offers a novel and well-motivated perspective on class-wise disparity in adversarial training by interpreting it as an implicit bias problem related to head-weight norm imbalance. This viewpoint goes beyond existing fairness or reweighting approaches.\n2. The proposed methods, HWNwB and DecoSAM, are simple, lightweight, and practical. HWNwB requires no additional training, while DecoSAM involves only one epoch of head-only fine-tuning, making them computationally efficient.\n3. Extensive experiments across multiple datasets demonstrate consistent improvements in worst-class robustness and reduced class-wise disparity, with minimal impact on average accuracy or overall robustness."}, "weaknesses": {"value": "1. While the paper provides strong correlation evidence, it does not offer a full causal analysis showing whether head norm imbalance is the root cause or merely a symptom of deeper optimization dynamics.\n\n2. HWNwB and DecoSAM are both post-hoc or head-only fine-tuning methods, meaning they depend on an already adversarially trained model. The paper does not explore whether integrating these ideas directly into the training process could yield better or more stable results.\n\n3. The empirical improvements, while consistent, are moderate in some cases; the gains in worst-class robustness often come with small drops in clean or average accuracy, which could be discussed more thoroughly.\n\n4. The paper aims to study and improve robust fairness methods; however, the main experimental tables (e.g., Tables 2 and 3) mainly compare the proposed approaches with standard adversarial training baselines rather than with existing robust fairness methods. Moreover, in Table 4, it is not clearly specified which adversarial training algorithms were used as the underlying models for comparison.\n\n5. The claim that the method is compatible with a wide range of adversarial training algorithms makes the contribution appear less distinctive.\n\n6. The contribution stating that the paper theoretically and empirically demonstrates that adversarial training induces norm imbalances leading to class-wise performance disparities seems less novel, as a similar phenomenon has already been analyzed in prior work such as FRL."}, "questions": {"value": "1. It would be interesting to investigate whether integrating HWNwB and DecoSAM directly into the training process, rather than applying them post hoc, could lead to better or more stable results.\n2. It is not clearly explained what motivated the authors to focus specifically on the classifier head; the paper does not sufficiently justify why the head layer, rather than other components of the model, was chosen as the central point of analysis for class-wise disparity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kEZ5RTwKqe", "forum": "df4mr7eQg7", "replyto": "df4mr7eQg7", "signatures": ["ICLR.cc/2026/Conference/Submission14772/Reviewer_vsQN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14772/Reviewer_vsQN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081119362, "cdate": 1762081119362, "tmdate": 1762925125230, "mdate": 1762925125230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of **class-wise disparity in adversarial training**, where different classes show varying robustness even in balanced datasets. The authors reveal a strong correlation (ρ ≈ 0.95) between **the L₂-norms of class-specific head weights** and **their robust accuracies**, suggesting that adversarial optimization introduces an implicit bias into the classifier head.\n\nThey formalize this phenomenon theoretically by linking class hardness to gradient gaps and head-norm growth, then propose two lightweight solutions to mitigate it:\n\n- **HWNwB (Head Weights Normalization with Bias)**: post-training normalization of classifier heads while preserving bias terms.\n- **Deco-SAM (Decoupled Sharpness-Aware Minimization)**: adaptive class-wise fine-tuning that balances robustness through SAM-based optimization.\n\nExtensive experiments across multiple datasets (CIFAR-10/100, STL-10, OfficeHome) and adversarial training methods (PGD-AT, TRADES, MART, ARoW) show significant reductions in fairness disparity with minimal cost and no validation set required."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Establishes a clear theoretical link between class hardness, gradient gaps, and head-norm disparity.\n- Combines theoretical rigor with thorough empirical validation.\n- Introduces two lightweight, algorithm-agnostic mitigation strategies requiring no validation data.\n- Demonstrates consistent fairness improvements across datasets and training methods.\n- Writing, figures, and appendix materials are clear and reproducible."}, "weaknesses": {"value": "- Experiments cover small–to–mid-scale benchmarks (CIFAR, STL-10, OfficeHome) but lack validation on **large-scale settings** (e.g., ImageNet-like regimes), so scalability and generality remain untested.\n- Robustness evaluation is primarily based on PGD and AutoAttack, which already provide strong coverage. However, including at least one additional optimization-based (e.g., CW) or adaptive attack (e.g., BPDA/EOT) would make the evaluation more comprehensive and confirm that the improvements are not attack-specific.\n- Robustness evaluation fixes the perturbation budget at ε=8/255 and focuses on PGD / AutoAttack. Including **ε-sweep experiments** (e.g., evaluating robustness across different attack magnitudes) would clarify how stable the proposed fairness improvements remain as adversarial strength increases."}, "questions": {"value": "- **Deco-SAM hyperparameter clarity.** The paper lacks detail and sensitivity analysis for **τ**. Could you report the default τ, the rationale for its choice, and a brief ablation (e.g., τ ∈ {…}) showing how **worst-class accuracy**, **class-wise variance**, and **PGD/AA robustness** change across τ and learning-rate schedules?\n- **ε-sweep robustness.** Experiments fix the perturbation budget at **ε = 8/255** under PGD-20 and AutoAttack. Have you evaluated whether the **fairness improvements persist across different ε values** (i.e., varying attack strengths)? A compact ε-sweep would clarify the stability of the effect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nM7xJw80EB", "forum": "df4mr7eQg7", "replyto": "df4mr7eQg7", "signatures": ["ICLR.cc/2026/Conference/Submission14772/Reviewer_Zd1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14772/Reviewer_Zd1o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089314769, "cdate": 1762089314769, "tmdate": 1762925124553, "mdate": 1762925124553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the class-wise performance disparities in adversarial training: despite balanced class frequencies, some classes end up with much lower robust accuracy than others. The authors identify a empirical correlation between the ℓ₂-norms of the classifier head weights and class-wise robust accuracy, where classes with larger head norms tend to have higher robustness. They show that adversarial training exacerbates this norm imbalance, and that this drives the disparity across classes. To mitigate it, the authors propose two lightweight methods that adjust or fine-tune the head parameters with the feature extractor frozen. Experimental results show reductions in class-wise disparity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors reframe class-wise robustness gaps through an implicit bias in head-weight norms, revealing a tight link between last-layer weight norms and per-class robust accuracy—a fresh angle beyond data imbalance or attack heuristics.\n\nThe authors provide theoretical and empirical evidence tying adversarial training to growing head-norm disparities and, in turn, to uneven robust accuracy across classes."}, "weaknesses": {"value": "Disparities in class-wise robustness have already been extensively studied both theoretically and empirically [1–3]. While the observed correlation between the ℓ₂-norms of classifier head weights and class-wise robust accuracy is interesting, it is unclear how is the identified correlation can be related with existing findings, and the paper’s contribution appears incremental relative to prior analyses that connect class margins, logit norms, and adversarial robustness (it is more like a different perspective of the same issue, rather than discovering a under-discovered issue in the disparities of class-wise robustness). Moreover, based on the presented experiments, the proposed method neither clearly outperforms existing approaches nor demonstrates strong complementarity with them.\n\nSeveral claims seem over-stated or implicit. For example, why is \"no validation set required\" an important issue? Especially in terms of adversarial training, the usage or acquisition of a validation set seems trivial.\n\nSeveral important assumptions are deferred to the Appendix. I would strongly suggest the authors to include the assumptions in the main paper with proper justifications. Otherwise, the theortical claims risk being misinterpreted.\n\nThe discussions seem limited to $l_{∞}$ attacks. Robustness under stronger or diverse attacks (AutoAttack, multi-target) is underexplored.\n\n[1] Xu, Han, et al. \"To be robust or to be fair: Towards fairness in adversarial training.\" International conference on machine learning. PMLR, 2021.\n\n[2] Ma, Xinsong, Zekai Wang, and Weiwei Liu. \"On the tradeoff between robustness and fairness.\" Advances in Neural Information Processing Systems 35 (2022): 26230-26241.\n\n[2] Wei, Zeming, et al. \"Cfa: Class-wise calibrated fair adversarial training.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not identify remarkable ethical concerns."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jo4AOIBAg1", "forum": "df4mr7eQg7", "replyto": "df4mr7eQg7", "signatures": ["ICLR.cc/2026/Conference/Submission14772/Reviewer_cmu4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14772/Reviewer_cmu4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141488877, "cdate": 1762141488877, "tmdate": 1762925124157, "mdate": 1762925124157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}