{"id": "4lAocDKU1A", "number": 7605, "cdate": 1758029227741, "mdate": 1759897843995, "content": {"title": "TrajP-L: A Trajectory-Based Plugin with LoRA for Sampling Direction Correction in Distilled Diffusion Models", "abstract": "Diffusion models (DMs) have shown remarkable capability in image synthesis, yet they typically require hundreds of sampling steps to produce high-quality outputs. To alleviate this inefficiency, prior work has explored distilling the sampling trajectories of pre-trained models. However, these approaches often disrupt the original parameter space and incur substantial distillation training costs. Recent findings suggest that the sampling space of DMs can be effectively captured by as few as three basis vectors, with the resulting low-dimensional trajectories exhibiting strong structural similarity. Based on this insight, we propose TrajP-L (Trajectory-Based Plugin with LoRA), a trajectory similarity-based learnable plugin. It achieves efficient DMs distillation via the synergy of LoRA and a trajectory correction module (TrajP). Specifically, we construct a student model by combining LoRA with the weights of a pre-trained model to initialize the distillation process. We then extract the coordinate information of the current and next sampling timesteps from a fitted 3D trajectory representation, and employ TrajP to refine the student’s sampling direction. Extensive experiments show that TrajP-L requires only a small number of sampling trajectories for fine-tuning, while substantially mitigating discretization errors. For example, on CIFAR-10, TrajP-L trains on merely 5k trajectories for 10 minutes on a single NVIDIA RTX 3090 GPU, improving DDIM performance from 169.50 FID (NFE=2) to 5.02.", "tldr": "We propose TrajP-L  for fast distillation—builds student model via LoRA, corrects sampling direction with trajectory coordinates, cuts discretization errors.", "keywords": ["Diffusion Models", "Distillation", "Trajectory-based", "Plugin"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f217a047e0b4d474f15dcce75661b1b6b01e2787.pdf", "supplementary_material": "/attachment/4dcf8190c18571d0521c407af47d760e83521dc0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a lightweight, distillation-based algorithm for accelerated sampling, centered around a learnable, trajectory-similarity-based plugin. The method aims to achieve this acceleration by performing a corrective sampling step that aligns the student model's output with the teacher model's output at each stage of the reverse process.\n\n\nHowever, the methodological description is unclear, and after multiple readings, I find the core algorithmic logic difficult to fully grasp. My understanding, based on Algorithms 1 and 2 in the appendix, is as follows: The process begins by sampling a set of trajectories from the diffusion model. The training stage involves three components: a pretrained model fine-tuned with LoRA (the student), the TrajP plugin, and a trajectory fitting model. For a given trajectory starting from $x_N$, the student model first samples to produce an estimate, denoted as $d_{t_N}$. Following this, the trajectory fitting model is used to derive three components: $u, v,$ and $w$.\n\n\nMy primary point of confusion arises here: the necessity and role of this trajectory fitting model are not well-motivated. These components ($u, v, w$) are then combined with the student's initial estimate ($d_{t_N}$) and input into the TrajP plugin to yield a corrected output, which is subsequently aligned with the teacher's sample.\n\n\nWithout the trajectory fitting model, the algorithm would be a more intuitive process of directly aligning the student's trajectory with the teacher's. The introduction of this intermediate step and the decomposition into $u, v,$ and $w$ complicates the logic considerably. Consequently, I was unable to understand the motivation behind the sequence of equations presented from Eq. 9 to Eq. 12. I would strongly encourage the authors to clarify the rationale for this trajectory fitting step and better explain why this decomposition is an essential part of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The method devised in this paper is novel. Using $t_n$ and $t_{n-1}$ to construct the components u, v, and w for correcting the model output is an interesting idea. However, this introduction makes the entire paper difficult to understand.\n\n2. The experiments in this paper are quite comprehensive, with the authors conducting a series of ablation and comparative experiments on CIFAR-10. However, the experimental diversity is seriously lacking."}, "weaknesses": {"value": "1. The core logic of the paper is not well-articulated. After a thorough review, I still do not understand the motivation for introducing the \"coordinate information of the current and next sampling timesteps\" to correct the student model's output. The authors fail to provide a clear rationale for why this specific information is necessary or effective, leaving a critical gap in the paper's foundation. In essence, the central \"why\" of the proposed approach remains unexplained.\n\n2. The novelty of the work is questionable when viewed in the context of existing accelerated sampling algorithms. \n\n- Unsupported Efficiency Claims: The authors claim their algorithm consumes fewer computational resources. However, the paper provides no quantitative comparison of the training-time computational cost against mainstream step distillation algorithms, making this claim unsubstantiated.\n\n- Missing Baselines: The experimental section notably omits comparisons with several mainstream and state-of-the-art step distillation algorithms. This makes it impossible to properly situate the proposed method's performance and contribution within the current literature.\n\n- Limited Novelty: If the vaguely explained \"trajectory fitting\" component is set aside, the method appears to be a straightforward distillation of the teacher model's trajectory to the student model. In this light, the work does not seem to offer a significant conceptual or technical contribution beyond existing techniques.\n\n\n3. The empirical evaluation is not comprehensive enough to validate the effectiveness and generalizability of the proposed method\n\n- Limited Datasets: Experiments are confined to the CIFAR-10 dataset. For a method in this domain, evaluation on more challenging, higher-resolution datasets like ImageNet (e.g., 64x64 or 256x256) or LSUN is standard practice and essential for demonstrating scalability.\n\n- Lack of Text-to-Image Experiments: Many contemporary methods for accelerated sampling are applicable to large-scale text-to-image diffusion models. The absence of any experiments in this widely-used domain is a major weakness and limits the perceived applicability and impact of the work."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qOnGnrP35d", "forum": "4lAocDKU1A", "replyto": "4lAocDKU1A", "signatures": ["ICLR.cc/2026/Conference/Submission7605/Reviewer_SCid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7605/Reviewer_SCid"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760534357352, "cdate": 1760534357352, "tmdate": 1762919691443, "mdate": 1762919691443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TrajP-L, a trajectory-distillation based plugin for few-step diffusion sampling that (i) builds a LoRA student on top of a pretrained model and (ii) adds a trajectory correction module (TrajP) that takes the coordinates of the current/next step in a fitted 3-D trajectory representation to adjust the student’s sampling direction, aiming to reduce discretization error at very small NFE. Compared to traditional sampling solvers at small NFE, the results are much better in terms of FID across relatively simple image generation tasks (ImageNet 64×64, LSUN-Bedroom 256x256), and the paper claims to have lower training cost than traditional distillation methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has a clearly stated objective, reducing discretization error for very small NFE via a direction-correction head is a concrete and relevant target for the fast-sampling community\n2. Using LoRA to keep the base model largely intact is sensible, aligns with recent distillation practice."}, "weaknesses": {"value": "1. The paper states that sampling space can be “captured by as few as three basis vectors” and then uses a fitted 3-D representation. There is a lack of rigorous reasoning/ablation studies behind this to explain: why were only the top-3 PCA components chosen, and why are they formulated in this way?\n2. The paper highlights “~10 minutes on a 3090,” but does not provide a \"wall-clock\" or \"total FLOPs\" comparison of its entire pipeline (trajectory generation + plugin training) against the end-to-end training of the baselines it omits. The current efficiency claims are unsubstantiated.\n3. Generalizability of the method is not fully tested, on models with different architectures or larger-resolution tasks."}, "questions": {"value": "1. Can the authors clarify the precise mechanism for deriving the \"fitted 3D trajectory representation\"? How are those three terms: a quadratic polynomial term, an exponential term, and a Gamma term chosen ?\n2. Is it possible to provide an ablation study on the dimensionality $k$ of the trajectory subspace ?\n3. To avoid ambiguity, regarding the terminology “large-step sampling”, does it actually mean “few-step sampling with large intervals” ? Please consider revising this terminology for clarity and consistency with the literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LTZfqNmh1s", "forum": "4lAocDKU1A", "replyto": "4lAocDKU1A", "signatures": ["ICLR.cc/2026/Conference/Submission7605/Reviewer_cpJY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7605/Reviewer_cpJY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385157407, "cdate": 1761385157407, "tmdate": 1762919689796, "mdate": 1762919689796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TrajP-L, a plugin aimed at accelerating distilled diffusion models, particularly in low-step sampling regimes. The method is built on the observation that DM sampling trajectories share structural similarities. The authors propose to first fit these trajectories into a low-dimensional (3D) space using a specific mathematical expression. This fitted trajectory shape is then used as a prior via a 'TrajP' module to correct the sampling direction of a student model, which itself is created efficiently using LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the primary appeal of this work lies in its efficiency. The method combines LoRA for parameter-efficient fine-tuning, which reportedly reduces trainable parameters by over 95% , with a training process that requires a very small number of teacher trajectories (e.g., 3K-7K). The reported training time of 10 minutes on a single GPU for CIFAR-10  makes this approach seem highly practical.\n\nThe quantitative results presented are quite strong, particularly in the low-NFE setting ($NFE \\le 6$). For example, the paper shows a drop in FID from 169.50 (DDIM) to 5.02 on CIFAR-10 for $NFE=2$. This level of performance in a challenging, few-step regime is also demonstrated across other datasets like FFHQ and ImageNet.\n\nThe core idea of first fitting sampling trajectories into a low-dimensional 3D space  and then using this fitted representation as a prior to correct the student model's sampling direction  is an interesting approach to mitigating discretization errors."}, "weaknesses": {"value": "I'll start with a disclaimer: I haven't been following the diffusion distillation literature for a long time, so my perspective might be off. That said, I'm finding it a bit difficult to position this work relative to other *distillation* methods. The paper compares heavily against training-free solvers (like DDIM, DPM-Solver)  but less so against other trajectory-based distillation approaches. For instance, the ablation in Table 4 compares TrajP-L (TrajP + LoRA) against 'w.o. TrajP' (just LoRA) and 'w.o. LORA' (just TrajP). I'm wondering how this compares to a full-parameter fine-tuning on the same small set of 5K trajectories (which seems to be the 'Trajectory-based Distillation' in Fig 2b )? It would be helpful to clarify if LoRA's main benefit is just parameter-efficiency, or if it's also crucial for preventing overfitting on this small dataset.\n\nPersonally, I'm not entirely convinced by the 3D trajectory fitting component in Section 3.1. It feels a bit arbitrary. Why exactly 3D? The paper mentions this is based on recent findings, but I'm not sure if this was an ablated choice. I'm concerned that the performance might be sensitive to the specific, and rather complex, mathematical expression chosen (the combination of quadratic, exponential, and Gamma terms ). I wonder if a simpler polynomial basis or even using 4-5 dimensions would have worked just as well, or perhaps even better.\n\nPerhaps I missed this, but the exact mechanism of the TrajP module ($T_{\\theta}$) isn't very clear from the main text. Section 3.3 and Equation 17 are quite high-level . Figure 2(c)  provides a block diagram, but it's not obvious how the cached information $Q$ (which includes the starting point and all previous directions ) is processed. Is it a recurrent module? How is the \"implicit PCA\" mentioned  actually performed? Given this is a core part of the correction, a more detailed explanation of its architecture would strengthen the paper."}, "questions": {"value": "I have a few questions that I hope the authors can address in their rebuttal, as the answers could help clarify some of my concerns:\n\n* I'm trying to better position this work. The main comparisons in Tables 1, 11, 12, and 13  are against training-free solvers. While the performance is good, I'm curious how TrajP-L compares to other *distillation* methods (e.g., ) when they are *also* trained on the same small data budget (3K-7K trajectories ). Is the main advantage of TrajP-L that it *enables* distillation with so few trajectories, while others would fail?\n\n* Related to this, I'm wondering about the true benefit of LoRA here. The ablation in Table 4  is helpful, showing that LoRA alone ('w.o. TrajP') provides a large boost over DDIM. Could you clarify if LoRA is primarily for parameter efficiency, or if it's also acting as a crucial regularizer? For instance, what happens if you try full-parameter fine-tuning (like the 'Trajectory-based Distillation' in Fig 2b ) on the same 5K trajectories? My hypothesis is that it might overfit, making LoRA a necessary component for this low-data regime, but I'd like to see this confirmed.\n\n* Could you provide more rationale for fitting the trajectories into exactly 3D space? This seems like a strong and specific design choice. I'm wondering what the performance looks like if you use, say, 2 or 5 principal components instead. Along the same lines, the mathematical expression (Eqs. 9-12 ) is quite specific. How sensitive is the method to this exact combination of quadratic, exponential, and Gamma terms?\n\n* I found the description of the TrajP module ($T_{\\theta}$)  a bit high-level. The paper mentions it uses cached information $Q$ and performs \"implicit PCA\". Could you perhaps elaborate on the architecture of this module? For example, how is the history $Q$ consumed (is it a recurrent net, attention, a simple concatenation?), and how is the 3D trajectory information $[u,v,w]$  used to correct the high-dimensional sampling direction $\\hat{d}_{t_{n}}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ihtdvo527", "forum": "4lAocDKU1A", "replyto": "4lAocDKU1A", "signatures": ["ICLR.cc/2026/Conference/Submission7605/Reviewer_W2Pn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7605/Reviewer_W2Pn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656712097, "cdate": 1761656712097, "tmdate": 1762940957474, "mdate": 1762940957474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TrajP-L, a lightweight training-based method to reduce the sampling steps of diffusion models. The proposed TrajP-L fits the trajectory of reverse ODE of diffusion model with a simple set of basis functions, which is combined with LoRA to train a few-step student model. Experiments on CIFAR-10, FFHQ, ImageNet, and Stable Diffusion v1.5 show that TrajP-L achieves strong FID scores with small NFE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed TrajP-L is lightweight and can be trained with minimal compute, requiring only a few thousand sampled trajectories and limited GPU resources. Accelerating diffusion model sampling with minimal training compute has great practical value."}, "weaknesses": {"value": "- LCM-LoRA[1,2] is highly relevant but not discussed or compared throughout the paper. - Given that both approaches employ LoRA for efficient fine-tuning and target fast sampling for Stable Diffusion, this important comparison is missing. \n- The design of the trajectory fitting component lacks sufficient theoretical or empirical grounding. While Table 4 provides an incremental FID improvement, the choice of basis functions seems heuristic. Stronger theoretical or empirical justifications are needed to clarity the foundation of the method's core innovation.  \n- Writing and formatting:\n\t- Reference format: Eq(?) should be Eq. (?)\n\t- Most equations except for 3,4 are missing punctuations.  \n\t- Captions of many tables and figures lack ending punctuations as well.\n\t- $t_{min}$ is not rendered correctly in line 336 and 338. \n\t- Line 341 contains a typo \"training settings\". \n\n[1]: Luo, Simian, et al. \"Lcm-lora: A universal stable-diffusion acceleration module.\" _arXiv preprint arXiv:2311.05556_ (2023).\n\n[2]: Thakur, Ayush, and Rashmi Vashisth. \"A unified module for accelerating stable-diffusion: Lcm-lora.\" _arXiv preprint arXiv:2403.16024_ (2024)."}, "questions": {"value": "- Why are only the top three principal components used to represent trajectories? How sensitive are the results to this dimensionality choice? \n- How does the proposed set of basis functions (polynomial, exponential, Gamma) compare to theoretically better-conditioned bases like Fourier, or more expressive learned bases like neural networks?\n- The benefit of using trajectory fitting seems limited from Table 2. Can authors provide more ablation on Stable Diffusion to further examine its significance? \n- Figure 1 reports results only for NFE ≤ 6. What does the curve look like for larger NFEs, and how does it compare to other methods at that regime? Does the method's advantage persist or saturate at higher step counts? \n- Can the authors discuss and provide direct comparisons against LCM-LoRA to contextualize TrajP-L’s advantages or differences?\n- Can authors try more recent text-to-image models like FLUX? The qualitative results in Fig 12 do not seem competitive in image quality. \n- What are the definition of $x_{buffer}$  and B in Fig. 3(c)? How are they related to $Q$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qrtGZl5RcQ", "forum": "4lAocDKU1A", "replyto": "4lAocDKU1A", "signatures": ["ICLR.cc/2026/Conference/Submission7605/Reviewer_hB7Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7605/Reviewer_hB7Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158681721, "cdate": 1762158681721, "tmdate": 1762919682931, "mdate": 1762919682931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}