{"id": "RCUZVHXlkj", "number": 21429, "cdate": 1758317487331, "mdate": 1763754078568, "content": {"title": "Long-term Fairness with Selective Labels", "abstract": "Long-term fairness algorithms aim to satisfy fairness beyond static and short-term notions by accounting for the dynamics between decision-making policies and population behavior. Most previous approaches evaluate performance and fairness measures from observable features and a label, which is assumed to be fully observed. However, in scenarios such as hiring or lending, the labels (e.g., ability to repay the loan) are _selective labels_ as they are only revealed based on positive decisions (e.g., when loan is granted). In this paper, we study long-term fairness in the selective labels setting, and analytically show that naive solutions do not guarantee fairness. To address this gap, we then introduce a novel framework that leverages both the observed data and a label predictor model to estimate the true fairness measure value, by decomposing into the observed fairness and bias from labels predictions. This allows us to derive the sufficient conditions to satisfy true fairness from observable quantities by using the confidence on the predictor model.  Finally,  we rely on our theoretical results to propose a novel reinforcement learning algorithm for effective long-term fair decision-making with selective labels. In semisynthetic environments, the proposed algorithm reached comparable fairness and performance to an agent with oracle access to the true labels.", "tldr": "We study satisfying long-term fairness with respect to labels in the selective labels setting", "keywords": ["long-term fairness", "reinforcement learning", "selective labels"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90d875cbdc1dde7e5ff2dc6880f005efe015303d.pdf", "supplementary_material": "/attachment/9f615632871949611e9d9260aa2ed2a6865c7964.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies long-term fairness in a sequential decision setting where only the labels of admitted samples are available. The paper first proves that only ensuring fairness in the observed samples cannot guarantee overall fairness. Motivated by this, the authors proposed a RL-based algorithm with an added $L^{Renyi}$ term to upper bound the fairness divergence. Experiments on synthetic datasets demonstrate the effectiveness of the algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is reasonable, and the selection biases do exist in real-world settings.\n2. The decomposition of the disparity and the upper bound derivation seem to be correct.\n3. The algorithm design is clear, and experimental results support the claims."}, "weaknesses": {"value": "1. The method heavily depends on the accuracy of the label predictor $\\phi$. But $\\phi$ itself is only correct under the overlap assumption that accepted and rejected samples share enough support. In real-world settings such as loan application, it is reasonable to believe some applicants will never be accepted,i.e., some features will never be covered. \n\n2. $\\phi$ is also implemented as a simple logistic regression model, and the generalization to complex, high-dimensional data is uncertain.\n\n3. Synthetic experiments may not reflect the applicability in real-world settings.\n\n4. Minor: I feel that this work is closely related to fairness in sequential strategic classification and performative prediction settings, while some related works are missing (e.g., [1,2])\n\n[1] Xie, Tian, and Xueru Zhang. \"Automating data annotation under strategic human agents: Risks and potential solutions.\" Advances in Neural Information Processing Systems 37 (2024): 127436-127482.\n\n[2] Somerstep, Seamus, Ya'acov Ritov, and Yuekai Sun. \"Algorithmic fairness in performative policy learning: Escaping the impossibility of group fairness.\" Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. 2024."}, "questions": {"value": "I feel that $c$ can be very important for the performance of the algorithm. Did the authors explore how the cost affects the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1s9vmOodcm", "forum": "RCUZVHXlkj", "replyto": "RCUZVHXlkj", "signatures": ["ICLR.cc/2026/Conference/Submission21429/Reviewer_xA3L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21429/Reviewer_xA3L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760559119259, "cdate": 1760559119259, "tmdate": 1762941768170, "mdate": 1762941768170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of satisfying long-term fairness in scenarios with selective labels, that is, where action feedback is only observable based on positive decisions. The authors argue that this specific setting can create a flawed objective where optimal policies can learn to minimize $\\Delta^{A=1}_t$ without necessarily minimizing disparity within the rejected population. They propose a framework for approaching these problems and an algorithm to solve it based on advantage regularization of PPO. Finally, they conduct experiments on 2 case studies and show that they are able to achieve higher reward and lower disparity than an oracle baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper studies an important problem in considering long-term fairness with partial observability on rejected candidate labels. The context the authors study this in is high-stakes and can have a significant effect on people's lives.\n* The proposed method is fairly simple to implement since it is just regularizing the PPO advantage. \n* The experimental results support the claims made by the authors and their proposed framework/algorithm."}, "weaknesses": {"value": "* There seems to be some highly related works [1, 2] not mentioned in the paper. Could the authors please give a comparison with these works?\n* It would be a more compelling paper to include at least one more case study (ideally studying Accuracy Parity now, since the other two proposed fairness formulations are studied by the two given case studies), and more baseline methods, to compare with SELLF. For example, [1] considered the tasks of criminal justice, health care, and insurance. Another example could be long-term exposure fairness in recommendation [4]. For baselines, [1] provided a contraction method to address this problem. Another potential baseline is an constrained RL approach such as FOCOPS [2] or CPO [3] and treat the disparity as an expected cost to minimize.\n\nI am happy to raise my score if my concerns above are addressed.\n\nSome minor Typos (probably run a typo checker upon revision):\n* Line 104: selection -> select\n* Line 211: cofounded -> confounded\n* Line 228 -> depends -> depend\n* Line 286: Labes -> Labels?\n* etc.\n\n\n[1] Lakkaraju, H., Kleinberg, J., Leskovec, J., Ludwig, J., & Mullainathan, S. (2017). The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 275–284. Presented at the Halifax, NS, Canada. doi:10.1145/3097983.3098066\n\n[2] Zhang, Y., Vuong, Q., & Ross, K. W. (2020). First Order Constrained Optimization in Policy Space. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/2002.06506\n\n[3] Achiam, J., Held, D., Tamar, A., & Abbeel, P. (2017). Constrained Policy Optimization. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1705.10528\n\n[4] Mansoury, M., & Mobasher, B. (2023). Fairness of Exposure in Dynamic Recommendation. arXiv [Cs.IR]. Retrieved from http://arxiv.org/abs/2309.02322\n\n[5] Chang, T., & Wiens, J. (07 2024). From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions. Proceedings of Machine Learning Research, 235, 6286–6324.\n\n[6] Yu, E. Y., Qin, Z., Lee, M. K., & Gao, S. (2022). Policy optimization with advantage regularization for long-term fairness in decision systems. Proceedings of the 36th International Conference on Neural Information Processing Systems. Presented at the New Orleans, LA, USA. Red Hook, NY, USA: Curran Associates Inc."}, "questions": {"value": "Please see weaknesses. Also, some additional questions:\n* I notice you use a linear predictor architecture. Does increasing the number of layers have any effect on your performance?\n* Line 136: Should the beta distribution $Be(\\cdot)$ require 2 parameters, $\\alpha, \\beta$, instead of just the one you provided?\n* Is there a reason why you put $L^{\\text{Renyi}}$ into the objective function in Line 109, rather than creating an additional regularization term in the advantage? I am wondering if placing this penalty term in the objective vs advantage will incur any reward hacking issues as seen in [6]. \n* In Line 326, what is semisynthetic about the environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9WoMUTnzKe", "forum": "RCUZVHXlkj", "replyto": "RCUZVHXlkj", "signatures": ["ICLR.cc/2026/Conference/Submission21429/Reviewer_RQL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21429/Reviewer_RQL1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932782888, "cdate": 1761932782888, "tmdate": 1762941767344, "mdate": 1762941767344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Improvements"}, "comment": {"value": "# Summary of Improvements\n\nWe thank the reviewers for their constructive feedbacks, and for acknowledging the novelty and rigor our work;  “_relevant and, to the best of my knowledge, previously understudied_” (reviewer 5xkx), “_mathematic framework of this paper is rigorous_” (reviewer rN9Y), and that empirical results support our proposal; “_experimental results support the claims_” (reviewer xA3L) and “_the experimental results support the claims made by the authors_” (reviewer RQL1). \n\nWhile we appreciate these compliments, we took the reviewers' concerns seriously. Addressing the raised issues not only improved the manuscript but also provided an opportunity to demonstrate that our method performs reliably even under stricter conditions. In summary, we have updated the paper to include:\n\n- Inclusion of missing references in related works discussions (Sec. 1.1 and Appendix A).\n- Emphasis on the safeguards of our proposal to ensure IPW stability  (lines 259 and Appendix F.3) and its relation to Renyi regularization (lines 314 and Appendix F.1).\n- Inclusion of two extra baselines, ELBERT [2] and FOCOPS [1], in experiments (Sec. 5 and Appendix F.4), and a third usage scenario with crime recidivism (Sec. 5.2).\n- Experiments on the robustness of our proposal in a configuration where the overlap assumption does not hold (Sec. 5) and when a complex neural network is used as the predictor (Appendix F.2).\n\nWe believe these extensive updates bridge the gap between positive comments and the quantitative scores, and we kindly ask reviewers to reconsider the assessment of our paper.\n\n\n\n[1] Zhang, Yiming, Quan Vuong, and Keith Ross. \"First order constrained optimization in policy space.\" Advances in Neural Information Processing Systems 33 (2020): 15338-15349. \n\n[2] Xu, Yuancheng, et al. \"Adapting Static Fairness to Sequential Decision-Making: Bias Mitigation Strategies towards Equal Long-term Benefit Rate.\" International Conference on Machine Learning. PMLR, 2024."}}, "id": "ns1ZKew3aN", "forum": "RCUZVHXlkj", "replyto": "RCUZVHXlkj", "signatures": ["ICLR.cc/2026/Conference/Submission21429/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21429/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21429/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754373105, "cdate": 1763754373105, "tmdate": 1763754373105, "mdate": 1763754373105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical problem of achieving long-term fairness in sequential decision-making systems where the true outcome (label) is only observed for selections. This \"selective labels\" setting is common in real-world scenarios like lending (where repayment ability is only known if a loan is granted) or hiring (where job performance is only known for hired candidates). The authors first demonstrate formally that naive approaches, such as measuring fairness only on the sub-population with observed labels, are insufficient and can fail to guarantee fairness for the overall population. Then the paper introduces a novel theoretical framework that uses a label predictor to impute the labels for the \"rejected\" population. The core of their theoretical contribution is a decomposition (Theo. 3.1) that precisely links the true disparity to the observed disparity calculated using the predictor's imputed labels. This decomposition shows that the observed disparity is confounded by the policy's rejection rate and the predictor's error on the rejected group.\n\nTo address the problem, the authors propose a new reinforcement learning algorithm, SELLF (SElective Labes in Long-term Fairness). SELLF is based on PPO and incorporates the paper's theoretical insights through two key mechanisms: (1) It penalizes the observed disparity; and (2) It introduces a novel regularization term ($L^Renyi$) that penalizes the Renyi divergence. This new loss term directly corresponds to the theoretical bounds and encourages the policy to take actions that reduce the predictor's error and improve the confidence of its estimates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary originality lies in its formal problem formulation to model the intersection of long-term fairness and selective labels. \n\n- The authors provide a clear, formal progression from demonstrating the failure of naive methods to a full decomposition of the observed disparity in Theorem 3.1 and to actionable, observable conditions in Theorem 3.4.\n\n- The mathematic framework of this paper is rigorous. The authors have shown how did they identify the disparity, and its error bounds directly motivates the design of the proposed learning algorithm."}, "weaknesses": {"value": "- The motivation for choosing Inverse Propensity Weighting (IPW) to estimate the predictor's error on the rejected population ($D_R^i$) using data from the accepted population ($D_A^i$) feels abrupt. This justification is critical because the entire framework and algorithm are now built upon IPW, which is notoriously unstable and suffers from high variance, especially when acceptance probabilities are low. And I would suggest the authors to provide some implication on Assumption 1.\n- The paper's core premise of *selective labels* is a potentially problematic and imprecise way to frame the problem. A more accurate conceptualization would be data selection bias. The issue is not necessarily that the label $Y$ is selectively realized only upon a positive action $A\\_t=1$. Rather, a true, latent qualification $Y$ should be assumed to exist for all individuals, and a separate selection variable $S$ (in this work, the policy's action, $A_t$, which is based on the model's predictions) merely determines whether $Y$ is observed by the decision-maker. The data selection is standard in missing data and causal inference literature. \n- The introduction of the $L^\\text{Renyi}$ loss in Eq. (6) is confusing. The loss is justified as a practical way to control the theoretical error bound from Theorem 3.3. However, this relies on the unsubstantiated assertion that this specific Renyi divergence term \"will be dominated\" by other terms in the bound. This makes the $L^\\text{Renyi}$ term feel like a complex and indirect proxy, obscuring the direct connection between the algorithm's objective and the actual goal of minimizing fairness disparity. \n- The framework depends on a label predictor ($\\phi$) to estimate the unobserved error ($\\epsilon_t^i$). However, this predictor is itself trained on the same selectively-labeled, biased data (Eq. 7). The paper does not fully address how errors or biases in this predictor (resulting from unstable IPW training) might in turn corrupt the disparity estimate ($\\tilde{\\Delta}$) and the error bounds ($\\bar{\\epsilon}^i$).\n- Although the paper is framed as a \"long-term\" fairness study, the objective (Eq. 1) is to satisfy a static fairness constraint $|\\Delta_t| \\le \\omega$ at every timestep $t$. This formulation does not fully capture the dynamics of fairness over time, such as how unfairness at one step might be permissibly traded for greater fairness at a later step. The experiments show the results of fairness over time, but the problem's objective remains a per-step constraint rather than a truly holistic long-term objective."}, "questions": {"value": "1. Line 50:  presents great in impact in sequential decision-making -> presents a great impact on sequential decision-making\n2. Line 52: make it not trivial to obtain -> make it non-trivial to obtain\n3. Line 62: *presents* conditions\n4. Line 104: The decision-maker will *select* actions to maximize a reward function.\n5. In Definition 2, where does $\\mathcal{F}$ of $\\mathcal{F}$-MDP come from? And what is the notation of $Be(\\cdot)$?\n6. As shown in Figure 1, the selection variable $A$ is only dependent on the features $X$ and $Z$. Should not this also depend on the true qualification $Y$?\n7. Line 228: as they depends on -> as they depend on\n8. Line 256: In the first sentence of Theorem 3.3, Let $d < \\infty$ be the psuedo-dimension"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OZvFE5LGKU", "forum": "RCUZVHXlkj", "replyto": "RCUZVHXlkj", "signatures": ["ICLR.cc/2026/Conference/Submission21429/Reviewer_rN9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21429/Reviewer_rN9Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150807330, "cdate": 1762150807330, "tmdate": 1762941765899, "mdate": 1762941765899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies long-term fairness by addressing the selective labels problem, where outcomes are only observed for positively decided cases. The authors show that naive approaches to measuring fairness under selective labels fail to guarantee true population-level fairness. They introduce a framework that uses a label predictor to impute missing labels and derive theoretical conditions under which observed disparity bounds translate to true disparity bounds. Experiments are conducted on lending and school admission environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The combination of long-term fairness and selective labels is both relevant and, to the best of my knowledge, previously understudied. The authors correctly point out the gap and address it to an extent.\n2. The theoretical contributions are strong (disclaimer: I have not thoroughly verified the proofs).\n3. The proposed method has a principled algorithmic design grounded in the theoretical results and can handle multiple fairness notions."}, "weaknesses": {"value": "1. Assumption 1, that every feature combination with non-zero rejection probability must also have non-zero acceptance probability, is very strong and potentially unrealistic. In practice, certain subpopulations may be systematically excluded. The authors should better acknowledge these limitations. While I understand these assumptions are needed for the theoretical results, the authors could improve the paper’s practical relevance by experimentally evaluating cases where these assumptions do not hold. The stationarity assumption of the F-MDP is only briefly discussed at the end; further investigation of its implications in practice would also be valuable.\n\n2. Some aspects of the algorithmic design are unclear. The paper moves from bounding predictor error to minimizing it using Renyi divergence as a proxy. In Line 307, the authors state that “in practice, the bound from Theo. 3.3 will be dominated by the divergence term,” but no proof or empirical validation supports this claim. Why not directly optimize for the prediction error?\n\n3. The baseline selection is quite limited and POCAR is the only fair RL algorithm compared against. I suggest including at least two additional baselines from the list below. To my knowledge, Xu et al. (2023) is a particularly strong comparison.\n\n\n\n4. Some citations are missing; I suggest the authors include the following (the list is not exhaustive):\n* Jabbari, Shahin, et al. \"Fairness in reinforcement learning.\" International conference on machine learning. PMLR, 2017.\n* Satija, Harsh, et al. \"Group fairness in reinforcement learning.\" Transactions on Machine Learning Research (2023).\n* Xu, Yuancheng, et al. \"Adapting static fairness to sequential decision-making: Bias mitigation strategies towards equal long-term benefit rate.\" arXiv preprint arXiv:2309.03426 (2023).\n* Rezaei-Shoshtari, Sahand, et al. \"Fairness in Reinforcement Learning with Bisimulation Metrics.\" arXiv preprint arXiv:2412.17123 (2024).\n* Deng, Zhihong, et al. \"What hides behind unfairness? exploring dynamics fairness in reinforcement learning.\" Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. 2024.\n* Frauen, Dennis, Valentyn Melnychuk, and Stefan Feuerriegel. \"Fair off-policy learning from observational data.\" Proceedings of the 41st International Conference on Machine Learning. 2024."}, "questions": {"value": "Please answer my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9C4uAoLuz4", "forum": "RCUZVHXlkj", "replyto": "RCUZVHXlkj", "signatures": ["ICLR.cc/2026/Conference/Submission21429/Reviewer_5xkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21429/Reviewer_5xkx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21429/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188741544, "cdate": 1762188741544, "tmdate": 1762941764865, "mdate": 1762941764865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}