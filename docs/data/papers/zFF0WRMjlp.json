{"id": "zFF0WRMjlp", "number": 4903, "cdate": 1757794791089, "mdate": 1763718468172, "content": {"title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier", "abstract": "Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost limits their scalability in practice.\nTo address the efficiency concern, we investigate generative classifier built upon recent advances in visual autoregressive (VAR) modeling. Owing to their tractable likelihood, VAR-based generative classifier enable significantly more efficient inference compared to diffusion-based counterparts. Building on this foundation, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which further improves accuracy while reducing computational cost, substantially enhancing practical usability.\nBeyond efficiency, we also study several properties of VAR-based generative classifiers that distinguish them from conventional discriminative models. In particular, the tractable likelihood facilitates visual explainability via token-wise mutual information, and the model naturally adapts to class-incremental learning without requiring additional replay data.", "tldr": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier", "keywords": ["generative classifier", "generative model", "autoregressive model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16451406d49298f7966e22dfdc0e2c4acc0539e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper leverages Vector Auto Regressive (VAR) models, a class of generative models to the classification task by applying Bayes rule."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths of var:\n\n- Faster (81x) and better quality compared to diffusion models  [Tian paper ]\n- AR models have tractable likelihood\n\nThis paper, puts these strengths of the VAR models to use in the task of explainability and classification."}, "weaknesses": {"value": "### There are multiple claims in the paper that are not validated.\n\n**Superior Tradeoff / Pareto Dominance**\nLet $a, b \\in \\mathbb{R}^2$ with objective values\n$f(a) = (f_1(a), f_2(a))$ and $f(b) = (f_1(b), f_2(b))$.\nWe say that $a$ has a superior tradeoff to $b$\n(or $a$ Pareto-dominates $b$), denoted $a \\prec b$, if\n$f_1(a) \\le f_1(b), \\quad f_2(a) \\le f_2(b), \\quad \\text{and} \\quad \\exists j \\in \\{1,2\\}: f_j(a) < f_j(b).$\n\nHowever on the accuracy front VAR does not dominate. so at best you can say tradeoff and lies on Pareto front but cannot say Superior Tradeoff.\n\n**Ablation on other candidate pruning**\n\nCandidate pruning is a popular technique in diffusion classifier. There are multiple ways to prune, and more the pruning Figure 4.[https://arxiv.org/pdf/2303.16203] and more samples in the pruning process better the performance. You will get the same Figure 2 if you do uniform scale, the performance improves with number of selected scales. Can you provide experiments on various pruning techniques to validate the partial-scale hypothesis discussed in Section 4.2\n\n**Ablation on Smoothness**\n\nI agree with the authors that the adversarial noise, causes token drift. Is the model evaluated on Adversarial noise? No, So this section does not contribute to any new information. Do Authors perform ablation on S= [0, ….. N]? No. So How can authors conclude adding smoothness lead to improved performance?\n\n**Unfair Comparison**\n\n1. Class incremental learning can be seen in other generative models as well\n    \n    Class incremental learning is not just related VAR, but also observed in other types of generative models.  Consider the control net architecture in diffusion models, where you can add additional classes without even adding other model complexity. Can authors put to comparison how VAR model fares on class incremental learning in comparison to ControlNet? \n    \n2. DiT vs VAR\n    1. Over the years there are multiple improvement to Diffusion classifier to address inference speed \n        1. Rectified flow, few steps to clean image ( Instead of sampling 25 noise scales, you can get away with 5 noise scales. )\n        2. Parallelisation, speed of  Diffusion classifier can be significantly improved by Parallelisation.  Pruning to 25 noise scales to an image and for an 100 class classification, you can pass everything in a 25 * 100 times batch and obtain the results quickly. ( time will be much faster ). \n    \n    I agree that even with improvements it is very difficult to beat VAR with respect to inference time, it would also be helpful to mention flops, should be a better metric.\n\n\n**Strong Claims**\n\nI am not sure what does “novel” generative classifier mean? All the works, prior to this method, such as diffusion classifier does not call them as a novel classifier. It is applying Bayes’ rule to generative models to perform classification task.\n\nI don’t agree generative classifiers, particularly diffusion classifiers are not popular they are very popular on zero shot image classification tasks. In addition, diffusion models achieve better compositional generalisation [ https://arxiv.org/abs/2501.05707, https://arxiv.org/pdf/2503.04687]. However, using as a pure classifier, low adoption of generative classifiers can be attributed to the linear scalability with respect to number of classes, and not accurately modelling of $P(x|y)$"}, "questions": {"value": "Please refer to weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2PVF2VF32m", "forum": "zFF0WRMjlp", "replyto": "zFF0WRMjlp", "signatures": ["ICLR.cc/2026/Conference/Submission4903/Reviewer_1Nsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4903/Reviewer_1Nsc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431623501, "cdate": 1761431623501, "tmdate": 1762917750966, "mdate": 1762917750966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a VAR-based generative classifier (A-VARC+) that adds likelihood smoothing, partial-scale candidate pruning, and CCA finetuning. It argues for better efficiency than diffusion-based generative classifiers and offers token-level PMI explanations and a small class-incremental learning demo."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation for exploring autoregressive generative classifiers.\n\n2. Techniques are simple, practical, and likely easy to reproduce.\n\n3. Tractable likelihood enables straightforward token-wise visual explanations.\n\n4. Broad empirical sweep (in-distribution + several shift datasets)."}, "weaknesses": {"value": "1. Technical novelty feels incremental; mainly a combination/adaptation of known ideas.\n\n2. Accuracy advantages are limited; strong discriminative baselines still perform better.\n\n3. Robustness/CL claims are weakly evidenced (small-scale setups, mostly qualitative).\n\n4. Limited analysis of trade-offs (e.g., compute/memory vs. accuracy) and few quantitative explainability metrics."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zgziMO33ES", "forum": "zFF0WRMjlp", "replyto": "zFF0WRMjlp", "signatures": ["ICLR.cc/2026/Conference/Submission4903/Reviewer_kzRp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4903/Reviewer_kzRp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640739812, "cdate": 1761640739812, "tmdate": 1762917750198, "mdate": 1762917750198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Questions"}, "comment": {"value": "We would like to thank all the reviewers for their constructive and insightful comments. Below, we address the common questions raised by multiple reviewers in a unified manner.\n\n> Quantitative analysis of the visual explainability.\n\nIn this work, we present using token-wise mutual information (TMI) for visual explanation. This can be viewed as an attribution method that produces an attribution score for each token. To evaluate attribution quality, we adopt the insertion and deletion metrics introduced in [a], which are widely used in the explainability literature. Tokens are first sorted according to their attribution scores; then, they are gradually inserted or removed to measure the change in the predicted probability of the ground-truth class. The area under the curve (AUC) is used for evaluation. Intuitively, for the insertion metric, a higher AUC is preferred, as it indicates that tokens with high attribution scores meaningfully support the ground-truth class. For the deletion metric, a lower AUC is preferred, as removing highly attributed tokens should decrease the ground-truth probability. We additionally report LIME [b] and SHAP [c] as baselines. Please refer to Sec. C in the Appendix of the revised paper for additional details.\n\nFor A-VARC, LIME achieves the best overall performance, while TMI performs comparably to SHAP on the insertion metric and achieves the second-best result on the deletion metric. For A-VARC$^+$, TMI achieves the best performance on both insertion and deletion. This result aligns with the qualitative observation in Figure 5 of our paper: after finetuning, TMI becomes more concentrated on class-relevant regions.\n\n* Result of A-VARC\n| Method      | LIME      | SHAP     | TMI (Ours) |\n|-------------|-----------|----------|------------|\n| **Insertion (↑)** | **0.979** | 0.853 | 0.845   |\n| **Deletion (↓)**  | **0.192** | 0.432 | 0.346   |\n\n* Result of A-VARC$^+$\n| Method      | LIME      | SHAP     | TMI (Ours) |\n|-------------|-----------|----------|------------|\n| **Insertion (↑)** | 0.939 | 0.902 | **0.944** |\n| **Deletion (↓)**  | 0.614 | 0.746 | **0.605** |\n\n[a] RISE: Randomized Input Sampling for Explanation of Black-box Models, BMVC 2018\n\n[b] \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier, KDD 2016\n\n[c] A Unified Approach to Interpreting Model Predictions, NeurIPS 2017\n\n> Ablation study of different S for likelihood smoothing\n\nFollowing the setting in Table 2, we provide an additional ablation study using different values of $S$. For A-VARC, the accuracy increases as $S$ grows and saturates at $S = 16$, yielding an overall improvement of 5.12%. For A-VARC+, saturation occurs earlier at $S = 8$ with a smaller improvement of 1.06%, indicating that the gain from smoothing is reduced when combined with CCA. In both cases, the results consistently show that applying likelihood smoothing improves classification accuracy.\n\n| Method    | S=1   | S=2   | S=4   | S=8   | S=10  | S=16  | S=32  |\n|-----------|-------|-------|-------|-------|-------|-------|-------|\n| A-VARC    | 83.30 | 85.30 | 87.18 | 88.18 | 88.26 | 88.42 | 88.42 |\n| A-VARC+   | 88.68 | 89.42 | 89.30 | 89.74 | 89.72 | 89.54 | 89.72 |\n\n> Diffusion classifier for class-incremental learning\n\nTo investigate whether other generative classifiers can address the class-incremental learning task, we include a diffusion classifier based on DiT-S/2 (we observed that larger models tend to overfit in this small-scale setting) and train it for 2,000 epochs. The results show that the diffusion classifier can also adapt naturally to this setting and achieves performance comparable to the VAR-based method.\n| Method | None | CWR | DC | A-VARC+ |\n|--------|------|------|-------------|--------|\n| Task1  | 0.0  | 83.2 | 78.4 | 72.4 |\n| Task2  | 82.4 | 61.6 | 73.6 | 82.4 |\n| Avg    | 41.2 | 72.4 | 76.0 | **77.4** |"}}, "id": "UWHEmCf4Uo", "forum": "zFF0WRMjlp", "replyto": "zFF0WRMjlp", "signatures": ["ICLR.cc/2026/Conference/Submission4903/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4903/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4903/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763716615681, "cdate": 1763716615681, "tmdate": 1763716615681, "mdate": 1763716615681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of generative classification with an autoregressive model. Traditional discriminative classifiers directly model $p(y|x)$, while a generative model can be turned into a classifier via $\\text{argmax} p(x|y)$. This paper proposes a pipeline that has 3 steps: 1) likelihood smoothing (add gaussian noise to the latent features), 2) partial-scale pruning (use first few scales to give rough likelihood estimates for a few classes), and 3) CCA  coupled with a multi-scale autoregressive image model to perform classification. More specifically, to turn an autoregressive image model into a classifier, for each class, we then compute the class conditional log likelihood using the already trained model, and then pick the class with the highest likelihood. The paper shows that this pipeline is resistant to catastrophic forgetting, and that brings a bonus of interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The key inspiration behind this paper is the need to move away from a diffusion-centric approach that has limited efficiency. Here are concrete strengths of this work: \n\n- The paper evaluates comprehensively on the ImageNet dataset testing ImageNetV2, R, etc. More concretely, the method proposed here actually is close in top-1 accuracy to baseline methods but almost 2 orders of magnitude faster. \n- The pipeline proposed here is quite simple and easy to put together for common autoregressive models. The insight that using an autoregressive model allows for a factorization of the exact likelihood that is a nice one, and the use of the next-scale prediction approach from Tian et. al. is critical for this work. \n- The interpretability benefits from this work also are impressive. Because VAR models have tractable likelihoods, the paper define token-wise pointwise mutual information (PMI) to quantify how much each token (or image patch) contributes to the predicted class. This is an interesting side benefit of this approach. \n- Finally, the paper is quite well written. In particular, the introduction and the related work guide the reader through the motivation for this work."}, "weaknesses": {"value": "The weaknesses are provided below: \n- The focus of the evaluation is primarily on the Imagenet dataset using 50 images per class. Is this standard in this literature? I wonder whether more extensive evaluations can be performed in other settings. \n- The interpretability claims are interesting, but would need more than qualitative images to thoroughly validate. \n- CCA Fine-tuning might be doing discriminative training in disguise. See the question section for more details on this."}, "questions": {"value": "What is the discriminative finetuning method doing? To be more concrete, I think we can interpret this loss function (equation 11) as a contrastive discriminative objective. If we define the score: $s_\\theta(x, y) = \\log p_\\theta(x|y)$, then under uniform priors, the posterior can be expressed as a softmax over scores: \n$$ p_\\theta(y|x) = \\frac{e^{s_\\theta(x, y)} }{\\sum_y's_\\theta(x, y')}.$$ \nWe can then say that the CCA enforces: $-\\log  \\sigma( \\beta[ s_{\\theta(x, y)} - s_{\\theta(x, y_{\\text{neg}})} ])$. This is equivalent to the binary logistic loss used in discriminative contrastive learning setting. To clarify in words, it might be the case that: CCA is discriminative training on log-likelihood ratios, using the generator’s likelihood as the scoring function. This means: CCA finetuning is functionally identical to discriminative contrastive learning, but applied to generative likelihoods rather than logits. \n\nQuestions: \n- Wouldn't this bias the samples from the generative models towards class discrimination rather than sample realism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EoeukhYLWo", "forum": "zFF0WRMjlp", "replyto": "zFF0WRMjlp", "signatures": ["ICLR.cc/2026/Conference/Submission4903/Reviewer_XVX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4903/Reviewer_XVX3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916454688, "cdate": 1761916454688, "tmdate": 1762917749674, "mdate": 1762917749674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Given the existing diffusion classifiers, Adaptive VAR Classifier$^+$ is proposed in this paper as a new variant of generative classifiers. It is claimed to be  efficient, visually explainable and robust to distribution shifts. Experiments are conducted accordingly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clarity**: This work is clearly presented with the linear logic. The core motivation and contributions of Adaptive VAR Classifier$^+$ is articulated with examples and comprehensive comparisons to existing generative classifiers. The mechanism of the model is also coherently explained.\n\n2. **Originality**: It is not previously proposed and studied on VAR classifiers. In past research, VAR and its variants are workhorses for generative tasks. To the best of my knowledge, this work is from the original attempts and findings of VAR as classifiers."}, "weaknesses": {"value": "1. The core novelty of this work should be improved. The idea to propose VAR as a generative classifier follow the same logic as substituting a module in an existing model with a more recent one, in specific, replacing diffusion models in diffusion classifiers with VAR. In Abstract, it should be articulated clearly on what the pressing problem is in classification for this era of large generative models and why proposing VAR as a generative classifier can solve this problem to highlight the significance.\n\n2. From results in Table 1, it seems that for the robustness against distribution shifts, Adaptive VAR Classifier$^+$ does not demonstrate clear advantage compared to diffusion classifiers and traditional classifiers such as ResNets. The claimed contributions on robustness to distribution shifts are not sufficiently achieved in this case."}, "questions": {"value": "1. For likelihood smoothing, why are Gaussian noises used to avoid the sensitivity to the small perturbations introduced by adding Gaussian noises? Could the authors provide an intuitive explanation? Compared to $\\epsilon$ in Equation (8), is $\\epsilon_i$ in Equation (9) scaled? In numerical experiments, could the authors provide an ablation study on $S$?\n\n2. Why is the novelty to apply CCA in the proposed Adaptive VAR Classifier$^+$? It seems that CCA is already proposed and published in Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment [1] which is also for autoregressive models.\n\n[1] Huayu Chen, Hang Su, Peize Sun, and Jun Zhu. Toward guidance-free ar visual generation via condition contrastive alignment. arXiv preprint arXiv:2410.09347, 2024b."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dIQlC86HBS", "forum": "zFF0WRMjlp", "replyto": "zFF0WRMjlp", "signatures": ["ICLR.cc/2026/Conference/Submission4903/Reviewer_d2kX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4903/Reviewer_d2kX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013464324, "cdate": 1762013464324, "tmdate": 1762917749168, "mdate": 1762917749168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}