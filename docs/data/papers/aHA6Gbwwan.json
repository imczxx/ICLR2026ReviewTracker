{"id": "aHA6Gbwwan", "number": 19712, "cdate": 1758298621410, "mdate": 1759897023893, "content": {"title": "LLM knowledge is brittle: truthfulness representations rely on superficial resemblance", "abstract": "For Large Language Models (LLMs) to be reliable, they must learn robust knowledge that can be generally applied in diverse settings---often unlike those seen during training. Yet, extensive research has shown that LLM performance can be brittle, with models exhibiting excessive sensitivity to trivial input variations. In this work, we explore whether this brittleness is a direct result of unstable internal knowledge representations. To explore this question, we build on previous work showing that LLM representations encode statement truthfulness---i.e., true, factual statements can be easily separated from false, inaccurate ones. Specifically, we test the robustness of learned knowledge by evaluating representation separability on samples that have undergone superficial transformations to drive them out-of-distribution (OOD), such as typos or reformulations. By applying semantically-preserving perturbations, we study how separability degrades as statements become more OOD, across four LLM families, five evaluation datasets, and three knowledge probing methods. Our results reveal that internal representations of statement truthfulness collapse as the samples' presentations become less similar to those seen during pre-training. While LLMs can often distinguish between true and false statements when they closely resemble the pre-training data, this ability is highly dependent on the statement's exact surface form. These findings offer a possible explanation for brittle benchmark performance: LLMs may learn shallow, non-robust knowledge representations that allow for only limited generalizability. Our work presents a fundamental challenge for the utility of truthfulness probes, and more broadly, calls for further research on improving the robustness of learned knowledge representations.", "tldr": "We show that LLMs' ability to separate true from false statements collapses under minor reformulations, suggesting that their knowledge representations are brittle, non-generalizable, and tied to superficial resemblance to the pretraining data.", "keywords": ["LLMs", "robustness", "generalization", "representations", "probing", "benchmark reliability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1a956582303a00afd311cb071daad52b6f4be7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines the brittleness of knowledge representations in Large Language Models (LLMs) by testing how well they maintain truthfulness separability under out-of-distribution (OOD) transformations (e.g., typos, Yoda-speak, translations). Using perplexity as an OOD proxy, the study applies linear, non-linear, and P(True) probes across four LLM families and datasets (True-False, MMLU, OpenBookQA, TruthfulQA), finding that separability degrades with increasing OOD-ness, suggesting reliance on superficial training data resemblance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The study is comprehensive, testing across multiple LLM families (10 models total), datasets, probing techniques, and transformation types. This broad scope enhances the generalizability of findings within the evaluated setups.\n\nBy linking OOD-ness (via perplexity) directly to representation separability, the paper offers a plausible mechanistic explanation for why LLMs struggle with subtle input variations.\n\nClear Visualization: The paper is well-written and easy to follow, with figures (e.g., AUC vs. perplexity plots) effectively illustrating degradation trends."}, "weaknesses": {"value": "As noted, the core finding—that OOD data leads to worse probe performance—aligns with prior work on truthfulness probes (e.g., [1] cited by the authors). Degraded separability on perturbed data has been observed in similar contexts. Thus, this paper does not introduce novelty on ideas or methods.\n\nAlso, the P(True) experiments also show degraded performance, consistent with expectations based on prior output-based brittleness studies, suggesting the findings are not unexpected.\n\nLack of Deep Interpretability: The degraded separability is still the observation. Observing degraded separability does not provide deeper insights into LLM knowledge brittleness. The paper attributes this to \"shallow\" representations but lacks exploration. \n\nOverall, this paper finds the correlation of degraded representation with data OOD. However, this is under expectation, and this paper lacks further explanation.\n\n[1] Levinstein et al.Still no lie detector for language models: Probing empirical and conceptual roadblocks."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4iBPzPjMIq", "forum": "aHA6Gbwwan", "replyto": "aHA6Gbwwan", "signatures": ["ICLR.cc/2026/Conference/Submission19712/Reviewer_k7BS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19712/Reviewer_k7BS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621506463, "cdate": 1761621506463, "tmdate": 1762931549612, "mdate": 1762931549612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether the widely observed brittleness of LLM performance stems from unstable internal knowledge representations. The authors apply semantically-preserving transformations (typos, punctuation noise, syntactic reordering, and translation) to drive evaluation samples out-of-distribution, then measure how well trained probes can separate true from false statements using internal representations. Across 10 models from 4 families (OLMo, Llama, Gemma), 4 datasets (True-False, MMLU, OpenBookQA, TruthfulQA), and 3 probing methods (linear probes, non-linear probes, and P(True)), the results consistently show that truthfulness separability degrades as samples become more out-of-distribution, as measured by perplexity. Fine-grained analyses reveal that certain topics (e.g., sociology, marketing) are more robust than others (e.g., history), though this variation is not explained by pre-training data coverage. Notably, representation robustness shows little correlation with whether models answer questions correctly in standard benchmarking settings, suggesting a disconnect between task performance and internal knowledge encoding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses an important and timely research question: whether the widely observed brittleness in LLM benchmark performance stems from unstable internal knowledge representations or merely from task execution issues. By attempting to distinguish between \"brittle performance\" and \"brittle knowledge\", the authors provide a new perspective on understanding LLM capabilities and limitations. The findings, if validated, could have implications for both pre-training strategies and post-training alignment methods.\n\n2. The paper demonstrates commendable experimental breadth, evaluating 10 models across 4 model families, 4 diverse datasets, and 3 distinct probing methodologies. This comprehensive evaluation across multiple dimensions, spanning different model architectures, knowledge domains, and probing techniques, strengthens the generalizability of the observed patterns."}, "weaknesses": {"value": "1. The authors claim that the applied transformations (typos, Yoda-style reordering, and translation) are semantically preserving. However, translated sentences or those with Yoda-like word order may not be strictly semantically equivalent and could disrupt the syntactic dependencies of the original content. The paper does not include any human validation or semantic-similarity evaluation to confirm the equivalence of the transformed samples. This ambiguity undermines the interpretation of the observed \"degradation\": does it truly indicate that the model's knowledge representations are non-robust, or does it simply reflect natural activation shifts caused by syntactic restructuring?\n\n2. The experimental setup is insufficiently described, and the probe's training data remain unspecified. Although the related-work section states that \"our probes are both trained and tested in out-of-distribution settings\", lines 828–829 claim that the probes were \"trained for 5 epochs on a balanced dataset of true/false\". It is unclear whether transformed OOD samples were used for probe training, under the paper's current description, these would all correspond to false samples, making the setup highly ambiguous.\n\n3. In practice, the method reduces to using the hidden states of the entire [question + candidate answer] pair as encoder representations, combined with externally provided true/false labels, to train a binary classifier as a probe. These labels do not reflect the LLMs' own beliefs, meaning the LLM is effectively treated as a frozen \"encoder\". In a classification pipeline, we would rarely attribute non-robustness to the encoder when the downstream classifier underperforms because the issue could simply stem from the probe itself. Therefore, the causal inference drawn in the paper, i.e., probe performance drops on OOD data → representation non-robust (representation collapse), is questionable. It is equally plausible that the degradation originates from the probe's limited generalization capacity. While the authors acknowledge the limitations of probing, I remain unconvinced by the claim that the LLM's internal representations are brittle. Even though multiple probes yield consistent results, most are linear or shallow MLPs, which are too simplistic to support such a strong conclusion.\n\n4. The authors report results from the \"best-performing layer\" but provide no analysis of inter-layer variation. If representational differences across layers are substantial, the observed \"degradation\" might occur only in the upper decoding-related layers rather than in the core knowledge layers. Selecting the single best layer could exaggerate or obscure the true behavior of the model’s internal representations."}, "questions": {"value": "1. Were the probes trained solely on the original (in-distribution) data, or was a separate probe trained for each dataset? In Figures 2–5, what do the plotted points for different datasets represent, are they generated by the same probe evaluated under different OOD transformations, or by distinct probes trained per dataset? Specifically, was each probe trained on the \"Original\" data and then tested on the transformed OOD variants, or were the probes re-trained for each OOD condition?\n\n2. **Please address the questions in the Weakness section.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5hit9WGDc", "forum": "aHA6Gbwwan", "replyto": "aHA6Gbwwan", "signatures": ["ICLR.cc/2026/Conference/Submission19712/Reviewer_DeBU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19712/Reviewer_DeBU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921782648, "cdate": 1761921782648, "tmdate": 1762931549197, "mdate": 1762931549197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the brittleness of internal knowledge representations in large language models (LLMs) by examining how well truthfulness is encoded and how robust this encoding is to out-of-distribution (OOD) inputs. Using a range of semantically-preserving transformations the authors test truthfulness separability across multiple LLMs, datasets, and probing methods. The study reveals that truthfulness representations degrade significantly as inputs become less similar to pre-training data, demonstrating that LLMs rely heavily on superficial surface-form similarities. This brittleness persists across models and datasets, raising concerns about the robustness of knowledge encoded by LLMs and their reliability in real-world applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive benchmark design – The evaluation framework is thorough, incorporating multiple model architectures, probing methods and diverse datasets.\n- Clear conceptual framing – The distinction between “brittle performance” and “brittle knowledge” is valuable and helps situate the paper within the broader robustness literature.\n- Methodological clarity – The probing setup and transformations are well-documented, ensuring reproducibility and transparency."}, "weaknesses": {"value": "- Validity of transformations - some of the sentence transformations are unlikely to occur naturally, raising questions about how the findings translate to everyday use cases. Some natural paraphrasing or longer contextual shifts could improve ecological validity.\n\n- No human validation – There is no human-based validation of how meaningful the observed “truthfulness degradation” is, or whether humans perceive similar brittleness.\n\n- Limited discussion of solutions – While the results clearly expose brittleness, the discussion of practical steps to improve robustness is limited\n\n- Scope of claim – The discussion occasionally implies that all knowledge representations are brittle, though the experiments focus only on truthfulness separability. Clarifying this boundary would improve precision."}, "questions": {"value": "Please see the concerns raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Q4tLXZydyr", "forum": "aHA6Gbwwan", "replyto": "aHA6Gbwwan", "signatures": ["ICLR.cc/2026/Conference/Submission19712/Reviewer_Vfu7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19712/Reviewer_Vfu7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19712/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962301790, "cdate": 1761962301790, "tmdate": 1762931548824, "mdate": 1762931548824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}