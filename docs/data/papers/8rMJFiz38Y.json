{"id": "8rMJFiz38Y", "number": 22352, "cdate": 1758329949333, "mdate": 1759896870964, "content": {"title": "Beyond Marginals: Capturing Dependent Returns through Joint Moments in Distributional Reinforcement Learning", "abstract": "Distributional reinforcement learning (DRL) has emerged as a paradigm that aims to learn full distributions of returns under a policy, rather than only their expected values. The existing DRL algorithms learn the return distribution independently for each action at a state. However, we establish that in many environments, the returns for different actions at the same state are statistically dependent due to shared transition and reward structure, and that learning only per-action marginals discards information that is exploitable for secondary objectives. We formalize a joint Markov decision process (MDP) view that lifts an MDP into a partially-observable MDP whose hidden states encode coupled potential outcomes across actions, and we derive joint distributional Bellman equations together with a joint iterative policy evaluation (JIPE) scheme with convergence guarantees. We introduce a deep learning method that represents joint returns with Gaussian mixture models with optimality and convergence guarantees. Empirically, we first validate the JIPE scheme on MDPs with known correlation structure. Then, we illustrate the learned joint structure in control and Arcade Learning Environment tasks using neural networks. Together, these results demonstrate that modeling return dependencies yields accurate joint moments and joint distributions that can help interpretability and be used in deriving safe and cost-efficient policies.", "tldr": "", "keywords": ["reinforcement learning", "distributional reinforcement learning", "interpretability", "safe reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2240258130a04a695d32a2fd885908a5c3c8f9a1.pdf", "supplementary_material": "/attachment/8eaae8d6ffe2729d13bcaba32ff7ce1dfeec5212.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes modeling the joint return distribution in distributional reinforcement learning. The authors formalize this problem by constructing a joint MDP and deriving the corresponding joint Bellman equations and policy evaluation schemes. Building on this theoretical framework, they introduce an algorithm that represents the joint return distribution using a deep Gaussian Mixture Model (GMM). The proposed approach is then evaluated by numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall well-written and easy to follow.\n* The theoretical derivation is clear and sound.\n* The idea of considering joint return distribution is intuitive."}, "weaknesses": {"value": "* Most importantly, the method relies on access to a $\\tau^2$ trajectory, an assumption that is incompatible with the standard RL framework. Although the author argues such a trajectory can be obtained by assuming an omniscient simulator (like digital twins), this is not the case for nearly all known practical applications. This core assumption severely limits the practical scope of the paper.\n* The experiments are all done in environments where $N$ is small. The authors should discuss whether the proposed approach can scale up as $|\\mathcal{S}|$ and $|\\mathcal{A}|$ become large.\n* Section 4.2 appears to contain an error. The authors define a safe control objective $\\max_\\pi \\pi^\\top\\mu-\\lambda\\pi^\\top\\Sigma\\pi$ and call it a quadratic program. Since $\\mu$ and $\\Sigma$ depend on $\\pi$, this is not a QP. This formulation needs to be corrected or clarified."}, "questions": {"value": "* The paper appears to primarily leverage the first and second-order moments of the joint return distribution for the downstream analysis. Could the authors clarify the necessity of modeling the entire distribution? What specific advantages does this full-distributional approach offer over a simpler method that might directly model only the first two moments?\n* The framework's benefits for the interpretability of MDPs are clear. However, could the authors elaborate on its utility for decision-making? Specifically, does modeling the joint return distribution enable policy improvements or optimization strategies that are not achievable with existing methods (e.g., standard distributional RL or moment-based approaches)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5vx2D2L0aX", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_7hCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_7hCi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558002535, "cdate": 1761558002535, "tmdate": 1762942181556, "mdate": 1762942181556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The motivation is clear. Modeling the joint distribution among actions is natural and technically sound. It would be well expected that the additional correlation information among actions would be beneficial for policy optimization or the general decision-making.\n\n2. Experiments are comprehensive, which involves multiple domains. I found it interesting to establish the connection between the correlation information among actions with the interpretability of sequential decision-making."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation is clear. Modeling the joint distribution among actions is natural and technically sound. It would be well expected that the additional correlation information among actions would be benefits for policy optimization or the general decision making.\n\n2.Experiments are comprehensive, which involves multiple domains. I found it is interesting to establish the connection between the correlation information among actions with the interpretability of sequential decision making."}, "weaknesses": {"value": "1. **Paper organization and writing need to be largely improved**. (1) The empirical results are provided very early, which is too far away from the experimental sections, undermining the readability. (2) I think the motivation of a joint modeling of return distribution among actions is easy to understand. Therefore, there is no need to list two naïve examples in the introduction part. It would be more suggested to directly focus on the general setting with a clear illustration in the introduction. (3) It is not clear whether it is necessary to propose the POMDP framework, which seems less connected with the practical algorithmic design and following theoretical analysis. (4) Jointly learning the correlations of actions is not a novel idea, which is natural in the policy gradient literature. However, this idea is not widely adopted, as additionally learning the correlation, such as the covariance matrix, often increases the computational instability. Therefore, this paper should focus on discussion of this practical factors. What is the price we need to pay for practical algorithms?\n\n2. **Theoretical results are less convincing**. (1) Some formulations and explanations are on binary action settings, e,.g., in Line 209, and it is less clear how to extend them to the general action case. (2) This paper implicitly applies the Gaussian assumption or is restricted to the first and second moments of learning in the joint distribution learning. This limitation should be elaborated. What is the consequence? (3) In Section 2.2, it is less clear how the joint Bellman operator works on both the first and second moments of return distribution. (4) If we only consider the first two moments, the results can also be extended to Wasserstein distance in Theorem 1. In Theorems 2 and 3, can we derive the contraction property of the joint Bellman operator?\n\n3. **Limited novelty of algorithm**. (1) Using the mixture Gaussian modeling is limited in novelty. (2) In line 346, does the summation increase exponentially in terms of the state space?\n\n3. **Explanations of experimental results are less satisfactory**.  In my opinion, the experiments should focus on large-scaled setting with more Atari games and Mujoco environments. The policy evaluation setting and Cliff walking are weak. This is because readers will easily expect the benefits of using joint distribution modeling in practice. They expect some off-the-shelf methods to further improve the existing (distributional) RL algorithms. I think the explanations in the section 4.3 look interesting, and relevant explanations on more experiments are expected. This would also contribute to interpretable RL."}, "questions": {"value": "Overall, I think this research direction is potential, but the current version of the paper fails to sufficiently present the research value with the critical weaknesses limited above. Here are my other questions of this paper.\n\n1.Figure 1 is introduced in the first section, but there lacks sufficient explanation, e.g., about the benefits of a joint learning. It is more suggested to use a more general and clear illustration in the intro with more detailed explanations.\n\n2.It is hard to follow the motivation of POMDP. More explanations are helpful.\n\n3.In line 177, we estimate the true parameters where $\\hat{\\mu}_1$ and so on are the estimates instead of the true parameters.\n\n4.How do we do the inference when we have the covariance matrix among actions? The drawback is that it reduces the inference speed in practice. This limitation also needs to be emphasized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q7vMM0oWes", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_XMpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_XMpF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601456687, "cdate": 1761601456687, "tmdate": 1762942181290, "mdate": 1762942181290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of outcome (reward and next state) dependence among different actions in reinforcement learning (RL). To estimate the joint distribution of returns of all actions at a state, the paper models a POMDP, named the joint MDP, and designs the joint iterative policy evaluation (JIPE) algorithm.\nThe authors derive the joint distributional Bellman equations, analyze their contractive properties, and thereby establish the convergence of the JIPE algorithm. For practical implementation, the paper proposes using a Gaussian mixture model to model the joint distribution, and employs neural networks to estimate the mean function and covariance function.\nFinally, the authors conduct extensive experiments to demonstrate the effectiveness of their proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors clearly illustrate the problem they address. For instance, Figure 1 provides an intuitive visualization of the outcome dependence phenomenon, which effectively helps readers understand the problem.\n\n2. The paper presents error analysis. Additionally, the authors conduct a detailed analysis of their experimental results."}, "weaknesses": {"value": "1. The paper’s motivation lacks clarity: While I acknowledge that the phenomenon of outcome dependence is natural in RL, the authors fail to clearly elaborate on why this problem is critical. \nFrom an algorithmic perspective, learning an optimal policy does not necessarily require accounting for such outcome dependence. Although the authors attempt to demonstrate the superiority of their proposed JIPE algorithm across various problems in the experimental section, these demonstrations are not sufficiently convincing, as detailed below:​\n\nIn Section 4.2, the authors consider the cliff walking problem and attempt to show that accounting for outcome dependence can yield a risk-averse policy. However, they do not explain how to obtain the optimal solution to the proposed Markowitz problem—this problem is, in fact, highly similar to the mean-variance optimization problem discussed in Chapter 7 of Bellemare et al. (2023) and is notoriously difficult to solve. Furthermore, the experimental results only briefly report mean, standard deviation, maximum, and minimum, without including common risk measures (e.g., Value-at-Risk, Conditional Value-at-Risk). Additionally, risk-sensitive RL algorithms are absent from the baselines in this set of experiments.\n\nIn Sections 4.3 and 4.4, the authors use examples such as Cartpole and Pong to argue that the effective rank of the covariance matrix can distinguish whether the agent is in a \"critical state\". They claim that it can help reduce operational costs when action execution incurs costs. While this analysis seems intuitively appealing, the argument is incomplete and unconvincing. For instance, the authors do not rule out the possibility that the value functions (i.e. expected returns) of different actions are close in \"post-critical states\"; in such cases, using only first-order information (value function) to decide whether to act would be simpler and more justifiable. I recommend that the authors refine this part and provide more rigorous reasoning to support their claims.\n\n2. The proposed algorithm relies on a simulator capable of counterfactual reasoning, which imposes significant limitations on its practical applicability."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D1IoNBdTug", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_v334"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_v334"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728442780, "cdate": 1761728442780, "tmdate": 1762942180897, "mdate": 1762942180897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The main contribution of the paper is to highlight the importance of joint return modeling in Distributional Reinforcement Learning (DRL). It proposes to explicitly model the coupling between the returns of different actions at a given state using K-component Gaussian Mixture Models (GMMs). By introducing a joint MDP formulated as a POMDP with hidden coupled potential outcomes, the paper derives joint distributional Bellman relations and, in particular, introduces a Joint Iterative Policy Evaluation (JIPE) method that comes with convergence guarantees for joint means and second moments.\n\n* The experimental section focuses on synthetic MDPs and small-scale control tasks to validate the theoretical results and to qualitatively demonstrate some benefits of the proposed approach, namely interpretability (through alignment with state criticality), safety (via improved control of the mean/variance trade-off), and cost-effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a principled and mathematically rigorous formulation, supported by solid proofs including convergence guarantees. The problem is clearly specified, particularly through the joint MDP/POMDP formulation with joint transitions. Provided that a simulator or digital-twin environment is available—capable of counterfactual replay of potential outcomes (state transitions and rewards)—the estimation of cross-moments is feasible.\n\n* Although the experiments are limited to synthetic MDPs and relatively simple “toy” problems, they convincingly validate the theoretical framework and effectively illustrate the potential advantages of the proposed method."}, "weaknesses": {"value": "* The assumption that one can counterfactually replay next-state/reward outcomes for other actions at the same state may be infeasible in many real-world scenarios. It would be valuable to provide practical insights or strategies for mitigating this limitation.\n\n* The modeling of the joint return distribution is restricted to second-order statistics (means, variances, and covariances). The authors could strengthen their discussion by justifying why this second-order approximation is often sufficient, or by outlining possible extensions to higher-order moments and non-Gaussian distributions.\n\n* Similarly, it would be helpful to discuss potential extensions of the proposed approach to continuous action spaces."}, "questions": {"value": "(No question in particular. See previous section for suggestions of improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ELIgdZBNv", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_693N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_693N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888649779, "cdate": 1761888649779, "tmdate": 1762942180304, "mdate": 1762942180304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}