{"id": "8rMJFiz38Y", "number": 22352, "cdate": 1758329949333, "mdate": 1763767898485, "content": {"title": "Beyond Marginals: Capturing Dependent Returns through Joint Moments in Distributional Reinforcement Learning", "abstract": "Distributional reinforcement learning (DRL) has emerged as a paradigm that aims to learn full distributions of returns under a policy, rather than only their expected values. The existing DRL algorithms learn the return distribution independently for each action at a state. However, we establish that in many environments, the returns for different actions at the same state are statistically dependent due to shared transition and reward structure, and that learning only per-action marginals discards information that is exploitable for secondary objectives. We formalize a joint Markov decision process (MDP) view that lifts an MDP into a partially-observable MDP whose hidden states encode coupled potential outcomes across actions, and we derive joint distributional Bellman equations together with a joint iterative policy evaluation (JIPE) scheme with convergence guarantees. We introduce a deep learning method that represents joint returns with Gaussian mixture models with optimality and convergence guarantees. Empirically, we first validate the JIPE scheme on MDPs with known correlation structure. Then, we illustrate the learned joint structure in control and Arcade Learning Environment tasks using neural networks. Together, these results demonstrate that modeling return dependencies yields accurate joint moments and joint distributions that can help interpretability and be used in deriving safe and cost-efficient policies.", "tldr": "", "keywords": ["reinforcement learning", "distributional reinforcement learning", "interpretability", "safe reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66cc7eba779afcedfa6fe8481ab9b27de7234f2e.pdf", "supplementary_material": "/attachment/8eaae8d6ffe2729d13bcaba32ff7ce1dfeec5212.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes modeling the joint return distribution in distributional reinforcement learning. The authors formalize this problem by constructing a joint MDP and deriving the corresponding joint Bellman equations and policy evaluation schemes. Building on this theoretical framework, they introduce an algorithm that represents the joint return distribution using a deep Gaussian Mixture Model (GMM). The proposed approach is then evaluated by numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is overall well-written and easy to follow.\n* The theoretical derivation is clear and sound.\n* The idea of considering joint return distribution is intuitive."}, "weaknesses": {"value": "* Most importantly, the method relies on access to a $\\tau^2$ trajectory, an assumption that is incompatible with the standard RL framework. Although the author argues such a trajectory can be obtained by assuming an omniscient simulator (like digital twins), this is not the case for nearly all known practical applications. This core assumption severely limits the practical scope of the paper.\n* The experiments are all done in environments where $N$ is small. The authors should discuss whether the proposed approach can scale up as $|\\mathcal{S}|$ and $|\\mathcal{A}|$ become large.\n* Section 4.2 appears to contain an error. The authors define a safe control objective $\\max_\\pi \\pi^\\top\\mu-\\lambda\\pi^\\top\\Sigma\\pi$ and call it a quadratic program. Since $\\mu$ and $\\Sigma$ depend on $\\pi$, this is not a QP. This formulation needs to be corrected or clarified."}, "questions": {"value": "* The paper appears to primarily leverage the first and second-order moments of the joint return distribution for the downstream analysis. Could the authors clarify the necessity of modeling the entire distribution? What specific advantages does this full-distributional approach offer over a simpler method that might directly model only the first two moments?\n* The framework's benefits for the interpretability of MDPs are clear. However, could the authors elaborate on its utility for decision-making? Specifically, does modeling the joint return distribution enable policy improvements or optimization strategies that are not achievable with existing methods (e.g., standard distributional RL or moment-based approaches)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5vx2D2L0aX", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_7hCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_7hCi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558002535, "cdate": 1761558002535, "tmdate": 1762942181556, "mdate": 1762942181556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [1/3]"}, "comment": {"value": "## Part (1/3)\n\nWe thank the reviewers for their kind praise of the work and their constructive criticism. We are honored to see that the reviewers remarked that the work is well-written and easy to follow **(Reviewer 7hCi)**, principled and mathematically rigorous, supported by solid, clear and sound theory **(Reviewers 693N, 7hCi)**, that the potential advantages of the proposed method are effectively illustrated **(Reviewer 693N)**, that the experimental results are comprehensive **(Reviewer XMpF)** and analyzed in detail **(Reviewer v334)**, that the motivation is clear and the main interest of the paper is natural, intuitive and technically sound **(Reviewers 7hCi, XMpF)**. We believe that a work that simultaneously possesses all of these qualities is worthy of publication in ICLR.\n\nIn the following, we would like to address some of the common points raised collectively by the reviewers.\n\n1. **The method is based on counterfactual next-state/reward outcomes for multiple actions at the same state (what is stylized as $\\tau^2$ trajectories in the manuscript). This may be unattainable in real-world scenarios and some practical applications. (Reviewers 693N, v334, 7hCi)**\n\nWe would like to remind the reviewers that **the main contribution of this paper is meant to be theoretical**. Our main goal is to draw attention to this unexplored perspective on reinforcement learning and primarily attempt to lay its theoretical foundations. We envision that follow-up works can potentially leverage the ideas presented here in a more application-oriented, practically-polished manner. With this being said, it is not out of the ordinary to see, in theoretical analyses pertaining to reinforcement learning, to presume access to a generative model, which one can query at will like an oracle to obtain samples of the reward or the next state of a state-action pair [1, 2, 3]. We believe it is not far-fetched to make a similar assumption on the existence of a generative model which would instead give us joint trajectories $\\tau^2$. In fact, **this is essentially what is being assumed in lines 207-211 of the original manuscript.** There already exist published works that assume such access in explainable RL literature, cf. [4] for one example.\n\nStill, although one can ideally assume any premise to draw theoretical conclusions, we are aware that such conclusions are of little worth if the assumptions are almost sure not to hold in practice. For this reason, we also wanted to provide **practical scenarios where it is straightforward to implement such an oracle access. As can be read in lines 212-230 of the original manuscript, in common benchmark simulation environments that virtually all RL works use, such as the ALE Atari environments or Gymnasium control environments, it is remarkably simple to achieve this with only a handful lines of code changed.** It is as easy as saving a temporary backup of the state of the random number generator(s) and the environment before taking an action, recording the next state and reward that follow, restoring the state of the random number generator(s) and the environment to the backed-up states, taking a different action and recording its next state, and rewards, and so on, for the required number of joint actions. Because all such rewards/next states are obtained under the same, common source of stochasticity, we then view these as joint samples from the joint POMDP. We invite the reviewers to view our implementation within the supplementary files we have shared along with our original submission.\n\n**We do acknowledge that there will naturally be scenarios and applications where we will not be able to achieve this, or have a simulator at all. Even in these scenarios, we posit that there are possible workarounds.** For instance, we refer the reviewers to [5], which proposes the use of causal models to estimate counterfactual next state/reward outcomes for actions alternative to the one taken. We believe such works can be a viable alternative in scenarios where we are not able to sample counterfactual outcomes through an oracle or a simulator."}}, "id": "FLxM8npEWr", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763768378546, "cdate": 1763768378546, "tmdate": 1763768378546, "mdate": 1763768378546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The motivation is clear. Modeling the joint distribution among actions is natural and technically sound. It would be well expected that the additional correlation information among actions would be beneficial for policy optimization or the general decision-making.\n\n2. Experiments are comprehensive, which involves multiple domains. I found it interesting to establish the connection between the correlation information among actions with the interpretability of sequential decision-making."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation is clear. Modeling the joint distribution among actions is natural and technically sound. It would be well expected that the additional correlation information among actions would be benefits for policy optimization or the general decision making.\n\n2.Experiments are comprehensive, which involves multiple domains. I found it is interesting to establish the connection between the correlation information among actions with the interpretability of sequential decision making."}, "weaknesses": {"value": "1. **Paper organization and writing need to be largely improved**. (1) The empirical results are provided very early, which is too far away from the experimental sections, undermining the readability. (2) I think the motivation of a joint modeling of return distribution among actions is easy to understand. Therefore, there is no need to list two naïve examples in the introduction part. It would be more suggested to directly focus on the general setting with a clear illustration in the introduction. (3) It is not clear whether it is necessary to propose the POMDP framework, which seems less connected with the practical algorithmic design and following theoretical analysis. (4) Jointly learning the correlations of actions is not a novel idea, which is natural in the policy gradient literature. However, this idea is not widely adopted, as additionally learning the correlation, such as the covariance matrix, often increases the computational instability. Therefore, this paper should focus on discussion of this practical factors. What is the price we need to pay for practical algorithms?\n\n2. **Theoretical results are less convincing**. (1) Some formulations and explanations are on binary action settings, e,.g., in Line 209, and it is less clear how to extend them to the general action case. (2) This paper implicitly applies the Gaussian assumption or is restricted to the first and second moments of learning in the joint distribution learning. This limitation should be elaborated. What is the consequence? (3) In Section 2.2, it is less clear how the joint Bellman operator works on both the first and second moments of return distribution. (4) If we only consider the first two moments, the results can also be extended to Wasserstein distance in Theorem 1. In Theorems 2 and 3, can we derive the contraction property of the joint Bellman operator?\n\n3. **Limited novelty of algorithm**. (1) Using the mixture Gaussian modeling is limited in novelty. (2) In line 346, does the summation increase exponentially in terms of the state space?\n\n3. **Explanations of experimental results are less satisfactory**.  In my opinion, the experiments should focus on large-scaled setting with more Atari games and Mujoco environments. The policy evaluation setting and Cliff walking are weak. This is because readers will easily expect the benefits of using joint distribution modeling in practice. They expect some off-the-shelf methods to further improve the existing (distributional) RL algorithms. I think the explanations in the section 4.3 look interesting, and relevant explanations on more experiments are expected. This would also contribute to interpretable RL."}, "questions": {"value": "Overall, I think this research direction is potential, but the current version of the paper fails to sufficiently present the research value with the critical weaknesses limited above. Here are my other questions of this paper.\n\n1.Figure 1 is introduced in the first section, but there lacks sufficient explanation, e.g., about the benefits of a joint learning. It is more suggested to use a more general and clear illustration in the intro with more detailed explanations.\n\n2.It is hard to follow the motivation of POMDP. More explanations are helpful.\n\n3.In line 177, we estimate the true parameters where $\\hat{\\mu}_1$ and so on are the estimates instead of the true parameters.\n\n4.How do we do the inference when we have the covariance matrix among actions? The drawback is that it reduces the inference speed in practice. This limitation also needs to be emphasized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q7vMM0oWes", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_XMpF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_XMpF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601456687, "cdate": 1761601456687, "tmdate": 1762942181290, "mdate": 1762942181290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [2/3]"}, "comment": {"value": "## Part (2/3)\n\nMore specifically, building on Example 2 from our submission, we posit that the environment's stochasticity at time $t$ stems from a shared, unobserved noise vector $U_t$. The joint outcomes (next states and rewards for all actions) are a function of the current state and this noise: $(S'\\_{t+1, a\\_1}, ..., S'\\_{t+1, a\\_N}, R\\_{t+1, a\\_1}, ..., R\\_{t+1, a\\_N}) = f(s\\_t, U\\_t)$. This $f$ is the true joint structural causal model (SCM). A single observed transition $(s\\_t, a\\_i, r\\_i, s'\\_{i})$ is thus one marginal realization from this joint function. Then, instead of rewinding, we would collect only standard experience tuples $\\tau = (s, a, r, s')$ and learn a generative model that captures $f$ given only these marginal samples. In doing so, we would learn both a generative model $G(s\\_t, u\\_t)$ (approximating $f$) and an inference network (or encoder) $E(s\\_t, a, r, s')$ that seeks to infer the underlying noise $\\hat{u}\\_t$ that must have occurred to produce the observed transition. To generate a full joint transition $\\tau^2$ from state $s\\_t$, we would (1) sample a real transition $(s\\_t, a, r, s')$ from the replay buffer, (2) infer the latent noise $\\hat{u}\\_t = E(s\\_t, a, r, s')$, (3) generate the full set of counterfactual outcomes using this same noise vector: $(\\hat{S}'\\_{a\\_1}, ..., \\hat{R}\\_{a\\_N}) = G(s\\_t, \\hat{u}\\_t)$. This generated $\\tau^N$ tuple can then be used by our algorithm. \n\nWe have added remarks on this topic to Appendix I of the updated manuscript to address any concerns related to the matter.\n\n2. **Why are we restricted to the first and second moments / Why do we assume \"that the distribution is Gaussian?\" (Reviewers 693N, XMpF, 7hCi)**\n\nWe would like to first emphasize that our modeling of the return distributions in Section 3 is not by Gaussian distributions, but by **mixtures of Gaussian distributions**. This is a critical distinction. Gaussian distributions are limited in expressivity and come with plenty of implicit restrictions about the shape of the distribution: That it is unimodal, symmetric, etc., which customarily do not hold for return distributions in many RL scenarios of interest (see Section 5.4 of [2] for a discussion). Gaussian mixture models, on the other hand, are much more expressive and lift many such restrictions. Indeed, it is well known that **finite Gaussian mixtures are dense in the space of continuous probability distributions under standard metrics, i.e., a Gaussian mixture model with enough (but finitely many) components can approximate any continuous distribution arbitrarily well.** Our Theorems 5 and 7, in the appendix, also highlight this fact in the context of RL, under the Wasserstein and Cramér distances.\n\nThe sufficiency of the second-order approximation follows as a design choice. Any DRL algorithm has to make a choice on how they want to model the return distributions and this consequentially dictates what they need to estimate to parameterize their model. **In our case, we decide to model the distributions as GMMs, and second-order statistics are sufficient to do this.** However, ideally, a smart design choice would involve choosing a model which can then be exploited favorably for the considerations that the designer wants to perform well in. **In our manuscript, we have purposefully highlighted numerous concrete applications for which first-order statistics (the means) are not sufficient, but the second-order statistics are. Namely, these applications are safety, interpretability, and cost-efficiency.** We have provided explicit formulations where the use of these second-order statistics leads to improvement in secondary considerations, in Sections 4.2, 4.3, and 4.4, respectively. It is reasonably straightforward to extend the proposed framework to even higher-order moments, such as skewness or kurtosis, with minimal changes. Notably, these higher order statistics would require joint samples involving more counterfactual actions, i.e., trajectories of form $\\tau^3 = (s, a\\_1, a\\_2, a\\_3, r\\_1, r\\_2, r\\_3, s'\\_1, s'\\_2, s'\\_3, a\\_1', a\\_2', a\\_3')$, and so on.\n\nFinally, an additional factor in favor of using Gaussian mixtures to model joint distributions is the fact that **many of the more granular distributions that are used in the marginal, univariate setting (such as the categorical distribution of C51) scale exponentially in the number of parameters with the dimensionality of the distribution, whereas the number of parameters of Gaussian distributions scale only polynomially.** Additionally, quantile (QR-DQN-like) methods are not easily extendable to our joint setting, as a multivariate quantile function is not uniquely, tractably defined for optimization, and does not offer a simple regression target."}}, "id": "hA80olvLUW", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763768455794, "cdate": 1763768455794, "tmdate": 1763768455794, "mdate": 1763768455794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of outcome (reward and next state) dependence among different actions in reinforcement learning (RL). To estimate the joint distribution of returns of all actions at a state, the paper models a POMDP, named the joint MDP, and designs the joint iterative policy evaluation (JIPE) algorithm.\nThe authors derive the joint distributional Bellman equations, analyze their contractive properties, and thereby establish the convergence of the JIPE algorithm. For practical implementation, the paper proposes using a Gaussian mixture model to model the joint distribution, and employs neural networks to estimate the mean function and covariance function.\nFinally, the authors conduct extensive experiments to demonstrate the effectiveness of their proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors clearly illustrate the problem they address. For instance, Figure 1 provides an intuitive visualization of the outcome dependence phenomenon, which effectively helps readers understand the problem.\n\n2. The paper presents error analysis. Additionally, the authors conduct a detailed analysis of their experimental results."}, "weaknesses": {"value": "1. The paper’s motivation lacks clarity: While I acknowledge that the phenomenon of outcome dependence is natural in RL, the authors fail to clearly elaborate on why this problem is critical. \nFrom an algorithmic perspective, learning an optimal policy does not necessarily require accounting for such outcome dependence. Although the authors attempt to demonstrate the superiority of their proposed JIPE algorithm across various problems in the experimental section, these demonstrations are not sufficiently convincing, as detailed below:​\n\nIn Section 4.2, the authors consider the cliff walking problem and attempt to show that accounting for outcome dependence can yield a risk-averse policy. However, they do not explain how to obtain the optimal solution to the proposed Markowitz problem—this problem is, in fact, highly similar to the mean-variance optimization problem discussed in Chapter 7 of Bellemare et al. (2023) and is notoriously difficult to solve. Furthermore, the experimental results only briefly report mean, standard deviation, maximum, and minimum, without including common risk measures (e.g., Value-at-Risk, Conditional Value-at-Risk). Additionally, risk-sensitive RL algorithms are absent from the baselines in this set of experiments.\n\nIn Sections 4.3 and 4.4, the authors use examples such as Cartpole and Pong to argue that the effective rank of the covariance matrix can distinguish whether the agent is in a \"critical state\". They claim that it can help reduce operational costs when action execution incurs costs. While this analysis seems intuitively appealing, the argument is incomplete and unconvincing. For instance, the authors do not rule out the possibility that the value functions (i.e. expected returns) of different actions are close in \"post-critical states\"; in such cases, using only first-order information (value function) to decide whether to act would be simpler and more justifiable. I recommend that the authors refine this part and provide more rigorous reasoning to support their claims.\n\n2. The proposed algorithm relies on a simulator capable of counterfactual reasoning, which imposes significant limitations on its practical applicability."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D1IoNBdTug", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_v334"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_v334"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728442780, "cdate": 1761728442780, "tmdate": 1762942180897, "mdate": 1762942180897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response [3/3]"}, "comment": {"value": "## Part (3/3)\n\n3. **The Markowitz problem of Section 4.2 is erroneous / not a quadratic program / solving it is near infeasible in practice. (Reviewers v334, 7hCi)**\n\nWe have realized that at least two reviewers have some questions about the Markowitz problem of Section 4.2, and both of these ultimately tie back to the same source of confusion. Firstly, this is a valid quadratic program, and the quantities $\\mu(s)$ and $\\Sigma(s)$ at each state $s$ are independent of the optimization variable $\\pi$ and are functions of the unsafe policy $\\pi_u$. As we state in the manuscript, $\\mu$ and $\\Sigma$ follow from the evaluation (by JIPE) of the unsafe policy $\\pi_u$. They thus constitute the state-indexed collection of mean vectors and covariance matrices of returns belonging to policy $\\pi_u$. We subsequently solve the quadratic program $\\max_{\\pi\\in\\Delta_N} \\pi^T\\mu(s) -\\lambda\\pi^T\\Sigma(s)\\pi$ at each state $s$, and we use a solution $\\pi_M$ of this problem as our deployment policy. **This process can alternatively be viewed as a post-hoc one-step policy improvement step, which, given relevant (first- and second-order) statistics of a policy $\\pi_u$, returns a policy $\\pi_M$, improved in the sense of safety.** We have taken the general confusion as feedback that this part should be clarified and better worded, and made the necessary updates.\n\n### References\n\n[1] Agarwal, Alekh, et al. \"Reinforcement learning: Theory and algorithms.\" CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32 (2019): 96.\n\n[2] Bellemare, Marc G., Will Dabney, and Mark Rowland. Distributional reinforcement learning. MIT Press, 2023.\n\n[3] Kearns, Michael, and Satinder Singh. \"Finite-sample convergence rates for Q-learning and indirect algorithms.\" Advances in neural information processing systems 11 (1998).\n\n[4] Amitai, Yotam, Yael Septon, and Ofra Amir. \"Explaining reinforcement learning agents through counterfactual action outcomes.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 9. 2024.\n\n[5] Lu, Chaochao, et al. \"Sample-efficient reinforcement learning via counterfactual-based data augmentation.\" arXiv preprint arXiv:2012.09092 (2020)."}}, "id": "uO0W7eRDKy", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763768498099, "cdate": 1763768498099, "tmdate": 1763768498099, "mdate": 1763768498099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The main contribution of the paper is to highlight the importance of joint return modeling in Distributional Reinforcement Learning (DRL). It proposes to explicitly model the coupling between the returns of different actions at a given state using K-component Gaussian Mixture Models (GMMs). By introducing a joint MDP formulated as a POMDP with hidden coupled potential outcomes, the paper derives joint distributional Bellman relations and, in particular, introduces a Joint Iterative Policy Evaluation (JIPE) method that comes with convergence guarantees for joint means and second moments.\n\n* The experimental section focuses on synthetic MDPs and small-scale control tasks to validate the theoretical results and to qualitatively demonstrate some benefits of the proposed approach, namely interpretability (through alignment with state criticality), safety (via improved control of the mean/variance trade-off), and cost-effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a principled and mathematically rigorous formulation, supported by solid proofs including convergence guarantees. The problem is clearly specified, particularly through the joint MDP/POMDP formulation with joint transitions. Provided that a simulator or digital-twin environment is available—capable of counterfactual replay of potential outcomes (state transitions and rewards)—the estimation of cross-moments is feasible.\n\n* Although the experiments are limited to synthetic MDPs and relatively simple “toy” problems, they convincingly validate the theoretical framework and effectively illustrate the potential advantages of the proposed method."}, "weaknesses": {"value": "* The assumption that one can counterfactually replay next-state/reward outcomes for other actions at the same state may be infeasible in many real-world scenarios. It would be valuable to provide practical insights or strategies for mitigating this limitation.\n\n* The modeling of the joint return distribution is restricted to second-order statistics (means, variances, and covariances). The authors could strengthen their discussion by justifying why this second-order approximation is often sufficient, or by outlining possible extensions to higher-order moments and non-Gaussian distributions.\n\n* Similarly, it would be helpful to discuss potential extensions of the proposed approach to continuous action spaces."}, "questions": {"value": "(No question in particular. See previous section for suggestions of improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9ELIgdZBNv", "forum": "8rMJFiz38Y", "replyto": "8rMJFiz38Y", "signatures": ["ICLR.cc/2026/Conference/Submission22352/Reviewer_693N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22352/Reviewer_693N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888649779, "cdate": 1761888649779, "tmdate": 1762942180304, "mdate": 1762942180304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}