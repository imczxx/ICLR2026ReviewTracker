{"id": "P2o8wOzZMX", "number": 23342, "cdate": 1758342442383, "mdate": 1759896819915, "content": {"title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "abstract": "Large language models (LLMs) are increasingly used as reasoning partners in domains such as mathematics, coding, and decision support, where reliable expression of confidence is essential for human–AI interaction. However, current LLMs often generate incorrect answers with high confidence, causing significant risks in downstream decision-making. Prior approaches for inducing verbalized confidence, including reinforcement learning or external probing, have shown limited generalization across complex reasoning and unseen tasks. We introduce Confidence-Supervised Fine-Tuning (CSFT), a simple method that trains models to output both an answer and an explicit confidence statement. CSFT substantially reduces calibration errors while also improving accuracy, induces emergent self-verification behaviors such as self-checking under low confidence, and reshapes token distributions into a locally smooth structure around correct answers. Furthermore, although trained only on reasoning tasks, CSFT generalizes to non-reasoning benchmarks such as MMLU. These results establish verbalized confidence as a scalable mechanism for improving calibration, reasoning, and generalization in LLMs.", "tldr": "CSFT trains LLMs to verbalize confidence, yielding better calibration, accuracy, self-verification, and cross-task generalization.", "keywords": ["calibration", "reasoning", "LLMs", "self-verfication"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec7b5956f9d0bc9c4faa255bb3598b352ae91e1d.pdf", "supplementary_material": "/attachment/317de47731fe94181fe9e33e6580dcde344dcf10.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents confidence-supervised fine-tuning (CSFT), a training paradigm where models are trained to express verbalized confidence. The ground truth confidence for a particular question is approximated as the empirical accuracy of the model on that task. The model is then optimized to output well-calibrated confidence statements. Optimizing with this simple objective improves both accuracy and calibration, and leads to emergent CoT self-checking reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Simple, relevant and well-motivated**: The proposed method is simple yet tackles the important problem of model calibration. Calibration is increasingly critical for deploying reasoning models in real-world applications. \n\n- **Interesting results**: Despite its simplicity, the method appears to produce notable and somewhat surprising empirical results (e.g., accuracy improvements), which merit deeper investigation."}, "weaknesses": {"value": "- **Novelty**: It is unclear what the novelty of this framework is. There have been previous works from as long as 3 years ago [1] which use a very similar setup. \n- **Experimental Design and Strength**: No baselines (except a simple pre-trained model) are presented in the results, making it difficult to identify the strengths of the method relative to existing works in the field. Some results presented in this paper are also surprising, and merit deeper analysis. In particular, it is unclear to me how this method improves accuracy without optimizing for it in any way. \n- **Writing**: The writing of the paper is informal at times and can be significantly improved. For example “we basically do not apply loss to the reasoning trace” (Line 185) makes it unclear whether a loss to the reasoning trace is being applied or not. \n\n[1]: Lin, S., Hilton, J., & Evans, O. (2022). Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334."}, "questions": {"value": "- **Confidence supervision**: Why is empirical accuracy used as a proxy for confidence? This approach marginalizes out the reasoning trace, which is often key to determining confidence (e.g., models may realize mid-reasoning that a solution is correct).\n\n- **Relation to classifiers**: How does this method differ from training a standard classifier that predicts correctness given a reasoning trace? If the model simply outputs a confidence score without reasoning about it, the setup seems equivalent to classification fine-tuning.\n\n- **Novelty**: How does this work differ from prior studies (e.g., [1]) that fine-tuned verbalized confidence using accuracy supervision? How exactly is this different from training a classifier?\n\n- **Accuracy Improvement**: Given that no loss is applied to reasoning traces or answers, why does accuracy improve? A deeper investigation of why this is happening is important, as this is an unintended by-product. Please clear my understanding if I am wrong, but since there is no optimization over the reasoning traces and final answer, and there is a KL to the original model, then optimizing such an objective should not alter the reasoning traces or accuracy at all. Also, the claim of accuracy improving does not seem to be generally substantiated. The accuracy improvements in table 1 for the Qwen model are negligent, suggesting that this trend does not hold generally. \n\n- **Confidence variance**: What is the variance of predicted confidence across different reasoning traces or answers for the same question? \n\n- **Confidence-Guided Reasoning Path Refinement**:  Please provide more details on this. Is this method only applicable for the prefix confidence variant? If a model is prompted for confidence directly after the question, then how will resampling when it outputs low confidence increase accuracy, even if the prompt is different? The resampling prompt asks it to consider an alternative approach but based on my understanding, it has not considered any approach yet (since the confidence was directly after the question). What does alternate refer to in such a scenario? Instead, a better alternative might be to do sequential sampling with the suffix prompt. If the confidence turns out to be low, reprompt the model with its original attempt and predicted confidence, and ask it to generate a new answer with higher confidence.  \n\n- **Figure 1:** Fig 1 is not a good way to present results. The bars in the graph represent different metrics and thus the deltas are not easily comparable. Table 1 is a better way to summarize these results. \n\n- **Baselines**: Please add more baselines to allow placing this work relative to existing work: \n     - *Shared Classifier*: Train a classifier on top of the reasoning model. This baseline should also have the same accuracy as the pre-trained model, but it uses representations of the model which is generating the answers.\n     - *Separate Classifier*: Train a separate classifier model conditioned on generations from the reasoning model. This baseline should have same accuracy as the pre-trained model.\n\n[1]: Lin, S., Hilton, J., & Evans, O. (2022). Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9deCiylfE6", "forum": "P2o8wOzZMX", "replyto": "P2o8wOzZMX", "signatures": ["ICLR.cc/2026/Conference/Submission23342/Reviewer_V8Nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23342/Reviewer_V8Nd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678509804, "cdate": 1761678509804, "tmdate": 1762942616081, "mdate": 1762942616081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests a method to finetune including information on confidence and output answers and confidence about the result. They train only on reasoning tasks but the trained method generalizes to other tasks too. Two main methods are known to output directly confidence information or to use a meta network to predict this information by probing the internal states. They claim their method goes beyond those as it generalizes to unseen tasks. In addition, they show that it improves downstream tasks results too."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The method provides confidence along the answers whiout altering the model too much due to KL-based normalization. \n\nThe method improves on other tasks ie. generalizes\n\nThe mothod seems to improve answer quality \n\nI enyoed reading the paper and was informative too me, good method."}, "weaknesses": {"value": "I could not figure out imediatley how they get a spectrum of confidence while the feedback is binary? Correct vs incorrect? Could you write this more clear pls?\n\nI wonder how much we loose actualy of the models abilities even which could be assessed. Maybe, missed this point. \n\nMissed out on some literature on attribution to a source like retrieval in a loop, e.g. attributed question answering which has similar aims which provides evidence via retrieval."}, "questions": {"value": "Great paper. Really worth reading it. I was still wondering how you get percentage confidence scores while the feedback is binary.\n\nI wonder if you train on the traces, I guess the improvment might leak over to a correct answers also if you only provide the confidence? However, you get also benefits on tasks you do not train on? Is thsi perception correct that you might have indirekt leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ykUxDT6t3m", "forum": "P2o8wOzZMX", "replyto": "P2o8wOzZMX", "signatures": ["ICLR.cc/2026/Conference/Submission23342/Reviewer_7KJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23342/Reviewer_7KJt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739872495, "cdate": 1761739872495, "tmdate": 1762942614341, "mdate": 1762942614341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates verbalized confidence in LLMs which is the ability to state how confident they are in the answers. The authors propose confidence-supervised fine-tuning (CSFT) to train LLMs to generate both an answer and a corresponding confidence statement. \nRather than relying on external clasisifers or RL, CSFT uses uses direct supervision on confidence tokens which enables the model to self-calibrate its uncertainty. Experiments on reasoning and non-reasoning benchmarks show that CSFT improves both calibration and accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper shows a major conceptual insight, when trained to verbalize confidence, models could self-check low-confidence responses, which contributes to the understanding the relationship between verbalized uncertainty and model's reasoning ability.\n2. This papers demonstrates strong empirical results by achieving consistent improvements on both reasoning like MATH, GPQA and non-reasoning tasks like GSM8K, ARC, HellaSwag."}, "weaknesses": {"value": "1. The idea of introducing confidence into LLM training is not new. There are several relevant studies: [1-3]. Especially, the paper misses the reference to those relevant studies.\n2.  The evaluation lack deeper qualititative analysis. Espeically more real-world scenarios are not explored. Current reasoning models are quite strong on more complex tasks even for a 4B model like Qwen3-4B. More evaluation should be included on more complex tasks like AIME, CodeLiveBench.\n3. Since only the confidence token span is supervised, the model may learn superficial correlations rather than genuine uncertainty calibration. For example, the model may learn how to output stylistic confidence phrases.\n\n[1] R-Tuning: Instructing Large Language Models to Say `I Don't Know'\n\n[2] TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning\n\n[3] UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models"}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tEPMMJIohU", "forum": "P2o8wOzZMX", "replyto": "P2o8wOzZMX", "signatures": ["ICLR.cc/2026/Conference/Submission23342/Reviewer_6NDw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23342/Reviewer_6NDw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936725272, "cdate": 1761936725272, "tmdate": 1762942613927, "mdate": 1762942613927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a simple supervised fine-tuning method where a language model outputs both an answer and a verbalized confidence score. The model is trained using self-generated labels derived from its own sampling-based accuracy estimates, discretized into 10% confidence bins. The authors claim their method improves calibration (ECE, Brier Score) and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-written and easy to follow and understand.\n\n- Some of the experiments are novel and interesting, such as the cross-domain generalization that suggests that the learned  verbalized confidence transfers beyond the original reasoning tasks."}, "weaknesses": {"value": "The proposed method was already introduced by Lin, Hilton, and Evans (2022) in “Teaching Models to Express Their Uncertainty in Words”. That paper presented the exact same method of training a model to output natural-language confidence statements using the model's own empirical accuracy as labels. The only difference here is that the authors round the model’s empirical confidence to the nearest 10% instead of using continuous estimates.\n\nDespite this near-identity, the authors do not cite or discuss Lin et al., nor do they explain how their work differs conceptually or methodologically. Given the overlap in both framing and implementation, the novelty of CSFT is extremely limited."}, "questions": {"value": "Can the authors please explain what is their main contribution and how it is differs from Lin et al.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GPv3lZR2Ot", "forum": "P2o8wOzZMX", "replyto": "P2o8wOzZMX", "signatures": ["ICLR.cc/2026/Conference/Submission23342/Reviewer_YHJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23342/Reviewer_YHJH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23342/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938472667, "cdate": 1761938472667, "tmdate": 1762942613690, "mdate": 1762942613690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}