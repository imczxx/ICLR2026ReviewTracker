{"id": "6GWYvfsfgg", "number": 9566, "cdate": 1758127869605, "mdate": 1763186673207, "content": {"title": "LLaSO: A Reproducible Foundation for Large Speech-Language Models", "abstract": "The development of Large Speech-Language Models (LSLMs) has been limited by fragmented architectures and poor transparency, making reproducibility and fair comparison difficult. In contrast to the vision–language domain, where open resources have driven rapid progress, LSLMs are often released only as model weights without their training data or configurations, leaving the field without common baselines.\nWe present LLaSO, the first fully open, end-to-end framework for large-scale speech–language modeling. LLaSO consists of three key components: (1) LLaSO-Align, a 12M-instance speech–text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset for speech–text understanding; and (3) LLaSO-Eval, a standardized, reproducible benchmark for cross-modal evaluation.\nTo demonstrate its utility, we train LLaSO-Base, a 3.8B-parameter reference model built entirely on public data. LLaSO-Base achieves a normalized score of 0.72, outperforming comparable models and providing a strong, reproducible baseline. Our analysis further shows that while broader training coverage improves performance, significant generalization gaps remain, especially in speech-only scenarios.\nBy releasing datasets, benchmarks, and models together, LLaSO establishes an open standard for LSLMs, enabling unified research and faster community progress.", "tldr": "We present LLaSO, an open LSLM stack, 12M Align, 13.5M Instruct, 15K Eval, and a 3.8B model. Across models, broader task coverage helps, but generalization to unseen modalities, esp. pure audio, lags; interleaving/parallel decoding helps robustness.", "keywords": ["Large Speech-Language Models", "speech-text alignment", "instruction tuning", "multimodal evaluation", "paralinguistics", "modality robustness", "open-source benchmark", "reproducibility"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/04d80f00e38671c90c1a5c2913bcd54bd1577e32.pdf", "supplementary_material": "/attachment/4ab8e632afbde30c726ea167ca117182856ba4e7.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors open source a large-scale speech–language modeling system including model, training configuration, training and evaluation dataset. It is helpful for the community for the future research work on the spoken language model. My main concern is clarity. There are a lot of essential information missing, especially on the dataset construction. \nIt makes other researchers hard to adapt this baseline giving the missing information. Detailed questions are listed in the questions section."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- An open source large-scale speech–language modeling system is provided. \n- Many efforts have been done to make the ASR datasets good for modality alignment task, such as introducing hand-crafted instruction templates to convert the ASR datasets for speech language model alignment. \n- Many efforts have been done on the instruct dataset, which includes linguistic tasks, semantic tasks, and paralinguistic tasks. \n- An evaluation dataset is provided \n- A baseline model is provided with detailed analysis"}, "weaknesses": {"value": "- Some essential dataset construction information is missing\n- The dataset doesn't include multi-turn dialogue dataset"}, "questions": {"value": "- LLASO-ALIGN. \n  -  ``18 hand-crafted instruction templates that frame the task with varying specificity and constraints''. Why 18 instruction templates? What aspects are they addressed? What can be improved? Is there any stasts for training samples with different instruction templates?  \n  -  Figure 2 and Appendix Q describe dataset standardization. What exactly have been achieved compared with the original datasets? \n- LLASO-Instruct. \n  -  ``for each task, we manually construct 20 text instructions across four prompt styles''. Why every task has 20 text instructions, nothing more or less? What's the main motivation/goal for four prompt styles? What's the stats for the training data with those instructions? \n- LLASO-Eval: \n   - How do we compute the normalized score across different tasks? \n- How do you define open-ended/closed ended instruct settings in Table 11? \n- ``LLaSO-Eval covers 15,044 samples across 20 tasks''. Is this big enough? \n- The comparison in Table 13 is questionable. For example, the WER for other models are too high and many of them are above 50\\%. Is this expected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "A dedicated ethics section is provided."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMjNXqsAtO", "forum": "6GWYvfsfgg", "replyto": "6GWYvfsfgg", "signatures": ["ICLR.cc/2026/Conference/Submission9566/Reviewer_4yJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9566/Reviewer_4yJd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761506146212, "cdate": 1761506146212, "tmdate": 1762921120394, "mdate": 1762921120394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "zpRwTxlL2L", "forum": "6GWYvfsfgg", "replyto": "6GWYvfsfgg", "signatures": ["ICLR.cc/2026/Conference/Submission9566/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9566/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763186671659, "cdate": 1763186671659, "tmdate": 1763186671659, "mdate": 1763186671659, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLaSO, a fully open framework for large speech-language models that integrates data, evaluation, and a reference model for transparency and reproducibility. It provides an alignment corpus and a multi-task instruction set that covers linguistic, semantic, and paralinguistic tasks, along with a benchmark supporting multiple input and instruction modalities. A reference model trained on these public resources serves as a baseline, showing that broader task coverage improves performance, though significant gaps remain in pure-audio and cross-modality generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper aims to integrate training (Align + Instruct) and evaluation within a single, open framework that supports multiple modality configurations, helping reduce the entry barrier for large speech-language model research.\n- Having 40% of tasks to paralinguistics is a positive shift from the purely semantic objectives that have dominated many previous datasets.\n- The analysis of abstention rates and modality stability offers useful insights into model robustness across different input types."}, "weaknesses": {"value": "- The novelty is very limited. The model design, which projects audio into the LLM space followed by alignment and supervised fine-tuning, has become standard in recent speech language model research.\n- The authors claim that prior instruction datasets are \"primarily semantic with limited modalities,\" but several existing works already include style or paralinguistic coverage (e.g., SIFT-50M, Audio-FLAN).\n- Using GPT-4o for evaluation raises reproducibility concerns. Even with the temperature fixed at 0, GPT-4o outputs can still vary due to hidden randomness or version updates.\n- It is unclear how GPT-4o behaves as an evaluator, as there are no statistics showing its alignment with human judgment in the reported experiments.\n- Treating ASR as the sole \"speech-text alignment\" objective is not well motivated for paralinguistic learning, because ASR focuses on transcript accuracy and may ignore or discard speaker and prosodic information.\n- In the paper, the authors mention \"semantic tasks are already well-represented.\" However, to the best of my knowledge, evaluations on existing benchmarks show speech LMs still lag behind ASR+LLM pipelines on several SLU tasks.\n- The related work section provides very limited discussion. Since the paper aims to present a unified framework for training data, model architecture, and benchmarking, the authors should include more detailed comparisons of each component with prior work."}, "questions": {"value": "Please refer to my comments in the weakness section. I also have a few additional comments and questions below:\n\n- Why are duration (hours) missing for AVQA, COTA, OpenAQA, OpenASQA, and SIFT‑50M in Table 1?\n- Some task-dataset pairings do not seem reasonable. For example, using MELD for speaker verification is questionable since VoxCeleb is typically used for that task.\n- The proposed evaluation benchmark appears to overlap with Dynamic-SUPERB. Several tasks and datasets are already covered there, and both frameworks share a similar design that includes abstention rate analysis for certain tasks. I would appreciate if the authors can justify why a new benchmark is necessary instead of extending existing open ones. Although this work introduces more modality combinations (e.g., audio instruction with text input), its coverage for text instruction with audio input remains noticeably behind existing benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RFnB9cA2cg", "forum": "6GWYvfsfgg", "replyto": "6GWYvfsfgg", "signatures": ["ICLR.cc/2026/Conference/Submission9566/Reviewer_GTZ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9566/Reviewer_GTZ1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975800330, "cdate": 1761975800330, "tmdate": 1762921119976, "mdate": 1762921119976, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLaSO, an open, end-to-end stack for Large Speech-Language Models (LSLMs) consisting of: 1. LLaSO-Align (12M instruction-formatted ASR pairs), 2. LLaSO-Instruct (13.5M multi-task instruction-tuning samples over 20 tasks), and 3. LLaSO-Eval (15,044 stratified evaluation samples). \n\nThe authors also release LLaSO-Base, a 3.8B parameter reference system (Whisper-large-v3 encoder + 2-layer MLP projector + Llama-3.2-3B-Instruct) trained on public data only; it attains a min-max normalized overall score of 0.72 on LLaSO-Eval, exceeding the next best model’s 0.65. The work argues that broader task coverage reduces abstention and improves generalization, while speech-only setups remain the hardest."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The key strength of this paper is open and end-to-end release."}, "weaknesses": {"value": "English-only (for now). The authors acknowledge this as a limitation; given the positioning as a “foundation,” multilingual extensions should be higher priority or at least partially demonstrated (e.g., a pilot split).\n\nThe evaluation is dependent on GPT-4o. Having some human eval or cross check from different LLM would be better.\n\nSynthetic data make scientific study harder, e.g. how's different TTS model affect the quality, scaling etc.\n\nNo scaling behavior been studied.\n\nLack of architecture study and no generation."}, "questions": {"value": "As a foundational open model for LLM, scaling behavior and different architecture study are important. Given the paper is still hook up different model, I feel more experimental results with different size and open model are necessary to support the foundation claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CxnUyEWMdD", "forum": "6GWYvfsfgg", "replyto": "6GWYvfsfgg", "signatures": ["ICLR.cc/2026/Conference/Submission9566/Reviewer_UjN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9566/Reviewer_UjN1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9566/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181309243, "cdate": 1762181309243, "tmdate": 1762921119659, "mdate": 1762921119659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}