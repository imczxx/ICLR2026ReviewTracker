{"id": "URRq4xm6kb", "number": 10856, "cdate": 1758183485360, "mdate": 1759897624462, "content": {"title": "Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models", "abstract": "This paper presents a systematic study on enabling *medical* reasoning models (MRMs) to generate **ranked lists** of answers for open-ended questions. Clinical decision-making rarely relies on a single answer but instead considers multiple options, reducing the risks of narrow perspectives. Yet current MRMs are typically trained to produce only one answer, even in open-ended settings. We propose an alternative format: *ranked lists* and investigate two approaches: *prompting* and *fine-tuning*. While prompting is a cost-effective way to steer an MRM's response, not all MRMs generalize well across different answer formats: *choice*, *short text*, and *list answers*. Based on our prompting findings, we train and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT teaches a model to imitate annotated  responses, and RFT incentivizes exploration through the responses that maximize a reward. We propose new reward functions targeted at ranked-list answer formats, and conduct ablation studies for RFT. Our results show that while some SFT models generalize to certain answer formats, models trained with RFT are more robust across multiple formats. We also present a case study on a modified MedQA with multiple valid answers, finding that although MRMs might fail to select the benchmark's preferred ground truth, they can recognize valid answers. To the best of our knowledge, this is the first systematic investigation of approaches for enabling MRMs to generate answers as *ranked lists*. We hope this work provides a first step toward developing alternative answer formats that are beneficial beyond single answers in medical domains.", "tldr": "We study how medical reasoning models can produce ranked lists for open-ended questions. Comparing prompting, SFT, and RFT, we find RFT more robust, highlighting ranked lists as a promising alternative to single answers.", "keywords": ["medical", "reasoning", "list", "answer", "reinforcement learning", "supervised fine-tuning", "prompting", "chain-of-thought"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54fec9d26576a9525d6b0cbdbe47f69166039b6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the alignment problem between the multiple choice question format and real world medical complexity where a single answer may not be enough to accurately represent the best next steps and uncertainty due to incomplete information.\n\nThe authors propose to transform existing MCQ evaluations into a ranked list and open QA format to compensate the limitations of the MCQ format. Using these conversions they perform multiple performance comparison and proceed to use different techniques including prompt engineering, SFT and RFT to study the impact of different methods to adapt existing models to new formats."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Moving from MCQ to more representative methods is well motivated. In addition, the authors provide a comparison of proprietary models, general open models and medical finetuned models. The three methods used to train models provide an interesting comparison of these techniques to adapt existing models and show how RFT generalizes better to new formats.\n\nOverall the paper is well written and easy to follow with a clear graphical abstract that gives a good overview of the contribution and the structure of the paper."}, "weaknesses": {"value": "While the motivation to move beyond standard accuracy on single choice MCQ is detailed and sound, the authors do not discuss prior work that introduced different testing methodologies (e.g. AI Hospital, MetaMedQA, MAI-DxO, HealthBench) [1-4]. The authors do not sufficiently position their work compared to other approaches to justify the use of ranked lists and possibly explain how ranked lists could be integrated in non MCQ formats.\n\nThe reliance on existing benchmarks such as MedQA and MedMCQA imports the known limitations of board style vignettes which mostly test pattern recognition with shortcuts [4, 5]. In addition, the authors do not discuss the alignment between board style vignettes and real-world clinical work, for instance, the absence of longitudinal and noisy data severely limits their relevance to real world data. As the authors explain in the limitations, the source datasets are single answer which limits the use of ranked lists where multiple options can be correct and their relative priority informs us on the understanding of appropriate next steps or diagnosis. Finally, these QA samples are not representative of medicine as a whole and priorities may shift depending on the context, for instance, in a high resource setting for a patient with a suspected STEMI we direct the patient to a cath lab immediately but in lower resource settings we would consider thrombolysis as a first step. A ranked list would fail to accurately represent two correct answers depending on context, these limitations are not discussed.\n\nThe ablation studies use relatively small models due to compute availability which limits generalizability of the results. A single experiment with a large model would help in demonstrating the scalability of this method.\n\n[1] AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator (Fan et al., COLING 2025)\n\n[2] Large Language Models lack essential metacognition for reliable medical reasoning (Griot et al., Nature Communications 2025)\n\n[3] Sequential Diagnosis with Language Models (Nori et al. Preprint 2025)\n\n[4] HealthBench: Evaluating Large Language Models Towards Improved Human Health (Arora et al. Preprint 2025)\n\n[5] Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine (Griot et al., ACL 2025)\n\n[6] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks (Gallifant et al., Findings 2024)"}, "questions": {"value": "Suggestions:\n\n1) Improving the related work introduction to better position this paper in the boarder realm of medical evaluations for LLMs would strengthen the motivation for this particular approach.\n\n2) A small (n = 50-100) benchmark designed by experts with multiple ranked options as a test evaluation would strengthen the findings and support the proposed methods.\n\n3) Discussing existing limitations of the source datasets and how the authors compensate for these limitations to support their conclusions would improve the validity of the experiments.\n\n4) Discussing the alignment problem between automated testing and real-world applications is necessary to position the work as the goal of application research is to translate to real-world benefits. For instance, training models to use a ranked list format is not motivated in relation to the applicability of these lists in real-world settings. When in clinical workflows would this be useful?\n\n5) Scaling the ablation to a larger model would add credibility to the approach to specialize models to new formats."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0j7bDh8slb", "forum": "URRq4xm6kb", "replyto": "URRq4xm6kb", "signatures": ["ICLR.cc/2026/Conference/Submission10856/Reviewer_2LD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10856/Reviewer_2LD3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761147308571, "cdate": 1761147308571, "tmdate": 1762922073117, "mdate": 1762922073117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors highlight concerns about the tendency of medical reasoning models to produce a single final answer even in open-ended settings. They explore prompting and fine-tuning methods in order to produce ranked lists of outputs, including reinforcement fine-tuning and supervised fine-tuning across several models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "There are a few strengths of this paper. The issue of overly narrow benchmarks failing to consider the full breadth of clinical responses in evaluation is a valid one (and is in line with broader concerns in the field about the limitations of MCQs). Better and more nuanced assessment of the quality of differential diagnoses and clinical reasoning is important. \n\nThe authors apply appropriate technical rigor to the implementation of methods of supervised and reinforcement fine-tuning. They also engage with an interesting array of medical-specific models, even if frontier models are absent."}, "weaknesses": {"value": "Speaking frankly, it is not entirely clear to me that the authors substantiate the concern underlying this paper. Both in my personal experience with these models as well as in the literature I am familiar with (e.g. https://arxiv.org/abs/2412.10849 and others), it is relatively trivial to instruct any of the state-of-the-art language models to produce a clearly ranked differential diagnosis list (e.g., multiple outputs). While it is true that overconfidence in models can be a problem (e.g. https://ai.nejm.org/doi/pdf/10.1056/AIdbp2500120), it remains much more complex than simply a limitation in the length of outputs. \n\nPerhaps these problems are more substantial for specific medical reasoning models like some of those models that the authors use (I have rarely seen these evaluated or used), but this does not appear to be an issue for the main frontier models. Unfortunately, the authors only include extensive evaluation of these various undersized models, and extrapolate conclusions more generally to LLMs as a class based on them. This is a severe limitation of this work, and although it can be cost-prohibitive, this work really should be performed at the frontier. \n\nFurther, it is not clear that the ranked list generation methods that the authors employ actually map cleanly onto any of the aspects of list ranking that are valued by clinicians in reality. For example, a robust differential diagnosis should also include \"rare but dangerous\" considerations, or \"rare but treatable\" considerations in order that they are fully considered by the clinicians at hand, even if these would not technically maximize score based on rank list structure (see discussions of this in e.g. https://pmc.ncbi.nlm.nih.gov/articles/PMC3270234/ and https://www.nature.com/articles/s41586-025-08869-4/). Ultimately, the assessment of ranked lists is an important and nuanced question, that depends on the specifics of the clinical problem at hand. Basic probabilistic metrics, or knowledge-based metrics like their expanded medQA are insufficient. \n\nThe use of LLM-as-judge is inappropriate without at least some degree of human validation by physicians. Medical answers and diagnoses are very complex and nuanced, and it cannot simply be assumed that the semantic equivalence matching is high-quality. While it is reasonable to use a validated LLM-as-judge as an extension, you must use core evaluation methods for any specific implementation. The entire section evaluating different judge models is incoherent in this context. You cannot state that \"to change the judge model significantly impacts performance\", because the underlying performance of the systems are the same. What has changed is the performance onf the judge model, which the authors have not appropriately validated."}, "questions": {"value": "1. Are these concerns about the brevity of model answers substantiated in frontier models?\n2. How do these concerns about answer length manifest across the range of various clinical tasks, beyond multiple choice? Does this apply to differential diagnosis alone? What of clinical decisionmaking in other realms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TPOrkUmY5L", "forum": "URRq4xm6kb", "replyto": "URRq4xm6kb", "signatures": ["ICLR.cc/2026/Conference/Submission10856/Reviewer_XJqA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10856/Reviewer_XJqA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572853556, "cdate": 1761572853556, "tmdate": 1762922072788, "mdate": 1762922072788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that clinical decision-making inherently involves considering multiple possible answers, yet current medical reasoning models (MRMs) are trained to produce only one. To address this limitation, the authors propose enabling MRMs to generate ranked lists of plausible answers through prompting, supervised fine-tuning (SFT), and reinforcement fine-tuning (RFT) with novel ranking-based reward functions. Experiments across medical QA benchmarks show that RFT models generalize more robustly across answer formats (MCQ, QA, list) than SFT models, and that ranking-oriented rewards (e.g., MRR, LLM-judge) improve list quality. A case study on a modified MedQA dataset with multiple valid answers further demonstrates that MRMs often recognize correct alternatives overlooked by single-answer benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The evaluation is comprehensive, covering a wide range of models and datasets, which convincingly demonstrates the effectiveness and generality of the proposed methods. \n\n2. The case study on MedQA with multiple valid answers is particularly interesting and provides valuable insights into the limitations of single-answer benchmarks and the potential of ranked-list reasoning in clinical contexts."}, "weaknesses": {"value": "1. Overstated novelty claim (“first systematic study”). Several recent works have already prompted LLMs to generate ranked differential diagnoses and evaluated them using top-k or position-aware metrics across diverse medical datasets and model families [1, 2, 3]. The paper should more clearly delineate its unique contribution, maybe emphasizing the reinforcement fine-tuning (RFT) reward design or curriculum learning over answer formats.\n\n2. Limited analysis of SFT vs RFT generalization. The claim that “SFT-MCQ generalizes across formats while RFT generalizes both across formats and unseen examples” is intriguing but under-explained. The concept of answer format is not formally defined, and the paper lacks controlled ablations or causal evidence clarifying why RFT achieves superior transfer.\n\n[1] Zhou, Yuxuan, et al. \"Reliable and Diverse Evaluation of LLM Medical Knowledge Mastery.\" The Thirteenth International Conference on Learning Representations. \n\n[2] Lin, Tianwei, et al. \"HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation.\" Forty-second International Conference on Machine Learning. \n\n[3] Lim, Seungseop, et al. \"H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis.\" The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance."}, "questions": {"value": "1. On novelty and positioning: How does this work differ concretely from prior ranked-differential evaluations\n\n2. On generalization mechanics: Could you provide controlled ablations isolating why RFT transfers across formats—such as (a) keeping compute and data identical for SFT vs RFT, and (b) varying prompt templates to rule out prompt or distribution confounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "djKdaMxrzU", "forum": "URRq4xm6kb", "replyto": "URRq4xm6kb", "signatures": ["ICLR.cc/2026/Conference/Submission10856/Reviewer_MGaR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10856/Reviewer_MGaR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889791648, "cdate": 1761889791648, "tmdate": 1762922072416, "mdate": 1762922072416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper conducted a systematic study of steering Medical Reasoning Models (MRM) via prompting and fine-tuning for generating ranked-list outputs. \n\nThe authors introduced an evaluation framework covering three answer formats and shown that many models know the correct answers but often fail to select the benchmark-preferred one.\n\nThey then conducted a comprehensive study of SFT and RFT under specific answer formats, finding that SFT with MCQ generalizes well across formats, while RFT models generalize both to unseen examples and across answer formats.\n\nThe paper also provided practical insights into RFT design choices, including reward design and curriculum-style sequencing of RFT to improve answer robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is a comprehensive systematic investigation of methods to enable MRMs to generate answers as ranked lists, a useful alternative to single answers in medical QA.\n\nThe ranked-list format aligns with clinicians’ real-world decision processes (e.g., weighing multiple differentials or treatments), which can reduce risk from over-reliance on a single LLM output. \n\nThe paper also explored reward designs for RFT on ranked lists and conducted ablations to identify optimal design choices such as the training curriculum, which is are actionable for the community and likely to generalize to other medical reasoning settings."}, "weaknesses": {"value": "While the ranked-list format is a promising alternative to single-answer outputs, if a few incorrect answers are included and passed to clinicians, wouldn’t that still mislead or contribute to diagnostic failures? Does your training discourage listing incorrect options (e.g., via loss shaping or negative rewards)? A discussion of error propagation and interpretive risks from ranked outputs would clarify the safety implications.\n\nAs noted in the limitations, the ranked lists do not reflect probability magnitudes. Do you have strategies, via prompting or training, to elicit calibrated probabilities (e.g., model-reported confidence scores) or to learn them?\n\nHow does a simple baseline that ranks options by model likelihoods over the option tokens (e.g., a clinician pre-select top 5 likely diagnosis options) compare to explicit ranked-list training in terms of answer accuracy? Adding this baseline would quantify the added value of ranked-list training over simply outputting top choices, especially given your claim that models often already “know” the correct answer.\n\nDid you include qualitative examples in the appendix illustrating SFT vs. RFT behaviors and their output formats? Including such examples would help readers interpret results sections like “RFT-QA Exhibits List-like Behavior” and “Reward Function Effects With RFT-List.” (If present, please point to them; if not, consider adding.)\n\nMinor: please fix small typos in the introduction. Sections 3 and 4 both describe experimental setups; reducing subsection titles and headings could improve readability."}, "questions": {"value": "Just for clarification, in 'Reward Function Effects With RFT-List', when you say 'RFT performs robustly on non-list formats and \ngeneralizes better to unseen answer formats, what exactly do you mean: do you mean that RFT also obeys other formats you defined in training well other than ranked-list, or do you mean RFT sticks with ranked-list outputs even that there is instruction on other answer format?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h11B4QbBoB", "forum": "URRq4xm6kb", "replyto": "URRq4xm6kb", "signatures": ["ICLR.cc/2026/Conference/Submission10856/Reviewer_qJ9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10856/Reviewer_qJ9Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10856/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914526540, "cdate": 1761914526540, "tmdate": 1762922071848, "mdate": 1762922071848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}