{"id": "MPhnlqdU9Z", "number": 24804, "cdate": 1758360526317, "mdate": 1759896747442, "content": {"title": "MIRA: Quantifying Neural Network Monitorability via Feature Space Analysis", "abstract": "Monitoring neural networks is increasingly important for detecting potential failures in safety-critical applications. Although out-of-distribution (OoD) detection and uncertainty estimation have been widely studied, they often rely on the assumption that neural networks learn high-quality features. However, this assumption may not hold in practice, potentially leading to undetected failures.\nIn this work, we introduce the concept of monitorability, which captures the intrinsic ability of a model to highlight potential inference errors through internal activations. We provide a formal definition of monitorability and propose the Monitorability via Input peRturbAtion (MIRA) Score, a practical measure that quantifies this property without requiring access to external OoD data. \nOur method accounts for the behavior of the model near the decision boundary by applying norm-bounded input perturbations and evaluates how distinguishable the resulting internal representations are by using Mahalanobis distance.\nSince no established baseline exists for monitorability, we validate MIRA by comparing it against the best achievable OoD detection performance across three representative methods.\nThrough experiments across multiple architectures and domain applications, we show that the MIRA Score correlates with the strongest actual detection performance, providing a tool for evaluating and comparing monitorability across different models.\nTo the best of our knowledge, this is the first formalization and quantitative measure of monitorability. Our findings offer both theoretical grounding and empirical insight into the conditions under which model failures become detectable.", "tldr": "We propose the MIRA Score, a metric that quantifies a neural networkâ€™s ability to expose its own failures by perturbing inputs and measuring feature separability.", "keywords": ["Neural Networks", "Monitorability", "Out-of-Distribution Detection", "Anomaly Detection", "Runtime Monitoring", "Activation Patterns"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39aeaf204c0c02f739ec2c9b74a8a5e0bfbb78bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents MIRA, a monitorability measure for neural networks based on feature space analysis. The proposed method works via input perturbations, and the Mahalanobis distance is used to measure the discrepancy between perturbed inputs and the training set in the feature space. Several experiments validate the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The provided visualizations are clear and effectively demonstrate the alignment between the MIRA score and the \"separability\" of trained neural network features. This work also sheds light on the importance of monitorability in the deep neural network literature."}, "weaknesses": {"value": "The connection between Section 3.2 and Section 3.3 is weak. Section 3.2 gives Definition 1 regarding abstract monitorability, while Section 3.3 provides an empirical metric serving as the main contribution of this paper. However, there is no guarantee nor clear theoretical connection establishing how the method given in Section 3.3 relates to Definition 1.\n\nFurthermore, the data distribution $P_{in}$ in Definition 1 is somewhat ambiguous. Intuitively, what would be an example of $P_{in}$? If $P_{in}$ is the training distribution, then if the model achieves near-zero training loss, the $\\ell$-monitorability becomes meaningless. On the other hand, if $P_{in}$ is supposed to be a mixture between training and (out-of-distribution) test distributions, how is this distribution reflected by only the training set (potentially with perturbation)?"}, "questions": {"value": "Please see the weaknesses above. Minor questions:\n\n- The formulation in line 175 is very similar to some kind of adversarial robustness. Are there any links between Equation (2) and other adversarial robustness metrics?\n- What is $\\tilde{x}$ in line 187?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ghn5KHJwY5", "forum": "MPhnlqdU9Z", "replyto": "MPhnlqdU9Z", "signatures": ["ICLR.cc/2026/Conference/Submission24804/Reviewer_BxyR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24804/Reviewer_BxyR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977361484, "cdate": 1761977361484, "tmdate": 1762943202315, "mdate": 1762943202315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the concept of monitorability, whereby the inference maybe assigned a score that showcases whether said inference maybe trusted. The authors claim that existing uncertainty estimates are based on well-trained networks and features. Additionally, OOD estimation generally separates ID from OOD without addressing the correctness of ID sample inference. The authors propose MIRA, a method to monitor neural network predictions."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well written with a clear logical flow.\n\n2. The results are showcased on a variety of datasets."}, "weaknesses": {"value": "**Incorrect motivation and claims**:\n\nThe paper makes some fundamentally incorrect claims:\n\n1. (Line 12) *Although out-of-distribution (OOD) detection and uncertainty estimation have been widely studied, they often rely on the assumption that neural networks learn high-quality features.*: This is false. Early-stopping is a well known method in UQ that explicitly states that network training must stop long before overfitting.\n\n2. (Line 87) *However, it is important to note that misclassifications may also occur for ID inputs, which is a distinct scenario not directly addressed by OoD detection.*: True. But the application that does look at ID classification and OOD detection is Open set recognition [1].\n\n3. (Line 57) *To the best of our knowledge, this is the first work to formally define and quantify monitorability as a distinct property of neural networks.*: True in the sense of using the word monitorability. But there are a large number of simple UQ metrics (margin sampling, entropy), and more complex internal state-based gradient metrics [2, 3] that can monitor the outputs and provide an alternative score.\n\n[1] Recent Advances in Open Set Recognition: A Survey\n\n[2] Probing the Purview of Neural Networks via Gradient Analysis\n\n[3] Counterfactual Gradients-based Quantification of Prediction Trust in Neural Networks\n\n**Paper positioning**: Without referencing and comparing against prediction trust, UQ, open set recognition, it is hard to evaluate the contributions and results in the paper."}, "questions": {"value": "Please note that my rating is based on the weaknesses above. I would be willing to reevaluate my rating if I have missed something fundamental about the concept of monitoriability that is not covered by UQ, prediction trust, or open set recognition."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GMk5SWpGlS", "forum": "MPhnlqdU9Z", "replyto": "MPhnlqdU9Z", "signatures": ["ICLR.cc/2026/Conference/Submission24804/Reviewer_w33X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24804/Reviewer_w33X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011084261, "cdate": 1762011084261, "tmdate": 1762943202091, "mdate": 1762943202091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to relax the implicit assumption that most black-box OOD and UQ approaches make, which is that models have learned semantically meaningful features. In practice, if this assumption does not hold, it may lead to undetected errors. The authors formalize the concept of monitorability and an associated MIRA score, which is intended to highlight inference errors at the internal model layer level, rather than merely relying on a black-box assessment. The score quantifies this property by applying norm-bounded input perturbations and measuring the separability of resulting feature representations using Mahalanobis distance. MIRA is validated across computer vision, tabular, and NLP tasks by demonstrating correlation with OOD detection performance.\n\nThe paper addresses an important problem for safety-critical applications and makes a valuable theoretical contribution, however, the work has limitations: (1) MIRA's layer-dependency lacks principled aggregation to provide an overall monitorability estimate, the perturbation range selection requires domain-specific threshold choices, and the validation has some circularity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper has the following strengths:\n\nFormalizing monitorability as a distinct property of networks is valuable and contributes to trustworthiness and explanability that is sought from today's models (I think integrating formal methods and verification, as pointed out as extensions, is an interesting direction too).\n\nThe use of Mahalanobis distance with the surprisal score normalization (Eq 3) appropriately handles dimensionality differences, which would have been a concern if not addressed.\n\nExperiments span three data modalities: vision, tabular data, and NLP with diverse architectures, demonstrating consistent correlation with OOD detection performance.\n\nMIRA captures monitoring of internal model semantics which has a  potential to be applied independently of any specific detection method."}, "weaknesses": {"value": "There are two primary weaknesses of the approach:\n\nThe primary weakness of the proposed score is that it is layer l dependent and there is a lack of aggregation of this score at the layer level to provide an overall aggregate estimate of how the effect of cascading layers affects overall monitorability of the network as a whole. Specifically, the missing multi-layer analysis has huge architectural dependence and implications. The experimental results show comparisons for the monitoring methods across different classes, but not across different layers. The authors should: (1) provide empirical evidence and theoretical justification for selecting a specific layer (e.g., final layer) as representative or (2) develop a principled aggregation scheme that accounts for the cascading effects of these networks. Investigating how MIRA scores vary across different layers within the same network would be a valuable insight.\n\nThe claim of not requiring external OOD is a lucrative one, however, it is undermined by the requirement for a user-defined distribution for perturbation magnitudes p(epsilon) in definition 2 on page 4. While Appendix B.6 proposes a threshold-based heuristic (selecting epsilon to reduce accuracy below pre-determined thresholds), this approach introduces domain-dependent choices (50% for some datasets, 15% for CIFAR-100, and 75% for NLP) without principled guidance on how this should be selected in practice. Since perturbation magnitudes may vary significantly across test domains, the lack of a principled selection strategy for p(epsilon) represents a critical gap. The authors should provide: (1) a theoretical or empirical justification for p(epsilon) choices and/or (2) an adaptive method for automatically determining p(epsilon) across datasets."}, "questions": {"value": "On page 2, the alignment of the Mahalanobis distance with the softmax classifier remains specific to this type of classifier. Does this assumption hold for other types of classifiers? The broader question is about the generalization of the method to other model architectures.\n\nOn page 4, the perturbation delta(x,epsilon) needs to move x toward the decision boundary -> how is this perturbation selected in practice? Taking this further, in definition 2 on the same page, p(epsilon) is a user-defined probability distribution over perturbation magnitudes. It is not clear as to how the user should select this probability distribution. Are there any insights here that can be leveraged? \n\nIn practice, the magnitude of perturbations is not known and hence this is a critical assumption. So, while no external OOD data is required, the perturbation magnitude distribution, which is assumed to be user-specified, is unknown and a principled method to obtain this should be explored further and formalized. While section 4.2 on page 5 provides details on the use of FGSM to obtain perturbations, it is not clear as to why the authors \"consider this as a better choice.\"\n\nThe authors state on page 4 that MIRA is not intended for runtime detection, but rather as a pre-deployment evaluation metric. It is not clear, then, how this is used downstream at runtime. Can models be explicitly trained to improve their MIRA scores?\n\nThe evaluation creates a somewhat circular argument: the MIRA metric is validated by showing it correlates with detection performance, but monitorability is itself defined as detectability. I understand there is a lack of baselines in the space, however, is there a theoretical justification that can be provided here to back up the results? Addressing the multi-layer analysis would help.\n\nNo error bars or confidence intervals are provided for MIRA scores or OOD detection performance. Given the sensitivity shown in Table 8, understanding variance is important."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPl4yXx8Xp", "forum": "MPhnlqdU9Z", "replyto": "MPhnlqdU9Z", "signatures": ["ICLR.cc/2026/Conference/Submission24804/Reviewer_SrJp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24804/Reviewer_SrJp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762710812843, "cdate": 1762710812843, "tmdate": 1762943201873, "mdate": 1762943201873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}