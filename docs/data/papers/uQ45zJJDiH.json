{"id": "uQ45zJJDiH", "number": 23277, "cdate": 1758341574532, "mdate": 1759896823292, "content": {"title": "Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions", "abstract": "Large language models (LLMs) have seen increasing popularity in enterprise applications where AI agents and humans engage in objective-driven interactions. However, these systems are difficult to evaluate: data may be complex and unlabeled; human annotation is often impractical at scale; custom metrics can monitor for specific errors, but not previously-undetected ones; and LLM judges can produce unreliable results. We introduce the first set of unsupervised metrics for objective-driven interactions, leveraging statistical properties of unlabeled interaction data and using fine-tuned LLMs to adapt to distributional shifts. We develop metrics for labeling user goals, measuring goal completion, and quantifying LLM uncertainty without grounding evaluations in human-generated ideal responses. Our approach is validated on open-domain and task-specific interaction data.", "tldr": "Unsupervised metrics for evaluating the quality of objective-driven human-LLM conversations without human annotation or LLM judges.", "keywords": ["large language models", "unsupervised learning", "large language model evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0a5cee776cd2625a1abb2cb278b794df6c571aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an unsupervised framework for evaluating large language models (LLMs) in multi-turn, goal-oriented interactions without relying on human labels or LLM judges. The framework has three modules: a) Goal Identification uses LLM summarization and embedding-based clustering to discover latent user intents in unlabeled dialogues; b) Completion Detection fine-tunes a Llama 3 8B model to predict whether a conversation has achieved its goal using a special <|end|> token; c) Response Trees – introduces a novel visualization of model uncertainty based on the branching structure of possible next responses. Experiments across datasets such as LMSYS-Chat-1M, Code-Feedback, Insurance, WebShop, and SQL+OS+KB show that the small completion model can approach or even outperform large LLM judges on binary goal-completion classification (F1 up to 0.99). However, the Response Tree component is conceptual and not empirically validated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on a highly relevant and underexplored challenge in the current LLM research landscap: how to evaluate multi-turn, goal-oriented agents without relying on costly human annotations or unstable LLM-as-a-judge paradigms. As LLMs increasingly act as autonomous or semi-autonomous agents engaging in extended task-driven interactions, scalable and objective evaluation becomes a bottleneck for both research and deployment."}, "weaknesses": {"value": "1. The “unsupervised” claim is misleading: all three modules depend on LLM summarization, synthetic labeling, or fine-tuning.\n2. The negative sampling strategy (random truncation) does not accurately simulate real incomplete dialogues.\n3. The goal-identification module requires embedding and clustering the entire dataset at once, relying on iterative global merging and cross-cluster sampling. This design makes the method inherently batch-based, it cannot process streaming data without full re-clustering (e.g., in online systems).\n4. The Response Tree idea is not evaluated quantitatively; no correlation analysis links branching structure to uncertainty or correctness.\n5. The overall empirical contribution is confined to the completion detector, leaving the framework’s other parts remain speculative."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lqnMfmpuqI", "forum": "uQ45zJJDiH", "replyto": "uQ45zJJDiH", "signatures": ["ICLR.cc/2026/Conference/Submission23277/Reviewer_hA7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23277/Reviewer_hA7u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822189560, "cdate": 1761822189560, "tmdate": 1762942586395, "mdate": 1762942586395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces three unsupervised metrics for evaluating multi-turn objective-driven interactions between users and LLMs: (1) goal identification through LLM-guided clustering that combines k-means with LLM-generated labels, (2) goal completion detection using fine-tuned LLMs that predict an \"end tag\" for completed conversations, and (3) response uncertainty quantification via response trees that enumerate probable LLM outputs. The authors validate their approach on diverse datasets including LMSYS-Chat-1M, code-feedback, insurance underwriting, WebShop, and technical assistance tasks (SQL+OS+KB), demonstrating that 8B fine-tuned models can match or exceed 70B LLM judges on completion labeling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core ideas—unsupervised goal clustering via LLM+k-means hybridization, completion detection via distributional fine-tuning, and uncertainty quantification via response trees—are highly original.\n\n2.  The methods address a real and pressing need in enterprise AI development. The ability to perform evaluation without labels or human judges is a substantial practical advance."}, "weaknesses": {"value": "1. The method assumes \"a majority of interactions are complete\" to train the completion detector, which fundamentally undermines its unsupervised nature and creates a validation problem.\n\n2. The pipeline likely depends on embedding model, initial k, prompt phrasing for merges/labels, and decoding hyperparameters. Robustness to these choices is not fully demonstrated."}, "questions": {"value": "1. How do you address the fundamental circularity in assuming most interactions are complete to detect completion? Could you validate on datasets with known failure rates?\n\n2. Why does completion detection fail so dramatically on Code-Feedback (F1=0.21)? This seems to invalidate the approach for follow-up conversations.\n\n3. What is the computational cost vs. using LLM judges? Is fine-tuning + inference cheaper?\n\n4. In Algorithm 1, an LLM is prompted to decide if two clusters should be merged. Given the paper's valid critiques of LLM judge instability, what steps were taken to ensure this \"merge\" decision is stable and reliable? Did you observe any of the instability (e.g., order sensitivity) that you found in the LLM-only baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MzeAUphz9D", "forum": "uQ45zJJDiH", "replyto": "uQ45zJJDiH", "signatures": ["ICLR.cc/2026/Conference/Submission23277/Reviewer_wVNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23277/Reviewer_wVNW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966971720, "cdate": 1761966971720, "tmdate": 1762942586115, "mdate": 1762942586115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the first unsupervised metrics to evaluate objective-driven AI–human interactions, addressing the limits of human annotation, bespoke checks, and unreliable LLM judges on complex unlabeled data.\nLeveraging statistical signals and fine-tuned LLMs to handle distribution shifts, the metrics label user goals, assess goal completion, and estimate LLM uncertainty, with validation on open-domain and task-specific datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses key shortcomings in current LLM evaluation methods.\n- It presents extensive experiments demonstrating the effectiveness of the proposed approaches."}, "weaknesses": {"value": "- The overall paper structure could be improved. A concise introduction is fine, but it should still highlight the main points to orient the reader.\n- The selected attributes require a clear rationale for why they are important for evaluating LLM responses.\n- The metrics used to measure them also need clearer explanation and justification: how well do these metrics reflect the degree or quality of the attributes?\n- The title is potentially misleading. What exactly is meant by “unsupervised”? Is a fine-tuned model used for completion evaluation?\n- How do you clearly justify the claim that a larger number of leaves implies greater uncertainty in LLM responses?"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TXReoq9976", "forum": "uQ45zJJDiH", "replyto": "uQ45zJJDiH", "signatures": ["ICLR.cc/2026/Conference/Submission23277/Reviewer_Xmjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23277/Reviewer_Xmjj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986781912, "cdate": 1761986781912, "tmdate": 1762942585853, "mdate": 1762942585853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical and timely problem in the deployment of LLMs in enterprise settings: the evaluation of multi-turn, objective-driven interactions (e.g., customer service, task-oriented agents) without relying on human annotations or unreliable LLM-as-a-judge paradigms. The authors propose a novel suite of three unsupervised metrics:\nLLM-Guided Clustering: An algorithm that combines k-means on text embeddings with LLM-based labeling and merging to automatically discover and label user goals from unlabeled conversation data.\nInteraction Completeness: A method that fine-tunes a small LLM on a dataset of (mostly) completed conversations, tagged with a special \"end\" token, to predict whether a given interaction is complete. Incomplete interactions are flagged as outliers.\nResponse Uncertainty: A framework using \"response trees\" to quantify the LLM's uncertainty during a conversation by exploring the branching factor and log-probabilities of potential responses, serving as a proxy for interaction quality and potential error likelihood.\nThe methodology is empirically validated on a range of datasets, including open-domain chats and specialized, tool-using interactions. The key result is that their approach, often using a fine-tuned 8B parameter model, can match or exceed the performance of a much larger (70B) LLM judge on the task of completion labeling, demonstrating the potential for efficient, specialized evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novelty and Practical Impact: The core idea of moving beyond LLM judges and human references to unsupervised, statistically-grounded metrics is highly novel and addresses a significant pain point in real-world AI system development. The potential for online monitoring and resource-saving interventions is a compelling practical contribution.\nHolistic Evaluation Framework: The paper doesn't propose a single metric but a suite of three complementary metrics that address different aspects of an interaction (goal, completion, uncertainty), providing a more comprehensive evaluation toolkit.\nStrong Empirical Validation: The paper is thorough in its experimentation, using multiple diverse datasets (LMSYS, Code-Feedback, Insurance, WebShop, etc.) to demonstrate the generality of the approach. The ablation studies (e.g., \"No end tag,\" comparison of fine-tuned vs. base models) effectively justify key design choices.\nEfficiency Focus: The successful use of small (8B parameter) models, fine-tuned for specific distributions, is a major strength. It challenges the prevailing trend of using ever-larger models as judges and opens avenues for cost-effective, scalable evaluation.\nClarity of Core Methodology: The underlying concepts for the completeness and uncertainty metrics—modeling the token distribution of completed interactions and constructing response trees—are well-explained and theoretically sound."}, "weaknesses": {"value": "Reliance on Key Assumptions: The methodology rests on two strong assumptions that may not always hold in practice: a single user goal per interaction and that failures are \"rare.\" The performance degradation on Code-Feedback, where follow-up questions violate the \"single well-defined end\" assumption, highlights this fragility. The applicability in noisier, real-world environments with frequent failures is not fully established.\nLimited Statistical Rigor: While the results are promising, the statistical significance of the improvements is not tested. Table 2 reports accuracy and F1 scores, but without confidence intervals or statistical tests, it's difficult to gauge the robustness of the claims, especially on smaller datasets like Insurance (n=380).\nWriting and Presentation Issues:\nInconsistent Metric Naming: The three core metrics are introduced as labeling user goals, measuring goal completion, and quantifying LLM uncertainty. However, in the methodology and experiments, they are referred to as \"clustering,\" \"completion labeling,\" and \"response trees.\" A consistent, clear naming scheme would improve readability.\nUndefined \"Stability\": The clustering algorithm is praised for its \"stability,\" but this term is not quantitatively defined. Figure 4 is visual and compelling, but a quantitative measure of cluster stability (e.g., Adjusted Rand Index between runs) would strengthen the claim.\nSupervision in \"Unsupervised\" Learning: The approach requires a dataset where \"the majority of interactions are complete\" to fine-tune the completeness model. While not requiring manual labels, this does require a curated set of complete conversations, which is a form of weak supervision. The term \"unsupervised\" should be nuanced to acknowledge this data curation requirement."}, "questions": {"value": "Assumptions and Generalizability: How would your completeness metric perform in a production environment where user interactions frequently contain multiple, interleaved goals or where the failure rate is significantly higher than in your training data? Are there strategies to make the approach more robust to such violations of its core assumptions?\nStatistical Significance: Given the varying sizes of your datasets, have you performed any statistical significance testing (e.g., bootstrapping) on the results in Table 2? This would help clarify whether the observed performance differences, particularly on smaller datasets like Insurance, are reliable.\nClustering Stability Metric: You state your clustering algorithm produces \"highly stable clusters.\" Beyond the confusion matrices, did you calculate a quantitative stability metric like the Adjusted Rand Index between different runs? If so, what were the results?\nResponse Tree Practicality: The response tree is a powerful conceptual tool for quantifying uncertainty. However, the computational cost of building these trees for long, complex conversations must be non-trivial. Could you comment on the computational overhead of this method and its feasibility for near-real-time monitoring?\nDefining \"Completion\": The \"end\" tag is a clever heuristic, but it conflates syntactic and semantic completion. A conversation could end with the user giving up. Does your method have a way to distinguish between a successfully resolved goal and an abandoned one, or is it solely based on the model's learned distribution of \"end-of-dialogue\" tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kr1M7JRF1d", "forum": "uQ45zJJDiH", "replyto": "uQ45zJJDiH", "signatures": ["ICLR.cc/2026/Conference/Submission23277/Reviewer_UJoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23277/Reviewer_UJoU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762526861518, "cdate": 1762526861518, "tmdate": 1762942585609, "mdate": 1762942585609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}