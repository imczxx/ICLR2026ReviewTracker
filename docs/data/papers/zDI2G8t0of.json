{"id": "zDI2G8t0of", "number": 13084, "cdate": 1758213418241, "mdate": 1759897466470, "content": {"title": "A Statistical Benchmark for Diffusion Posterior Sampling Algorithms", "abstract": "We propose a statistical benchmark for diffusion posterior sampling (DPS) algorithms in linear inverse problems. Our test signals are discretized Lévy processes whose posteriors admit efficient Gibbs methods. These Gibbs methods provide gold-standard posterior samples for direct, distribution-level comparisons with (DPS) algorithms. They also serve as oracle denoisers in the reverse diffusion, which enables the isolation of the error that arises from the approximations to the likelihood score. We instantiate the benchmark with the minimum-mean-squared-error optimality gap and posterior coverage tests and evaluate popular algorithms on the inverse problems of denoising, deconvolution, imputation, and reconstruction from partial Fourier measurements. We release the benchmark code at https://github.com/emblem-saying/dps-benchmark. The repository exposes simple plug-in interfaces, reference scripts, and config-driven runs so that new algorithms can be added and evaluated with minimal effort. We invite the community to contribute and report results.", "tldr": "We made an evaluation pipeline for diffusion posterior sampling algorithms for Bayesian linear inverse problems that relies on the construction of posteriors with known posteriors that we can efficiently sample from.", "keywords": ["Diffusion models", "Bayesian inverse problems", "statistical evaluation", "Gibbs sampling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f9578eea5c5bad6ab0a3d05d9caf1ed7d4a3955.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new benchmark for evaluating different posterior sampling algorithms using diffusion models (dubbed DPS in this paper; Different from DPS [1]), where the posterior samples can be computed analytically, so that the ground truth is given. \nPrevious *benchmarks* that admit analytical posterior samples were constrained to settings where the prior is a mixture of Gaussians, which largely differs from the natural data statistics. The prior distributions considered in this paper is much larger, and the authors propose methods to efficiently compute ground truth posterior distributions. Several widely established baselines are compared.\n\n**References**\n\n[1] Chung et al., \"Diffusion posterior sampling for general noisy inverse problems\", ICLR 2023"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, this is the first approach to go beyond mixture of gaussian priors when attempting to build a ground truth posterior distribution.\n\n2. The paper is well-written and easy to follow, with sufficient background given in the appendix.\n\n3. The method of acquiring the posterior distribution by extending Kuric et al. [1] is sound.\n\n\n**References**\n\n[1] Kuric et al., \"The Gaussian latent machine: Efficient prior and posterior sampling for inverse problems\", arxiv 2025"}, "weaknesses": {"value": "1. Being able to use different prior/posterior distributions as ground truth is, in and of itself, important. Nevertheless, the argument would be strengthened if the paper shows that the proposed distributions in this paper are closer to real-world statistics in some cases. Currently, only some references are given.\n\n2. The authors mention that the proposed framework can be extended to higher-dimensional settings, but there are complications. It would add much value if the authors were to include experiments with $d$ that match the typical image resolutions. Currently, it seems like the experiments are conducted with low dimensionality ($d$). What's the value of $d$ chosen here?"}, "questions": {"value": "Is there any reason to constrain the benchmark for *diffusion* posterior sampling algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qBVdgq9kk", "forum": "zDI2G8t0of", "replyto": "zDI2G8t0of", "signatures": ["ICLR.cc/2026/Conference/Submission13084/Reviewer_pM9c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13084/Reviewer_pM9c"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760494236820, "cdate": 1760494236820, "tmdate": 1762923812930, "mdate": 1762923812930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a statistical benchmark for evaluating diffusion posterior sampling algorithms using discretized Lévy processes with tractable Gibbs posteriors as ground truth. While the framework enables rigorous distribution-level comparisons, the evaluation is severely limited to low-dimensional (d=64) linear inverse problems, raising serious concerns about scalability and practical relevance to realistic imaging applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Developing a benchmark for posterior sampling in high-dimensional problems is important."}, "weaknesses": {"value": "• All experiments use d=64 signals with only linear operators. No evidence is provided that the framework scales to realistic dimensions (e.g., 256×256 images) or nonlinear problems, fundamentally limiting the practical applicability and making it unclear whether findings transfer to problems researchers actually solve.\n\n• The authors cite power-law phenomena in finance and images to motivate heavy-tailed priors, but never demonstrate that their 1D discretized Lévy processes meaningfully capture structure in realistic signals. The connection to actual image statistics remains unsubstantiated.\n\n• Table 4 shows learned denoisers often match or exceed oracle performance, undermining claims about isolating likelihood approximation errors. The paper doesn't establish whether likelihood errors dominate versus other sources (discretization, hyperparameter sensitivity), weakening the diagnostic utility argument.\n\n• DPS algorithms are tuned with learned denoisers but evaluated with oracle denoisers using the same hyperparameters (lines 276-278). This mismatch means oracle results may be suboptimal, contradicting claims about properly isolating algorithmic errors.\n\n• Claims of \"efficient implementations\" and \"acceptable runtimes\" (lines 231-234, 822-823) lack any quantitative evidence; no runtime comparisons, memory usage, or scalability analysis is provided to substantiate efficiency claims or assess practical feasibility at higher dimensions."}, "questions": {"value": "Does this benchmark can be used for amortized diffusion sampling methods, i.e., learning the full posterior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XUACQbtL5B", "forum": "zDI2G8t0of", "replyto": "zDI2G8t0of", "signatures": ["ICLR.cc/2026/Conference/Submission13084/Reviewer_TmEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13084/Reviewer_TmEt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923234242, "cdate": 1761923234242, "tmdate": 1762923812487, "mdate": 1762923812487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Diffusion posterior sampling algorithms have become prominent methods for sampling from posterior diffusion with a denoising diffusion model prior. While many methods have been proposed in the recent years, most of the interesting benchmarks do not come with ground-truth posterior samples to which one can compare against. The aim of this paper is to close this gap by proposing a statistical benchmark that mimicks the behaviour of realistic data (power-law-like extremes as stated in the paper). To this end the authors consider the posterior associated to Lévy processes and use an efficient Gibbs sampler to obtain gold-standard posterior samples that serve as reference."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This paper tackles a fundamental problem in the evaluation of diffusion posterior samplers and proposes a very useful benchmark which in my opinion could be useful to the community and should be present in all the forthcoming papers. \n- The model is general enough to contain different instantiations such as Laplace and spike and slab and thus goes beyond the existing gaussian mixture toy examples. \n- The paper is rather well-written and quite pedagogical, I enjoyed reading it."}, "weaknesses": {"value": "The only weakness I see is the structuring of the main paper. For example I think that some parts of the related works (such as the first two paragraphs) could be moved to the appendix as they are slightly relevant to the content of the paper. This space could be used to provide for example more background on the GLM framework, as one needs to go to the appendix to read more interesting details about it. \nI also think that Figure 1 and 2 are misplaced as at this stage of the paper the Lévy process is not introduced and we don't know yet what St(1) means."}, "questions": {"value": "I have a few suggestions and related works to be considered: \n- I think it would have been interesting to include samples from a conditional diffusion model, by either training the conditional denoiser or estimating the denoiser using Monte Carlo samples as is done for DPS methods. I believe that it could be relevant since it provides a lower bound on the performance that one hopes to achieve with DPS methods. \n- [1] considers an actual real world setting where gold standard samples can be obtained using MCMC. \n- The toy Gaussian mixture benchmark is introduced in [2, 3] \n\n[1] Cardoso, G.V. and Pereira, M., 2025. Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data.  \n[2] Cardoso, G., Idrissi, Y.J.E., Corff, S.L. and Moulines, E., 2023. Monte Carlo guided diffusion for Bayesian linear inverse problems.  \n[3] Boys, B., Girolami, M., Pidstrigach, J., Reich, S., Mosca, A. and Akyildiz, O.D., 2023. Tweedie moment projected diffusions for inverse problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ecx5TSO86m", "forum": "zDI2G8t0of", "replyto": "zDI2G8t0of", "signatures": ["ICLR.cc/2026/Conference/Submission13084/Reviewer_bQ8j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13084/Reviewer_bQ8j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957963122, "cdate": 1761957963122, "tmdate": 1762923812111, "mdate": 1762923812111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors introduce a benchmark suite for evaluating algorithms designed to solve linear inverse problems with diffusion model priors\n- The benchmark is built on a synthetic setup derived from discretized Lévy processes\n- It hence include setting of heavy-tailed/power-law–like distributions beyond the Gaussian case\n- The key motivation lies in the fact that Lévy processes possess explicit marginal distributions and can be targeted using Gibbs sampling\n- This property allows the benchmark to generate ground-truth posterior samples (from inverse problem and denoising posterior) for quantitative comparison across algorithms"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and accompanied with concise explanations in the appendix\n- The motivation of the paper is well articulated namely for principled benchmarking in diffusion-based inverse problem solvers\n- The proposed benchmark is a valuable contribution, as it extends evaluation on Gaussian setup to a broader family of distributions"}, "weaknesses": {"value": "**Overstated or misleading claims**\nThe repeated use of the term \"oracle\", e.g., Lines 56, 129, 277, 355 is misleading.\nThe samples used in the benchmark are produced via Gibbs sampling—an iterative procedure—hence they are approximate, not exact. The quality of these samples depends on choices such as burn-in time, which are hyperparameters of the framework.\nThis issue becomes more apparent when the benchmark is applied to algorithms requiring gradients of the denoiser (Line 257-263 and equation (60)): the paper substitutes the latter with a covariance estimator of $X_0 | X_t,$ and hence further deviating from the notion of an \"oracle\".\n\n\n**Template for posterior samplers**\nThe proposed benchmark template seems overly restrictive. By focusing on algorithms that use only the denoiser, it neglects methods that require the Jacobian of the denoiser.\nAlthough the paper connects this to the covariance $Cov(X_0 \\mid X_t)$, estimating this covariance is far more computationally demanding and less stable, and therefore it downgrade the claim that the benchmark offers \"oracle\" quantities with minimal approximation error.\n\n**Evaluation design**\n- The inclusion of learned denoisers in the evaluation is conceptually inconsistent with the paper’s stated goal of removing approximation errors (Section 1.1).\nIf the benchmark aims to isolate algorithmic performance, learned denoisers reintroduce training-dependent variability. While the authors justify this by citing robustness testing, the notion of robustness is loose and in practice requires hyperparameter tuning, which introduces additional confounding factors.\n- The experimental comparison is limited. Only 3 algorithms are evaluated, and these do not represent the diversity of available approaches, e.g., optimization-based, variational, or midpoint-guided methods; see the literature in [1] and [2]\n\n**Remarks and minor issues**\n\n- In background, rephrase the statement in Line 132 about DDPM, sampling in fact depend on several parameters and it is bold to say \" researchers typically use\"; I would argue that frequently DDIM sampling is used with $\\eta = 0$ (simulating the probability-flow ODE) for sharp samples with few diffusion steps\n- The used abbreviation **DPS** is already/actually the name of a well-known algorithm in diffusion models and inverse problems [4], hence the abbreviation might be misleading using it here to refer to something else may cause confusion.\n- The authors may also consider adding the following reference on inverse problems benchmarks [3]\n- Line 288: The statement that DiffPIR is an extension of C-DPS is incorrect. DiffPIR follows a distinct formulation based on quadratic half-splitting with an auxiliary variable and does not rely on the VJP of the denoiser.\n\n\n---\n\n.. [1] Daras, Giannis, et al. \"A survey on diffusion models for inverse problems.\" arXiv preprint arXiv:2410.00083 (2024).\n\n.. [2] Oliviero-Durmus, Alain, et al. \"Generative modelling meets Bayesian inference: a new paradigm for inverse problems.\" Philosophical Transactions A 383.2299 (2025): 20240334.\n\n.. [3] Zheng, Hongkai, et al. \"Inversebench: Benchmarking plug-and-play diffusion priors for inverse problems in physical sciences.\" arXiv preprint arXiv:2503.11043 (2025).\n\n.. [4] Chung, Hyungjin, et al. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022)."}, "questions": {"value": "- I generally found the figures hard to understand and interpret, I'm referring namely to figure 1, it says that it shows reverse using the oracle denoiser, but it is not clear, similarly, for figure 3, it is hard to interpret namely to say wether the algorithms performs well or not \n- can the authors provides hints/explanation on the derivation of equation (13)\n- the authors claim that introduced framework can also assess the approach where the conditional components is learned (Line: 168-170), but it is not clear how it can be achieved given that in some tasks the likelihood is not known, e.g. tasks such deraining or dehazing, see for instance [1]; yet the benchmark is built on the ability to explicitly write the posteriors/marginals and target them using Gibbs sampling\n\n- A more broad question: did the authors think about how the benchmark can be extend to nonlinear inverse problems ?\n\n---\n\n.. [1] Wang, Hanting, et al. \"IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models.\" arXiv preprint arXiv:2505.24406 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qh8Nh3DeU4", "forum": "zDI2G8t0of", "replyto": "zDI2G8t0of", "signatures": ["ICLR.cc/2026/Conference/Submission13084/Reviewer_tkeZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13084/Reviewer_tkeZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080426313, "cdate": 1762080426313, "tmdate": 1762923811722, "mdate": 1762923811722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}