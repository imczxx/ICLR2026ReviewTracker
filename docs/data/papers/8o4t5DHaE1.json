{"id": "8o4t5DHaE1", "number": 11863, "cdate": 1758204330506, "mdate": 1759897550118, "content": {"title": "Lost in the Non-convex Loss Landscape: How to Fine-tune the Large Time Series Model?", "abstract": "Recently, large time series models (LTSM) have become popular and important because they exhibit characteristics similar to large language models, such as flexible context length, scalability, and task generality, outperforming the advanced task-specific models in the domain. \nHowever, existing research indicates that the pre-trained LTSM can show a poor non-convex loss landscape (indicating poor trainability). Hence, directly fine-tuning pre-trained LTSM shows overfitting, which leads to poor fine-tuning performance, even worse than training from scratch on the downstream datasets. \nThis severely diminishes the value of the pre-trained LTSM.\nTo address this, we propose a new fine-tuning method called \\textit{Smoothed Full Fine-tuning} (SFF).\nSpecifically, before fine-tuning, we first construct an auxiliary LTSM with a smooth\nloss landscape (indicating good trainability) through random initialization. \nSecond, we utilize it to smooth the loss landscape of the pre-trained LTSM through linear interpolation between their weights. \nAs a result, the smoothed LTSM acquires good trainability while retaining good pre-training knowledge, thereby achieving better performance when fine-tuned on the downstream dataset. \nWe also explain why SFF is effective from the perspective of optimization theory: interpolation perturbs sharp minima without obviously harming originally flat regions, thereby aiding sharp minima escape to better and smoother basins.\nExtensive experiments on popular datasets show that our method indeed improves the performance of eight popular LTSMs, e.g., Timer, TimesFM, MOMENT, UniTS, MOIRAI, Chronos, TTMs, and Sundial, in different downstream tasks.", "tldr": "We find that large models may suffer from performance limitations during fine-tuning due to overfitting in pre-training. We propose to smooth the loss landscape then fine-tuning to improve the fine-tuning performance.", "keywords": ["Time series analysis", "large models", "fine-tuning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32a3b63a12dcb5586b4ad615ef66316c9ac3bb76.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes the Smoothed Full Fine-tunning (SFF) to finetune LTSMs. SFF linearly interpolate the parameters of a pretrained LTSM with a randomly initialized auxiliary model, with the goal of smoothing sharp, non-convex regions of the loss landscape. The main contribution lies in the interpolation that perturbs the parameter of the model to reach flatter regions. Such perturbation towards flat minima is a well-known strategy for enhance generalization. Experimental results show the method can obtain better results than compared baselines and that SFF can perform fine-tuning with minimal memory and computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors identify a significant and practical problem, the poor fine-tuning performance of modern LTSMs due to the non-convexity of their loss landscape.  \n- SFF is simple to implement, requiring only a one-time linear interpolation of weights before fine-tuning. It adds no computational or memory overhead during the actual fine-tuning process."}, "weaknesses": {"value": "- Exploration of interpolation strategies. The paper states that $\\alpha$ controls the proportion of pre-trained knowledge retained, and the experiments show a sensitivity to $\\alpha$. However, the optimal $\\alpha$ appears to be model-dependent. A more robust, perhaps adaptive, method for selecting or tuning $\\alpha$ would significantly strengthen the work.\n\n- The auxiliary model is defined as a randomly initialized LTSM. The paper claims this model has a smooth loss landscape, however is not clear why a randomly initialized model is guaranteed to be smoother than a pre-trained one, especially considering the vast differences in initialization schemes and model architectures. A more rigorous analysis or reference to why random initialization leads to flat minima would be beneficial.\n\n- The theoretical analysis in Section 3.1 is relatively brief and qualitative, relying heavily on existing literature. I suggest the authors to present a more in depth analysis of the loss landscape for finetuning, such analysis would provide strong theoretical justification."}, "questions": {"value": "Refer to the weakness part. \n\n- Can the authors provide a more detailed explanation or empirical evidence as to why a randomly initialized LTSM is guaranteed to have a smoother loss landscape than a pre-trained one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9J9tNRKub0", "forum": "8o4t5DHaE1", "replyto": "8o4t5DHaE1", "signatures": ["ICLR.cc/2026/Conference/Submission11863/Reviewer_r8RN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11863/Reviewer_r8RN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233431034, "cdate": 1761233431034, "tmdate": 1762922883639, "mdate": 1762922883639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Smoothed Full Fine-tuning (SFF), a lightweight method to improve fine-tuning of large time-series models by interpolating pretrained weights with a randomly initialized model before training. This simple step smooths sharp loss landscapes, enhancing optimization without extra computational cost. Tested on eight major LTSMs across forecasting and anomaly detection tasks, SFF consistently improves fine-tuning and often boosts zero-shot accuracy. The approach is practical and broadly effective, though the paper’s novelty is mainly within time-series models and lacks comparisons to established flatness-based fine-tuning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is simple and inexpensive. Smoothing is done with a one-shot linear interpolation using a randomly initialized copy before fine-tuning. It only needs a few lines of PyTorch and does not add compute or memory cost.\n- The motivation is clear. The paper argues that pretraining can leave large time-series models stuck in sharp, non-convex regions, and interpolation helps escape these while keeping flat regions stable.\n- The evaluation covers many settings. The authors test eight large time-series models across forecasting, anomaly detection, and imputation tasks."}, "weaknesses": {"value": "- The novelty claim may be overstated. The idea of mixing weights for fine-tuning already exists in other domains, so the contribution should be limited to time-series foundation models.\n- The baselines are limited. The comparisons use full fine-tuning and linear probing but omit other regularization or smoothing approaches such as SAM, SWA, Mixout, or L2-SP.\n- Some zero-shot results decline after smoothing. Models like MOIRAI and Chronos perform worse on certain datasets.\n- Interpolation with a random model might cause misalignment. It can disturb normalization or scale between layers, and the paper does not analyze these risks in detail."}, "questions": {"value": "Please address the identified weaknesses and limitations noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ibjSDQSsnM", "forum": "8o4t5DHaE1", "replyto": "8o4t5DHaE1", "signatures": ["ICLR.cc/2026/Conference/Submission11863/Reviewer_wYp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11863/Reviewer_wYp2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631753973, "cdate": 1761631753973, "tmdate": 1762922882983, "mdate": 1762922882983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical problem in the practical application of large time series models: pre-trained models often exhibit poor trainability when fine-tuned on downstream tasks. The authors attribute this to the models converging to sharp minima during pre-training, resulting in a non-convex loss landscape that leads to overfitting during fine-tuning—sometimes to the point of underperforming models trained from scratch. To address this, the paper proposes a simple and effective method called Smoothed Full Fine-tuning. SFF works by first creating an auxiliary, randomly initialized model, which possesses a smooth loss landscape  but no pre-trained knowledge. Before fine-tuning, SFF performs a single linear interpolation between the weights of the pre-trained model and this auxiliary model. The resulting \"smoothed\" model is shown to retain the valuable knowledge of the pre-trained model while inheriting the superior trainability of the random model , allowing it to escape sharp minima and find better, flatter basins. The authors provide extensive empirical validation, showing that SFF consistently improves the performance of eight different LTSMs  on forecasting and anomaly detection tasks compared to standard fine-tuning methods, without incurring any additional memory or computational overhead during the fine-tuning step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a significant and practical limitation of pre-trained LTSMs. The observation that direct fine-tuning can lead to overfitting and even perform worse than training from scratch is a crucial finding that motivates the need for better fine-tuning strategies. The problem is clearly illustrated using loss landscape visualizations.\n\nThe proposed SFF method is exceptionally simple, consisting of a single linear interpolation of model weights before fine-tuning begins. This makes it easy to implement and, importantly, it adds no additional memory or computational overhead to the actual fine-tuning process, making it a highly practical solution.\n\nIn a particularly strong finding, the smoothing process by itself is shown to improve zero-shot forecasting performance, suggesting it guides the model to a better, more generalizable basin in the loss landscape even without fine-tuning."}, "weaknesses": {"value": "The theoretical motivation in Section 3.1 is more of a high-level, intuitive argument rather than a formal analysis. While the explanation is plausible and well-supported by citations to related work, the paper does not provide a rigorous theoretical proof for why this specific form of interpolation is an optimal or principled way to achieve this smoothing\n \nThe interpolation coefficient $\\alpha$ is a new, critical hyperparameter introduced by SFF. The paper shows SFF is robust, outperforming FF across a wide range of $\\alpha$ values However, the paper also shows that the optimal $\\alpha$ for zero-shot performance differs from the optimal range for fine-tuning. The paper provides limited guidance on how $\\alpha$ should be selected for a new model or dataset, other than selecting from a predefined set.\n\nThe method relies on constructing an \"auxiliary LTSM\" through random initialization of the same architecture. The paper does not explore or justify this specific choice. It is unclear if a different, or perhaps simpler, randomly initialized model could achieve a similar or even better smoothing effect."}, "questions": {"value": "Listed in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VjTjoGNiwK", "forum": "8o4t5DHaE1", "replyto": "8o4t5DHaE1", "signatures": ["ICLR.cc/2026/Conference/Submission11863/Reviewer_t8mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11863/Reviewer_t8mv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711107845, "cdate": 1761711107845, "tmdate": 1762922882453, "mdate": 1762922882453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fine-tuning challenges of large time series models and identify non-convex loss landscapes in pre-trained LTSM is the key root cause that lead to poor trainability and overfitting during fine-tuning stage. To alleviate the challenge, the authors propose smoothed full fine-tuning, which linearly interpolates the weights of a pre-trained LTSM with a randomly initialized auxiliary model to smooth the loss landscape, to improve trainability while preserve pre-trained knowledge. The method evaluation on forecasting, imputation, and anomaly detection tasks using eight existing LTSMs across multiple benchmark datasets shows consistent improvements over baselines (i.e., full fine-tuning, training from scratch, linear probing, and linear probing then full fine-tuning) with no added computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an interesting issue that pre-trained LTSMs may have sharp loss landscapes that hinder fine-tuning with empirical evidence and visualization.\n\n2. The proposed method is simple yet effective, requiring only a one-time weight interpolation before fine-tuning.\n\n3. The experiment is comprehensive, covering wide range of LTSM architectures and tasks."}, "weaknesses": {"value": "1. Lack of novelty: The core idea of linear weight interpolation to smooth landscapes is not a new idea. Weight averaging/interpolation [1] has been widely studied in model merging [2], continual learning [3], yet none of these have been discussed in related work. Although the paper claims to be \"the first\" for LTSMs, this domain-specific application does not justify novelty, which seems more like a repackaging of existing optimization tricks with application on LTSMs.\n\n2. Lack of theoretical justification: The paper attempts to explore the challenge of model fine-tuning from optimization theory perspective. However, the discussion in Section 3.1 seems hand-wavy. Specifically, the claim, \"interpolation perturbs sharp minima without harming flat regions\" is intuitive but lacks rigor. Providing some theoretical proofs or connection to sharpness-aware minimization can better provide theoretical merit to the readers.\n\n3. Baselines for comparison: None of the parameter-efficient methods like LoRA, QLoRA are included, these are standard for fine-tuning large models. Also, there is no ablation on other smoothing techniques such as Gaussian noise or label smoothing. Without those baselines, it is hard to evaluate the contribution of this paper as the claim of paper is to tackle the \"fine-tuning\" problem of time series foundation model.\n \n[1] Vlaar, Tiffany J., and Jonathan Frankle. \"What can linear interpolation of neural network loss landscapes tell us?.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Wortsman, Mitchell, et al. \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.\" International conference on machine learning. PMLR, 2022.\n\n[3] Kozal, Jędrzej, et al. \"Continual learning with weight interpolation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "questions": {"value": "1. How does SFF compare to simply adding random noise to pre-trained weights (e.g., Gaussian perturbation)?\n2. How sensitive is SFF to the random init distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UsDBkDEzPl", "forum": "8o4t5DHaE1", "replyto": "8o4t5DHaE1", "signatures": ["ICLR.cc/2026/Conference/Submission11863/Reviewer_hGpf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11863/Reviewer_hGpf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062588812, "cdate": 1762062588812, "tmdate": 1762922881993, "mdate": 1762922881993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}