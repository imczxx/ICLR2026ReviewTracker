{"id": "1PRZhOM0vk", "number": 3924, "cdate": 1757568332407, "mdate": 1759898062410, "content": {"title": "Reflective Reinforcement Tool Learning", "abstract": "Tool learning enables large language models (LLMs) to interact with real-world environments. While prior work mainly relies on supervised fine-tuning (SFT), recent reinforcement learning (RL) methods have shown promise in improving the tool-use capabilities of LLMs by leveraging richer reward signals. However, during RL rollouts, failures often stem from environmental perturbations such as network issues or tool instability rather than policy errors. These failed trajectories are typically discarded, resulting in low data efficiency and high costs, especially when using paid tools. To solve the issue, we find that many failures can be recovered through simple retries, reasoning, or reflection. Yet these augmented new policies for self-correction introduce distribution shifts that hinder the reuse of recovered data for origin policy learning. In this paper, we propose Tool-Reflective Reinforcement Learning (Tool-ReRL), an off-policy RL framework that equips LLMs with a reflection mechanism to temporarily adjust the rollout policy, thus analyzing failures, attempting self-correction, and exploring diverse solution paths. To bridge the distribution gap between modified and original policy, we introduce an importance sampling estimator, enabling rewards from reflection-enhanced trajectories to effectively guide the optimization of the original policy. Our extensive experiments on four tool-learning benchmarks demonstrate that, given the same training data, Tool-ReRL significantly improves data efficiency and achieves average performance gains of up to 7.60% and 6.11% over standard RL algorithms based on Qwen2.5-7B and LLaMA3.1-8B, respectively.", "tldr": "", "keywords": ["large language models", "tool learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c1817f23f06f37396da42fa2233b08bfe2c4f6b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates reinforcement learning for large language models in tool-use scenarios. The authors propose a new off-policy framework Tool-Reflective Reinforcement Learning with two key designs: (i) a reflection mechanism to adjust the failure trajectory during the rollout; (ii) an importance sampling estimator to bridge the gap between the current policy and the one that guides the reflection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well-originalized and clearly written.\n- The motivation is well-established. Specifically, this paper seeks to improve the data efficiency during reinforcement learning in tool-LLM."}, "weaknesses": {"value": "- The authors propose using samples with reflective responses, which may introduce distributional shifts. I recommend that the authors provide experimental validation, such as a distributional comparison, to make the argument more convincing.\n- The description of **Assumption 1** is vague. Does it imply that $\\pi_{\\theta_{rfl}}(\\tau^\\*|q^\\*) = \\pi_{\\theta}(\\tau^\\*|q^\\*)$? If so, the use of importance sampling would be unnecessary.\n- Based on the experimental results (e.g., **CoT** vs. **CoT + IS**), importance sampling does not appear to significantly improve performance. This point requires further explanation, even though the proposed method achieves notable improvements.\n- The paper lacks a concrete training example to illustrate how the reflection mechanism is designed.\n- The overall contribution of this work is limited. For example, importance sampling is a standard component of PPO, and the paper does not offer substantial changes."}, "questions": {"value": "See them in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8EwQm5opr", "forum": "1PRZhOM0vk", "replyto": "1PRZhOM0vk", "signatures": ["ICLR.cc/2026/Conference/Submission3924/Reviewer_sJZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3924/Reviewer_sJZG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650540453, "cdate": 1761650540453, "tmdate": 1762917100157, "mdate": 1762917100157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates that fixing negative samples during the RL training process with an importance sampling to offset the distribution mismatch between refined trajectory and existing trajectory can improve the model's tool learning performance.\n\n* It shows many negative samples generated are almost positive samples with minor correction.\n* It proposes a way to fix the negative sample issue by invoking an additional model call to point out the error with suggested edit and ask the model being trained to regenerate a new trajectory based on existing input, original output and the reflection.\n* It shows using the reflection based behavior policy in the importance sampling function can resolve the degradation from the new generated trajectory and further improve the performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper performs extensive experiments on various tool use dataset including RotBench, TaskBench, BFCL and Seals with different training approaches.\n* The reflection-recovery strategy is clearly explained and the overall approach is easy to understand."}, "weaknesses": {"value": "* The detailed setup for the reflection module used in the experiment is missing.\n* The ablation should include training with positive samples only as well to show the increment is not from removing negative samples and training with same number of positive samples after refinement to show the  increment is not from more training signals.\n* Experiment on PPO + IS should also be included.\n* Details of the reflection module are missing, such as the actual module used, what’s the input/output.\n* The results from 2 model on the distribution shift does not provide strong evidence that additional refinement will cause a performance drop given one of the results does not show improvement.\n* The table 1’s benchmark variant is not defined.\n* Using the same policy to generate a refined trajectory should still consider on-policy. The distribution shift is not due to mixing off-policy data with on-policy data but rather mixing different task’s data\n* The analysis on data with near-success or environment-related issues in a single dataset is not extensive to support the claim that many failures can be recovered through simple retries, reasoning, or reflection."}, "questions": {"value": "* What’s the efficiency cost with additional refinement module call.\n* Do all the CoT and Ref variants in all experiments only apply to negative samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o8dFhse0hV", "forum": "1PRZhOM0vk", "replyto": "1PRZhOM0vk", "signatures": ["ICLR.cc/2026/Conference/Submission3924/Reviewer_e2tm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3924/Reviewer_e2tm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836878292, "cdate": 1761836878292, "tmdate": 1762917099656, "mdate": 1762917099656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Tool-Reflective Reinforcement Learning (Tool-ReRL), a framework that addresses data inefficiency in reinforcement learning for tool use by large language models. The authors identify that 44.7% of failed trajectories in standard RL training are near-success cases discarded due to environmental perturbations rather than policy errors. Tool-ReRL incorporates a reflection mechanism that analyzes failures and generates corrective feedback, combined with importance sampling to handle the distributional shift from reflection-augmented trajectories. Experiments on four benchmarks using Qwen-2.5-7B and LLaMA-3.1-8B models demonstrate performance gains of up to 7.60% and 6.11% respectively over standard RL algorithms, with the framework consistently outperforming supervised fine-tuning, DPO, and PPO baselines. The results show that both reflection and importance weighting components are necessary - reflection alone decreased performance by 3.4% without proper correction, while the complete framework transforms previously wasted near-success trajectories into valuable training signals, improving data efficiency in tool learning scenarios where environmental instabilities are common."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Good problem statement, data efficiency in RL training is an important problem\n2. Fixing issues with tools or minor changes to trajectory as correction is a good idea and makes sense\n3. Comparison against a good set of baselines and a broad set of tool use benchmarks\n4. Importance weighting is clearly motivated, explained, and evaluated with ablations\n5. Presentation is clear and easy to follow"}, "weaknesses": {"value": "- One of the baseline methods should be on-policy distillation [1]. The \"reflection\" model is much more powerful than the model being trained. Such a \"teacher\" model is not available in SFT or PPO. And use of distillation will likely be more efficient than PPO. \n- The paper claims that the method is more data efficient. But there is no measurement of how efficient the proposed method is. How much less number of epochs does the proposed method need to reach the same score as the baseline models?\n- The paper lacks a measure of computational efficiency. A much larger model (Deepseek R1) is used as a reflection model. It is possible that the compute and time used to create such reflections exceeds the time it takes to recover from discarding erroneous data. \n\n\n\n\n[1] Agarwal, Rishabh, et al. \"On-policy distillation of language models: Learning from self-generated mistakes.\" The twelfth international conference on learning representations. 2024."}, "questions": {"value": "1. \"we identified 834 failures in total, of which 373 (44.7%) were attributable to near-success or environment-related issues.\" How do you identify \"near-success\"? \n2. How do you verify that the reflection model is correct? What happens when it is incorrect? \n3. Why is this method only limited to \"near-success\" trajectories? Why not apply it to all failed trajectories?\n4. \"if the policy lacks sufficient capability to sample positive examples with reasonable success rates, its contribution\nto RL effectiveness becomes negligible.\" What is the evidence for this claim?\n5. \"tool invocation does not require long-form reasoning\" -- what is the evidence for this claim?\n6. I did not understand this: \"+CoT+IS allows the model to generate its own internal thought sequence but does not perform reflection, while applying importance weights to reduce distributional discrepancy.\"\n7. Paper claims PPO halves the compute required compared to DPO. PPO requires use of a critic model, which is very expensive. Can you substantiate this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "atd792Rfzz", "forum": "1PRZhOM0vk", "replyto": "1PRZhOM0vk", "signatures": ["ICLR.cc/2026/Conference/Submission3924/Reviewer_ysiG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3924/Reviewer_ysiG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964274724, "cdate": 1761964274724, "tmdate": 1762917099389, "mdate": 1762917099389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Tool-ReRL, an off-policy RL framework for tool learning that repairs failed rollouts online via a reflection module and re-uses the repaired trajectories with importance-weighted correction inside a PPO objective. This design converts many environment-induced or near-success failures—typically discarded by prior work—into useful training signals while controlling distributional drift. On four tool-use benchmarks, Tool-ReRL consistently improves average performance over strong baselines with the same data budget (up to +7.60% / +6.11% on Qwen2.5-7B / LLaMA3.1-8B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and reasonable. Environment perturbations create many “false negatives”; turning them into learnable signals can improve the sampling efficiency.\n\n- The analysations and ablations are strong and comprehensive."}, "weaknesses": {"value": "- The method hinges on the inverse-prompt equivalence to estimate behavior policy for IS correction. However, quantitative (or theoretical) bias analysis of the assumption is omitted.\n\n- Computational cost & scalability are not carefully analysed. Reflection attempts and extra sampling add non-trivial wall-clock costs. The paper should report env-steps / wall-clock time / money against strong RL baselines under matched budgets."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WK4vnxQzfg", "forum": "1PRZhOM0vk", "replyto": "1PRZhOM0vk", "signatures": ["ICLR.cc/2026/Conference/Submission3924/Reviewer_T1Ru"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3924/Reviewer_T1Ru"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283394985, "cdate": 1762283394985, "tmdate": 1762917099227, "mdate": 1762917099227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}