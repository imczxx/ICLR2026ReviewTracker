{"id": "hmAviop5rm", "number": 6926, "cdate": 1758002227670, "mdate": 1763545702109, "content": {"title": "BlockSpec: Blockwise Speculative Decoding for Diffusion LLMs", "abstract": "In diffusion-based Large Language Models (dLLMs), parallel decoding is usually realized through threshold-based or top-k strategies. While effective in high-confidence tokens, these strategies often collapse on low-confidence tokens, forcing the model into inefficient single-token decoding. To address this limitation, we propose Block Speculation (BlockSpec), a novel training-free blockwise speculative decoding method that explores multiple future decoding trajectories in parallel. Our method introduces a new tree-based trajectory generation strategy and a blockwise parallel verification module, where decoding tokens are organized into tree exploration paths and then multiple decoding trajectories can be simultaneously verified. Unlike traditional speculative decoding that focuses only on fixed-order left-to-right token speculation, our approach is the first attempt to introduce block-level speculation, which jointly explores both token choices and decoding trajectories for dLLMs. We also design two complementary speculation formulations—intra-block and inter-block speculation—that jointly accelerate dLLMs within and across blocks. Extensive experiments show that the proposed BlockSpec model reduces iteration steps by up to 40\\%, accelerating over 80\\% of decoding steps. As a result, our model achieves up to 7–14× speedup over vanilla dLLMs, together with an additional 1.3× improvement over state-of-the-art methods.", "tldr": "This paper propose BlockSpec, a novel trajectory speculative decoding framework tailored for diffusion-based LLMs, which significantly reduces inference steps and accelerates parallel decoding.", "keywords": ["speculative decoding", "diffusion LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/113b5b537ae33b4ea208b60c0cfa40f6257f1d63.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BlockSpec, a novel, training-free speculative decoding framework designed to mitigate the \"low-confidence degradation\" problem in diffusion-based Large Language Models (dLLMs), where parallel decoding efficiency collapses under uncertainty. BlockSpec addresses this by exploring multiple decoding trajectories simultaneously using a new tree-based generation strategy to create candidate token blocks in parallel. These blocks are then efficiently validated through a corresponding blockwise verification mechanism tailored for the any-order, bidirectional nature of dLLMs, with performance further enhanced by intra- and inter-block speculation. Experimental results demonstrate that this approach significantly reduces decoding steps, achieving up to a 14x speedup over vanilla dLLMs and a 1.3x improvement over strong baselines, establishing a more robust and efficient inference paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed BlockSpec in the paper is a novel training-free decoding acceleration strategy that attempts to consider more branches to alleviate the degradation problem in parallel decoding. Experimental results show that this method is effective on different datasets, different models, and different hardware."}, "weaknesses": {"value": "1. In Section 3.3, the authors state that “…allows key–value caching on the prompt…”. However, the rest of the paper (including the experimental settings) does not clarify whether KV caching was actually used. If KV cache was indeed applied, the authors should provide its design details and implementation description.\n\n2. The stated motivation of the paper is to address the “low-confidence degradation problem of parallel decoding.” However, intuitively, the proposed method should lead to better performance, which is not reflected in the experimental results. I suggest that the authors revise the Introduction to better align it with the presented method and findings.\n\n3. In Figure 3(3), for the current block, the subsequent masked tokens (the rightmost three blocks with ellipses) should be invisible. Is this a plotting error? This visualization seems to indicate that the model is aware that this is the final block to be generated, which could substantially influence the effective generation length and thereby degrade model performance."}, "questions": {"value": "1-2. See weakness 1 and 3.\n3. Although Section 3.4 introduces the concept of inter-block speculation, it still lacks sufficient implementation details. For example, it does not appear in any of the provided pseudocode segments. My question is: if both the current block and the next block are generated simultaneously, and each block is supposed to attend to the prompt as well as all preceding blocks, then which branch of the current block should the next block attend to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qJjILDZ1LM", "forum": "hmAviop5rm", "replyto": "hmAviop5rm", "signatures": ["ICLR.cc/2026/Conference/Submission6926/Reviewer_DaQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6926/Reviewer_DaQS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760784923080, "cdate": 1760784923080, "tmdate": 1762919160243, "mdate": 1762919160243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BlockSpec, a training-free, blockwise speculative decoding method for diffusion language models (dLLMs). Through tree-structured trajectory generation and block-level parallel verification, the method achieves efficient decoding with high parallelism even under low-confidence conditions. The paper further introduces intra-block and inter-block speculation mechanisms, evaluates performance on reasoning and code-generation benchmarks, and analyzes computational overhead and limitations, reporting up to a 14× speedup over a vanilla dLLM."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed BlockSpec framework delivers substantial decoding acceleration, reducing iteration steps by up to 40% and achieving 7–14× speedups over vanilla diffusion LLMs, while maintaining comparable output quality on reasoning and code-generation benchmarks.\n\n2. By combining tree-based trajectory generation with block-level parallel verification, the method effectively mitigates low-confidence degradation in diffusion LLMs, sustaining parallelism without compromising accuracy."}, "weaknesses": {"value": "1. According to Fig. 3(3), all draft blocks are invisible to subsequent masked tokens. Thus, when unmasking a block, the process essentially reduces to dLLM-style generation with a generation length equal to the block size, which can lead to issues such as prematurely forcing an answer or emitting eos early. This suggests that the current blockwise attention mask is problematic.\n\n2. Based on Tables 1 and 2, BlockSpec’s high throughput (TPS) speedup seems attributable not only to fewer forward passes but also to reduced matrix computation induced by the specialized attention mechanism. However, the experiments and appendix do not clearly isolate or quantify this contribution."}, "questions": {"value": "1. See Weakness 1.\n\n2. What is the structure of the blockwise attention mask in inter-block speculation.\n\n3. The experiments appear to show that a blockwise attention mask that breaks the bidirectional attention mechanism can still yield generation that is nearly lossless relative to full bidirectional attention. Have the authors investigated the underlying reasons for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Um4QxvQm27", "forum": "hmAviop5rm", "replyto": "hmAviop5rm", "signatures": ["ICLR.cc/2026/Conference/Submission6926/Reviewer_XYVg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6926/Reviewer_XYVg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761293986325, "cdate": 1761293986325, "tmdate": 1762919159526, "mdate": 1762919159526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Common unmasking strategies, such as 'confidence threshold' and 'top-k', are slower on low-confidence tokens since only a single token can be sampled per step. The authors argue that low confidence occurs when dLLMs consider multiple possible trajectories. The paper, therefore, proposes a novel self-speculative decoding method called BlockSpec, which simultaneously searches for and verifies possible trajectories to select multiple tokens. BlockSpec achieves much higher throughput than the baseline and is twice as fast as concurrent work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- BlockSpec is a customised, self-speculative decoding method designed specifically for dLLMs. It effectively addresses issues such as multi-token verification and changing the hidden state prefix.\n- The paper also proposes a novel inter-block speculation method that substantially increases block efficiency.\n- BlockSpec achieves a much higher TPS speedup compared to the baselines.\n- BlockSpec is twice as fast as concurrent work."}, "weaknesses": {"value": "- W1: There is no empirical evidence to support the argument that the low confidence tokens are due to plausible candidates. For example, the model would display low confidence if the problem was too complex for it to handle.\n- W2: Since dLLMs use bidirectional attention, the hidden state of the prefix tokens varies as the model explores different nodes in the search tree. Therefore, the method is based on Fast-dLLM, which fixes the prefix token during the denoising process. However, since prefix caching leads to performance degradation, dependency on Fast-dLLM must be a weakness.\n- W3: Despite its fast speed, BlockSpec shows similar performance to Fast-dLLM (dual cache). Formal Speculative Decoding methods maintain the original performance of the target models.\n- W4: There has been no ablation study on the role of 'medium-confidence tokens'. \n- W5: Short generation length. The generation lengths are fixed to 512, which is relatively short."}, "questions": {"value": "- Typo in section 2 \"When facing everal...\"\n- Is there any way to find an optimal tree configuration for arbitrary settings, such as model size, dataset, GPU FLOPS, and GPU memory bandwidth?\n- W1: Could you show the results for more complex tasks, such as AIME?\n- W3: Where does the performance degradation come from? I believe that the Fast-dLLM's performance would be the upper bound."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RdTCwo6bag", "forum": "hmAviop5rm", "replyto": "hmAviop5rm", "signatures": ["ICLR.cc/2026/Conference/Submission6926/Reviewer_VSE3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6926/Reviewer_VSE3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894534974, "cdate": 1761894534974, "tmdate": 1762919159088, "mdate": 1762919159088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Speculative Parallel Decoding (SPD) for diffusion-based LLMs, a topic gaining traction in the community. The method is training-free, maintains near-lossless performance on studied benchmarks, and introduces a simple draft tree generation algorithm with verification. The speed-up gains are good"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes SPD for diffusion LLMs, which is an emerging and relevant area.\n- Training-free approach, reducing complexity and resource requirements.\n- Maintains almost lossless performance across evaluated benchmarks.\n- Introduces a simple draft tree generation algorithm combined with verification.\n- Works effectively with multi-token unmasking while maintaining accuracy."}, "weaknesses": {"value": "- Draft tree generation algorithm lacks clarity in certain cases (e.g., W3D3(6) configuration).\n- Missing citations for speculative decoding and diffusion-related prior work.\n- Some results and configurations are not fully explained (e.g., latency trade-offs, inter-block setup).\n\nPlease see questions for more details"}, "questions": {"value": "Draft Tree Generation:\nFor W3D3(6), why is there no [1,4] node at step (or level) 2, given that the likelihood of [1,4] > [1,3,4] always?\n\n\nMain Results:\nCould the authors clarify that W2D2(3) was chosen because it offers lower latency compared to W3D3(6) on A800 early on in the results section?\n\n\nFigure 5:\nWhat does “base” refer to, and on which dataset are these results generated? Also, what is the difference between Fast-dLLM (dual cache) and the proposed method on H800, given that Fast-dLLM performs second best on A800?\n\n\nTable 1:\nHumanEval(0) shows BlockSpec outperforming baseline methods, while in other benchmarks BlockSpec performs slightly worse (as expected due to lack of lossless generation). Is there any justification for this anomaly?\n\n\nTable 3:\nInter-block has the highest average tokens per step—what about its latency? Is the inter-block setup used only for 2 blocks? If extended to more than 2 blocks, does it start to hit TPS limits? Also, when doing inter-block, are draft trees for respective blocks generated independently?\n\n\nMissing Citations:\nCitations for speculative decoding and diffusion-based approaches are missing. This idea essentially extends speculative decoding for images as discussed in prior works.\n\n- Diffusion Speculation\n\n\n1. Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation\nHengyuan Hu, Aniket Das, Dorsa Sadigh, Nima Anari\narXiv:2505.03983\n\n\n2. Accelerated Diffusion Models via Speculative Sampling\nValentin De Bortoli, Alexandre Galashov, Arthur Gretton, Arnaud Doucet\narXiv:2501.05370\n\n\n- Speculative Decoding\n\n\n1. Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs\nRaghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott\narXiv:2403.00858\n\n\n2. Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement\nWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott\narXiv:2402.14160"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wNdXQz9i7y", "forum": "hmAviop5rm", "replyto": "hmAviop5rm", "signatures": ["ICLR.cc/2026/Conference/Submission6926/Reviewer_VDuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6926/Reviewer_VDuL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939085042, "cdate": 1761939085042, "tmdate": 1762919158515, "mdate": 1762919158515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}