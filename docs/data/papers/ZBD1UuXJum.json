{"id": "ZBD1UuXJum", "number": 10592, "cdate": 1758176748207, "mdate": 1759897641748, "content": {"title": "Learning Diverse Skills for Behavior Models with Mixture of Experts", "abstract": "Imitation learning has demonstrated strong performance in robotic manipulation by learning from large-scale human demonstrations. While existing models excel at single-task learning, it is observed in practical applications that their performance degrades in the multi-task setting, where interference across tasks leads to an averaging effect. To address this issue, we propose to learn diverse skills for behavior models with Mixture of Experts, referred  to as Di-BM. Di-BM associates each expert with a distinct observation distribution, enabling experts to specialize in sub-regions of the observation space. Specifically, we employ energy-based models to represent expert-specific observation distributions and jointly train them alongside the corresponding action models. Our approach is plug-and-play and can be seamlessly integrated into standard imitation learning methods. Extensive experiments on multiple real-world robotic manipulation tasks demonstrate that Di-BM significantly outperforms state-of-the-art baselines. Moreover, fine-tuning the pretrained Di-BM on novel tasks exhibits superior data efficiency and the reusable of expert-learned knowledge.", "tldr": "", "keywords": ["robotic learning; manipulation; behavior model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/462882c8022879034ba5bf0eaa637b5fb0a294fd.pdf", "supplementary_material": "/attachment/2a43ad9cc538252748dc72f26cfb204d3e5047e4.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Di-BM, a method integrating a Mixture of Experts (MoE) into the robotic learning framework, using an Energy-Based Model (EBM) to route tasks to suitable experts. The primary goal is to address the performance degradation and \"averaging effect\" of existing imitation learning models in multi-task settings. The authors propose an MoE structure that enables experts to specialize in sub-regions of the observation space, and which can be easily integrated into existing imitation learning methods as a \"plug-and-play\" module. The paper conducts comprehensive real-world experiments and analyses to show the effectiveness of the approach, demonstrating strong multi-task performance and superior data efficiency when fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors performed a thorough empirical investigation, including key ablation studies on MoE formulation and the number of experts, to show how their proposed approach compares against baselines.\n- Di-BM performs well across multiple real-world manipulation tasks, indicating that the proposed diverse skill learning approach can effectively address the \"averaging effect\" in a challenging multi-task environment."}, "weaknesses": {"value": "- The discussion of related work on MoE in robotics appears to be missing several relevant, recent methods [1, 2, 3]. It would be difficult to determine the precise contribution and novelty of Di-BM without discussing how it compares to or differs from these existing approaches.\n- The experiment settings could be clearer. For instance, the paper's central claim is about 'multi-task training', but the exact training procedure isn't explicitly defined. The authors should clarify if \"multi-task training\" means a single policy was trained on an aggregated dataset containing all 9 real-world manipulation tasks, as this is critical to evaluating the claims about mitigating the \"averaging effect\".\n- The real-world experiment results are based on \"10 trials per task\". This sample size may be too small to robustly justify the effectiveness of the approach, as the difference between, for example, a 0.80 and 0.60 success rate (as seen in Table 1) might not be statistically significant. The authors should consider running more trials or, at a minimum, acknowledging this as a limitation.\n\n[1] Efficient diffusion transformer policies with mixture of expert denoisers for multitask learning\n\n[2] Mixture-of-experts network with task-oriented perturbation for visual reinforcement learning\n\n[3] Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning"}, "questions": {"value": "Regarding the discussion in Section 4.1:\n1. The paper states that the action entropy bonus $H(\\pi(a|o))$ is omitted from Equation (2). However, this term was not present in Equation (2) as defined in Section 3.2. Could the authors please clarify this discrepancy in the presentation?\n2. Can the author elaborate on how the diffusion model's architecture specifically replaces the function of this entropy bonus, as it is a key design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RvFmLcbE1w", "forum": "ZBD1UuXJum", "replyto": "ZBD1UuXJum", "signatures": ["ICLR.cc/2026/Conference/Submission10592/Reviewer_o12B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10592/Reviewer_o12B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280580698, "cdate": 1761280580698, "tmdate": 1762921862095, "mdate": 1762921862095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Di-BM**, a novel Mixture of Experts (MoE) framework designed to improve multi-task performance in robotic imitation learning. The key problem it addresses is the \"averaging effect\" that plagues policies trained on diverse datasets. \nIt utilizes Energy-Based Models (EBMs) as the gating mechanism, which models each expert's favored observation distribution.\nThis allows experts to autonomously specialize in sub-regions of the observation space, which correspond to primitive skills. \nThe method is implemented as a plug-in for Diffusion Policy (DP) and evaluated on 9 real-world manipulation tasks. The results show significant improvements in success rates over the valinla DP baseline and MoE variants. \nFurthermore, the paper demonstrates that the pre-trained Di-BM model exhibits superior data efficiency when fine-tuned on novel tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel Direction**: The core idea of applying MoE to imitation learning is novel.\n\n2. **Strong Empirical Results**: The paper provides strong quantitative evidence for its claims. The performance leap over the baselines is substantial. The real-world experiments demonstrate the potential of practical applications.\n\n3. **Analysis of Learned Experts**: The visualization of expert activation $\\pi(e|o)$ in Fig. 3 supports the hypothesis that the experts are specializing in distinct phases or primitive skills that are shared across different tasks.\n\n4. **Data Efficiency for Fine-Tuning**: The fine-tuning experiment in Sec. 5.6 shows that the pre-trained Di-BM model adapts to new tasks more efficiently than baselines. This suggests the learned primitive skills are reusable and indicates potential for future scalability.\n\n5. **Clarity**: The paper is well-written and easy to follow. The methodology and motivation are clearly explained, supported by effective figures and visualizations."}, "weaknesses": {"value": "1. **Limited Methodological Novelty**: Although applying MoE to imitation learning is novel, the core methodology follows the prior work of MoE in RL [1].\n\n2. **Sensitivity to Hyperparameter**: The authors mention the model's sensitivity to the KL regularization coefficient $\\beta$ (Sec. 5.5, Fig. 4). A $\\beta$ that is too small causes all experts to \"slack off\" and avoid difficult parts of the observation space. This sensitivity could present challenges when scaling Di-BM to more complex tasks or datasets.\n\n3. **Missing Analysis of Computational Overhead**: The paper claims \"minimal computational overhead\" without explicit analysis. At inference, the model must first compute the expert probabilities via the gating network $g_{\\phi}$ before executing the selected expert $f_{\\theta}$. Especially in robotics applications, where real-time inference and reaction speed are crucial. An analysis of inference-time overhead is important."}, "questions": {"value": "1. Figure 3 shows that the router utilizes different experts as a task progresses. How can we be sure the router is learning to select the *best* expert for each stage? The paper shows performance for individual experts on *full* tasks (Table 3), but it would be more convincing to see an analysis of individual expert performance on manually-partitioned task stages to verify the router's choices.\n\n2. How does the routing strategy evolve during training? One would expect experts to have similar weights initially and then specialize. Is this specialization and the resulting routing strategy consistent across different training runs with different random seeds? In other words, do the experts learn the same set of \"primitives\" each time?\n\n3. In Sec. A.6, the authors state that when using 8 experts, some are underutilized or \"pruned\". If these experts are simply pruned, why does the model's overall performance drop substantially (Table 8)  instead of matching the 5-expert model? Furthermore, Figure 3 also shows that some experts are underutilized. Given this, would it be possible to develop a method that dynamically adjusts or prunes the number of experts during training?\n\n4. The paper's main novelty is adapting the Di-SkilL [1] framework from RL to IL. Could you elaborate on the specific challenges encountered and design choices in this adaptation?\n\nI am willing to raise my score if my concerns are addressed.\n\n[1] Celik, Onur, Aleksandar Taranovic, and Gerhard Neumann. \"Acquiring diverse skills using curriculum reinforcement learning with mixture of experts.\" ICML, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKp8xPMvPl", "forum": "ZBD1UuXJum", "replyto": "ZBD1UuXJum", "signatures": ["ICLR.cc/2026/Conference/Submission10592/Reviewer_tBc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10592/Reviewer_tBc1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525862934, "cdate": 1761525862934, "tmdate": 1762921861667, "mdate": 1762921861667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Di-BM, a Mixture of Experts (MoE) framework designed to improve multi-task imitation learning for robotic manipulation. Traditional behavior models trained on diverse demonstrations suffer from task interference and “averaging effects.” Di-BM addresses this by associating each expert with a distinct observation distribution, modeled via an energy-based model. The gating network automatically allocates data to the most suitable expert, allowing each to specialize in a subset of primitive skills. They represent each expert using a diffusion model and evaluate their model on real-world robotic manipulation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Gating visualisations nicely show that the model utilises different experts\n- The methods shows strong empirical results, showing improvement on several real-world robotic tasks, verified through ablations and visualizations\n- The method can be incorporated seamlessly into existing imitation learning architectures"}, "weaknesses": {"value": "- The paper does not mention related work that uses very similar methodology and goals, namely [1] and [2]. \n\n- In [1] they show that the optimal gating can be computed in closed form, making it unnecessary to learn a model in every iteration but it is sufficient to only learn a gating at the end of training. What benefit do the authors see when learning the gating?  \n\n- Additionally, in [1] the authors establish convergence guarantees from an expectation-maximisation perspective. Do the authors think similar results could be applied here? As an additional comment, in e.g. [3] they show that the diffusion noise matching loss is a lower bound on the marginal likelihood. In that sense, the expert objective could be seen as a lower bound on a weighted maximum likelihood objective. \n\n- The authors only consider real-world experiments. It would be good to also include comparisons in established simulation-based benchmarks. On that note, there exists a benchmark that is designed for diverse behaviour learning, see [4]. It would be interesting to see if the proposed method improves behaviour diversity over existing methods.\n\n- It would be good if the authors provide an ablation study showing how sensitive the performance is with respect to the KL regularisation parameter. From Figure 4, it seems like even minor changes can have a huge impact on the learned model. How difficult is it to choose the parameter? Did the authors tune the parameter per task or did they find a setting that worked for all?\n\n[1] Information maximizing curriculum: A curriculum-based approach for learning versatile skills\n[2] Curriculum-based imitation of versatile skills\n[3] Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations\n[4] Understanding Diffusion Models: A Unified Perspective"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ITXr055hNO", "forum": "ZBD1UuXJum", "replyto": "ZBD1UuXJum", "signatures": ["ICLR.cc/2026/Conference/Submission10592/Reviewer_myRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10592/Reviewer_myRN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558229444, "cdate": 1761558229444, "tmdate": 1762921861158, "mdate": 1762921861158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Diverse Skills for behavior models (Di-BM), an imitation learning method that can learn policies for multi-task settings. More concretely, Di-BM employs Mixture of Experts policies that can specialize on a subset of the available training data by employing an energy-based gating network. The gating network represents an observation distribution per expert, thereby allowing each expert to specialize in observation-action regions it favors. The paper shows, on real-robot experimental tasks, that the proposed method works nicely and is able to learn multi-task policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written. The reader can easily follow the story of the paper and understand the motivation behind the proposed methods\n- The benefits of choosing an MoE policy representation are well-grounded by intuitive figures (e.g., Fig.3) on the training data\n- The method is validated on real-robot experiments, emphasizing its strengths"}, "weaknesses": {"value": "- The work lacks important related works that have employed similar ideas on learning parameterized distributions over the input (observation) space [1,2]. How does the proposed method algorithmically differ from these methods? \n- Better description of the data set; i.e., which tasks are included? Are different robot types used? .... It's hard to infer the \"difficulty\" of a task. Commenting on the task difficulty could help the reader. \n- Although I appreciate the real-robot experiments, I believe the paper would be strengthened if Di-BM could be benchmarked on common benchmark suites, for example, such as LIBERO [3] or Robocasa [4].\n\n[1] M. X. Li, et al. Curriculum-based imitation of versatile skills. ICRA 2023.\n[2] D. Blessing, et al. Information Maximizing Curriculum: A Curriculum-Based Approach for Imitating Diverse Skills. NeurIPS 2023.\n[3] O. Mees, et al. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. RA-L 2022. \n[4] B. Liu, et al. Libero: Benchmarking knowledge transfer for lifelong robot learning. NeurIPS 2023."}, "questions": {"value": "- In Section 4.2 it is said that the observations $o \\sim p(o)$ are sampled to cover a sufficiently large batch of observations. Does this mean the observations are sampled from the offline data set, or do the observations come from the environment by sampling from it?\nIf the observations come from the data set, how does the method behave if we can not sample a representative batch of observations for approximating the normalization constant? Or in general, was this a problem observed during the experiments?\n\n- It seems that single experts are even better than the baselines from Table 2. For example, all experts from Table 3 are better on the Rearrange cup task than the Task-wise MoE model from Table 2. Is there an intuitive explanation behind it? Intuitively, I would have expected that the Task-wise MoE performs better consistently compared to the single experts. \n\n- Does the MoE integration also work for goal-conditioned, score-based diffusion policies[5]? \n\n\n[5] M. Reuss, et al. Goal-conditioned Imitation Learning using Score-based Diffusion Policies. RSS 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nlhcDexKDy", "forum": "ZBD1UuXJum", "replyto": "ZBD1UuXJum", "signatures": ["ICLR.cc/2026/Conference/Submission10592/Reviewer_jt5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10592/Reviewer_jt5F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573314503, "cdate": 1761573314503, "tmdate": 1762921860489, "mdate": 1762921860489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an MoE architecture and training method for robotic multi-task imitation learning. To mitigate multi-task gradient interference, a gating network is used to map the observation to the distribution of experts, and each expert policy is trained to handle a specific subset of observations. The training framework can adaptively learn the gating network using energy-based models, rather than predefine task assignment. Experiments on some real-world manipulation tasks demonstrate its effectiveness compared with previous MoE strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The MoE framework introduced in this paper can learn task assignment autonomously, enhancing the potential to learn from large-scale, unstructured multi-task datasets.\n- Experimental results on real-robot tasks demonstrate its effectiveness. Qualitative visualizations show that the models do learn some meaningful task assignments.\n- The paper writing is well organized and easy to follow."}, "weaknesses": {"value": "- Limited technical contributions. After reading the method sections, it seems that most of techniques are adopted from the two prior works (Celik et al., 2022; 2024). The main difference is changing their reinforcement learning setting to the imitation learning setting.\n- Lack of reproducible simulation experiments. There are a lot of multi-task imitation learning benchmarks in simulation that are widely used in prior robotic imitation learning research, like Meta-World, Libero, RoboCasa, and RoboTwin. However, the paper only uses real-world experiments, making the results less reproducible.\n- The effectiveness on large-scale datasets, like Open-X-Embodiments, has not been studied."}, "questions": {"value": "- What are the main technical differences between the proposed method and Celik et al., 2022; 2024?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rVqPD5IYsH", "forum": "ZBD1UuXJum", "replyto": "ZBD1UuXJum", "signatures": ["ICLR.cc/2026/Conference/Submission10592/Reviewer_43vA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10592/Reviewer_43vA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980753006, "cdate": 1761980753006, "tmdate": 1762921859445, "mdate": 1762921859445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}