{"id": "m3HXHQYmZu", "number": 24686, "cdate": 1758359347915, "mdate": 1759896754621, "content": {"title": "ProxyAttn: Guided Sparse Attention via Representative Heads", "abstract": "The quadratic complexity of attention mechanisms limits the efficiency of Large Language Models (LLMs) on long-text tasks. Recently, methods that dynamically estimate block importance have enabled efficient block sparse attention, leading to significant acceleration in long-text pre-filling of LLMs. However, their block-level coarse-grained estimation inevitably leads to performance degradation at high sparsity ratios. In this work, we propose ProxyAttn, a training-free sparse attention algorithm that achieves token-level estimation by compressing the dimension of attention heads. Based on our observation of the similarity among multiple attention heads in long texts, we use the attention scores of pooled representative heads to approximate the scores for all heads. To account for the varying sparsity among heads, we also propose a block-aware dynamic budget estimation method. By combining the scores from a set of representative heads with a multi-head dynamic budget, we can achieve a more fine-grained block attention evaluation at a low computational cost. Experiments on a variety of mainstream models and extensive benchmarks confirm the underlying similarity among attention heads in long texts. Leveraging a token-level fine-grained estimation, the proposed method achieves substantial gains in performance and efficiency compared to existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention acceleration and 2.4x prefilling acceleration without significant performance loss.", "tldr": "", "keywords": ["Efficient LLM", "Sparse Attention"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/820c3eccdc78b72a828e9bfe9675483788e29b12.pdf", "supplementary_material": "/attachment/45787409bc0a040e3cc74fdd1351874e4f25fa90.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ProxyAttn, a method for improving block selection in block-sparse attention. Instead of reducing along the sequence dimension to estimate high-attention regions, ProxyAttn computes a subset of attention heads and uses them as proxies to estimate which blocks to compute for the remaining heads. Experiments shows improved performance and efficiency when using the proposed sparse attention method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting insight into the similarity of attention trends across different heads.\n2. The design choices of the proxy attention mask and dynamic budget allocation are well-motivated by corresponding empirical observations and oracle analyses, enhancing the interpretability of the proposed method.\n3. The proposed approach outperforms other sparse attention baselines on retrieval tasks (RULER benchmark), matching or even surpassing the dense attention baseline."}, "weaknesses": {"value": "1. **Inconsistent performance on long-context understanding tasks:** Although the proposed method performs well on retrieval tasks, its advantage is less obvious on more realistic long-context benchmarks. This raises questions about whether the observed attention pattern similarity generalizes beyond retrieval settings, since real-world tasks often involve richer semantics and more diverse attention behaviors.\n\n2. **Definition clarity:** Some core concepts, such as *attention score*, are ambiguously defined. For instance, does it refer to the $(i, j)$ entry of the attention matrix, or the aggregated attention value across columns?\n\n3. **Figure clarity:** Several figures are not clearly explained. For example, the meaning of the x-axis in Figure 2(c) is unclear. How does it support the claim that the primary variation between heads lies not in the tokens they attend to?"}, "questions": {"value": "1. In the attention head consistency oracle experiment (Figure 2a), how much of the overlap is attributable to attention sinks? What happens if the influence of attention sinks is removed?\n2. What does the term “average score” in Line 149 refer to? Is it the mean *attention score* across heads or something else?\n3. Does the proxy head mechanism perform consistently across both retrieval and question-answering scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MzwjA13ehN", "forum": "m3HXHQYmZu", "replyto": "m3HXHQYmZu", "signatures": ["ICLR.cc/2026/Conference/Submission24686/Reviewer_kQrf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24686/Reviewer_kQrf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873403056, "cdate": 1761873403056, "tmdate": 1762943163581, "mdate": 1762943163581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We have reviewed ProxyAttn with great interest and appreciate its contribution to advancing sparse attention techniques. We would like to draw the authors’ and reviewers’ attention to a prior related work, SharePrefill (https://arxiv.org/abs/2505.19578, May 2025), which appears to share several core ideas with ProxyAttn. Given SharePrefill’s temporal precedence and the conceptual similarities between the two works, in particular:\n\n- Both studies observe consistent similarity across attention heads,  as stated in SharePrefill: “(1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs” and in ProxyAttn: “attention heads exhibit consistency in token focus.”\n\n- SharePrefill’s approach of  \"(1) Offline clustering to group heads based on the similarity of their attention score maps. (2) Online inference, where pivotal attention is constructed dynamically and shared with other heads during the inference process.\" appears conceptually related to ProxyAttn’s method of “using scores from pooled representative heads to approximate those of all heads.”\n\nWe believe that explicitly clarifying the distinctions between ProxyAttn and SharePrefill would further strengthen the paper by highlighting its unique contributions within the existing body of work. We would be happy to provide more detailed technical comparisons if helpful."}}, "id": "UkPJ8Hwuv1", "forum": "m3HXHQYmZu", "replyto": "m3HXHQYmZu", "signatures": ["~Zewen_Ye1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Zewen_Ye1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24686/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763030602110, "cdate": 1763030602110, "tmdate": 1763030602110, "mdate": 1763030602110, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ProxyAttn, an attention approximation method designed to accelerate the pre-filling phase.\n\nThe method is based on the key observation that attention patterns across different heads within the same layer are highly similar, differing primarily in their sparsity. Therefore, ProxyAttn employs a pooling head to generate shared attention scores for top-K block retrieval.\n\nFurthermore, the authors propose using the query from each head's final block to estimate its specific sparsity budget, which enables a more effective resource allocation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper's motivation is well-articulated. Through multi-faceted comparisons and visualizations, it substantiates the hypothesis that differences between attention heads lie not in their *patterns*, but rather in their *magnitude* and *sparsity*. This insight validates the use of a pooling head for important block selection as a reasonable approach.\n\n2.  The experiments are thorough, conducted across the Ruler, LongBench-v2, and InfiniteBench benchmarks. The evaluation covers two mainstream models, Qwen and LLaMA, lending strong reliability to the results."}, "weaknesses": {"value": "1.  The paper critiques prior methods for coarse-grained block retrieval. However, its own method still employs a stride, resulting in block-wise retrieval and top-k computations. This merely mitigates the coarse-granularity issue rather than fundamentally solving it—a true solution is likely infeasible due to prohibitive retrieval overhead.\n\n2.  The method offers limited speedup, achieving only ~2x on a 70B model. The baseline selection seems insufficient, as current mainstream models often use 7x or 8x GQA ratios. In such high-ratio scenarios, FlashAttention's computational speed increases significantly, representing a more practical setting (however, not optimal for your end-to-end TTFT comparison). Moreover, with the rise of local + global attention models (e.g., Minimax, Llama 4), the significance of pre-filling acceleration is diminishing."}, "questions": {"value": "1. Regarding line 142: \"the score at position $(i,j)$ represents the cumulative attention score at head $j$ obtained by the top 1024 tokens from head $i$.\"This statement is unclear to me. Could you please provide the formula to explain this metric?\n\n2. What is the TTFT of your method on the Qwen2.5-1.5B, 3B, and 7B models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BoGSXCDnR6", "forum": "m3HXHQYmZu", "replyto": "m3HXHQYmZu", "signatures": ["ICLR.cc/2026/Conference/Submission24686/Reviewer_C449"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24686/Reviewer_C449"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882996902, "cdate": 1761882996902, "tmdate": 1762943163349, "mdate": 1762943163349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ProxyAttn, a training-free sparse attention algorithm for long-context prefill. The key idea is to compress along the head dimension: group attention heads and use a small number of representative “proxy” heads to compute near token-level scores, then max-pool and aggregate them into fine-grained block importance estimates. To accommodate that different heads prefer different sparsity levels, the method adds a block-aware per-head dynamic budget based on a lightweight cumulative-probability criterion. This yields fine-grained, block-sparse masks at low estimation cost, delivering attention speedups and 2.4× prefill acceleration with no significant performance loss."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel sparse attention design. Instead of compressing along the sequence dimension (tends to be coarse), the paper compresses along the head dimension to obtain token-level importance at the cost of only a few proxy heads, which is conceptually clean and practically effective.  \n2. No retraining or model changes; works as a drop-in prefill accelerator and ports across model families.\n3. Strong speedups while maintaining full-attention accuracy on long context benchmarks.\n4.  The approach is validated by the observation that heads share token focus on long context, justifying proxy-head sharing."}, "weaknesses": {"value": "I don't see any substantive weakness."}, "questions": {"value": "1. The method hinges on cross-head token-focus similarity; corner cases with highly specialized heads may degrade the shared ranking quality. Do authors observe tasks or head types where the shared ranking under-represents important blocks (e.g., rare long-range dependencies)?\n2. How sensitive is performance to the group size and the choice of the representative head? Could learned or data-driven method further help beyond simple averaging (e.g., fine-tune somes head as proxyheads)? \n3. Could the proxy-heads be reused for decoding with query-aware sparse attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a9DWtlowPB", "forum": "m3HXHQYmZu", "replyto": "m3HXHQYmZu", "signatures": ["ICLR.cc/2026/Conference/Submission24686/Reviewer_N8YJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24686/Reviewer_N8YJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971998808, "cdate": 1761971998808, "tmdate": 1762943163170, "mdate": 1762943163170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ProxyAttn, a training-free sparse attention method designed to speed up long-context Transformers by exploiting redundancy across attention heads. Instead of pruning tokens directly, the authors group similar heads into “proxy heads” and use their aggregated (max-pooled) attention scores to estimate token importance. Each real head then receives its own Top-K allocation based on its intrinsic sparsity. This approach effectively compresses along the head dimension rather than the sequence dimension, which is both novel and impactful. Experiments on large-scale LLMs (LLaMA 3.1, Qwen 2.5) show up to a 10× speedup with almost no drop in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The method can plug into existing Transformer architectures without retraining or parameter tuning, making it practical for deployment.\n2.Using proxy heads to estimate token-level importance leads to higher overall sparsity while preserving key contextual information—often matching or slightly outperforming full-attention baselines on long-context benchmarks.\n3.The reported efficiency gains are impressive: up to a 10.3× reduction in kernel-level attention time and a 2.4× speedup in total prefilling latency, with accuracy losses staying under one percentage point—an excellent trade-off for latency-critical use cases."}, "weaknesses": {"value": "1.The method is only evaluated during the prefill stage; it’s unclear whether similar benefits (or issues) would arise during autoregressive decoding, especially for long generations.\n2.Important hyperparameters—like the sparsity threshold and minimum Top-K—are tuned via grid search. The lack of an adaptive or principled selection rule could make it harder to apply the method out-of-the-box to new models or domains.\n3.While computational savings are detailed, the paper doesn’t discuss the memory overhead from storing proxy key–value tensors, which could be a concern on GPUs with limited memory."}, "questions": {"value": "see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "38fCjyFUPX", "forum": "m3HXHQYmZu", "replyto": "m3HXHQYmZu", "signatures": ["ICLR.cc/2026/Conference/Submission24686/Reviewer_wqai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24686/Reviewer_wqai"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013000066, "cdate": 1762013000066, "tmdate": 1762943162948, "mdate": 1762943162948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}