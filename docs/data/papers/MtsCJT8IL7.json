{"id": "MtsCJT8IL7", "number": 21281, "cdate": 1758315794410, "mdate": 1759896931015, "content": {"title": "Collaborative Coding with Mixed Initiative LLMs", "abstract": "Large Language Models (LLMs) is quickly arising as a partner for users to solve complex task through multiple interaction turns. To study such interaction, we introduce \\ourtask, a task and evaluation framework for multi-turn User-LLM interaction for collaborative coding task, where a user work with an LLM assistant to design a website. Most existing LLM assistant work study single-initiative settings, where the LLM assistant generates only output attempts or only clarifying questions to ask the user. We demonstrate both are suboptimal: attempting to predict at every turn is inefficient, as it significantly increases interaction length. Asking questions at every turn is ineffective, as LLM are not very good at asking good clarifying questions consecutively without attempting the task. Given these tradeoffs, we propose mixed-initiative interactions, where LLM alternates between generate clarifying questions and attempting an output, achieving 99% of the output quality from such single-initiative interactions with conversations that are only 55% as long. Lastly, we investigate why mixed-initiative interactions are so effective, demonstrating that mixed-initiative interactions can lead to more helpful user answers to clarifying questions and more efficient communication between the user and assistant.", "tldr": "", "keywords": ["code generation", "LLMs", "muli-turn interaction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6a97c3c1060fbdf4c3ee26cf8fbb091fba2dfdc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The benchmark an important question in human llm collaboration, specifically investigating mixed initiative vs single initiative collaboration between humans and llm. The idea is simple, but framed cleanly and the insights / takeaways are interesting. Overall, I believe this paper has significant potential. However, as is, this paper would be more suited to a different venue that focuses more on analysis of Human-LLM behavior. The content is currently too narrow and the contributions are too limited to be accepted as a completed work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Benchmark tackles a significant and important question in human llm collaboration.\n- The idea is simple, but framed cleanly and the insights/takeawys are interesting"}, "weaknesses": {"value": "- Experiments need evaluate on more models (at least all major providers or all open source models).\n- The methods in the paper are currently relatively simple. In fact, I would consider all approaches currently in the paper as a baseline method. There needs to be a method that actually improves human-llm collaboration. \n- The \"mixed initiative\" method underperforms in terms of LLM as a judge score, with the gains coming mostly from efficiency / turns."}, "questions": {"value": "My main questions is why there are so little models and methods. It feels that the experiments described in the paper would not be prohibitively expensive to run."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yYlg51Zqk9", "forum": "MtsCJT8IL7", "replyto": "MtsCJT8IL7", "signatures": ["ICLR.cc/2026/Conference/Submission21281/Reviewer_BHCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21281/Reviewer_BHCs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858080654, "cdate": 1761858080654, "tmdate": 1762941667623, "mdate": 1762941667623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduce CoWebDesign, a task and evaluation framework for multi-turn UserLLM interaction for collaborative coding task. The user and AI assistant together design a webpage in HTML + CSS. To evaluate the quality of the web design, the model use VLM judge to compare the rendered webpage against the gold webpage image to score in 1 to 10 scale. The model evaluate 2 LLMs as the AI assistants: Llama 3 8B  and Owen3 8B."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to read, the task is interesting"}, "weaknesses": {"value": "The evaluator performance is very low with the best evaluator GPT-4o achieving only 54 accuracy\nThere is no evaluation on the user simulator\nOnly evaluate 2 LLMs as the AI assistant"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UlUoAxtiTp", "forum": "MtsCJT8IL7", "replyto": "MtsCJT8IL7", "signatures": ["ICLR.cc/2026/Conference/Submission21281/Reviewer_Aaos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21281/Reviewer_Aaos"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890100011, "cdate": 1761890100011, "tmdate": 1762941667372, "mdate": 1762941667372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies multi-turn interaction patterns for collaborative web design. The authors introduce CoWebDesign, where users work with LLM assistants to implement websites. They compare single-initiative approaches (always predicting or always asking questions) against mixed-initiative patterns that alternate between both. Results suggest mixed strategies achieve comparable quality with shorter conversations. Ablations indicate intermediate outputs help by allowing implicit communication and enabling more informative user responses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The web design task enables user simulation based on target images rather than gold code (avoiding information leakage). Critically, the framework tracks different interaction costs—answering questions versus evaluating outputs and providing feedback—which prior work ignored.\n2. Results hold across two models. The ablations in Section 4 go beyond performance comparison, showing how intermediate generations enable implicit assumption communication."}, "weaknesses": {"value": "1. Section 3.2 states \"We report our main results in Table 2\" but no such table exists—presumably referring to Figure 2 instead. More problematic, the paper does not provide numerical tables with actual performance values. Figure 2 shows only line graphs. Without clear numerical data and statistical testing (especially given κ=0.31 evaluation reliability), it's hard to verify the quantitative claims. The main experimental results should be presented in a readable numerical table.\n2. The paper frames the work around LLMs deciding when to ask questions versus predict. However, Alternate and Swap follow fixed schedules. The model never makes context-dependent decisions about whether it has enough information.\n3. Always Clarify baseline gets no refinement (predict once at end) while Always Predict gets M refinements—seems intentionally weak but doesn't test questions+iteration.\n4. Missing details like sampling parameters and the rationale for maximum turns."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPzXksMfXO", "forum": "MtsCJT8IL7", "replyto": "MtsCJT8IL7", "signatures": ["ICLR.cc/2026/Conference/Submission21281/Reviewer_cqwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21281/Reviewer_cqwv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936160869, "cdate": 1761936160869, "tmdate": 1762941667085, "mdate": 1762941667085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work studies how to improve LLM's performance in code design interactive environments, where they explore mixed initiative interaction patterns, where assistants have the ability to utilizemultiple strategies through the interaction. The work has reasoned that both extreme cases where the assistant keeps asking questions and providing answers are not ideal. The mix-initiative pattern has improve the performance and scale up well with the context size."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation is clear and the method is easy to understand.\n- The problem of studying interactive settings is increasingly important."}, "weaknesses": {"value": "- There are a few false statements to my knowledge, for example:\n   ```\n   Recent work (Wu et al., 2025; Laban et al., 2025) address settings that allow for mixed-initiative interactions, where conversations may be furthered by either party, but do not explicitly track the LLM’s actions or consider the subsequent cost to the user or assistant in their evaluations. \n   ```\n\n    This is not precise/true, for example, in CollabLLM,  they have considered the subsequent cost to the user as far as I understand.\n\n   Another example,\n   ```\n   While prior works (Li et al., 2024a;b) have explored similar settings where models must decide to Clarify or Predict an output attempt at each turn, these works have exclusively studied single-initiative interaction patterns, where the LM assistant always places the initiative on exclusively theuser or the assistant (ψ(a1) = ψ(a2) = . . . ) in this work we examine the benefits of mixed-initiative interaction settings \n   ```\n\n   I don't think this is true, a lots of work that study llms in multiturn settings do not explicitly define these two actions (as also listed by the author in Tab1), meaning that there's already a mixture of both or sometimes, a response can contain both the prediction and clarification in CollabLLM. This seem to view previous works in a very narrow way and the claim that the previous work doesn't track actions may not be a limitation or advatange of this work compared to them. Because we don't know if the optimal interation is coming from a discrete selection of actions or an uncontrained paradigm. I think the setting needs to be discussed more comprehensively. \n\n- Given last two points, it seems the work's novelty comes from an explicit definition of actions where LLM alternates between generating clarifying questions and attempting an output, which poses contraints towards LLM's actions, and this may not be necessary or flexible in some cases.\n\n- In Figure 1, the phenomeno seems to be conditional on the llm type as well. I agree that for small llms, especially those open-sourced ones, the models can be asking questions over and over, but i don't think this is a problem for bigger llms with slightly better context understanding and instruction following. For example, the user can just say, please answer directly and the model stops asking questions in the real world. \n\n- Web design is an interesting task, but i am wondering what prevents this work from applying to other datasets. The other tasks doesn't seem to be more complex than web design and are easier to evaluate as well.\n\n\nMinor:\n- typos:\nLine 21: \n```\nbetween generate -> between generating\n```\nline 125:\n```\nbroadens -> boarden\nallows -> allow\n```"}, "questions": {"value": "- What is the key constraint and limitation of your method by defining the explicit actions?\n- What prevents the work from applying to other multiturn datasets?\n- What is the novelty of this work compared to the existing ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kj4pNP1RV2", "forum": "MtsCJT8IL7", "replyto": "MtsCJT8IL7", "signatures": ["ICLR.cc/2026/Conference/Submission21281/Reviewer_Wufb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21281/Reviewer_Wufb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762832617367, "cdate": 1762832617367, "tmdate": 1762941666795, "mdate": 1762941666795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}