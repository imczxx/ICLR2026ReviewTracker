{"id": "rh09TrBXHD", "number": 2169, "cdate": 1757008315266, "mdate": 1759898165154, "content": {"title": "SPF-Portrait: Towards Pure Text-to-Portrait Customization with Semantic Pollution-Free Fine-Tuning", "abstract": "Fine-tuning a pre-trained text-to-image (T2I) diffusion model on a tailored portrait dataset is the mainstream method for text-to-portrait customization. However, existing methods often severely impact the original model’s behavior (e.g., changes in ID, posture, layout, etc.) while customizing portrait. To address this issue, we propose SPF-Portrait, a pioneering work to achieve pure text-to-portrait customization, which necessitates direct text-conditioned personalized portrait generation and introduces differences purely through target attributes while preserving the original model's behavior before and after portrait customization. To eliminate the interference of conventional customization on the original model, SPF-Portrait designs an additional dual-path alignment stage after the standard fine-tuning. This stage introduces the pre-trained T2I diffusion model as a reference for the fine-tuned model and achieves behavioral alignment by contrastively constraining intermediate features in diffusion models between the dual paths. To accurately align target-unrelated attributes with the original behavior without affecting the effectiveness of the target response, we propose a novel Semantic-Aware Fine Control Map, which perceives the desired response region of target semantics to adaptively guide the alignment process, preventing over-alignment of the customized portrait with the original portrait. Furthermore, to improve the fidelity of target attribute, we introduce a novel target response enhancement mechanism that utilizes our proposed representation bias as a supervisory signal to mitigate the cross-modal discrepancy in direct text-image supervision, thereby reinforcing the performance of target attributes and the overall quality of the portraits. Extensive experiments demonstrate the superiority of our method.", "tldr": "", "keywords": ["Diffusion Model", "Text-to-Image", "Concept Customization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91ae184917face2c116904f3eff6b381f111cb2d.pdf", "supplementary_material": "/attachment/7533b28befca5ee2c494ea5e6bc97b6b9da39aba.zip"}, "replies": [{"content": {"summary": {"value": "To address the issue of Semantic Pollution in existing Text-to-Portrait Customization—where irrelevant attributes such as identity, posture, and background are disrupted when fine-tuning target attributes—this work proposes the SPF-Portrait framework. This framework achieves \"pure text-driven portrait customization\". Experiments have verified the effectiveness of this method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work first defines the critical issue of Semantic Pollution in existing text-to-portrait customization—where fine-tuning for target attributes (e.g., hairstyle, freckles) disrupts irrelevant attributes like identity, posture, and background. It further proposes the new task of \"Pure Text-to-Portrait Customization\".\n2. Comprehensive ablation studies confirming component necessity.\n3. Rigorous user study validating human-centric performance.\n4. This paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Dual-path training incurs additional memory overhead. The second-stage dual-path alignment requires loading a frozen pre-trained model as the Reference Path, which increases GPU memory consumption.\n2. Lack of validation on real human face datasets. All training and evaluation in the work use synthetic portrait images rather than real human face data.\n3. No quantitative evaluation of continuous customization. While the work demonstrates qualitative results of continuous customization in Figure 7a, it lacks quantitative metrics to assess performance stability over multiple customization steps.\n4. The visual results presented in this paper are all simple close-up shots, lacking the display of results in complex scenes."}, "questions": {"value": "1. Dual-path training incurs additional memory overhead. The second-stage dual-path alignment requires loading a frozen pre-trained model as the Reference Path, which increases GPU memory consumption.\n2. Lack of validation on real human face datasets. All training and evaluation in the work use synthetic portrait images rather than real human face data.\n3. No quantitative evaluation of continuous customization. While the work demonstrates qualitative results of continuous customization in Figure 7a, it lacks quantitative metrics to assess performance stability over multiple customization steps.\n4. The visual results presented in this paper are all simple close-up shots, lacking the display of results in complex scenes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EgKa0YihYk", "forum": "rh09TrBXHD", "replyto": "rh09TrBXHD", "signatures": ["ICLR.cc/2026/Conference/Submission2169/Reviewer_4ZgM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2169/Reviewer_4ZgM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914368571, "cdate": 1761914368571, "tmdate": 1762916090837, "mdate": 1762916090837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this manuscript, the author proposes SPF-Portrait, a fine-tuning framework for text-to-portrait diffusion models that mitigates semantic pollution by introducing dual-path alignment and a semantic-aware control map.\nWith the proposed components, they achieve outstanding performance on text-to-portrait generation task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly identifies the semantic pollution problem and how to address it by their novel mechanism.\n- The proposed two-stage fine-tuning (standard + contrastive alignment) is well-motivated and well-structured.\n- Visual results analyses  support the claim that SPF-Portrait better preserves identity and structure while adding target attributes."}, "weaknesses": {"value": "I have several questions and concerns, mostly regarding the model analysis and ablation results. While these points are listed as weaknesses, some of them are also open questions.\n- Unclear behavior of SFCM and attention losses. The advantage of using a soft mask is not clearly justified. What happens if the attention map is directly replaced (e.g. attention swapping) instead of softly blended during training, similar to Prompt-to-Prompt approaches?\n- Fragility of the CLIP-based one-step loss. One-step predictions are inherently noisy. Is the CLIP-based supervision stable enough in this case, or would a specialized or fine-tuned CLIP model be required to make it reliable?\n- Dependence on x_0 diversity. The enhancement loss may overfit to a limited set of ground-truth samples. How sensitive is the performance to the diversity of x_0 in the training dataset?\n- Behavior without SFCM. It is unclear why the outputs and attention maps in the w/o SFCM setting (Fig. 4) behave as shown. Since the alignment process mainly adjusts non-target embeddings, shouldn’t the target-token–based attention preserve the target region?\n- Limited generalization evidence. The extension to broader T2I tasks (Fig. 18) lacks quantitative validation. It remains unclear whether the same mechanism would hold under more complex or multi-object prompts."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gCFZZCzGwY", "forum": "rh09TrBXHD", "replyto": "rh09TrBXHD", "signatures": ["ICLR.cc/2026/Conference/Submission2169/Reviewer_yuKJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2169/Reviewer_yuKJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981186607, "cdate": 1761981186607, "tmdate": 1762916090531, "mdate": 1762916090531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SPF-Portrait, a semantic pollution-free fine-tuning framework for pure text-driven portrait customization. The framework consists of three core components: (1) a dual-path alignment stage (added after standard fine-tuning) to preserve the pre-trained T2I model’s inherent generation capabilities; (2) a semantic-aware fine-grained control map to balance alignment of target attributes (e.g., hairstyle, clothing) and preservation of non-target attributes (e.g., background, lighting); (3) a target response enhancement mechanism to mitigate cross-modal representation bias between text and image, improving the fidelity of customized attributes. The core goal of SPF-Portrait is to achieve pure text-driven customization (no auxiliary inputs), retain the original T2I model’s behavior, and enhance target attribute fidelity. Extensive experiments (as stated in the abstract) demonstrate that the proposed method outperforms existing approaches in text-to-portrait customization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a pioneering contribution by explicitly defining the \"semantic pollution\" problem in text-to-portrait customization (distortion of non-target attributes due to fine-tuning) and proposing the first semantic pollution-free fine-tuning framework. The three core modules are not incremental improvements but innovative designs: the dual-path alignment stage introduces a novel post-fine-tuning correction mechanism; the semantic-aware control map innovatively integrates semantic region discrimination into attribute alignment; the target response enhancement mechanism addresses cross-modal bias from a representation learning perspective—collectively forming a unique technical pipeline"}, "weaknesses": {"value": "1、Lack comparison with sota t2i customization methods[1,2]\n2、Is preserving the posture of the reference task a goal during the image customization process? There is generally no restriction on this.\n3、Training time requires analysis, as this method introduces an additional model in the reference path.\n4、This method has only been validated on UNet-based text-to-image (T2I) models—are there any results for DiT-based models?\n\n[1] ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation\n[2] Multi-Concept Customization of Text-to-Image Diffusion"}, "questions": {"value": "The questions are provided in the weakness part, i will raise my score if author solves my concern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5snvgkD51G", "forum": "rh09TrBXHD", "replyto": "rh09TrBXHD", "signatures": ["ICLR.cc/2026/Conference/Submission2169/Reviewer_FvA4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2169/Reviewer_FvA4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052235019, "cdate": 1762052235019, "tmdate": 1762916089983, "mdate": 1762916089983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPF-Portrait, a fine-tuning framework that achieves pure text-to-portrait customization in diffusion models by addressing semantic pollution. \nSemantic pollution means unwanted changes to identity, pose, or layout when adapting to new attributes. By visualizing attention maps, this paper finds that semantic pollution comes from inaccurate response region location of the target semantics in standard fine-tuned diffusion models. \nTo address this issue, this paper introduces a dual-path alignment mechanism, where a frozen reference model constrains the fine-tuned model to preserve original behavior. A Semantic-Aware Fine Control Map (SFCM) guides alignment spatially to prevent overfitting unrelated regions. Additionally, a representation bias–based response enhancement improves attribute fidelity by reducing text–image supervision gaps. Experiments on SD1.5 and SD3.5 show superior performance over LoRA, AdaLoRA, and decoupling baselines in preserving identity and visual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed task is novel and the dual-path solution is well-motivated. \n\n2. Comprehensive experiments and clear empirical superiority."}, "weaknesses": {"value": "1. This paper raises the concern of semantic pollution. However, this paper does not propose any quantitative evaluation metric specifically designed to measure it, while still relies on general metrics such as FID, LPIPS, etc. \n\n\n2. The trade-off between preservation and adaptability needs more discussion. Does the proposed methods still maintain the ability to change attributes such as background or human pose when required. For example, given an image of a person in office, generate an image of the person in a given outdoor scene, and given a image of a person standing, generate an image of the person sitting on a given chair, etc."}, "questions": {"value": "1. Can the author proposes some quantitative metric to measure the semantic pollution? I am not sure if the copy-paste ability of the original image is a metric to pursue. \n\n2. Can the author presents some cases that require changes in background or pose—does the strict alignment mechanism limit flexibility in those scenarios?”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VRxRKmEPoe", "forum": "rh09TrBXHD", "replyto": "rh09TrBXHD", "signatures": ["ICLR.cc/2026/Conference/Submission2169/Reviewer_9aJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2169/Reviewer_9aJP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155303533, "cdate": 1762155303533, "tmdate": 1762916086616, "mdate": 1762916086616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}