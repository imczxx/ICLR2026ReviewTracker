{"id": "j60dWQaYbH", "number": 18395, "cdate": 1758287163060, "mdate": 1759897106051, "content": {"title": "Alignment-Aware Decoding", "abstract": "Alignment of large language models remains a central challenge in natural language processing. Preference optimization has emerged as a popular and effective method for improving alignment, typically through training-time or prompt-based interventions. In this paper, we introduce alignment-aware decoding (AAD), a method to enhance model alignment directly at inference. Theoretically, AAD can be interpreted as implicit reward optimization, yet it requires no specialized training beyond the standard DPO setup. Empirically, AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales. Moreover, in data-constrained settings, AAD can produce high-quality synthetic data to improve alignment under standard decoding, providing a practical solution when labeled data is limited.", "tldr": "", "keywords": ["alignement", "inference", "decoding", "llms"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/69a3d28755b2603a831bded873ad72eea3bdf32e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses current issues with common alignment techniques such as RLHF and DPO by proposing a new method for implicit reward optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong experimental section with multiple LLMs and a diverse set of baselines.  \n- The method consistently outperforms prior work across several datasets.  \n- Good ablation studies that help isolate the effects of different components in the pipeline.  \n- Empirical results are reproducible and sufficiently robust."}, "weaknesses": {"value": "- The method section is a bit unclear, and more importantly how the modification can fit into the overall algorithm.\n- No theoretical analysis is provided to support claims about implicit optimization or stability.  \n- The use of *Best-of-2* as a decoding strategy is limiting; larger \\(N\\) values would strengthen the conclusions, especially on the Nectar datasets, similar to what was done for Argilla and Skwy\n- Missing comparisons with alternative controllable generation strategies such as controlled decoding [1] and strong search-based methods (e.g., beam search). We understand that beam search cannot be used in conjunction with this method, but maybe as a baseline. \n\n---\n\n**References**  \n[1] Mudgal, Sidharth, et al. \"Controlled decoding from language models.\" arXiv preprint arXiv:2310.17022 (2023)."}, "questions": {"value": "**Questions for the Authors**  \n1. You state that Best-of-\\(N\\) will eventually outperform for larger \\(N\\), but suffers from increased compute. Can you quantify this trade-off?\n2. Can you explain more clearly how Equations (7)–(9) integrate into the overall optimization framework? It is not obvious how they operationalize the main objective\n\n**Suggestions for Improvement**  \n- Add clear algorithmic pseudocode.  \n- Spend more effort explaining intuition and the core method instead of re-describing RLHF and DPO (which readers already know).  \n- Improve Figure 1 explanation. The color codes are not very clear.   \n- Add theoretical discussion or at least a more formal insight into why the method should work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JVxxdSZzg2", "forum": "j60dWQaYbH", "replyto": "j60dWQaYbH", "signatures": ["ICLR.cc/2026/Conference/Submission18395/Reviewer_1wWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18395/Reviewer_1wWf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342735411, "cdate": 1761342735411, "tmdate": 1762928101631, "mdate": 1762928101631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Alignment-Aware Decoding, a simple and training-free inference method to improve the alignment of large language models. AAD operates by using the token-level log-likelihood ratio between a DPO-aligned model and its SFT reference model as an implicit reward signal to guide text generation. The core contributions are its simplicity and strong empirical results, demonstrating consistent outperformance against strong baselines like Best-of-N sampling across multiple benchmarks and model scales. This paper shows AAD can generate high-quality synthetic data to effectively enhance model alignment through iterative DPO, providing a practical solution for data-scarce scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's core strength is its simplicity. It proposes a clean, training-free method that cleverly leverages existing models from the DPO training process to enhance alignment.\n2. The experimental evidence is consistent. AAD consistently outperforms standard decoding baselines across multiple model families and benchmarks.\n3. A major contribution is its application in synthetic data generation. The paper demonstrates that data generated by AAD can nearly close the performance gap in low-resource scenarios, which is a highly practical and valuable finding."}, "weaknesses": {"value": "1. The paper's primary weakness is its limited novelty. The core mechanism of AAD is using the log-probability ratio between an 'expert' model and an 'amateur' reference model to guide generation, which is conceptually almost identical to Contrastive Decoding (VCD) and its variants.\n2. The paper fails to include any direct experimental comparison against VCD or its variants. Without this crucial baseline, it is impossible to assess whether AAD offers any tangible benefits over existing techniques.\n3. The method requires two forward passes, doubling the inference cost, which is prohibitive for latency-sensitive applications."}, "questions": {"value": "1. Is the specific SFT model strictly necessary as the 'amateur' model, or could it be replaced with a generic, weaker model or a noise-corrupted model, similar to VCD? This is crucial for understanding AAD's novelty.\n2. Have you compared AAD's performance against a standard VCD setup using a strong model against a weak model? This is essential to isolate whether the gain comes from DPO's alignment signal or just the contrast between any strong and weak model.\n3. The 2x inference cost is a major limitation. Have you explored any mitigation strategies that don't require two forward passes at runtime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QpifOrxQC3", "forum": "j60dWQaYbH", "replyto": "j60dWQaYbH", "signatures": ["ICLR.cc/2026/Conference/Submission18395/Reviewer_XYjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18395/Reviewer_XYjx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707057485, "cdate": 1761707057485, "tmdate": 1762928101025, "mdate": 1762928101025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Alignment-Aware Decoding (AAD), a method to enhance the alignment of models already trained with Direct Preference Optimization (DPO). AAD operates at inference time by using the token-level probability ratio between the DPO model and its Supervised Fine-Tuning (SFT) base model as an implicit reward to guide the decoding process. Experimental results demonstrate that AAD significantly outperforms baselines such as greedy decoding, best-of-N (BoN), and Emulated Fine-Tuning (EFT) across several alignment benchmarks. The paper also shows that synthetic data generated by AAD can be used for iterative DPO training, creating a positive feedback loop to further improve alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "AAD is an inference-time technique. Its computational overhead is approximately twice that of greedy decoding, yet it demonstrates performance that surpasses other methods with similar computational costs.\n\nThe method proves effective even when the DPO model is trained on only 10% of the available preference data, highlighting its utility in low-data regimes. Moreover, synthetic samples generated by AAD can be fed back into the DPO process, to further boost model alignment."}, "weaknesses": {"value": "1, What is the effect of plausible token ranges determined by min-$\\alpha$ filtering? Ablation studies of $\\alpha$ are needed.\n\n2, The main results (e.g., in Table 1) appear to be based on DPO models trained with only 10% of the training data . It is unclear whether AAD provides a similar performance boost over Greedy DPO and other baselines when all available training data is used for the DPO training.\n\n3, While being backed by reasons, the core idea of contrasting the DPO model with the SFT model at a token level bears a strong resemblance to contrastive decoding . This similarity may diminish the perceived technical novelty of the work."}, "questions": {"value": "1. The method relies on a \"properly fitted\" DPO model. In practice, how can one determine the optimal point to stop DPO training before applying AAD or training on AAD-generated data? Furthermore, what is the relationship between the quality of the DPO model (e.g., trained on more data or for more steps) and the relative gains from AAD? Does AAD provide diminishing returns as the base DPO model becomes stronger?\n2. In Figure 2, the performance gap between BoN-Oracle and BoN-Picker widens much more significantly on the Skywork dataset than on the Argilla dataset as N increases. Could the authors provide more dataset-specific analysis or insights to explain this? \n3. Presentation issue: The last sentence on Page2 appears to be unfinished."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SevLm774PL", "forum": "j60dWQaYbH", "replyto": "j60dWQaYbH", "signatures": ["ICLR.cc/2026/Conference/Submission18395/Reviewer_gJtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18395/Reviewer_gJtS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981199322, "cdate": 1761981199322, "tmdate": 1762928100438, "mdate": 1762928100438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Alignment-Aware Decoding (AAD), a novel inference-time method designed to improve the alignment of large language models without additional training. The core idea is to exploit the log-likelihood ratio between a DPO-trained model and its reference SFT model  as a token-level implicit reward, guiding decoding decisions toward tokens that DPO training implicitly upweighted. By decoding only within a constrained “feasible token subset,” AAD mitigates instability and mode collapse while effectively amplifying alignment signals during inference. The method is theoretically motivated by analyzing the analytical optimum of DPO and empirically validated on multiple preference datasets (Ultrachat, Argilla, HHRLHF, Nectar, Skywork). Results show consistent gains in win rate and average reward over greedy, Best-of-N, and emulated fine-tuning baselines, particularly in low-data or high-β (strong KL constraint) regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper convincingly identifies reference bias as a fundamental weakness of DPO training and derives AAD directly from the analytical form of the DPO optimum. This theoretical clarity distinguishes it from prior heuristic inference-time alignment methods.\n\n2.AAD requires no retraining, no reward model, and minimal computation overhead—only two forward passes (DPO and SFT). The implementation is straightforward yet conceptually powerful, making it easy to adopt in existing alignment pipelines.\n\n3.The empirical study is broad, covering diverse datasets and model scales. AAD consistently improves both reward and human-aligned metrics, outperforming stronger baselines under equal compute."}, "weaknesses": {"value": "1.AAD requires both a DPO-aligned model and its original SFT counterpart. This dependency limits applicability to settings where both checkpoints are available, which might not hold for commercial or closed-source models.\n\n2.Each decoding step requires two model forward passes, doubling inference time. While computationally modest compared to full fine-tuning, a quantitative analysis of runtime overhead would improve clarity.\n\n3.Most results rely on internal or proxy reward models as alignment judges. Additional human evaluation or LLM-as-judge should also be included.\n\n4.The paper could include more diagnostic studies, e.g., where AAD overemphasizes high-ratio tokens that hurt coherence or factual accuracy, to better characterize its limits."}, "questions": {"value": "1.Could AAD be extended to other alignment objectives (e.g., SimPO, ORPO, PPO-trained models), or is it specific to the DPO formulation?\n\n2.Could AAD interact beneficially with diversity-promoting sampling (e.g., nucleus or contrastive sampling), or would that dilute the alignment effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vffmIphkoE", "forum": "j60dWQaYbH", "replyto": "j60dWQaYbH", "signatures": ["ICLR.cc/2026/Conference/Submission18395/Reviewer_dygm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18395/Reviewer_dygm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988199274, "cdate": 1761988199274, "tmdate": 1762928099061, "mdate": 1762928099061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}