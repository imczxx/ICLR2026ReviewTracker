{"id": "btEiAfnLsX", "number": 24413, "cdate": 1758356581918, "mdate": 1759896767737, "content": {"title": "Why DPO is a Misspecified Estimator and How to Fix It", "abstract": "Direct alignment algorithms such as Direct Preference Optimization (DPO) fine-tune models based on preference data, using only supervised learning instead of two-stage reinforcement learning with human feedback (RLHF). We show that DPO encodes a statistical estimation problem over reward functions induced by a parametric policy class. When the true reward function that generates preferences cannot be realized via the policy class, DPO becomes misspecified, resulting in failure modes such as preference order reversal, worsening of policy reward, and high sensitivity to the input preference data distribution. On the other hand, we study the local behavior of two-stage RLHF for a parametric class and relate it to a natural gradient step in policy space.  Our fine-grained geometric characterization allows us to propose AuxDPO, which introduces additional auxiliary variables in the DPO loss function to help move towards the RLHF solution in a principled manner and mitigate the misspecification in DPO. We empirically demonstrate the superior performance of AuxDPO on didactic bandit settings as well as LLM alignment tasks.", "tldr": "DPO is not sound by design and can fail due to misspecification, we fix it with careful analysis.", "keywords": ["Direct Preference Optimization", "Reinforcement Learning", "Reinforcement learning with human feedback"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31d94bfb30e12513f9f294c0d5f428e37ab6e5ae.pdf", "supplementary_material": "/attachment/332836dbf38fac1a721fdf24aa5ef52dce058134.pdf"}, "replies": [{"content": {"summary": {"value": "The paper studies the misspecification of the parametric reward model in the DPO framework. \nThe standard DPO framework assumes that the LLM is expressive enough to be able to fit the ideal solution characterized by RLHF.\nThe authors raise critical questions that the LLM models can be misspecified with a concrete example (Section 3.1). This example shows that such misspecification may lead to undesirable behavior. \nBased on the local analysis of the geometry, they propose to add additional optimization variables which allow enough slackness to the model to fit to the desired RLHF solution. \nThe experiment results show clear advantage of the proposed method AuxDPO."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes an interesting idea to further improve the quality of DPO, based on principled local information geometry.\nThe proposed technique is not only principled but also effective in practice."}, "weaknesses": {"value": "The only downside of the paper is that it is very notation heavy and not easy to follow in the first read."}, "questions": {"value": "- I cannot understand the implication of Remark 4. Do the authors want to argue that Song et al. (2024)'s argument is wrong? If the authors can end up with a different conclusion, what is the difference? I believe that the difference comes from the misspecification, but I appreciate if the authors can clarify.\n- I have a question about the notation in Section 4. The authors use $\\mathcal{N}(A)\\subset\\mathbb{R}^m$ to denote the nullspace of $A$. Given this, I do not know how to parse $\\\\|\\\\mathcal{N}(A)\\\\delta\\\\|^2$, and how it can manifest as in the final objective function. I think this paragraph deserves a further expansion.\n\n#### **Suggestions**\n- The notation for dataset is a bit weird. For example, in line 138, consider $\\mathcal{D}=\\\\{(\\text{tuple}\\_i)\\\\}_{i=1}^n$.\n- The shorthand b/w and w.r.t. read weird. Please expand them in the final version.\n\n---\n\nOverall, I find the core idea compelling and the paper a solid contribution. However, a thorough revision to streamline and simplify the notation would greatly improve the manuscriptâ€™s readability and its accessibility to a broader audience."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Zy2BjeU5vc", "forum": "btEiAfnLsX", "replyto": "btEiAfnLsX", "signatures": ["ICLR.cc/2026/Conference/Submission24413/Reviewer_egJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24413/Reviewer_egJi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922077175, "cdate": 1761922077175, "tmdate": 1762943074450, "mdate": 1762943074450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first demonstrates that DPO implicitly projects the weights of the optimal solution to the manifold of possible reward functions under the given policy class. The authors prove that this can lead to mis-specifications where the reward function does not actually obey preferences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* the paper is mathematically rigorous in demonstrating its claims. \n* the work does a good job demonstrating why the mis-specification is a problem through a useful example and follow-up points.\n* the results demonstrate strong performance in both in distribution and out of distribution settings."}, "weaknesses": {"value": "* the derivation of the aux DPO objective makes sense, but is justified using a local approximation. While this makes sense, it could be worth pointing out the the AuxDPO solution (at least to my understanding) holds under these approximations only. \n* The paper is a bit hard to follow at times as it is particularly dense. I think at various points in the manuscript having more motivation and explanation would be helpful. Why do we want to do a first order approx? What is the meaning of the A matrix? \n* In a similar manner, the actual instantiation of AuxDPO is a bit unclear. AuxDPO adds variables in the null space of the A matrix, but how are these variables actually represented in code? Is $\\delta$ just predicted as another head of the LLM? \n* I understand why the authors used MMLU etc. as a preference learning benchmark, but it (in spirit) does not seem exactly like one since its more of a QA dataset. While DPO can be used in this case I think more benchmarks actually based on human preferences woudl be preferred."}, "questions": {"value": "See weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qGwqLv7KZm", "forum": "btEiAfnLsX", "replyto": "btEiAfnLsX", "signatures": ["ICLR.cc/2026/Conference/Submission24413/Reviewer_7Hcd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24413/Reviewer_7Hcd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945908651, "cdate": 1761945908651, "tmdate": 1762943074142, "mdate": 1762943074142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a fundamental limitation of Direct Preference Optimization (DPO), a widely used direct preference alignment method. It shows that when using parametric policy classes (as opposed to the tabular assumption underlying DPO's derivation), DPO suffers from a misspecified statistical estimation problem. This misspecification arises when the true reward function that generates preferences cannot be represented by the chosen policy class, leading to undesirable outcomes. To address this, the authors propose AuxDPO, which augments the DPO loss with auxiliary variables to mitigate misspecification. They demonstrate the empirical effectiveness of AuxDPO on LLM preference alignment tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and clearly structured.\n\nProvides an insightful theoretical analysis of DPO and RLHF via Taylor approximation, revealing:\n- the local geometry of DPO under parametric policies,\n- the local geometry of RLHF optimization, and  \n- the relationship between RLHF equivalence classes and DPO linearization.\n\nProposes a novel and principled solution (AuxDPO) to address the identified misspecification issue."}, "weaknesses": {"value": "Could oversampling or undersampling preference pairs to balance frequencies before DPO training mitigate the misspecification issue and yield comparable performance?\n\nThere is no discussion or experimental analysis of AuxDPO's sensitivity to its core hyperparameters ($\\lambda$ and $n$). How should these values be chosen in practice?"}, "questions": {"value": "(Related to the constructive example in Proposition 3): If preference data are balanced (uniform) across $(s, a, a')$, would DPO still exhibit the preference reversal issue shown in the example?\n\nFor the datasets used in the experiments, were the preference frequencies balanced or imbalanced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g1If48lwYQ", "forum": "btEiAfnLsX", "replyto": "btEiAfnLsX", "signatures": ["ICLR.cc/2026/Conference/Submission24413/Reviewer_zAc8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24413/Reviewer_zAc8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24413/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986537389, "cdate": 1761986537389, "tmdate": 1762943073921, "mdate": 1762943073921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}