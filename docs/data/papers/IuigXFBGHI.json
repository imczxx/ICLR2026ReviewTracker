{"id": "IuigXFBGHI", "number": 1136, "cdate": 1756848067449, "mdate": 1759898225632, "content": {"title": "A Mechanistic Analysis of Low-Precision Instabilities in Microscaling Formats", "abstract": "Training large language models is expensive and compute-bound, and it must be repeated as models scale, algorithms improve, and new data is collected. To address this, next-generation hardware accelerators like NVIDIA’s Blackwell increasingly support lower-precision arithmetic formats, including Microscaling (MX) formats. In this work, we investigate the challenges and viability of block-scaled precision formats during model training. Across a broad sweep of weight-activation precision combinations and compute budgets from \\( 2 \\times 10^{17} \\) to \\( 4.8 \\times 10^{19} \\) FLOPs, we generally observe that training in MX formats exhibits sharp, stochastic instabilities in the loss, particularly at larger compute scales. To explain this phenomenon, we conduct controlled experiments and ablations on a smaller proxy model that exhibits instability behavior similar to the language model, sweeping across architectural settings, hyperparameters, and precision formats. These experiments motivate a simple model in which multiplicative gradient bias introduced by the quantization of layer-norm affine parameters and a small fraction of activations can trigger runaway divergence. Through \\textit{in situ} intervention experiments on our proxy model, we demonstrate that instabilities can be averted or delayed by modifying precision schemes mid-training.  Guided by these findings, we evaluate stabilization strategies in the LLM setting and show that certain hybrid configurations recover performance competitive with full-precision training.", "tldr": "We identify the mechanisms of precision-driven instabilities in LLM training and potential mitigation strategies.", "keywords": ["large language models", "precision", "quantization", "microscaling", "scaling law", "pretraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/137a512abc9727c83016fdf1bafcb229a206b08b.pdf", "supplementary_material": "/attachment/7357068a3747c13106408666e62752bebabe5bed.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the training dynamics of models trained with microscaling (MX) quantization in the forward and backward passes. MX formats (and other similar block scaled formats such as NVFP4) are become widely used for low precision training, making this a practical area to study. The paper studies this problem by training many models and considering a toy example with a student/teacher model setup. The paper finds that the main reason for loss spikes and divergence training is due to overflow in layernorm parameters during MX quantization, which can be mitigated by modifying the MX quantization algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors trained a large number of models to show that when applied \"incorrectly,\" MX datatypes can cause training instability. \n- The minimal synthetic model is very interesting, as it is able to mostly model the effect of different norms and activation functions on training instabilities."}, "weaknesses": {"value": "- The main takeaway of this paper seems to not quantize layernorms, but people generally do not quantize non-decoder layer linear layers already. Can the authors explain why you would want to quantize things beyond linear layers? The point of using hardware supported datatypes for quantization is to accelerate compute bound matrix multiplications, and these operations are almost entirely contained in projection layers and self attention, neither of which appears to be source of training instabilities according to this paper.\n- Since the main remedy appears to be avoiding overflow during rounding, I would have liked to see more experiments on different rounding methods that solve this. Increasing the exponent is just one way - https://aisystemcodesign.github.io/papers/FP4.pdf discusses more methods that get around this."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ueQ7nGnpkw", "forum": "IuigXFBGHI", "replyto": "IuigXFBGHI", "signatures": ["ICLR.cc/2026/Conference/Submission1136/Reviewer_qRPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1136/Reviewer_qRPj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457111096, "cdate": 1761457111096, "tmdate": 1762915687790, "mdate": 1762915687790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts a systematic study on the stability of MXFP8 format for training LLMs. Through extensive experiments on a proxy model, they find the instabilities to be the result of gradient bias due to quantization. This bias stems mainly from layer-norm weight distribution and sometimes other activations. Bridging to the LLM case, they suggest keeping the layer norm (and activations) in bfloat16, and only quantizing the forward pass."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is generally well-written\n- Systematic approach"}, "weaknesses": {"value": "- While the study is interesting, the findings are not significant: in quantized/quantization-aware training, layer norms are generally skipped as they don't have significant memory or computation overhead (if implemented correctly).\n- Some assumptions/conclusions are loose (see Questions below)"}, "questions": {"value": "1. The fact that layer norm should be kept in high precision is not new, e.g., [1] shows even FP16 layer-norm is difficult (even before that for batch norm [2]). Additionally, to my knowledge, practical papers in quantization-aware training such as QuEST [3] already know not to touch the layer-norm layers. Even standard implementations of normalization layers make sure to operate in FP32, such as [4]. So my question is that what is the novelty in this paper's findings?\n\n2. It's not clear to me why such proxy model is \"good.\" In the authors' words: \"we caution that stability in this minimal model as a necessary (though perhaps not sufficient) condition for stability in the full LM.\" Why is this a necessary condition? Note that this means if LLMs fail in a certain setting, then this proxy model will also fail.\n\n3. Can you verify that you have FP32 master weights? Maybe I missed it, but I didn't find it explicitly in the paper.\n\n4. Since the finding is that instability is due to gradient bias, how would the results look like if stochastic rounding is employed? Theoretically, quantization errors due to stochastic rounding should be unbiased. Although I understand it does not affect overflow.\n\n5. Can you elaborate on why there is no layer-norm is the teacher?\n\n6. How would your studies interact with Hadamard transformations [3], which is now common practice in QAT?\n\nIn the current state, despite the systematic investigation, I don't believe this paper offers any significant contribution.\n\n[1] https://arxiv.org/pdf/2410.10553\n[2] https://arxiv.org/pdf/1710.03740\n[3] https://arxiv.org/pdf/2502.05003\n[4] https://github.com/huggingface/transformers/blob/4d0b6758b90aa54e4077171e6d42c55e0c01c622/src/transformers/models/llama/modeling_llama.py#L64"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "67GWASaTvY", "forum": "IuigXFBGHI", "replyto": "IuigXFBGHI", "signatures": ["ICLR.cc/2026/Conference/Submission1136/Reviewer_oB7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1136/Reviewer_oB7k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671252905, "cdate": 1761671252905, "tmdate": 1762915687642, "mdate": 1762915687642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanistic origin of training instabilities observed when applying Microscaling (MX) low-precision formats (FP6/FP8) to LLM pretraining. The authors combine OLMo-based LLM experiments with controlled synthetic proxy models to demonstrate that the observed failures are not attributable to activation choices or hyperparameter settings, but instead arise from systematic quantization effects on LayerNorm affine parameters.\n\nDuring training, LayerNorm affine parameters become highly clustered, and under MX block-scaled quantization, these values saturate into the same maximum bin due to shared scaling constraints. This saturation introduces persistent gradient bias, which accumulates across updates and ultimately leads to irreversible divergence in training loss. The paper provides empirical evidence for this failure mode, demonstrating both its emergence at LLM scale and its reproducibility within a simplified synthetic setting.\n\nBuilding on this analysis, the authors propose practical stabilization strategies, including disabling LayerNorm affine quantization, keeping activations in bfloat16, and applying quantization only in the forward pass. Among these approaches, the configuration using MXFP8 (E4M3) weights and BF16 activations achieves validation performance comparable to full BF16 while avoiding instability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper offers a clear mechanistic analysis of instability in MX-based low-precision training.\n* It identifies a coherent failure chain in which LayerNorm affine parameters saturate under block-scale quantization, introducing gradient bias that ultimately triggers training divergence.\n* The authors combine OLMo-based LLM experiments with a controlled synthetic proxy model, enabling both realistic evaluation and reproducible causal analysis.\n* In-situ intervention experiments further provide empirical evidence that LayerNorm affine quantization is a primary causal factor behind the observed failures.\n* The paper proposes practical and easily deployable mitigation strategies—such as disabling LN-affine quantization, using BF16 activations, or restricting quantization to the forward pass.\n* These techniques restore stable training while matching the validation performance of full-BF16 systems, underscoring their practical utility."}, "weaknesses": {"value": "* Approaches such as disabling LN-affine quantization or increasing activation precision offer limited throughput benefits compared to standard BF16 training.\n* Most empirical validation is conducted on OLMo-scale models, leaving generalization to frontier-scale systems uncertain.\n* Although the paper reports activation overflow phenomena, its relative contribution is not quantitatively evaluated against the LayerNorm-induced mechanism, leaving the importance of this additional pathway insufficiently characterized.\n* The paper identifies the core mechanism behind instability, but does not provide a systematic exploration of the conditions under which instability emerges—e.g., across compute scale, dataset size, or architectural variations—making it difficult to determine when MX training is expected to fail.\n* Comparisons with other low-precision formats (e.g., per-channel scaling, PQ) are limited."}, "questions": {"value": "* Can the relative contributions of activation overflow and LN-affine saturation to the observed instability be quantitatively compared?\n* Has this failure mechanism been evaluated in other architectures, such as MoE, MQA, or non-Transformer blocks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BtKuHUU162", "forum": "IuigXFBGHI", "replyto": "IuigXFBGHI", "signatures": ["ICLR.cc/2026/Conference/Submission1136/Reviewer_4KaL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1136/Reviewer_4KaL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939955718, "cdate": 1761939955718, "tmdate": 1762915687491, "mdate": 1762915687491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}