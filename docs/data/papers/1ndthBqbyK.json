{"id": "1ndthBqbyK", "number": 25217, "cdate": 1758365426187, "mdate": 1759896729675, "content": {"title": "TSDINO: Teacher–Student Self-Distillation Framework for Robust Pre-training of Time-Series Foundation Models", "abstract": "Building time-series foundation models (TSFM) poses challenges in terms of learning stability due to limited data availability and heterogeneous temporal dynamics across various time-series datasets. We propose TSDINO, a teacher-student framework for robust pre-training of TSFM based on the principle of self-distillation with no labels. TSDINO offers a model-agnostic approach that combines two complementary objectives: (i) feature preservation under augmentations and (ii) masked patch prediction. A meta-architecture comprising teacher-student networks and projection heads enables adaptation to various models. We evaluate TSDINO on classification and forecasting tasks using diverse publicly available benchmarking datasets. TSDINO consistently achieves competitive zero-shot performance over gradient-based pre-training.", "tldr": "", "keywords": ["time series", "self-distillation", "time-series foundation models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43b325484b4b3f4b77f48594f8c1a9cdfc8f6fef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper the authors propose a distillation based pretraining paradigm for time series foundation models, in particular TSFM encoders. They suggest that the teacher uses the exponential average of students' weights, while the students match the teacher's hidden representations on both the patch and the sequence level using obfuscated inputs. In the empirical study section the authors conclude that such training paradigm yields a better pretrained encoder compared to standard SGD when using TSPulse as the backbone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They paper introduces a novel and legit research question regarding the best training paradigm for TSFM. The proposed method is theoretically sound."}, "weaknesses": {"value": "The conclusions made in the paper are not well supported. There are questions in both the theoretical and the empirical perspectives that need to be addressed to make the conclusions credible. See questions.\n\nThe writing of the paper is a bit ambiguous and is lacking key details."}, "questions": {"value": "Regarding theory:\n\n1. To be nitpicky, with the focus actually on the teacher model, the proposed paradigm is more aligned with meta learning than (self?)-distillation, that the whole teacher student interaction seems equivalent to an adaptive learning rate scheduler with momentum. Can you provide some theoretical justification (high level is fine) on why the proposed method is superior over modern SGD?\n\n2. Can yoy justify the assumption that the chosen augmentations for the student's inputs should not affect the student's hidden representation of such inputs?\n\n3. Minor: how is the ground truth label factored in the students' loss?\n\nRegarding the empirical study:\n\n1. Can you provide the training dynamics of TSDINO TSPulse and Gradient TSPulse, or alternatively a notion of \"being equally trained\" between the two models.  For example suppose both approaches are trained with the same steps then the distillation approach uses more FLOPS. An ideal evidence here is that both model trainings have converged. Without such evidence it is unclear whether the edge of TSDINO TSPulse comes from a better training paradigm or from more training.\n\n\n2. If the claim is that paradigm is model class agnostic, then another backbone model class should be involved to at least attempt to test the generalizability of such claim.\n\n3. Figure 4 is lacking evidence on statistical significance. \n\n4. Minor: for zero shot forecasting, the mentioned 7 datasets are limiting and overused. The practice of using a window of 96 to forecast at most 720 points is not sound. Consider a slightly more diversified and comprehensive benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MEIv2JtqVE", "forum": "1ndthBqbyK", "replyto": "1ndthBqbyK", "signatures": ["ICLR.cc/2026/Conference/Submission25217/Reviewer_aesc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25217/Reviewer_aesc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761544817266, "cdate": 1761544817266, "tmdate": 1762943367485, "mdate": 1762943367485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a teacher-student self-distillation framework (TSDINO) for TSFM pre-training. TSDINO is an architecture-independent approach, meaning it can use different neural network architectures. Its training objective consists of feature preservation under augmentation plus masked patch prediction. The pre-trained model is suitable for prediction and classification tasks. The authors evaluated TSDINO on several publicly available benchmark datasets, and the experimental results validate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is written in a clear and easy-to-understand manner."}, "weaknesses": {"value": "- The paper lacks originality. It directly uses the DINO method from the image domain to train the time series foundation model, without providing an analysis of the method's applicability to time series scenarios and evidence to explain why the method is effective.\n- The paper's experiments are relatively weak, lacking validation on large-scale benchmarks."}, "questions": {"value": "- A deep analysis to know why the DINO training strategy can work on time series data would be interesting. \n- In the original DINO paper, specialized neural network architectures (such as VIT and ResNet) were required for representation extraction. Why can the authors claim here that TSDINO does not depend on the form of the network architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S0uF1vCPRO", "forum": "1ndthBqbyK", "replyto": "1ndthBqbyK", "signatures": ["ICLR.cc/2026/Conference/Submission25217/Reviewer_umYb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25217/Reviewer_umYb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719250182, "cdate": 1761719250182, "tmdate": 1762943367187, "mdate": 1762943367187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a DINO-like pre-training for TFMs based on a teacher-student distillation setup. The authors argue that such pre-training methods were not explored in the TSFM field before and suggest that they can work equally well in time series. The authors adapt DINO-based pre-training to time series and produce two models that follow such pre-training: one that distills TTM models for forecasting and another that distills the TSPulse model for discriminative tasks. Large-scale evaluations hint at the potential efficiency of this approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tA new pre-training approach adapted to time series from DINO pre-training.\n2.\tVariants of the model for discriminative and generative tasks.\n3.\tPromising performance in the considered tasks."}, "weaknesses": {"value": "1.\tAlthough distillation is not that explored in the TSFM field, different TSFMs have already explored other ways to pre-train models. Flow-based (FlowState) models based on SSM, diffusion-based models (Sundial), masked-based (MOMENT), etc. In classification, MANTIS-8M was pre-trained using contrastive learning, too, just as the self-supervised variant of UniTS. It is thus a bit of a stretch to claim that different pre-training strategies were not explored in TSFMs. \n2.\tTTMs are very far from being competitive in time series forecasting. It will be more instructive to distill into stronger models and compare with them (TimesFM 2.5 or TiRex). \n3.\tSame as above, Mantis-8M (Feofanov et al.) performs stronger in zero-shot than other FMs and was pre-trained contrastively. While I understand the argument that this is a proof-of-concept, I guess it still needs to be comparable to the best models for people to explore this idea further, not just be comparable to those that are far from the top of the leaderboard."}, "questions": {"value": "1.\tIs it possible to provide more evidence of the suitability of this approach for stronger forecasting/classification models? If such evidence is provided, I will increase my score to 8.\n2.\tTime series augmentations were explored in time series forecasting before (see TiRex paper), as well as patch masking. What other novelties do authors consider in this work that are specific to time series and/or novel?\n3.\tIs there any evidence of the pitfalls of the common pre-training approaches that DINO-like pre-training can solve in time series? It would be instructive to have motivational examples providing more intuition for why we expect the improvement with this kind of pre-training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Rifx9TLft", "forum": "1ndthBqbyK", "replyto": "1ndthBqbyK", "signatures": ["ICLR.cc/2026/Conference/Submission25217/Reviewer_WNzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25217/Reviewer_WNzk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927236613, "cdate": 1761927236613, "tmdate": 1762943366827, "mdate": 1762943366827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a self-supervised pre-training framework for Time-Series Foundation Models (TSFMs). The authors argue that existing TSFMs mostly focus on architectural innovations, rather than the training strategies. Therefore, TSDINO proposes to extend the teacher-student self-distillation paradigm originally popularized by DINO in computer vision to the time-series domain.\n\nIn TSDINO, there is a student network trained to match the outputs of a teacher network, which is maintained as an Exponential Moving Average (EMA) of the student. The training objective has two components: 1) sequence-level alignment, which aims to align the global representation of the selected time series window of the teacher and the student, and 2)  masked patch prediction, which aims to capture local temporal structures by predicting the representations of masked patches. The authors argue that a key design choice here is the asymmetric input strategy. The student processes augmented and masked data, while the teacher processes the original time series, as the authors attribute this to time series being more sensitive to noise.\n\nTSDINO is designed to be model-agnostic. The evaluation involves integrating existing TSFM architectures as backbones for different tasks and comparing TSDINO against the original pre-training strategies of these models. The results show consistent improvements in zero-shot forecasting and zero-shot classification (retrieval), along with improved embedding quality as measured by clustering metrics."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The adaptation of the DINO framework to time series seems interesting.\n2. The authors show good performance when compared to the limited baselines."}, "weaknesses": {"value": "1. There are many writing and notation issues. \n- Line 146: \"The model fθ is a forecaster that given xi with time steps 1 : L, predict future values zi = ˆxi with time steps L + 1 : d without task-specific fine-tuning.\" --> Do you mean to write d here?\n- Line 229: \"Therefre, at inference, we use the teacher network fθT for prediction\" --> \"Therefore\"\n- Line 245: \"Forecasting loss for the foerecasting taks is MAE\" --> But then you showed the formula for MSE?\n- In Equation 8, \"\\sum_{p=1\\in P}\" --> This is not proper notation.\n- In Line 254, the projection and reconstruction heads are defined using parameters \\eta and \\xi (e.g., h_{\\eta_{S}}). However, in Section 4.3, they are referred to using \\theta (e.g., h_{\\theta_{T}}). Notation must be consistent.\n- What is the dimensionality of a_T, a_S in the sequence embeddings?\n- Line 215: \"forecasiting\" -> \"forecasting\".\n- Line 190: \"where A is time-series augmentation and A perform patch-wise value-fill with the mask indicatorI (further details are in §4.4).\" --> The second A should be M?\n2. Can you add more comparisons against other established self-supervised learning frameworks for time series (e.g., TS2Vec, contrastive methods) using the same backbone?\n3. Can you present an ablation study on whether the asymmetric augmentation strategy is necessary?\n4. No hyperparameters are given (e.g., dimensions of the heads, the choice of alpha when balancing the two losses, masking ratio, noise levels, etc.).\n5. The proposed framework seems to be very similar to many of the existing time series pretraining papers that use contrastive learning."}, "questions": {"value": "1. Do TTM (Ekambaram et al., 2024) and TSPulse (Ekambaram et al., 2025) also use a subset of 1B time points drawn from the combined dataset of the Monash (Godahewa et al., 2021) and LibCity (Wang et al., 2021; Woo et al., 2024) data collections? I am confused by your writing in Section 5, Pre-training Setup and Training Dataset. What are the original training objectives for TTM and TSPulse?\n2. Can you describe the experiment settings in Section 5.1.1 in more details?\n3. Can you redraw Figure 1? What does the word \"Centering\" mean in this figure? In the last column, the visible patch for teacher should be b_T^{(p)}, while the reconstructed patch for student should be b_S^{(p)}? The figure currently looks very messy. I suggest to redesign it.\n4. In Table 2, TSDINO shows clear gains in zero-shot tasks but is identical to the baseline when finetuned. Why do the representation improvements not translate to the fine-tuning setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fjNp9ECigI", "forum": "1ndthBqbyK", "replyto": "1ndthBqbyK", "signatures": ["ICLR.cc/2026/Conference/Submission25217/Reviewer_dBGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25217/Reviewer_dBGd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25217/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002385288, "cdate": 1762002385288, "tmdate": 1762943366469, "mdate": 1762943366469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}