{"id": "s9cAknJjRB", "number": 13637, "cdate": 1758220233333, "mdate": 1759897423288, "content": {"title": "MindX: A Low-Cost Benchmark System for Real-Time Brain-Controlled Gaming", "abstract": "We introduce MindX, an open benchmark platform for real-time EEG-based game control using low-cost, consumer-grade hardware. The system features three directional-control games—Hide-and-Seek, Rhythm Game, and Snake Game—designed to elicit motor imagery and attention-related neural signals. A lightweight CNN+LDA model processes 4-channel EEG from a Muse S headset and issues directional predictions (left and right) every 300\\~ms, with end-to-end latency under 350\\~ms. A user-centered co-adaptive loop enables lightweight personalization based on gameplay feedback. In a pilot study with three users, MindX achieved 76\\% accuracy, well above the 50\\% baseline. The framework provides a reproducible and extensible testbed for evaluating real-time EEG decoding pipelines.", "tldr": "", "keywords": ["Gaming", "BCI", "Brain-Computer Interfaces", "EEG", "CNN", "Transformer"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15f0b63232dc3e63657e4b8c6d9a63b3931d8ed6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MindX, a low-cost, real-time EEG gaming “benchmark” using a Muse S headset with three left/right control games and a lightweight ConvNet feature extractor with an LDA classifier. It reports directional accuracy with three human subjects and and offers a personalization loop."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. A real-time BCI system with measured latency on consumer hardware and a simple architecture."}, "weaknesses": {"value": "1. There is no research question and hypothesis in the paper. The paper reads like an applied project report instead of a scientific contribution. The authors train a small ConvNet LDA model to classify EEG signals with no additional conclusion.\n\n\n2. The evaluation protocol is flawed and there is no mention of different training and test trials. No statistical testing or confidence intervals are provided. Ablations are limited and no insight is provided.\n\n3. A few baselines based on old and conventional methods (SVM, Random Forest etc) have been used for comparison and all recent architectures in the literature of EEG decoding are ignored. Details of baselines are not provided.\n\n4. The gap and contribution in the end of section 2 is not supported by the paper. No code is provided and reproducibility details are not enough.\n\n5. The quality of the figures is poor and the paper is verbose."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OpAClSMePW", "forum": "s9cAknJjRB", "replyto": "s9cAknJjRB", "signatures": ["ICLR.cc/2026/Conference/Submission13637/Reviewer_9wgw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13637/Reviewer_9wgw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798271319, "cdate": 1761798271319, "tmdate": 1762924216448, "mdate": 1762924216448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a low-cost, consumer-grade EEG benchmark for real-time game control built around a 4-channel headset, three game paradigms, and a lightweight CNN→linear classifier pipeline with a co-adaptive/personalization loop. The goal is to standardize end-to-end evaluation for online BCI with accessible hardware and to release code/games for community use."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing consumer-grade online BCI as a benchmarkable, end-to-end problem is timely and useful.\n\n2. Emphasis on low latency, simple models, and accessible hardware lowers the barrier for education/labs.\n\n3. Clear system decomposition (acquisition, inference, control, games) that could, in principle, enable plug-and-play baselines and community extensions.\n\n4. If well-documented and released, this could catalyze more realistic online evaluations rather than offline dataset exercises."}, "weaknesses": {"value": "1. Only four (dry) electrodes substantially limit SNR and spatial coverage; this undermines many intended applications and claims.\n\n2. With such sparse, noisy channels, strong interpretability is not credible; what can be interpreted is likely limited to coarse spectral trends, not robust source-level insights.\n\n3. No dispersion/variance analysis; without per-subject metrics the “benchmark” validity is unclear.\n\n4. No code/games/API/full demo at review time makes it difficult to estimate the validity of a benchmark itself. Without code, games, and API/docs, integration risks (e.g., adding new classifiers, latency hooks) are unknown.\n\n5. Very small study; limited support for generalization (cross-session/subject/game) and robustness (re-donning, fatigue).\n\n6. Modeling choices and tasks feel incremental; the “benchmark” label is premature without standardized protocols/splits and public assets.\n\n7. Lacks strong/common baselines under a unified pipeline, and limited reporting of control-centric metrics (false commands, dwell/stability, behavioral latency)."}, "questions": {"value": "1. Exactly what will be released (games, streaming code, models, docs, example integrations) and when? Will there be fixed protocols/splits to justify the “benchmark” label?\n\n2. Please report per-subject and cross-session performance (and cross-game transfer, if applicable). How stable is performance after re-donning?\n\n3. What form of interpretability is realistically supported by 4 dry electrodes? Can you narrow claims to what is physiologically plausible and show minimal spectral/topographic sanity checks?\n\n4. Can you include well-tuned, standard baselines (e.g., compact CNNs and linear spatial filters) under the same preprocessing/windowing, with ablations?\n\n5. Please report command-level metrics (false positives per minute, dwell, stability) and clarify latency definitions (I/O vs behavior).\n\n6. Show a minimal “add-your-own-classifier” example and latency hooks. How do you ensure easy integration for third-party models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "12DQV2nKLI", "forum": "s9cAknJjRB", "replyto": "s9cAknJjRB", "signatures": ["ICLR.cc/2026/Conference/Submission13637/Reviewer_cfJs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13637/Reviewer_cfJs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921292067, "cdate": 1761921292067, "tmdate": 1762924214429, "mdate": 1762924214429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents MindX, a framework designed for real-time EEG-based game control using consumer hardware (Muse S headset). It contains three interactive games eliciting diverse cognitive signals (motor imagery, attention, planning) and utilizes a lightweight CNN+LDA hybrid model. Major contributions are a benchmark suite, a processing pipeline and a user study."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The motivation for the work is rightly placed, and the timely need for benchmarking online EEG decoding in the context of interactive, real-time applications is well justified. Recognising that most existing evaluation paradigms for EEG-based brain-computer interfaces primarily focus on offline tasks, clinical-grade equipment, or datasets that are not well-suited for real-world, low-latency feedback scenarios, such as gaming."}, "weaknesses": {"value": "The key weakness of the MindX paper lies in ambiguity regarding what precisely is being benchmarked and what is the novelty in the framework. Is it primarily the hardware device, the decoding algorithms, or the variation in user capacity and adaptability that is benchmarked? This lack of a clearly defined benchmarking focus makes it challenging to interpret the scope and impact of the contribution. \n\n\nAdditionally, the paper does not fully justify why existing EEG datasets and paradigms cannot be leveraged, leaving the necessity of new game-based data collection somewhat unclear. \n\nThe design and mapping of game controls to EEG signals would benefit from more explicit descriptions of user instructions and control strategies to improve reproducibility. \n\nThe pilot study’s limited participant diversity and small sample size constrain the generalizability of findings, and the omission of detailed ethical review or demographic information leaves gaps in transparency. Addressing these points would enhance clarity, rigor, and adoption potential of the benchmark."}, "questions": {"value": "Following are the queries or comments regarding the manuscript:\n\nWhat precisely does the benchmark evaluate? Is it the hardware device, the decoding algorithms, or user capacity and adaptability that limit the performance? The manusciprt mentions MindX as a framework or a pipeline for decoding and the contributions talk about it as a benchmark suite. The contributions need to be clear and well-justified against the current literature.\n\nWhy are existing EEG datasets and paradigms insufficient, necessitating new custom games and data collection? Why can't the paradigms be used in a real-time decoding setting is not clear to the readers.\n\nThe paradigms for the game and the instructions provided to users are not very clear. How exactly are users instructed or trained to generate the directional signals controlling the games? What are the specific tasks users perform, and how are different EEG signals mapped to control directions?\n\nHow is continuous control (e.g., Snake game) managed and differentiated from discrete commands?\n\nWhat are the ethical considerations, including IRB approval and user demographic details, and how do these impact generalizability?\n\nThe discussion section appears to be superficial and rushed, lacking insightful discussions."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The manuscript claims to have benchmarked the work on 3 individuals. However, there are gaps in the discussions on methods and the responsible research practice that was followed."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ii4eyeAFsW", "forum": "s9cAknJjRB", "replyto": "s9cAknJjRB", "signatures": ["ICLR.cc/2026/Conference/Submission13637/Reviewer_4eKg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13637/Reviewer_4eKg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982445895, "cdate": 1761982445895, "tmdate": 1762924213837, "mdate": 1762924213837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MindX is a modular, open real-time BCI benchmark on consumer EEG (Muse S, four frontal channels). Three games share a left/right control vocabulary. A CNN+LDA pipeline with a simple personalization loop achieves about 72–76 percent accuracy at roughly 300 ms latency in a three-participant pilot, outperforming several neural and classical baselines. Ablations support 2 s windows and smoothing. Discrete games are easier than continuous control."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Clear end-to-end system with practical latency; compact model competitive with heavier baselines; co-adaptive loop that fits real-world constraints; unified multi-game design likely to help community benchmarking; intention to open-source."}, "weaknesses": {"value": "The evaluation is very small (three participants) and appears single-session or limited in session diversity, so generalization is unclear. Several protocol details are ambiguous: the data split across windows and sessions, prevention of leakage during pseudo-labeling, and whether hyperparameters and thresholds were fixed a priori. Motor imagery with only frontal electrodes is atypical, so stronger justification or evidence is needed. Statistical analysis is limited: no variance across subjects, confidence intervals, permutation tests, or effect sizes are reported for accuracy or usability. Claims of being the first open benchmark for multi-directional real-time game control on consumer EEG are broad and need careful situating against existing toolkits. The paper inconsistently references the number of games (TUX Racer appears in tables but not in the core description), which muddles the benchmark scope."}, "questions": {"value": "How were training, validation, and test windows separated to avoid temporal and contextual leakage within a session, and were personalization updates strictly applied after evaluation blocks? Were thresholds for decision gating selected on a held-out set or tuned post hoc per subject or per session? How many sessions per subject were recorded, over what time span, and with what electrode placements or re-donnings? Does personalization ever reinforce systematic mislabels from a user’s transient strategy, and what safeguards prevent drift? Why were frontal channels chosen for motor imagery, and do results change with a simple mastoid or average reference? Can the pipeline sustain multi-class or continuous control beyond two directions without large accuracy drops?\nIn Abstract and Intro, “Hide-and-Seek, Rhythm Game, and Snake” should be named consistently throughout; later tables mention “TUX Racer” without prior introduction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WJyO1AxavW", "forum": "s9cAknJjRB", "replyto": "s9cAknJjRB", "signatures": ["ICLR.cc/2026/Conference/Submission13637/Reviewer_JaSg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13637/Reviewer_JaSg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110060917, "cdate": 1762110060917, "tmdate": 1762924213465, "mdate": 1762924213465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}