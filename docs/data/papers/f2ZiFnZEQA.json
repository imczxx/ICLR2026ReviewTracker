{"id": "f2ZiFnZEQA", "number": 5869, "cdate": 1757942571560, "mdate": 1763562692851, "content": {"title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents", "abstract": "Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information.\nHowever, unreliable search results may also pose safety threats to end users, establishing a new threat surface.\nIn this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors.\nTo counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. \nBuilding on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). \nUsing this dataset, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 9 proprietary and 8 open-source backend LLMs. \nOur results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5\\% for GPT-4.1-mini under a search workflow setting.\nMoreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting.\nThis emphasizes the value of our framework in promoting transparency for safer agent development.\nOur codebase and test cases are publicly available: https://anonymous.4open.science/r/SafeSearch.", "tldr": "", "keywords": ["Large language model", "LLM-based search agent", "Red teaming", "Safety and trustworthiniess"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b8dba5bad718e8ba82e637d03ce86ece3111257f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces SafeSearch, a benchmark to test how LLM-based search agents react to adding unreliable sources added to the retrieved webpages. The proposed pipeline leverages an LLM to create different risky scenarios and instantiate them via fictional webpages. Then, an LLM-judge evaluates the effectiveness of the injected webpage in affecting the reply of the search agent. The experimental evaluation shows that many LLMs, with various scaffolding mechanisms, are susceptible to these attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Adding search capabilities to LLM-based agents create new security vulnerabilities, which are worth studying.\n\n- The proposed pipeline to generate test-cases is automated and can be potentially scaled to a large number of samples."}, "weaknesses": {"value": "- While it'd be a useful feature of search agents to disregard unreliable sources, it is unclear whether it's realistic to detect that the generated websites are unreliable (what makes them such? is there a clear reason their information should be disregarded?), especially considering that these are artificially added to the top-5 retrieved pages, which one can expect to be popular, and thus likely reliable, sources.\n\n- In Table 2, GPT-5 (mini) seems to be largely immune to the unreliable sources, and even open-source models like Qwen3 or DeepSeek-R1 attain reasonable results (~30% ASR). Then, it seems that the proposed benchmark may not be particularly challenging, and therefore useful, in the long term."}, "questions": {"value": "- What would be the results in Table 2 without adding the unreliable sources to the search results? This would help understanding how much the injected sources need to change the agent replies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IeYKPCNZTq", "forum": "f2ZiFnZEQA", "replyto": "f2ZiFnZEQA", "signatures": ["ICLR.cc/2026/Conference/Submission5869/Reviewer_xAAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5869/Reviewer_xAAT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839692818, "cdate": 1761839692818, "tmdate": 1762918313557, "mdate": 1762918313557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SafeSearch, a red-teaming benchmark that stress-tests LLM-powered search agents by letting them retrieve and reason over poisoned web content. Covering 300 test cases across 5 risk types, it delivers the first large-scale safety snapshot of 15 mainstream LLMs under 3 search paradigms. Instead of running expensive and ethically risky black-hat SEO campaigns, the authors inject a single LLM-synthesised unreliable webpage into otherwise genuine search results. This keeps the test harmless to real users, cheap to run at scale, and fully reproducible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This study shifts the red-teaming focus from malicious user queries to the inherent unreliability of search tool outputs as the primary safety concern.\n- This study introduces adversarial content through simulated search results, enabling controlled red-teaming without real-world SEO manipulation.\n- This study conducts a systematic assessment across 300 test cases, 5 risk categories, 15 models, and 3 agent architectures to quantify vulnerabilities."}, "weaknesses": {"value": "- The authors provide limited evidence regarding the quality and real-world grounding of the 300 test queries. While Appendix C validates the reliability of the LLM-as-a-Judge metric, it does not demonstrate that the queries themselves are representative, unbiased, or cover the nuanced distribution of actual user interactions\n\n- The core problem investigated in this study, i.e., LLM systems' vulnerabilities to knowledge poisoning, has been validated in most existing RAG poisoning studies. The transition from offline to online retrieval does not alter the fundamental nature of this threat. SafeSearch is an engineering contribution to RAG security.\n\n- The author primarily focuses on extreme attack scenarios, where the searched content is optimized for high topical relevance and persuasive impact. The lower quality and relevance of malicious content in search engine results, which is the most likely scenario encountered in practice, remains under-examined for whether the model exhibits similar vulnerabilities (Even if studied, this appears to fall within the scope of RAG)."}, "questions": {"value": "- How is the authenticity and representativeness of the user query scenarios and interaction flows envisioned in the experiment ensured?\n- Could it be that the model simply prioritizes highly relevant content rather than specifically harmful content? Would an equally relevant but harmless AI-generated piece of content be given the same priority by the model?\n- The experimental evaluation covered 15 models, but notably missing was Anthropic's Claude series.\n- Is it appropriate to classify relatively simple workflows like Deep Research as an agent architecture?\n- What is the difference between Agent with the setting of “LLM w/ Search Workflow” and knowledge augmented Chatbot? \n- In multi-round search scenarios, only one round is poisoned, with no additional poisoning applied in subsequent rounds. Isn't the order of poisoning also important?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ds0JPNOWKn", "forum": "f2ZiFnZEQA", "replyto": "f2ZiFnZEQA", "signatures": ["ICLR.cc/2026/Conference/Submission5869/Reviewer_PTH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5869/Reviewer_PTH6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878564889, "cdate": 1761878564889, "tmdate": 1762918313308, "mdate": 1762918313308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the safety implications of unreliable search results on widely used search agents. It proposes a systematic, scalable, and cost-efficient red-teaming framework that leverages LLM assistants to generate test cases automatically. On the basis of this framework, the authors construct the SafeSearch benchmark, covering 5 categories of risks that include both adversarial and non-adversarial risks. The experiment results show high vulnerabilities of LLM-based search agents to unreliable search results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose an automatic red-teaming framework that eliminates the reliance on human efforts to craft test cases, making it more scalable and cost-efficient.\n- Comprehensive experiments on three representative search agent scaffolds, covering search workflow, tool-calling, and deep research.\n- The utility and safety goals are not at odds on capable models like GPT-5 or sophisticated scaffolds like deep search. \n- The setting of experiments is well controlled."}, "weaknesses": {"value": "- Not sure if the problem of different stances in replies to the same question is caused by the presence of search tools. It seems not a common behavior for the agent to alter its response after having the access to a search tool, only accounting for 4.6% of health-related examples. So it's important to show more results, demonstrating that this is a serious problem and why it happens is not due to the unreliable inference over long contexts.\n- The situations that search vendors prioritize sponsored content seem to be a more common problem. It'd be interesting to see some results targeting this case and show the influence of sponsored content on the response of search agents.\n- The feasibility of using o4-mini as the generator of malicious test cases. I expect that sometimes the model has a non-zero refusal rate on generating test cases since it is sensitive to requests that contain unsafe keywords. Can you show some results on test cases generated by open-sourced models?\n- Could you please show some evaluation results on the effectiveness of GPT-4.1-mini as the judge in this paper?"}, "questions": {"value": "No other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ezXjW77N8i", "forum": "f2ZiFnZEQA", "replyto": "f2ZiFnZEQA", "signatures": ["ICLR.cc/2026/Conference/Submission5869/Reviewer_ddvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5869/Reviewer_ddvq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995106873, "cdate": 1761995106873, "tmdate": 1762918313030, "mdate": 1762918313030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SafeSearch, an automated red-teaming framework for evaluating the safety of LLM-based search agents when exposed to unreliable search results. The authors motivate their work with two in-the-wild experiments demonstrating that (1) low-quality websites are prevalent in search results, and (2) such content can significantly influence agent behavior. The framework employs a three-stage test case generation pipeline using LLM assistants, simulation-based testing by injecting unreliable websites into authentic search results, and checklist-assisted evaluation. The resulting SafeSearch benchmark contains 300 test cases across five risk categories (advertisements, bias, harmful output, indirect prompt injection, and misinformation). Comprehensive evaluation of 15 backend LLMs across three agent scaffolds reveals substantial vulnerabilities, with the highest attack success rate (ASR) reaching 90.5% for GPT-4.1-mini under a search workflow setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Substantial number of test cases (300) covering 5 risk categories.\n- The problem is well-motivated and is relatively underrepresented in the current literature.\n- The threat model is clearly stated.\n- Automated checklist-assisted safety evaluation.\n- Valuable findings and ablation studies.\n- A nice ablation for reasoning effort of GPT-5-mini and Gemini-2.5-Flash in Appendix D"}, "weaknesses": {"value": "- I don’t agree with the description of how RAG relates to agentic search outlined in Figure 2 and Section 2.1 (in particular in this sentence: *”Unlike RAG, which typically operates over a static, locally controlled, and well-curated corpus, search agents rely on often opaque search services that provide access to large-scale, dynamic, and up-to-date information on the Internet”*). Both RAG and agentic search can operate over the same set of documents, using the same retrieval routines. The main difference is single-turn and static in RAG vs. multi-step and dynamic queries in agentic search.\n- It would be good to discuss more explicitly some challenging cases where it’s hard to judge what is true and what constitutes misinformation, fraud, “low-quality website”, etc. This seems to be a particularly relevant question because of the usage of automated LLM evaluators.\n- There is a bit of discrepancy between the paper’s title and actual content. The title focuses on automated red-teaming, while the actual content focuses more on creating a static benchmark, where automated red-teaming is used as a data collection tool."}, "questions": {"value": "- Why aren’t GPT-5 models boldfaced in the main table (Table 2)? They seem to be better across the board, and it would be clearer to use boldfacing for them instead of using a special background color.\n- What kind of guarantees are meant here? *“As shown in Figure 6 (Left), our empirical findings demonstrate that this choice can implicitly influence the safety guarantees of search agents.”*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FggLDK0OvA", "forum": "f2ZiFnZEQA", "replyto": "f2ZiFnZEQA", "signatures": ["ICLR.cc/2026/Conference/Submission5869/Reviewer_WJQ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5869/Reviewer_WJQ9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762025293527, "cdate": 1762025293527, "tmdate": 1762918312782, "mdate": 1762918312782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}