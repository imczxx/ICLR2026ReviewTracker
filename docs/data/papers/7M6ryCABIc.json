{"id": "7M6ryCABIc", "number": 23550, "cdate": 1758345314873, "mdate": 1759896808776, "content": {"title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model", "abstract": "Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image–text–action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. \nTo address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data.\nExperiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by $10.1\\%\\sim28.7\\%$ over OpenVLA, while requiring only $1.5\\%$ of its pretraining cost.  These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.", "tldr": "We introduce PixelVLA, the first VLA designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework.", "keywords": ["pixel-level understanding", "vision-language-action model", "robotics", "multimodal large language model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4209f201c8850fc91d839f4c5682330d4baf15ba.pdf", "supplementary_material": "/attachment/ea7c6f755e6c5fc985822b302b84ca010f3735c9.pdf"}, "replies": [{"content": {"summary": {"value": "The authors introduce a pixel encoder to condition VLAs on visual prompts, construct a suitable automated data annotation pipeline, and instruction tune their modified VLA to gain pixel level awareness. Their proposed visual prompt encoder conditions on multi-scale features of the robot visual observation and its resulting tokens are consumed by an LLM along with regular visual and textual tokens. The LLM outputs are mapped to robot actions with a suitable action decoder. \nThey integrate their proposed PixelVLA into two strong VLAs, OpenVLA and Pi0. Extensive experiments highlight performance improvements from their framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The authors conduct extensive experiments to highlight improvements over SOTA baselines\n2) Simple idea of incorporating pixel prompts \n3) Clever multi-stage training setup to benefit from existing VLA's pretrained weights\n4) Thorough ablations"}, "weaknesses": {"value": "**1) Architecture is unclear**\n  - In Figure 2, part (a) \"Multiscale Pixel-aware Encoder\" also shows another Visual Prompting Decoder which is unclear. This figure needs to be explained better and updated suitably. \n  - \"Visual Prompting Decoder\" is not explained clearly. What is its exact input structure? How is does it \"preserve the spatial positional information\" of visual prompts? These need to be explained. \n  - How exactly are the visual prompts represented? Fig 2 shows them both overlaid on image and input separately. The separate input needs to be described more clearly. \n\n**2) Unclear Data Generation**\n  - \"detect the gripper to generate N_e region proposals\" - how are region proposals generated from gripper location?\n  - L262 \"we employ a LLM\" - which one? How is it prompted? \n  - \"we filter out approximately 19.2% failed samples\" - how are they filtered? Manually by humans? \n\n**3) Unclear Training Details**\n  - L307 \"uniformly discretize each continuous action representation into 256 bins\" - why? PixelVLA uses a continuous action decoder? The reasoning behind this is highly unclear. \n \n**4) Related Work**\n  - Consider discussing related VLM work Ferret (https://arxiv.org/pdf/2310.07704) and FerretV2 that performs similar visual prompt encoding to improve VLA performance."}, "questions": {"value": "See weaknesses. The paper is interesting and contains strong experimental evidence. However, several missing key details weakens it in current form."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l3x86InzPG", "forum": "7M6ryCABIc", "replyto": "7M6ryCABIc", "signatures": ["ICLR.cc/2026/Conference/Submission23550/Reviewer_FnpE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23550/Reviewer_FnpE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761098994856, "cdate": 1761098994856, "tmdate": 1762942708173, "mdate": 1762942708173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present PixelVLA, a VLA model that introduces diverse multimodal prompts to the VLA pipeline and whose architecture is designed and trained specifically for these prompts. The VLA itself consists of a vision encoder to process image observations, visual prompting encoder (from SAM) and multi-scale visual encoder to process multimodal prompts (points, boxes, lines etc), and LLM backbone and a continuous action decoder. To generate visuomotor instruction tuning data for training PixelVLA, the authors use an LLM to extract information about target objects, and use an object detector and segmentation model to extract multimodal prompts, all over the Fractal and Bridge datasets, and contribute the resulting dataset. The experiments are done over SimplerEnv and Libero, across multiple VLA backbones, to show the benefits of PixeVLA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Meaningful technical contribution in Pixel-160k dataset and in constructing a VLA that takes advantage of multimodal prompts.\n- Experiments test two SOTA VLA architectures, showing that PixelVLA can be built on top of multiple types of VLAs."}, "weaknesses": {"value": "- Unclear whether method is feasible to transfer to novel environments, due to lack of real world experiments.\n- Analysis of results are lacking and leave out failure cases; for example, why does PixelVLA perform so well on Libero Long but struggle on the Object/Goal splits (Table 3)? Why would the pixel-level understanding training damage performance on the open/close drawer task (Table 4)?"}, "questions": {"value": "l. 218 Where does the pixel-aware mask input come from?\nl. 237-239 How does the NTP loss work in this case? Is there causal masking?\nl. 252 You're only using the first gripper-close state – might that negatively affect training due to some domain shift between first gripper-close state and subsequent such states?\nSection 5.2: Were the $pi_{0}$ and OpenVLA baselines also trained in their typical fashion on the Fractal and Bridge datasets? This would ensure that the improvement is due to the PixelVLA method and instruction tuning approach rather than just seeing more data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TvAVsCBUBq", "forum": "7M6ryCABIc", "replyto": "7M6ryCABIc", "signatures": ["ICLR.cc/2026/Conference/Submission23550/Reviewer_DBFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23550/Reviewer_DBFX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966476018, "cdate": 1761966476018, "tmdate": 1762942707906, "mdate": 1762942707906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PixelVLA, a VLA that benefits from pixel-level grounding information. Based on a typical VLA model with a vision encoder, text tokenizer, and LLM, the authors introduced a pixel encoder that is designed to handle pixel-level understanding information. Meanwhile, to finetune such an encoder, the authors proposed a two-stage automated annotation pipeline and created a pixel-annotated visuomotor instruction tuning dataset, Pixel-160k. The experiments on SimplerEnv and LIBERO show that the proposed method achieves better performance than other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and well-structured, making it easy to follow. The experiments conducted on SimplerEnv and LIBERO are appropriate and demonstrate the effectiveness of the proposed approach. While introducing an additional pixel-level encoder could intuitively downgrade the pretrained VLM, the authors successfully solve this issue by curating a large 160K dataset and applying LoRA fine-tuning."}, "weaknesses": {"value": "The introduction of pixel-level annotations can be viewed as a relatively straightforward extension of prior work on visual prompting and image-level feature adaptation (e.g., TraceVLA, LLaRA, and related approaches). As a result, the paper’s novelty is somewhat limited. Nonetheless, the work offers useful insights and has potential value for the research community, particularly as a good practice in bridging pixel-level understanding with pretrained VLMs for VLAs. \n\nMeanwhile, the authors are highly encouraged to deploy the proposed method in the real world and verify the claim.\n\nTherefore, I would recommend a weak acceptance."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d8iwrIeaZ6", "forum": "7M6ryCABIc", "replyto": "7M6ryCABIc", "signatures": ["ICLR.cc/2026/Conference/Submission23550/Reviewer_sYs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23550/Reviewer_sYs6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977077127, "cdate": 1761977077127, "tmdate": 1762942707603, "mdate": 1762942707603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PixelVLA, which besides normal language and , visual encoder, PixelVLA add a modal that enable pixelwise grounding, then claim this enable the model learn more fine-grained represenations that further benefit the understanding and action prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n2. According to the results shown in Table 2, Table 3 and Table 4, it seems, add more pixel level prompting and intergrate this as a new modal can help increase the action control accuracy."}, "weaknesses": {"value": "1. I am confused about the design of the whole architecture. It seems, the author proposes a two-state automated pipeline to get the pixel level segmentation, how these segmentations are used? it is not very clear that which part these segmentation masks are used for in the model.\n\n2. If the segmentation is used as a input to learn the pixel-aware embedding, i am not sure the final optimization of loss for these visual encoder is action accuracy? seems not very relevent. What is the motivation of input these mask to learn the pixel aware embedding.\n\n3. The author gives some results and try to say pixel vla is better than OpenVLA or Pi0, but that is not fair since Pixel VLA needs extra prompting from user, these extra prompt actually gives more direct spatial information of the user's goal. The intuition is the using of these informaiton will give more shortcut to the backbone learning and make it less capable to do reasoning and understand the language."}, "questions": {"value": "Besides the main conern in weakness part. I still have questions:\n1. I might need the user better claim the motivation of add the segmentation mask or visual prompting as an input, i feel these will harm the model's potential ability to learn to locate the objects and understanding the scene itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H5EqpLleQB", "forum": "7M6ryCABIc", "replyto": "7M6ryCABIc", "signatures": ["ICLR.cc/2026/Conference/Submission23550/Reviewer_FaYs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23550/Reviewer_FaYs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980515160, "cdate": 1761980515160, "tmdate": 1762942707412, "mdate": 1762942707412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}