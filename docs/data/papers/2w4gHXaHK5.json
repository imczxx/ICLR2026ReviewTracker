{"id": "2w4gHXaHK5", "number": 17364, "cdate": 1758275061185, "mdate": 1759897179817, "content": {"title": "Harnessing Bayesian Optimism with Dual Policies in Reinforcement Learning", "abstract": "Deep reinforcement learning (RL) algorithms for continuous control tasks often struggle with a trade-off between exploration and exploitation. The exploitation objective of a RL policy is to approximate the optimal strategy that maximises the expected cumulative return based on its current beliefs of the environment. However, the same policy must also concurrently perform exploration to gather new samples which are essential for refining the underlying function approximators. Contemporary RL algorithms often entrust a single policy with both behaviours. However, these two behaviours are not always aligned; tasking a single policy with this dual mandate may lead to a suboptimal compromise, resulting in inefficient exploration or hesitant exploitation. Whilst state-of-the-art methods focus on alleviating this trade-off between exploration and exploitation to prevent catastrophic failures, they may inadvertently sacrifice the potential benefits of optimism that drives exploration. To address this challenge, we propose a new algorithm based on training two distinct policies to disentangle exploration and exploitation for continuous control and aims to strike a balance between robust exploration and exploitation. The first policy is trained to explore the environment more optimistically, maximising the upper confidence bound (UCB) of the expected return, with the uncertainty estimates for the bound derived from an approximate Bayesian framework. Concurrently, the second policy is trained for exploitation with conservative value estimates based on established value estimation techniques. We empirically verify that our proposed algorithm, combined with TD3 or SAC, significantly outperforms existing approaches across various benchmark tasks, demonstrating improved performance.", "tldr": "We propose to use two policies to address the exploration-exploitation tradeoff by leaveraging Bayesian principles.", "keywords": ["reinforcement learning", "exploration-exploitation trade-off"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/663fa68a3bac5acbd1a2e18a96ce1fb96327cec5.pdf", "supplementary_material": "/attachment/3fab9628d7a5d31b16ce825ff7818bebc5a57ad1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes BOXD, an online continuous-control RL framework that maintains separate “optimistic” and “task” actor–critic branches. The optimistic branch uses dropout-based critic ensembles whose maximum Q estimates approximate a UCB-style bonus, while the task branch follows standard TD3 or SAC training. Data collection stochastically alternates between the two policies via a linearly annealed schedule so the replay buffer mixes optimistic and conservative samples. On eleven DM Control tasks, BOXD is reported to outperform TD3, SAC, OAC, and a DERL-inspired baseline, with ablations exploring ensemble size, dropout samples, and policy-selection schedules."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Leverages a simple dropout-ensemble max operator to approximate a UCB bonus in continuous control without explicit variance estimation, making the optimism mechanism easy to graft onto SAC/TD3.\n- Demonstrates consistent or improved performance over baselines across a subset of DM Control Suite tasks."}, "weaknesses": {"value": "- The proposed “dual policy” story fails to substantiate a better exploration/exploitation balance: both actors write to the same replay buffer and the acting policy is chosen by a fixed linear annealing schedule, so the mechanism collapses to a single mixed policy without adaptivity or evidence of superior exploration; unlike truly dynamic strategies such as Thompson Sampling, the exploration rate never responds to uncertainty or task progress in different regions of the state space.\n- Comparisons omit closely related optimistic dual-policy methods (e.g., BRO [1]) and more recent strong model-free baselines (e.g., SimbaV2 [2]), leaving unclear whether BOXD advances state of the art beyond the specific baselines chosen.\n- The optimistic claim is not validated on harder or exploration-heavy tasks; the evaluation omits the Dog subset of DM Control and any sparse-reward benchmark (e.g., Maze2D), so the optimistic branch’s benefit remains untested where exploration pressure is high.\n- Reporting leaves gaps: the text alternates between IQM and average returns for Figure 4, wall-clock costs versus SAC/TD3 are missing.\n\n[1] Nauman et al., Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control, 2024.\n\n[2] Lee et al., Hyperspherical Normalization for Scalable Deep Reinforcement Learning, 2025."}, "questions": {"value": "- Why are recent state-of-the-art continuous-control baselines—such as BRO [1] and SimbaV2 [2]—absent from the comparison, and how might BOXD perform against them?\n- Why were the harder Dog tasks from the DM Control suite omitted, given the claimed exploration improvements?\n- Would the authors consider adding experiments on sparse-reward benchmarks (e.g., Maze2D) to showcase the optimistic branch’s exploration benefits?\n- In lines 196–200, the authors write that \"it may be computationally expensive.\" What does \"it\" refer to in this context, and what evidence supports the claimed expense?\n- The paper argues that early exploration on humanoid-run ultimately improves asymptotic performance despite an initial lag behind SAC. Could the authors provide analyses or ablations—e.g., varying the proportion of exploration early in training—to demonstrate that the optimistic sampling schedule is responsible for the late-stage gains?\n- The annealing rule in Eq. 9 uses a hard-coded factor of 10. Could the authors justify this choice or provide an ablation/sensitivity analysis?\n- Do the authors keep all other hyperparameters identical across tasks aside from $(n,k)$? If not, please detail any task-specific tuning.\n- Figure 4’s caption and main text alternate between IQM and simple average returns. Could the authors clarify which statistic is reported and update the plots accordingly?\n- Please report wall-clock training time relative to SAC/TD3, given the added actor/critic ensembles?\n\n[1] Nauman et al., Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control, 2024.\n\n[2] Lee et al., Hyperspherical Normalization for Scalable Deep Reinforcement Learning, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E05NMWA1VJ", "forum": "2w4gHXaHK5", "replyto": "2w4gHXaHK5", "signatures": ["ICLR.cc/2026/Conference/Submission17364/Reviewer_CpNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17364/Reviewer_CpNo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760990143189, "cdate": 1760990143189, "tmdate": 1762927278832, "mdate": 1762927278832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the exploration-exploitation dilemma of reinforcement learning in a continuous control context. The suggested approach trains separate policies for each of these conflicting purposes by treating the maximum critic response in Bellman target estimation for exploration and minimum as in the standard practise for exploitation. The suggested recipe has been appended to two mainstream base models, SAC and TD3, and has been tested on a number of continuous control tasks taken from the DeepMind Control Suite."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The main results reported in Figure 4 as well as the more detailed results provided in the appendix indicate a trend in favor of the suggested approach. Particularly, the approach consistently improves performance when dropped into a base learner such as SAC or TD3."}, "weaknesses": {"value": "None of the suggested contributions listed in Lines 85-98 are novel:\n * Epistemic uncertainty quantification with Monte Carlo dropout is a rather addendum as also suggested in DroQ.\n * Calculating UCB/LCB over multiple Q-functions has been extensively studied in prior work such as OAC cited by the authors and EDAC [1] in the offline RL context, the online application of which is straightforward. Neutralizing the over-pessimism of critic ensembles also has a thick literature even with algorithms that can tune its degree dynamically in the course of training. For example see [2,3] and their references. For example, the TOP algorithm [3] performs Bayesian optimization to dynamically tune the degree of optimism and pessimism in actors and critics, respectively. \n * I also expect the shown performance improvements to shrink when the method is appended to REDQ [4], which is the established state of the art in performing SAC with more than two critics\n *  To my take BOXD is only a special case of many of the more advanced algorithms suggested above and its demonstrated improvement stems from a model capacity extension, e.g. introducing more free parameters, which can also be done in arbitrary other ways.\n\nIt is a big weakness that the paper does not recognize these contributions, does not differentiate its solution from them, and does not provide quantitative comparison against them.\n\nLines 468-471 conclude the paper by presenting the novelty as *\"we propose training a dedicated exploration policy $\\pi^{\\text{explore}}$ guided by UCB that may be directed estimated from this uncertainty ...\"*. I am not able point out the novelty here what is already suggested in OAC and [1,2,3], at least conceptually. This may either be due to the lack of novelty or a too high-level description of it in the paper. In both cases, a major revision is essential before the publication of the work.\n\n[1] An et al., Uncertainty-Based Offline Reinforcement Learning\nwith Diversified Q-Ensemble, NeurIPS, 2021\n\n[2] Cetin and Celiktutan, Learning Pessimism for Robust and Efficient Off-Policy Reinforcement Learning, AAAI, 2023\n\n[3] Moskovitz et al., Tactical Optimism and Pessimism for Deep Reinforcement Learning, NeurIPS, 2021\n\n[4] Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model, ICLR, 2021"}, "questions": {"value": "* Can the authors confirm that in all plots where the models are compared, the suggested BOXD extension does not use more model capacity than the baselines, be it additional critic copies, a new network trained for exploration purposes and so on? If this is indeed the case, then we need to compare it with the plain baselines whose model capacities are also extended proportionally. Only in this way we can assess the value added of the contribution over sheer capacity enhancement. I wonder the answer to this question also because of Line 475 that puts forward computational cost as a limitation. It is fine if this overhead stems from extra computations made on a fixed hypothesis space. But if it stems from a capacity extension, I have to be sure that the baselines are fairly treated.\n\n * From Lines 320-322, it reads like DERL has many conceptual similarities to the suggested BOXD. Can the authors detail in which ways BOXD and DERL are similar and what is BOXD doing differently from it? I can see in Figure 4 that DERL tends to match better with TD3 than SAC, which is expected from a reward shaping approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YQBBNZG521", "forum": "2w4gHXaHK5", "replyto": "2w4gHXaHK5", "signatures": ["ICLR.cc/2026/Conference/Submission17364/Reviewer_RzfA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17364/Reviewer_RzfA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204937391, "cdate": 1761204937391, "tmdate": 1762927278373, "mdate": 1762927278373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed BOXD, a method for mediating exploration-exploitation tradeoff, by instantiating separate \"conservative\" policies that optimize the return given task-dependent rewards, and \"exploration\" policy that maximize the upper confidence bound (UCB) given the set of value estimates from an ensemble of value networks. An annealing scheme is proposed from exploration to greedy over time and experience with the task. The proposed method is evaluated on continuous control tasks from the dm-control suite."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to follow.\n- Empirical evaluation show promising results."}, "weaknesses": {"value": "- The idea of independent explorative and exploitative agents for exploration-exploitation tradeoff has been studied in Yu et al., 2022, and the presented study bears strong resemblance to this previous study. The missing citation and relevant discussion undermines the validity and novelty of the current paper.\n- The paper's claim to \"harnessing Bayesian optimism\" is a significant overstatement. The link between the algorithm and a principled UCB and computational principles behind Bayesian optimization is weak, and lack of theoretical justification. \n- The proposed method is expected to introduce significant computational overhead. The absent complexity analysis is concerning.\n\nReferences.\n\n[1] Yu, C., Mguni, D., Li, D., Sootla, A., Wang, J. and Burgess, N., 2022. SEREN: Knowing When to Explore and When to Exploit. arXiv preprint arXiv:2205.15064."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sMoSzQaNh0", "forum": "2w4gHXaHK5", "replyto": "2w4gHXaHK5", "signatures": ["ICLR.cc/2026/Conference/Submission17364/Reviewer_n68U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17364/Reviewer_n68U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764167990, "cdate": 1761764167990, "tmdate": 1762927277551, "mdate": 1762927277551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses a challenge in deep-learning exploration methods, that a focus on exploration using optimistic value functions can distract the agent from solving the task, while aiming for accurate, conservative value functions may reduce exploratory behavior and lead to inefficient data gathering. The authors propose training two separate Q estimates, and two separate policies, for exploration and for exploitation. The exploratory policy maintains optimism through bootstrapping with UCB value estimates computed through an ensemble of dropout Q networks. Using this method improves the performance of both SAC and TD3 on a suite of continuous control tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The derivation of a UCB result from EVT was interesting, and using dropout and ensembles to generate the required standard deviation is clever. The results do significantly improve the baselines. I have not seen using the max over an ensemble to produce optimistic targets."}, "weaknesses": {"value": "The authors do acknowledge this, but the novelty of separating exploration and exploitation is limited, as it has been done many times before. Besides the examples listed in the paper, [1] does this explicitly, and [2] learns a family of bonus-based exploration algorithms with different bonus scales (including b=0).\n\nThe EVT math is interesting, however as far as I can tell the variance from dropout is not explicitly used. So in practice, this reduces to “max over ensembles.” In addition, I doubt that dropout_rate=0.001 actually leads to significant variance. Strong performance with 2 networks and 2 samples similarly suggests that the optimistic UCB is not necessarily the important improvement.\n\nThough I have not seen this specific use of optimistic targets using ensembles, a large variety of work has covered using ensembles for bayesian upper bounds on value, such as Bootstrap DQN [3] and SUNRISE [4].\n\nThough the math presented in this paper is interesting, it does not feel strongly connected to the empirical results, and I believe the separate ingredients of ensembles, dropout, and disentangled exploration/exploitation policies have all been thoroughly explored before, if not their combinations.\n\nOne presentation issue I wanted to bring up: I believe Figure 6 is the same plot side-by-side?\n\n\n[1] “Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning”, Whitney et al, https://arxiv.org/pdf/2101.09458\n[2] “Never Give Up: Learning directed exploration strategies” Badia et al, https://arxiv.org/pdf/2002.06038 \n[3] “Deep Exploration via Bootstrapped DQN” Osband et al, https://arxiv.org/abs/1602.04621\n[4] “SUNRISE: a simple unified framework for ensemble learning in deep reinforcement learning”, Lee et al, https://arxiv.org/abs/2007.04938"}, "questions": {"value": "Do the authors have results clarifying the role of dropout in this method? Does it operate as a training regularizer or something more directly connected to bayesian optimism? Is this method sensitive to dropout coefficient?\n\nSimilarly for ensembles, can the authors separate the stability improvements of ensembles from the bayesian optimism? For example, would we expect a “mean over ensemble” to do significantly worse than “max over ensemble”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlPMxPoDZp", "forum": "2w4gHXaHK5", "replyto": "2w4gHXaHK5", "signatures": ["ICLR.cc/2026/Conference/Submission17364/Reviewer_jL2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17364/Reviewer_jL2p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17364/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791656947, "cdate": 1761791656947, "tmdate": 1762927277108, "mdate": 1762927277108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}