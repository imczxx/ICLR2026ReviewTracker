{"id": "62pn18XmAg", "number": 9237, "cdate": 1758116022177, "mdate": 1759897736101, "content": {"title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "abstract": "Transformers have become the de facto backbone of modern deep learning, yet their training typically demands an advanced optimizer with adaptive learning rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that it is mainly due to a heavy-tailed distribution of the gradients. In this paper, we introduce a Deeply Normalized Transformer (DNT), which is meticulously engineered to overcome this limitation enabling seamless training with vanilla mSGDW while yielding comparable performance to the Transformers trained via AdamW. To be specific, in DNT, we strategically integrate normalization techniques at proper positions in the Transformers to effectively modulate the Jacobian matrices of each layer, balance the influence of weights, activations, and their interactions, and thus enable the distributions of gradients concentrated. We provide both theoretical justifications of the normalization technique used in our DNT and extensive empirical evaluation on two popular Transformer architectures to validate that: a) DNT outperforms its counterparts (i.e., ViT and GPT), and b) DNT can be effectively trained with vanilla mSGDW.", "tldr": "We have introduced a novel architecture, Deeply Normalized Transformer (DNT), which enables efficient training with vanilla momentum SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers.", "keywords": ["Transformer", "Deep Normalization", "SGD"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b45b513b9954eeeae34022e82ca53b6e5bdd9e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript targets the demand for advanced optimizers with adaptive learning rates in the training of Transformers. A deep normalization transformer is proposed, in which the heavy-tail gradient problem is overcome by strategically integrating different normalization techniques into the appropriate positions of the Transformer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is timely, clearly presented, theoretically analyzed, and empirically evaluated on two popular Transformer architectures."}, "weaknesses": {"value": "However, key clarifications and stronger validation are necessary. The detailed comments are as follows:\n\n1. Based on the normalization techniques mentioned in Figure 4, is the contribution of this manuscript merely an engineering improvement regarding the strategic integration of different normalization techniques? What is the logic of cooperation between them? And how to determine the appropriate positions of each technology in the Transformer?\n\n2. In Tables 2-4 of Appendix C, is the selection of parameters specified by the authors or the relatively optimal settings obtained through search? In the experiment, key hyperparameters and design choices were lacking in ablation.\n\n3. There are inconsistencies or errors in the reference writing, such as:\n\"Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024.\"\nshould be:\n\"Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36:49205-49233, 2023.\"\nIt is suggested that authors should carefully check, revise and improve.\n\n4. In line 426, \"See Appendix H for the training parameters.\" In fact, \"Appendix H\" does not exist in the manuscript.\n\n5. In the manuscript, \"the norm of * is very large\", \"the norm of * is large\" and \"* become too large\" are mentioned times. How to define or quantitatively describe \"very large\", \"large\", or \"too large\"?  \n\t\n6. What is the definition of σ(∙) in lines 303-304 σ(W_1 ) and σ(W_2 )? In Equation (4), \"Y = Self-Attention(X′), where x′ = PreNorm(x) \". However, what kind of variable is X′? Or is there any connection between X' and x'? Furthermore, in line 256, \"Y = Self-Attention(X) and Y' = Self-Attention(X')\". Is it accurate that it is different from the \"Y = Self-Attention(X')\" in equation (4)?\n\t\n7. In order to facilitate readers' understanding, the authors should introduce and explain the alphabetic symbols and various operation symbols that appear in the manuscript. For example: The definition of \"⨂\" in the equation on lines 241-242 and in Equation (5) needs to be declared. What is \"C\" in Equation (5)? The \"⨀\" in line 254 needs to declare its definition. Furthermore, x_0, which appears in lines 372-373, seems not to have been used. So, where does it play any role? What parameter is \"C_dn\" in the formula on lines 730-731? What is \"C\" in Equation (13)?\n\n8. There is a cross-reference error in \"according to Equation ??\" in line 754. Please check and confirm whether other citations are standardized.\n\t\n9. In lines 759-758, there are ∂L/(∂vec(Y)) and  ∂L/(∂vec(W_q)),   ∂L/(∂vec(W_k)),   ∂L/(∂vec(W_v)), where L and L should be the same?\n\t\n10. The captions in Figures 7-10 are not consistent with those of the other figures mentioned earlier in the manuscript.\n\t\n11. What are the dimensions W_1 and W_2 that appear in Equation (6) respectively? In line 274, does the W_1 x>0 in \"W_2 diag(1(W_1 x>0)) W_1\" represent the relationship between the vector W_1 x and 0? How exactly is it defined? Furthermore, what kind of calculation is 1(W_1 x>0)? The authors should supplement the corresponding definitions or calculation methods.\n\t\n12. The \"if\" in line 754 should be \"If\"."}, "questions": {"value": "see above section for detailed information"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SLsFJfRvCR", "forum": "62pn18XmAg", "replyto": "62pn18XmAg", "signatures": ["ICLR.cc/2026/Conference/Submission9237/Reviewer_gReu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9237/Reviewer_gReu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720680421, "cdate": 1761720680421, "tmdate": 1762920892296, "mdate": 1762920892296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation in training Transformers: the poor performance of momentum SGD (mSGD) compared to adaptive optimizers like AdamW. The root cause is identified as the heavy-tailed distribution of gradients in Transformers, which causes uneven updates across parameters.\n\nTo resolve this, the authors propose Deeply Normalized Transformers (DNT), which strategically apply normalization at specific positions in the architecture to modulate Jacobians, balance weight and activation contributions, and reduce the heavy-tail behavior of gradients. Theoretical justifications are provided for why these normalization choices improve training with mSGD. Empirically, DNT achieves performance comparable to AdamW on ImageNet (Vision Transformers) and OpenWebText (GPT) while requiring less memory and computation, thanks to the ability to use mSGDW."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a well-known problem in training Transformers, namely why momentum SGD (mSGD) tends to fail compared to adaptive optimizers like AdamW. It provides a detailed theoretical analysis showing how the placement of normalization layers affects the conditioning of Jacobian matrices and the variance of gradients, explaining why these adjustments are crucial for stable training. \n\n2. The approach is practical, leveraging existing normalization techniques in new positions without introducing additional components. Empirically, the proposed Deeply Normalized Transformer (DNT) matches AdamW performance on ImageNet and OpenWebText, with gradients that are more concentrated and stable under mSGDW. \n\n3. Using mSGDW also reduces memory and computational requirements compared to AdamW, which is a significant practical advantage. \n\n4. Finally, the paper offers analytical insights linking the structure of normalization to optimizer behavior, providing a deeper understanding of the interplay between architecture and training dynamics."}, "weaknesses": {"value": "1. The paper has several limitations regarding the scope and scale of its experiments. It evaluates the proposed Deeply Normalized Transformer (DNT) only on two benchmarks, ImageNet and OpenWebText, and does not include large-scale or multimodal tasks. This narrow evaluation makes it difficult to assess how well the method generalizes to other domains or to the training of state-of-the-art large models.\n\n2. Another limitation is the increased complexity introduced by multiple normalization placements. While these placements are key to stabilizing mSGD, they also add implementation overhead and require careful hyperparameter tuning.\n\n3. The paper also lacks comparisons with other recent approaches designed to improve stability in Transformers, such as nGPT, Stable Transformer, or LipsFormer. Similarly, there is no evaluation against newer optimizers like Muon, which could provide important context for the relative benefits of DNT and mSGDW (Muon is closer to SGD than Adam).\n\n4. Finally, the evaluation on GPT2-small is somewhat limited in scale and may not reflect the challenges of training modern large language models. The optimizer shows instabilities in some experiments, and the loss gap between mSGDW and AdamW remains non-negligible, which could be a critical concern for training larger or more complex architectures."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J4Kr78jnMQ", "forum": "62pn18XmAg", "replyto": "62pn18XmAg", "signatures": ["ICLR.cc/2026/Conference/Submission9237/Reviewer_epVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9237/Reviewer_epVN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856023145, "cdate": 1761856023145, "tmdate": 1762920891952, "mdate": 1762920891952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Deeply Normalized Transformer (DNT), a Transformer architecture designed to be effectively trained with momentum SGD (mSGDW) instead of adaptive optimizers like AdamW. The authors identify that Transformers exhibit heavy-tailed gradient distributions, which make SGD-based optimizers unstable. DNT addresses this by inserting or repositioning normalization layers (InputNorm, PreNorm, MidNorm, and QKNorm) to modulate the Jacobian of each block, ensuring more concentrated gradient distributions and reducing training instability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical justification on how each normalization position affects the Jacobian and stabilizes gradient magnitudes.\n2. Empirical results showing that DNT trained with mSGDW performs comparably to standard Transformers trained with AdamW, both on ImageNet (ViT) and OpenWebText (GPT2)."}, "weaknesses": {"value": "1. Theoretical assumptions are idealized: The high-dimensional isotropy and orthogonality assumptions may not hold exactly for real Transformer activations.\n2. Similar ideas appear in nGPT, StableTransformer, and Lipsformer (which are cited), but the novelty claim is modest—it’s mostly a systematic integration and justification rather than a new normalization method. \n3. The comparison is primarily between mSGD and AdamW. It would be more compelling to see how it performs against other optimizers like Sophia or Lion.\n4. While comparing mSGDW and AdamW, the paper does not clarify if both optimizers were optimally tuned (learning rates, weight decay, warmup). The authors state \"we did not tune the learning rate too much\". However, the hyperparameter tables (e.g., Table 2, 4) show that the settings for mSGDW and AdamW are vastly different. For instance, L-DNT-Small uses LR=1.0 for mSGDW versus LR=6e-4 for AdamW , and V-DNT-Large uses LR=0.5 for mSGDW versus LR=1e-3 for AdamW.\n5. In addition, There's also no evidence that DNT maintains benefits under fine-tuning, transfer, or longer training schedules."}, "questions": {"value": "1. Could you include ablations isolating the impact of each normalization (InputNorm, PreNorm, MidNorm, QKNorm) individually on gradient statistics and performance?\n2. How much GPU memory and wall-clock time are saved when training with mSGDW compared to AdamW?\n3. How does DNT differ conceptually from “Transformers without normalization” (Zhu et al., 2025)? Could these approaches be unified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Qu2Uk4mXUD", "forum": "62pn18XmAg", "replyto": "62pn18XmAg", "signatures": ["ICLR.cc/2026/Conference/Submission9237/Reviewer_rT6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9237/Reviewer_rT6F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856434339, "cdate": 1761856434339, "tmdate": 1762920891475, "mdate": 1762920891475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Deeply Normalized Transformer (DNT), a Transformer variant designed to address the heavy-tailed gradient problem that hinders the performance of vanilla momentum SGD (mSGDW) in training Transformers. The authors provide theoretical analysis connecting the heavy-tail issue to the Jacobian matrices of different Transformer components and argue that strategically introducing normalization at specific positions (InputNorm, PreNorm, MidNorm, and QKNorm) can stabilize gradients and mitigate this issue. DNT is evaluated on both Vision Transformers (ViT) and GPT architectures, showing that it can be trained effectively with mSGDW to reach performance comparable to AdamW. The paper includes gradient distribution visualizations and empirical results on ImageNet and OpenWebText benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a practical and important problem—reducing dependency on adaptive optimizers like Adam—by improving Transformer architectures to work with simpler optimizers. \n\n- The theoretical analysis provides a clear connection between normalization placement and Jacobian conditioning, offering intuition for the design of DNT. They also did a comprehensive analysis of different normalization techniques.\n\n- Experimental results across both vision and language models demonstrate that DNT narrows the performance gap between mSGDW and AdamW, suggesting potential for simpler and more efficient training pipelines."}, "weaknesses": {"value": "- The experiments, while promising, are limited in scale (e.g., GPT2-Small/Large, ViT-Large) and lack validation on larger models or diverse datasets to confirm robustness.\n- The empirical novelty is modest, as the approach primarily reorganizes existing normalization techniques rather than introducing new mechanisms.\n- The paper does not include detailed ablation studies to isolate the contribution of each normalization type, which would strengthen the empirical validation."}, "questions": {"value": "- How does DNT scale when applied to very large LLMs (e.g., tens of billions of parameters)? Are there architectural or stability issues?\n- Could the authors clarify whether the performance parity with AdamW holds when hyperparameters (e.g., learning rates, momentum) are more extensively tuned for mSGDW?\n- How does DNT interact with modern optimizers such as Muon—does normalization reduce or amplify their benefits?\n- What are the computational overheads introduced by the additional normalization layers in large-scale training?\n- It appears that mSGDW with DNT generally performs worse than AdamW during the early stages of training—could the authors provide an explanation for this behavior?\n\nTypo:\n- line 158: any a forward layer"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HvjPDIvZ5j", "forum": "62pn18XmAg", "replyto": "62pn18XmAg", "signatures": ["ICLR.cc/2026/Conference/Submission9237/Reviewer_QNDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9237/Reviewer_QNDm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9237/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957449524, "cdate": 1761957449524, "tmdate": 1762920890811, "mdate": 1762920890811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}