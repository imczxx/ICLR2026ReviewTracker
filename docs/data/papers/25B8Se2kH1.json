{"id": "25B8Se2kH1", "number": 22688, "cdate": 1758334463541, "mdate": 1759896852175, "content": {"title": "LFQ: Logit-aware Final-block Quantization for Boosting the Generation Quality of Low-Bit Quantized LLMs", "abstract": "As large language models (LLMs) continue to scale, low-bit weight-only post-training quantization (PTQ) offers a practical solution to their memory-efficient deployment. Although block-wise PTQ is capable of matching the full-precision (FP) baseline on basic language modeling and understanding, its quality is degraded for \\textit{generative} tasks---especially at longer responses and extended chains of thought, which is critical in boosting task accuracy. We attribute this shortfall to two factors: (i) the omission of the unembedding layer (the LM head) in block-wise optimization and (ii) the reliance on the mean squared error (MSE) objective. Both factors cause the token probability distribution of the quantized model to misalign with that of the FP model, yielding notable accuracy drops on text generation benchmarks. To rectify the discrepancy, we introduce \\emph{Logit-aware Final-block Quantization (LFQ)}, a simple yet effective enhancement to block-wise PTQ that quantizes the final Transformer block by minimizing the cross-entropy between the logits of the FP model and those of its quantized counterpart. By aligning token probabilities at the logit level in the final block, LFQ consistently improves the accuracy of complex generation tasks over state-of-the-art block-wise PTQ across diverse model families and text generation tasks, while maintaining parity with FP baselines on language modeling and understanding.", "tldr": "", "keywords": ["Efficient Inference", "Post-Training Quantization", "Large Language Models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b1bd0da7426a1d54d0fd451ff7694ac5f8fa25c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes LFQ that solves two critical problems previous papers hasn't noted: previous methods don't optimize the LM-head and over-reliance on MSE loss. The author notes we should optmize the LM-head with the objective of cross-entropy loss. Author proposed a new optmization objective and adapt it different PTQ algorithms. Many empirical experiments show that this method can improve the performance on different benchmarks across different models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles the important problem of quantizaton and proposes a solution that is adaptable to different algorithm\n- This paper conducts empirical experiments across many different models and benchmarks.\n- The motivation of using cross-entropy instead of MSE is well-established."}, "weaknesses": {"value": "- As discussed in the sufficiency of quantizing solely the final block section, I wonder why applying not applying this to more layers would yield better performance? Could you try more layers(say last 10?) and provide more analysis?\n- LM head optimization produces minimal Gain? As the paper noted, many methods neglect the LM head, which takes relative small amount of memory but could critically influence the performance. Though this paper proposes a better method to optimize the LM head quantization, does it worth it to quantize it instead of leaving it in bf16 for better performance if we can better optimize [XWq-XW_fp|_f? I see the paper discuss it in section 2 and Table3, but I don't fully understand the importance of quantizing the LM head as well."}, "questions": {"value": "- Although this paper includes many different ways of quantization, the experiments with AWQ, which is the mainstream quantization method, is not included. I wonder whether authors can provide more experiments with AWQ to show the performance of this methods.\n- My main concern is adaption to AWQ and why do we need quantized LM-head(I agree if we have to quantize LM-head, cross-entropy would be a better option)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pfDaOrMqYI", "forum": "25B8Se2kH1", "replyto": "25B8Se2kH1", "signatures": ["ICLR.cc/2026/Conference/Submission22688/Reviewer_p8dA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22688/Reviewer_p8dA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947514116, "cdate": 1761947514116, "tmdate": 1762942337853, "mdate": 1762942337853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Logit-aware Final-block Quantization (LFQ) to enhance the performance of quantized LLMs. Unlike conventional block-wise post-training quantization (PTQ), which ignores the LLM head during quantization and minimizes the mean squared error (MSE) between the original and quantized model outputs, LFQ explicitly quantizes the LLM head by minimizing the cross-entropy between the token probability distributions of the original and quantized models. The authors apply LFQ to both general-purpose LLMs and reasoning-specialized models, such as DeepSeek-R1-Distill-Llama-8B, and evaluate the quantized models across multiple benchmarks, including language modeling, language understanding, instruction following, and mathematical reasoning tasks. Experimental results demonstrate that LFQ consistently improves the performance of quantized LLMs over existing block-wise PTQ methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. LFQ can be easily integrated into various existing block-wise PTQ methods to further enhance the performance of quantized LLMs.\n\n2. As shown in Table 2, the experimental results indicate that applying LFQ substantially enhances the performance of reasoning-specialized quantized LLMs on the challenging AIME 2024 benchmark when greedy decoding is used. This improvement is notable given the increasing prominence of reasoning-specialized LLMs."}, "weaknesses": {"value": "1. The performance of LLMs appears to be primarily evaluated based on a single run using greedy decoding, which may compromise the reliability of the experimental results, as the responses of reasoning-specialized LLMs can deteriorate under greedy decoding [1]. To ensure a more robust evaluation, I recommend reporting the Avg@K metric, which averages performance scores over K independent runs under a common stochastic decoding setting (e.g., temperature = 0.6, top-p = 0.95).\n\n2. The LLMs employed in the ablation study are not consistent with those used in the main experiments. Specifically, the ablation study is conducted exclusively on Llama-3.1-8B-Instruct, whereas the main experiments primarily involve Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, L1-Qwen-7B-Max, and DeepSeek-R1-Distill-Llama-8B. This inconsistency undermines the comparability of the experimental results and may limit the validity of conclusions drawn from the ablation analysis.\n\n[1] Incorrect Baseline Evaluations Call into Question Recent LLM-RL Claims"}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DX1fgVN6rg", "forum": "25B8Se2kH1", "replyto": "25B8Se2kH1", "signatures": ["ICLR.cc/2026/Conference/Submission22688/Reviewer_nk9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22688/Reviewer_nk9F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967469830, "cdate": 1761967469830, "tmdate": 1762942337078, "mdate": 1762942337078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Logit-aware Final-block Quantization (LFQ), a method that enhances block-wise post-training quantization (PTQ) for large language models by aligning token probability distributions at the logit level. Unlike traditional block-wise PTQ methods that rely on mean squared error and neglect the LM head, LFQ minimizes cross-entropy between logits of the quantized and full-precision models to better preserve generative quality. Experiments on Llama 3.1 and 3.2 show LFQ consistently improves text generation performance while maintaining comparable language modeling and understanding accuracy to the full-precision baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies and addresses a key limitation of existing PTQ methods (i.e., the mismatch between quantized and full-precision token distributions.)\n\n2. Proposes a conceptually simple yet empirically robust method that yields consistent improvements across multiple architectures and benchmarks."}, "weaknesses": {"value": "1. LFQ shows weaker performance compared to some baselines on certain benchmarks. For example, it underperforms LoRA-based quantization error compensation (RILQ) on some understanding tasks, indicating trade-offs in adaptation.\n\n2. The work lacks analysis of computational overhead or convergence behavior during LFQ optimization, which could affect practical deployment scalability."}, "questions": {"value": "could the method generalize to activation quantization or mixed-precision scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "XxhROOaZTA", "forum": "25B8Se2kH1", "replyto": "25B8Se2kH1", "signatures": ["ICLR.cc/2026/Conference/Submission22688/Reviewer_ACQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22688/Reviewer_ACQf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762493348172, "cdate": 1762493348172, "tmdate": 1762942336697, "mdate": 1762942336697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}