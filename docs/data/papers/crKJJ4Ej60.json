{"id": "crKJJ4Ej60", "number": 24938, "cdate": 1758362220368, "mdate": 1759896741722, "content": {"title": "Copy-Paste to Mitigate Large Language Model Hallucinations", "abstract": "While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose \\textbf{CopyPasteLLM}, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2\\% to 24.5\\% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples—\\textit{1/50th} of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the \\textit{Context-Parameter Copying Capturing} algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation.", "tldr": "We propose CopyPasteLLM that trains models to simply copy from context, achieving 12.2-24.5% accuracy improvements with only 365 training samples (1/50th of baseline) and revealing how copy-paste recalibrates parameteric knowledge.", "keywords": ["RAG Hallucination", "Contextual Faithfulness", "Model Interpretability", "Large Language Model", "Knowledge Conflict"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/942c621d35e45ff23d09823ec1f1015dc180dcef.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CopyPasteLLM, a two-stage framework to mitigate contextual faithfulness hallucinations in retrieval-augmented generation (RAG) systems. The authors observe an inverse correlation between copying degree and hallucination density, motivating them to develop methods that encourage high-copying behavior. The first stage generates high-copying responses through three prompting paradigms (CP-Order, CP-Link, CP-Refine), while the second stage uses these responses as preference data for Direct Preference Optimization (DPO) training. The method achieves 12.2%-24.5% improvements on FaithEval using only 365 training samples. The authors also propose a Context-Parameter Copying Capturing algorithm that reveals the model recalibrates reliance on parametric knowledge rather than enhancing contextual representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong empirical results with exceptional data efficiency**: The method achieves significant improvements (12.2%-24.5% on FaithEval) using only 365 training samples, which is 50× fewer than the strongest baseline (Context-DPO with 18,000 samples).\n2. **Valuable empirical observation**: The inverse correlation between copying degree and hallucination density provides an interesting insight that could inspire further research in this area.\n3. **Comprehensive experimental evaluation**: The paper evaluates on multiple datasets (RAGTruth, FaithEval, PubMedQA, ConFiQA) across both counterfactual and original contexts, with multiple model architectures.\n4. **Practical and implementable approach**: The automated preference data construction pipeline and straightforward copying mechanisms make the method accessible for practical deployment.\n5. **Mechanistic insights**: The Context-Parameter Copying Capturing algorithm provides interpretable analysis of how the model balances contextual versus parametric knowledge during generation."}, "weaknesses": {"value": "1. **Idealized Assumption of Infallible Context:** The framework operates under the strong assumption that the retrieved context is always correct and sufficient. In real-world applications, retrieved information can be inaccurate, biased, or incomplete. By training the model to trust the context unconditionally, this method might inadvertently discourage a crucial capability: the ability to critically assess or question a flawed context. Treating the context as absolute truth may not always be desirable for robust and reliable AI systems.\n2. **Reliance on Lexical Metrics for Semantic Faithfulness:** The core metrics for copying, copy coverage ($\\kappa$) and copy density ($\\delta$), are based on lexical overlap. While empirically shown to be effective here, faithfulness is ultimately a semantic concept. It is possible for a response to have low lexical overlap but be a perfect, faithful paraphrase. Conversely, a response with high lexical overlap could subtly alter the meaning by changing a single critical word (e.g., \"not,\" \"only\"). The reliance on a lexical proxy may not fully capture the nuances of semantic fidelity.\n3. **Lack of Deeper Theoretical Grounding:** The paper presents a strong empirical correlation between high copy-degree and reduced hallucination but lacks a theoretical analysis to explain this causation more formally. Following the previous point, it would be beneficial to discuss why forcing lexical copying successfully translates into a preference for semantic faithfulness at a deeper level. How would this approach compare, for instance, to an alternative that uses a semantic similarity metric (instead of lexical overlap) as its primary optimization target during the preference generation stage (or combine lexical overlap and semantic similarity metric)?"}, "questions": {"value": "1. **Rationale for Training Data Size:** The choice of 365 training samples is quite specific. Could the authors clarify if this number has a particular significance (e.g., the size of a specific training split from a source dataset)? More importantly, have the authors investigated the performance scaling of CopyPasteLLM with respect to the number of preference pairs? It would be interesting to see if performance continues to climb with more data (e.g., 1k or 5k samples) or if it plateaus quickly, which would further underscore the method's data efficiency.\n2. **Generalizability Beyond RAG:** The copy-paste mechanism is inherently tied to the presence of a source context. Could the authors elaborate on the potential generalizability of this approach beyond the classic RAG setting? For instance, could the core principle be adapted to other tasks where faithfulness to a source is paramount, such as abstractive summarization (remaining faithful to the original article) or long-form dialogue (remaining consistent with conversation history)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8j2DtjTzO", "forum": "crKJJ4Ej60", "replyto": "crKJJ4Ej60", "signatures": ["ICLR.cc/2026/Conference/Submission24938/Reviewer_S4zF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24938/Reviewer_S4zF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557553415, "cdate": 1761557553415, "tmdate": 1762943252286, "mdate": 1762943252286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CopyPasteLLM, a two stage framework to reduce context unfaithful hallucination in RAG. In stage 1 they create high copying answers via three prompting models, CP-Order, CP-link, CP-Refine, while in the stage 2 they convert these candidates plus standard baselines into preference data using multi criteria filtering.\nCopying degree is quantified with Copy Coverage and Copy Density, adapted from Newsroom. The paper also introduces Context Parameter Copying Capturing, a token-level probe that compares with-context vs no-context decoding to attribute reliance to contextual versus parametric knowledge through top-k logits along chain of thought. Experiments on various datasets report large gains with only 365 samples for DPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The author clearly operationalize “copying” as a proxy for faithfulness by using κ, δ metrics which are defined cleanly and tied to an algorithmic fragment detector, the paper keeps the goal explicit: increase lexical reuse from context while balancing relevance and fluency.\n2) The proposed CP-Order, CP-Link, and CP-Refine span hard to soft constraints, making the Stage 1 data diverse yet biased toward high copying.\n3) The authors also show that how by using with just 365 pairs for DPO, CopyPasteLLM outperforms Context-DPO trained on 18k pairs and other fine-tuning baselines across counterfactual settings, with large margins on FaithEval; non-counterfactual accuracy also improves"}, "weaknesses": {"value": "1) Lexical copying is not faithfulness, κ, δ reward verbatim reuse and long spans and a response can copy irrelevant or misleading context sentences which yet still be unfaithful to the question. The paper partially addresses relevance via embeddings and perplexity, but κ, δ might elevate answers that are faithful to the wrong passage.\n2) Stage 1 filtering and Stage 2 tournament rely on AlignScore, MiniCheck, and a Qwen-32B judge, plus embedding similarity for relevance. These components are themselves models with biases and variance, having inter annotator agreement with human would be essential.\n3) The “stamping” step may entangle selection by copying degree with label assignment. Without ablations, it’s hard to tell whether gains come from copying behavior or from aggressive negative construction."}, "questions": {"value": "1) Table 3 states PubMedQA is evaluated on 20k samples, whereas in Table 4 it is mentioned size of PubMedQA as 1000, this needs some clarification.\n2) Authors should introduce some sort of confidence intervals to their results reported, as the current results are point wise, and CI should strengthen their claims, also across multiple seed values."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "twldalpXCK", "forum": "crKJJ4Ej60", "replyto": "crKJJ4Ej60", "signatures": ["ICLR.cc/2026/Conference/Submission24938/Reviewer_ujdf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24938/Reviewer_ujdf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978609508, "cdate": 1761978609508, "tmdate": 1762943252023, "mdate": 1762943252023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CopyPasteLLM, a two-stage framework designed to mitigate hallucinations in RAG systems by promoting high-copying responses. The authors hypothesize that directly copying context fragments, rather than reinterpreting them, improves contextual faithfulness and reduces hallucinations. The framework employs CopyPaste-Prompting methods to generate high-copying responses and utilizes DPO to internalize these preferences into the model. The proposed method is shown to achieve significant improvements in both counterfactual and original contexts, with data efficiency being a notable strength, requiring only 365 training samples compared to existing methods that need much more data. The authors also introduce Context-Parameter Copying Capturing, an interpretability tool that tracks model reliance on contextual vs. parametric knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The CopyPaste framework introduces a novel approach to addressing the long-standing issue of contextual faithfulness in RAG systems. By focusing on maximizing lexical reuse and reducing re-interpretation, the method offers a clear and intuitive solution to mitigate hallucinations.\n\n2. Despite only using 365 high-copying samples for training, it outperforms the best baseline models that require significantly larger datasets (e.g., 18000 samples). This makes the method potentially very practical for real-world applications with limited data.\n\n3. The authors provide extensive experimental evaluations on multiple datasets (FaithEval, PubMedQA) and across various models, demonstrating the robustness of CopyPasteLLM. The method achieves consistent performance improvements, particularly in challenging counterfactual scenarios, and significantly reduces hallucination rates."}, "weaknesses": {"value": "1. The proposed copy-paste strategy directly copies context fragments into the generated response rather than reinterpreting them. While this improves contextual faithfulness, there is a potential downside in terms of response length and noise. For instance, copying large portions of text verbatim may lead to excessively long responses or include irrelevant content, ultimately degrading the quality of the output. Additionally, generating high-copying responses could introduce significant computational overhead, particularly when dealing with longer or more complex contexts. I suggest that the authors discuss this potential issue in more detail and consider possible solutions to mitigate any associated computational costs.\n\n2. I believe the copy-paste strategy may be sensitive to the complexity and type of knowledge involved. The method could perform differently when dealing with simple factual information compared to complex or abstract knowledge, where strict copying might not be as effective. For example, copying highly specialized content might not always improve the quality of the response if the copied content is not sufficiently detailed or applicable to the query. The authors should include ablative experiments to analyze how the method performs with different types of knowledge (e.g., simple factual knowledge, abstract reasoning, or specialized technical knowledge) and how it behaves across tasks of varying complexity.\n\n3. The paper suggests that different extraction strategies (strictly extractive and softly refined) play an important role in improving contextual faithfulness. However, the paper does not provide enough detailed analysis of how these methods differ in performance. For example, CP-Refine adopts a soft constraint and involves an iterative refinement process, which might introduce different performance characteristics in terms of fluency and contextual accuracy. I recommend the authors conduct additional experiments to analyze the impact of these extraction methods more thoroughly, including the effect of the initialization approach and other experimental details on the results."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vA5XVT1o9P", "forum": "crKJJ4Ej60", "replyto": "crKJJ4Ej60", "signatures": ["ICLR.cc/2026/Conference/Submission24938/Reviewer_jspG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24938/Reviewer_jspG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982685145, "cdate": 1761982685145, "tmdate": 1762943251732, "mdate": 1762943251732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}