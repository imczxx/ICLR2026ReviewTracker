{"id": "PMKpyXk0FO", "number": 5150, "cdate": 1757856181553, "mdate": 1759897991736, "content": {"title": "MMTS-Bench: A Comprehensive Benchmark for Multimodal Time Series Understanding and Reasoning", "abstract": "Time series data are central to domains such as finance, healthcare, and cloud computing, yet existing benchmarks for evaluating various large language models (LLMs) on temporal tasks remain scattered and unsystematic. To bridge this gap, we introduce MMTS-Bench, a comprehensive multimodal benchmark built upon a hierarchical taxonomy of time-series tasks, spanning feature analysis, temporal reasoning, and cross-modal alignment. MMTS-Bench comprises 2,424 time series question answering (TSQA) pairs across 4 subsets: Base, InWild, Match, and Align, generated through a progressive real-world QA framework and modular synthetic data construction. We conduct extensive evaluations on closed-source, open-source LLMs and existing time series adapted large language models (TS-LLMs), revealing that: (1) TS-LLMs significantly lag behind general-purpose LLMs in cross-domain generalization, (2) LLMs show weaknesses in local tasks compared to global tasks, and (3) chain-of-thought (CoT) reasoning and multimodal integration substantially improve performance. MMTS-Bench not only provides a rigorous evaluation framework but also offers clear directions for advancing LLMs toward robust, interpretable, and generalizable time-series reasoning.", "tldr": "", "keywords": ["time-series benchmark", "multimodal time series understanding", "temporal reasoning", "large language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebf6e3d87d95db5ad3c1fb683f9281858a19e15a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors proposed a new time series dataset for benchmarking LLM/MLLM's ability to understand and reason about time series input. 2424 TSQA pairs are generated across four subsets of tasks and five different domains. Human-in-the-loop curation is used together with LLM to generate the QA pairs for real-world time series data and further being validated by experts. Comprehensive experiments are conducted including both opensource and close source LLM models, showing that current SOTA LLMs still fall back on understanding and reasoning for time series for certain tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "S1: The authors proposed a new multi-modal, multi-dimensional benchmark covering different domains and tasks.\nS2: Human-in-the-loop curation is used to ensure the dataset quality.\nS3: Extensive experiments are conducted and show interesting results."}, "weaknesses": {"value": "W1: The category of the datasets could be enhanced. For instance, the hardness of the question (easy or hard) and the length (100 points or 10000 points) and the number of the time series in the questions.\nW2: Experiments could be improved. For instance, there is no token cost / latency experiments."}, "questions": {"value": "Q1: The authors listed \"controllable synthetic data pipeline\" as one of the main contributions. However, this part is not significantly different from previous work, as the author wrote in the paper. Why do the authors treat it as one of the key contribution.\nQ2: The authors concluded that TS-LLMs significantly lag behind general-purpose LLMs in cross-domain generalization. Are there any tasks TS-LLMs perform better? I guess they would perform better when the input time series contains very long context and multiple input time series. However, there are no such experiments.\nQ3: The authors wrote \"on InWild, humans achieve 67% accuracy\", why do humans achieve lower accuracy than GPT-5, when they label the datasets?\nQ4: It seems that all models perform worse in Base QA. Why is this the case? All models perform worse on synthetic datasets.\nQ5: The description of a time series is generated by LLM. Then it is used to evaluate ability of LLM to judge alignment. Is alignment task evaluation valid? \nQ6: The authors use prompt to evoke CoT. Did the author try thinking mode?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "S2qNMFJGTl", "forum": "PMKpyXk0FO", "replyto": "PMKpyXk0FO", "signatures": ["ICLR.cc/2026/Conference/Submission5150/Reviewer_Ay4A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5150/Reviewer_Ay4A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761118202394, "cdate": 1761118202394, "tmdate": 1762917911593, "mdate": 1762917911593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new dataset for multi-modal time-series and text understanding, addressing an important gap in existing datasets by combining temporal data with natural language question-answering tasks. The dataset is organized into several categories, with a combination of both synthetic and real datasets\n\nHowever, the dataset has quality and methodological concerns. The Base category relies heavily on synthetic data generation methods that closely follow existing work. More importantly, the InWild and Align categories depend on LLM-generated descriptions. With this in mind, examination of the provided anonymous repo reveals that descriptions for multi-variate datasets often fail to capture their multi-dimensional nature, hence contradicting the paper's claims about handling such complexity. Furthermore, the Align task descriptions are also overly specific and distinguishable, hence explaining the near-perfect model performance and suggesting that the task may be too easy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A dataset focusing on multimodal time-series and text is highly valuable, as significant gaps still exist in current datasets addressing this area."}, "weaknesses": {"value": "- **Base Category:** This category of the dataset relies on synthetic time-series data generated using preset trends, noise, and seasonality components to synthesize QnAs. The process closely follows the approach found in [Ref] (see Fig. 4 in [Ref]). Additionally, for this dataset category, the authors mention 17 expert-designed templates to create diverse, well-structured QnAs. However, there is no reference or detailed description of these templates provided anywhere in the paper.\n\n[Ref]: Zhe Xie, Zeyan Li, Xiao He, Longlong Xu, Xidao Wen, Tieying Zhang, Jianjun Chen, Rui Shi, and Dan Pei. 2025. *ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning.* Proc. VLDB Endow. 18, 8 (April 2025), 2385–2398. https://doi.org/10.14778/3742728.3742735\n\n- **InWild Category:** This category instead considers realistic datasets, which is important for capturing real-world behavior. The QnAs in this category rely on LLMs to generate descriptions. However, since LLMs are not yet strong at understanding time-series data (hence the motivation for multimodal models), the accuracy of these QnAs depends heavily on expert validation. It would be helpful to discuss what types of issues experts identified during validation and how frequently these occurred.\n\n- **Quality of the dataset:** The InWild and Align categories depend heavily on LLM-generated descriptions. However, closer inspection of the anonymous GitHub repository reveals several issues.\n  - First, for multi-variate datasets (e.g., the climate dataset in Appendix A.2), the descriptions fail to capture the multi-variate aspect that the paper claims to emphasize. For example:\n    > \"This series demonstrates irregular cyclical behavior with multiple peaks and valleys, showing significant amplitude variations across different time intervals.\"\n\n    Such a description does not reflect the multi-variate nature of the data, raising questions about how well the dataset captures this complexity as claimed by the authors on multi-dimensionality.\n  - Similarly, in the Align section (e.g., `ts2caption_qa_120.csv`), many descriptions focus on very specific events, such as:\n    > \"This time series shows an overall upward trend from around 55 to 72, with notable volatility and several significant peaks and valleys throughout the sequence.\"\n\n    These descriptions make the Align task relatively easy, as the differences between descriptions are clear and unambiguous. This could explain the near-perfect model performance on this task. This suggests potential quality issues in the Align category of the dataset.\n\n- **Hierarchy categorization:** The authors describe their dataset as having a hierarchical taxonomy of time-series tasks with an orthogonal structure, spanning from basic perception to advanced reasoning (page 2). However, it is unclear why these are considered orthogonal. For instance, structural awareness (the Base category) seems conceptually related to Temporal Reasoning (the InWild category). Thus, the rationale for this hierarchical and orthogonal categorization is not well justified.\n\n**Minor comments:**\n\n- Typo: “a innovative” → “an innovative.”\n- Figure 3: The arrow is tilted and should be fixed.\n- Presentation: The paper relies heavily on the reader to navigate appendices. It would improve readability to include explicit references to the relevant appendices throughout the main text."}, "questions": {"value": "Please refer to the Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c46vv6rtxB", "forum": "PMKpyXk0FO", "replyto": "PMKpyXk0FO", "signatures": ["ICLR.cc/2026/Conference/Submission5150/Reviewer_w7Fz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5150/Reviewer_w7Fz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202268317, "cdate": 1761202268317, "tmdate": 1762917910416, "mdate": 1762917910416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MMTS-Bench, a comprehensive and multi-dimensional benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in understanding and reasoning over time series data. The proposed benchmark comprises 2,424 question-answer pairs, systematically organized into four subsets (Base, InWild, Match, and Align). These subsets are designed to assess five core task dimensions: structural awareness, feature analysis, temporal reasoning, sequence matching, and cross-modal understanding. The data is sourced from both synthetically generated series and real-world domains (e.g.,transport, climate, economics, healthcare). The authors  further  conduct an extensive evaluation of a wide range of open-source, closed-source, and time-series-specialized LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed five-dimensional taxonomy (structural awareness, feature analysis, temporal reasoning, sequence matching, and cross-modal understanding) provides a systematic and hierarchical framework for evaluating different aspects of reasoning over time-series data.\n2. Another strength is the use of real-world time series from five diverse domains: Transport, Cloud Operations, Climate, Economics, and Healthcare. This diversity is crucial for rigorously assessing model generalization and robustness across different data distributions and real-world scenarios.\n3. The evaluation of models across different categories (closed-source, open-source, TS-LLMs) and modalities (text, vision, vision+text) provides a holistic view of the landscape."}, "weaknesses": {"value": "1. Since all tasks are multiple-choice, the evaluation mainly reflects recognition accuracy rather than deeper reasoning or explanatory ability. It would be helpful if future versions could include a small number of open-ended or explanation-based questions to capture richer reasoning behaviors.\n2. Some tasks, particularly in the *InWild* and *Align* subsets, appear to rely on textual descriptions derived from the same data as the QA pairs.  It might be worth clarifying how potential textual cues are controlled to avoid trivial answer hints or information leakage.\n3. Current metrics (mostly multiple-choice accuracy) capture correctness but not reasoning depth or interpretability. Additional analyses—such as reasoning trace evaluation or error categorization—would better support the claim of “understanding and reasoning.”\n4. The paper includes a CoT ablation showing a 10–20% improvement, which is encouraging. It might be helpful if some qualitative examples or error analyses (e.g., distinguishing logical vs. perceptual errors) were added, as these could provide deeper insight into how models reason through the tasks."}, "questions": {"value": "1. Can MMTS-Bench be extended for generative reasoning tasks (open-ended answers) rather than multiple-choice only?\n\n2. How do you ensure that QA pairs in InWild and Align subsets do not leak cues from textual descriptions that trivially hint at the answer?\n\n3. Section 3.4 mentions human verification, but it might be useful to provide a quantitative measure of annotation reliability—such as agreement rates or inter-annotator consistency—to strengthen confidence in the dataset’s quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YynZwwXOXv", "forum": "PMKpyXk0FO", "replyto": "PMKpyXk0FO", "signatures": ["ICLR.cc/2026/Conference/Submission5150/Reviewer_cxqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5150/Reviewer_cxqH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821632485, "cdate": 1761821632485, "tmdate": 1762917909875, "mdate": 1762917909875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMTS-Bench, a multimodal benchmark for evaluating Large Language Models (LLMs) on time series understanding and reasoning tasks. The benchmark comprises 2,424 question-answering (QA) pairs across four subsets (Base, InWild, Match, and Align), covering five orthogonal task dimensions: structural awareness, feature analysis, temporal reasoning, sequence matching, and cross-modal understanding. The authors propose a hierarchical task taxonomy and employ a three-stage framework for real-world QA generation combined with synthetic data construction. They conduct extensive evaluations on closed-source, open-source, and time-series-adapted LLMs, revealing that general-purpose LLMs outperform TS-LLMs in cross-domain generalization, and that CoT reasoning and multimodal fusion significantly improve performance.\n\nWhile the work addresses an important problem in time series LLM evaluation, several concerns limit its contribution: the distinction from prior benchmarks is not sufficiently clear, the dataset size is relatively small compared to existing work, and the benchmark lacks coverage of high-dimensional, long-sequence industrial time series that are common in real-world applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Hierarchical Task Taxonomy**: The multi-dimensional task classification framework with five orthogonal dimensions is well-structured and provides a systematic way to organize time series understanding tasks. The theoretical combination of 286 fine-grained task types demonstrates thoughtful design.\n2. **Comprehensive Evaluation**: The authors evaluate a wide range of models (closed-source, open-source, and TS-LLMs) across different modalities (text-only, vision-only, and multimodal), providing valuable insights into current model capabilities and limitations.\n3. **Quality Control**: The human-in-the-loop curation process with expert verification helps ensure dataset quality, as demonstrated by the experiments showing that expert review effectively prevents question errors and answer leakage.\n4. **Practical Insights**: The finding that CoT reasoning provides greater improvements than parameter scaling, and that multimodal fusion enhances performance, offers actionable guidance for future development.\n5. **Clear Methodology**: The three-stage progressive framework for real-world QA generation and the modular synthetic data construction pipeline are well-described and reproducible.\n\n---"}, "weaknesses": {"value": "1. **Limited Distinction from Prior Work**: The paper does not clearly articulate how MMTS-Bench significantly differs from or improves upon existing benchmarks (ChatTime-TSQA, Time-MQA, ChatTS, EngineMT-QA, etc.). While Table 6 provides a horizontal comparison, it lacks a clear summary highlighting what makes MMTS-Bench uniquely valuable. The authors should provide a more explicit comparison table or section that directly contrasts:\n\n   - Dataset sizes and scales\n   - Task coverage and taxonomy depth\n   - Evaluation dimensions\n   - Construction methodologies\n   - Unique contributions of MMTS-Bench\n2. **Limited Dataset Scale**: With 2,424 QA pairs, MMTS-Bench is relatively small compared to existing datasets:\n\n   - ChatTime-TSQA: 48K pairs\n   - Time-MQA-TSQA: 200K pairs (1.4K for testing)\n   - EngineMT-QA: 11K pairs\n\n   This limited scale may constrain the benchmark's ability to evaluate model performance comprehensively, especially for fine-grained capability assessments. The authors should discuss whether this size is sufficient for robust evaluation or provide justification for the chosen scale.\n3. **Insufficient Coverage of High-Dimensional, Long-Sequence Industrial Time Series**: Real-world industrial time series often exhibit:\n\n   - High dimensionality (tens to hundreds of variables)\n   - Long sequences (thousands to millions of time points)\n   - Complex temporal dependencies across multiple scales\n\n   However, MMTS-Bench primarily covers:\n\n   - Low-dimensional cases: Most datasets have 1-2 variables (only Climate has 4 variables)\n   - Moderate sequence lengths: Maximum length of 1,728 points (CloudOps), with most datasets having sequences of 100-700 points\n\n   The benchmark's current design may not adequately assess LLMs' capabilities on the types of industrial-scale time series that practitioners encounter. The authors acknowledge extending to longer sequences in future work, but this limitation significantly affects the benchmark's applicability to industrial settings.\n4. **Incomplete Comparison Analysis**: While Table 6 provides some comparison, it lacks quantitative metrics comparing:\n\n   - Task diversity and granularity\n   - Domain coverage\n   - Difficulty distribution\n   - Evaluation rigor (e.g., statistical robustness measures)\n5. **Limited Discussion on Dataset Selection**: The paper does not sufficiently justify why certain domains were selected over others, or how the chosen domains represent the diversity of real-world time series applications. The absence of certain critical domains (e.g., manufacturing, finance with high-frequency data, IoT sensor networks) limits generalizability.\n6. **Missing Evaluation of Scalability**: The benchmark does not systematically evaluate how model performance scales with sequence length or dimensionality, which would be valuable for understanding practical applicability."}, "questions": {"value": "* **Comparative Analysis**: Could the authors provide a more detailed comparison table or section that explicitly highlights:\n  - What specific capabilities MMTS-Bench evaluates that prior benchmarks do not?\n  - How the hierarchical taxonomy differs from the \"flat\" taxonomies in prior work?\n  - What quantitative advantages MMTS-Bench offers (e.g., task coverage, evaluation rigor)?\n* **Dataset Scale Justification**:\n  - What methodology was used to determine that 2,424 pairs is sufficient for comprehensive evaluation?\n  - Have the authors conducted power analyses or similar statistical assessments?\n  - What is the minimum dataset size required for reliable evaluation across different subtasks?\n* **Industrial Applicability**:\n  - What is the plan for extending the benchmark to high-dimensional (e.g., 50+ variables) and long-sequence (e.g., 10K+ points) cases?\n  - Can the authors provide preliminary experiments or analysis on how current models perform on such challenging cases?\n  - Would it be feasible to add an additional subset specifically targeting industrial-scale time series?\n* **Task Taxonomy Validation**:\n  - How was the five-dimensional orthogonal structure validated? Are there empirical studies demonstrating that these dimensions are indeed orthogonal?\n  - What is the coverage of the 286 theoretical task combinations in the actual dataset?\n* **Cross-Dataset Generalization**:\n  - Have the authors evaluated whether models trained on MMTS-Bench generalize to other time series datasets?\n  - What is the benchmark's utility for model development vs. evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m5p5c0oddU", "forum": "PMKpyXk0FO", "replyto": "PMKpyXk0FO", "signatures": ["ICLR.cc/2026/Conference/Submission5150/Reviewer_VshB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5150/Reviewer_VshB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5150/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921726944, "cdate": 1761921726944, "tmdate": 1762917909596, "mdate": 1762917909596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}