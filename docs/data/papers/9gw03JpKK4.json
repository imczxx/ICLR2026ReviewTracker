{"id": "9gw03JpKK4", "number": 17177, "cdate": 1758273082219, "mdate": 1759897192264, "content": {"title": "Gaia2: Benchmarking LLM Agents on Dynamic and  Asynchronous Environments", "abstract": "We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the “sim2real” gap. Gaia2 is built on the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2, we aim to provide the community with a foundation for developing, benchmarking, and training the next generation of practical agent systems.", "tldr": "Gaia2 evaluates LLM agents in asynchronous, dynamic environments with action-level verification, revealing fundamental trade-offs between reasoning, speed, and robustness.", "keywords": ["benchmark", "agents", "rlvr", "multi-agent systems", "reasoning", "large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f4c08bd0bcf4e076ea5a67f3a6f96dc6571cbbe7.pdf", "supplementary_material": "/attachment/1e72cc44d6a580dccea7f30aada42d8ce7e3f23a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. The results from this paper highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the “sim2real” gap. Gaia2 is built on the open-source Agents Research Environments platform and designed to be easy to extend."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1, I think the experiments are comprehensive, including all the state of arts models\n2, The contribution makes sense to me including releasing Agents Research Environments, a general-purpose platform for\nbuilding asynchronous, event-driven benchmarks that support scalable evaluation and data generation for RL; introducing Gaia2, the first benchmark unifying asynchronous execution, temporal reasoning, noise robustness, ambiguity resolution, and multi-agent collaboration under a verifiable evaluation framework directly usable for RLVR; evaluating leading proprietary and open-source models on Gaia2, exposing fundamental trade-offs between reasoning strength, efficiency, robustness, and cost."}, "weaknesses": {"value": "To be honest, I don't see any weakness of this paper. I really think all the experiments make sense to me and are comprehensive."}, "questions": {"value": "No question. Good paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FK47q75Ftq", "forum": "9gw03JpKK4", "replyto": "9gw03JpKK4", "signatures": ["ICLR.cc/2026/Conference/Submission17177/Reviewer_8Eq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17177/Reviewer_8Eq1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760796859740, "cdate": 1760796859740, "tmdate": 1762927158374, "mdate": 1762927158374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Gaia2, a benchmark and platform for evaluating LLM agents in dynamic, asynchronous environments using action-level verification. Gaia2 simulates smartphone-like environments with realistic apps and event timelines, testing seven essential agent capabilities such as execution, temporal reasoning, robustness to noise, ambiguity resolution, and multi-agent collaboration. Scenarios are built as event and action DAGs and evaluated by a custom ARE Verifier. Experiments across many leading models reveal that no single agent excels in all areas, and that inference speed can be a bottleneck for time-sensitive tasks. Gaia2’s fine-grained verification and open-source framework support RLVR training and future extensible research in agent evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "High-Quality, Extensible ARE Platform: The paper introduces the Agents Research Environments (ARE) platform, an open-source and extensible framework for building and testing agents. Simulating a smartphone environment with numerous apps and tools, it provides a robust foundation for community-driven benchmarking.\n\nAsynchronous Design Enables Temporal Awareness: A key innovation is the shift from static to dynamic, asynchronous environments. By having events occur independently of the agent, the benchmark can evaluate critical, real-world capabilities like temporal awareness, responsiveness, and the ability to operate under time constraints, revealing how model latency directly impacts task success.\n\nReliable, Fine-Grained Action-Level Verification: The paper proposes a \"write-action verifier\" that evaluates each state-changing action against a human-annotated oracle graph. This verifier is shown to be highly reliable (0.98 agreement with human labels, 0.99 precision) and moves beyond simple final-answer checks. This fine-grained approach not only provides robust evaluation but also makes the benchmark directly usable for RLVR."}, "weaknesses": {"value": "Dependence on a Single Orchestration Scaffold: The evaluation (Section 5) uses a single, \"simple ReAct-like scaffold\" for all models. It's unclear how much the reported performance is bottlenecked by this specific orchestration choice versus the model's intrinsic capabilities. The paper itself notes (Section 5.2) that its \"single-threaded scaffold\" cannot handle \"concurrent actions.\"\n\nLimited Scope (Mobile-Only): The paper claims to provide \"a foundation for developing, benchmarking, and training the next generation of practical agent systems\" (Abstract). However, the benchmark is currently limited to a simulated \"Mobile\" environment. While this is a complex and valuable domain, this scope is narrow for such a broad claim, and it's unclear how these findings would translate to other common agentic environments (e.g., desktop operation, web browsing).\n\nUnclear Agent2Agent Mechanism: The Agent2Agent (A2A) split (Section 4.1, 5.3) introduces \"app-agents\" that replace native tools, forcing collaboration. However, the mechanism for their creation and operation is not fully detailed. The findings (collaboration helping weak models but not strong ones) are interesting but difficult to interpret without a clearer understanding of the sub-agent's lifecycle.\n\nVerifier Fragility: The paper commendably includes a discussion of reward hacking (Appendix B.2.3), where an agent learned to exploit the LLM-based \"soft check.\" While the authors mitigated this, it underscores the inherent fragility of any evaluation pipeline that still relies on an LLM as a judge, even in a hybrid system."}, "questions": {"value": "How might the results change with more sophisticated orchestration instead of just ReAct? Is the benchmark senstive to this?\n\nWhere do these sub-agents come from?\n\nThe paper mentions mitigating the observed reward hacking by adding a \"style\" check. Could the authors elaborate on this? Do they anticipate other, more subtle forms of hacking? What is the long-term strategy for ensuring verifier robustness as agents become more adept at gaming these systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RgNpogGUi3", "forum": "9gw03JpKK4", "replyto": "9gw03JpKK4", "signatures": ["ICLR.cc/2026/Conference/Submission17177/Reviewer_QMSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17177/Reviewer_QMSH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991535736, "cdate": 1761991535736, "tmdate": 1762927157800, "mdate": 1762927157800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Gaia2, a novel benchmark for LLM agents, and the underlying platform Agents Research Environments (ARE) building it. This paper's contributions are threefold, aimed at advancing the evaluation of LLM agents in dynamic environments and realistic scenarios.:\n\n- First, it open-sourced the ARE platform for building and running agentic evals. ARE is designed from the ground up for building asynchronous, event-driven benchmarks while also support scalable evaluation and data generation for Reinforcement Learning (RL).\n\n- Second, the paper presents the Gaia2 benchmark. Gaia2 is the first benchmark to unify a set of critical, real-world agent capabilities under one evaluation, including asynchronous execution, temporal reasoning, robustness to environmental noise, ambiguity resolution, and multi-agent collaboration.\n\n- Third, the authors conduct a comprehensive empirical study on Gaia2 using a suite of leading models. This study exposes fundamental trade-offs between agent reasoning strength, efficiency (latency), robustness, and operational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- True Asynchronicity and Temporal Dynamics enabled by the async nature of the ARE platform and the event driven/DAG design for constructing more tass.\n\n- A Scalable, Open-Source Platform (ARE) that allows the community to build new, more complex scenarios on top of this work, ensuring its long-term relevance.\n\n- Well-Designed Verifier for RLVR: The new action-level verifier is proven to be effective, achieving much higher precision and recall than a LLM-only judge.\n\n- The analysis of cost-performance-time trade-offs provides a practical, multi-dimensional view of agent performance that is highly relevant for real-world deployment.\n\n- The benchmark is not saturated and successfully identifies clear weaknesses in current SOTA models. The low scores on Noise, Ambiguity, and Agent2Agent splits provide a clear roadmap for future research."}, "weaknesses": {"value": "- The verifier is designed to check against a \"minimal oracle sequence\" of write actions, which might be brittle in some cases. It appears to evaluate path optimality rather than goal completion. An agent that makes a mistake, self-corrects (e.g., books the wrong cab, cancels, books the correct one), and ultimately reaches the correct state would be marked as a failure. \n\n- The \"inverse scaling\" result might be confounded by the agent's orchestration. Given that all models use a single threaded simple ReAct-like scaffold, this might conflate model inference latency with scaffold inefficiency. A more advanced orchestration (e.g., parallel tool calls, or a planner that coordinate) might allow a \"slow\" model to succeed. And overall the extremely low score on this split seem more for consideration under specific circumstance rather than general capability limitation.  \n\n- The capability taxonomy is somewhat muddled, mixing action types (Execution, Search) with problem constraints (Time, Noise). While the design to stress test one capability in isolation is reasonable. Real-world tasks are almost never this clean; they require a composition of skills and the lack of a \"Compositional\" split is a weak point"}, "questions": {"value": "- Could the verifier be modified to support \"goal completion\" rather than \"path optimality\"? How would you propose handling valid, non-minimal trajectories that involve agent self-correction? Overall feel it might be helpful to better structure the categorization of the capability, and some break down/analysis on individual categories, e.g., does task complexity/length affect the performance on execution/search\n\n- Given that the current scenarios test capabilities in isolation, why not adding compositional split or there are some tests showing it is not needed?\n\n- Agent2Agent setting seem to be a bit different than the other settings, which can be used to evaluated both the main agents' planning and coordination skill, especially given partial observability of the action space, as well as the sub-agents ability on effectively communicate with the main agent and complete task, the experiments and design for this part could worth further exploration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8IkQ4gVHj", "forum": "9gw03JpKK4", "replyto": "9gw03JpKK4", "signatures": ["ICLR.cc/2026/Conference/Submission17177/Reviewer_cZHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17177/Reviewer_cZHA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248043757, "cdate": 1762248043757, "tmdate": 1762927157548, "mdate": 1762927157548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}