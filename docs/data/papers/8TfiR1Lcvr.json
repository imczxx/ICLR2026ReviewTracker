{"id": "8TfiR1Lcvr", "number": 19807, "cdate": 1758299538757, "mdate": 1759897018215, "content": {"title": "Observational Auditing of Privacy", "abstract": "Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset—for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead.  We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.", "tldr": "", "keywords": ["privacy auditing", "label differential privacy", "protected attributes"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3162b0b50b1fea2e0a51f905735f8e2f447aa64b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces an observational privacy auditing framework for evaluating differential privacy (DP) guarantees without modifying or re-running the training pipeline. Unlike interventional audits that inject canaries or retrain models on modified datasets, the proposed approach leverages natural variation in the data and a proxy label-generating distribution to construct a post-training attribute/label inference game. The framework is formalized through simulation-based DP for protected attributes, an imputation-based simulator, and an observational attribute inference game (Algorithm 2). Theoretical results provide auditing guarantees both in the ideal case (no distribution shift) and under bounded distributionla shift between the true and proxy distributions. Experiments on CIFAR-10 and Criteo illustrate that the method is competitive with interventional baselines while requiring no training-time intervention. Overall, the paper aims to lower the engineering barrier of auditing Label-DP mechanisms in practical settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Practical relevance and usability:** The approach avoids training-time interventions (no canaries, no retraining), which significantly lowers engineering overhead and makes the audit feasible for third-party or post-hoc evaluation settings.\n- **Clear formalization of observational auditing:** The use of simulation-based DP for protected attributes, together with the imputation-based simulator and the observational game, provides a principled lens for analyzing privacy leakage without data modification.\n- **Addresses real-world setting of distribution shift:** The theoretical treatment of bounded distribution shift is both interesting and important, as it reflects realistic deployment scenarios where training and proxy distributions may drift.\n- **Empirical evaluation is fairly comprehensive:** The paper covers two qualitatively different domains (vision and large-scale tabular data) and several Label-DP mechanisms, and includes synthetic randomized response experiments to illustrate tightness and limitations.\n- **Potential impact for practice:** If validated further, the technique could make privacy auditing more accessible to practitioners who cannot modify training workflows or re-run large models."}, "weaknesses": {"value": "- **Scope of presentation regarding Label-DP vs. general DP:**  The paper focuses on Label-DP, a more narrow neighboring relation. While the framework may extend more broadly, the current exposition remains Label-DP centric. Providing more explicit guidance on how it generalizes to standard record-level DP could broaden its relevance.\n\n- **Choice of presenting both $(\\varepsilon,\\delta)$-DP and $f$-DP results:**   Both formalisms are presented, but the benefits of including both are not fully explained. Since $f$-DP can subsume $(\\varepsilon,\\delta)$: it is well known that by taking the convex conjugate of an $f$-DP, you can recover the $\\epsilon, \\delta(\\epsilon)$ curve, see Dong et al. (2022) Proposition 2.12. Furthermore, the authors provide proof only for the $(\\epsilon, 0)$ case, claiming that the $(\\epsilon, \\delta)$ proof follows from the original proof. I would personally stick with the f-DP and present it in detail, as it is essentially duplicating information without additional value for the reader.\n\n- **Unclear impact of $M'$** : The impact of $M'$ is largely underexplored experimentally. What does it mean to have a \"good\" $M'$, what's the impact of not having a good enough one and how it reflects into the lowerbound is largely undiscussed.\n\n- **Assumptions required for observational auditing are not clearly highlighted:**   Beyond access to a proxy distribution $D'$, the implicit assumptions (e.g., on the adversary’s knowledge, calibration/quality of $M'$, independence assumptions) are not clearly stated. A concise assumptions summary would make the framework more transparent.\n\n- **Positioning relative to prior “security game” formulations:**  The paper claims to reframe auditing as a security game, but prior works  also use game-based formulations. Clarifying what is novel in this framing would avoid confusion.\n\n- **Generality claim around neighboring relations (Lines 143–147):**  The claim that the simulator game generalizes add/remove, leave-one-out, or zero-out relations could be seen as overstated. The simulator still implicitly encodes a neighboring relation, this could be clarified.\n\n- **Heavy reliance on background knowledge:**   The paper depends on familiarity with ideas from Steinke et al. (2024) and Mahloujifar (2025). Brief intuitive overviews would help make the paper more self-contained, for example, how refusing to decide on the membership actually aids auditing.\n\n- **Explaining observed gaps between audit and theoretical upper bounds:**  The paper does not sufficiently discuss why the empirical lower bounds are sometimes noticeably below theoretical ones. Additional interpretation would benefit readers.\n\n- **Assumptions for $f$-DP auditing prior:**  For f-DP auditing, you mention you pick as a prior that you expect to see a Gaussian DP tradeoff curve. How is this valid/justified? For auditing a single subsampled mechanism or a pure-DP mechanism, this is clearly incorrect. Furthermore, you do not clearly highlight the need for this before the appendix (lines 735-740). This seems an essential note for your auditing technique. Furthermore, if I want to audit the composition of a Gaussian and an RR mechanism, there is no clear/known structure you could know beforehand (assuming that the adversary only observes the output of some composition). Can the authors clarify this?\n\n- **Unclear behaviour under distributional shifts:** From my point of view, the central contribution of this paper is actually handling the distribution shifts. Still, it is a bit unclear how to integrate this idea into practice, and the paper does not fully explore it. Given some practical scenario, it is unclear how to upperbound the distributional shift, as we don't have access to $D | x$. The paper does not provide practical estimators, proxies, or diagnostics to quantify the change between the real data and the proxy distribution. Also,  it would be helpful to show audit quality as a function of artificially increasing divergence (e.g., perturbing D′ in a controlled way).\n\nMinor points\n-  Some terms (e.g., “PPML”) appear without definition.\n- Showing $(\\varepsilon,\\delta(\\varepsilon))$ curves (or multiple $\\delta$ values such as $10^{-1},10^{-2},10^{-3} \\cdots)$ would provide a more complete picture, as suggested by Gómez et al. (2025).\n\n- Algorithm 1 is quite abstract and introduces notation that is not really reused afterwards. It may be better suited for the appendix or supported with a running example to improve readability.\n\n- Some Label-DP mechanisms are non-standard. A short summary or table of the mechanisms and their structure in the Appendix would aid reader understanding, especially for the experimental section.\n\n- Clarity of paragraph around lines 085–090:  The messaging here is hard to follow and could benefit from restructuring for clarity.\n\n\nGomez et al., 2025: https://arxiv.org/abs/2503.10945\n\nDong et al., 2022: https://arxiv.org/abs/1905.02383\n\nNasr et al. 2021: https://arxiv.org/abs/2101.04535"}, "questions": {"value": "- What concrete steps are required to generalize this framework beyond Label-DP to standard record-level DP?  A short discussion or example would help clarify applicability beyond the label setting.\n\n- Could the authors clarify the specific novelty of their security-game formulation compared to earlier works such as Nasr et al. 2021?\n\n- Can the authors explicitly list all assumptions required for their observational auditing guarantees (beyond access to a proxy distribution $D'$)?\n\n- How should practitioners choose or evaluate the proxy model $M'$?   For example, how good does $M'$ need to be for the audit to provide a meaningful bound, and how can one detect when a proxy is too weak?\n\n- The gap between the audit and the theoretical upper bound is sometimes significant in the experiment, but the authors provide minimal context for why that is happening. Is it because of the audit, or is the upper bound loose? Can other (that are not necessarily one-run) auditing techniques perform better? If yes, how well? All these questions should be clarified.\n\n- Is it possible to avoid assuming a Gaussian prior for $f$-DP auditing, or to provide alternatives for mechanisms where Gaussianity is not a natural fit? (see the Weaknesses section)\n\n- How does your guarantee behave under different numbers of abstentions from a decision by the adversary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RXWhCkwAWI", "forum": "8TfiR1Lcvr", "replyto": "8TfiR1Lcvr", "signatures": ["ICLR.cc/2026/Conference/Submission19807/Reviewer_5SJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19807/Reviewer_5SJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392982585, "cdate": 1761392982585, "tmdate": 1762931664886, "mdate": 1762931664886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Differential privacy (DP) is a privacy measure of a mechanism - a randomized function from a set of data points to some output. Its privacy is measured by the ability of an adversary to distinguish between two possible input datasets differing in a single element (referred to as neighbors), based on a single output of the mechanism. The better the privacy, the close is the success rate of the most sophisticated analyst to a random guess. This framing as an hypothesis testing naturally lends itself to an empirical lower bound scheme; select two neighboring datasets, repeatedly run the mechanism on one one of the two datasets selected at random, and guess the identity of the used dataset based on the mechanism's output. The more accurate the guesser, the lower the privacy. Recently, a more efficient method has emerged, where rather than repeating many time the experiment to determine a change in a single element, the mechanism is called once and the auditing attempts to identify the participation of many elements, but the general idea remains.\n\nThis work considers a combination of several modifications over the the undermentioned recent baseline. First, it considers the restrictive case where the training set cannot be altered, and instead the auditing is carried between elements independently sampled from the same distribution. Second, it generalizes to the case where the elements not used for training were not sampled from the same distribution but from a similar one. Finally, it generalizes the results from classical DP which is defined with respect to a change of a single element, to label-DP where part of the elements attributes are known, and the change applies only to a subset of features referred to as labels.\n\nFor this setting the authors provide theoretical guarantees which generalize previously known results, and report some empirical results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "All three generalization proposed by this work are natural useful goals, and this work sufficiently motivates them.\nUnfortunately, I found the presentation confusing and the results either vacuous or suspicious (as detailed in the next section), so I find it hard to elaborate more on these strengths.\n\nOne clear contribution I've managed to extract from this work is the fact that in random elements-based auditing of label-DP, the resampling of the alternative labels can be done using a distribution close to the original one, for which the authors extended the lower bound guarantee by Steinke et al.."}, "weaknesses": {"value": "The main challenge I've experienced in understanding this work is the relation between the three generalization directions discussed above. In fact, I am not even sure the authors will agree with the way I represented their contribution as three separate generalizations, since they are represented in a way that gives the impression they are all interconnected, but I don't see that connection.\n\n* From the problem setting perspective, I do not understand how does the supposedly new observational auditing method differ from a simple setting where $2 n$ points are sampled from $\\mathcal{D}$, they are matched in pairs, from which one is added to the training set as random. The authors seem to consider the option of \"excluding a subset of the data from training\" as \"severely restricting applications of these auditing methods\", while their proposal is essentially identical. The only difference is that they represent it as a sampling of $n$ elements for the training set followed by sampling of $n$ additional elements, which is indistinguishable from sampling $2n$ elements and using a random subset of size $n$.\n* On the theoretical level, the contribution is not clear to me. The Label DP aspect cannot affect the privacy guarantees, since the public features are akin to any other auxiliary information publicly known to the mechanism and auditor alike. The restriction to canaries sampled from the underlying distribution rather than adversarial constructed only weakens the auditing power, so I fail to understand what does what is the purpose of Theorem 5 if it is simply a special case of the known result by Steinke et al. (again, using the perspective where we first sampled $2n$ elements then selected a subset at random).\n* On the experimental front I find the results surprising and somewhat suspicious. I will focus on the CIFAR10 experiment, which also appeared in previous works. Since both training and auditing examples are sampled according to the (nearly) same distribution, and the accuracy of the non-private model is $\\approx 90\\\\%$, the public $x$ nearly determines the label $y$ so for $> 80\\\\%$ of data points the labels will be identical. Under such conditions, the auditing power of the random elements with known features $x$ must be significantly weaker than that of random elements where both $x$ and $y$ are resamples, but as figures 8 and 9 in Steinke et al. indicate, even changing the entire elements makes it very hard to achieve reasonable bounds, so I cannot understand how using only label DP with no adversarial canaries can reach such a (relatively) tight bound of $\\epsilon=1$. I am aware of the fact the audited learning algorithm is different, but this should provide a useful rule of thumb. I suspect this might have to do with the following statement, which might lead to incorrect statistical analysis \"We sweep $c' \\in \\{1,2,...,100\\}$ and report the highest $\\epsilon$ achieved...\". For example, does the analysis take the multiple hypotheses into account?\n\nAs mentioned in the previous section, the only novel contribution I have found is Theorem 6 which is essential for performing label-PD auditing under random sampling (that is, no adversarial canaries construction), but considering the poor performance of one run auditing in black box setting even for classical DP, I fail to understand how it can be even remotely useful for label DP, where the label is predicted quite well by the features, leaving very little randomness to begin with.\n\n**Minor comments**\n1. I fail to understand the need for the simulation based privacy definition. It seems like this is akin to the commonly used zero-out adjacency notion for the label-DP setting.\n2. I found the choice to denote $D^{b}$in Algorithm 2 as $o_{2}$ despite representing a very different quantity to $o_{1}$ quite confusing and I recommend avoiding it.\n3. The choice of $\\tau$ in Theorem 5 is confusing, since $\\tau$ represents the TV distance in Theorem 6 and this quantity is denoted by $\\gamma$ in the algorithm statement.\n4. More generally, Theorem 5 is nearly impossible to understand without reading the abstract, as it is stated in terms of Algorithm 3 which does not appear in the main body."}, "questions": {"value": "Please address the concerns raised in the previous section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NtQKc30GaN", "forum": "8TfiR1Lcvr", "replyto": "8TfiR1Lcvr", "signatures": ["ICLR.cc/2026/Conference/Submission19807/Reviewer_jL12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19807/Reviewer_jL12"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419155242, "cdate": 1761419155242, "tmdate": 1762931663267, "mdate": 1762931663267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an observational approach to auditing the privacy parameters of a mechanism under the context of Simulation-based Attribute privacy.\nMost prior work on auditing is *interventional* in the sense that the audit needs to change something about the data the mechanism runs on (for example, it might insert canaries) in order to carry out the audit.\nThis has non-trivial costs for two reasons: first, we care about the mechanism's output on the original data, so if the intervention significantly changes the output of the mechanism then we are paying in utility. Second, there is sometimes non-trivial engineering overhead to modify the mechanism in the ways required by the audit.\nBy contrast, the audit proposed in this work is *observational* in the sense that the data input to the mechanism is unchanged.\n\nAt a high level, the idea behind the proposed observational attack is as follows: Let $D^0$ denote the real dataset which consists of records $(x,y)$ where $x$ is public data that need not be protected and $y$ is sensitive data. Generate a new dataset $D^1$ that contains the same public records $x$, but which resamples the private data for each $x$ from the conditional distribution of $y$ given $x$. Finally, create a mixed dataset $D^b$ by flipping a coin $b_i$ for each row and if $b_i = 0$ we include the row from $D^0$ and if $b_i = 1$ we include the row from $D^1$. Then the attacker observes the output of the mechanism run on $D^0$ and the dataset $D^b$ and is required to guess for each row of $D^b$ whether it came from $D^0$ or $D^1$.\nIntuitively, since the rows of $D^0$ and $D^1$ are identically distributed, the only way for the attacker to identify the rows in $D^b$ that come from $D^0$ are to use the fact that the mechanism output was computed from $D^0$.\n\nThe authors prove that if an attacker is able to guess a significant number of rows then this leads to a lower bound on the privacy parameters of the mechanism $M$. They also handle the case when $D^1$ contains private data that is resampled from an approximation to the real conditional distribution of $y$ given $x$, in which case the lower bound on $\\epsilon$ degrades with the total variation distance between the approximate and true conditional distributions.\n\nFinally, they conduct experiments on CIFAR-10 and Criteo with a concrete attacker and show that the lower bounds on $\\epsilon$ computed through their audit are competitive with other auditing techniques for a range of mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Auditing the privacy parameters of a mechanism without intervention is an important and extremely relevant problem. For example, training or fine-tuning LLMs via DP-SGD may be too expensive to tolerate multiple runs or interventions in the training data that degrade model quality. The proposed auditing setup is reasonable and I believe the lower bounds on the privacy parameters derived from the audit are both believable and useful."}, "weaknesses": {"value": "I have a few minor concerns with the paper that I do not feel should be barriers to its publication, but I do think some additional discussion could be helpful.\n\nFirst, a subtle point of the audit is that after the attacker commits to their attack $A$, the true data $D^0$ is sampled i.i.d. from the data distribution $\\mathcal{D}$.\nThis randomness appears to be necessary for the analysis to go through, since it is the only way we prevent the attacker $A$ from knowing the records in $D^0$ as side information.\nBut, in practice, the attacker may legitimately have side-information about $D^0$, especially if it is a benchmark dataset.\n\nFor example, in the CIFAR-10 data, we could implement an attacker that accurately guesses the artifacts $b_0, \\ldots, b_m$ without looking at the output of the mechanism.\nInstead, for each $x_i$ in $D^b$, the attacker searches the public CIFAR-10 data for an exact match and then predicts $b_i = 0$ if $y^{b_i}_i$ matches the label in the public data and predicts $b_i = 1$ otherwise.\nThis attack will correctly guess $b_i$ for every record such that $y^1_i \\neq y^0_i$ (which intuitively seem like the only records where there is any hope to correctly guess better than chance).\nBut, the success of this attack should not provide a lower bound on the privacy parameters of the mechanism $M$, since it did not involve $M$ at all.\nPractically speaking, I think this just means that we need to be careful when we design the attack for the audit: we should try as much as possible to not let the design of the attacker be influenced by the true dataset.\n\nThe second concern is that this audit will only provide non-trivial lower bounds on the privacy parameters in situations where the conditional distribution of $y$ given $x$ is not too concentrated. For example, $y$ is a deterministic function of $x$, then when we resample $y^1_i$ from the conditional distribution given $x_i$, we will always have $y^1_i = y^0_i$ and so $D^1 = D^0$ and it seems that this should imply that no attacker can do better than chance. This is a minor concern because in these situations, the guarantee provided by attribute privacy is already weak (e.g., if your private record is a deterministic function of your public record, then there is nothing you can do to hide it whether the mechanism $M$ operates on your data or not)."}, "questions": {"value": "I would be especially curious to hear your thoughts about the requirement that $D^0$ be random.\n\nA related question is whether or not it is important that $D^0$ contain i.i.d. records drawn from some distribution $\\mathcal{D}$ (as opposed to a more complicated data collection process)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y7BWTUnkVi", "forum": "8TfiR1Lcvr", "replyto": "8TfiR1Lcvr", "signatures": ["ICLR.cc/2026/Conference/Submission19807/Reviewer_9U5d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19807/Reviewer_9U5d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863412109, "cdate": 1761863412109, "tmdate": 1762931662569, "mdate": 1762931662569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes an observational privacy auditing framework: instead of modifying the training data or pipeline (no canaries, no deletions), it leverages randomness in the data distribution itself to audit privacy for protected attributes such as label-DP, and gives a way to convert attack success rates into lower bounds on (ε,δ). The key assumption is the existence of a proxy label distribution that is (for a reasonable attacker) indistinguishable from the true one; in streaming/incremental training, an earlier checkpoint can serve as this proxy, keeping engineering cost very low. They cast auditing as a challenger–adversary game, define SIM-DP via an “imputation simulator,” and show empirically that their observational audit yields ε lower bounds comparable to calibrated, lightweight MIA baselines on CIFAR-10 and Criteo."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The presentation quality is pretty good and the story is easy to follow.\n\nThe problem is indeed an important one because the efficiency has always been the bottleneck of auditing DP in practice.\n\nThe theory makes sense and the derived method show good performance."}, "weaknesses": {"value": "There is not obvious weakness of theory and intuition part. But some weaknesses in the experiments part may downgrade of the soundness of the contributions you claimed.\n\nHave you ever tried to audit the membership-inference-target DP, e.g., the traditional DP-SGD on bounding indistinguishability of each sample? How does it look like? Although the attribute inference is an important problem, the traditional membership inference is also important. Now that you show your game based on sample-level membership inference problem, I hope to see how your architecture perform in practice.\n\nThe methods are only tested on two relatively simple set. You claimed that the observational method makes auditing efficient and can be deployed in practice, e.g., some larger models or datasets. However, the experiments are only on two datasets. Criteo is fine, but CIFAR-10 is way too simple. At least you should try your method in some cases where previous methods are not feasible, e.g., huge dataset or model.\n\nAlthough the method looks efficient, but no one knows what will happen in practice. You should mention the speed up of your methods in experiments to support the claim that your observational way does accelerate auditing. Or you can simply do the amortized analysis on your method and previous baseline methods.\n\n\nI think the paper's intuition is novel and the problem itself is important. But on these three weaknesses of evaluation, I cannot say the soundness is good. However, if you can show some more evaluation to support your claim \"\\textit{By lowering the complexity of privacy auditing, our approach enables its application in a wider variety of contexts.}\" and solve these weaknesses, I'll be happy to increase my scores."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GUfl3rQjSE", "forum": "8TfiR1Lcvr", "replyto": "8TfiR1Lcvr", "signatures": ["ICLR.cc/2026/Conference/Submission19807/Reviewer_Agft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19807/Reviewer_Agft"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947468106, "cdate": 1761947468106, "tmdate": 1762931661809, "mdate": 1762931661809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach for privacy auditing of machine learning models, which relies on the intrinsic characteristics of the data distribution. The main strength of the proposed approach is that it does not require to modify the training dataset. However, the approach works in the restricted setting of label differential privacy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "-The paper is well-written but an outline at the end of the introduction would help to clarify its structure. The privacy auditing is also well-explained and formally proven and is based on previous results from the auditing literature. \n\n-One of the novel aspect of the proposed auditing approach is that it relies on an attribute inference attack rather than a membership inference attack. Additionally, it does not require an alteration of the training pipeline of the model. \n\n-The approach has been validated experimentally on two datasets and against a wide range of mechanisms ensuring label-DP. The results demonstrate that the approach is able to obtain meaningful estimate for low value of epsilon."}, "weaknesses": {"value": "-The setting considered is that of label DP, which is rather restricted as it aims to protect only the privacy of the label of a particular record. Thus, while the approach is interesting and has the benefit of not require to alter the training pipeline, its applicability beyond label DP mechanisms appear to be limited. \n\n-The number of canaries reported for the Criteo and CIFAR-10 experiments is quite high. Ideally, the paper should contain some figures displaying how the quality of the estimate is impacted by the number of canaries. Furthermore the quality of the estimate appears to be quite low on Criteo for values of epsilon beyond 2. \n\n-A key component of the approach is the proxy model that is trained to infer a label based from the profile itself. The paper is currently lacking an in-depth discussion on the possible alternative to create it and the impact that it has on the quality of the estimate. \n\n-The comparison with other state-of-the-art approaches for privacy auditing is limited as it consists only to the MIA approach proposed by Watson et al. 2022.\n\nA small typo : \n-« and the of the trained » -> « and of the trained »"}, "questions": {"value": "Please see the main points raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0EeR65irWy", "forum": "8TfiR1Lcvr", "replyto": "8TfiR1Lcvr", "signatures": ["ICLR.cc/2026/Conference/Submission19807/Reviewer_w4LJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19807/Reviewer_w4LJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762203993922, "cdate": 1762203993922, "tmdate": 1762931660706, "mdate": 1762931660706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}