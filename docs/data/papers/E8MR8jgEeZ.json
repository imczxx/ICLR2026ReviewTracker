{"id": "E8MR8jgEeZ", "number": 13225, "cdate": 1758215304382, "mdate": 1759897454517, "content": {"title": "Entropy-preserving reinforcement learning", "abstract": "Policy gradient algorithms have been a driver of much recent advancement in language model reasoning. One of their most appealing properties is the ability to learn from exploration on their own trajectories, a process crucial for discovering diverse approaches and fostering creative solutions. As we show in this paper, most policy gradient algorithms naturally reduce the entropy---and thus the diversity of explored trajectories---as part of training, yielding a policy increasingly limited in its ability to explore. However, not all algorithms exhibit this collapse in entropy equally. In this paper, we formally analyze the contributions of leading policy gradient objectives on entropy, show which mechanisms they employ to implicitly limit entropy collapse, and propose a new regularization method, REPO, that stabilizes entropy over training through the use of an adaptive controller. Models trained with REPO preserve entropy throughout training, yielding final policies that are, on average, more performant. By preserving entropy in the final policy, REPO-trained models can even be re-trained on evolving data distributions in new environments, unlike their non-entropy-preserving counterparts.", "tldr": "A study of the entropy dynamics of policy gradient algorithms for LLMs and new algorithms for controlling entropy and preventing entropy collapse.", "keywords": ["Large language model", "reinforcement learning", "entropy", "GRPO", "PPO"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9344252de2d1875aff6ef0280112593d6a92164f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present REPO, a 'novel' on-policy RL algorithm for LLM fine tuning that adds an adaptive entropy penalty to ensure the policy entropy is maintained at a desired (initial) level. They provide some theoretical analysis of the entropy behaviour of existing RL for LLMs algorithms, and propose an adaptive entropy regulariser that tries to keep the entropy of the policy at an initial desired value. They then present a set of empirical results on the AppWorld and AIME benchmarks, showing how REPO sustains higher entropy than existing algorithms and results in slightly better benchmark performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of encouraging exploration efficiently in RL (and in LLMs) is very relevant and pressing. \n- The analysis the authors carry out on how different algorithms affect entropy is interesting.\n- The paper and its objectives are clear."}, "weaknesses": {"value": "- I am not convinced of the novelty or the contribution weight of the work. Entropy regularisation in RL is a very well studied problem, with MaxEnt RL being almost a subfield on its own. While the analysis of the effects of each algorithm in entropy are interesting, it is perhaps unsurprising that 1) algorithms push entropy to decrease fast (since policy gradient algorithms push the policy to be deterministic: this is optimal from an MDP perspective) and 2) that entropy regularisation helps with exploration and with preventing collapse to local minima (since this is essentially the main argument behind MaxEnt RL).\n- The idea of having an adaptive coefficient to sustain a desired level of policy entropy is also not novel in itself [1] (which is not cited in the paper).\n- Some of the mathematical statements are superficial or formally imprecise.\n\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)."}, "questions": {"value": "1. What is the reason for trying to keep entropy at the same level as the base (before post-training) level? Is there a reason to believe that the models have the 'appropriate amount' of entropy after pre-training?\n2. I would like to further understand the observation the authors make in lines 46-53. In general, entropy in RL policies is good during training to ensure exploration and to avoid collapsing to local minima too fast. It is not clear to me that sustained entropy is desirable in general, all the way through the resulting policy after post-training. In all tasks with verifiable rewards, one could argue that a model that collapses to a zero entropy policy that always achieves maximum reward for any query is optimal in an RL sense. What is the case for learning language models with high entropy policies? \n3. I do not fully agree with the motivation behind REPO-R. Biasing the regularisation towards positive advantage actions seems quite myopic: I would hypothesise that you observe positive results because, in essence, the complexity of the tasks (and thus the amount of exploration required) is low. Another reason for this is that the RL algorithms considered have a big simplifying assumption compared to classic RL algorithms: rewards are episodic and there is no need for constructing value function estimates. In general, an agent may need to explore bad actions for a while (or, equivalently, good actions may seem bad due to bad value estimates) to find good solutions in the long term, so discouraging negative advantage actions early on may cause catastrophic failure in most general RL settings. It also does not seem to add much improvement when compared to REPO-D, so I would appreciate if the authors would further motivate this decision and their assessment that REPO-R is a good idea.\n4. Theorem 1 is not stated in a formally correct way. A\\approx B is not a logically falsifiable claim and is ambiguous, as I could say 1\\approx 2 or 1\\approx 10 depending on the order of magnitude of the approximation error.\n5. Similarly, corollary 1 states 'approximately' and then formulates a 'proportional to' relation. Is it an approximation, or a proportional to?\n6. I don't know if I would formulate Theorem 2 as a theorem, as it suggests that it somehow presents a novel theoretical contribution of significant weight (perhaps a corollary of Gibbs inequality + the bounded ratio assumption). If the distance between two probability distributions is bounded, it follows from standard arguments that the entropy change is bounded (this is a minor point and a matter of personal preference)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w1VlPgbn0i", "forum": "E8MR8jgEeZ", "replyto": "E8MR8jgEeZ", "signatures": ["ICLR.cc/2026/Conference/Submission13225/Reviewer_CecX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13225/Reviewer_CecX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666833194, "cdate": 1761666833194, "tmdate": 1762923915631, "mdate": 1762923915631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies entropy behavior in policy gradient optimization within the context of large language model (LLM) reasoning tasks. It identifies the issue of entropy collapse during training and proposes a regularization approach called Regulated Entropy Policy Optimization (REPO), which adaptively reweights advantages and log-probabilities to maintain stable entropy. The method is evaluated on reasoning benchmarks such as AppWorld and AIME 2024, showing stable entropy dynamics and comparable or slightly improved performance over existing RL algorithms. The paper provides theoretical insights into entropy modulation in policy gradients and discusses which RL variants naturally preserve or collapse entropy during optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly identifies and analyzes the problem of entropy collapse in policy gradient methods for LLM reasoning.\n- Introduces a simple and interpretable approach (REPO) to stabilize entropy during training.\n- Provides theoretical insight into how different RL algorithms modulate entropy.\n- Demonstrates that REPO maintains stability without degrading baseline performance.\n- Presents the paper with strong structure, clear notation, and consistent motivation."}, "weaknesses": {"value": "- Lack of variance reporting or multiple-seed averaging, making statistical reliability of results unclear.\n- Performance improvements are modest, and their statistical significance is not demonstrated.\n- Evaluation scope is limited, focusing mainly on two model sizes within a single model family.\n- Distinction between REPO and conventional entropy regularization is not well articulated, leaving the novelty somewhat unclear."}, "questions": {"value": "**Detailed Review:**\n\nThe paper presents a focused and well-motivated analysis of entropy dynamics in reinforcement learning for LLM reasoning. It highlights how entropy often collapses prematurely in standard policy gradient methods, leading to limited exploration and suboptimal convergence. The proposed Regulated Entropy Policy Optimization (REPO) method adaptively scales the advantage term during optimization to counteract entropy collapse and maintain stable training dynamics.\n\nThe theoretical framing is coherent and connects policy gradient updates with entropy evolution. The resulting algorithm is straightforward and practical, which makes it potentially useful for a wide range of RLHF applications. However, it would be beneficial to include a more detailed comparison to standard entropy regularization methods, such as those used in actor-critic frameworks, to clarify conceptual and empirical differences.\n\nExperimental results demonstrate that REPO stabilizes entropy without harming model performance. The reported trends across benchmarks such as AppWorld and AIME 2024 show promising consistency. Nevertheless, the magnitude of improvement is relatively small, and the lack of variance across multiple seeds limits confidence in the statistical strength of the findings. Clarifying how many independent runs were performed and whether results are averaged would help substantiate the claims.\n\nSome figures (2, 3) would benefit from normalization, as differing y-axis ranges obscure direct comparisons. The discussion of entropy per token (Figure 1) is interesting but requires a clearer explanation of its computation and interpretation, especially when entropy appears to collapse to a single value. Likewise, the differing entropy behaviors of RLOO and REPO across Figures 2 and 3 require deeper discussion, particularly regarding model size and task difficulty.\n\nFrom a methodological standpoint, the paper’s simplicity is an advantage. The adaptive scaling via $\\beta$ provides a structured mechanism to control entropy without introducing new objectives. However, a broader range of experiments, including other model families and additional reasoning benchmarks, would improve the generality of the results. The motivation of Section 5.3 could also be clarified, especially regarding whether it demonstrates task transferability or robustness.\n\n\n**Questions:**\n\n1. How many independent runs were conducted for each experiment, and can variance or confidence intervals be reported?\n2. Why are the y-axis scales inconsistent across subplots in Figure 1 (and 2, 3), and could they be standardized for clearer comparison?\n3. How is the per-token entropy computed, and why does it appear to converge to a specific point in certain figures?\n4. Why does RLOO show opposite entropy trends between Figures 2 and 3, and why do the baseline algorithms differ across these figures?\n5. How significant are the reported performance differences, and were any statistical tests conducted?\n6. Can REPO integrate with PPO or other policy gradient methods, and how would that adaptation look in practice?\n7. How is the entropy bonus (lines 372–373) computed, and how is its scale selected?\n8. What is the motivation behind Section 5.3, and how does it relate to the core objective of entropy regulation and generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZcKW3BoYT6", "forum": "E8MR8jgEeZ", "replyto": "E8MR8jgEeZ", "signatures": ["ICLR.cc/2026/Conference/Submission13225/Reviewer_CBf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13225/Reviewer_CBf6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969101555, "cdate": 1761969101555, "tmdate": 1762923915346, "mdate": 1762923915346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of \"entropy collapse\" in reinforcement learning , where policy gradient algorithms, used to train language models, naturally reduce the diversity of explored solutions during training. This premature narrowing of focus can limit performance by causing the model to get stuck in local optima. The authors analyze how different algorithms either amplify this collapse (like PPO) or implicitly mitigate it (like DAPO and GSPO). They then propose a new method, REPO (Regulated Entropy Policy Optimization), which uses an adaptive controller to actively monitor and stabilize entropy throughout training. Experiments show that models trained with REPO preserve entropy, achieve higher average performance, and, crucially, can be effectively re-trained on new tasks in novel environments, unlike models that have suffered entropy collapse."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes an interesting idea to analyze how the entropy evolves during RL finetuning. The writing is clear and easy to follow (especially Section 3)."}, "weaknesses": {"value": "The performance difference of proposed algorithms (REPO-R and REPO-D) and RLOO does not seem to be statistically significant, making me wonder whether the proposed method is useful. (Even if the difference is statistically significant, the magnitude of the difference seems to be small.) Furthermore, while RLOO has no mechanism to keep early collapse according the proposed analysis, its final entropy is somehow high. This also makes me wonder whether the analysis really explains how entropy evolves and consequently allows us to derive a better algorithm."}, "questions": {"value": "The performance difference of proposed algorithms and RLOO does not seem to be statistically significant. Maybe I am simply missing, but would you tell me what $\\pm$ means in tables?\n\nAs I wrote in Weakness, RLOO does not seem to have any implicit mechanism to avoid entropy collapse and does not really align with the analysis in the paper. Would you explain why?\n\nIn the proof of Theorem 2, I do not really understand why clipping of the importance ratio ensures that no policy gradient update is performed if the policy drifts outside a trust region as in Line 200. Due to function approximation error, $(1+\\epsilon) \\pi_\\theta^{old} (a|s) < \\pi_\\theta^{new} (a|s)$ can occur if one really updates policies according to Line 130.\n\nI do not really understand the description on the entropy dynamics of GSPO. The trajectory length is also dependent on a policy, and the current proof of Theorem 2 cannot be straightforwardly extended. Would you add more explanation on this paragraph?\n\nIn Line 191, I think \"Corollaries 1 and 2\" is a typo, and it must be \"Theorem 1 and Corollary 2\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "osFCqB7Hwn", "forum": "E8MR8jgEeZ", "replyto": "E8MR8jgEeZ", "signatures": ["ICLR.cc/2026/Conference/Submission13225/Reviewer_k7r1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13225/Reviewer_k7r1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979114561, "cdate": 1761979114561, "tmdate": 1762923915052, "mdate": 1762923915052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the entropy dynamics of existing policy gradient algorithms for LLM post-training, then proposes a method for preventing entropy collapse (REPO). Figure 1 shows the trajectories of training in terms of Test Accuracy vs entropy, which seemed like an interesting visualization method. The results showed that low entropy trajectories tend to have low final test accuracy. In terms of theoretical results, they present the change in entropy based on 1 policy gradient step, when applying REINFORCE (this seems mostly like an extension of Cui et al. 2025).\nREPO is motivated by their derivation of the change in entropy per step, and adds a weighted term (beta*(logpi - E[logpi])) to the policy gradient advantage to counteract the drop in entropy per step.\nThey add two heuristic variants for tuning beta to allow for greater robustness: (REPO-D) This sets beta to zeta * \\Delta H, where \\Delta H is the computed change in entropy from a gradient step. Also, zeta is clipped between a max and min value, and it is adaptively tuned to reach a target policy entropy, similarly as done in the original PPO paper (they double or halve the zeta constant depending on which direction to move to reach the target entropy). (REPO-R) This variant instead uses \\beta = -\\zeta * max(A, 0), i.e., it multiplies by the positive advantage, and sets negative advantages to 0 (the motivation being to prevent converging to large advantage values while still penalizing wrong actions).\n\nExperimentally, they compare against REINFORCE leave-one-out (RLOO), GRPO (a normalized PPO variant), LOOP (a PPO variant with leave-one out return estimation), DAPO (LOOP with asymmetric weight clipping), GSPO (variant of LOOP with with trajectory based clipping). They test using a Qwen3 (8B and 32B) base models on the AppWorld tasks as well as on AIME 24 and 25 math tasks and achieved good performance (a bit better than RLOO on the 32B App tasks, but maybe slightly worse or similar on 8B tasks). For REPO, they also had a DAPO variant as well as a GSPO variant (the former worked better on 32B tasks, the latter on 8B tasks). REPO-R seemed to always be better than REPO-D."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I liked the entropy trajectory visualization.\n\n- The performance is slightly better than the current published SOTA on these tasks (LOOP)"}, "weaknesses": {"value": "- One major point for me was a discrepancy between the results in the paper and previous published LOOP results (this discrepancy likely comes from the previous results being with Qwen2.5, while the current results being with Qwen3.0).\n\nThe existing LOOP scores can be seen in the below links:\nhttps://appworld.dev/leaderboard\nhttps://arxiv.org/pdf/2502.01600\n\nThese results are with Qwen2.5 (instead of Qwen3 like in the current paper), and it achieves results of 71.3 and 45.7 compared to the results in the current paper (64 and 40). Moreover, their performance is better than the results they present with RLOO. It seems unlikely that the performance drops that much when switching from Qwen2.5 to Qwen3, so I would guess that there may be implementation issues in LOOP. (for reference the RLOO scores in the LOOP paper are 57.2 and 36.7 for the normal and challenge tasks respectively, and switching to the Qwen3 model in the current paper lead to 71 and 52 for RLOO).\n\nOn the AIME tasks, there is also no consistent improvement and RLOO seems to work reasonably well in the current paper.\n\n- Another major point for me was a conceptual point about the method. The REPO method as presented on line 227, adds the log pi term to the advantage, and if we take the expectation pi log pi, this is just the negative entropy, so the method appears to be simply adding an entropy bonus, which is a well-known technique. Different to past implementations is that they do not directly compute the derivative of the entropy, but instead perform a REINFORCE estimate of the gradient of the entropy (the subtraction of the average log pi value can also be interpreted as the baseline); whether this is a good or bad thing remained unclear to me. Perhaps it induces different learning dynamics than simply directly computing the exact entropy and adding that as a bonus, but these points were not examined.\n\nTo make clear what I mean by this, consider the derivation: E_pi[dlogpi/dtheta * (logpi - E(logpi))] = E_pi[dlogpi/dtheta * (logpi)] = d/dtheta E_pi[logpi] = d\\dtheta H(pi), is the derivative of the negative entropy.\n\nThey did actually add a comparison with a pure entropy bonus based method in a similar fashion as their REPO, however, I had some concerns about this. This was done on the Qwen8B task, and they added the entropy to the DAPO method. However, on 8B, GSPO-REPO performed better, so adding an entropy bonus to that would have been more convincing to me. \n\n- The added tuning methods did not seem to have a strong theoretical justification."}, "questions": {"value": "To me, the major conceptual issue is that the method appears very similar to an entropy bonus, but just with the gradients estimated in a noisy way. Perhaps there is some advantage to this as perhaps the change in entropy can be coupled to sampled action, and this may alter the entropy dynamics, which may be beneficial, but this is not discussed in the paper. Can you elaborate on why your method would be beneficial over a simple entropy bonus?\n\nMy experimental concerns listed in the weaknesses are also a major concern for me. One way to mitigate this would be to run experiments using Qwen2.5, as this allows direct comparison with existing published results (in particular, your results for LOOP with Qwen3.0 are weaker than published results with Qwen2.5).\n\nAnother small comment is that your derivations appear closely related to Cui et al. 2025, but it appears cited after you introduce your results. It may be better to cite it before, and make the connection clear.\n\nAnother small comment I have is about the phrase in the abstract: “As we show in this paper, most policy gradient algorithms naturally reduce entropy”. However, this is a well-known issue, so I think it may be better to present it as such."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3hJY7RwQU5", "forum": "E8MR8jgEeZ", "replyto": "E8MR8jgEeZ", "signatures": ["ICLR.cc/2026/Conference/Submission13225/Reviewer_KAD6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13225/Reviewer_KAD6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13225/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997715333, "cdate": 1761997715333, "tmdate": 1762923914507, "mdate": 1762923914507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}