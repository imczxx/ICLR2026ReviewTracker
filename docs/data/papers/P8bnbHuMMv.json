{"id": "P8bnbHuMMv", "number": 2062, "cdate": 1756982996954, "mdate": 1759898171561, "content": {"title": "Loss Transformation Invariance of the Damped Newton Methods", "abstract": "The Newton method is one of the most widely used second-order optimization techniques, valued for its conceptual simplicity and extremely fast local convergence. A key advantage is its invariance under affine transformations (e.g., choice of coordinate basis), which greatly facilitates implementation. However, the classical Newton method fails to converge when initialized far from the solution, motivating the development of various globalization techniques.\nIn this work, we focus on step size damping, which, when appropriately scheduled, ensures fast global convergence while preserving both affine-invariance and superlinear local rates. Although highly effective in convex settings, existing algorithms offer limited guarantees for problems that are only nearly convex. To address this, we investigate loss transformations that convexify the objective. We show that Newton step size schedules are invariant under such transformations and that stepsize scheduling implicitly searches over the space of objective transformations. Our theoretical findings are further supported by comprehensive experimental validation.", "tldr": "We introduce the concept of loss transformation invariance and prove that it holds for the stepsized Newton method. This allows improving loss properties, enables transformation-induced stepsizes, and justifying unconventional Newton stepsizes.", "keywords": ["optimization", "second-order methods", "damped newton method", "invariance"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3e12d9028921473e67bf8f5b1464fed4d556833.pdf", "supplementary_material": "/attachment/d3965ed3881b17ed37082aa27ac0e33cb06bbd1d.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates loss transformation invariance in the damped Newton method, showing that the Newton stepsize schedules are invariant under monotonic loss transformations. \nThe authors prove that, by carefully designing such transformations, applying the damped Newton method to the original objective function $f(x)$ is equivalent to applying the vanilla Newton method to a transformed objective function $L(x)$, extending its applicability to certain nonconvex problems. \nThe paper also offers insights for why stepsizes greater than one and negative stepsizes can still be effective in second-order methods."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The main theory results are innovative, which provide explanations for several empirical observations such as why stepsizes larger than one and negative stepsizes can be effective for second-order methods.\n\n2. The motivations for using loss transformation and stepsize scheduling are clear, and the writing flows smoothly, from the discussion on convex and pseudoconvex problems to the convexification of a broad class of nonconvex problems."}, "weaknesses": {"value": "1. Such loss transformation assumes the objective function is pseudoconvex (sufficient) or its sublevel sets are convex (necessary), which are quite strong for real-world problems and difficult to verify.\n\n2. All experiments are conducted on 1D and 2D synthetic problems which are insufficient. Larger and higher-dimensional problems should be included. Moreover, since Theorems 3 and 4 are stated for $x \\in \\mathbb{R}^d$, it is necessary to include higher-dimensional experiments to validate these claims.\n\n3. Only the vanilla Newton method and the damped Newton method are compared. Other second-order baselines, such as quasi-Newton, cubic regularization, trust region, and Newton-Krylov methods, should also be evaluated on large, real-world nonconvex problems. It would be helpful to show whether the damped Newton method achieves better robustness, or whether the stepsize scheduling achieves better efficiency (in time) than traditional strategies like line search.\n\n4. Some minor typos:\n    - lines 41 - 42: should be $O(1/k^2)$ instead of $O(1/k^-2)$, and similar elsewhere.\n    - line 62: \"than\" -> \"then\"\n    - line 274: clarify the phrasing \"strict pseudoconvexity is sufficient properties are sufficient\"\n    - line 278: should be $\\phi \\circ f$ instead of $f \\circ \\phi$"}, "questions": {"value": "1. Theorem 3 assumes a global upper bound. How can such a bound be defined, or what are the consequences if it is not tight?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CFZWumAsEP", "forum": "P8bnbHuMMv", "replyto": "P8bnbHuMMv", "signatures": ["ICLR.cc/2026/Conference/Submission2062/Reviewer_dgoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2062/Reviewer_dgoH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608449680, "cdate": 1761608449680, "tmdate": 1762916009304, "mdate": 1762916009304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the convergence analysis of Newton’s method to pseudoconvex functions (possibly nonconvex) by composing the function $f$ with a ‘convexifier’ $\\phi$ so that $\\phi \\circ f$ is convex. Applying Newton’s method for the composed, convex function corresponds to Newton’s method for the original nonconvex function with an adjusted step size, i.e., we can leverage the theoretical guarantees for convex functions to the nonconvex case, and such a convexifier always exists for strictly pseudoconvex functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The writing is clear and easy to read overall.\n- The paper includes numerical experiments that support the theoretical results."}, "weaknesses": {"value": "See **Questions.**"}, "questions": {"value": "- To run the proposed algorithm in practical situations, the only constructive way suggested here seems to be finding the function $h$ and then using some $\\phi$ that satisfies the inequality in Theorem 3. Can you explain a bit more detail on how this could be done given an arbitrary pseudoconvex functions? I would like to know how much computation will the process of computing $r(x)$ and then $\\phi$ via Theorem 2 and 3 require, and whether it is negligible enough to consider doing all this to find the right step size schedules to run Newton.\n- Is there a possibility that we might have slow convergence guarantees (compared to real performance) in cases when the inequality in Theorem 3 is loose? (I understand that the focus of this paper is that the convergence guarantees for pseudoconvex functions are *new*, but I am wondering if there could be cases where a better choice of $\\phi$ can better capture the real dynamics or lead to a better step-size scheduling.)\n- For the star-convex case, is it also possible to convert cubic Newton for the star-convexified  loss into a step-size scheduled version of cubic Newton for the original function? Or are there technical difficulties in doing the same transformation as classical Newton here?\n\n**Typos?**\n\nPage 4. $f \\circ \\phi \\rightarrow \\phi \\circ f$\n\nTheorem 4. I think $g$ is supposed to be star-convex (instead of convex)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dAqwDU8EPX", "forum": "P8bnbHuMMv", "replyto": "P8bnbHuMMv", "signatures": ["ICLR.cc/2026/Conference/Submission2062/Reviewer_VjE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2062/Reviewer_VjE5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974254884, "cdate": 1761974254884, "tmdate": 1762916009023, "mdate": 1762916009023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies convergent stepsizes for the Newton method through loss transformations. First, it shows that running Newton on a transformed loss is equivalent to running Newton on the original loss with a rescaled stepsize, and for convexification and star-convexification, provides necessary and sufficient conditions for such transformations to exist. In doing so, this work extends the convergence of Newton’s method to certain nonconvex functions through rescaled stepsize scheduling. Experiments visualize regions where the scaling factor becomes negative and demonstrate expanded convergence neighborhoods on test functions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper offers an interesting approach to studying the Newton method’s stepsize via loss transformations, leading to nontrivial theoretical results not covered by classical analyses. It theoretically argued for a rescaled stepsize and the existence of suitable transformations for convexification, and the experiments are appropriately conducted to study the theoretical claims and practical effect."}, "weaknesses": {"value": "Since the theoretical results rely on pseudoconvexity and on a compact set, truly nonconvex is not covered. Also, the rescaled-stepsize theorems require knowledge minimum and the minimizer, which may be hard to compute in practice."}, "questions": {"value": "There is a typo on line 134.\n\nRegarding the rescaled step size in Theorems, the current formulation requires knowledge of the minimizer and the minimum value. Could this be improved using adaptive step-size strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no ethics concerns."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "dwlevSAo5s", "forum": "P8bnbHuMMv", "replyto": "P8bnbHuMMv", "signatures": ["ICLR.cc/2026/Conference/Submission2062/Reviewer_nbeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2062/Reviewer_nbeN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996184728, "cdate": 1761996184728, "tmdate": 1762916008797, "mdate": 1762916008797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the damped Newton's method with the goal of extending its global convergence to a class of mildly non-convex objectives via loss function transformations. Leveraging the observation that function range transformations provide a one-to-one correspondence between a Newton step on the original objective and one on the transformed objective with a modified stepsize, the authors identify a class of functions and corresponding transformations where global convergence is ensured on the new objective. Experiments are provided in support of the theory."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The topic is relevant to the research community, and the results are meaningful for computational science and engineering domains. The loss function transformation idea, while not novel in itself, is nicely leveraged for this class of problems/methods."}, "weaknesses": {"value": "Unfortunately, the paper has quite a few shortcomings. Broadly speaking, there's missing related literature, the presentation lacks conciseness and order, the technical part contains mistakes, and experiments are not well explained. In particular:\n\n1. **Related work**\n \t* This work falls within the broader class of approaches leveraging \"hidden convexity\" to establish convergence guarantees. There is a broad literature here dating from the 80's and 90's and a placement within and comparison with those approaches are missing (e.g., why/why not domain transformations, rather than range are suitable for your scenario, which other settings are served by range transformations, etc.). See [2] for an old survey, and references within [3] and [4] for more recent works.  \n \t* In addition, work [1] leverages both domain and range transformations to transfer convergence guarantees of Newton's method from one problem to the other. Please also see references therein.\n\n\n2. **Presentation**\n\t* The writing is carried out negligently, with \n         * many missing, inappropriate, extra or mismatched articles (e.g., lines 029, 038, 040, 042, and many others); \n         * typos, mathematical or otherwise (e.g., convergence rates in lines 041, 043 should be O(k^-2) and O(k^-3); line 107, dual norm argument is $g$, not $h$; the optimum is denoted both $x^{\\*}$ and $x_{\\*}$ e.g., in lines 316 and 320 and others; in line 053 \" not necessarily\" and many others); \n         * verbosity and gauche phrasing (examples throughout the paper, e.g. 087 --- \"Throughout the transformation invariance [...]\"; subtitle 2.1); \n         * missing structural components like a conclusion-type section. \n       \n      A vocabulary and grammar correction tool (e.g., Grammarly) could greatly improve the presentation. \n\t* Lemma 2 is stated without a concise, mathematical description of the correspondence between the Levenberg–Marquardt hessian regularization and the stepsized Newton method. As such, this result's relevance to the topic seems tangential. What is the implication of the result? Why is it important within the present work? \n\t* Technical assumptions are spread throughout the text, rather than being collected in statements marked explicitly as \"Assumption\". E.g., a central assumption about $\\phi$ being strictly increasing is stated in a **footnote** on page 3. \n\t* Theorem 3 and its corollary are proven as \"Lemma 5\" in the Appendix, which is confusing. Also, the statement proven in the appendix is an incomplete version of the statement of Theorem 3 (though the result of Theorem 3 can come out of it).\n\n\n3. **Technical details**\n\t* The current proof of Theorem 1 is incorrect. The authors use the pseudoinverse as if it were the inverse, which leads to mistakes, since the former has different properties. Specifically, the errors are \n         * Going from line 686 to 688, it seems the authors use $\\mathbf{H}^\\dagger \\mathbf{H} = \\mathbf{I}$, which does not necessarily hold for pseudoinverses. Moreover, the authors seem to also use $(\\mathbf{A}\\mathbf{B})^{\\dagger} = \\mathbf{B}^\\dagger\\mathbf{A}^\\dagger$ which, again does not hold in general. \n         * For the same reason as above, line 691 does not imply line 694\n\t* computation of $g$ in theorem 4 depends on the unique optimum $x^*$, and there is no discussion about this limitation.\n\n4. **Experiments**\n\t* Plots in sections 4.1 and 4.2 are not explained, and one has to guess what is illustrated: what do the colors code for? Does the method converge on the modified loss? The chosen test functions are not pseudoconvex (e.g., the Goldstein-Price function has several local minima), so the theory does not apply to this experiment. To my understanding, the question was whether convergence is aided by negative stepsizes, so experiments should seek settings with convergence guarantees.   \n\t* What is the transformation $\\phi$ used for the experiments in Fig. 1? Only $f$ and $L$ are stated.\n\n\n[1] Izmailov, Alexey F., and Mikhail V. Solodov. \"TRANSFORMATIONS OF VARIABLES AND TRANSFORMATIONS OF EQUATIONS VIA THE PERTURBED NEWTON METHOD FRAMEWORK.\" (2025).\n\n[2] Horst, Reiner. \"On the convexification of nonlinear programming problems: An applications-oriented survey.\" European Journal of Operational Research 15.3 (1984): 382-392.\n\n[3] Fatkhullin, Ilyas, Niao He, and Yifan Hu. \"Stochastic optimization under hidden convexity.\" arXiv preprint arXiv:2401.00108 (2023).\n\n[4] Xia, Yong. \"A survey of hidden convex optimization.\" Journal of the Operations Research Society of China 8.1 (2020): 1-28."}, "questions": {"value": "* Could you please provide examples of real-world applications where pseudoconvex functions emerge?\n* Could you please discuss the possible division by zero within the modified stepsize (this is in relation to the factor $\\left[1 + \\frac{\\phi^{\\prime \\prime}(f(x))}{\\phi^{\\prime}(f(x))} (\\\\|\\nabla f(x)\\\\|_x^{*})^2 \\right]^{-1}$ when $\\phi^{\\prime \\prime}(f(x))$ is negative)?\n* Could you please provide a comparison with the missing related literature mentioned above?\n* Could you please motivate your choice of test functions in the experiments of section 4? Also, please explain the color coding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k5AHvLAknz", "forum": "P8bnbHuMMv", "replyto": "P8bnbHuMMv", "signatures": ["ICLR.cc/2026/Conference/Submission2062/Reviewer_xceV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2062/Reviewer_xceV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092497612, "cdate": 1762092497612, "tmdate": 1762916008560, "mdate": 1762916008560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}