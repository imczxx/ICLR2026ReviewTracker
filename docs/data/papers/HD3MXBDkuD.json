{"id": "HD3MXBDkuD", "number": 20850, "cdate": 1758310920161, "mdate": 1763106397609, "content": {"title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining", "abstract": "What data should a CLIP model see? Many data curation efforts aiming to answer\nthis question center on the quality of a dataset. However, recent work has shown that\nwhile admitting impressive performance benefits, none of these curation methods\nare concept-centric, leading to them inheriting the biased properties of web-scale\ndata distributions. In this work, we go beyond such concept-agnostic methods and\nadvocate a more flexible online concept-based curation approach. To enable this,\nour first contribution is DATACONCEPT, a collection of 128M web-crawled image-\ntext pairs annotated with fine-grained details about their concept composition.\nBuilding on DATACONCEPT, we fill another critical gap in the literature: the lack of\na competitive, open-source alternative to highly performant batch sampling methods\nfor Language-Image Pretraining. Specifically, we introduce Concept-Aware Batch\nSampling (CABS), a simple yet effective batch-sampling algorithm that distills\nbatches with the broadest set of available concepts. Through rigorous evaluation on\na broad suite of 28 benchmarks, we demonstrate that CABS significantly benefits\nLanguage-Image Pretraining (LIP) and yields highly performant models on long-\ntailed evaluations (up to +2.4 p.p. on Let-it-Wag!), while enabling practitioners to\ndefine custom concept distributions that optimize for specific downstream tasks.\nImportantly, with only one hyperparameter tuned for a single (backbone, eval)\ncombination only, CABS shows full compatibility with both CLIP and SigLIP\nmodels. Both DATACONCEPT and the source code for CABS will be released", "tldr": "Concept-aware data curation and batch sampling improves the downstream performance of contrastive vision-language models.", "keywords": ["vision-language", "clip", "data curation", "batch sampling", "pretraining"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c406e9629fde0d4f28917d089f9657055ad3c63c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes: (1) a pipeline that automatically generates fine-grained, concept-focused image captions using state-of-the-art tagging, grounding, and VLM models; and (2) evidence that pretraining image-language model with batches whose concepts are uniformly distributed outperforms pretraining the same model using concept-biased batches. Combining these contributions yields considerable accuracy gains across diverse image–language pretraining settings, including different loss functions and base models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows that (1) relabeling captions to emphasize image concepts (DataConcept) and (2) pretraining with concept-balanced batches (CAPS-DM) consistently improve zero-shot classification across diverse settings. Zero-shot retrieval accuracy also increases with the optimal batching strategy (CAPS-FM). These conclusions are supported by extensive experiments."}, "weaknesses": {"value": "- The method performs well on zero-shot classification but degrades zero-shot retrieval accuracy under the default CAPS-DM. Although the CAPS-FM variant improves retrieval, relying on different setups for different applications weakens the contribution, as a single pretrained model is generally expected to work across tasks. If the model underperforms on either classification or retrieval, it may also struggle on downstream tasks such as detection and segmentation, which undermines the promise of foundation models.\n\n- Developing a data relabeling pipeline for pretraining are already well studied (e.g., https://arxiv.org/pdf/2311.06242 and references therein). To establish novelty, more comprehensive qualitative and quantitative comparisons are needed; comparisons limited to naive baselines (IID) and a hard-negative mining variants are insufficient."}, "questions": {"value": "- If CAPS-FM is applied for zero-shot classification, is that better than the baseline IID? I.E. Can the CAPS-FM be more general than CAPS-DM so that CAPS-FM works in both the applications (zero-shot classification and retrieval)?\n\n- Have author tried CAPS with the existing dataset without using DataConcept? This is important to understand what is the major contribution of the improvement. Is it because of DataConcept or CAPS?\n\n- What is the baseline performance on the benchmark even without DataConcept?\n\n- The ImageNet zeroshot accuracy in Figure 1 is generally too low. If author is using variant of ImageNet zeroshot benchmark, please clearly specify them in detail in the Figure 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uyZqr3cVBf", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Reviewer_nu4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20850/Reviewer_nu4g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524498766, "cdate": 1761524498766, "tmdate": 1763000000372, "mdate": 1763000000372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8SEaFEBuY1", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763106396627, "cdate": 1763106396627, "tmdate": 1763106396627, "mdate": 1763106396627, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the role of incorporating concept-level information during large-scale language–image pretraining, which is underexplored in the literature. To this end, the paper introduces DATACONCEPT, a large-scale, fully annotated pretraining dataset, which augments samples with fine-grained concept annotations and concept-driven synthetic captions. Besides, this paper proposes a flexible framework, CABS, for online, concept-aware batch sampling for LIP. Empirical experiments demonstrate the benefits of CABS over IID and other two batch-sampling baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe constructed concept-aware pretraining dataset with rewritten context-aware captions seems promising and useful.\n\n2.\tThe experiment in table1 shows a clear advantage of concept-aware re-captions in LIP."}, "weaknesses": {"value": "1.\tWhile the curated concept-aware pretraining dataset is meaningful to the community, it seems the proposed CABS doesn’t work well with the dataset. Different from other sampling strategies that may achieve consistent improvement across classification and retrieval tasks, CABS may perform well on classification but worse on retrieval tasks. Though the authors proposed an alternative CABS-FM to improve performance on retrieval tasks, it makes the total design complicated since a hard choice needs to be made and the trained model can't perform different tasks effectively, which means that it loses the advantage of pretrained models to generalize well across tasks.."}, "questions": {"value": "1.\tAlthough the authors claim that Evans et al. (2024a) and Udandarao et al. (2025) didn’t release their code, is it possible to reproduce them since their methods show strong performance and the paper only has two baselines?\n\n2.\tIt’s better to include an algorithm (pseudo code) to show how CABS-DM processes samples during training. Only natural language description makes it difficult to follow and understand the procedure clearly.\n\n3.\tWhat are the guidelines for choosing the hyperparameter f? Is it sensitive?\n\n4.\tPlease explain more about $f_c$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ctYXEuQbBf", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Reviewer_fQRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20850/Reviewer_fQRo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837953665, "cdate": 1761837953665, "tmdate": 1763000001012, "mdate": 1763000001012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper annotates 128M image-text pairs from DataComp XLarge, and proposes Concept-Aware Batch Sampling (CABS), a diversity-driven strategy that e.g. samples mini-batches based on the diversity of their constituent concepts instead of random sampling. By balancing concept coherence and intra-batch diversity, CABS yields improved performance across vision and language tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Achieves better zero-shot performance than random sampling. Another heuristic variant that samples examples with more concepts yields a better performance for retrieval.\n\n- Provides empirical validation on benchmarks."}, "weaknesses": {"value": "I don't see the novelty or new insights provided by this paper. The idea that balanced mini-batches improve the convergence and performance on smaller groups of data is well-known. This idea has been used before in many different domains, including federated learning, data selection, etc. From optimization perspective, the reason is that balanced mini-batches have smaller gradient variance which yield faster convergence, which is theoretically analyzed and shown by several existing papers in the literature (this is a relatively old concept). Besides, upsampling underrepresented groups is obviously beneficial. The main idea of the paper is to annotate the concepts in training example and use them to sample balanced mini-batches. While this is a good usage in a production pipeline, I don't see any new \"scientific\" insight. If the main contribution is the annotations, the paper is more suitable for the dataset and benchmark track."}, "questions": {"value": "What's the new scientific insight (finding, method, analysis, etc) from this paper, in authors' opinion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f3eUchXKn2", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Reviewer_MhEs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20850/Reviewer_MhEs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949707359, "cdate": 1761949707359, "tmdate": 1763000001634, "mdate": 1763000001634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper advocates for concept-aware data curation for contrastive language-image pre-training (CLIP) and first proposes a dataset DataConcept based on a subset of DataComp, and consists of 128M image-text pairs, where each image annotated with fine-grained concepts, bounding boxes, and concept-aware synthetic captions; Then, the method propose a Concept-Aware Batch Sampling strategy to improve the training effectiveness. Different from MetaCLIP, it seems that this method do not need to build a balanced dataset offline but adaptively adjust the sampling throughout the training to maintain a balanced distribution seen by the model. The experimental evaluation shows untrivial gain on classification and long-tailed benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- A new data curation mechanism with online method to adjust the data distribution seen by the model to improve the training effectiveness.\n- The motivation to change existing random sampling is appreciated.\n- The evaluation is comprehensive."}, "weaknesses": {"value": "- The definition of concept is critical for the method development. Then, the discussion on why the current concept bank definition is optimal is needed. As the author mentioned MetaCLIP many times, I am curious how the concept bank different from the metadata used in MetaCLIP (in MetaCLIP, the balanced distribution according to metadata is one critical standard for MetaCLIP dataset construction. \n- Following, when the concepts bank contains erroneous or missed concepts, how your method can robustly expand or update it in an online manner.\n- Performance trade-off: By comparing performance in Table 1 and Fig. 5, the CABS-DM helps classification but hurts retrieval, while the CABS-FM favors retrieval only. Then, I am curious whether these two mechanism variants can be combined, or whether it is always conflicting for optimizing the performane for classification & retrieval. \n- For your method, I wanna check whether the image encoder during training must be frozen or can also be updated."}, "questions": {"value": "For multi-modal pre-training, the current method primarily uses text to determine the context in batch sampling, which is similar to [2]. However, whether the visual information can also be utilized (e.g., the team of MetaCLIP also propose CIT for visual pre-training).\n\n[1 ]CIT: Curation in training for effective vision-language data (ICCV)\n\n[2] In-context pretraining: Language modeling beyond document boundaries (ICLR)\n\nPlease see my comments in the weakness. Happy to increase the score if all of my questions & weakness can be properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4E6pK4qi6v", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Reviewer_Vhed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20850/Reviewer_Vhed"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968061221, "cdate": 1761968061221, "tmdate": 1763000000996, "mdate": 1763000000996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time and intend on addressing the points made in a systematic manner:\n\n1. how the concept bank different from the metadata used in MetaCLIP(Vhed): Our concept curation is independent of MetaCLIP curation. They use 500k queries from Wikipedia (“base query list is all words occurring at least 100 times in the English version of Wikipedia, augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added”). Due to this, prepositions, articles and other part-of-speech variants other than nouns exist in the pool. Additionally, it only considers single-word queries. Our concept pool is different as it only considers nouns that can be visually localised, as we collect concepts from the vocabularies of works focussing on computer vision tasks(classification, object detection, referring segmentation, etc).\n\n2. What do you do when the concepts bank contains erroneous or missed concepts: a concept bank of 19261 nouns is fairly comprehensive, however we acknowledge that concepts may be missing. MetaCLIP suffers from the same problem - the metadata has one more missing concept whenever a new Wikipedia article is created\n\n3. “paper only has two baselines”: we intend to scale up baseline comparisons by including MetaCLIP. ACID and JEST remain close-sourced rendering a faithful reproduction difficult.\n\n4.“better to include an algorithm (pseudo code)”(fQRo): thank you for the suggestion, we will do so.\n\n5.“Please explain more about f_c”(fQRo): It is the  frequency of a concept(number of times it exists in a superbatch) from our concept bank  in superbatch before balancing\n\n6.“hard choice needs to be made and the trained model can't perform different tasks effectively”: Recent works have shown that separate curation is indeed necessary [1](we have discussed this in the submission too). We additionally show proof that retrieval and classification is distributionally divergent by using the same pipeline to annotate COCO as we used to construct DataConcept for a fair comparison.\n\n[1]https://www.datologyai.com/blog/productionized-multimodal-data-curation-at-the-billion-sample-scale\n\n\nMinor/major oversights from reviewers:\n\n1.“the paper is more suitable for the dataset and benchmark track.” Please note that we did indeed submit to the datasets and benchmark track\n\n2.“No scientific insight”(Mhes): Three major insights include online batch sampling trumps offline curation, performance boosts by sample repeats contribute more to model performance than previously estimated, old methods(GRIT, MAFA) don’t scale to modern regimes.\n\n3.“Current method primarily uses text to determine the context in batch sampling”(Vhed): we would like to clarify that our concept annotations are not text-based: they are distilled from visual information. This is fundamentally different from MetaCLIP, which relies solely on textual metadata.\n\n4.“What is the baseline performance on the benchmark even without DataConcept?”(nu4g): That is equivalent to training on DataComp.\n“The ImageNet zeroshot accuracy in Figure 1 is generally too low.” (nu4g): the results are in accordance to the number of samples seen (128M). Please refer to the Datacomp paper for a reference."}}, "id": "uyqucR6DDv", "forum": "HD3MXBDkuD", "replyto": "HD3MXBDkuD", "signatures": ["ICLR.cc/2026/Conference/Submission20850/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20850/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20850/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763106354987, "cdate": 1763106354987, "tmdate": 1763106354987, "mdate": 1763106354987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}