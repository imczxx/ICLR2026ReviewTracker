{"id": "sVeYIZHjrB", "number": 6906, "cdate": 1758001118325, "mdate": 1759897884742, "content": {"title": "Partial Diffusion Suffices: Solving General Inverse Problems via Score Evolution", "abstract": "Despite rapid advancements, pre-trained diffusion models still face dual challenges in solving inverse problems. On the one hand, current methods depend heavily on iterative optimization of the complete reverse diffusion chain, leading to difficulties in balancing manifold preservation and measurement constraints, especially under complex scenarios. On the other hand, strong assumptions about noise characteristics are necessitated, limiting their applicability in most practices. With the signal-noise variation of the diffusion score theoretically analyzed, this study proposes a plug-in alternative, dubbed Partial Diffusion with Score Evolution (PDSE), which optimizes the solution space by reconstructing a partial iterative path of the reverse diffusion flow. First, with the signal-noise equilibrium point of the score function derived, the critical diffusion steps are precisely identified, which preserve essential generative capabilities while controlling the propagation range of evolving errors. Second, by employing a standardized optimization approach, the iterative programme could suppress interference from noise of varying types and intensities. Extensive experiments across diverse scenarios demonstrate that PDSE significantly enhances adherence to measurement constraints while ensuring manifold feasibility, particularly exhibiting robust performance in nonlinear inverse scenarios with unknown noise characteristics. The aonoymous code is available on \\href{https://anonymous.4open.science/r/code-of-PDSE}{https://anonymous.4open.science/r/code-of-PDSE}.", "tldr": "", "keywords": ["image recostruction; diffusion model;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac19f14d3b6470e95578f97f13d16b3932ba2e7e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Partial Diffusion with Score Evolution (PDSE), a new framework for solving inverse problems using conditional score-based generative models (CSGMs). The core idea is to improve the efficiency and quality of CSGM-based approaches, which typically optimize a latent variable (e.g., initial noise $x_T$) by backpropagating through the entire diffusion chain. The author proposed a novel initialization technique that uses partially diffused samples $x_t$ at $t$. Along with the proposed early stopping technique, the proposed method improves CSGM's efficiency and quality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a key bottleneck in existing CSGM-style methods: the high computational and memory cost of backpropagating through the full diffusion.\n2. The core idea of starting from an intermediate timestamp t* is simple and effective. It naturally reduces the number of sampling steps required for optimization.\n3. The authors provide a clear theoretical derivation for their choice of t* based on the SNR=1 equilibrium point.\n4. The method demonstrates state-of-the-art or competitive performance against several recent baselines."}, "weaknesses": {"value": "1. The introduction claims to address \"two significant gaps\": slow convergence and \"exploding memory\". While the partial diffusion strategy clearly addresses convergence, the solution for memory costs (the ALES-based early stopping) is not a core part of the PDSE formulation itself and is only detailed in the appendix. This makes the writing inconsistent.\n\n2. In section 3.1, the novelty of initializing from a partially-noised state is questionable. This concept is well-explored in related literature (DiffPure [1], SDEdit [2]), and these highly relevant works are not cited. Proposition 3.1 can be considered as a special case of their analysis. A recent work, FMPlug [3], seems to have a more generalized form of the initialization, which allows data-adaptive timestamp optimization.\n\n3. In section 3.2, the explanation of extrapolation is confusing. Taking eq. 13 into eq. 14, the extrapolation step is $\\hat{x}^k_\\text{out} = \\hat{x}^k + \\lambda \\mu \\nabla_{\\hat{x}^k} \\ell$, which makes the $\\lambda$ not very useful. While the illustration of the gradient flow is poorly illustrated, the Alg 1 seems to also contradict it.\n\n4. Section 3.3’s focus is on the issue of previous work. The author should at least provide the improved formulation in the main body and explain why ALES can help with memory issues.\n\n5. The entire experimental results of image restoration task in the main paper (Tables 1 & 2, Figure 3) are conducted exclusively on human face datasets (FFHQ and CelebA). To support the paper's claim of solving \"general inverse problems\", a more diverse set of results should be presented and analyzed in the main text.\n\n6. The paper lacks crucial ablations in the main text.\n\n$\\quad$ a) The effect of the \"extrapolation\" strategy is only shown in a convergence plot, not with a quantitative table (PSNR/SSIM/LPIPS) to show its impact on final image quality.\n\n$\\quad$ b) The hyperparameters for the extrapolation strategy are not discussed or ablated.\n\n\n\n[1] Nie, Weili, et al. \"Diffusion models for adversarial purification.\" arXiv preprint arXiv:2205.07460 (2022).\n\n[2] Meng, Chenlin, et al. \"Sdedit: Guided image synthesis and editing with stochastic differential equations.\" arXiv preprint arXiv:2108.01073 (2021).\n\n[3] Wan, Yuxiang, et al. \"FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems.\" arXiv preprint arXiv:2508.00721 (2025)."}, "questions": {"value": "1. The optimal timestamp t* is derived from the prior's SNR=1 condition and appears to be fixed (e.g., 0.26T ) regardless of the task. Can you provide empirical analysis of how the type and severity of the degradation (e.g., 90% inpainting mask vs. 10%, or heavy blur vs. light blur) affect the true optimal t*? Is a single, fixed t* really optimal for all inverse problems?\n\n2. In Section 3.2, the author mentions a gradient being \"backpropagated through the entire computational graph, including the extrapolation and the decoder R(·)\". Can the author clarify this gradient flow and provide the specific hyperparameters used for this acceleration (e.g., the stepsize $\\mu$ and scaler$ \\lambda$)?\n\n3. The acceleration strategy shows faster convergence in Figure 3 (right). How does this compare, in both reconstruction quality (PSNR/SSIM) and wall-clock time, to the baseline method (w/o acceleration) simply run with a correspondingly larger learning rate? Is this a principled acceleration or an approximation of a more aggressive descent? \n\n4. While the paper claims to mitigate \"exploding memory\", the total number of diffusion steps $T=10$ which can cause more memory issues compared with DMPlug’s default setting $T=3$. Here are the questions for the author:\n\n$\\quad$ a) How does the ALES with a larger number of diffusion steps help with the memory issue?\n\n$\\quad$ b) How does the peak GPU memory usage of PDSE compare to a standard CSGM (e.g. DMPlug)? \n\n$\\quad$ c) How does the total number of diffusion steps affect the proposed methods’ performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TVs7NGPDf1", "forum": "sVeYIZHjrB", "replyto": "sVeYIZHjrB", "signatures": ["ICLR.cc/2026/Conference/Submission6906/Reviewer_JPGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6906/Reviewer_JPGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811275708, "cdate": 1760811275708, "tmdate": 1762919147810, "mdate": 1762919147810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Partial Diffusion with Score Evolution (PDSE), a new framework for solving inverse problems using conditional score-based generative models (CSGMs). The core idea is to improve the efficiency and quality of CSGM-based approaches, which typically optimize a latent variable (e.g., initial noise $x_T$) by backpropagating through the entire diffusion chain. The author proposed a novel initialization technique that uses partially diffused samples $x_t$ at $t$. Along with the proposed early stopping technique, the proposed method improves CSGM's efficiency and quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a key bottleneck in existing CSGM-style methods: the high computational and memory cost of backpropagating through the full diffusion.\n2. The core idea of starting from an intermediate timestamp t* is simple and effective. It naturally reduces the number of sampling steps required for optimization.\n3. The authors provide a clear theoretical derivation for their choice of t* based on the SNR=1 equilibrium point.\n4. The method demonstrates state-of-the-art or competitive performance against several recent baselines."}, "weaknesses": {"value": "1. The introduction claims to address \"two significant gaps\": slow convergence and \"exploding memory\". While the partial diffusion strategy clearly addresses convergence, the solution for memory costs (the ALES-based early stopping) is not a core part of the PDSE formulation itself and is only detailed in the appendix. This makes the writing inconsistent.\n\n2. In section 3.1, the novelty of initializing from a partially-noised state is questionable. This concept is well-explored in related literature (DiffPure [1], SDEdit [2]), and these highly relevant works are not cited. Proposition 3.1 can be considered as a special case of their analysis. A recent work, FMPlug [3], seems to have a more generalized form of the initialization, which allows data-adaptive timestamp optimization.\n\n3. In section 3.2, the explanation of extrapolation is confusing. Taking eq. 13 into eq. 14, the extrapolation step is $\\hat{x}^k_\\text{out} = \\hat{x}^k + \\lambda \\mu \\nabla_{\\hat{x}^k} \\ell$, which makes the $\\lambda$ not very useful. While the illustration of the gradient flow is poorly illustrated, the Alg 1 seems to also contradict it.\n\n4. Section 3.3’s focus is on the issue of previous work. The author should at least provide the improved formulation in the main body and explain why ALES can help with memory issues.\n\n5. The entire experimental results of image restoration task in the main paper (Tables 1 & 2, Figure 3) are conducted exclusively on human face datasets (FFHQ and CelebA). To support the paper's claim of solving \"general inverse problems\", a more diverse set of results should be presented and analyzed in the main text.\n\n6. The paper lacks crucial ablations in the main text.\n\n$\\quad$ a) The effect of the \"extrapolation\" strategy is only shown in a convergence plot, not with a quantitative table (PSNR/SSIM/LPIPS) to show its impact on final image quality.\n\n$\\quad$ b) The hyperparameters for the extrapolation strategy are not discussed or ablated.\n\n\n\n[1] Nie, Weili, et al. \"Diffusion models for adversarial purification.\" arXiv preprint arXiv:2205.07460 (2022).\n\n[2] Meng, Chenlin, et al. \"Sdedit: Guided image synthesis and editing with stochastic differential equations.\" arXiv preprint arXiv:2108.01073 (2021).\n\n[3] Wan, Yuxiang, et al. \"FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems.\" arXiv preprint arXiv:2508.00721 (2025)."}, "questions": {"value": "1. The optimal timestamp t* is derived from the prior's SNR=1 condition and appears to be fixed (e.g., 0.26T ) regardless of the task. Can you provide empirical analysis of how the type and severity of the degradation (e.g., 90% inpainting mask vs. 10%, or heavy blur vs. light blur) affect the true optimal t*? Is a single, fixed t* really optimal for all inverse problems?\n\n2. In Section 3.2, the author mentions a gradient being \"backpropagated through the entire computational graph, including the extrapolation and the decoder R(·)\". Can the author clarify this gradient flow and provide the specific hyperparameters used for this acceleration (e.g., the stepsize $\\mu$ and scaler$ \\lambda$)?\n\n3. The acceleration strategy shows faster convergence in Figure 3 (right). How does this compare, in both reconstruction quality (PSNR/SSIM) and wall-clock time, to the baseline method (w/o acceleration) simply run with a correspondingly larger learning rate? Is this a principled acceleration or an approximation of a more aggressive descent? \n\n4. While the paper claims to mitigate \"exploding memory\", the total number of diffusion steps $T=10$ which can cause more memory issues compared with DMPlug’s default setting $T=3$. Here are the questions for the author:\n\n$\\quad$ a) How does the ALES with a larger number of diffusion steps help with the memory issue?\n\n$\\quad$ b) How does the peak GPU memory usage of PDSE compare to a standard CSGM (e.g. DMPlug)? \n\n$\\quad$ c) How does the total number of diffusion steps affect the proposed methods’ performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TVs7NGPDf1", "forum": "sVeYIZHjrB", "replyto": "sVeYIZHjrB", "signatures": ["ICLR.cc/2026/Conference/Submission6906/Reviewer_JPGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6906/Reviewer_JPGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811275708, "cdate": 1760811275708, "tmdate": 1763760850865, "mdate": 1763760850865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an inverse problem algorithm based on diffusion models. Unlike most existing approaches that gradually increase the supervision weight from the measurement during sampling, the proposed method operates as follows: after iterating an existing diffusion model–based posterior sampling algorithm until a designated critical step t, it uses the diffusion model to pair samples between step 0 and step t applies measurement supervision to the step-0 sample, and updates the step-t sample accordingly. In addition, the authors introduce an early stopping strategy to prevent excessive descent after a certain number of iterations. While certain concerns remain regarding the method and experimental design (see the questions section for details), the work is conceptually interesting and potentially inspiring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a unique strategy in which, at a designated critical iteration \\(t^*\\), the diffusion model is used to pair the sample at step 0 with that at step \\(t\\), and measurement supervision is applied to the step-0 sample to update the step-\\(t\\) sample. This differs from the common approach of gradually increasing the measurement weight and is a distinctive contribution.  \n2. The incorporation of an early stopping strategy helps avoid excessive descent in later iterations, which is particularly beneficial in inverse problems where overshooting can degrade performance.  \n3. The experiments cover multiple task types (presumably various inverse problems such as image reconstruction), illustrating the potential applicability of the method across different scenarios."}, "weaknesses": {"value": "1. The proposed ALES method involves a large number of tunable hyperparameters. Although the authors claim that the chosen settings are applicable across different tasks, no experiments are provided to analyze the sensitivity of the method to these hyperparameters.  \n2. The description of certain experimental settings is insufficient (see the questions section), making it difficult for readers to fully understand or reproduce the results.  \n3. The notation is inconsistent; for example, the score function is sometimes denoted as \\(s\\) and at other times as \\(\\mathbf{s}\\). It is recommended that the symbols be standardized."}, "questions": {"value": "1. In Algorithm 1, there is a typographical error: after calling `optimizer.zero_grad()`, the corresponding optimization step is missing. This creates an obstacle to understanding the algorithm. In addition, the initialization method `sample` requires further clarification.  \n2. I notice that the influence of \\(x_{t}\\) on the iterations is entirely reflected through \\(\\hat{x}\\). Why, then, is an update performed on \\(x_{t}\\)? For sufficiently small step size,  \n\\[\n\\frac{dx_{t}}{dk} = -\\frac{d\\hat{x}}{dx_{t}} \\frac{d L}{d\\hat{x}}, \\quad \\frac{d\\hat{x}}{dk} = -\\frac{d L}{d\\hat{x}},\n\\]  \nwhich implies that the use of the diffusion model during iterations can be entirely avoided, and the diffusion model is needed only for initialization.  \n3. Is the comparison of algorithm efficiency fair? In Table 4 (right), the proposed method shows similar computational efficiency to DPS. However, the proposed method employs the R operator in every iteration, which involves 3 NFEs (as noted in the appendix) and requires gradient backpropagation. It is difficult to reconcile this with the claim that it matches the efficiency of a method that involves only 1 NFE per step and no backpropagation, under the same number of steps. The authors should provide a more detailed clarification of the experimental setting in Table 4 (right).  \n4. In inverse problems based on diffusion models, the number of iterations typically has a direct impact on performance. Therefore, I suggest reporting the iteration numbers for each baseline method as well as the average iteration number for the proposed method in the main table.  \n5. The proposed inverse algorithm largely follows the optimization-based approach. Adding comparisons with other optimization-based inverse algorithms would be beneficial. Furthermore, including experiments under noise-free conditions would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2yDPplwNzF", "forum": "sVeYIZHjrB", "replyto": "sVeYIZHjrB", "signatures": ["ICLR.cc/2026/Conference/Submission6906/Reviewer_DShK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6906/Reviewer_DShK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761037421543, "cdate": 1761037421543, "tmdate": 1762919147103, "mdate": 1762919147103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Partial Diffusion with Score Evolution (PDSE) for solving general inverse problems using diffusion models. Instead of running the entire reverse diffusion process, PDSE identifies a critical diffusion step with signal-to-noise ratio equals 1, and argues that partial diffusion suffices to preserve generative fidelity while improving efficiency and robustness. Additionally, adaptive learning and early stopping (ALES) strategy is proposed to prevent overfitting to noise and accelerate convergence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The partial diffusion design cuts down redundant steps and significantly reduces computation time while maintaining high reconstruction fidelity;\n2. The authors provide analysis for choosing the partial diffusion point based on score norm evolution and SNR balance;\n3. Benchmark results demonstrating consistent improvements under noise variations;"}, "weaknesses": {"value": "1. The KL-divergence bound in proposition 3.2 is qualitative; it provides no tight or empirical verification that ~93% of generative fidelity is consistently preserved across datasets or diffusion schedules.\n2. PDSE assumes the pretrained diffusion model’s score function is well-calibrated. In practice, small score miscalibration could drastically shift $t^*$ or amplify reconstruction bias, yet this sensitivity is unstudied.\n3. The paper does not compare PDSE to step-skipping or distillation-based acceleration methods. It’s unclear if PDSE’s partial strategy is superior or simply another form of truncated sampling.\n4. All experiments are at 256 × 256 pixels; scalability to higher resolutions (512 px, 1 K px) is not tested.\n5. The paper lacks qualitative discussion of where PDSE fails — e.g., extreme degradations, large measurement noise, or unmodeled forward operators."}, "questions": {"value": "1. Can PDSE be combined with data-consistency projection (like DPS) for hybrid performance?\n2. Is partial diffusion equivalent to performing denoising score matching over a truncated time interval? If so, can it be interpreted as optimizing an energy functional over an incomplete diffusion trajectory?\n3. Does PDSE have any formal convergence properties when coupled with ALES? How can one ensure that early stopping does not bias the reconstruction toward local minima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OacU9jdwjc", "forum": "sVeYIZHjrB", "replyto": "sVeYIZHjrB", "signatures": ["ICLR.cc/2026/Conference/Submission6906/Reviewer_iWfM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6906/Reviewer_iWfM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665069511, "cdate": 1761665069511, "tmdate": 1762919146170, "mdate": 1762919146170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}