{"id": "FiSafizPOA", "number": 9178, "cdate": 1758114032016, "mdate": 1763195120015, "content": {"title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "abstract": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering systems and dialogue generation tasks by integrating information retrieval (IR) technologies with large language models (LLMs). This strategy, which retrieves information from external knowledge bases to bolster the response capabilities of generative models, has achieved certain successes. However, current RAG methods still face numerous challenges when dealing with multi-hop queries. For instance, some approaches overly rely on iterative retrieval, wasting too many retrieval steps on compound queries. Additionally, using the original complex query for retrieval may fail to capture content relevant to specific sub-queries, resulting in noisy retrieved content. If the noise is not managed, it can lead to the problem of noise accumulation. To address these issues, we introduce HANRAG, a novel heuristic-based framework designed to efficiently tackle problems of varying complexity. Driven by a powerful revelator, HANRAG routes queries, decomposes them into sub-queries, and filters noise from retrieved documents. This enhances the system's adaptability and noise resistance, making it highly capable of handling diverse queries.  We compare the proposed framework against other leading industry methods across various benchmarks. The results demonstrate that our framework obtains superior performance in both single-hop and multi-hop question-answering tasks. We will release the code and benchmark after this paper is accepted.", "tldr": "", "keywords": ["RAG", "Multi-hop QA", "Adaptive retrieval"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b45a27902dc23c24b6bdc6943221948684623936.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces HANRAG which trains the Revelator to act as a query router, a query decomposer, a relevance labeler, and a retriever terminator. The authors also describe the data construction to train the Revelator with its multiple skills, and report gains over multiple baselines across single-hop and multi-hop datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The taxonomy of various queries in the preamble are clear.\n- The challenges are well discussed."}, "weaknesses": {"value": "- Typos: unknown symbols in line 293/324/327/and more, missing citation in line 297/349/and more.\n- HANRAG-Fair is not mentioned and described.\n- The training process is unclear. For example, how the constructed data are mixed and their distribution on different tasks, including decompositions, refinement, relevance/termination discrimination. \n- The paper reports results on single- and multi-hop datasets without analyzing the performance of Revelator in each component during the RAG process, and it is unclear how well it routers queries, filter out noises, and continue/terminate retrievals."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CBEWHRKEVF", "forum": "FiSafizPOA", "replyto": "FiSafizPOA", "signatures": ["ICLR.cc/2026/Conference/Submission9178/Reviewer_R16r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9178/Reviewer_R16r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739579700, "cdate": 1761739579700, "tmdate": 1762920854137, "mdate": 1762920854137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses key challenges in multi-hop QA for RAG systems, including over-reliance on iterative retrieval for compound queries, irrational query formulation, and noise accumulation.  The authors propose HANRAG, a heuristic framework driven by a multi-functional \"Revelator\" module that enables adaptive query routing, decomposition of compound queries, refinement of complex queries, and relevance-based noise filtering.   Experiments on single-hop and multi-hop benchmarks demonstrate that HANRAG outperforms state-of-the-art methods  in both effectiveness and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.This paper proposeds a heuristic framework driven by a multi-functional \"Revelator\" module that enables adaptive query routing, decomposition of compound queries, refinement of complex queries, and relevance-based noise filtering.\n\n2.Asynchronous parallel retrieval for compound queries and iterative seed refinement for complex queries significantly improve efficiency and reasoning accuracy.\n\n3.Experiments on single-hop and multi-hop benchmarks demonstrate that HANRAG outperforms state-of-the-art methods in effectiveness."}, "weaknesses": {"value": "1.The related work referenced and compared in this paper is limited to publications from 2024 and earlier. It is recommended that the authors further discuss the differences between the proposed method and more recent RAG approaches, particularly those based on query rewriting [1][2][3], and provide comparative results where applicable.\n\n[1]MaFeRw: Query rewriting with multi-aspect feedbacks for retrieval-augmented large language models\n[2] RaFe: Ranking Feedback Improves Query Rewriting for RAG\n[3]UniRAG: Unified Query Understanding Method for Retrieval Augmented Generation\n\n2.While HANRAG reduces retrieval steps, the Revelator’s multi-task integration (routing, decomposition, refinement, etc.) may introduce additional computational costs.  The paper does not report inference latency or memory usage, which are critical for real-world applications.\n\n3.The experiments use Llama-3.1-8B and FLAN-T5-XL for the Revelator and LLM generator.  There is no analysis of how HANRAG’s performance varies with different LLM sizes (e.g., 70B models) or architectures (e.g., Mistral, Qwen2), limiting generalizability.\n\n4.The compound query benchmark is generated using Qwen2-72B-instruct, but the paper lacks quantitative evaluation of the generated data’s quality (e.g., sub-query independence, answer accuracy).  No analysis is provided on potential biases or errors introduced by LLM-generated queries."}, "questions": {"value": "1.For the compound query benchmark generation: How did you validate the independence of sub-queries and the correctness of answers generated by Qwen2-72B-instruct?  Are there any metrics used to ensure data quality?\n\n2.The Revelator integrates multiple tasks (routing, decomposition, etc.) via multi-task learning.  Could you clarify whether these tasks are trained jointly or sequentially?  Would separate optimization of individual sub-modules yield better performance?\n\n3.Compared to graph-based multi-hop RAG methods, what are HANRAG’s advantages and disadvantages in handling complex logical reasoning?\n\n4.The relevance discriminator relies on semantic understanding to filter noise. How does this module perform in low-resource scenarios or domains with specialized terminology ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8lmwxqnJ4v", "forum": "FiSafizPOA", "replyto": "FiSafizPOA", "signatures": ["ICLR.cc/2026/Conference/Submission9178/Reviewer_EbtA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9178/Reviewer_EbtA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903109548, "cdate": 1761903109548, "tmdate": 1762920853601, "mdate": 1762920853601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HANRAG, a RAG framework that introduces heuristic-guided retrieval selection and noise-aware answer fusion to improve reasoning accuracy under noisy or redundant evidence. The method combines adaptive document filtering, token-level confidence weighting, and reasoning consistency checks within the generation process. Experiments on open-domain QA benchmarks (e.g., NQ, TriviaQA, HotpotQA) demonstrate improved robustness compared to several baseline RAG systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and relevant challenge—enhancing RAG robustness under retrieval noise, a known limitation of current retrieval-augmented systems.\n\n2. The paper demonstrates measurable performance gains on multiple QA datasets,"}, "weaknesses": {"value": "1. The methodology and experimental setup are difficult to follow in several sections; notation and algorithmic steps could be more systematically presented.\n2. The proposed heuristics and noise-aware fusion strategies are largely straightforward extensions of existing retrieval scoring and consistency-checking methods, with limited conceptual innovation.\n3. Citations for NQ and TriviaQA benchmarks are missing in line 349, and other related RAG literature from 2024–2025 for multi-hop QA (e.g., [1,2,3,4,5]) should be included for completeness.\n4. The paper lacks qualitative examples or deeper ablation analysis that would help interpret where and why HANRAG provides benefits or fails.\n\n[1] Jin et al. \"Search-r1: Training llms to reason and leverage search engines with reinforcement learning.\" arXiv preprint arXiv:2503.09516 (2025).\n\n[2] Song et al. \"R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\" arXiv preprint arXiv:2503.05592 (2025).\n\n[3] Wu et al. \"ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering.\" arXiv preprint arXiv:2506.00232 (2025).\n\n[4] Yu et al. \"Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\n\n[5] Yu et al. \"Rankrag: Unifying context ranking with retrieval-augmented generation in llms.\" Advances in Neural Information Processing Systems 37 (2024)."}, "questions": {"value": "1. Could you clarify how the heuristic weighting differs from prior rank-based or confidence-aware retrieval scoring approaches?\n\n2. How sensitive is the model’s performance to the heuristic thresholds and noise ratios introduced during retrieval filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g5rhQIizSf", "forum": "FiSafizPOA", "replyto": "FiSafizPOA", "signatures": ["ICLR.cc/2026/Conference/Submission9178/Reviewer_1TDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9178/Reviewer_1TDu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937074704, "cdate": 1761937074704, "tmdate": 1762920853131, "mdate": 1762920853131, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HANRAG, a RAG framework that introduces heuristic-guided retrieval selection and noise-aware answer fusion to improve reasoning accuracy under noisy or redundant evidence. The method combines adaptive document filtering, token-level confidence weighting, and reasoning consistency checks within the generation process. Experiments on open-domain QA benchmarks (e.g., NQ, TriviaQA, HotpotQA) demonstrate improved robustness compared to several baseline RAG systems.\n\nDisclaimer: I am surprised to find that my review was labeled as \"fully llm-generated\" in https://iclr.pangram.com/. Here I would like to clarify that all the points I wrote here are my own judgments after reading the paper, and I used GPT to make the review more fluent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practical and relevant challenge—enhancing RAG robustness under retrieval noise, a known limitation of current retrieval-augmented systems.\n\n2. The paper demonstrates measurable performance gains on multiple QA datasets,"}, "weaknesses": {"value": "1. The methodology and experimental setup are difficult to follow in several sections; notation and algorithmic steps could be more systematically presented. For example: \"REVELATOR_Rel\" is not formally defined. \n\n2. The proposed heuristics and noise-aware fusion strategies are largely straightforward extensions of existing retrieval scoring and consistency-checking methods with LLMs, with limited conceptual innovation.\n\n3. Citations for NQ and TriviaQA benchmarks are missing in line 349, and other related RAG literature from 2024–2025 for multi-hop QA (e.g., [1,2,3,4,5]) should be included for completeness.\n\n4. The paper lacks qualitative examples or deeper ablation analysis that would help interpret where and why HANRAG provides benefits or fails. It would be better to provide a few case studies to better highlight the advantages of the proposed method.\n\n[1] Jin et al. \"Search-r1: Training llms to reason and leverage search engines with reinforcement learning.\" arXiv preprint arXiv:2503.09516 (2025).\n\n[2] Song et al. \"R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\" arXiv preprint arXiv:2503.05592 (2025).\n\n[3] Wu et al. \"ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering.\" arXiv preprint arXiv:2506.00232 (2025).\n\n[4] Yu et al. \"Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\n\n[5] Yu et al. \"Rankrag: Unifying context ranking with retrieval-augmented generation in llms.\" Advances in Neural Information Processing Systems 37 (2024)."}, "questions": {"value": "1. Could you clarify how the heuristic weighting differs from prior rank-based or confidence-aware retrieval scoring approaches?\n\n2. How sensitive is the model’s performance to the heuristic thresholds and noise ratios introduced during retrieval filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g5rhQIizSf", "forum": "FiSafizPOA", "replyto": "FiSafizPOA", "signatures": ["ICLR.cc/2026/Conference/Submission9178/Reviewer_1TDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9178/Reviewer_1TDu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937074704, "cdate": 1761937074704, "tmdate": 1763343429353, "mdate": 1763343429353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the efficiency and effectiveness of RAG across different query types, including simple, single-hop, multi-hop, and compound queries. It introduces a master agent, Revelator, designed to perform multiple RAG-related tasks such as query routing, relevance evaluation, and query decomposition. Experiments conducted on various QA datasets and across multiple RAG-based methods demonstrate that the proposed approach achieves notable improvements in both performance and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation to route different categories of queries to specialized solutions is well-founded, and the idea of employing a master agent to coordinate multiple sub-tasks is particularly interesting.\n- The experiments and ablation studies are solid and effectively justify the design choices of each component in the proposed method. The analysis covers not only task performance but also efficiency, providing a comprehensive evaluation."}, "weaknesses": {"value": "- The overall writing quality of the paper requires substantial improvement. There are numerous typo errors and incorrect citations. For example:\n    - Inconsistent or unclear notations for data formats at Lines 293, 297, and 324–332.\n    - Issues around Line 348.\n- One of the paper’s main contributions is the **Revelator** component. However, its technical details are insufficiently explained. For instance, it remains unclear how Revelator is trained to handle multiple tasks and how its performance varies across tasks such as query routing and relevance evaluation. While some of these details appear in the Appendix, they are critical to understanding the proposed approach and should be included in the main text.\n- The idea of classifying queries into different categories and handling them with tailored strategies, such as decomposition or noisy context filtering, is somewhat similar to existing work. As a result, the novelty and overall contribution of the paper appear incremental."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bq6ibrPkBs", "forum": "FiSafizPOA", "replyto": "FiSafizPOA", "signatures": ["ICLR.cc/2026/Conference/Submission9178/Reviewer_hLVh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9178/Reviewer_hLVh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991054973, "cdate": 1761991054973, "tmdate": 1762920852741, "mdate": 1762920852741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}