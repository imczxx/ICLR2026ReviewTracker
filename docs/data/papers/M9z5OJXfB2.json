{"id": "M9z5OJXfB2", "number": 8979, "cdate": 1758105564697, "mdate": 1759897750291, "content": {"title": "Unveiling the Power of Shared Spaces: A Gating-Driven Mechanism for Semi-Supervised Domain Adaptation", "abstract": "omain adaptation (DA) aims to enhance the generalization ability of models in scenarios where labeled data in the target domain is scarce. In DA research, semi-supervised domain adaptation (SSDA) can utilize the labeled information in the target domain more effectively compared to unsupervised domain adaptation (UDA), thus achieving superior transfer performance and gaining widespread attention. Existing SSDA methods implicitly learn feature spaces in the process of aligning feature spaces between domains; however, the underlying mechanisms remain insufficiently explored. To address this issue, this paper first theoretically reveals the advantages of learning a shared feature space for enhancing transferability. Based on our theoretical insights, we develop a  framework to learn a shared space, which is implemented by a gating-driven SSDA enhancement mechanism.  It is feasible to explicitly filters out inconsistent features across domains compared with existing methods. Extensive experimental results demonstrate the significant improvements of the proposed gating-driven enhancement mechanism on state-of-the-art SSDA models. Our code is anonymously provided in https://anonymous.4open.science/r/ICLR_8979.", "tldr": "This paper first theoretically reveals the advantages of learning a shared feature space for enhancing transferability. Then, we develop a  framework to learn a shared space, which is implemented by a gating-driven SSDA enhancement mechanism.", "keywords": ["Semi-Supervised Domain Adaptation;  Shared Space; Gating-Driven Mechanism"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44ea13326fe81218ab6469c277f8f1c98c944ec5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies semi-supervised domain adaptation (SSDA) and proposes a gating-driven shared-space mechanism. A lightweight gating module is inserted between the feature extractor and classifier to “turn off” domain-specific channels and retain domain-invariant (shared) features. The authors provide a theoretical analysis arguing that enlarging the proportion of shared space reduces the source–target total variation (TV) distance, and they show plug-and-play gains when adding the gate to several SSDA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1）Simplicity & plug-and-play: The gating block is easy to integrate into existing SSDA pipelines without changing loss functions or training protocols.\n\n2）Theory–practice linkage attempt: The paper tries to connect the idea of “more shared space → smaller TV → lower target error” with empirical TV measurements.\n\n3）Broad empirical coverage: Multiple datasets and several representative SSDA baselines are evaluated; complexity overhead is small."}, "weaknesses": {"value": "1）Limited novelty of the core premise :\nThe claim that “explicitly learning a shared space benefits SSDA” is already widely recognized and underpins many prior SSDA/DA approaches (e.g., domain-invariant representation learning, feature disentanglement, conditional alignment). The theoretical section mostly formalizes a well-known intuition rather than delivering new insights or substantially stronger guarantees. As presented, the theory’s necessity to the method is questionable and feels incremental.\n\n2）“Explicitly turning off domain-specific channels” is not truly explicit:\nThe paper repeatedly emphasizes explicitly shutting down domain-specific features. However, the proposed gating is still a learned soft mask over latent channels driven by task loss. There is no external signal, constraint, or supervision that explicitly identifies domain-specific factors (style codes, backgrounds, frequency bands, etc.). Without architectural or optimization mechanisms that tie gates to domain cues, “explicit” remains more a narrative than a property of the method.\n\n3）Modest and inconsistent performance gains:\nReported improvements over strong baselines are generally small (often ~0.5–2% and not universal across directions/shots). Some tasks show overlapping confidence intervals or marginal deltas. Given the simplicity of the gating, small gains are plausible, but then the contribution should be framed as a lightweight regularizer rather than a fundamentally new mechanism.\n\n4）Theory–empirics gap:\nThe TV analysis assumes a clean separation between shared and domain-specific factors and (in parts) independence; actual deep features violate these assumptions. The measured TV drops after gating are encouraging, but do not isolate whether the effect comes from generic capacity control/sparsification rather than “shared-space enlargement” per se."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F1CgTGXTBJ", "forum": "M9z5OJXfB2", "replyto": "M9z5OJXfB2", "signatures": ["ICLR.cc/2026/Conference/Submission8979/Reviewer_WysJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8979/Reviewer_WysJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601805684, "cdate": 1761601805684, "tmdate": 1762920710952, "mdate": 1762920710952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first analyzes SSDA and separates the learning feature space into an essential shared space and a domain-related space. They analyze the error bound of shared space learning and conclude that it is helpful to highlight the shared space learning. Then they propose a gate network to learn shared features across both domains, and experiments with the proposed network as a SOTA method's plugin demonstrate the effectiveness of the gating mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. a straightforward but effective method."}, "weaknesses": {"value": "1. Lack of novelty in the theoretical analysis. The shared space analysis is mainly based on the community's common sense and does not add anything new to the SSDA study, which severely limits the submission's novelty.\n2. The details of the gated network are missing. The submission should clearly specify the gated network's design, including the number of layers, parameters, and channels, and so on. In the current version, it is hard to see why a gated network can improve learning in the shared feature space, which also limits the submission's novelty."}, "questions": {"value": "1. Please clarify the details of the gated network.\n2. Please clarify why gated network can improve the learning of shared feature space.\n3. Please provide a variant analysis of gated network design. For example. if cnn/transformer/cross-attention/MLP is used as the gated network, how about the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cdKLpL86ue", "forum": "M9z5OJXfB2", "replyto": "M9z5OJXfB2", "signatures": ["ICLR.cc/2026/Conference/Submission8979/Reviewer_CeJh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8979/Reviewer_CeJh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968529175, "cdate": 1761968529175, "tmdate": 1762920710328, "mdate": 1762920710328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of shared feature spaces in Semi-Supervised Domain Adaptation (SSDA) and proposes a gating-driven mechanism to explicitly filter out domain-specific features and emphasize domain-invariant ones. The authors provide a theoretical analysis showing that focusing on shared features reduces the total variation distance between domains and improves target-domain generalization. They implement a lightweight gating module that can be easily integrated into existing SSDA frameworks (e.g., MME, CDAC, ECB), achieving consistent accuracy gains across several benchmarks such as DomainNet and Office-Home."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tThe paper provides a clear theoretical analysis linking shared feature spaces to reduced domain discrepancy, which grounds the proposed method in formal reasoning.\n(2)\tThe proposed gating mechanism can be seamlessly incorporated into existing SSDA frameworks (e.g., MME, CDAC, ECB) without modifying their core objectives.\n(3)\tThe method consistently improves performance across multiple benchmarks and settings, showing both effectiveness and stability."}, "weaknesses": {"value": "(1)\tThe idea of leveraging shared feature spaces has already been explored in many prior SSDA and domain adaptation studies [1,2,3,4]. The authors’ related work does not discuss how this work provides a fundamentally new insight.\n(2)\tSome baselines, including a 2025 work cited by the authors, are not the most representative in the SSDA literature. It would strengthen the paper to compare with more recent and competitive baselines.\n(3)\tAlthough proposed mechanism improves the overall average performance, it does not achieve the best results on some specific transfer directions such as A→C and A→R in Table 3. The paper does not analyze or explain why the method underperforms in these cases.\n\n[1] Shared space transfer learning for analyzing multi-site fmri data, NeurIPS’20\n[2] Domain-specific feature unlearning for semi-supervised and unsupervised domain adaptation, ECCV’24\n[3] Domain Separation Networks, NeurIPS’16\n[4] Bridging Domains with Approximately Shared Features, Arxiv’24"}, "questions": {"value": "(1) Could the authors clarify what specific gap in prior shared-space research this paper aims to fill?\n(2) What are the possible reasons for lower performance on specific transfer pairs such as A→C and A→R (Table 3)?\n(3) Why were only subsets of DomainNet, Office-Home, and Office-31 chosen? (4) Do results generalize to other visual or non-visual domains (e.g., graph)?\n(5) Can the gating mechanism also benefit unsupervised DA (UDA) or multi-source DA? Have the authors tried zero-shot domain shifts?\n(6) Appendix C.5 mentions “minimal computational cost.” Can the authors provide concrete FLOPs or runtime comparisons versus baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dpvy9YArs7", "forum": "M9z5OJXfB2", "replyto": "M9z5OJXfB2", "signatures": ["ICLR.cc/2026/Conference/Submission8979/Reviewer_U88u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8979/Reviewer_U88u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969859480, "cdate": 1761969859480, "tmdate": 1762920709741, "mdate": 1762920709741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a framework to learn a shared space, which is implemented by a gating-driven SSDA enhancement mechanism. Furthermore, the paper theoretically reveals the advantages of learning a shared feature space for enhancing transferability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "While the paper theoretically analyzes the advantages of the shared feature space, offering valuable insights for newcomers to the field.  The overall logic of the paper is clear, the presentation is detailed, the theoretical hypotheses are elaborated on in depth, and the experimental results accurately demonstrate the experimental effects."}, "weaknesses": {"value": "1. Novelty is limited. \"Shared feature space\" is a fundamental concept in domain adaptation research, and this idea has been prevalent in the field for over a decade. The paper theoretically analyzes the advantages of the shared feature space, offering valuable insights for newcomers to the field. However, it does not introduce groundbreaking theoretical innovations to the core paradigm.\n2.  From an experimental validation perspective, the performance improvement brought by the proposed gating-driven SSDA enhancement mechanism is limited. As shown in Figure 3: (a)-(b), there is no significant observable change, making it difficult to discern the advantage of the proposed mechanism.\n3. Results comparison in Table 2 and Table 3 has no reference citation. It is suggested to add these citations.\n4. The dataset used in the paper can be expanded to include more datasets, and the scale of the dataset should also be verified using large-scale data."}, "questions": {"value": "As listed in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vjZNNk3tfy", "forum": "M9z5OJXfB2", "replyto": "M9z5OJXfB2", "signatures": ["ICLR.cc/2026/Conference/Submission8979/Reviewer_eoFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8979/Reviewer_eoFm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143240657, "cdate": 1762143240657, "tmdate": 1762920709204, "mdate": 1762920709204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}