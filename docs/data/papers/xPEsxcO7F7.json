{"id": "xPEsxcO7F7", "number": 3715, "cdate": 1757503334116, "mdate": 1763709784960, "content": {"title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward", "abstract": "A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. Despite numerous proposed methods, the community's focus on the standard reverse KL-divergence has led to a surprising oversight: the potential of alternative f-divergences as a proactive solution has been largely unexamined. We argue that standard RLVR objectives—both those using the mode-seeking reverse KL-divergence  and those forgoing a divergence term entirely—lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a 'rehearsal mechanism'. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Math and SQL generation experiments show that DPH-RL both improves in-domain Pass@1 and Pass@k scores and effectively prevents catastrophic forgetting on out-of-domain tasks. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.", "tldr": "We propose the DPH-RL framework, which uses f-divergence as a proactive 'rehearsal mechanism' to solve the solution diversity collapse problem that arises from fine-tuning LLMs with reinforcement learning.", "keywords": ["Large Language Model", "Reinforcement Learning with Verifiable Reward", "f divergence"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3d703ede2aa1ce803f447f54e47318e63e1572c5.pdf", "supplementary_material": "/attachment/03825b198988b0b77c92cf2a5b963d95d40b2ca9.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical paradox observed when fine-tuning large language models (LLMs) with Reinforcement Learning under Verifiable Rewards (RLVR): models often improve on single-attempt success (Pass@1) but see degraded multi-attempt performance (Pass@k) and suffer catastrophic forgetting of diverse skills. The authors identify that a key culprit is the choice of divergence in the RL objective. Prior approaches almost universally use a mode-seeking reverse KL divergence or even omit the divergence term, which provides no mechanism to retain broad knowledge. As a result, policies narrow toward a few high-probability solutions, sacrificing diversity.\nTo remedy this, the paper proposes Diversity-Preserving Hybrid RL (DPH-RL), a novel framework that uses mass-covering f-divergences (specifically forward-KL and Jensen–Shannon (JS) divergence) as a “rehearsal mechanism” to continually reference the initial policy. In effect, the divergence term itself becomes the solution: by penalizing divergence from the base model’s distribution, the agent is forced to maintain broad solution coverage and not forget previously learned behaviors. The approach is hybrid in that it selectively applies this divergence-based regularization, allowing unrestricted exploration on parts of the task where the base model wasn’t already proficient. Experiments on math problem solving and SQL query generation tasks demonstrate that DPH-RL not only improves in-domain Pass@1 accuracy, but crucially also boosts Pass@k (multiple attempts) performance above the base model’s level, while effectively preventing catastrophic forgetting on out-of-domain tasks. The method is also efficient: it computes the f-divergence via a generator function by sampling from the fixed initial policy, avoiding the need for an expensive separate reference policy network during training. Overall, this work highlights an overlooked axis in RL fine-tuning – the choice of divergence measure – as a powerful tool for building more general and diverse LLM-based problem solvers."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Important Problem & Insight: The paper tackles the important issue of diversity collapse in RL-based fine-tuning of LLMs, identifying an overlooked cause – the choice of KL divergence – and proposing it as a solution. This fresh insight (using forward-KL/JS to preserve knowledge) is highly original and addresses a real need.\nTheoretical Rigor: The work is grounded in rigorous theory. The Enhanced Monotonic Improvement Theorem 1 generalizes TRPO’s guarantee to f-divergence regularization, showing a strictly better improvement bound with an added positive term from the reference policy. The assumptions and proof techniques are sound and build on well-established analysis, lending strong credibility to the approach.\nStrong Empirical Results: The experiments are thorough and demonstrate clear benefits. DPH-RL consistently outperforms baseline methods (standard RL fine-tuning with reverse-KL, and variants without KL) by achieving higher Pass@k without sacrificing Pass@1. Notably, on an out-of-domain test (e.g. Spider SQL dataset), DPH-RL models maintain accuracy much closer to the base model than others, indicating greatly reduced forgetting. These results validate that the method fulfills its intended purpose of retaining diversity while improving performance.\nBroad Applicability: The approach is relatively general. It’s compatible with different divergence choices (the authors demonstrate forward-KL and JS, and discuss extensions to a family of f-divergences) and can be plugged into existing RL algorithms (they test with GRPO, DAPO, etc.). This generality means the contribution could impact various subfields, from RLHF for LLMs to other generative modeling scenarios where mode-covering behavior is desired.\nClarity and Execution: The paper is well-written with clear motivation and analysis. Figures and tables strongly back the claims (for example, qualitative differences in solution diversity under different divergences are illustrated in figures). The method is described in sufficient detail, and the authors even address practical considerations (like how to compute the forward/JS divergence term efficiently by sampling the base model, which avoids needing an extra model and thus is training-efficient). This makes the work not only theoretically strong but also feasible and useful in practice."}, "weaknesses": {"value": "While the paper presents a compelling and well-substantiated approach, a few limitations are worth noting:\n\nReliance on Base Policy Quality: The effectiveness of DPH-RL depends on the strength of the base model. If the base policy performs poorly on the target domain, there is little knowledge to rehearse, reducing the benefit of the divergence term.\n\nManual Partitioning: The hybrid framework splits training data into exploration and rehearsal sets using fixed thresholds on success rates. This manual setup introduces sensitivity to hyperparameters and may not adapt well across tasks without tuning.\n\nLimited Divergence Exploration: Although the authors motivate the use of forward-KL and JS divergence, they do not evaluate a broader set of f-divergences or analyze tradeoffs across this family. It's unclear if the performance gains are tied to these specific choices or generalize more broadly.\n\nScope of Evaluation: The experiments are confined to two domains (math and SQL) and modest-sized models. Broader tasks and larger-scale settings would strengthen claims of generality.\n\nPreservation of Undesirable Behavior: The method encourages retention of base model behavior, which may include suboptimal or biased responses. While not observed in current tasks, this risk should be acknowledged in future applications.\n\nThese issues do not undermine the core contributions but indicate valuable directions for further investigation and refinement."}, "questions": {"value": "Adaptive vs. Fixed Data Partition: In your method, you partition the training data into D<sub>pef</sub> (where the base policy is nearly perfect) and D<sub>exp</sub> (exploration needed) using a fixed success-rate threshold. How sensitive are the results to this threshold choice, and did you consider a more dynamic criterion? For example, could the algorithm online-adjust the weight of the divergence term based on the model’s current performance on a state or task (rather than a hard split)? Such an adaptive approach might remove the need for manual threshold tuning; we’re curious if you experimented with or have thoughts on this extension.\nChoice of Divergence Measure: You demonstrated significant gains with forward-KL and JS divergence. Did you try other f-divergences (e.g., intermediate alpha-divergences or Csiszár divergences) in practice, and if so, how did they compare? The theory suggests a family of options – is forward-KL essentially the strongest “mass-covering” extreme, or might a milder divergence also strike a good balance? Clarification on why forward-KL and JS were chosen (and whether one consistently outperforms the other in different conditions) would be helpful for practitioners looking to apply your framework.\nBehavior with Minimal Base Knowledge: In scenarios where the base model has very low capability (e.g., near 0% success on D<sub>exp</sub> tasks), does DPH-RL effectively revert to a standard RL algorithm? Your analysis indicates that if the reference policy has no advantage (delta=0), the improvement bound reduces to the normal TRPO guarantee. Empirically, have you observed any downside to applying DPH-RL in such cases (perhaps the forward-KL term just stays inactive)? It would be reassuring to know that one at least loses nothing by using DPH-RL when the base model provides no guidance – confirming the method is robust in worst-case scenarios.\nGenerality to Other Settings: Can you comment on applying DPH-RL beyond the RLVR paradigm? For instance, in a standard RLHF setup (with a learned reward model and possibly no programmatic correctness signal), do you foresee any challenges in using a forward-KL/JS regularizer to preserve the base model distribution? Intuitively it should still help prevent policy collapse, but are there issues if the reward signal is less directly tied to correctness? Any insight or preliminary experiments in non-code, non-math tasks (e.g., dialogue) would be interesting to hear about.\nBalancing Diversity and Quality: One might wonder if there are cases where preserving diversity could conflict with optimizing reward (e.g., if some of the base model’s diverse outputs are actually incorrect or suboptimal). Did you observe any instances where the forward-KL term kept the model generating suboptimal solutions for the sake of diversity? It seems your method focuses the preservation on high-performing reference behaviors (since D<sub>pef</sub> is constructed from successful attempts), which mitigates this. Still, any anecdotal observations on how DPH-RL balances keeping “good” diverse solutions versus discarding genuinely bad ones would provide intuition on the method’s nuances."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xNBmPoLOYo", "forum": "xPEsxcO7F7", "replyto": "xPEsxcO7F7", "signatures": ["ICLR.cc/2026/Conference/Submission3715/Reviewer_HqbP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3715/Reviewer_HqbP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785021464, "cdate": 1761785021464, "tmdate": 1762916943565, "mdate": 1762916943565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the effect of divergence function choice on catastrophic forgetting, **pass@k** degradation, and diversity collapse in RL with verifiable rewards. The authors study the effects of reverse-KL on knowledge retention (described as performance preservation on tasks already solved before RLVR). Doing away with the mode-seeking reverse-KL, the authors propose a diversity-preserving framework that maintains task coverage and leads to both **pass@1** and **pass@k** improvements. The method has two stages: (1) a pre-sampling stage where the dataset is separated according to how solvable the problems are for the base model, (2) online training with two distinct loss functions—one without KL on the exploratory partition and another using mass-covering divergence functions on the mastered partition from (1). This is validated across Llama and Qwen models, showing great promise in mitigating the specialized performance vs. sampling diversity tradeoff."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "-  **Line 86 to 88** : very good distinction of RL vs sequential generation with NNs. Important to disambiguate the problems between the two fields\n\n- Clear introduction and preliminaries, easy to follow and well-motivated\n\n- Addresses a core issue in the field\n\n- No need to run inference on the reference model in the online training loop"}, "weaknesses": {"value": "- Relies on a good base model and lacks analysis of how balanced the partitions in the pre-sampling phase stage should be \n\n- Figures axis are sometimes tough to read. Please use a standard plotting sofware with clear labels \n\n- Unclear use of the term \"stability\"\n\n- Trains only on two Qwen models which is insufficient given the claims of the paper"}, "questions": {"value": "1. What do the authors mean by in-domain and out-of-domain benchmarks at the end of the introduction? \n\n2. Why is assumption 1 well-founded? \n\n3. What happens if the partitions are not well-balanced? How possible is it and what are the contingencies? \n\n4. In section 4.2, why is JS more stable? Is it due to it being more symmetric? Please clarify in either case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3upEP5DN3e", "forum": "xPEsxcO7F7", "replyto": "xPEsxcO7F7", "signatures": ["ICLR.cc/2026/Conference/Submission3715/Reviewer_NVcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3715/Reviewer_NVcc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871340241, "cdate": 1761871340241, "tmdate": 1762916943220, "mdate": 1762916943220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-exames the KL penalty term in RLVR with LLMs. Most approaches adopt the reverse-KL, which may degrade LLMs’ Pass@K performance. To mitigate this problem, this paper proposes to use forward-KL and consider a more general divergence family, f-divergences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies an important and interesting problem in RLVR that how the KL divergence affects RL training."}, "weaknesses": {"value": "1. The motivation for forward-KL is not strong enough.\n    \n    The failure of reverse-KL does not point to forward-KL or JS divergence, especially when authors did not analyze the failure reason for reverse-KL.\n    \n2. The lack of discussion with replay buffer.\n    \n    The implementation of forward-KL requires collecting trajectories from the base model, which can be consider as a special rehearsal mechanism. Replay buffers and their application are widely studied in both of continual learning and RL. Some discussions of replay buffer may clarify this paper’s position.\n    \n3. The necessity of splitting dataset into “near-perfect” or “exploration”.\n    \n    > For queries where π_ref performs well, aggressive reward maximization is unnecessary and risks degrading performance.\n\n    If this claim is true, then any RL post-training should not include “easy” questions. However, LLMs tend to forget prior learned knowledge. Adding easy questions can help to remind LLMs, aligning author’s intention of rehearsal-stylish forward-KL.\n    \n4. The lack of in-depth analysis of forward-KL.\n    \n    Though forward-KL seems to achieve empirical success, its mechanism is still under unexplored. Although authors provide the Monotonic Improvement of the method, it is hard to connect it with the improvement. Moreover, the discussion in section 6 only shows a ratio between training, which does not contain too much information.\n\n5. Authors should set a pointer to explicitly link the definition of alpha-divergence."}, "questions": {"value": "See the above weakness.\n\nAdditionally:\n1. What does “greedy” mean in the experiment section? Pass@1 or greedy decoding?\n2. What is $\\pi_{\\text{per}}$ in section 4.3?\n3. What does the alpha in Table 2 mean? alpha-divergence?\n4. If our goal is to improve pass@K performance and we know that the base model has the highest pass@K performance, then what is the point of RL post-training?\n5. Do authors have any idea why RL training hurts pass@K performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X6d1E6F5yN", "forum": "xPEsxcO7F7", "replyto": "xPEsxcO7F7", "signatures": ["ICLR.cc/2026/Conference/Submission3715/Reviewer_8NDo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3715/Reviewer_8NDo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180982304, "cdate": 1762180982304, "tmdate": 1762916942893, "mdate": 1762916942893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a divergence-based policy optimization framework that replaces the conventional reverse KL term with forward KL or Jensen–Shannon (JS) divergence. The authors argue that forward KL can mitigate mode-seeking behavior and stabilize updates. Experiments on two representative models demonstrate improved training stability and comparable or better performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method is conceptually simple and easy to implement.\n* Theoretical motivation is clear: reinterpreting divergence choice (reverse vs. forward KL) in policy optimization.\n* Experimental results on two models provide empirical support for the claimed benefits."}, "weaknesses": {"value": "* The main idea—switching from reverse KL to forward KL or JS divergence—has been extensively discussed in prior work [1]. The paper should (a) more directly situate DPH-RL relative to that prior theory, (b) clarify what is new beyond applying these known divergence properties to RLVR for LLMs, and (c) compare empirically to algorithms motivated by those analyses.\n\n[1] Chan A, Silva H, Lim S, et al. Greedification operators for policy optimization: Investigating forward and reverse kl divergences[J]. Journal of Machine Learning Research, 2022, 23(253): 1-79."}, "questions": {"value": "* How sensitive are the results to the choice of temperature or scaling in the KL term?\n* Did you tune the KL coefficients separately for reverse and forward cases to ensure fairness?\n* Can the method be applied to off-policy or batch RLHF settings, where distribution mismatch is more severe?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L58BuruGto", "forum": "xPEsxcO7F7", "replyto": "xPEsxcO7F7", "signatures": ["ICLR.cc/2026/Conference/Submission3715/Reviewer_3S5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3715/Reviewer_3S5Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762775594829, "cdate": 1762775594829, "tmdate": 1762916942697, "mdate": 1762916942697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the diversity collapse problem in RLVR for LLM post-training. The authors identify that the commonly used reverse-KL divergence is a key contributor to this problem, as its mode-seeking nature forces policies to converge on single high-probability solutions, leading to degraded Pass@k performance and catastrophic forgetting. To address this, the paper proposes DPH-RL, which leverages mass-covering f-divergences as a \"rehearsal mechanism\" to maintain broad solution coverage. Experiments on SQL generation and mathematical reasoning tasks with Llama and Qwen models demonstrate that DPH-RL improves both Pass@1 and Pass@k metrics while mitigating catastrophic forgetting on out-of-domain tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper targets an important limitation of current RLVR methods: the degradation of Pass@K performance despite improvements in Pass@1.\n\n- The paper provides a comprehensive analysis of diversity collapse from the perspective of the KL divergence term, with both theoretical and empirical evidence.\n\n- The method is orthogonal to existing approaches, allowing for potential combinations and broader applicability."}, "weaknesses": {"value": "- The improvements are modest, and no single DPH variant consistently outperforms the baselines, which limits practical usage, i.e., different variants achieve best results on different datasets, making it unclear which configuration to use in practice.\n\n- Although the paper's motivation is to address Pass@K degradation in RLVR, the proposed method does not convincingly alleviate this problem. Besides, the evaluation is limited to Pass@64, and larger K values (e.g., Pass@256, Pass@1024 done in [1]) are not reported. Such evaluations would better demonstrate whether the method truly preserves diversity at scale.\n\n- The data partitioning setting lacks justification and ablation studies. The correctness threshold differs across models (6/8 for Llama, 7/8 for Qwen) without a clear explanation or ablation studies. It is unclear how robust the method is to different threshold choices.\n\n[1] Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g01OiWWWCK", "forum": "xPEsxcO7F7", "replyto": "xPEsxcO7F7", "signatures": ["ICLR.cc/2026/Conference/Submission3715/Reviewer_jdRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3715/Reviewer_jdRw"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762858903567, "cdate": 1762858903567, "tmdate": 1762916941705, "mdate": 1762916941705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}