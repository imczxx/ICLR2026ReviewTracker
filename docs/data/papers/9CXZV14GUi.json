{"id": "9CXZV14GUi", "number": 19241, "cdate": 1758294738939, "mdate": 1759897050291, "content": {"title": "Can VLMs Reason Through Multiple Views?", "abstract": "Recent advances in Vision–Language Models (VLMs) have opened new possibilities for complex spatial reasoning. Benchmarks for VLMs largely assess single- or limited-view perception, leaving untested the core ability to integrate observations across viewpoints into a coherent 3D understanding. We introduce MVBench, a benchmark expressly designed to evaluate multi-view integration for holistic 3D scene comprehension. MVBench is paired with a highly extensible data-generation pipeline that supports plug-and-play 3D assets (synthetic or real), configurable distractors, and flexible camera positions and orientations, enabling researchers to readily instantiate new datasets by swapping assets or altering viewpoint configurations. Beyond benchmarking, MVBench serves as a fundamental diagnostic that VLMs should pass before being deployed as agents operating 3D software for downstream tasks such as 3D assets generation and part assembly for mechanical engineering. We evaluate a broad set of frontier VLMs and uncover consistent failure modes: strong performance on 2D planar relations from a single image, but marked difficulty with 3D spatial relations and with aggregating information across views. We further identify biases in VLMs, including handling unconventional axis directions and sensitivity to object colorways and texture variations. Acknowledging these limitations, we propose ViewNavigator, a multi-agent framework that actively selects informative viewpoints, perceive, and fuses multi-view evidence through belief-updating. ViewNavigator improves the performances of diverse base models on MVBench by more than 50%. MVBench and its extensible pipeline are designed to equip researchers with a principled testbed for strengthening VLMs’ 3D scene understanding, paving the way for more capable VLM-based agents that can support a wide range of downstream 3D tasks.", "tldr": "We benchmarked VLMs' multi-view reasoning ability, discovered common failure patterns and biases, proposed a multi-agent system that improves the performance.", "keywords": ["LLM/VLM", "Benchmark", "Agentic System", "Multi-view Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dba4d59d224d5eaa54347142798e45aa8f25d3f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark to test the spatial reasoning of Vision Language Models (VLMs).  The benchmark is deliberately designed to test the ability to VLMs to utilize information from multiple views.  The authors use the new benchmark to test a variety of frontier VLMs and identify common failure modes.  They also propose an agentic framework for selecting informative views, which is shown to give a large performance for all the VLMs tested."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This task seems very useful for any workflows related to the assembly of individual assets or components in a virtual environment.  The benchmark will be useful for the community.\n\nThe analysis of failure modes was interesting and useful.\n\nIn addition to identifying problems, the authors propose the ViewNavigator system which clearly helps boost performance."}, "weaknesses": {"value": "I couldn’t see any discussion for how the object poses were chosen.  Section 4.2 discusses only the relative placement (translation) of the objects, but not their pose (rotation).  Looking at Figure 2 I’m expecting the pose of the chair to be facing the table.  Using this semantic information, I can guess the relative placement of the two objects.   It feels like the benchmark is designed to check that the VLM doesn’t make this kind of assumption.\n\nThe additional cost of using the ViewNavigator is not discussed.  While it clearly provides great benefit, it would be useful to weigh this against the additional cost."}, "questions": {"value": "How much did it cost to get the observed performance boost using the ViewNavigator?  Figure 7b is a nice comparison of the performance for a base method and  ViewNavigator.  Knowing the relative cost for each VLM would be useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P8TImMY8qz", "forum": "9CXZV14GUi", "replyto": "9CXZV14GUi", "signatures": ["ICLR.cc/2026/Conference/Submission19241/Reviewer_rgfn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19241/Reviewer_rgfn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960348652, "cdate": 1760960348652, "tmdate": 1762931219599, "mdate": 1762931219599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MVBench, a benchmark expressly designed to evaluate a VLM's ability to integrate information from multiple viewpoints for 3D scene understanding. A key contribution is the benchmark's extensible data-generation pipeline, which allows researchers to procedurally create new scenes with varied 3D assets and camera setups. An evaluation of several frontier VLMs on MVBench reveals consistent failure modes: while models can handle 2D planar relations, they exhibit marked difficulty with 3D spatial relations and aggregating information across views. The study also uncovers strong inductive biases, such as a reliance on conventional coordinate axis orientations. To address these shortcomings, the authors propose ViewNavigator, a multi-agent framework that actively selects informative viewpoints and fuses multi-view evidence via belief-updating. This framework is shown to improve the performance of base models on MVBench by over 50%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A contribution is the proposed benchmark is not a static dataset. It allows researchers to construct VQA using different 3D assets, which has further room to be extended in the future.\n\n2. The analysis is thorough, moving beyond simple accuracy to uncover further failure patterns. It reveals that VLMs excel at 2D planar relations but fail to aggregate this information into a coherent 3D model\n\n3. The paper presents an interesting motivation and explain why such benchmark would be valuable."}, "weaknesses": {"value": "1. As a paper where the new benchmark is the core contribution, the paper fails to explain the dataset composition clearly, which includes how many data is created, how many objects are typically involved in a scene and so on. Also, the entire pipeline is synthetic and all images are rendered in Blender. The benchmark does not test the ability of models to handle the complex lighting, textures, and occlusions of real multi-view 3D scenes.\n\n2. The benchmark claims it is targeted at 3D scene comprehension, but the only task evaluated is determining the relative position of a target object from a central object. This is a limited, low-level proxy for the high-level reasoning, like part assembly, that the paper uses as its core motivation.\n\n3. For a benchmark, the evaluation set is small if I understand it correctly. The paper reports using 100 tasks per task variant for the main evaluation and only 50 tasks for the ViewNavigator experiments. If each task is one VQA pairs, the small evaluation set may not be robust and informative enough to draw conclusions from.\n\n4. The ViewNavigator agent is relatively simple. It is heuristic and based on jittering views and vote-counting. The LLM planner's job is reduced to resolving uncertainty on a simple 3-vector."}, "questions": {"value": "Please refer to the weakness session."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IgquPRlPEC", "forum": "9CXZV14GUi", "replyto": "9CXZV14GUi", "signatures": ["ICLR.cc/2026/Conference/Submission19241/Reviewer_Jiec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19241/Reviewer_Jiec"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771330386, "cdate": 1761771330386, "tmdate": 1762931218931, "mdate": 1762931218931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MVBench, a benchmark designed to specifically test VLMs on their ability to perform multi-view integration for holistic 3D scene understanding. The core task involves inferring the 3D relative position of two objects from a set of rendered images. The authors identify key failure modes in current frontier close-sourced VLMs concerning 3D spatial reasoning and multi-view integration, leading to the proposal of ViewNavigator, an active planning, multi-agent framework designed to boost performance on the benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper's structure is clear, progressing from the motivation behind the problem to the construction of the benchmark, and finally to the testing and analysis of the results.\n2. The paper includes rich charts, attempting to analyze model performance from multiple aspects."}, "weaknesses": {"value": "1. The visual quality of the synthesized images is poor. The geometries used are simple and limited, and the rendering is far from photo-realistic, making it distant from real-world application domains. Overall, the data collection method is simple and direct, lacking significant technical difficulty or contribution.\n2. The question format is monotonous, and it lacks obvious application-level significance. It's unclear what practical value this specific format provides beyond a diagnostic label. Basic information about the benchmark size, such as the total number of Q&A pairs, is not clearly stated. \n3. The evaluation primarily relies on testing commercial, closed-source model APIs, with a lack of testing on other open-source or academic VLM architectures, which limits its practical guidance for VLM developers.\n4. The proposed ViewNavigator, presented as a major contribution, is only allocated a very small section of the paper, and the description of its modules, workflow, and underlying mechanisms is insufficiently detailed."}, "questions": {"value": "1. What is the specific positioning of this benchmark regarding difficulty and the problem it aims to solve? Compared to the numerous existing VLM benchmarks, where do the unique value and distinctiveness of MVBench lie?\n2. Given the finding that performance drastically improves when breaking down the task into 2D single-view prediction, does the primary value of MVBench reside in diagnosing the failure of 3D integration, rather than being a challenging task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XlXuIOrTOy", "forum": "9CXZV14GUi", "replyto": "9CXZV14GUi", "signatures": ["ICLR.cc/2026/Conference/Submission19241/Reviewer_qFhw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19241/Reviewer_qFhw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884757872, "cdate": 1761884757872, "tmdate": 1762931218564, "mdate": 1762931218564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MVBench, a benchmark designed to test whether VLMs can fuse information from multiple views to form a coherent 3D understanding, along with a framework for generating scenes to scale the benchmark. Evaluating frontier VLMs on MVBench, authors find failure patterns that models struggle with 3D spatial relations and aggregating information across views, and they show biases to axis conventions and color/texture variations. To mitigate these issues, the paper proposes ViewNavigator, a multi-agent framework that selects informative viewpoints and fuses multi-view evidence via belief updating, yielding large gains over base models on MVBench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Presents a new novel dataset that is tailored for measuring multi-view-based reasoning ability of VLMs, along with detailed descriptions on the dataset construction process.\n\n* Delivers specific failure pattern analysis of state-of-the-art VLMs based on the proposed benchmark, offering new insights on their limitation on multi-view inputs.\n\n* Along with the evaluation framework, authors further suggest a solution agentic system that performs accurate spatial reasoning based on multi-view observations."}, "weaknesses": {"value": "* Overall, while the paper addresses an important problem of **multi-view reasoning** for VLMs, the proposed benchmark and method are highly task-specific and do not convincingly demonstrate generalized multi-view reasoning ability. Given that prior benchmarks already cover multi-view reasoning, using both synthetic and real images, the contribution of this work is unclear.\n\n* There are multiple missining citations and discussions regarding closely related benchmarks on multi-view spatial reasoning. Discussions on the novely of MVBench in comparison to these benchmark should be provided. Benchmarks like **SITE [1], MM-Spatial [2], and SPAR-Bench [3]** include a rich set of multi-view QA tasks to asses multi-view reasoning of VLMs. Moreover, these works also include real-image data.\n\n* From the examples, the dataset appears to be a trivial, synthetic collection of tasks, raising concerns about whether it truly assesses VLM's ability to reason over multi-view inputs in the wild. Given that several prior benchmarks already provide principled data-generation pipelines for multi-view reasoning with real-world scenes (e.g., MindCube [4], SPAR-Bench [3], MM-Spatial [2]), the motivation and technical contribution of introducing a much simpler dataset are unclear.\n\n* The connection between the **furniture part–assembly task** and real-world multi-view reasoning is unclear. The task appears overly specialized rather than general, and in many cases part assembly seems solvable with a single view. Could the authors clarify why a multi-view formulation is necessary in this case?\n\n* **ViewNavigator** may address the specific problem posed, but it is unclear whether it improves the underlying VLM’s intrinsic spatial reasoning. While the paper’s main goal is to diagnose limitations in multi-view reasoning, the ViewNavigator framework primarily builds an agentic system that compensates for those limitations. There seems to be a misalignment between the problem definition and solution in the paper.\n\n---\n\n[1] SITE: towards Spatial Intelligence Thorough Evaluation, Wang et al., ICCV 2025\n\n[2] MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs, Dexberger et al., ICCV 2025\n\n[3] From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D, Zhang et al., NeurIPS 2025\n\n[4] Spatial Mental Modeling from Limited Views, Yin et al., 2025"}, "questions": {"value": "* While the paper focuses on synthetic rendering setups, could the proposed framework be extended to more realistic data synthesis? For instance, adding a room layout and placing multiple object and rendering from multiple viewpoints could extend MVBench to include more realistic images.\n\n* Does fine-tuning VLMs on the proposed benchmark lead to improvement in VLMs' mutli-view reasoning? I believe this would clarify whether the proposed benchmark is a reasonable task or not. For reference, multiple spatial reasoning benchmark works show that their data indeed improves the base VLMs through fine-tuning: SAT [1], MindCube [2], SPAR-Bench [3], RoboSpatial [4].\n\n---\n\n[1] SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models, Ray et al., COLM 2025\n\n[2] Spatial Mental Modeling from Limited Views, Yin et al., 2025\n\n[3] From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D, Zhang et al., NeurIPS 2025\n\n[4] RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics, Song et al., CVPR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zOCylZ9jbQ", "forum": "9CXZV14GUi", "replyto": "9CXZV14GUi", "signatures": ["ICLR.cc/2026/Conference/Submission19241/Reviewer_W6cC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19241/Reviewer_W6cC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963828004, "cdate": 1761963828004, "tmdate": 1762931218095, "mdate": 1762931218095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}