{"id": "aQWSEjcN9V", "number": 4884, "cdate": 1757783465823, "mdate": 1759898007403, "content": {"title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "abstract": "In this paper, we explore how to empower general-purpose Vision-Language Models (VLMs) to control humanoid agents. General-purpose VLMs (e.g., GPT-4) exhibit strong open-world generalization, and remove the need for additional fine-tuning data. To build such an agent, two key components are required: (1) an embodied instruction compiler, which enables the VLM to observe the scene and translate high-level user instructions into low-level control parameters; and (2) a motion executor, which generates human motions from these parameters while adapting to real-time physical feedback.\nWe present BiBo, a VLM-driven humanoid agent composed of an embodied instruction compiler and a diffusion-based motion executor. The compiler interprets user instructions in context with the environment, and leverages a chain of visual question answering (VQA) to guide the VLM in specifying control parameters (e.g., motion captions, locations). The diffusion executor extends future joint trajectories from prior motion, conditioned on both control parameters and environmental feedback.\nExperiments demonstrate that BiBo achieves an interaction task success rate of 90.2\\% in open environments, and improves the precision of text-guided motion execution by 16.3\\% over prior methods. BiBo handles not only basic interaction but also diverse motions, and even dancing while striking at a sandbag. The code will be released upon publication.", "tldr": "", "keywords": ["human-scene interaction", "VLM agent", "motion generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/268b6c32974b0229c50c13f0a70565f74eeccc88.pdf", "supplementary_material": "/attachment/03146b4972cf61928e1624e937eda8dd2be97815.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the BiBo framework, which aims to leverage off-the-shelf Vision-Language Models (VLMs) to control humanoid agents for interaction in open physical environments. The paper designs an embodied instruction compiler and a diffusion model-based motion executor to realize the translation from high-level instructions to low-level actions, and verifies their effectiveness on multiple tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See Summary."}, "weaknesses": {"value": "1.The core innovations of this study are \"reducing data dependency via off-the-shelf VLMs\" and \"solving motion continuity issues using LDM (Latent Diffusion Model) + VAE (Variational Autoencoder)\". However, the generalization ability in real physical environments has not been fully verified. BiBo’s interaction capability has not been validated in real-world scenarios (non-InfiniGen-generated simulated environments), making it impossible to prove its effectiveness under complex real conditions such as lighting changes and irregular objects.\n2.The dataset used in this study has limitations, and its generalization is questionable. Experiments are only conducted on the HumanML3D dataset and InfiniGen-generated scenes, without validation on other public datasets (e.g., BABEL, AMASS). This makes it difficult to demonstrate the model’s generalization ability across a wider range of action types and scenes.\n3.The evaluation metric system is incomplete. Although FID and R-Precision are used to evaluate motion quality, quantitative analysis of key dimensions in interaction tasks—such as \"interaction naturalness\" and \"task coherence\"—is omitted. It is recommended to supplement quantitative analysis of metrics like Interaction Accuracy and Task Coherence.\n4.There is a lack of real-time performance analysis. Although the paper claims that BiBo supports real-time control (>20 Hz), it does not provide analysis of specific memory usage or GPU memory consumption. Performance benchmarks on typical hardware platforms should be provided to verify its deployment feasibility.\n5.There is no systematic analysis of the method’s limitations. The paper does not discuss BiBo’s performance in extreme scenarios (e.g., occlusion, lighting changes) nor analyze the impact of VLM reasoning errors on the entire system. Such discussions should be supplemented in the main text or appendix.\n6.In the ablation study (Table 2), control groups for \"without Pose Reasoning (w/o Pose Reasoning)\" and \"without Joint Generation (w/o Joint Generation)\" need to be added to quantify the impact of each stage on task success rates (e.g., positioning accuracy for sitting tasks, joint control accuracy for touching tasks) and clarify the necessity of the three-stage process.\n7.It is recommended to supplement failure case analysis in the main text or appendix, including the category of failed tasks, root causes (e.g., pose recognition errors, insufficient collision handling), and potential improvement directions, to enhance the transparency of the research."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BeOF08n2l2", "forum": "aQWSEjcN9V", "replyto": "aQWSEjcN9V", "signatures": ["ICLR.cc/2026/Conference/Submission4884/Reviewer_hAat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4884/Reviewer_hAat"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535954349, "cdate": 1761535954349, "tmdate": 1762917737351, "mdate": 1762917737351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BiBo (Building humanoId agent By Off-the-shelf VLMs), a novel framework that leverages off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents. The core idea is to reduce data collection and training costs by combining pre-trained VLMs with a tailored embodied system for humanoid control. BiBo consists of two main components:\n\n- Embodied Instruction Compiler: Converts high-level natural language commands into low-level structured motor commands by reasoning over scene context.\n- Diffusion-based Motion Executor: Generates smooth, human-like motion trajectories while dynamically adapting to environmental feedback using a combination of Latent Diffusion Models (LDMs) and Inverse Kinematics (IK) optimization.\nThe authors highlight BiBo's ability to perform diverse and complex physical interactions in dynamic environments, achieving a task success rate of 90.2% and improving motion execution precision by 16.3% compared to prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- VLM Agent Workflow for Complex Task Understanding\n\nThe Embodied Instruction Compiler is well-designed, using a structured three-step reasoning process (attribute analysis, pose reasoning, and joint generation) to translate high-level commands into low-level motor instructions. This design allows BiBo to accurately interpret user intent and adapt to complex tasks in dynamic physical environments, such as sitting, lifting objects, or interacting with multiple scene elements. The use of voting mechanisms and multi-view representations further improves the system's robustness in understanding intricate tasks.\n\n- Novel Integration of CLoSD + IK for Diffusion Motion Updates\n\nThe combination of CLoSD (a physics-based motion-tracking policy) and Inverse Kinematics for refining humanoid motion is inspiring. The framework dynamically corrects motion trajectories by incorporating physical feedback from the environment, ensuring smooth and continuous motion even in challenging scenarios (e.g., collisions, external forces). This joint optimization approach enhances both the adaptability and precision of motion generation, particularly for tasks requiring fine-grained control (e.g., grasping or touching objects). The use of Latent Diffusion Models (LDMs) further enables the generation of high-fidelity motions while maintaining computational efficiency. This part could be considered the most inspiring in this paper."}, "weaknesses": {"value": "- Unclear Execution of Motion with CLoSD for Dynamic Objects\n\nThe paper lacks clarity on how the generated motion trajectories are passed to CLoSD for execution. For instance, when an object moves unpredictably, does the system rely on CLoSD alone for tracking, or does it dynamically update the motion plan using feedback? What if the dynamic object encounters collision with hands? While the authors mention incorporating physical feedback into motion updates, the explanation of how BiBo handles motion retargeting or re-planning in the presence of dynamic objects is insufficient. This aspect deserves more detailed discussion and evaluation.\n\n\n- Limited Discussion and Citation of Related Work\n\nThe paper does not sufficiently discuss its approach to prior work in key areas, such as LLM planning, long-term task completion, or the use of diffusion models in HSI motion generation. For instance:\n\n[1] SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation. ICCV2025\n\n[2] Synthesizing Physically Plausible Human Motions in 3D Scenes. 3DV2024\n\n[3] Generating Human Interaction Motions in Scenes with Text Control. ECCV2024\n\nIt is clear that TESMO[3] is exactly the type of previous approach this paper aims to compare against: it introduces discontinuity by conditioning on past generated rather than executed results.\nAt the very least, providing sufficient discussion and citations would improve my impression of this paper.\n\n- Not complicated enough environments as claimed\n\nMost environments in demo videos feature a single piece of furniture on flat ground, not challenging enough as claimed."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BAzlbREWg3", "forum": "aQWSEjcN9V", "replyto": "aQWSEjcN9V", "signatures": ["ICLR.cc/2026/Conference/Submission4884/Reviewer_V4s2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4884/Reviewer_V4s2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891389894, "cdate": 1761891389894, "tmdate": 1762917736892, "mdate": 1762917736892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BiBo (Building humanoId agent By Off-the-shelf VLMs) — a framework that connects general-purpose Vision-Language Models (VLMs) like GPT-4 to humanoid control. The key idea is to use powerful pre-trained multimodal models to bypass costly humanoid-specific data collection and training. BiBo has two major components: 1. Embodied Instruction Compiler – translates high-level natural language instructions (e.g., “have a rest”) into structured, low-level control commands (e.g., sitting location, facing direction, joint targets) through a three-stage visual Q&A process. 2. Diffusion-based Motion Executor – a latent diffusion model (LDM) that generates continuous, physically-plausible humanoid motion conditioned on those commands and on real-time physical feedback."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe idea of directly plugging an off-the-shelf VLM (GPT-4o) into a humanoid control pipeline is innovative. Avoids re-training large models by adding a lightweight compiler layer.\n2.\tThe compiler–assembler analogy is clear and intuitive: the VLM acts like a “compiler” converting high-level language into structured commands, while the motion diffusion module serves as an “assembler” for physical actuation.\n3.\tThe Latent Diffusion Model with joint decoding of executed and generated latents ensures temporal continuity and environmental awareness — addressing a major weakness in previous motion diffusion frameworks.\n4.\tThe experiment is also comprehensive"}, "weaknesses": {"value": "1.\tThe motions shown in the video do not fully comply with physical laws. During interactions with objects, there are visible cases of hovering and penetration, which make it appear that the human keypoints are rule-based attached to the objects rather than physically constrained. The interactivity seems weaker compared with methods such as UniHSI.\n2.\tThe motion generation in the video appears to heavily depend on the VLM’s outputs. However, the VLM tends to exhibit strong hallucination problems during grounding. It is unclear how the authors constrain the frame-to-frame consistency — both in the diffusion process and within the Embodied Instruction Compiler.\n3.\tDuring locomotion, the agent sometimes floats or teleports. Yet, in Table 3, BiBo is reported to outperform others on the skating and floating metrics. Providing a more detailed definition and evaluation standard for these metrics would make the results more convincing."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MB4XWncktq", "forum": "aQWSEjcN9V", "replyto": "aQWSEjcN9V", "signatures": ["ICLR.cc/2026/Conference/Submission4884/Reviewer_U7bF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4884/Reviewer_U7bF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900216081, "cdate": 1761900216081, "tmdate": 1762917708197, "mdate": 1762917708197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the interesing problem of humanoid agents which can handle flexible and diverse interactions in open environments. To avoid collecting massive dataset to train the model, the paper presents a new solution by utilizing the capability of the strong VLMs and a diffusion-based motion generator. The former is used to generate the primitive commands based on the user instruction and the latter is used to generate the corresponding motions. Experiments show the promising results of the proposed algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of utlizing the strong capability of VLM to decompose the primitive commands and later handled by a motion generator is interesting. \n\n* The experiments show the proposed algorihtm obtains promising results in the challenging problem of interaction with the open environments. \n\n* The paper is well presented and the proposed algorithm should be easy to reproduce."}, "weaknesses": {"value": "* The paper relies on two components to handle the target problem. On one hand, currently, even the SOTA vlms may not be able to produce the precise primitive actions. To simply the problem, the paper presents a set of predefined actions but it still cannot guarantee a robust results. On the other hand, assume vlms can produce the accurate action motions, how to obtain a good motion is not a trivial task. It should provide more justification that why the presented motion executor can produce the desired results.\n\n* The experimental results in Table 1 is evaluated based on a setting proposed by this paper. The task as well as the setups are introduced in this paper. How about the generation of the proposed algorithm to other benchmarks? \n\n* For the results in Table 2, the result of \"Lift\" is much lower than other categories. What are the potential reasons for this? \n\n* Currently, BiBo are operated in the virtual setting. Is it possible to proivde some evaluations which can show that the proposed algorithm can generate to real-cases like robots?"}, "questions": {"value": "Please address the questions in the weakness section. More specifically, please mainly address the questions related work the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "81ndCq1Qfk", "forum": "aQWSEjcN9V", "replyto": "aQWSEjcN9V", "signatures": ["ICLR.cc/2026/Conference/Submission4884/Reviewer_u3aD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4884/Reviewer_u3aD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920784066, "cdate": 1761920784066, "tmdate": 1762917702254, "mdate": 1762917702254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}