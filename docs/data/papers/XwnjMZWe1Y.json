{"id": "XwnjMZWe1Y", "number": 6068, "cdate": 1757952038174, "mdate": 1759897936970, "content": {"title": "LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model", "abstract": "While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \\textbf{LUMA} (\\textit{\\textbf{L}ow-dimension \\textbf{U}nified \\textbf{M}otion \\textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.", "tldr": "", "keywords": ["Human Motion Generation", "Text-to-Motion", "Diffusion Models", "Representation Alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8135fab574776f2cf5c65b355006deea0448a858.pdf", "supplementary_material": "/attachment/6bfe857f1b989be0745af919be1559c5883e5e3b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes LUMA, a unified motion alignment with dual-path anchoring for text-to-motion diffusion model. Authors introduce the gradient vanishing problem and design the frequency and temporal modules to enhance the semantic alignment, which improves the generation performance in HumanML3D and KIT-ML datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Authors point out the gradient vanishing problem and try to solve that. Through some visualized results, they prove that this phenomenon exists.\n2.This work enhances the performance of motion generation and improve the convergence speed, which reduces the time cost during training."}, "weaknesses": {"value": "1. I believe the vanishing gradient problem mentioned by the authors does exist, but the method proposed in the paper does not seem to be specifically designed to address this issue. In my understanding, introducing additional loss functions and certain heuristic tricks during training can also alleviate the vanishing gradient problem. The authors should further elaborate on the necessity of temporal and frequency alignment as well as other advantages it brings.  \n\n2. For tasks like motion generation, the absence of video results is quite unusual. The paper only presents three static visualization figures; more cases and video demos are needed to demonstrate the effectiveness of the method.  \n\n3. The baseline lacks comparisons with more methods, such as LaMP and ReMoDiffuse.  \n\n4. The authors state that this work does not use a pre-trained foundation model as an alignment supervisor. However, they trained a MoCLIP from scratch and aligned it with the CLIP text encoder. I believe this is inconsistent with the authors' statement.  \n\n5. The font size in some tables of the paper is excessively large, which affects readability and aesthetics (e.g., Table 8). Additionally, the font formatting in the pseudocode of the algorithm appears somewhat abrupt, and the framework diagrams are relatively unclear. There are also inconsistencies in certain expressions—for instance, \"downsampling\" is used in Line 75, while \"down-sampling\" is used in Line 103."}, "questions": {"value": "The main issues are listed in the \"Weakness\" section. Here, I have an additional question: I hope the authors can specify what the frequency information of motions specifically represents, and clearly state the exact role of such alignment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IJhT8zkPhK", "forum": "XwnjMZWe1Y", "replyto": "XwnjMZWe1Y", "signatures": ["ICLR.cc/2026/Conference/Submission6068/Reviewer_phkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6068/Reviewer_phkH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660895830, "cdate": 1761660895830, "tmdate": 1762918442999, "mdate": 1762918442999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to improve the UNet-based diffusion models for the text-to-motion generation task. It first identifies the gradient attenuation issue in the deep layers of U-Net, and proposes Low-dimension Unified Motion Alignment (LUMA) for better representation learning via extra supervision signals to the hidden feature. LUMA provides hidden feature supervision signals with features from a motion encoder and DCT. By adopting a better text encoder, \"deep supervision\" for specific hidden features, the proposed model shows improved performance compared to the baseline on which the model is built. Ablation experiments also quantitatively show the effectiveness of the proposed designs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall presentation is nice and clear, which is easy to follow.\n2. The proposed method shows better performance compared to the baselines, according to the quantitative results.\n3. The ablation experiments effectively show that removing any proposed component will lead to performance degradation.\n4. The paper reveals a relatively new issue on text2motion generation, with respect to the architecture design.\n5. The code for reproduction and the detailed appendix are greatly appreciated."}, "weaknesses": {"value": "1. The scope of this work/experiment is relatively narrow, which focuses on text2motion with diffusion UNet, specifically. \n2. The discussion in the introduction section is not reasonable. a) It mentions that \"increasingly sophisticated architectures\" lead to performance improvement, yet \"diminishing efficiency and limited practical improvement.\" This is not logical, and the proposed method has nothing to do with efficiency and still relies on deeper and larger networks. b) Diffusion UNet \"are limited by their high computational cost during training\". The proposed method also requires the same or extra computational cost because of the use of additional latent encoders for \"deep supervision\".\n3. \"Deep supervision\" or representation learning for diffusion models is a relatively well-explored topic. \n4. DCT for \"low-frequency components, recognized for their rich semantic content,\" I'm a bit skeptical that the DCT feature will help semantic representation learning. According to Table 2, removing L_fre does not affect the R@3, which is related to semantic alignments.\n5. Adaptive FiLM modulation is a good design choice, but is a common design for the diffusion model, which is also used in the Adaptive Normalization. \n6. Overall, the method is relatively incremental, and the performance gain is not signicant. For example, it worse than the SOTA reported from a work from ICLR last year [1].\n7. There's no video result to assess the performance qualitatively, which is crucial for animation tasks.\n8. According to Table 3 and 4. It seems that removing FiLm or the deep supversion signal can achieve better performance than inject the supervsion into improper position, e.g. Bottleneck. This emprical result make the overall design becomes a bit tricky.\n\n[1] Li, Zhe, et al. \"LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. If the gradient vanishing is a severe issue, will the proposed method also generalize to improve the performance of e.g. text-to-image Diffusion UNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0wPmohpzA7", "forum": "XwnjMZWe1Y", "replyto": "XwnjMZWe1Y", "signatures": ["ICLR.cc/2026/Conference/Submission6068/Reviewer_BfWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6068/Reviewer_BfWw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702125998, "cdate": 1761702125998, "tmdate": 1762918442476, "mdate": 1762918442476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address semantic misalignment and kinematic artifacts in text-to-motion generation, The authors propose LUMA, a diffusion-based text-to-motion model that mitigates semantic misalignment and kinematic artifacts through dual-path anchoring: a lightweight MoCLIP for temporal semantic guidance and low-frequency DCT features for complementary frequency-domain alignment."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors conducted extensive experiments to support their method, although the improvements achieved are quite limited.\n- The paper is clearly written and the language is generally fluent and coherent."}, "weaknesses": {"value": "- The motivation is vague. The authors claim that experimental analysis reveals gradient shifts in the deeper layers as the root cause of the problem, but it is unclear what specific issue this refers to.Is it low semantic fidelity, high computational cost, or something else?\n\n- Figure 2 is not clearly presented, making it difficult for readers to understand the overall structure and flow of the proposed method.\n\n- The Method and Preliminaries sections are poorly written. They do not clearly explain how the proposed approach addresses the problem.\n\n- The concept of Low-Dimensional Unified Motion Alignment is insufficiently explained. In addition, the Method section lacks clarity and coherence, which makes the proposed approach hard to comprehend.\n\n- The relationship between gradients and high-level feature extraction is not clearly explained. The paper does not clarify why gradient behavior would affect the learning of high-level features.\n\n- In Table 1, there are several errors in the bold and underline markings. For example, in the fifth column, the best result should be 9.724 rather than 9.466. Moreover, compared with StableMoFusion, the performance improvement appears to be quite limited.\n\n- Table 2 shows that, except for a slightly improved FID score, the proposed modules contribute only marginal gains. In particular, rows 3–5 perform worse than the full LUMA configuration in the last row.\n\n- The paper lacks visual results to demonstrate or analyze the semantic alignment between text and high-level features. \n\n- The authors claim that previous methods suffer from high computational cost; however, the paper does not provide any analysis or comparison to demonstrate that the proposed method is computationally efficient."}, "questions": {"value": "- Since deeper layers are expected to capture high-level abstract representations, the loss of fine-grained details should be attributed primarily to the downsampling operations rather than the upsampling stages. This is somewhat inconsistent with the authors’ stated motivation.\n\n- While the authors assert that gradients in the downsampling and bottleneck layers are considerably weaker or even vanish compared to those in the upsampling layers, the paper lacks concrete experimental evidence to substantiate this observation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper lacks clear logical coherence between the methodology and the experimental sections. It is difficult to follow how the described experiments are designed to verify the proposed method."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4sE8S6WOxh", "forum": "XwnjMZWe1Y", "replyto": "XwnjMZWe1Y", "signatures": ["ICLR.cc/2026/Conference/Submission6068/Reviewer_fn1y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6068/Reviewer_fn1y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893821807, "cdate": 1761893821807, "tmdate": 1762918442092, "mdate": 1762918442092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyze the gradient descent in downsampling and bottleneck layers in UNet. And it proposes a dual-path semantic information injection mechanism: one path uses mo-clip to extract semantic information of motion frames, another path utilizes the low-frequency part of spectrum after DCT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It analyzes the gradient descent phenomenon in UNet. It points out that the small gradient norm in the deep layers of UET affects the learning of high-level semantics, and verifies that the proposed method can improve this phenomenon.\n2. Compared to related works such as Repa, this work does not require a pre-trained visual model. Thus it solves the problem in the text-to-motion domain where the lack of large-scale pre-trained understanding models prevents the use of Repa.\n3. This work achieves state-of-the-art performance while accelerating convergence."}, "weaknesses": {"value": "1. Lack of comparison with alternative gradient enhancement techniques: The paper focuses on dual anchors but does not compare with other methods for mitigating gradient attenuation in diffusion models (e.g., residual connections tailored for U-Net deep layers, adaptive learning rate scheduling for bottleneck layers). \n2. The dual-path method will introduce additional cost while training and inference. The paper does not have a detailed quantitative discussion.\n3. Recently, diffusion models are always using DiT as backbone. How does this method perform on DiT?"}, "questions": {"value": "1. How does MoCLIP’s text-motion alignment performance compare to other SoTA encoders?\n2. For motions with distinct frequency characteristics (e.g., fast, jerky actions vs. slow, smooth actions), does the optimal k (number of DCT coefficients) change? If so, how might the framework be adapted to dynamically adjust k based on input text or motion type?\n3. What is the training cost of MoClip and how does it compare to other similar models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AYmOMyRVxT", "forum": "XwnjMZWe1Y", "replyto": "XwnjMZWe1Y", "signatures": ["ICLR.cc/2026/Conference/Submission6068/Reviewer_XAZg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6068/Reviewer_XAZg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6068/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926630710, "cdate": 1761926630710, "tmdate": 1762918441581, "mdate": 1762918441581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}