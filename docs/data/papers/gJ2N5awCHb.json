{"id": "gJ2N5awCHb", "number": 14137, "cdate": 1758229029098, "mdate": 1759897388112, "content": {"title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents", "abstract": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.", "tldr": "A dataset for training mobile control agents for thorough app usage", "keywords": ["AI Agents", "Android", "Dataset", "LLM judge"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/920414a4640de26b6dc1eede35592ebf1754dd32.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "DigiData is presented as a large-scale, high-quality, diverse, multi-modal dataset engineered for training general-purpose mobile control agents. DigiData employs a multi-phase collection protocol involving goal curation through comprehensive app feature exploration, human demonstrations, and trajectory verification to ensure high data quality and complexity. Digidata comprises of 152k trajectories and 8k unique goals across 26 Android apps. DigiData-Bench is also provided, a benchmark that introduces dynamic evaluation protocols, supported by human-assisted methods, demonstrating that task success rate is required to reliably assess agent capabilities, as the traditional step-accuracy metric is often insufficient and unreliable for performance ranking"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The contribution is well motivated: constructing large-scale datasets is essential in advancing and developing mobile device control agents. \n* The multi-phase data collection pipeline with humans incorporated in each phase ensures that high-quality data is collected.\n* The dataset is comprehensively constructed for each app, represented by the number of tasks per app."}, "weaknesses": {"value": "* In Table 2, there is limited comparison between existing agents and models fine-tuned using DigiData, which limits understanding of the amount of improvement in mobile control capability DigiData provides. As this is an important aspect of this work, I suggest evaluating more mobile agents on DigiData-Bench. It would also help to include PLM as a baseline, as it will help understand the magnitude of improvement from the base model via fine-tuning on DigiData.\n* The number of tasks per app appears quite large. While the proposed similarity metric suggests that the tasks remain diverse, it is difficult to assess the actual degree of diversity without reviewing the task lists themselves. Conceptually, there is a natural limit to the number of genuinely distinct tasks an app, especially simple ones like Google Calendar or Google Clock, can support. This raises concerns that some tasks may be variations of the same or very similar actions. Providing examples or additional evidence of task distinctiveness would strengthen the validity of this claim."}, "questions": {"value": "* Are there any evaluation results for DigiData-Bench-Auto? This benchmark seems likely to be more generally used by other researchers. Providing insights related to the auto benchmark would be useful.\n\nI am willing to raise my score given that my questions or weaknesses are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xTD83pjNpU", "forum": "gJ2N5awCHb", "replyto": "gJ2N5awCHb", "signatures": ["ICLR.cc/2026/Conference/Submission14137/Reviewer_Fk85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14137/Reviewer_Fk85"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815394355, "cdate": 1761815394355, "tmdate": 1762924602112, "mdate": 1762924602112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DigiData, a large-scale mobile GUI dataset comprising 8,275 unique goals across 26 Android applications. The dataset was constructed by skilled annotators following a goal generation protocol and is characterized by its high complexity (average of 9.2 steps/goal), multi-modal nature (including screenshots, UI trees, and Chain-of-Thought annotations), and high quality (almost 100% trajectory verification pass rate). Furthermore, the authors introduce the DigiData-Bench, an online evaluation benchmark that supports task assessment by either LLMs or human evaluators based on an evaluation protocol. The authors use this dataset to fine-tune the Perception Language Model. Experiments show that an 8B parameter model trained with Chain-of-Thought (CoT) data outperforms existing baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper's appendix provides exhaustive details on data construction and in-depth data analysis.\n2.\tBy leveraging skilled annotators who systematically explore application functionalities based on a goal generation protocol, the generated goals cover advanced features of the apps, enhancing the dataset's depth and utility.\n3.\tDigiData is currently the second-largest mobile control dataset. It boasts high quality (100% trajectory verification pass rate) and provides rich information, including screenshots, UI trees, and LLM-generated Chain-of-Thought data, offering a valuable resource for training GUI agents.\n4.\tThe paper conducts an detailed evaluation of using LLM judges for GUI tasks, providing valuable empirical evidence."}, "weaknesses": {"value": "1. The experimental evaluation covers limited datasets and benchmarks, omitting important comparisons such as the GUIOdyssey [1] dataset, which features longer interaction sequences (average 15.3 steps) than the proposed DigiData.\n2. The experimental design lacks consistent evaluation conditions and ablation studies, making it unclear whether DigiData independently enhances generalization. Current results merely show good performance on AitW and DigiData-Bench, without achieving SOTA on AndroidControl. Demonstrating cross-benchmark improvements would strengthen the argument.\n3. Evaluation on DigiData-Bench involves only two zero-shot models (Qwen2.5VL and GPT-4o) and omits comparisons with open-source GUI agents. This limits the validation of task difficulty and dataset utility. Including models like OdysseyAgent [1] or GUI-Owl [3] would provide a more comprehensive assessment.\n4. According to [2], GUI step accuracies can be approximated as i.i.d., implying that task success ≈ (step accuracy)^steps. However, in Table 2, GPT-4o’s step accuracy (40.0%) and task success rate (27.8%) deviate substantially from expected ratios and from reports such as OdysseyAgent [2] (3.22% success at 59.53% step accuracy). The reported success rates thus appear inconsistent and require clarification.\n5. Although Figure 5 shows some correlation between LLM-as-Judge and human evaluations, the noticeable gap suggests that LLM-as-Judge cannot yet provide fully reliable absolute performance estimates, warranting further validation.\n\n[1] Lu, Quanfeng, et al. ”GUIOdyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices.”\n\n[2] Li W, Bishop W E, Li A, et al. On the effects of data scale on ui control agents[J]. Advances in Neural Information Processing Systems, 2024, 37: 92130-92154.\n\n[3] GUI-Owl，Ye J, Zhang X, Xu H, et al. Mobile-agent-v3: Fundamental agents for gui automation[J]. arXiv preprint arXiv:2508.15144, 2025."}, "questions": {"value": "1. DigiData includes only 26 Android applications, which is significantly fewer than datasets such as AitW, AndroidControl, and GUIOdyssey. How does this limited app coverage affect the dataset’s representativeness and generalizability? The authors are encouraged to justify why this scale is sufficient for training general-purpose GUI agents.\n2. The paper does not clearly explain how the benchmark goals in DigiData-Bench were selected. Please elaborate on the filtering criteria, the balance of goal distributions, and the measures taken to avoid tasks that are either too simple or excessively difficult.\n3. The validity of DigiData-Bench depends heavily on its evaluation protocol. More details are needed regarding how this protocol was constructed and how its accuracy and reliability are ensured.\n4. Both human-assisted evaluation and LLM-as-Judge incur significant costs, with human evaluation being particularly time-intensive. The paper does not report the deployment cost of DigiData-Bench (e.g., time overhead or computational resources). Please provide information on the evaluation efficiency and overall cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mw9K8vIWcT", "forum": "gJ2N5awCHb", "replyto": "gJ2N5awCHb", "signatures": ["ICLR.cc/2026/Conference/Submission14137/Reviewer_PvGF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14137/Reviewer_PvGF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879253515, "cdate": 1761879253515, "tmdate": 1762924601747, "mdate": 1762924601747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DigiData, a large-scale, high-quality multimodal dataset (152k trajectories, 8k+ goals) for training general-purpose mobile control agents. It is built via a structured three-phase pipeline: goal curation, human demonstrations, and hybrid LLM–human verification, to ensure depth and diversity. The authors also propose DigiData-Bench, a dynamic benchmark featuring human- and AI-assisted evaluation protocols that address limitations of step-accuracy metrics. Experiments show DigiData-trained agents outperform existing baselines, and LLM judges correlate strongly with human assessments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "DigiData provides a well-structured and reproducible dataset with higher goal complexity and diversity than prior work. Experimental results are robust and clearly demonstrate performance gains and scaling effects. The dataset and benchmark together contribute to a valuable foundation for research in mobile control."}, "weaknesses": {"value": "While DigiData is carefully engineered, it shows limited methodological novelty. Its contributions mainly concern dataset scale and organization rather than new paradigms. The motivation for creating DigiData is not clearly articulated, as existing datasets like AitW and AndroidControl already support complex mobile control research with similar design goals. The introduction of LLM judges is also incremental rather than innovative, and the paper does not situate its evaluation method within the broader literature on LLM-based judging in mobile control tasks."}, "questions": {"value": "1.\tThe paper claims DigiData’s trajectories achieve 94.6% goal success (before filtering) compared to AitW’s 84%, but both were verified under the authors’ own system. Could this introduce evaluation bias or unfairness? How do the authors ensure comparability, and could a case study or qualitative inspection clarify what “quality difference” actually looks like?\n2.\tWhat concrete gaps or shortcomings in AitW and AndroidControl specifically motivated the creation of DigiData?\n3.\tSince the use of LLM-as-a-judge is not novel, how does this implementation differ from or improve upon prior research on automated trajectory evaluation?\n4.\tCan the authors benchmark their LLM judging pipeline against other existing studies applying LLMs for mobile control assessment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hbn4YctUDX", "forum": "gJ2N5awCHb", "replyto": "gJ2N5awCHb", "signatures": ["ICLR.cc/2026/Conference/Submission14137/Reviewer_FLBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14137/Reviewer_FLBY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908405918, "cdate": 1761908405918, "tmdate": 1762924601050, "mdate": 1762924601050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Digidata, a large-scale dataset designed for training mobile control agents. The trajectories in this dataset are constructed through goal collection, human demonstration, and trajectory verification assisted by both human and model evaluation. The goals in Digidata are collected by exploring comprehensive features of mobile apps, thereby enhancing their diversity. Furthermore, Digidata provides multiple input modalities, including UI trees and chain-of-thought data. The paper also introduces a benchmark named DigiData-Bench for dynamically evaluating mobile agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The dataset is large-scale and diversified, facilitating robust training and evaluation of mobile agents."}, "weaknesses": {"value": "- In the trajectory verification process, which specific LLM is used as the judge? Does this method incur high costs?\n\n- There is a lack of comparison with other state-of-the-art mobile agent methods, such as UI-TARS[1], UI-Genie[2], and the Mobile-Agent series[3].\n\n- Digidata-bench is proposed to address the inaccuracies of step accuracy metrics in offline evaluation. However, several online dynamic evaluation benchmarks already exist (e.g., AndroidWorld[4], SPA-Bench[5], A3[6]), and methods like LLM-as-a-judge have also been employed in SPA-Bench and A3. What is the core difference of Digidata-bench compared to these existing dynamic evaluation methods?\n\n- As a benchmark, Digidata-bench lacks evaluation results on advanced closed-source models (e.g., Claude, Gemini) as well as leading open-source models like UI-TARS, UI-Genie, and the Mobile-Agent series.\n\n[1] Qin Y, Ye Y, Fang J, et al. Ui-tars: Pioneering automated gui interaction with native agents[J]. arXiv preprint arXiv:2501.12326, 2025.\n\n[2] Xiao H, Wang G, Chai Y, et al. UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents[J]. arXiv preprint arXiv:2505.21496, 2025.\n\n[3] Ye J, Zhang X, Xu H, et al. Mobile-agent-v3: Fundamental agents for gui automation[J]. arXiv preprint arXiv:2508.15144, 2025.\n\n[4] Rawles C, Clinckemaillie S, Chang Y, et al. Androidworld: A dynamic benchmarking environment for autonomous agents[J]. arXiv preprint arXiv:2405.14573, 2024.\n\n[5] Chen J, Yuen D, Xie B, et al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation[C]//NeurIPS 2024 Workshop on Open-World Agents. 2024.\n\n[6] Chai Y, Li H, Zhang J, et al. A3: Android agent arena for mobile gui agents[J]. arXiv preprint arXiv:2501.01149, 2025."}, "questions": {"value": "- Will the training dataset be open-sourced?\n\n- How does Digidata-bench fundamentally differ from existing dynamic evaluation benchmarks such as AndroidWorld, SPA-Bench, and A3?\n\n- Can you provide missing comparisons with other state-of-the-art mobile agent methods, including UI-TARS, UI-Genie, and the Mobile-Agent series?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IsSfkyPSDL", "forum": "gJ2N5awCHb", "replyto": "gJ2N5awCHb", "signatures": ["ICLR.cc/2026/Conference/Submission14137/Reviewer_iMrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14137/Reviewer_iMrZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968075876, "cdate": 1761968075876, "tmdate": 1762924600400, "mdate": 1762924600400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}