{"id": "mNeitRAdWV", "number": 7507, "cdate": 1758025312802, "mdate": 1759897848796, "content": {"title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning", "abstract": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to enhance their internal reasoning ability by integrating external tools. However, models with TIR often exhibit suboptimal behaviors, including insufficient tool calls, excessive tool calls, and overthinking after receiving tool call results. How to empower LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open challenge.\nIn this paper, we first analyze the impact of tool calls on model reasoning from the perspective of information entropy. We find that when tool call results are provided, the information entropy of subsequent reasoning content will show a clear trend of change, and the overall information entropy of the reasoning chain will vary depending on the number of tool calls. Based on these observations, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework consists of dataset construction and multi-stage fine-tuning. For dataset construction, we use the trained model for continuous self-evolved sampling, integrating two methods: vanilla sampling and entropy-guided sampling. At the same time, during the sampling process, we design strict criteria for selecting positive-negative pairs. For the training process, we introduce a two-stage method, which includes a Supervised Fine-Tuning (SFT), and Self-Evolved Direct Preference Optimization (DPO).\nTest results on 10 datasets reveal the effectiveness of Tool-Light, significantly improving the efficiency and accuracy of the model in completing TIR tasks.", "tldr": "", "keywords": ["reasoning model", "tool-integrated reasoning", "self-evolved training", "information entropy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/275b8e64728d86f45f706b384e5ec71bc66171de.pdf", "supplementary_material": "/attachment/7fdea3dd565314fb16a32d7a6c975995a0dbf8b2.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the Tool-Integrated Reasoning for LLMs, and proposes a new framework namely Tool-Light to resolve the problem of excessive tool calls during the reasoning. The key designs of the proposed framework contain dataset construction and multi-stage fine-tuning. Specifically, for dataset construction, Tool-Light employs two sampling strategies and for multi-stage fine-tuning, it introduces a two-stage training method. To support the effectiveness of Tool-Light, extensive empirical studies are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors provide extensive experiments to demonstrate the effectiveness of **Tool-Light**, particularly through the experiments in **Figure 1**, which clearly illustrate the motivation behind the work.\n- The paper is clearly written and well organized, making it easy to follow."}, "weaknesses": {"value": "- **Figure 1** lacks sufficient explanation. For instance, the meaning of *Token Index* and the specific roles of *Step 1–4* are unclear.\n- In **Figure 4(c)**, it is not specified whether the response length includes the tool-calling part. The figure shows that **Tool-Light** produces shorter responses than **Tool-Star**, yet the examples in the appendix (Examples 1 and 2) suggest otherwise.\n- In **Line 257**, the computational complexity is claimed to be $O(n\\log m)$, but the derivation of this result is not provided.\n- The authors adopt two sampling strategies but do not include an ablation study to compare their effects.\n- The experiments are conducted only on the 7B model. To further validate the effectiveness of **Tool-Light**, the reviewer encourages testing on both smaller (e.g., 3B) and larger (e.g., 72B) models.\n- In **Line 80**, the authors state that *Pre-Aligned DPO Training* can reduce redundant tool calls. However, it is unclear why this is the case, as *Pre-Aligned DPO Training* does not appear to include mechanisms that explicitly control the number of tool calls.\n- It is recommended to report the standard deviation across multiple runs to demonstrate the robustness of the experimental results."}, "questions": {"value": "Answer questions in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vnvv9TULZe", "forum": "mNeitRAdWV", "replyto": "mNeitRAdWV", "signatures": ["ICLR.cc/2026/Conference/Submission7507/Reviewer_nxVk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7507/Reviewer_nxVk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656353409, "cdate": 1761656353409, "tmdate": 1762919617076, "mdate": 1762919617076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies “tool-intelligence” in LLM agents and targets two failure modes: overthinking (too many tool calls/long chains) and underuse (skipping necessary tools). It first runs a pre-experiment that tracks token-level entropy over reasoning traces and observes patterns around tool-call moments. Building on this, it proposes Tool-Light, a two-stage pipeline: (1) SFT to establish a reasonable tool-using policy, then (2) self-evolved DPO using curated preference pairs to favor accurate, tool-efficient traces. In inference, it adds entropy-guided branching that expands alternatives at high-uncertainty steps while keeping compute controlled. Experiments span math and knowledge-intensive QA with web search and a code interpreter, reporting accuracy alongside two bespoke metrics—Efficiency (tool economy) and Necessity (using tools when needed). Results show competitive or better accuracy with fewer tool calls and shorter outputs compared to strong baselines (e.g., Tool-Star, Search-R1), plus initial ablations on sampling knobs. The paper positions entropy as a practical signal for both training supervision and test-time exploration control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Actionable diagnostic. Using token-level entropy to profile traces is simple, implementation-light, and yields intuitive visualizations to reason about when/why tools are called.\n\n(2) Modular training recipe. The SFT → self-evolved DPO pipeline is straightforward to adopt on top of popular backbones and existing tool-use frameworks; no exotic infra required.\n\n(3) Competitive results. Shows strong performance relative to recognized baselines while reducing tool calls—evidence that “lighter” tool use need not sacrifice accuracy."}, "weaknesses": {"value": "**Weaknesses**\n\n(1) Missing details. The pre-experiment omits decoding settings. No temperature, logit scaling, or sampling vs. greedy are reported. These choices change entropy levels and trends. Please specify the exact decoding config used to measure entropy and justify it. You say the entropy study spans “multiple QA datasets,” but the figure does not list them. It’s also unclear whether the same model/datasets are used later to train Tool-Light. Please enumerate the datasets in Figure 1 and clarify any reuse. For the method, entropy decides branching at top-k steps and uses top-i prefix averages. The values of k and i and their stability are not reported.\n\n(2) Missing ablations. Table 2 varies loop count and a few sampling knobs only. Missing: (i) no-entropy (vanilla) baseline, (ii) entropy-only,  (iii) β sensitivity in DPO, (iv) reference-policy choice, and (v) branch-width sensitivity.\n\n(3) “Multi-tool” over-claim. Most experiments use only two tools (web search + code interpreter), and several evaluations appear to be single-tool setups. There’s no test on unseen tool types or cross-tool composition tasks. As a result, the paper does not yet support broad “multi-tool generalization” claims.\n\n**Suggestions**\n\n(1) Control for length. Lower entropy often comes with shorter or more templated outputs. Your own results show Tool-Light reduces sequence length (Fig. 4). The same factor could explain both “lower entropy” and “fewer tool calls.” Please report entropy at fixed token positions or use length-normalized entropy.\n\n(2) Link entropy to correctness. “Low-entropy chains use fewer tool calls” may reflect early commitment, not better answers. Please test whether entropy predicts correctness. Report AUROC for path-level mean or area-under-entropy, controlling for tool-call count.\n\n(3) Stress test with noisy tools. Overthinking and underuse show up when tools are imperfect. Add controlled corruptions: vary retrieval precision/recall, inject code stderr/noise. Show how Tool-Light adapts tool frequency and preserves accuracy vs. Tool-Star/Search-R1. This directly probes the “analysis paralysis” claim."}, "questions": {"value": "(1) Entropy dips after tool calls.\nCould the “drop before the next tool call” be driven by inserting long, deterministic tool outputs (i.e., context length/format effects) rather than better reasoning? What happens if you replace tool results with semantically equivalent, length-matched paraphrases? Does the pattern remain?\n\n(2) Scope to multi-tool.\nDo the pre-experiment findings hold in true multi-tool settings, not just single-tool or search-heavy cases? Please show the same entropy analysis when multiple heterogeneous tools are available."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LOwvAw2Mby", "forum": "mNeitRAdWV", "replyto": "mNeitRAdWV", "signatures": ["ICLR.cc/2026/Conference/Submission7507/Reviewer_mPkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7507/Reviewer_mPkz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949419640, "cdate": 1761949419640, "tmdate": 1762919616640, "mdate": 1762919616640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses inefficiencies in Tool-Integrated Reasoning (TIR), where LLMs overuse, underuse, or mismanage tool calls. The authors analyze tool-usage dynamics through information entropy, finding that tool call results significantly influence reasoning entropy patterns. Building on this insight, they propose Tool-Light, a framework for more efficient and accurate TIR. Tool-Light combines self-evolved dataset construction—using vanilla and entropy-guided sampling with strict positive-negative selection—with a two-stage training scheme of Supervised Fine-Tuning and Self-Evolved DPO. Experiments on 10 datasets (e.g., AIME24, MATH, HotpotQA, etc) show that Tool-Light notably enhances both the efficiency and accuracy of tool-integrated reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Tool-Integrated /Agentical reasoning is a important topic with many potential practical applications.\n2. The information-entropy based analysing after calling tools is novel and insightful. \n3. The experiment evaluation is comprehensive and the performance boost is obvious."}, "weaknesses": {"value": "1. Discuss more related work from the Self-Evolved Preference Learning perspective, such as Zeng et al., Evolving LLMs' Self-Refinement Capability via Synergistic Training-Inference Optimization, and Su et al., Trust Region Preference Approximation: A Simple and Stable Reinforcement Learning Algorithm for LLM Reasoning.\n2. Clarify whether the information entropy observation generalizes across all tools, and list the specific tools used in the experiments for completeness."}, "questions": {"value": "Please see the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tloPB47yvO", "forum": "mNeitRAdWV", "replyto": "mNeitRAdWV", "signatures": ["ICLR.cc/2026/Conference/Submission7507/Reviewer_WYvp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7507/Reviewer_WYvp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995466447, "cdate": 1761995466447, "tmdate": 1762919616295, "mdate": 1762919616295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Tool-Light, a framework designed to enhance the efficiency of tool-integrated reasoning (TIR) in large language models. The core empirical finding demonstrates that invoking external tools causes significant shifts in downstream token-level uncertainty, as measured by entropy. Based on this observation, the authors propose: (i) an entropy-guided sampling procedure for constructing training data, and (ii) a two-stage self-evolved Direct Preference Optimization (DPO) pipeline comprising Pre-Aligned and On-Policy phases. Experimental evaluation across ten benchmarks covering mathematical reasoning and multi-hop question answering shows comparable or superior accuracy while reducing redundant reasoning and improving both efficiency and necessity of tool usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured with clear writing that facilitates comprehension of the proposed methodology.\n2. The investigation of information entropy changes during tool invocation processes is particularly interesting. The authors effectively leverage these insights to guide their methodological design, providing a principled foundation for their approach.\n3. While some components of the proposed data construction and self-evolved DPO framework draw upon established techniques, the overall approach remains intuitive and theoretically sound.\n4. The authors conduct thorough experiments demonstrating the effectiveness of their method. The entropy distribution analysis particularly convincingly shows that the approach achieves lower entropy distributions, validating the theoretical motivation."}, "weaknesses": {"value": "1. The analysis of entropy in tool invocation has been explored in prior work, and the conclusions drawn are not particularly surprising or groundbreaking.\n2. The proposed techniques, including entropy-based sampling and evolved DPO, represent relatively incremental advances rather than significant methodological innovations.\n3. While the core idea is interesting, the evaluation is restricted to DPO. The work would be significantly strengthened by demonstrating applicability to other preference learning methods such as GRPO or PPO, which would provide stronger evidence for the generalizability and robustness of the approach."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wut71EO4AL", "forum": "mNeitRAdWV", "replyto": "mNeitRAdWV", "signatures": ["ICLR.cc/2026/Conference/Submission7507/Reviewer_2Xcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7507/Reviewer_2Xcg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998088210, "cdate": 1761998088210, "tmdate": 1762919615402, "mdate": 1762919615402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all the reviewers for their time and constructive feedback. We are thrilled that our core contributions have been recognized. Our Tool-Light framework is considered **interesting and novel** (2Xcg, WYvp). Our Tool-Light framework is also regarded as **effective and competitive** (2Xcg, mPkz, nxVk). Additionally, the reviewers believe that **our writing is clear** (2Xcg, nxVk).\n\nThe reviewers' main concerns focus on several key areas, which we have addressed through detailed responses and new quantitative analyses. We summarize the following points.\n\n| Focus Area | Reviewer Concerns | Our Actions |\n| :--- | :--- | :--- |\n| Lack of Novelty | 2Xcg | **Action:** We carefully listed the research content of previous works [1-7] and detailed the novelty of our work compared to previous efforts. |\n| Applicability to Other Methods | 2Xcg | **Action:** We detailed potential issues that may arise when adapting our method to other algorithms [8-9] and stated that we will use more RL algorithms to explore TIR efficiency tasks in the future. |\n| Insufficient Discussion of Related Work | WYvp | **Action:** We promise to discuss related work more thoroughly in the revised version. |\n| Insufficient Exploration of Generalizability in Preliminary Experiments | WYvp, mPkz | **Action:** (1) Following the settings of the preliminary experiments, we supplemented experimental results on Mathematical-Reasoning Tasks.<br>(2) We conducted a deeper exploration of the preliminary experiments following the reviewers' suggestions and drew new conclusions.<br>(3) Using the GPQA dataset as an example, we explored the differences between preliminary results and existing results when using multiple heterogeneous tools. |\n| Missing Experimental Setup Details | mPkz | **Action:** We detailed the parameter settings used in the preliminary experiments and conducted an ablation study on the top-k experimental parameter. |\n| Missing Ablation Studies | mPkz, nxVk | **Action:** (1) Regarding the ablation study on sampling methods, we have placed it in Appendix C with comprehensive supplementation.<br>(2) Regarding the ablation study on DPO algorithm settings, we explained that we did not find the Tool-Light training framework to be highly sensitive to these settings in our experiments.<br>(3) Regarding the exploration of branching width, we believe its impact is similar to changing top-k. |\n| Overstated Multi-Tool Claims | mPkz | **Action:** (1) We elaborated on the usage of tools regarding the specific characteristics of the tasks.<br>(2) We promise to restate the relevant content in the Introduction section of the revised version. |\n| Lack of Specificity in Writing | mPkz, nxVk | **Action:** (1) We provided detailed explanations for parts that were not specific or clear.<br>(2) We promise to rewrite relevant content in the revised version. |\n| Missing Training Results on Other Model Sizes | nxVk | **Action:** (1) We used the Tool-Light framework to retrain Qwen2.5-3B-Instruct and Qwen2.5-14B-Instruct.<br>(2) We promise to explore training on larger models in future continuous research. |\n| Missing Standard Deviation in Results | nxVk | **Action:** We re-ran the experiments 10 times for each benchmark and recorded the standard deviation for each benchmark. |\n\nWe believe these clarifications and additional quantitative results have thoroughly addressed the reviewers' concerns.\n**Our team has sincerely invested tremendous efforts into this work, and we sincerely hope that our work can contribute to research in the TIR field.**\n\nBest regards,\n\n---\n### Reference\n\n[1] Wang et al. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning https://arxiv.org/pdf/2506.01939\n\n[2] Cheng et al. Reasoning with Exploration: An Entropy Perspective https://arxiv.org/pdf/2506.14758\n\n[3] Zheng et al. First Return, Entropy-Eliciting Explore https://arxiv.org/pdf/2507.07017\n\n[4] Hao et al. Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective https://arxiv.org/pdf/2510.10150\n\n[5] Dai et al. CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models https://arxiv.org/pdf/2509.09675\n\n[6] Huang et al. Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent https://arxiv.org/pdf/2505.07596\n\n[7] Qian et al. SMART: Self-Aware Agent for Tool Overuse Mitigation https://aclanthology.org/2025.findings-acl.239.pdf\n\n[8] Wu et al. It Takes Two: Your GRPO Is Secretly DPO https://arxiv.org/pdf/2510.00977\n\n[9] Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models https://arxiv.org/pdf/2402.03300"}}, "id": "EgXDfi0sY2", "forum": "mNeitRAdWV", "replyto": "mNeitRAdWV", "signatures": ["ICLR.cc/2026/Conference/Submission7507/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7507/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission7507/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763704463622, "cdate": 1763704463622, "tmdate": 1763704463622, "mdate": 1763704463622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}