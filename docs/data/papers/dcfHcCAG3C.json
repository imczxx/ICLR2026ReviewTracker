{"id": "dcfHcCAG3C", "number": 5892, "cdate": 1757943811677, "mdate": 1759897946678, "content": {"title": "Towards a Multimodal Foundation Model for Time Series Analysis", "abstract": "Time series analysis supports a wide range of real-world applications. While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it introduces key challenges: 1) the scarcity of large-scale and high-quality multimodal time series data; 2) how to effectively integrate heterogeneous modalities and enhance model generalization across both modalities and domains. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first construct MM-TS, a large-scale multimodal dataset spanning time series, text, and image across six domains, with more than one billion time points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. HORAI integrates two core components: a Frequency-guided Cross-Modality Encoder, which leverages the correspondence between modality-specific information and different frequency components of time series to effectively fuse multiple modalities, and a Time-Frequency Decoder, which incorporates frequency information into a MoE router to improve pattern discrimination and generalization. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong task versatility and generalization.", "tldr": "", "keywords": ["Time Series Analysis;  Foundation Models;  Multi-Modality;"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7ea831c64f994183d8c8ccc47824f946e91b733.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents HORAI, a frequency-enhanced multimodal foundation model for time series analysis. Specifically, HORAI integrates two core components: a frequency-guided cross-modality encoder that leverages the correspondence between modality-specific information and different frequency components of time series to effectively fuse multiple modalities, and a time-frequency decoder that incorporates frequency information into a MoE router to improve pattern discrimination and generation. Experimental results show that HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong task versatility and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a creative and technically sound approach by introducing a frequency-guided cross-modality encoder. This design captures frequency-domain relationships across multiple modalities, allowing the model to align modality-specific representations through shared frequency patterns. Such integration not only enhances multimodal fusion but also enables the model to better capture both global and local temporal dependencies that are often overlooked in standard time-domain fusion methods.\n2. The proposed combination of a frequency-guided encoder and a time-frequency decoder integrated with a Mixture-of-Experts (MoE) router demonstrates careful architectural planning. The encoder effectively bridges modality-specific representations, while the decoder utilizes frequency information to improve pattern discrimination and generation. This modular design makes HORAI versatile, interpretable, and easily extendable to a wide range of time series analysis tasks, including forecasting, classification, and anomaly detection.\n3. The experimental section provides convincing evidence that HORAI achieves state-of-the-art performance on several benchmark datasets for time series forecasting and anomaly detection. Notably, its zero-shot capability highlights the generality of the proposed foundation model, showing robustness to unseen tasks and modalities. The consistent improvement across multiple datasets underscores both the effectiveness and broad applicability of the approach."}, "weaknesses": {"value": "1. While the model demonstrates strong empirical performance, the paper lacks sufficient theoretical grounding or analytical explanations for why frequency-guided fusion enhances cross-modal representation learning. Without formal justification or interpretability studies, it remains unclear how much of the observed improvement stems from the proposed frequency-guided mechanism versus other architectural factors.\n2. The experimental results, although promising, do not include detailed ablation studies to quantify the contribution of each key component—such as the frequency-guided encoder, MoE router, or specific frequency-selection strategies. Similarly, the paper does not provide sensitivity analysis regarding key hyperparameters, leaving open questions about the model’s robustness and reproducibility.\n3. The computational complexity of HORAI, particularly its frequency-domain transformations and multimodal fusion layers, may pose scalability challenges for long time series or real-time applications. The paper does not clearly report training or inference costs, memory requirements, or optimization difficulties, which are crucial for understanding its feasibility in large-scale or resource-constrained environments."}, "questions": {"value": "See weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bexjdBzoVJ", "forum": "dcfHcCAG3C", "replyto": "dcfHcCAG3C", "signatures": ["ICLR.cc/2026/Conference/Submission5892/Reviewer_3z9H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5892/Reviewer_3z9H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760762080128, "cdate": 1760762080128, "tmdate": 1762918331465, "mdate": 1762918331465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an initial step towards developing multimodal foundation models for time series analysis by addressing the challenges of data scarcity and multimodal integration. The core contributions include the construction of MM-TS, a large-scale multimodal pre-training dataset that integrates time series, text, and image modalities across six diverse domains, containing over one billion time points. Furthermore, the authors propose HORAI, a frequency-enhanced multimodal foundation model, which features a Frequency-guided Cross-Modality Encoder for effective fusion by aligning low-frequency components with text and mid-to-high-frequency components with vision, and a Time-Frequency Decoder that utilizes a Mixture-of-Experts (MoE) Feed-Forward Network (FFN) and a frequency-informed router for enhanced generalization. After pre-training on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong versatility and generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The pursuit of a multimodal foundation model for time series analysis is a timely and valuable research direction, which addresses the limitations of existing unimodal time series foundation models. The construction of the large-scale, multimodal MM-TS dataset specifically tackles the critical challenge of data scarcity and provides a solid base for future research.\n\n2. The proposed HORAI architecture is well-designed, particularly the Frequency-guided Cross-Modality Encoder that introduces a theoretically sound mechanism to fuse heterogeneous modalities by aligning them to different frequency components of the time series data (text with low-frequency, image with mid-to-high-frequency).\n\n3. Relatively Complete Experiments: The experimental section is relatively comprehensive, evaluating the model on two key tasks, forecasting and anomaly detection, across numerous real-world datasets. The impressive state-of-the-art zero-shot performance compared to both unimodal foundation models and full-shot time-series-specific models demonstrates the strong generalization ability of HORAI."}, "weaknesses": {"value": "1. The text and image modalities in MM-TS are derived from the numerical time series data (LLM-generated descriptions and line-plot visualizations ), which fundamentally means they do not introduce genuinely new external information. This limits the novelty and contribution of the multimodal data generation, essentially functioning as merely augmented representations of the time series itself. Given the reliance on LLMs for text generation, the authors must provide an evaluation of the fidelity and quality of the generated text descriptions.\n\n2. The paper refers to using a \"pre-trained text encoder\" and a \"pre-trained vision encoder\", but the specific models are not identified. This lack of transparency makes reproduction difficult. Furthermore, a crucial ablation study comparing the impact of using different, representative encoders (e.g., different LLMs or Vision Transformers) is missing, which is necessary to confirm the robustness of the HORAI framework regardless of the underlying encoder choice.\n\n3. The claim in Section 2.1 that existing multimodal methods \"cannot generalize to new scenarios through zero-shot inference\" is questionable, as foundational models like ChatTime are known to perform zero-shot forecasting, and a direct comparison is necessary. More critically, the forecasting results in Table 1, where Zero-Shot Time Series Models frequently outperform Full-shot Time Series Models, is highly counter-intuitive. This abnormal finding suggests that the small sample sizes of the evaluation datasets might be preventing the Full-shot models from converging properly. A comprehensive analysis and experimental investigation of this phenomenon must be provided.\n\n4. Given the high complexity of anomaly evaluation, assessing performance solely based on the three threshold-independent metrics (AUC-ROC, VUS-ROC, and VUS-PR) is insufficient for a complete analysis. To align with the standard practice in the anomaly detection field, the authors should provide a multi-dimensional comparison utilizing a broader and more diverse set of evaluation metrics."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7a9a9zqQ24", "forum": "dcfHcCAG3C", "replyto": "dcfHcCAG3C", "signatures": ["ICLR.cc/2026/Conference/Submission5892/Reviewer_rxHD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5892/Reviewer_rxHD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553196730, "cdate": 1761553196730, "tmdate": 1762918330799, "mdate": 1762918330799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes MM-TS, a large-scale multimodal time-series dataset that covers multiple domains with extensive time points. Based on MM-TS, the paper introduces HORAI, a multimodal time-series foundation model trained on MM-TS, featuring adaptive fusion selection and frequency MoE decoding. The evaluation results demonstrate the effectiveness of HORAI on multimodal time-series forecasting across multiple domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work presents MM-TS, a large-scale multimodal time-series pre-training dataset that could potentially benefit future research on multimodal time-series forecasting.\n\n2. The proposed HORAI is a multimodal time-series foundation model trained on MM-TS, incorporating multimodal fusion (mainly frequency-enhanced). The evaluations show strong zero-shot forecasting performance across domains beyond those included in its training data."}, "weaknesses": {"value": "1. While constructing MM-TS involves substantial effort, the dataset itself does not contain newly collected time-series data. Most of the data in MM-TS are from well-known unimodal datasets such as Monash or PEMS. MM-TS mainly aggregates these datasets and performs data augmentation for additional modalities, which may reduce the novelty and overall contribution of the dataset.\n\n2. In constructing MM-TS, the prompts specify the start and end times for generating textual summaries. Does this imply that models trained on MM-TS must rely on a fixed historical lookback window, since the summaries correspond to fixed time ranges? Clarifying this would help understand whether MM-TS supports flexible temporal contexts.\n\n3. Regarding the modality alignment in the frequency domain, it is unclear why the vision component is specifically aligned with mid-to-high frequency parts. Could this introduce noise instead of meaningful signals? How is $\\alpha$ properly set to avoid this issue in a foundation model (or is one $\\alpha$ good for all cases)? Moreover, how does this frequency-based alignment differ from a simpler time-series decomposition approach, which is easier to interpret and does not rely on complex-valued representations?\n\n4. For the forecasting evaluation on Time-MMD, this dataset already includes textual information, but its text structure may differ from that in MM-TS. Does the evaluation directly use the original Time-MMD text, or is it reconstructed following the MM-TS text generation process?\n\n5. Although the paper includes results with and without additional modalities, more detailed ablation studies would strengthen the analysis of multimodal benefits—especially regarding the frequency alignment. For example, comparing forecasting with (a) text only, (b) image only, and (c) text + image without frequency-based alignment could provide clearer insights into how each modality contributes to performance improvements."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M2xYbkkIkJ", "forum": "dcfHcCAG3C", "replyto": "dcfHcCAG3C", "signatures": ["ICLR.cc/2026/Conference/Submission5892/Reviewer_oNEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5892/Reviewer_oNEh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837433119, "cdate": 1761837433119, "tmdate": 1762918330367, "mdate": 1762918330367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a time series analysis framework using foundation models under multi-modal learning scenario. It builds a large-scale datasets and proposes a corresponeding method for model learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Time series analysis is always a valuable field for research, then the topic of this paper is solid.\n2. This paper proposes a large scale dataset, which should be regarded as a contribution for the community.\n3. Overall, the proposed framework is reasonable, and empirical performance shows its effectiveness."}, "weaknesses": {"value": "1. I am confused about the main goal of this draft. Is the dataset contribution the main part? or the proposed method. If it is the former one, probably a dataset tract fits more and the current version needs more dataset based analysis. If the later one, the proposed method is relatively straightforward, while the multi-modal scenario is a good point which requires great experimental effort.\n2. The draft looks not ready yet, especially some formats.  For example, tab1 and 2 are not well adapted with the main text. Figs are not informative with small font size.\n3. Time series analysis is a huge research area, adding more ablation analysis will strongly support this draft, while current empirical focus is more on performance comparison aspect."}, "questions": {"value": "Please check above section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2z4gHCvkS", "forum": "dcfHcCAG3C", "replyto": "dcfHcCAG3C", "signatures": ["ICLR.cc/2026/Conference/Submission5892/Reviewer_8MWe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5892/Reviewer_8MWe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008157171, "cdate": 1762008157171, "tmdate": 1762918329855, "mdate": 1762918329855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}