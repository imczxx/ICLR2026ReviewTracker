{"id": "iVeqLVyWO8", "number": 15826, "cdate": 1758255768166, "mdate": 1759897279349, "content": {"title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning", "abstract": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce **ToolRM**, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields *ToolPref-Pairwise-30K*, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_\\text{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28\\% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66\\%. We release data and model checkpoints to facilitate future research.", "tldr": "", "keywords": ["tool learning", "reward modeling", "large language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10fb981e603d71a2ce3a9e17ffbfcc3d639230c9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ToolRM, a family of lightweight generative reward models for LLM tool use. The authors propose a two-stage data construction pipeline that yields ToolPref-Pairwise-30K, a pairwise preference dataset created via rule-based scoring and multidimensional sampling. The paper also presents TRBENCH$_{\\mathrm{BFCL}}$, a new benchmark for evaluating tool-use reward models. Extensive experiments demonstrate that ToolRM-trained models outperform strong baselines, including leading foundation and specialized models on pairwise reward judgment, scaling, and self-correction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses the clear bottleneck in reward model development for agentic tool-use with LLMs.\n- The pipeline for ToolPref-Pairwise-30K is carefully devised. I checked some data samples and found them of high quality.\n- TRBENCH$_{\\mathrm{BFCL}}$ provides an OOD, error-rich environment for fair evaluation. This could potentially be a valuable asset for the tool-use RM community."}, "weaknesses": {"value": "- The central algorithmic innovations are mostly in the data construction and evaluation pipeline, whereas the reward modeling itself leverages the well-established GRPO methods. No other significant technical contributions on the reward model side (e.g, architectural novelty) are shown.\n- The distillation data construction pipeline may nevertheless introduce subtle biases or data artifacts that could be exploited by reward models. For instance, does the skew of preference intensity bins or complexity scores create blind spots? Seems the paper does not analyze or discuss the risk of overfitting to rule-based artifacts instead of learning genuine task competence.\n- More error analysis would strengthen the paper. For example, do the models ever prefer responses that \"look correct\" but are subtly wrong, or exhibit brittle reasoning?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "03BPnUaXE9", "forum": "iVeqLVyWO8", "replyto": "iVeqLVyWO8", "signatures": ["ICLR.cc/2026/Conference/Submission15826/Reviewer_MrPn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15826/Reviewer_MrPn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912796206, "cdate": 1761912796206, "tmdate": 1762926053683, "mdate": 1762926053683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors look at improving reward modeling capabilities for tools using responses. To do so, the authors propose a pipeline for generating preference data from tool use demonstrations. Based on the dataset generated from their pipeline, the authors train a series of ToolRM models which have improved tool critiquing capabilities. In addition, the authors propose a new tool use preference evaluation suite which they use to benchmark different methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* With the growing importance of agnetic tool using AI, collecting preference data in such settings seems very useful and like a directionally important research direction.\n* The paper introduces a tool use preference eval which is a useful contribution as a benchmark for the RM research community to hill climb against."}, "weaknesses": {"value": "* The ablations section does not ablate parameters at a useful granularity. There are many different factors that contribute to the BMDS algorithm, however, BMDS is treated as a monolithic block without decomposing which dimension matters most (source diversity, preference intensity, or task complexity)\n* The performance between the base and fine-tuned llm judges in Table 2 doesn’t seem to be a fair comparison as the base models aren’t trained to be judges. To understand how useful the tool use preference data is, it would be useful to include a baseline of training the models to be judges on normal preference data. Without doing such it doesn’t seem possible to get a calibrated understanding of how useful the tool use preference data is."}, "questions": {"value": "* Did the authors look at training non reasoning reward models on their task (i.e. standard single forward pass reward models [1]. It would be helpful to understand how useful reasoning is for verifying tool usage correctness.\n* Similar to the second weakness, but for models not trained on the Hermes tool usage format there is a distribution shift when evaluating responses with such a format. It would be a useful comparison to simple SFT the base models on the preferred samples from the formatted tool use dataset and see how much of the gap is recovered.\n\n[1] Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" Advances in neural information processing systems 35 (2022): 27730-27744."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V3c3f8xb7q", "forum": "iVeqLVyWO8", "replyto": "iVeqLVyWO8", "signatures": ["ICLR.cc/2026/Conference/Submission15826/Reviewer_Ft4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15826/Reviewer_Ft4k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955960124, "cdate": 1761955960124, "tmdate": 1762926053277, "mdate": 1762926053277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on reward models for function-calling tasks and proposes ToolRM, a generative reward model designed for tool-call scenarios. The authors develop a pipeline for constructing pairwise preference data based on rule-based scoring and multidimensional sampling, generating the ToolPref-Pairwise-30K dataset for critique tasks. Additionally, to evaluate RMs, the authors introduce TRBench-BFCL. Experimental results demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reward modeling for tool-calling scenarios is crucial, and this work addresses this area by proposing a comprehensive and robust pipeline for preference data construction. The creation of ToolPref-Pairwise-30K and the accompanying benchmark represents a substantial contribution to the field.\n2. The data construction process incorporates multiple verification methods to ensure data quality and employs diverse sampling strategies to enhance data diversity and quality. This systematic approach is commendable.\n3. The experimental results demonstrate significant superiority of the proposed method, substantially improving accuracy across multiple models. The scaling experiments and analysis of critique effectiveness provide valuable insights."}, "weaknesses": {"value": "1. For tool-calling reasoning, while many scenarios can be constrained through rule-based outcome rewards, process rewards for multi-turn tool calls are equally important. However, the paper focuses exclusively on outcome rewards without addressing process rewards, which limits the comprehensiveness of the approach.\n2. The experiments lack direct application of the reward model to downstream tasks. For instance, there is no evaluation of how the reward model enhances tool-calling agents when combined with reinforcement learning methods such as PPO or GRPO, which would provide stronger evidence of practical utility.\n3. The title mentions \"efficient reasoning,\" but it is unclear which specific aspect this refers to. Intuitively, using RMs to constrain agentic tool-use should introduce computational overhead and potentially slow down the overall process, making the efficiency claim questionable without proper justification."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DP0TSViFIw", "forum": "iVeqLVyWO8", "replyto": "iVeqLVyWO8", "signatures": ["ICLR.cc/2026/Conference/Submission15826/Reviewer_usxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15826/Reviewer_usxe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996943687, "cdate": 1761996943687, "tmdate": 1762926052805, "mdate": 1762926052805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a generative reward model for tool-use evaluation. The authors construct a synthetic pairwise preference dataset by verifying tool-call trajectories, sampling candidate model responses, and scoring them with a rule-based function. A balanced multi-dimensional sampling strategy selects challenging examples. The model is trained via GRPO, where each pairwise example is converted into a forced-choice evaluation task, and the model receives a binary reward depending on whether its <choice> tag matches the reference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows useful engineering contributions. It constructs an LLM-generated tool-use preference dataset, although not compare with previous preference dataset generation pipeline.\n\n2. It introduces a benchmark for tool-RM evaluation. The curated dataset considers diversity and preference density.\n\n3. Experiments on classification accuracy include diverse LLMs/APIs."}, "weaknesses": {"value": "1. **Missing positioning and comparison against existing tool-augmented reward modeling work**. The paper does not cite or compare with prior works that explicitly frame and study tool-augmented reward models (ICLR’24 and follow-up papers). These directly address reward modeling for tool agents, and omitting them weakens the novelty claim and contextualization.\n\n2. Pairwise dataset but no pairwise optimization objective. Although the dataset contains pairwise preferences, the training uses binary RL reward instead of pairwise margin (e.g., Bradley-Terry, InfoNCE for prefs). This forfeits preference strength information and deviates from standard RM objectives. A comparison with classical pairwise RM training is highly required.\n\n3. **Relies on SFT / thinking-tuned Qwen models, without ablation on initialization**. All experiments are initialized from instruction-tuned and thinking-tuned Qwen checkpoints. No experiment isolates the effect of RL vs. SFT initialization or evaluates purely pretrained baselines. \n\n4. **Unclear presentation**. e.g., Table 2 uses symbols (S, M, P, PM, LS, LM, LP, LPM), but no definitions are given in main text. It is unclear what those symbols mean.\n\n5. Limited failure analysis / error taxonomy. The paper would benefit from qualitative cases where TOOLRM fails or is brittle in long-horizon or multi-tool settings.\n\n6. **Limited LLM family choice**. The paper only conducts experiments on Qwen models, instead of others. It is unclear about the generalization to other LLMs, considering that some phenomenon may only occur on Qwen [2,3].\n\n7. **Lack of human evaluation or expert validation**. All preference labels are either rule-based or model-generated. No human annotator study or expert evaluation is provided.\n\nThe idea of using tool-use preferences to build generative RMs is beneficial, but conceptual novelty is modest relative to expanding prior work on tool-augmented RL, tool-aware critics, and RM-as-reasoning architectures.\n\n\n**References**:\n\n[1] Tool-augmented reward modeling. ICLR 2024.\n\n[2] Spurious Rewards: Rethinking Training Signals in RLVR. arxiv 2025.\n\n[3] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination. arxiv 2025."}, "questions": {"value": "1. I suggest the authors compare with and cite prior tool-augmented RM literature (ICLR’24 and successors).\nWhat is the conceptual delta relative to those works?\n\n2. Why choose binary RL reward over pairwise margin loss? Would a hybrid objective help?\n\n3. Can you provide SFT-initialization ablation, especially compare with cold-started data?\n\n4. What are the results on other LLMs? Can the reported conclusion generalize to other LLMs?\n\n5. Could the authors provide a failure mode breakdown analysis? Also, the authors can share qualitative examples of incorrect reward assignments.\n\n6. How sensitive are results to the rule-based scoring heuristics? Would human preference data outperform purely rule-based sampling? \n\n7. Did the authors conduct any human or expert evaluation to verify whether ToolRM's preference decisions align with real human judgments for tool-use effectiveness and correctness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HQguNaWq0z", "forum": "iVeqLVyWO8", "replyto": "iVeqLVyWO8", "signatures": ["ICLR.cc/2026/Conference/Submission15826/Reviewer_F85k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15826/Reviewer_F85k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168357257, "cdate": 1762168357257, "tmdate": 1762926052304, "mdate": 1762926052304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}