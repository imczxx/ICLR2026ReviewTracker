{"id": "ZNWpUfwisS", "number": 12170, "cdate": 1758206133749, "mdate": 1763022781252, "content": {"title": "Adaptive Test-Time Compute Allocation via Query Complexity Estimation in Large Language Models", "abstract": "Recent advances in test-time compute scaling have demonstrated substantial performance improvements for large language models through increased inference-time computation. However, existing approaches uniformly allocate computational resources regardless of query complexity, leading to significant inefficiencies. We propose AdaptiveComp, a principled framework that dynamically allocates test-time compute based on query complexity estimation. Our approach introduces: (1) a theoretically-grounded complexity estimator using information-theoretic measures, (2) a continuous resource allocation strategy with provable optimality guarantees, and (3) an uncertainty-aware early stopping mechanism.Through comprehensive evaluation on 8 benchmarks spanning mathematical reasoning, code synthesis, and multi-step planning, we demonstrate that AdaptiveComp achieves comparable performance to uniform high-compute baselines while reducing computational costs by 47.3±3.2% (p<0.001). Moreover, we establish theoretical connections between query complexity and optimal compute allocation, providing the first formal treatment of this problem. Our analysis reveals that complexity-aware allocation becomes increasingly beneficial as task diversity increases, with efficiency gains of up to 73% on heterogeneous datasets.", "tldr": "", "keywords": ["Adaptive Compute Allocation 、Large Language Models 、Complexity Estimation 、Inference Efficiency 、Resource Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b107b630dc8bffdcb3e7bc4c50e817fb8947c0cd.pdf", "supplementary_material": "/attachment/604b2a91b4f0ccf3a2192b7dfa54bd2a1e29e5d2.zip"}, "replies": [{"content": {"summary": {"value": "This paper discusses the idea of dynamically allocating computational resources to large language models based on the estimated complexity of the query. Although this direction is meaningful, the paper is clearly an incomplete manuscript, full of confusing omissions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The formalization of the performance and cost functions in the PROBLEM FORMULATION section of this paper is reasonable to some extent, although it lacks empirical support and does not guarantee generalizability."}, "weaknesses": {"value": "1. The paper is filled with statements such as “they apply uniform computational budgets across all queries.” However, many studies [1][2] have already discussed how to dynamically allocate computational resources for LLMs during inference based on prior or posterior information. The authors should clearly explain how their work differs from and relates to these existing studies. Moreover, LLMs—especially LRMs—already adaptively allocate computational resources during training according to the difficulty of the task. As in the example you gave, “What is 2+2?” versus “Prove that the sum of the first n odd numbers equals n²,” any large language model will allocate more tokens to the latter to improve answer accuracy. Therefore, the authors’ statement is clearly biased.\n\n2. Many symbols (t) and functions (nesting) in the METHOD section are not explained, nor are their specific computational procedures provided, making the paper difficult to read and reproduce.\n\n3. The proofs in the THEORETICAL ANALYSIS section lack detailed derivations.\n\n4. The EXPERIMENTAL SETUP section is particularly careless and clearly incomplete.\n\nIn fact, submitting such an unfinished manuscript is an explicit waste of the community’s reviewing resources, and such behavior should be subject to appropriate sanctions.\n\n[1] *Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs*\n\n[2] *Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning*"}, "questions": {"value": "As stated above.\n\nI want to emphasize again: authors who submit manuscripts like this are explicitly wasting the community’s peer-review resources, and similar behavior should be subject to some form of punishment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KRZXy77tUA", "forum": "ZNWpUfwisS", "replyto": "ZNWpUfwisS", "signatures": ["ICLR.cc/2026/Conference/Submission12170/Reviewer_3ifh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12170/Reviewer_3ifh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721346165, "cdate": 1761721346165, "tmdate": 1762923120734, "mdate": 1762923120734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "jtIIzG46Yw", "forum": "ZNWpUfwisS", "replyto": "ZNWpUfwisS", "signatures": ["ICLR.cc/2026/Conference/Submission12170/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12170/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763022779286, "cdate": 1763022779286, "tmdate": 1763022779286, "mdate": 1763022779286, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**AdaptiveComp** allocates inference time compute by estimated query complexity rather than a uniform budget. A complexity estimator based on information theoretic metrics and model representations produces a continuous mapping from score to budget and uses uncertainty aware early stopping during generation. The authors present formal framework linking query complexity to optimal resource allocation, and across benchmarks AdaptiveComp matches the accuracy of uniform high budget baselines while cutting average compute by about half."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper makes a strong contribution to the modeling and theory of resource allocation for test time scaling. The authors explicitly formulate test time compute allocation as a budget constrained optimization problem, clearly specify the relationships among performance, cost, and constraints, and provide practical optimality conditions together with efficiency upper bounds. The result is a coherent and testable framework."}, "weaknesses": {"value": "1. **The manuscript is difficult to follow.** Many sections present only formulas or figures without accompanying exposition, including definitions of variables and interpretations of observed trends. Even in the framework section, the workflow of the method remains unclear. The experiments section omits parameter settings and baseline configurations. A substantial revision is strongly recommended to improve clarity and reproducibility.\n\n2. **The related work survey is insufficient, and the experimental comparisons are incomplete.** A sizable body of research leverages query complexity or difficulty priors; for example, see reference [1][2]. Please include a structured review of this literature and add the corresponding comparative baselines.\n\n3. **The inference overhead of the complexity estimator and the feature extraction pipeline is not systematically quantified.** In low-resource settings, this overhead could significantly erode the claimed savings in compute.\n\nReference:\n\n[1] Learning How Hard to Think: Input-Adaptive Allocation of LM Computation\n\n[2] Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning"}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "izPJ3xrgfq", "forum": "ZNWpUfwisS", "replyto": "ZNWpUfwisS", "signatures": ["ICLR.cc/2026/Conference/Submission12170/Reviewer_dnXj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12170/Reviewer_dnXj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823618821, "cdate": 1761823618821, "tmdate": 1762923120067, "mdate": 1762923120067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ADAPTIVECOMP, an adaptive test-time compute allocator conditioned on a learned “query complexity” score. Components: (i) information-theoretic and linguistic features + a neural predictor, (ii) a continuous budget mapping via a sigmoid, and (iii) “uncertainty-aware” early stopping. Claimed results: similar accuracy to high-compute baselines with ~47% lower cost across 8 tasks, plus a “first” formal treatment of optimal allocation."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I do not think there are any strengths."}, "weaknesses": {"value": "1. This paper falls far below the standards of an academic publication. The proposed problem is overly simple and lacks novelty. The related work is insufficient. The content lacks too many details to understand. The experimental analysis is superficial. Necessary citations are missing.\n\n2. The paper’s central claim, existing methods *uniformly allocate computational resources regardless of query complexity*, is incorrect. [1] explicitly considers test-time scaling strategies that allocate compute based on query complexity, and [2] further demonstrates that different tasks require distinct budget allocations. As a result, the proposed motivation, method, and subsequent experiments in this paper are invalidated.\n\n3. Equation (1) is incorrect. As pointed out in [2], model performance is **not** a monotonically increasing function with respect to compute allocation.\n\n[1] *Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters*\n\n[2] *AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks*"}, "questions": {"value": "1. Specify the loss function, regularization, and calibration procedure for $f_\\theta$. Train/val split? Hyperparameters?\n2. Report compute units (FLOPs / tokens / latency) and per-query budget distributions in different TTS strategies (reasoning steps, beam search width, or verification iterations)  to demonstrate genuine adaptivity, not aggregate averaging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pvcO814yIm", "forum": "ZNWpUfwisS", "replyto": "ZNWpUfwisS", "signatures": ["ICLR.cc/2026/Conference/Submission12170/Reviewer_JwQX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12170/Reviewer_JwQX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900520048, "cdate": 1761900520048, "tmdate": 1762923119472, "mdate": 1762923119472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}