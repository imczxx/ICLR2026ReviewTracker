{"id": "71iwUe98cI", "number": 13197, "cdate": 1758214985453, "mdate": 1763762397879, "content": {"title": "Sample-Efficient Online Distributionally Robust Reinforcement Learning via General Function Approximation", "abstract": "The deployment of reinforcement learning (RL) agents in real-world tasks is frequently hampered by performance degradation caused by mismatches between the training and target environments. Distributionally Robust RL (DR-RL) offers a principled framework to mitigate this issue by learning a policy that maximizes worst-case performance over a specified uncertainty set of transition dynamics. Despite its potential, existing DR-RL research faces two key limitations: reliance on prior knowledge of the environment -- typically access to a generative model or a large offline dataset -- and a primary focus on tabular methods that do not scale to complex problems. In this paper, we bridge these gaps by introducing an online DR-RL algorithm compatible with general function approximation. Our method learns an optimal robust policy directly from environmental interactions, eliminating the need for prior models and enabling application to complex, high-dimensional tasks. Furthermore, our theoretical analysis establishes a near-optimal sublinear regret for the algorithm under the total variation uncertainty set, demonstrating that our approach is both sample-efficient and effective.", "tldr": "", "keywords": ["distributionally robust", "reinforcement learning", "function approximation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e91bd7b305d7c80d9416165e8c15fff1f9841f98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a near-optimal algorithm in online distributional robust reinforcement learning under the general function approximation setting, demonstrating sample efficiency. The algorithm is built on the classical UCB algorithm, making it easy to implement and robust. The authors provide both the regret and the sample complexity, clearly comparing their results with other papers, which makes their contribution easy to follow."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear Presentation: The paper is well-structured, effectively defining robust reinforcement learning and the robust value function. The definitions and assumptions are well-cited, enhancing the paper's credibility. The proofs are straightforward and easy to follow.\n\nUCB-Based Algorithm: The authors provide a comprehensive analysis of both sample complexity and regret for their proposed algorithm in an online learning context, demonstrating its efficiency. The identification of the appropriate robust coverage is crucial for learnability, making this the first online DR-RL algorithm suitable for large-scale problems with minimal structural assumptions."}, "weaknesses": {"value": "Lack of Regret Lower Bound: While the paper presents upper bounds for sample complexity and regret, it fails to include lower bounds for both metrics, which are essential for establishing the optimality of the algorithm. The absence of lower bounds may give the impression of insufficient rigor, as deriving the regret should be straightforward once the sample complexity is established.\n\nLack of Novelty: Despite the low workload of the paper, the algorithm appears to build on existing works, lacking originality. Additionally, the technical lemmas seem to follow established research, raising concerns about the novelty and technical depth of the contributions. It appears that the authors merely adapted existing methods to a new setting.\n\nUnclear Contribution: It would be beneficial for the authors to clearly outline their contributions in the introduction. Given that the technical lemmas are derived from existing works, the overall problem appears to be relatively straightforward, which may not meet the expected quality standards for such a conference."}, "questions": {"value": "1. Why is there no regret lower bound provided in the paper? What do you think are the main technical difficulties in deriving such a lower bound? I am considering raising my score if you can add the right lower bound in the final version of the paper.\n2. What is the main technical difficulty in proving the upper bound of the proposed algorithm? In my opinion, it all follows standard methods. Choosing the right coverage and the assumptions seems to also follow existing works.\n3. This algorithm is designed for the online RL setting. Can we extend it to the offline setting? Will simply changing the UCB to LCB be sufficient for the extension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GGZBxvhOxd", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["ICLR.cc/2026/Conference/Submission13197/Reviewer_oVvU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13197/Reviewer_oVvU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572239558, "cdate": 1761572239558, "tmdate": 1762923891550, "mdate": 1762923891550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Substantial Overlap with Prior ICML 2024 Publication Requires Attribution"}, "comment": {"value": "To the authors, reviewers, and Area Chair,\n\nI am writing to bring attention to a foundational prior work that is not cited in this submission, and with which this submission shares a substantial and uncredited overlap. \n\nThe prior work is: Panaganti, K., Wierman, A., & Mazumdar, E. \"Model-Free Robust $\\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data.\" ICML 2024 (https://arxiv.org/pdf/2405.05468, https://openreview.net/forum?id=wNu9YSYP8v). One of the results in this work focuses on the hybrid (offline and online data) version of the DR-RL problem. I thank the authors of this submission for citing my earlier work (Panaganti, K., Xu, Z., Kalathil, D., & Ghavamzadeh, M. \"Robust reinforcement learning using offline data.\" NeurIPS 2022) that is focusing on the offline version of the DR-RL problem. I am thrilled to see this ICLR 2026 submission focuses on the complete online version of the DR-RL problem.\n\nHowever, the overlap between this ICLR 2026 submission and the published Panaganti et al. (ICML 2024) paper is significant and covers the core problem formulation, key technical definitions, and the main theoretical proof structures. I request that the authors and reviewers compare the two manuscripts (the ICLR 2026 submission PDF and the provided ArXiv link). I provide side-by-side comparisons in this Google Slides link: https://docs.google.com/presentation/d/1l6jpzvuWbJi0XzhDlOQ0RHtU047r87itKnqJgNaLw7Y/edit?usp=sharing \n\nI request that the authors be instructed to: \n1. Add a thorough citation and acknowledgement of the Panaganti et al. (ICML 2024) paper. \n2. Reframe the contributions of this submission to clearly delineate what is novel versus what has already been established by that foundational work. (For example, modify the statements like \"first algorithm for online DR-RL with general function approximation under the total-variation uncertainty set\" as Panaganti et al. (ICML 2024) paper is also an online setting for the total-variation case.)\n3. Actual placement of this work in comparison to the Panaganti et al. (ICML 2024) paper. (For example, as discussed in Slides 5 and 6 in the Google Slides link, our work improves the coverage assumption.)\n\nFinal comments:\nThank you for your attention to this matter, which is critical for maintaining the integrity of the scientific record. I also thank the ICLR community for being open during the review period for a chance of public discourse. I have already updated the ICLR 2026 PCs regarding this case, but I decided to post it here, as ICLR welcomes public discourse. To respect everyone's (the authors, reviewers, ACs, SACs, and my) time, I am not expecting any form of interaction with this public comment, but rather a simple acknowledgement of reading this comment suffices.\n\nWell-wishing researcher,\n\nKishan Panaganti, on behalf of \"Model-Free Robust $\\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data.\" ICML 2024."}}, "id": "F06upMju7t", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["~Kishan_Panaganti1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Kishan_Panaganti1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763019370071, "cdate": 1763019370071, "tmdate": 1763019370071, "mdate": 1763019370071, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose to use function family to facilitate the robust RL. \nTheoretical analysis are provided to justify the advantages of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and the topic is interesting. The idea of using function family to improve robut RL sounds reasonable. \nThe theoretical part is useful to provide some insights."}, "weaknesses": {"value": "There is no specific examples even toy examples are provided to illustrate the proposed methods.\nThis makes it is difficate to judge whether it is easy to implement and train the models\nand how to choose the family of functions in pratice."}, "questions": {"value": "What kind of large-scale problems could be used to demonstrate the advantages of the proposed framework?\nIf experiments are designed, what kinds of alternative methods could be used to compare?\nHow to choose the function family for practical problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ySnj0pXpWK", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["ICLR.cc/2026/Conference/Submission13197/Reviewer_zaKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13197/Reviewer_zaKP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920328402, "cdate": 1761920328402, "tmdate": 1762923891288, "mdate": 1762923891288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper, *Sample-Efficient Online Distributionally Robust Reinforcement Learning via General Function Approximation (RFL-TV)*, proposes one of the first **online DR-RL algorithms** that operate under **general function approximation**.\n\nUnlike tabular RMDP works (e.g., *NeurIPS 2024 – Near-Optimal DR-RL with General Lp Norms*), this study focuses on the online setting, where agents interact with an unknown environment and must learn robust value functions via dual functional optimization.\n\nThe authors reformulate the robust Bellman operator under TV-divergence uncertainty as a **functional convex optimization** problem, introducing a dual function $g(s,a)$ to replace state-action-wise scalar dual variables.\n\nThis yields a tractable empirical loss function, enabling robust fitted learning through convex minimization.\n\nBuilding on this, the proposed **RFL-TV algorithm** constructs global confidence sets based on the fitted dual loss and uses the **optimism in the face of uncertainty (OFU)** principle to ensure exploration.\n\nThe resulting **robust regret bound** is\n\n$$ \\tilde{O}\\!\\left(H \\sqrt{C_{\\text{rcov}} H \\min\\{H, 1/\\sigma\\} K}\\right) - O(C_{\\text{rcov}} \\xi_{\\text{dual}}),$$\n\nwhere the newly defined **robust coverability** $C_{\\text{rcov}}$ quantifies exploration difficulty under model uncertainty.\n\nTo make function approximation theoretically valid, the paper introduces **Assumption 3 (Dual Realizability)**, requiring the dual function class $\\mathcal{G}$ to approximate the true dual optimizer.\n\nThis assumption is stronger than the Bellman completeness condition used in *Jin et al., 2021* but is necessary to bound bias from empirical dual optimization.\n\nIn summary, RFL-TV provides one of the first theoretically grounded frameworks for **distributionally robust online RL with general function approximation**, albeit under additional realizability assumptions and bounded coverage conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **general-function-approximation framework for distributionally robust online RL**\n    \n    The paper establishes the first principled framework for online reinforcement learning under distributional robustness, extending beyond tabular or offline DR-RL settings.\n    \n2. **Dual functional optimization for robust Bellman approximation**\n    \n    The authors reformulate the TV-divergence-based robust Bellman operator into a tractable **convex functional optimization** problem by introducing a global dual function $g(s,a)$, replacing pointwise scalar dual variables and enabling efficient empirical learning.\n    \n3. **Introduction of Robust Coverability and Sublinear Regret Guarantees**\n    \n    The work introduces **robust coverability** $C_{\\text{rcov}}$, a weaker and more general complexity measure than standard coverability or Bellman rank, capturing exploration difficulty under uncertain transitions.\n    \n    Based on this metric, RFL-TV achieves provable efficiency with the regret bound\n    \n    $$    \\tilde{O}\\!\\left( H \\sqrt{C_{\\text{rcov}}\\, H\\, \\min\\{H, 1/\\sigma\\}\\, K}    \\right)     - O(C_{\\text{rcov}}\\, \\xi_{\\text{dual}}),$$\n    \n    ensuring **sublinear regret** and **PAC-style sample efficiency** even under distributional uncertainty."}, "weaknesses": {"value": "1. **Strong dual realizability assumption (Assump. 3)**\n    \n    The theoretical results hinge on the additional **dual realizability** assumption, which requires the dual function class $\\mathcal{G} $ to approximate the true dual optimizer within $ \\xi_{\\text{dual}} $.\n    \n    This condition is non-verifiable in practice and notably stronger than the Bellman completeness assumption in *Jin et al., 2021*, limiting the theoretical generality and real-world robustness of the framework.\n    \n2. **Lack of empirical validation despite a concretely defined algorithm**\n    \n    Although the paper defines a clear algorithmic structure based on well-specified optimization objectives and assumptions, it does not provide any experimental results or numerical demonstrations.\n    \n    Given that the proposed RFL-TV method is theoretically implementable, empirical evaluation—even in simplified environments—would have significantly strengthened the paper’s completeness and practical credibility.\n    \n    In its current form, the work remains purely theoretical under restricted assumptions, leaving open whether the proposed dual functional optimization and confidence-set construction yield tangible performance benefits in practice.\n    \n3. **Restricted uncertainty model (TV-divergence only)**\n    \n    The analysis is limited to **TV-divergence** uncertainty sets, which, while foundational, restricts the generality of the conclusions.\n    \n    Extending the framework to broader f-divergence or Wasserstein uncertainty metrics—as explored in *NeurIPS 2024 Lp-norm DR-RL*—would enhance the universality and comparative significance of the results."}, "questions": {"value": "Compared with [A], this paper’s theoretical development remains less consolidated and would benefit from clearer connection between assumptions, algorithmic implementation, and empirical outcomes.\n\n---\n\n**Questions to the Authors**\n\n1. Could you treat the uncertainty level \\( \\sigma \\) and **Assumption 3’s approximation constant** \\( \\xi_{\\text{dual}} \\) as tunable hyperparameters and show how performance or regret changes as these values vary?\n2. The paper defines a seemingly implementable algorithm. Could you provide **empirical results** or demonstrations to verify its practical effectiveness under realistic settings?\n\n**Reference**\n\n[A] Pierre Clavier, Laixi Shi, Erwan Le Pennec, Eric Mazumdar, Adam Wierman, and Matthieu Geist,\n\n*Near-Optimal Distributionally Robust Reinforcement Learning with General Lp Norms*,\n\nNeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XfiKbizaml", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["ICLR.cc/2026/Conference/Submission13197/Reviewer_ii9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13197/Reviewer_ii9i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995290271, "cdate": 1761995290271, "tmdate": 1762923890871, "mdate": 1762923890871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RFL-TV, an theoretical online distributionally robust reinforcement learning algorithm that supports general function approximation under a total-variation (TV) uncertainty set. The authors adopt a functional optimization formulation of the robust Bellman operator that enables efficient estimation and learning without requiring state-action-wise optimization. They further use the notion of robust coverability to characterize the information deficit between the nominal and worst-case dynamics. Under  certain assumptions, the proposed RFL-TV algorithm achieves a sample complexity of $\\widetilde{O}\\left(\\frac{H^2 \\min(H, \\sigma^{-1})\\, C_{\\text{rcov}}}{\\varepsilon^2} + \\frac{C_{\\mathrm{rcov}}\\cdot\\xi_{\\mathrm{dual}}}{\\varepsilon}\\right)$, independent of the state–action space size, confirming its scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is the first one to provide a provably sample-efficient algorithm for the theoretical problem of distributionally robust reinforcement learning with both (i) online data collection and (ii) general function approximations.\n2. The paper potentially introduces new techniques to handle the error accumulation of functional-optimization-based robust Bellman equation estimator in the non-stationary online learning situation."}, "weaknesses": {"value": "1. The comparison of the results with previous arts could be further improved and made more detailed, in terms of the comparison of sample complexity, model structure assumiptions, coverage assumptions, etc.\n\n2. The paper lacks empirical or simple numerical validation of the proposed algorithm.\n\n3. Some other issues and questions that I detailed in the Questions part."}, "questions": {"value": "1. The theory relies on the completeness assumption that the robust Bellman operator with robust radius $\\sigma$ is closed in the function class $\\mathcal{F}$. I am curious would the assumption implicitly incur any necessary dependence of the function class size $|\\mathcal{F}|$ (which appears in the sample complexity bounds) on the robust radius $\\sigma$? \n2. The theory is developed based-on the assumption of finite robust coverability assumption proposed in He et al., 2025. In another series of works on robust RL with online data collection the sample complexity does not depend on the robust coverage coefficient due to the usage of other types of structural assumptions, e.g., failure states assumption. The authors said that the  the failure state assumptionguarantees the finiteness of the robust coverability. Then how to compare the sample complexity results obtained by these two types of theoretical assumptions?\n3. Can you highlight more what kind of new techniques are developed to handle the error accumulation of functional-optimization-based robust Bellman equation estimator? \n4. Inconsistent equations (8) and (9) for the terms $(1-\\sigma)\\cdot g(s,a)$ and $(\\sigma/2-1)\\cdot \\eta$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rm0mOXqisF", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["ICLR.cc/2026/Conference/Submission13197/Reviewer_tnWQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13197/Reviewer_tnWQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762208456256, "cdate": 1762208456256, "tmdate": 1762923890498, "mdate": 1762923890498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Main Revisions and Responses"}, "comment": {"value": "In our revised submission, we have made several changes to better highlight the core contributions, empirical support, and positioning of our work. We summarize the main clarifications and additions below.\n\n**Novelties and distinction from prior FA work.**\n\nIn the revised paper, we clarify that our main contribution is a *purely online* TV-robust RL algorithm with general function approximation and regret / sample-complexity guarantees expressed in terms of a new *robust coverability* coefficient $C\\_{\\mathrm{rcov}}$. Conceptually, we extend the dual functional machinery of offline DR-RL in [Panaganti-NeurIPS2022, Panaganti-ICML2024] to an online, off-dynamics setting with evolving visitation distributions and explicit optimistic exploration, rather than a fixed offline dataset or a $\\varphi$-regularized hybrid model. These points are now emphasized in the revised version of the paper.\n\n**Numerical experiments.**\nTo address concerns about practicality, we added a CartPole-v1 robustness study (Appendix B). We instantiate RFL-TV as a deep value-based algorithm with a dual network and evaluate it under both action perturbations and dynamics perturbations (force-magnitude and pole-length). We compare against DQN, GOLF and GOLF-DUAL [Xie-2022], and a tabular TV-robust baseline (OPROVI-TV). The results show that RFL-TV consistently improves robust return over the function-approximate baselines and matches or exceeds the tabular robust planner for moderate $\\sigma$, while also illustrating the effect of dual network capacity and the $\\sigma$–performance tradeoff. The full setup, hyperparameters, and main curves are documented in the new *Numerical Experiments* subsection and Appendix B of the revised manuscript.\n\n**Comparison: tabular, linear, and general FA.**\nOur bound is the first polynomial-order guarantee for robust online learning with general function approximation. We then *specialize* it to $d$-rectangular linear TV-RMDPs in the comparison section and Appendix~D. In the tabular case, taking $\\mathcal{F},\\mathcal{G}$ as all bounded functions yields a better dependence on state space $S$ and a worse dependence on horizon $H$, which indicates the scalability of our method. In the linear case, we show that $C\\_{\\mathrm{rcov}} \\leq \\mathcal{O}(d)$ and obtain regret / sample-complexity rates that are near-opto up to $\\mathcal{O}(d^2H^2)$ when compared to linear robust MDP lower bounds [Liu-2024], while explicitly tracking $C\\_{\\mathrm{rcov}}$ and not relying on offline data [Panaganti-NeurIPS2022] or $\\varphi$-regularization as in [Panaganti-ICML2024]. This positions our result as a unifying general-FA theory whose tabular and linear instances recover (and in some regimes sharpen) known DR-RL guarantees. However, our algorithm has greater applicability in general function approximation.\n\n** References **\n\n[Xie-2022]: Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online reinforcement learning. arXiv preprint arXiv:2210.04157, 2022.\n\n[Ma-2022]: Xiaoteng Ma, Zhipeng Liang, Jose Blanchet, Mingwen Liu, Li Xia, Jiheng Zhang, Qianchuan Zhao, and Zhengyuan Zhou. Distributionally Robust Offline Reinforcement Learning with Linear Function Approximation. arXiv preprint arXiv:2209.06620, 2022.\n\n[Liu-2024]: Zhishuai Liu, Weixin Wang, and Pan Xu. Upper and Lower Bounds for Distributionally \nRobust Off-Dynamics Reinforcement Learning. arXiv preprint arXiv:2409.20521, 2024.\n\n[Panaganti-NeurIPS2022]: Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. \nRobust Reinforcement Learning using Offline Data. Advances in Neural Information Processing Systems, \n35:32211–32224, 2022.\n\n[Panaganti-ICML2024]: Kishan Panaganti, Adam Wierman, and Eric Mazumdar.  Model-Free Robust $\\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data. \narXiv preprint arXiv:2405.05468, 2024."}}, "id": "pkFK9PqAXc", "forum": "71iwUe98cI", "replyto": "71iwUe98cI", "signatures": ["ICLR.cc/2026/Conference/Submission13197/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13197/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission13197/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763763424571, "cdate": 1763763424571, "tmdate": 1763765989428, "mdate": 1763765989428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}