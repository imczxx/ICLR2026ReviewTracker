{"id": "wjjqurlXvg", "number": 19083, "cdate": 1758293456519, "mdate": 1759897061617, "content": {"title": "Few-Shot Paraphrase Generation with LLMs: An Empirical Study of Models and Hyperparameters", "abstract": "The rapid progress of Large Language Models (LLMs) has made them widely used for data augmentation tasks, notably through paraphrase generation. For that task, paraphrases are expected to preserve the original meaning while exhibiting lexical diversity.\n\nIn this work, we conduct an empirical study of various off-the-shelf LLMs for paraphrase generation. We examine different prompting and decoding strategies, and compare systems with respect to their ability to follow predefined templates, retain semantic fidelity, and produce lexical variation.\n\nOur results show that LLMs are generally effective at generating paraphrases. However, guiding the generation process by providing initial tokens significantly improves adherence to required patterns. Under this condition, repetition penalties in decoding can further enhance output diversity. Interestingly, we also find that few-shot prompting may reduce lexical diversity.", "tldr": "", "keywords": ["paraphrase", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1d6baccde93ae8b27ae4aa7ad1576cb9afa60c74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is an empirical comparison of paraphrase-generation capabilities of different language models -- including small models that are fine-tuned for the task and larger models that perform this task through prompting. Besides comparing models, the authors in the paper also investigate how the decoding penalties and prompting strategies affect the generated paraphrases. To evaluate the paraphrases, authors use different metrics, such as whether it is possible to extract the sentence from the answer/or whether it follows the scheme from the prompts (\"sentence production rate\"), whether the meaning of the sentence is preserved, and diversity calculated as Levenshtein or Jaccard distance between the sentences."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors focus on different models, including both fine-tuned smaller versions as well as larger language models used through prompting -- not focusing only on prompting with LLMs leads to a more comprehensive comparison"}, "weaknesses": {"value": "**Limited novelty of the paper**\n\nOverall the main contribution of the paper is comparing different LLMs in their capability of generating the paraphrases and finding that using larger generation penaly leads to parahphrases that are more diverse -- which is kind of an expected observation, as it forces the model to not reuse the words that already appeared and so generate some wild sentences with larger vocabulary. Similarly, using the metrics such as edit distance does not really tell that much regarding the paraphrases, just that more words from the vocabulary is used -- if we would consider simple paraphrase generation methods, such as word swap or word insertion, these would also achieve high scores in these metrics, while also preserving the meaning (which seems to be problematic for different models) [3].\n\nAt the same time, there are already multiple papers that consider paraphrase generation and increasing their diversity/usefulness (e.g., [1, 2, 3, 4, 5] to name a few) which, in my opinion, go beyond the experiments in this paper. As such, I believe the paper does not bring anything new of value to the conferences of this rank. To be considered, I would expect there to be either significantly larger set of experiments (more models, strategies, approaches, etc.) or introduction of completely different way to evaluate or improve the paraphrasing quality.\n\n**Choice of metrics**\n\nThe choice of metrics for evaluating the quality of paraphrasing is a bit questionable. In essence, there is only evaluation of whether the generated sentence corresponds to the format (or in other words, whether it can be \"extracted\" from the generated text), whether it preserves the meaning (by measuring perplexity using a method introduced in different paper) and the edit distance, which can be easily high even for very similar sentences (using word swaps, inserts, etc.) or by using very niche words. However, I do not believe this provides any information about the quality of the paraphrases -- especially by penalising the generation, the sentences quickly become very strange or even incoherent for humans and are not really useful for any downstream task. Have you checked the generated sentences to check for this?\n\n\n**Missing related work**\n\nThere are already multiple papers that deal with paraphrasing. However, there is almost no mention of previous works (and the paper is completely missing the Related Work section) that would deal with the different problems -- such as the diversity of the paraphrases and how to increase it, other approaches for improving paraphrase generation, or even approaches that use non-LLM approaches (e.g., [1, 2, 3, 4, 5, 6]). As the focus of the paper is an empirical study of paraphrasing, I would expect there would be a larger number of approaches that are compared with the metrics in the paper.\n\n\n**Readability of the figures/paper as a whole**\n\nThe figures in the paper are hard to interpret. The radar plots use large amount of abbreviations which provides difficulties when interpreting it, while at the same time the choice of colours (and the number of models) do not really help (especially for Figure 2, where there are 3 very similar colours). \n\nAt the same time there are details missing for some of the metrics. For example, Figure 3 represents the \"mean edit/Jaccard distance\". However, most of the models achieve score of 0.6 -- should I interpret it that by changing only less than 1 character I can get to the paraphrase? I would suggest better explaining how this \"mean edit distance\" is calculated and/or normalised -- as it appears to be in the 0-1 interval it would help with interpreting the figures if one can understand what achieving score of 1 means.\n\nFinally, there is a lot of fluff introductory sentences in different sections (e.g., \"Let‚Äôs look toward the...\") and grammatical mistakes that make the paper harder to read.\n\n**References**\n1. Diversity-oriented Data Augmentation with Large Language Models\n2. Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation\n3. LLMs vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?\n4. Is ChatGPT the ultimate data augmentation algorithm?\n5. Srl-aco: A text augmentation framework based on semantic role labeling and ant colony optimization\n6. Data Swarms: Optimizable Generation of Synthetic Evaluation Data"}, "questions": {"value": "See weaknesses for more details:\n\nHave you checked the generated sentences how they look like? Whether the model is not just forced to use words that are seldomly used?\n\nHow should the \"Mean edit/Jaccard distance\" be interpreted? What does the score of 1 (or 0) mean in this case? And if we observe the score of 0.6 what does that mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GKxGymjYdJ", "forum": "wjjqurlXvg", "replyto": "wjjqurlXvg", "signatures": ["ICLR.cc/2026/Conference/Submission19083/Reviewer_5cT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19083/Reviewer_5cT3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760532490212, "cdate": 1760532490212, "tmdate": 1762931106810, "mdate": 1762931106810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically compares several medium-sized, imperative language models with several smaller models fine-tuned specifically for paraphrase tasks. The experiments uniformly employ a setup consisting of few-shot prompts, no-sample generation, and the introduction of a repetition penalty. The models' performance is comprehensively examined across three dimensions: adherence to the pre-set format or template, semantic fidelity of the generated content to the original sentence, and lexical diversity. The evaluation results demonstrate that adding a starting word or template prefix to the input significantly improves adherence to the output format, while introducing a repetition penalty consistently enhances lexical diversity, thereby increasing diversity. While few-shot prompts can guide the model's learning of the task pattern to some extent, they unexpectedly lead to a more concentrated lexical output in multiple scenarios, reducing diversity. Overall, this study reveals the specific impact of various generation strategies and input settings on paraphrase performance, providing empirical evidence for the effective use of imperative models in language generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This research is highly systematic, comprehensively comparing different prompting methods, decoding penalty strategies, and directive and non-directive models. The comparative experiments covered 1,456 source sentences from four recent corpora, with a unified evaluation process. The results are highly valuable. In terms of experimental design, the authors proposed a \"continuous generation\" approach, which guides model generation by presetting initial words, ensuring consistency in the format of candidate interpretations. This strategy is concise, effective, and easily reusable in engineering practice. Furthermore, the study found that under certain circumstances, few-sample prompts can reduce the lexical diversity of generated results. This negative finding is also significant, serving as a warning against the currently popular \"in-context\" learning approach. This finding is clearly demonstrated through distribution plots and mean tables."}, "weaknesses": {"value": "First, the paper's experimental design for \"multiple generation\" is logically inconsistent. The authors claim that sampling is not used to ensure reproducibility, and that greedy or fixed-beam decoding is used. However, the paper also states that each imperative model generates ten candidate results for the same sentence, repeating the process under different prompt templates and decoding parameters, resulting in 10 outputs per sentence. Under strict fixed-beam settings, the same input should not produce diverse outputs. The authors need to clearly explain the source of the randomness of these ten generation attempts, such as whether random scoring, random seeds, beam grouping perturbations, or input noise injection are involved. Otherwise, these samples are not statistically independent, and subsequent significance analysis will lose its basis. \n\nSecond, the construction of the overall indicators ùêª_{ùëö, ùëù, ùëë} lacks theoretical support. In formula (1), semantic fidelity and format adherence are given equal weights, while edit distance and Jaccard coefficient related to lexical diversity are each included with half weight. The paper does not explain the rationale for this weighting, nor does it perform dimensional normalization or sensitivity analysis. The authors are advised to standardize each sub-metric, such as using z-score or min-max normalization, and report the changes in results under different weighting combinations to verify the robustness of the composite metric. They should also clarify why \"format conformance\" and \"semantically equivalent interpretations\" are given equal weight, and why diversity is assessed separately rather than combined.\n\nFor the semantic fidelity assessment, the paper relies entirely on the LLM judge, ParaPLUIE, which introduces potential common bias. Although the authors have added regularization checking and language detection modules, ParaPLUIE itself belongs to a similar language model system and is prone to similar biases as the tested model when faced with sensitive topics (such as questions related to climate change). The authors are advised to include three human reviewers on a subset of samples to calibrate ParaPLUIE's accuracy and supplement it with multiple heterogeneous metrics for cross-validation, such as STS-B, BLEURT, or a calibrated BARTScore. Furthermore, metrics and threshold sweep curves could be reported separately for both highly controversial and common registers to verify the consistency of the model's judgments.\n\nThe conclusion that \"few-shot prompts reduce diversity\" also needs to be interpreted with caution. The current experiment strictly prohibits sampling and imposes a strong repetition penalty. Under this setting, the examples presented in the few-shot prompts can form a significant style prior, causing the model output to approach the linguistic neighborhood of the example template. This may be an artifact of the setting. It is recommended that the authors introduce slight temperature or top-p sampling as a control, without compromising the overall control conditions, to verify the robustness of this conclusion. Furthermore, the authors could compare the \"constrained decoding\" and \"prefix start token\" control methods and report length-normalized diversity metrics to eliminate the influence of sentence length on edit distance and Jaccard.\n\nThe LLM dataset used in the paper is potentially contaminated. The authors acknowledge that the LLM corpus is derived from PAWS and MRPC, and this data is likely present in the training corpus of some models. To ensure the fairness of the evaluation, the overlapping parts should be further detected, such as by using n-gram or MinHash to remove duplicate statistics, and the performance of the model on seen samples and unseen samples should be reported separately. In addition, the version number and release time of each model should be clearly stated to avoid misleading results caused by the overlap of training and evaluation data in time.\n\nFinally, it is suggested that there is an inherent coupling problem between the template and the format adherence rate. Since the \"continuous generation\" method itself imposes an output pattern in the input, the \"format adherence rate\" indicator partially reflects the constraint effect of the template design rather than the model's own compliance ability. It is recommended that the author report this indicator separately when not using continuous generation to remove the bias caused by the control measures.\n\nThere are still some details that need to be improved in terms of charts and expressions. The field naming and typesetting of Tables 3 and 4 are not standardized, Figures 1 to 3 lack confidence intervals or significance marks, and the symbols in Formula (1) are not systematically explained in the text. Overall, the clarity of the charts and symbols needs to be improved so that readers can accurately understand the experimental design and results.\n\nThe current version ignores research on unsupervised, search-based paraphrasing, making the article's methodological comparison incomplete. The authors are advised to supplement their discussion of this area in the revision and clarify the differences and connections between this paper and unsupervised methods based on global search, such as Unsupervised Paraphrasing by Simulated Annealing."}, "questions": {"value": "(1) Under the premise of no-sampling, how to obtain diverse candidates of \"10 times per sentence\"? Is there implicit randomness or input perturbation? Please provide the generation mechanism and deduplication statistics.\n\n(2) What is the discrimination threshold of ParaPLUIE and the false positive/missing negative rate in sensitive domains such as HC-Q? Has a consistency test with three manual reviews been conducted (Œ∫ value)?\n\n(3) Will \"continuous generation\" overfit the template token, thereby transferring diversity to the \"non-template area\" rather than the entire sentence? Has a comparison of \"edit distance after template removal\" been conducted?\n\n(4) The paper concludes that \"few-shot reduces diversity\". If mild Top-p/temperature is turned on or constrained decoding is used to preserve the format, does the conclusion still hold?\n\n(5) Do the negative results (mostly 0) of \"vulgar prompts\" and toxicity come from format extraction and strong suppression of taming? Can it be verified by free generation control?\n\n(6)Missing citation on unsupervised, search-based paraphrasing. Please add Liu et al. ‚ÄúUnsupervised Paraphrasing by Simulated Annealing‚Äù, as a canonical global simulated-annealing search approach, complementary to your local penalty/prefix-constrained decoding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "93AOM6YRHt", "forum": "wjjqurlXvg", "replyto": "wjjqurlXvg", "signatures": ["ICLR.cc/2026/Conference/Submission19083/Reviewer_g2GD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19083/Reviewer_g2GD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418243011, "cdate": 1761418243011, "tmdate": 1762931106216, "mdate": 1762931106216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper performs an empirical study of various LLMs on their paraphrase generation capacities. The paper explores multiple hyperparameters - such as decoding style, few shot prompting, vulgar vs standard examples, repetition penalties, and so on."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The studies conduction do provide some insights of effect of some hyperparameters on paraphrase generation."}, "weaknesses": {"value": "1. Overall the study feels incremental - building on an already existing paper [1]. In other words, the paper seems to mainly follow the framework and settings of [1] but explores a couple of hyperparameter variations. The results are also not particularly surprising - such as repetition penalty can increase diversity. \n\n2. Terminology seems inconsistent - especially in evaluation. For example: in the beginning \"Sentence production rate\" and \"Meaning preservation\" is mentioned. The later section mentions the use of ParaPLUIE. Then this is not mentioned anymore. Harmonic score mentions \"Para\" and \"Scheme\" without clearly linking back to whether it is talking about ParaPLUIE and sentence production rate. \n\n3. Unclear how well the proposed harmonic overall score reflects human judgment.\n\n4. Scheme adherence evaluation seems a bit superfluous. It seems like mainly a matter of adhering to a format like \"x) [paraphrase]\". This doesn't really seem like a real problem for modern LLMs and tools, as one can do structured generation using outlines/instructor etc. and/or use retries. But in this case, it's not even clear if adherance to structure is all necessary here.\n\n5. I am unclear on the effectiveness or meaningfulness in using an LLM-generated paraphrase corpora in evaluating LLMs for paraphrasing. If we are at the point where LLM-generated corpus can be treated as ground truth - shouldn't this be considered essentially a solved problem? What's a selling point here? We can just use the same strategy as was used for generating the ground truth in practice then. \n\n6. Unclear why no penalty setting uses beam size 1, and others beam size 5. Saying decoding strategy as greedy and beam size as 5 sounds like a contradiction because greedy decoding is beam size with 1 effectively. Moreover, the study seems to miss out the most basic strategy for increasing diversity - i.e using some temperature. Beam decoding is known for lower diversity, and greedy search is effectively temperature 0. \n\n[1] Paraphrase Generation Evaluation Powered by an LLM:\nA Semantic Metric, Not a Lexical One"}, "questions": {"value": "Why use different beam size for no penalty vs small/high penalty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xXncurJ26n", "forum": "wjjqurlXvg", "replyto": "wjjqurlXvg", "signatures": ["ICLR.cc/2026/Conference/Submission19083/Reviewer_dJea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19083/Reviewer_dJea"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558291204, "cdate": 1761558291204, "tmdate": 1762931105748, "mdate": 1762931105748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Review Summary of 'Few-Shot Paraphrase Generation with LLMs: An Empirical Study of Models and Hyperparameters'**\n\nThis paper offers a detailed empirical evaluation of LLMs on paraphrase generation, with a particular focus on maximizing lexical diversity while maintaining meaning. It compares two types of models: small fine-tuned paraphrasers (e.g., BART, T5) and medium-sized instruction-tuned chat models (3B‚Äì8B), tested across four curated datasets. Each model is evaluated with varying prompting schemes (zero-, one-, and four-shot), two prompt styles (standard vs. vulgar), and two output formats (free vs. continuation-style). A total of 10 prompting templates are defined.\n\nThe decoding strategy uses beam search only, with three penalty settings (none, small, huge) to promote diversity, notably omitting stochastic sampling. Evaluation spans three axes: format adherence, semantic fidelity (via ParaPLUIE), and lexical diversity (edit and Jaccard distance).\n\nKey findings include: (1) prefix-guided output formatting significantly boosts adherence, (2) diversity penalties increase lexical diversity without heavily sacrificing meaning, and (3) few-shot prompting often reduces diversity‚Äîlikely due to overfitting to example styles. These trends are robust across model types and datasets.\n\nHowever, the work's novelty is modest. The use of penalties and prompt variants confirms known behaviors, and the exclusion of larger-scale models (e.g., GPT-3.5/4) limits generality. The absence of sampling-based decoding is a methodological gap, as it‚Äôs a common tool for increasing output diversity. Additionally, the inclusion of vulgar prompts is only lightly justified and contributes little to the core goals.\n\nWhile the methodology is systematic and consistent across models, the evaluation lacks deeper qualitative analysis and human validation. The paper‚Äôs structure and writing are adequate but occasionally unclear, especially regarding prompt setup and model formatting. Overall, the study provides a useful, though incremental, benchmark on prompt and decoding effects for LLM-based paraphrasing, with practical insights into how format constraints and decoding knobs affect paraphrase diversity and fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**\n\n1. **Thorough Experimental Design**: The study explores a wide range of prompting strategies and decoding configurations across several instruction-tuned and fine-tuned models, enabling consistent comparisons.\n\n2. **Focus on Lexical Diversity**: Emphasizing lexical variation is a valuable angle for tasks like data augmentation, and metrics like edit and Jaccard distance provide useful indicators.\n\n3. **Effective Format Control**: The continuation prompting strategy notably improves adherence to output templates, simplifying evaluation and increasing reliability.\n\n4. **Insightful Prompting Observation**: The paper identifies that few-shot prompting can reduce diversity, a counterintuitive but practically relevant finding.\n\n5. **Use of Automatic Semantic Metric**: Leveraging ParaPLUIE for meaning preservation allows for scalable semantic evaluation beyond overlap-based metrics.\n\n6. **Reproducibility and Scope**: Testing publicly available models across over 200 outputs per input and applying a consistent evaluation protocol adds credibility to the results.\n\nWhile the contribution is largely empirical and incremental, it offers actionable insights into how prompting and decoding strategies impact paraphrasing performance."}, "weaknesses": {"value": "1. Lack of Novel Methodology**: The paper presents no new model or algorithmic innovation. Its contribution lies primarily in benchmarking existing models with prompt and decoding variants.\n\n2. Omission of Sampling-Based Decoding**: The study excludes common stochastic decoding methods such as top-*k* or nucleus sampling, despite focusing on diversity. This limits the completeness and applicability of the findings.\n\n3. Limited Model Range: Only mid-sized models (up to 8B) are evaluated, with no comparison to larger instruction-tuned models like GPT-3.5 or GPT-4. This restricts generalizability.\n\n4. Superficial Result Analysis: The evaluation relies solely on automated metrics, without qualitative error analysis or human evaluation to validate semantic judgments.\n\n5. Questionable Prompt Variant (Vulgar Examples)**: The inclusion of vulgar prompts is poorly motivated and adds little to the core insights. Its relevance remains unclear.\n\n6. Format Dependency for Evaluation: Strict output formatting is enforced to facilitate automated scoring, but this design choice may penalize otherwise valid paraphrases and obscure natural generation behaviors."}, "questions": {"value": "1. What was the rationale for excluding sampling-based decoding methods, especially given the goal of maximizing paraphrastic diversity?\n2. Were the few-shot examples static across all inputs? If so, did you consider using in-context examples closer in domain or structure to the test inputs?\n3. How consistent are the findings across datasets? Did any datasets show contradicting trends (e.g., few-shot prompting improving diversity)?\n4. What prompted the inclusion of vulgar prompts? Could you clarify what insights, if any, this condition provided relative to the study‚Äôs main goals?\n5. Did you conduct any human evaluation or spot checks to confirm the accuracy of ParaPLUIE classifications, particularly for borderline or ambiguous paraphrases?\n6. How do you anticipate the results might change with larger models or state-of-the-art APIs (e.g., GPT-4)?\n7. For practitioners aiming to generate diverse paraphrases, what specific prompting and decoding configurations would you recommend based on your findings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qjdTNlFY8v", "forum": "wjjqurlXvg", "replyto": "wjjqurlXvg", "signatures": ["ICLR.cc/2026/Conference/Submission19083/Reviewer_1J2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19083/Reviewer_1J2K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19083/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789707463, "cdate": 1761789707463, "tmdate": 1762931105049, "mdate": 1762931105049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}