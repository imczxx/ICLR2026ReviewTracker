{"id": "cPxmLZmFa7", "number": 17977, "cdate": 1758282622428, "mdate": 1759897141451, "content": {"title": "Local Entropy Search over Descent Sequences for Bayesian Optimization", "abstract": "Searching large and highly complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of arbitrary iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, yielding a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a practical combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.", "tldr": "Local Entropy Search is a Bayesian optimization algorithm that reduces uncertainty over an optimizer’s descent path, enabling sample-efficient local search for high complexity problems.", "keywords": ["Bayesian Optimization", "Entropy Search", "Local Bayesian Optimization"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/725397b3ed729169ac019e8ea1d8dd9c724dcde3.pdf", "supplementary_material": "/attachment/e694e34cc7371852aad2c1fd0339aa352e513771.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a new Acquisition Function (ACF) for Bayesian Optimization (BO), which especially improves the local search using the idea from two other main papers (Hennig & Schuler, 2012) and (Muller et al., 2021). In general, replacing the conventional sampler of the ACF at one point with a trained Gaussian Process model (GP), then maximizing utilities, this method proposes a gradient descent-based ACF with finite sequences to determine the potential candidates. The gradients of the GP model are used to create the sequences from different samples, and the potential candidates should minimize the entropy of these sequences, based on the assumption that reaching local extrema will have a stable gradient path. Their main contribution is the improvement of the stability and robustness of the local search within the GP model. Experimentally, the algorithm delivers remarkable performance in synthetic benchmarks, while showing marginal improvement in real-world problems. In addition, the ACF-only benchmarks are impressive, and the computational cost is handled well. The discussion is well done, which explains clearly the working conditions and the limitations of the approach. Overall, this work proves a slight contribution, which can potentially improve the outputs of low-dimensional problems (under 100 dimensions)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work demonstrates several notable strengths across originality, quality, clarity, and significance. First, it introduces a comprehensive and well-structured benchmarking framework, making the experimental evaluation both reproducible and conceptually clear. The presentation is straightforward, allowing readers to easily grasp the main ideas without being overwhelmed by technical complexity. Second, the paper’s core contribution, an efficient entropy-based search strategy for finding optimal values within GP models, is conceptually sound and addresses a gap in current Bayesian optimization approaches, where inner GP models' search efficiency has been underexplored. Third, the authors’ methodological choices, including their assumptions and theoretical lemmas, are particularly aligned with the challenges of local optimization under uncertainty, especially the computational cost problem. Finally, the synthetic benchmark results are consistently strong, showing that the proposed Local Entropy Search (LES) method outperforms existing baselines across multiple complexity levels, which is both empirically impressive and practically significant."}, "weaknesses": {"value": "While the paper is well written and experimentally solid, several aspects could be strengthened to better support its claims. First, the motivation for emphasizing local search over global exploration is not clearly justified. The paper references use cases in neural network optimization and other machine learning settings, which differ substantially from the BO context. Although improving local search efficiency is meaningful, the trade-off between local exploitation and global convergence remains underexplored. As a result, the proposed method effectively bridges an inner-GP optimization gap but simultaneously introduces ambiguity regarding its global behavior.\n\nSecond, the experimental evaluation is limited primarily to synthetic benchmarks. While these results are strong, the real-world tasks show marginal improvement, and the performance quickly degrades when computational budgets are constrained. They did compare with the current state-of-the-art high-dimensional BO methods, but the benchmarks are set for low-dimensional problems. Besides, their synthetic benchmark is GP-based, which raises a question of the harmony between the target model and the optimizer GP model. Third, although the authors provide lemmas ensuring that the acquisition function (ACF) can cover the entire input space, they omit theoretical analysis on regret bounds and convergence guarantees. Aligned with the first problem, the convergence condition is properly needed in this case, despite the logical assumption that reaching local extrema will have a stable gradient path. Finally, while computational efficiency is discussed, the ACF evaluation may become a bottleneck in high-dimensional settings, as it dominates runtime when the dimension increases. Since improving acquisition-function efficiency is an active research direction, this approach may be less competitive in high-dimensional or highly multi-modal problems. Consequently, LES performs strongly in low-dimensional spaces with relatively few local optima but may struggle to maintain its advantages as dimensionality grows. This shows a narrow possible implementation range for LES, where it matches the ideal condition."}, "questions": {"value": "There are some concerns that need to be clarified to strengthen the robustness of the work.\n\nIs there any mathematical proof or empirical validation regarding the behaviour of the LES ACF, particularly concerning the issue of incumbent search? This could be clarified if the data-point maps indicate whether the samples are regionally concentrated. The incumbent search problem was raised by (Hvarfner et al., 2024). Although the authors demonstrate that the LES ACF can balance exploration and exploitation through entropy minimization and achieve full-space coverage, stabilizing sequential gradient chains may still induce incumbent search and reduce the ability to perform large exploratory jumps. Even though the use of optimizers such as ADAM supports escaping local optima, and the authors state that the global value is not the target, being easily trapped in local minima remains a major bottleneck. Furthermore, regionally clustered data points may cause the hyperparameters to reflect only the characteristics of nearby areas, making it difficult to escape those regions. Addressing this could lead to discovering higher-quality local optima or even the global optimum.\nIn the experimental setting, the number of initialization samples is only two. Why is this number so low compared with other works? Has any empirical study been conducted to justify this hyperparameter choice? At the beginning, with only two samples, BO behaviour is purely exploratory or even random, relying heavily on the prior hyperparameters. BO typically requires a sufficient number of observations to adequately represent the search space, whereas the LES ACF amplifies this early-stage phenomenon by over-exploiting incumbent values. Consequently, the exploration phase, crucial for finding global solutions, is further reduced during the stage that contributes most to global discovery. A minor question: since BoTorch supports standardization, what does the statement “data is not standardized” specifically mean?\n(Hvarfner et al., 2024) claim their method improves performance in high-dimensional problems, most exceeding 100 dimensions. A benchmark with a higher dimensionality is therefore necessary to clearly reveal the limitations. Common synthetic benchmarks such as Levy or Hartmann should be included for more comprehensive comparison. Other local search methods have also shown strong results without neglecting global optimization, such as BAxUS (Papenmeier et al., 2022). The concern about BO-based benchmarks arises from the alignment between the target model and the surrogate model used by the optimizer. When both share similar characteristics and nearly identical uncertainty-quantification mechanisms, especially in within-model comparisons, the results may be biased. This study employs Matheron’s rule for sampling, which provides superior posterior draws, while other methods do not leverage it. Thus, much of the observed improvement might be attributed to the Matheron’s rule sampler rather than the LES ACF itself. Adding additional synthetic benchmarks would help clarify this effect. There is no need to test multiple levels of GP-based benchmarks.\nThere are further suggestions for improving the presentation:\n\nClaiming that the global optimum is not the objective is acceptable; however, the ability to escape local optima deserves stronger emphasis. Completely disregarding global-optimum search is a significant drawback that limits the method’s applicability. If this design choice is justified more clearly and explained earlier in the paper, the overall persuasiveness would increase considerably.\nFor your consideration, it may be beneficial to adopt a multi-start ACF. Although this may slow down computation, it could prevent the method from getting trapped in a single region. Using only one current best value is risky; incorporating two or three additional starting points in different regions, while reducing the number of gradient chains, could achieve a better balance between efficiency and reliability. Increasing the number of initial points, as noted above, would also enhance robustness and exploration consistency. Combining an exploration-oriented ACF with the LES ACF could further expand the applicability beyond strictly local optimization tasks.\nA brief note on structure: Section 4 contains only one subsection (4.1) and no 4.2, which seems redundant. Figure 4 appears in isolation and is discussed almost two pages later, perhaps due to layout constraints, but this should be improved. Early in the paper, when introducing the concept of gradient-based BO and entropy-based ACF, Figure 1 does not effectively illustrate the overall concept. A clearer overview diagram would help, ideally explaining the gradient-chain mechanism directly instead of using dense phrasing such as “The algorithm propagates the posterior belief over the objective through the optimizer, yielding a probability distribution over descent sequences.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "laLVJzr7ME", "forum": "cPxmLZmFa7", "replyto": "cPxmLZmFa7", "signatures": ["ICLR.cc/2026/Conference/Submission17977/Reviewer_8PaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17977/Reviewer_8PaV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609503262, "cdate": 1761609503262, "tmdate": 1762927770426, "mdate": 1762927770426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Local Entropy Search (LES) , a Bayesian optimization method for local optimization that aims to find the best solution reachable by an iterative optimizer starting from an initial design, rather than searching for the global optimum. The core idea is to reduce uncertainty about descent sequences by sampling GP posterior, running gradient optimizer on each sample to obtain finite step trajectories, then querying points that maximally reduce the average entropy of these trajectories. Empirically, LES outperforms global entropy search methods and existing local BO baselines on various benchmark problems up to 124 dimensions, though the method provides no theoretical guarantees and involves significant computational overhead compared to simpler local BO approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The acquistion function is very straightforward.\n- The combination of entropy search with local search is freshing"}, "weaknesses": {"value": "- The empirical benchmark, although demonstrate up to 124 d, is on somwhat easy and not very representative BO benchmark, its unclear that the method will really be competitive in high input dimensionality or not. I think to demonstrate the real benifit of the acquisition function, which to me is the most promising point of this work, is to conduct more thorough empirical results to provide more compelling results.\n- The algorithm is rather heuristic without too much theoretical insight. The existing theoretical analysis does not provide any insight."}, "questions": {"value": "- Is there any intuition that the algorithm will work better in high dimension? Because this is slightly counterintuitive to me as I would imagine the local trajectory get randomly distributed in high dimenison, making an entropy reduction also very random."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WOjV0JlFOk", "forum": "cPxmLZmFa7", "replyto": "cPxmLZmFa7", "signatures": ["ICLR.cc/2026/Conference/Submission17977/Reviewer_gXxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17977/Reviewer_gXxy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611078113, "cdate": 1761611078113, "tmdate": 1762927769107, "mdate": 1762927769107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of black-box optimization, in which one is trying to optimize a function while having access only to noisy function calls: no gradient, convexity, or other information is available.\nIn this setting, Bayesian Optimization (BO) is the state-of-the-art approach. BO learns a statistical surrogate for the objective and plugs it into an _acquisition function_ (AF), whose maximization yields the next design to evaluate, in an exploration-exploitation balancing way.\nThe authors contribute to this field with the introduction of a novel acquisition function, Local Entropy Search (LES), which belongs to the realm of information-theoretic AFs. Such AFs aim to find designs that maximize the _information gain_ about a random quantity, usually of interest for BO purposes, like the optimum $f^\\star$, its location $x^\\star$, or even $(x^\\star,f^\\star)$. The proposed LES selects a design that maximizes the information gain about a _local optimum_, here characterized as the last iterate of an arbitrary optimizer like gradient descent. After introducing their theoretical framework, the authors derive an efficient algorithm for computing LES and perform an extensive set of experiments, comparing LES against other state-of-the-art baselines on multiple synthetic and real-world examples, jointly with numerous ablation studies. LES emerges as a strong competitor by delivering the best overall performance, thus providing empirical evidence about the relevance of focusing on local optima rather than global optima when searching large, complex design spaces."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed AF belongs to the class of information-theoretic AFs, a class of theoretically grounded strategies. Framing the problem as trying to gain information over local optima rather than global optima is novel and well-motivated, specifically for high-dimensional settings, and the practical execution into a computationally reasonable algorithm is also a contribution in itself. \n\n- The experiments section is quite extensive, to say the least. I have to say that I rarely encounter BO papers involving this level of thoroughness at the experiment section level. This was quite pleasant.\n\n- ``Despite'' such a large breadth of experiments, LES does not seem to suffer any major flaw, compared to other baselines, and achieved the overall best performance. While one can always pretend that problems were cherry-picked to favor LES, I consider that the range of experiments here is wide enough to provide a fair representation of the test cases usually considered in the BO literature.\n\n- Besides comparing to other baselines, the authors carried out several ablation studies to assess the impact of modifying individual blocks, which did not reveal any major flaw either."}, "weaknesses": {"value": "I cannot pinpoint concrete weaknesses. The limitations of the approach are clearly stated, albeit briefly in the conclusion, but are presented in more detail in the appendix. I agree with such limitations; they do not represent grounds for rejection in my opinion."}, "questions": {"value": "Since I don't think the paper suffers from any major weaknesses, my questions stem from genuine curiosity rather than a desire to challenge the method. \n\n- Throughout the paper, an RBF kernel is employed for the GP surrogate. Draws from that surrogate are used to compute the acquisition function, where a finite number of optimization steps are performed for each draw, leading to a discretized descent sequence, again for each draw.\nGiven that the RBF kernel produces infinitely differentiable function draws, these should be quite amenable to gradient descent /ADAM, putting aside convexity. If one knows that the black-box function is not as regular as draws from an RBF kernel, one could opt for, say, a Matern 1/2 GP surrogate, whose draws should be more difficult to optimize. Have you tried to see how the performance of the AF changes as the kernel changes? In particular, I would assume that the results reported in Appendix E.6 (comparing different iterative optimizers) might vary conditionally on the kernel?\n\n- It might be interesting to discuss the case of latent space BO, where one learns a continuous latent representation $\\mathcal{Z}$ of a structured space $\\mathcal{X}$ (e.g., JT-VAE [1,2]) and performs BO within that latent space, decoding any selected design $x=h(z)$ and then evaluating it. Without any incentive, there is little reason to think that the function $z \\mapsto f(h(z))$ is smooth, and hence an AF based on maximizing the information about sequences produced by a gradient descent optimizer might not perform as well. As in the previous question, perhaps then a zero-order optimizer like CMAES might outperform ADAM/GD. \n\n[1] Junction Tree Variational Autoencoder for Molecular Graph Generation, ICML 2018\n[2] Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining, NeurIPS 2020\n\nSome comments:\n- Appendix E.6: performing a fair comparison with CMA-ES is always tricky, in my mind, given the number of hyperparameters it involves. You mentioned $\\sigma$, but there is also the popsize, the evolution strategy conducted every generation (e.g., bipop), to only mention two.\n- noise is kept pretty low throughout the paper, between $0.001$ and $0.02$ for standardized data. I know this is not the point of the paper, but varying the noise to see the impact would have been interesting as well. \n- page 21, a superscript \"2\" points to a footnote \"3\".\n- appendix F, I would also write the definition $f_t^\\star = \\text{sup}_x f_t(x)$\n- Defintiion 2: use $\\varepsilon$ or $\\epsilon$ not both ?\n- Algorithm 1 L9 involves $k_{test}$, shouldn't it be $\\delta_{test}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A5d88t4Xy3", "forum": "cPxmLZmFa7", "replyto": "cPxmLZmFa7", "signatures": ["ICLR.cc/2026/Conference/Submission17977/Reviewer_yURN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17977/Reviewer_yURN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836157942, "cdate": 1761836157942, "tmdate": 1762927768710, "mdate": 1762927768710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Local Entropy Search (LES) is presented as a local, information-theoretic Bayesian optimization framework aimed at finding better solutions in complex, large design spaces. LES combines iterative local optimizers with the entropy-search principle to identify the best reachable solution from a given initial design $x_0$ for an expensive-to-evaluate black-box function. The key idea is to target the solution produced by a chosen local optimizer by focusing on its descent sequence, and to propagate the uncertainty of a Gaussian process (GP) surrogate through that optimizer to induce a distribution over possible descent paths starting from the initial design. This leads to an LES acquisition that selects the next candidate by minimizing the entropy of the descent sequence, equivalently maximizing the mutual information between the new observation and the descent sequence conditioned on current data. The derivation uses a closed-form expression for predictive entropy and a Monte Carlo estimate of the conditional entropy over sampled sequences. With these components in place, the paper presents the LES algorithm. For practicality, conditional entropy is evaluated at discretized support points along the descent sequences, and an early-stopping criterion is employed. Empirically, LES shows strong performance on high-complexity settings and remains competitive with TuRBO."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper shows real depth, both theoretically and empirically. The authors clearly know the literature, handle the fine details, and take a principled path to build LES. \n\n2. The empirical study is broad, with enough variants to support strong claims about LES performance across complex settings, different distribution regimes (in-model and out-of-model), and a thoughtful set of baselines.\n\n3. The paper gives a robust treatment of edge cases. It shows the one-step equivalence to GIBO, explains why certain alternative formulations are intractable, introduces an early stopping rule based on probabilistic regret, and provides demonstrations across multiple levels of complexity.\n\n4. LES works across optimizers. The paper backs this with results for Adam, CMA ES, and others, which makes the robustness claim of LES credible.\n\n5. The authors paid careful attention to scalability, including a batched variant (QLES) for practical deployment and efficient sampling via decoupled GP.\n\n6. The experimental setup is clean and complete, which makes the work reproducible.\n\n7. The authors also provide a clear and sufficient exposition of all components of the LES algorithm, with careful pointers to the appendix for detailed explanations, which helps both interested and unfamiliar readers."}, "weaknesses": {"value": "1. The most prominent weakness, which the authors already acknowledge, is the tendency to get trapped in a local minimum basin, especially in complex settings with a highly multimodal posterior. The paper offers a careful discussion of this issue and introduces a stopping rule to trigger restarts, but the risk remains in challenging landscapes.\n\n2. LES performance depends on several core components, including the chosen optimizer, the surrogate model specification, and the underlying problem complexity. All of these are however not uncommon in BO settings, and the authors provided a clear account of this."}, "questions": {"value": "1. On local minima traps: the paper mentions multistart or switching between local and global algorithms. A recent ICLR paper by Adebiyi et al. (2025) proposes optimizing posterior samples via root-finding to identify starting points that lie near the global optimum of a posterior sample. Can the authors comment on the possibility of integrating such sorting algorithm in that paper to guarantee the selection of the ideal starting point to further extend the applicability of LES in multimodal scenarios?\n\n2. From the results, it appears LES and TuRBO remains competitive in several complex settings, can the authors comment on the incentives of LES over TuRBO?\n\n3. Although minor, could the authors comment on the selection of learning rate for LES-Adam? This can materially affect performance, especially in GP based settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tiSi67xtS1", "forum": "cPxmLZmFa7", "replyto": "cPxmLZmFa7", "signatures": ["ICLR.cc/2026/Conference/Submission17977/Reviewer_o3dG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17977/Reviewer_o3dG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956451254, "cdate": 1761956451254, "tmdate": 1762927768348, "mdate": 1762927768348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}