{"id": "n25T76QbxQ", "number": 17187, "cdate": 1758273211273, "mdate": 1759897191885, "content": {"title": "Activation Steering via Contrastive Causal Mediation", "abstract": "Where should we intervene in a language model (LM) to control behaviors that are diffuse across numerous tokens? To answer this question, we introduce Contrastive Causal Mediation (CCM), a procedure for selecting steerable model components from long-form responses. In CCM, we construct a dataset of contrasting inputs and LM responses that define a goal for the intervention, such as generating text in verse instead of prose. We then quantify how model components mediate the effect of the contrastive input signal on producing the contrasting LM responses and select the strongest mediators for steering. We evaluate CCM across three tasks—refusal, sycophancy, and style transfer—and three models, and find that it consistently outperforms correlational baselines that use probes to select attention heads for steering. Moreover, a lightweight CCM variant using a gradient approximation technique achieves equivalent performance. Finally, we show that while steering all attention heads succeeds on held-in test data, only steering a localized set of attention heads produces effects that generalize to held-out test datasets. Together, these results demonstrate how causally grounded mechanistic interpretability can enable effective control of LMs generating long-form text.", "tldr": "We show that steering models through contrastive signals from long-form text can lead to effective model control.", "keywords": ["steering", "interpretability", "localization", "causal mediation analysis", "control"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/73e00063a08b3f63669a6cbcc8553a213a4fd45d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an approach to steer models when generating multiple tokens, as opposed to a single token. To do so, the authors assemble a set of contrasting prompts that are defined by the goal of the intervention. Their method, contrastive causal mediation (CCM), enables steering with fewer attention heads and lower steering factors, allowing steering across long form text generation. They also explore how their selection of attention heads approach transfers to out-of-domain datasets and show the viability of transferring the selection on different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors contribute a fairly useful approach to steering over multiple tokens, that of which (to my knowledge) has little prior work in. The solution is simple and can be applied to multiple datasets. In addition, the solution is a ranking technique and can also be used across multiple attention patching techniques, showcasing its applicability."}, "weaknesses": {"value": "1. The current contribution is rather incremental because the contribution is mostly a method to create contrastive pairs for steering. One way to make the paper less incremental is to further investigate why steering certain datasets does not generalize well, because the way the datasets are investigated leaves the reader confused about why, for instance, why global steering fails on refusal and verse (for instance, what qualities of each dataset make one less amenable for steering than the other?).\n2. The results on MMLU are seemingly unmotivated and out of scope. It is unclear why MMLU results are included when the dataset as a whole does not make sense as a use case for steering (why would one like to steer with attention heads selected from any of the prior three tasks for MMLU, and why should we care?). If this is better motivated, it would be very helpful.\n3. The approach does not seem too generalizable since the method requires gridsearching over a number of hyperparameters, per dataset. Is there a more generalizable way to select hyperparameters?\n4. In general, there are some incomplete phrases (lines 433-434) and generally typos (lines 479-480), those of which are closer to the end of the paper/conclusion section. Also, repeated description of the held-out dataset on lines 303-305 with Sec 4.3."}, "questions": {"value": "1. Can you explain how the LLM-as-a-judge evaluation is done? It is unclear why 1-3 is considered inaccurate, and 4/5 are mapped to discrete notions of accuracy.\n2. Is there a reason why human evaluation is not done with LLMs-as-a-judge to verify alignment to humans?\n3. Following Q1, is there a table showing the accuracy of the LLM-as-a-judge for the queries in Table 1? Furthermore, it is a bit unclear how this is used based on the current paper description (essentially, are the queries that are given a score of 4 or 5 essentially used for the contrastive pairs?)\n4. Did you try other models for the LLM-as-a-judge, since different models will rate different statements \"more\" or \"less\" contrastive?\n5. How were the OOD datasets chosen?\n6. Were experiments done on non-DPO instruction-tuned models (or, even base models)? Some of the earlier instruction-tuned models may not be mid-trained or post-trained with RLHF/DPO, but it would be interesting to see how the attention head selection differ between the post-training regimes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PdmuXAmiar", "forum": "n25T76QbxQ", "replyto": "n25T76QbxQ", "signatures": ["ICLR.cc/2026/Conference/Submission17187/Reviewer_gaHv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17187/Reviewer_gaHv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761335171328, "cdate": 1761335171328, "tmdate": 1762927163117, "mdate": 1762927163117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to extract localized model components for steering models around concepts that are diffused over multiple tokens. They design a method, Contrastive Causal Mediation, that identifies subsets of attention heads that most impact model behavior on a dataset of contrastive pairs illustrating the concept to steer. The paper shows that the CCM selection methodology in numerous variants applies to multiple different state-of-the-art steering techniques, often requiring to patch only a small fraction of attention heads. They also show that patching attention heads can generalize to held-out, in-domain datasets, and showed an experiment that showed how LLMs can be steered even when performing QA on MMLU."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "W1. The idea of deriving steering attention heads from causal mediation on contrastive sets is novel (to my knowledge), clever, and effective. \n\nW2. The paper's results are substantial, and it considers numerous state-of-the-art baselines and ablations to the to selection method (CCM) and steering methods.\n\nW3. The paper is very clear, well written, and a pleasure to read. \n\nW4. The selection approach, which is based on relative log probs, is modular and agnostic to steering algorithm; this means it can be generally applied to future, improved methods that may develop, making the contribution likely to last longer than other methods."}, "weaknesses": {"value": "W1. The CCM approach is illustrated on relatively short prompts; it would be beneficial for the paper to show results on tasks with prompts that are nontrivially long, matching many realistic task settings.\n\nW2. (minor) The results for OlMo and SOLAR are slightly worse than for Qwen, but the grid search still generally achieves adequate steerability rates"}, "questions": {"value": "1. In Figure 4, what explains the high varaibility in OLMo performance with local steering on the refusal task? \n\n2. a couple of \\cite{} should be \\citep{},e.g. L.360 (REFT Wu et al), 427 Park et al\n\n3. Sentence ends prematurely on l479 - \"and single-token responses, and how do they Using\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IJHJXuj5rG", "forum": "n25T76QbxQ", "replyto": "n25T76QbxQ", "signatures": ["ICLR.cc/2026/Conference/Submission17187/Reviewer_FEVr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17187/Reviewer_FEVr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869040251, "cdate": 1761869040251, "tmdate": 1762927162697, "mdate": 1762927162697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method Contrastive Causal Mediation for selecting which attention heads to intervene on when attempting to steer for a particular concept or behavior. The method is agnostic as to the actual form of intervention, it just tells you which attention heads are the most important. Specifically the method ranks attention head importance by measuring the causal effect of each head on inducing the behavior when intervened upon. The paper tests the efficacy of the method across three tasks: refusal induction, sycophancy, and a \"talk in verse\" style transfer task. They compare against random baselines and other techniques such as probing for the concept using head activations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tests a range of hyperparameters for steering such as steering magnitude and fraction of attention heads for intervention. The paper evaluates their method across three distinct settings: style transfer, refusal, sycophancy. They also test for capabilities preservation by evaluating against MMLU."}, "weaknesses": {"value": "I feel the main weakness of this paper is that its core finding - that testing the causal effect of intervening on attention heads is an effective method for selecting intervention sites - is not novel: Function Vectors in Large Language Models (https://arxiv.org/pdf/2310.15213) (Todd 2024) introduces a \"causal indirect effect\" score for ranking attention head importance as intervention sites (equation 3). Their score and the \"indirect effect\" score in this paper (line 165) are largely the same."}, "questions": {"value": "Nit: for figure 2 I guess the final columns of each graph per row should be identical? There are small differences for sycophancy - maybe would make sense to separate out the final column visually. \n\nA question about interpreting the efficacy of steering for the verse task - would it be that the capability is localized or distributed? It seems only by intervening on all heads does it work to induce the model to speak in verse."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pn1ffFgsl4", "forum": "n25T76QbxQ", "replyto": "n25T76QbxQ", "signatures": ["ICLR.cc/2026/Conference/Submission17187/Reviewer_AQhS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17187/Reviewer_AQhS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926717057, "cdate": 1761926717057, "tmdate": 1762927162490, "mdate": 1762927162490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method to select attention heads for steering, particularly useful long-form generation. The method ranks the attention heads by identifying which ones are associated with a higher probability of a contrastive response w.r.t. the original response. The authors claim the method is more efficient than a random baseline and standard probing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel method for steering in long-form generation by identifying which attention heads increase the probability of contrastive pairs.\n- Evaluation was conducted on held-in and held-out datasets."}, "weaknesses": {"value": "- LLM as a judge set up: \n    - Unclear scale: First, the model is asked to come up with a Likert scale 1-5, then 1-3 is mapped to 0, 4 to 80 and 5 to 100 to get a score. Any reason why not just ask LLM to directly map to the 3 categories? \n    - Lack of validation: No validation of LLM as a judge is mentioned (particularly it is not clear how random or purposeful are 80 and 100 distinctions or 4/5 likert scale distinctions).\n\n- Experimental set up: the experimental set up does not seem to align with the central claim of the paper. The central claim of the paper is that \"CCM is consistently better than correlational baselines that use probes to select attention heads for steering.\" To show this, the authors need to show that across models, steering methods and for best hyperparameters CCM outperforms Inference-time-interventions and Random selections (at least -- these are the baselines identified by the authors). However, in their actual set up only includes difference-in-means steering to compare the effectiveness of CCM and ITI. It seems 4.2 only compares the steering methods themselves, and it is unclear whether CCM was best for each of these methods.\n\n- Analysis and presentation of results: the main results of the paper are supported by Figure 2, which presents a multitude of heatmaps that are difficult to parse. For example, to support their claim that CCM variants are more efficient than probing, the authors vaguely refer to \"upper left corner\" of CCM heatmaps. However, depending on how this corner is defined, it appears only the sycophancy task has a somewhat noticeable difference, while others are pretty much comparable. Moreover, there is simply no notion of statistical significance presented in the results. There were no tests or p values, no confidence intervals, and hence all of these trivial differences may have just occurred due to noise. Visually it does seem that sometimes the CCM selection is more efficient, but the authors need to devise a more rigorous procedure to test that, as currently they rely mostly on exploratory analysis."}, "questions": {"value": "nitpicks:\n- 2.2 and 2.3 should be joined in one section\n- Table 1 refers to ratings as Likert scales but they seem more like preference or concept detection prompts. Later it is described that they request LLMs to output on a scale but it was confusing initially.\n- The formula on line 256 is a bit out of context and not explained, could probably be removed\n- line 297 space before comma\n- section 2.1 and Tasks in Section 3 seem a bit repetitive. It is also confusing what overall was the pipeline for data generation. These sections need significant revisions for clarity. For example, distinction between base and source queries is not introduced and hence very confusing.\n- not clear what \"strict\" means in line 322. Also this is redundant with lines 306-310 just above.\n- figrue 2 appears before it is referred to"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2EOuCmgK2n", "forum": "n25T76QbxQ", "replyto": "n25T76QbxQ", "signatures": ["ICLR.cc/2026/Conference/Submission17187/Reviewer_kdRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17187/Reviewer_kdRW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930290584, "cdate": 1761930290584, "tmdate": 1762927162234, "mdate": 1762927162234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}