{"id": "0ucmlIQTlu", "number": 20711, "cdate": 1758309275360, "mdate": 1763740681624, "content": {"title": "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method", "abstract": "Facial expression recognition (FER) models are employed in many video-based affective computing applications, such as human-computer interaction and healthcare monitoring.  However, deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. To improve their performance, source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario, where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive. In this paper, personalized feature translation (PFT) is proposed for SFDA. Unlike current image translation methods for SFDA, our lightweight method operates in the latent space. We first pre-train the translator on the source domain data to transform the subject-specific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification. Using PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to image-based translation.  Extensive experiments on four challenging video FER benchmark datasets, BioVid, StressID, BAH, and AffWild2, show that PFT consistently outperforms state-of-the-art SFDA methods, providing a cost-effective approach that is suitable for real-world, privacy-sensitive FER applications.", "tldr": "we propose a new efficient source free domain adaptation method for personalization using feature-based translation for facial expression recognition in videos.", "keywords": ["unsupervised domain adaptation", "source free", "personalization", "videos", "facial expression recognition", "domain translation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d8c172660fe9d8ff6611ec0234cc01bb9bb7758.pdf", "supplementary_material": "/attachment/32e8d3d00919436dacb7384a28824483301dbc7b.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Personalized Feature Translation (PFT), a novel source-free domain adaptation method for facial expression recognition. PFT performs personalized adaptation by translating features in the latent space, requiring only neutral expression data from target subjects. This approach eliminates the need for complex image synthesis and source data access during adaptation, while maintaining high computational efficiency. Extensive experiments demonstrate that PFT consistently outperforms state-of-the-art methods across multiple datasets。"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The key strength of the proposed PFT method lies in its innovative feature-space translation paradigm. This method avoids the need for complex image synthesis and achieves computational efficiency through lightweight parameter adaptation. It demonstrated superior performance over state-of-the-art methods across four FER benchmarks."}, "weaknesses": {"value": "1 While the empirical results are strong, the paper lacks an analysis explaining the reasons why feature-space translation is more effective.\n\n2 The experimental validation is centered primarily on FER. It would be valuable to discuss the potential of PFT for other tasks that require subject-specific adaptation, such as face recognition or person re-identification."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "82rsZu8vFK", "forum": "0ucmlIQTlu", "replyto": "0ucmlIQTlu", "signatures": ["ICLR.cc/2026/Conference/Submission20711/Reviewer_2fSs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20711/Reviewer_2fSs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827603229, "cdate": 1761827603229, "tmdate": 1762934105117, "mdate": 1762934105117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Personalized Feature Translation (PFT), a source-free domain adaptation method for facial expression recognition that operates entirely in the feature space. By translating target subject features toward source-style prototypes using only neutral expressions, PFT achieves higher accuracy with significantly lower computational cost than image-based SFDA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces a highly original and impactful approach to source-free domain adaptation for facial expression recognition. By formulating a novel feature-level translation method that operates using only neutral target data, it achieves state-of-the-art performance while being dramatically more efficient than image-based alternatives. The work is exceptionally well-supported through rigorous experiments on four diverse benchmarks and presents a practical solution to key real-world constraints like data privacy and computational cost."}, "weaknesses": {"value": "1．\tWhile the proposed PFT method is well-motivated, the paper lacks a clear theoretical or intuitive explanation of why feature translation in latent space is inherently more robust than image-level translation for expression preservation.\n\n2．\tThe paper lacks a rigorous explanation or analysis of how the proposed losses ensure that the translator modifies only identity-related features while preserving expression-related ones.\n\n3．\tThe proposed PFT method relies on pre-training a feature-space translator, but there is no analysis of training stability or overfitting risks. Given the challenge of disentangling expression and identity, is there a risk of overfitting to identity features? Are there issues with convergence or conflicting objectives (style vs. expression)?\n\n4．\tThe experiments are conducted on only 10 target subjects per dataset. This small sample size raises concerns about the statistical significance and generalizability of the results. The authors should include confidence intervals or perform cross-validation across multiple random splits of target subjects.\n\n5．\tThe paper mentions hyperparameters such as λexpr and λstyle, but does not discuss their sensitivity or how they were tuned."}, "questions": {"value": "review the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ICHcmxy0UA", "forum": "0ucmlIQTlu", "replyto": "0ucmlIQTlu", "signatures": ["ICLR.cc/2026/Conference/Submission20711/Reviewer_7pCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20711/Reviewer_7pCo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831068518, "cdate": 1761831068518, "tmdate": 1762934102797, "mdate": 1762934102797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": ": This paper proposes a personalized feature translation method named PFT, which efficiently achieves domain adaptation for facial expression recognition models using only target user's neutral expression data. It outperforms existing methods across multiple datasets while significantly reducing computational costs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "（1） Structure: The paper features a clear structure, with algorithms presented in easily understandable diagrams. The problem definition is precise, and the motivation is well-articulated.\n（2） Innovation: The experiments introduce a personalized translation approach within the feature space, circumventing the instability inherent in traditional image-based methods.\n（3） Quality: The experiment design is rigorous, validating the method's effectiveness across four distinct datasets. Detailed ablation studies are provided to demonstrate the contribution of each component."}, "weaknesses": {"value": "（1） Although comparisons are made with multiple SFDA methods, the paper does not include more recently proposed personalized approaches based on generative models or meta-learning.\n（2） The paper notes performance degradation on elderly subjects but does not propose adaptive strategies for age differences. Further exploration of stratified or age-aware adaptation methods is recommended."}, "questions": {"value": "（1） Compared to models like SHOT and NRC that also update only partial parameters, how does PFT perform in terms of the number of iterations and total time required to reach convergence?\n（2） The core method involves using style loss to align identity features while employing expression loss to preserve facial emotion information. Is there any quantitative evidence demonstrating that the translator network indeed learns decoupled representations of these two factors? If the reference image itself carries strong non-neutral expressions, could this contaminate the translation process, leading to loss or confusion of expression information?\n（3） Was comparing PFT against such lighter-weight baselines considered to more clearly demonstrate the added value of feature translation over simple normalization or alignment strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fs9PYeWnJR", "forum": "0ucmlIQTlu", "replyto": "0ucmlIQTlu", "signatures": ["ICLR.cc/2026/Conference/Submission20711/Reviewer_nYCY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20711/Reviewer_nYCY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904968596, "cdate": 1761904968596, "tmdate": 1762934101849, "mdate": 1762934101849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "We would like to express our sincere gratitude to all the reviewers for their thoughtful and constructive feedback. Their comments have greatly helped us clarify and strengthen the paper. We appreciate the positive recognition of our work, including the novelty of Personalized Feature Translation (PFT) as a source-free, feature-level adaptation method (2fSs), the practicality of our neutral-only target setting (nYCY, 7pCo), the clarity of the presentation (nYCY), and the strong empirical results and computational efficiency (2fSs). The strong experimental validation across four challenging FER benchmarks is emphasized by nYCY and 7pCo also .\n\nReviewers mainly asked us to further clarify why feature-space translation is more robust than image-level translation, how the proposed losses lead to a disentanglement of identity and expression, and how stable and lightweight PFT is compared to SFDA baselines such as SHOT, NRC, and normalization/alignment-based methods. We have addressed all comments point by point, integrated the requested clarifications and analyses into the manuscript, and added new results and plots."}}, "id": "JJ4vZtFVpK", "forum": "0ucmlIQTlu", "replyto": "0ucmlIQTlu", "signatures": ["ICLR.cc/2026/Conference/Submission20711/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20711/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20711/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738483777, "cdate": 1763738483777, "tmdate": 1763738483777, "mdate": 1763738483777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}