{"id": "PvTxIdZc1E", "number": 19879, "cdate": 1758300191212, "mdate": 1759897014718, "content": {"title": "Weight Decay may matter more than µP for Learning Rate Transfer in Practice", "abstract": "Transferring the optimal learning rate from small to large neural networks can enable efficient training at scales where hyperparameter tuning is otherwise prohibitively expensive. To this end, the Maximal Update Parameterization (µP) proposes a learning rate scaling designed to keep the update dynamics of internal representations stable across different model widths. However, the scaling rules of µP rely on strong assumptions, particularly about the geometric alignment of a layer’s inputs with both its weights and gradient updates. In this large-scale empirical investigation, we show that these assumptions hold only briefly at the start of training in the practical setups where learning rate transfer is most valuable, such as LLM training. For the remainder of training it is weight decay rather than µP that correctly stabilizes the update dynamics of internal representations across widths, facilitating learning rate transfer. This suggests µP's scaling primarily acts as a form of implicit learning rate warmup, allowing us to largely replace it with modified warmup schedules. Together these findings fundamentally challenge prevailing beliefs about learning rate transfer and can explain empirical practice such as why µP requires the independent weight decay variant for successful transfer.", "tldr": "Empirically-focused study showing µP requires weight decay to successfully transfer learning rates across model sizes in practice.", "keywords": ["maximal update parametrization", "llm", "pretraining", "hyperparameter transfer", "learning dynamics", "adamw", "mup", "weight decay", "hyperparameter tuning", "scaling law", "transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a3f6b8e1a5438401291fd07052f52681f811ecf8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper discusses a previously known fact that $\\mu$P, which is commonly thought to give learning rate transfer between different model widths, actually does not unless paired with independent weight decay.\nAuthors highlight that $\\mu$P's alignment assumptions, when viewed in terms of relative updates, are incompatible with IndepWD. This can be reconciled by the fact that ultimately $\\mu$P's alignment assumptions are not correct.\nAuthors show empirically that IndepWD enables transfer of 'relative representational changes', whereas $\\mu$P does not.\nFinally authors propose particular learning rate warmup schedules as alternatives to $\\mu$P, and show that these are comparable to $\\mu$P."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "To my knowledge, emphasising the role of _relative_  representational changes in learning-rate transfer is novel, as is the perspective that weight decay, rather than the learning rate itself, governs successful transfer. The observation that $\\mu$P effectively acts as a learning-rate warm-up schedule is particularly insightful, as is the clarification that $\\mu$P conflicts with the usual recommendation to keep $\\eta\\lambda = \\text{const}$.\n\nThe paper convincingly demonstrates how $\\mu$P’s infinite-width assumptions fail in practice—since realistic networks do not maintain a small batch-size-to-width ratio—and consequently why $\\mu$P alignment can break down.\n\nThe authors illustrate their results in both transformers and resnets.\n\nI think the observations in this paper are of great interest to those training neural networks."}, "weaknesses": {"value": "It's not immediately obvious what this paper's contributions are, versus what already existed. I would appreciate the standard bulleted list at the end of the introduction to declare contributions unambiguously. I feel this would improve the clarity, and highlight the novelty of the paper.\n\nI would like to see more justification that RRC are what we should be interested in, as opposed to absolute norms of representations. As far as I can tell, there are not any plots for this. It's also unclear if transfer of the RRC appears for all weights, or just the one shown (from layer 13), possibly a figure to illustrate many/all layers should be included.\n\nI would like to see some tables with performance metrics. Some Tables in the Appendix confirming that optimal performance is similar across different settings ($\\mu$P+IndepWD, vs. $\\mu$P) would help, as well as performance figures to compare $\\mu$P to the proposed learning rate schedules."}, "questions": {"value": "- How did you pick which weight to show for Figures 3, and 4? The alignment ratio is almost exactly 1 for this weight, but it doesn't seem to hold in general based on Figure 16/Appendix G, and thus I'm not sure how valid the approximation for $\\alpha_{\\Delta W}/\\alpha_W$ is (in section 5.2).\n- Have you thought about $\\mu$P/IndepWD/RRC during fine-tuning? In particular with LoRA, and how your observations interact with work like [1]\n- I would be interested to see how the RRC plots behave for different optimizers?\n\n[1]: Hayou, Soufiane, Nikhil Ghosh, and Bin Yu. \"Lora+: Efficient low rank adaptation of large models.\" _arXiv preprint arXiv:2402.12354_ (2024).\n\nMinor points:\n\n- Is there a typo in line 105/106? 'high update alignment means that $w_k$ and $x_b$ tend to point in similar directions'\n- What is $\\eta_\\tau$ in Eq. (13)?\n- 'scaling effect' should be defined in Figure 5's caption, or at the very least, the right panel should be explicitly described.\n\n\nI would generally be in favour of acceptance if the weaknesses and my questions above can be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9d9eEaVjEw", "forum": "PvTxIdZc1E", "replyto": "PvTxIdZc1E", "signatures": ["ICLR.cc/2026/Conference/Submission19879/Reviewer_aDn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19879/Reviewer_aDn7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844952134, "cdate": 1761844952134, "tmdate": 1762932042262, "mdate": 1762932042262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Maximal Update Parameterization ($\\mu$P), a learning rate scaling for transfer between different model widths, and its relation to weight decay. To do this, the authors develop a framework based on relative updates (i.e., the size of updates proportional to the weights). They use this to show that independent weight decay effectively counteracts $\\mu$P scaling, which counterintuitively leads to an improvement in transfer. They show that this is because the alignment assumptions of $\\mu$P do not hold as training progresses in practice. However, because of this, independent weight decay stabilizes feature learning by counteracting $\\mu$P and maintaining the same relative representation changes across network widths. The authors show that early in training, $\\mu$P provides a benefit by acting as an effective learning rate warmup and support this by showing that a stronger learning rate warmup can replace $\\mu$P’s learning rate scaling with similar results."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper studies $\\mu$P scaling and weight decay, a subject of significant practical relevance for hyperparameter transfer. The authors identify why $\\mu$P scaling works with independent weight scaling, diverging from the reasons assumed by the theory used to develop it. The framework, approach, and results appear original. The paper supports its claims with theory and solid experiments using LLaMa and ResNet. The paper is well-written, clear, and well-motivated; the overall presentation of the paper is excellent."}, "weaknesses": {"value": "As someone who is less familiar with this area, I found it somewhat challenging to follow and get an intuition for the framework being introduced—in particular, comparing and contrasting between update alignment, weight alignment, relative weight updates, alignment ratio, and relative representation. The summaries and discussion helped with getting an overall understanding, but it would have been helpful to perhaps use less jargon or to give more intuition for the different terms."}, "questions": {"value": "How is update or weight alignment related to data eigenvector alignment (if it is at all)? I’m wondering if the “silent alignment” effect (Atanasov et al., 2022) is related to the alignment and muP “warmup” discussed here or if these are separate concepts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "58Cv4zsx1b", "forum": "PvTxIdZc1E", "replyto": "PvTxIdZc1E", "signatures": ["ICLR.cc/2026/Conference/Submission19879/Reviewer_x5KA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19879/Reviewer_x5KA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924778290, "cdate": 1761924778290, "tmdate": 1762932041674, "mdate": 1762932041674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies learning-rate transfer under the Maximal Update Parameterization ($\\mu$P) and the role of weight decay. $\\mu$P scales learning rates to keep feature updates stable across widths, supporting hyperparameter transfer from small to large models. The paper observes that $\\mu$P’s theoretical alignment assumptions only hold early in training, especially in large-batch settings common in practice.\n\nEmpirically, the authors show that independent weight decay, where the product of learning rate times weight decay $\\lambda \\eta$ is scale-invariant, is essential for stable learning-rate transfer in large models, while standard weight decay fails to provide transfer. \n\nThe authors further demonstrate that the primary effect of $\\mu$P in practice is similar to an implicit learning-rate warm-up. They show that explicit warm-up strategies can partly replicate the benefits of $\\mu$P."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Understanding why the learning-rate transfer works under $\\mu$P, and what happens under $\\mu$P scaling over long training horizons, are certainly interesting and of fundamental importance. In this sense, the experiments on the alignment ratio are a valid contribution, showing that no matter how weight decay is scaled (Figure 3, middle plot), the alignment ratio has a very weak dependence on the width.\n\n- The experiments are very well documented in the appendix, where there are details for each Figure.\n\n- The experiments on achieving good transfer when the width scaling is incorporated only in the warm-up phase are intriguing. It seems to suggest that $\\mu$P scaling is mainly required close to initialization (although there is increased instability, probably due to the fact that the updates explode with width after the warm-up phase). These warm-up experiments isolating early-training effects suggest that $\\mu$P’s impact may be concentrated near initialization."}, "weaknesses": {"value": "A central narrative of the paper is that independent weight decay “contradicts” $\\mu$P by overriding its scaling. Quoting from the paper: *“This fundamentally differs from $\\mu$P’s prescribed scaling in Equation 6, meaning that independent weight decay scaling contradicts $\\mu$P by eventually overriding its update scaling.”*\n\nMy interpretation differs: under *standard* weight-decay scaling, the contribution of weight decay vanishes as width grows (as also noted in [1]: *“However, the implied EMA timescale now changes with model size, while we hypothesize that the optimal timescale should not vary with model size.”*), which breaks width-invariance of feature updates — a core $\\mu$P premise. From this perspective, independent weight decay does not contradict $\\mu$P, but rather restores its intended property that the components of the feature updates remain width-invariant. Under standard weight decay, weight-decay’s contribution vanishes in the large-width limit (this can be shown via Tensor-Programs-style calculations), which, in my view, explains why the optimal learning rate shifts to the right in Figure 1 (middle plot): increasing the learning rate partially compensates the vanishing contribution of weight decay.\n\nThat said, the paper raises an important possibility: over long training horizons (e.g., when total steps scale with width), the training trajectory may move sufficiently far from initialization that $\\mu$P’s assumptions no longer hold. This is a compelling direction. However, the conceptual leap from “assumptions break late in training” to “learning-rate transfer fails” would benefit from further justification and clearer separation from the effect of vanishing weight decay. In fact, the paper’s experiments seem to show deviations from the $\\mu$P regime under both independent and standard weight-decay scaling (as observed in the alignment ratio plots, Figure 3 middle). To me, this suggests instead that the vanishing contribution of weight decay with width is the cause of poor learning-rate transfer. This does not contradict $muP$'s philosophy of maintaining scale-invariant contributions to the feature updates.\n\n\n**Clarification request**\n\nIs Figure 4 conducted under a setup that actually uses $\\mu$P? In the appendix, it is written: *“the hyperparameters are exactly the same and the specified learning rate is applied to all parameters”*, which seems to suggest that **no $\\mu$P scaling** was applied. If so, how can this be used to claim $\\mu$P assumptions break, if $\\mu$P was not used? I would appreciate a clarification.\n\n**Warm-up discussion**\n\nIn light of the above discussion, the result that learning-rate warm-up alone cannot replace $\\mu$P (Figure 6 left) is not surprising. This is because the exploding updates with width cause the optimal learning rate to shift left.\n\n**Overall**\n\nThis paper contains valuable experiments, but I believe the narrative requires restructuring. In particular, I would focus more on the investigation of $\\mu$P over long training horizons and the role of weight decay.\n\n\n[1] Xi Wang, Laurence Aitchison, How to set AdamW's weight decay as you scale model and dataset size."}, "questions": {"value": "How is $\\Delta Y / Y$ computed? At the same time step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a0EcivPGFo", "forum": "PvTxIdZc1E", "replyto": "PvTxIdZc1E", "signatures": ["ICLR.cc/2026/Conference/Submission19879/Reviewer_rEs9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19879/Reviewer_rEs9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106487654, "cdate": 1762106487654, "tmdate": 1762932041226, "mdate": 1762932041226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the role of weight decay in learning rate transfer. In particular, the paper argues, both empirically and theoretically, that independent weight decay is crucial to learning rate transfer, i.e. the value of \\eta * \\lambda in PyTorch's AdamW implementation, should be fixed for different model widths. The key role of the independent weight decay is about controlling the relative update size of the feature. According the MuP's assumption at model initialization, the relative update size should scale with model widths, however the authors point out that as training proceeds, the assumption on independency between the  layer inputs and gradients, would break, and the relative update should become a constant value with respect to the model widths, which can be achieved by scaling the weight decay together with model width.\n\nThe authors then answers a couple of natural questions that arise, such as why would the assumption break and why we still need MuP scaling of learning rate anyway?\n\nThe paper conducts large scale empirical experiments on LLM pre-training under extensive hyperparameter sweep. \n\nOverall I find the paper's claims and analysis convincing; the arguments and analysis angles from the paper can inform future works on hyperparameter transfer."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The obesrvations and the arguments from the submission are not surprising, MuP's scalnig relies on lots of assumptions that don't hold and are unverified in later phases of the training, so it is not surprising that things will break. However, this paper is the first work, as far as I am concerned, that shows rigorously which ingredient in the MuP scaling breaks and why that is the case.\n\n- The paper addresses most of the questions that naturally arise with the arguments, e.g. how and why the MuP assumptions break or why we still need MuP scaling of learning rate in practice.\n\n- Although the paper involves lots of non-trivial technical details, I personally find the paper easy to follow, and the presentation of the results very clear.\n\n- The experiments are fairly large-scale and convincing."}, "weaknesses": {"value": "- It is unclear when the phase change will happen, and how does the phase changing will happen, does it have anything to do with the model size, the model width, etc.\n\n- The paper does not seem to provide lots of new practical takeaways; the main conclusion seems to be that: One should use decoupled weight decay, which is already explicitly pointed out by many recent works.\n\n- The paper seems only to argue that if this \\eta * \\lambda is **not fixed**, then learning rate transfer would break, i.e Eq.4 would start to depend on model widths. However, there is no direct argument on why learning rate becomes transferable as we fix \\eta * \\lambda. \n\n- The paper is mostly around how to choose \\eta, but does not show how to choose the value of \\eta * \\lambda or how to choose the value of \\lambda."}, "questions": {"value": "- Line 114, in the caption, should it be \"... with a fixed absolute size |\\Delta W|\" rather than \"|W|\"?\n\n- While I do undersand the argument intuitively, can the author clarify more on \"... while a given relative change ∥∆W∥/∥W∥ always has the same impact ...\", is \"same impact\" measured by the norm of \\Delta Y, or is it measured by the value of Y?\n\n- Could the fact that MuP with independent weight decay is needed because we want to restrict the absolute size of both the weights and the activations? I imagine under low precision LLM pre-training, we certainly want to weight norm and the activation norm. In particular, I believe the activation norm should depends on weight norm and the widths, and in order to prevent it from blowing up, we want to control the norm of W. Especially from the middle column in Fig.6, it seems that the consequence of not using MuP + independent weight decay is mostly on training stablity rather than the optimal learning rate starts to shift.\n\n- If \\eta * \\lambda should be kept constant and \\eta should scale with model width, do we have to scale \\eta ~ 1 / m? Can we e.g. \\eta as \\eta_base / \\sqrt(m) or \\eta_base / m^p ?\n\n- One question that I feel unanswered by the paper is: How should we pick the value of \\eta * \\lambda? The paper's experiments show that, if we keep \\eta * \\lambda fixed, then optimal \\eta_base becomes transferable, but how should we choose the value of \\lambda or the value of \\eta * \\lambda?\n\n- It seems that there are two phase changes happening during the training, the first one is when the alignment ratio goes from width dependent to a value closer to 1, the second one is when the model entering the \"equilibrium\" and Eq.3 starts to hold, are these two phase changes happening at the same time?\n\n- Why is the norm used for Frobenius norm rather than, e.g. Spectral norm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MYAjdrKnpN", "forum": "PvTxIdZc1E", "replyto": "PvTxIdZc1E", "signatures": ["ICLR.cc/2026/Conference/Submission19879/Reviewer_wA1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19879/Reviewer_wA1t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19879/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146192369, "cdate": 1762146192369, "tmdate": 1762932040693, "mdate": 1762932040693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}