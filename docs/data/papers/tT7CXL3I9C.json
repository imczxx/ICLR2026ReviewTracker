{"id": "tT7CXL3I9C", "number": 14510, "cdate": 1758237666971, "mdate": 1759897365911, "content": {"title": "Tilt matching for scalable sampling and fine-tuning", "abstract": "We propose a simple, scalable algorithm for using stochastic interpolants to perform sampling from unnormalized densities and for fine-tuning generative models. The approach, Tilt Matching, arises from a dynamical equation relating the velocity field for a flow matching method to the velocity field that would target the same distribution tilted by a reward. As such, the new velocity inherits the regularity of stochastic interpolant transport plans while also being the minimizer of an objective function with strictly lower variance than flow matching itself. The update to the velocity field that emerges from this simple regression problem can be interpreted as the sum of all joint cumulants of the stochastic interpolant and copies of the reward, and to first order is their covariance. We define two versions of the method, Explicit and Implicit Tilt Matching. The algorithms do not require any access to gradients of the reward or backpropagating through trajectories of the flow or diffusion. We empirically verify that the approach is efficient, unbiased, and highly scalable, providing state-of-the-art results on sampling under Lennard-Jones potentials and is competitive on fine-tuning Stable Diffusion, without requiring reward multipliers. It can also be straightforwardly applied to tilting few-step flow map models.", "tldr": "scalable algorithm for sampling and fine-tuning generative models by correcting the velocity field with a novel covariance term.", "keywords": ["sampling", "generative modeling", "flow matching", "stochastic interpolants"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10d10e2c52aae74ea89f5662b7f9facfd34f9e67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method to adapt flow matching models for sampling from tilted distributions. Starting from a flow matching model that samples from $\\rho_1$, the goal is to generate samples from $\\rho_{1,a}(x) \\propto \\rho_1(x) \\exp(a, r(x))$ where $r$ is a reward function and $a \\in [0,1]$ controls the tilt. This allows sampling from unnormalized densities and performing reward-based fine-tuning. The authors derive the optimal velocity field transporting to $\\rho_{1,a}$ and its derivative with respect to $a$. Using a Taylor expansion in $a$, they obtain mean-square objectives whose minimizers approximate the optimal velocity fields. The first-order approximation defines Explicit Tilt Matching (ETM), while higher-order corrections lead to Implicit Tilt Matching (ITM), which requires a fixed-point solution. They show that an importance-weighted version of the classical flow matching loss corresponds to a special case of the ITM loss with a control variate. For small time steps, this makes the importance-weighted loss higher in variance, highlighting the stability of ITM. Experiments on (i) Lennard-Jones sampling (from the iDEM paper) and (ii) diffusion fine-tuning (from the Adjoint Matching paper) confirm that ETM and ITM achieve superior performance, demonstrating scalability and robustness in both sampling and fine-tuning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a very elegant and well-formulated idea.\n* The paper acknowledges its limitations (e.g., approximation error due to incomplete minimization of the objective, discretization error) and proposes reasonable solutions to address them"}, "weaknesses": {"value": "* The paper contains numerous typographical errors.\n* The paper does not mention or discuss the closely related work [1], which introduces a very similar idea referred to as \"temperature guidance\". This work was released contemporaneously with Adjoint Sampling and PITA.\n* The experimental evaluation of the sampling component is weak, as it compares only to neural samplers and omits well-established sampling techniques such as (Adaptive) Parallel Tempering [2] or (Adaptive) Sequential Monte Carlo [3,4]. Moreover, several baselines used for comparison (e.g., DDS, PIS, iDEM) are already known to perform poorly, as reported in prior work [5,6].\n* There are no ablation studies on simple Gaussian mixture models to analyze the behavior and properties of the proposed sampling algorithm (see [5,7] for examples in sampling).\n* The paper does not include an empirical comparison with the importance-weighted variant (WFM).\n* There is no clear description of key experimental details, including training hyperparameters, time discretization (with respect to $t$ or $a$), ODE/likelihood integration method, noising schedule, number of samples used, standard errors for the sampling experiments, or any ablation on computational cost.\n* The fine-tuning metrics reported in Table 2 do not demonstrate a clear improvement with the proposed method, particularly in terms of the ClipScore, which appears largely unchanged.\n\n[1] Rissanen, S., Ouyang, R., He, J., Chen, W., Heinonen, M., Solin, A., & Hernández-Lobato, J. (2025). Progressive Tempering Sampler with Diffusion. In Proceedings of the 42nd International Conference on Machine Learning (pp. 51724–51746). PMLR.\n\n[2] Syed, S., Bouchard-Côté, A., Deligiannidis, G., & Doucet, A. (2021). Non-Reversible Parallel Tempering: A Scalable Highly Parallel MCMC Scheme. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(2), 321-350.\n\n[3] Saifuddin Syed, Alexandre Bouchard-Côté, Kevin Chern, & Arnaud Doucet. (2024). Optimised Annealed Sequential Monte Carlo Samplers.\n\n[4] Yan Zhou, Adam M. Johansen, & John A.D. Aston (2016). Toward Automatic Model Comparison: An Adaptive Sequential Monte Carlo Approach. Journal of Computational and Graphical Statistics, 25(3), 701–726.\n\n[5] Blessing, D., Jia, X., Esslinger, J., Vargas, F., & Neumann, G. (2024). Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling. In Proceedings of the 41st International Conference on Machine Learning (pp. 4205–4229). PMLR.\n\n[6] Maxence Noble, Louis Grenioux, Marylou Gabrie, & Alain Oliviero Durmus (2025). Learned Reference-based Diffusion Sampler for multi-modal distributions. In The Thirteenth International Conference on Learning Representations.\n\n[7] Grenioux, L., Noble, M., & Gabrié, M. (2025). Improving the evaluation of samplers on multi-modal targets. In Frontiers in Probabilistic Inference: Learning meets Sampling."}, "questions": {"value": "* In the paragraph \"Approximation error due to incomplete minimization of the objective\", you mention using MCMC steps. How sensitive is the method to the number steps? What is their computational cost relative to the overall procedure?\n* Regarding the sampling procedure, a more advanced version of the WFM approach is proposed in [8], where importance sampling is replaced by an Independent Metropolis–Hastings (IMH) scheme. This method follows the same principle as yours (progressively training at decreasing temperatures) but without the clever use of the $a$-derivative of the velocity field. Instead, it reuses the network trained at step $a$ as a warm start for step $a+h$. **(a)** Could you provide an empirical comparison against WFM (IS + FM) and MFM [8] (IMH + FM)? **(b)** Could you perform an ablation study to demonstrate the benefit of incorporating the $a$-derivative, e.g., by comparing your current strategy (with MCMC refreshments) against a variant that simply reuses the previous network as a warm start for the next step?\n* The energy interpolation path defined in Eq. (24) is known to suffer from mode-switching issues (see, e.g., [9]), meaning that the relative proportions of different low-energy regions can change significantly and abruptly as $a$ varies. Such rapid shifts may invalidate the Taylor approximations and reduce the reusability of the velocity field learned at step $a$ for step $a+h$. How do you address or mitigate this potential instability in your approach?\n\n[8] Cabezas, A., Sharrock, L., & Nemeth, C. (2024). Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows. In Advances in Neural Information Processing Systems (pp. 104383–104411). Curran Associates, Inc.\n\n[9] Bálint Máté, & François Fleuret (2023). Learning Interpolations between Boltzmann Densities. Transactions on Machine Learning Research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WBNkK0hRxD", "forum": "tT7CXL3I9C", "replyto": "tT7CXL3I9C", "signatures": ["ICLR.cc/2026/Conference/Submission14510/Reviewer_PMF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14510/Reviewer_PMF8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760704479010, "cdate": 1760704479010, "tmdate": 1762924906470, "mdate": 1762924906470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a theoretically novel approach, tilt matching. The analysis is interesting and different from prior work, and the proofs appear sound. Empirically, the authors conduct experiments in the traditional energy function land and also in the text-to-image generation land."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "There are two main streams for obtaining the posterior distribution from the prior and the reward function.  AM like approach requires the reward to be differentiable while other approaches do not.  The latter approaches typically fall into the importance sampling land which is traditionally be known for having poor sample efficiency like iDEM. The proposed approach improved the sample efficiency compared to other importance sampling baselines which offer an alternative approach when dealing with non-differentiable reward.  \n\nIn addition, the theory behind tile matching is also interesting as it formulates the changes needed as a regression objective which we have known from FM/DM that it is stable."}, "weaknesses": {"value": "While the paper’s theoretical perspective is sound, the empirical results are less compelling than expected.\nTable 1 reports higher ESS than iDEM and substantially better performance, but a strong baseline, ASBS is not included. Assuming comparable experimental settings, ASBS achieves better results on the more complex energy function (LJ-55), which raises questions about the scalability of the proposed approach.\n\nFor Table 2: if you followed the same experimental setup as Domingo-Enrich et al. (2025), did you train and test on the same set of 100 prompts? Although ETM marginally outperforms AM, the diversity score drops considerably. This may be due to how you sample $X_1$, the ODE/SDE noise schedule, etc. In addition, there is no comparison of computational cost with AM. I suspect your approach is more expensive; reporting number of samples, number of function evaluations would be valuable to the community.\n\nPlease also check the question section. \n\n[1] Liu, Guan-Horng, et al. \"Adjoint Schr\\\" odinger Bridge Sampler.\" arXiv preprint arXiv:2506.22565 (2025)."}, "questions": {"value": "- The derivation of equation (26) is missing. How did you arrive at (26) from the tilted distribution defined at X_1? \n- Table 2, Looking at the results reported in Table 2 for AM, the default scheduler’s results are used compared to the fine-tuning scheduler.  Also, why did the author choose to omit the ImageReward metric?\n- Importantly, can the author compare the sample efficiency in Table 2? How many samples do you need to generate from iterative denoising to  arrive at the results compared to AM? \n- Could the author explain the purpose of flow matching recentering in the algorithm and provide a theoretical justification? “A few flow matching steps” sounds vague and makes the algorithm hard to reproduce."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ly6k43N8Hd", "forum": "tT7CXL3I9C", "replyto": "tT7CXL3I9C", "signatures": ["ICLR.cc/2026/Conference/Submission14510/Reviewer_ojYB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14510/Reviewer_ojYB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889360356, "cdate": 1761889360356, "tmdate": 1762924906129, "mdate": 1762924906129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to tilt stochastic interpolants in the objective either to sample from a target distribution known up to normalization or to fine-tune the model according to a given reward function. \n\nThe proposed method is iterative, progressively annealing the strength of the titling from an untilted model up to the desired reward function/unormalized target distribution. The idea is to rely on the predicted evolution of the optimal velocity field along this annealing of the tilting parameter to derive a loss for the velocity field at tilt a+h (where h is the increment step) when the velocity has been learned at tilt a. This derivative can be estimated using covariances and yields losses that can be estimated using generated samples at previous values of the tilt. Two versions of the algorithm are discussed according to different strategies to discretize the evolution of the velocity field along the variation of tilt a. \n\nA variance reduction method is also proposed that allows to reduce the variance of the flow matching objective, but it is unclear if it is implemented in the numerical experiments. \n\nNumerical experiments are presented on:\n-  sampling from the Boltzmann distribution of clusters of Lennard-Jones particles of either 13 or 55 particles where an increase of performance seems to be achieved compared to other samplers using continuous time generative models. \n- and fine-tuning of Stable-Diffusion using the ImageReward score where the performance seem to be competitive with adjoint matching, in particular without the need to fine-tune the strength of a reward multiplier."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- 1 - The paper is overall well written and easy to follow. \n- 2 - The proposed approach is to my knowledge entirely novel and the numerical experiments are limited but encouraging."}, "weaknesses": {"value": "- 3 - The method requires retraining iteratively to a satisfactory accuracy as the training objective at tilt a+h leverages samples from the model at tilt a: This entails a rather important computational budget and the methods proposed by the authors to “refresh” the model to circumvent this issue are not entirely clear. \n- 4 - The second main limitation of the paper is the limited numerical evaluation of the proposed approach as no systematic studies are conducted: \n\t- On the effect of the discretization step of the annealing is the tilt, while this governs the computational cost of the method. While the ITM is supposedly free of “discretization error” it seems unlikely that one can go in one step from a=0 to a=1. This discussion is currently missing in the paper. \n\t- For sampling from unnormalized densities, the paper produces benchmarks that are usual but that are providing little information on the expected behavior of the method beyond these benchmarks. I note that for LJ55, nor the ESS, nor the histograms of energies and pairwise distances are provided. As multimodality of the target distribution and high-dimensionality are two big challenges in sampling that justify going past traditional MCMC or Sequential Monte Carlo methods, systematic experiments of increasing dimension and increasing separation of the modes are simple to run and offer crucial information to assess the method (see e.g. Grenioux et al 2025). \n\t- It is unclear whether the variance reduction approach proposed is employed and how in the experiments.\n\n\nMinor: \n- Figures are not referenced in the main text.\n- line 410, typo, Tilt Madtching. \n- Some sentences in the introduction can be made more precise:\n\t- L35 “These models work by building a continuous time map connecting a base distribution to a target distribution, realized by solving a differential equation whose coefficients are neural networks.” I am not sure that we should say that the coefficients are neural networks rather than terms themselves of the differential equations involve neural networks. \n\t- L078 “where we achieve state-of-the-art performance on sampling Lennard-Jones potentials”, the authors should add “with diffusion based samplers”, since for these simple LJ clusters at relatively high-temperature it is unclear that classical samplers are not more efficient in wall-clock time.\n\nRef: \nGrenioux, Louis, Maxence Noble, and Marylou Gabrié. “Improving the Evaluation of Samplers on Multi-Modal Targets.” Paper presented at Frontiers in Probabilistic Inference: Learning meets Sampling. ICLR Workshop on Frontiers in Probabilistic Inference: Learning Meets Sampling, April 24, 2025. https://openreview.net/forum?id=d91E9RhVFU."}, "questions": {"value": "- 5 - Related to 3, can the authors comment on the computational budget of their approach compared to competitors in the experiments they ran?\n- 6 - Related to 4, Can the authors clarify their refresh procedure? On which newly generated samples would they perform flow matching? Which samples would a buffer contain? Would the refined steps be run with a local MCMC steps? What would be the limitation of the approach?\n- 7 - Related to 4, for LJ55, although the ESS is computationally costly to compute, the histograms of energies and pairwise distances only necessitate generating a batch of samples, can you please report them?\n- 8 - line 269 - Which ESS exactly would be computed to dynamically decide for the update step in explicit tilt matching? ESS in sampling at step a+h with model trained at tilt a?\n- 9 - In loss (17), can the authors clarify if the residual is evaluated using the estimated field $\\hat b_t$ where $b_{t,a+h}$ lies in the definition of residual?\n- 10 - How exactly would the control variate function be learned? Is it used in the presented experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nA1yBslp73", "forum": "tT7CXL3I9C", "replyto": "tT7CXL3I9C", "signatures": ["ICLR.cc/2026/Conference/Submission14510/Reviewer_QPg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14510/Reviewer_QPg2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899079246, "cdate": 1761899079246, "tmdate": 1762924905698, "mdate": 1762924905698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Tilt Matching, a framework for: sampling from unnormalized densities and fine-tuning large-scale generative models based on reward functions. The core contribution is the derivation of a \"Covariance ODE\": a dynamical equation that describes the evolution of the stochastic interpolant's velocity field with respect to the tilting parameter. Two variants ETM and ITM are proposed to learn the parameterized models for tilting. A key advantage of the proposed methodis that it doesn’t require gradients of the reward function or backpropagation through the flow's trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Writing: I found the writing to be easy to follow, though the work assumes familiarity with a lot of background material.\n- Theoretical Insights: The derivation of the Covariance ODE is very interesting and the connection to the Esscher transform is quite insightful as well.\n- Promise of practical Advantages: Proposed ETM/ITM hold the promise to solve key problems with existing methods: 1) avoid backprop through trajectories, 2) avoid reward gradients, and 3) lower variance than WTM. The empirical validation starts to offer some evidence in this direction."}, "weaknesses": {"value": "- Disconnect between theory and empirical validation: Paper offers ITM as a superior alternative to ETM. However, ETM seems to outperform ITM on the tested small scale configuration for sampling as well as in term of stability for finetuning (no ITM results are reported due to it’s instability in finetuning experiments). Given that the paper spends a significant amount of space motivating and deriving the ITM variant, I find the empirical validation preliminary and lacking. Trusting Table-1, I am willing to believe that ITM is more scalable, however rest of the experiments fail to establish it’s superiority over ETM variant. Further, while ETM appears to do well on sampling, it’s empirical advantage over alternatives seems to be statistically insignificant from Table 2. While I am willing to believe the hypothesis that ETM is infact superior, further experimentation is clearly needed to support that. Lastly, the sampling experiment doesn’t seem to be actually testing sampling from a tilted distribution, but rather from the same distribution at a different temperature. This can be seen by substituting E_0=E_1/T_0. This presumably is much easier to do than if E_0 was an entirely different density, e.g. a Gaussian. Similarly, ITM is proven to offer better variance than WFM, however, this is not empirically demonstrated. More importantly, it is not clear from experiments that the theoretical result actually translates into any practical gain. Overall, in my view, the experiments just don’t support the theoretical results yet, and significant non-trivial work is needed to establish that. \n\n- Lack of comparison to reweighted flow matching: reweigthing is an obvious and easy to implement alternative that doesn’t require much of the theoretical machinery, while offering a much simpler alternative. No evaluation is provided that reject this as a preferable alternative. Again, hinting towards an incomplete experimental evaluation.\n\nOverall, while the theory is promising, I think the paper is severely lacking in empirical validation and needs significant amount of work to support the hypothesized advantages of theoretical results."}, "questions": {"value": "Please see the weaknesses section for key issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wdbC4bssBE", "forum": "tT7CXL3I9C", "replyto": "tT7CXL3I9C", "signatures": ["ICLR.cc/2026/Conference/Submission14510/Reviewer_dbLF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14510/Reviewer_dbLF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954248553, "cdate": 1761954248553, "tmdate": 1762924904647, "mdate": 1762924904647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}