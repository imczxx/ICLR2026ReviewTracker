{"id": "oiz0QHejVj", "number": 7305, "cdate": 1758014972428, "mdate": 1763717932738, "content": {"title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression", "abstract": "Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement.  However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel $\\textit{mapping-based}$ CLIP compression framework, $\\textbf{\\textit{CLIP-Map}}$. It leverages learnable matrices to map and combine pretrained weights by $\\textit{Full-Mapping with Kronecker Factorization}$, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose $\\textit{Diagonal Inheritance Initialization}$ to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.", "tldr": "", "keywords": ["Model Compression", "Pruning", "Contrastive Language Image Pretrain"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cada3a1cb00e6edf65108609316f284e9799e02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new framework CLIP-Map, which uses learnable matrices to map and combine pre-trained weights to retain more original weight information; it also addresses optimization challenges by reducing distribution shifting. Experiments show CLIP-Map outperforms select-based frameworks across compression ratios, with notable gains under high compression."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It introduces Kronecker product to reduce the complexity.\n\n2. The testing datasets including general and specific domain as shown in Table 2 is adequate for the experiments.\n\n3. It demonstrates three types of CLIP-Map(base, small and tiny), which can meet different requirements."}, "weaknesses": {"value": "1. For depth compression, it assumes the new layer can be expressed as a linear function of the old layer but it may not be true. Its primary goal is to preserve a portion of the original weights, which is already demonstrated by diagonal initialization. Can we also construct other Kronecker product matrices with a block product of 1 to preserve some weights?\n\n2. The performance drops dramatically especially on Stanford Cars(from 88.5 to 50.8) and MNIST(from 65.8 to 13.0).\n\n3. The results of other methods like quantization and distillation shall be compared."}, "questions": {"value": "This paper essentially performs selective initialization on weights, which can reduce training overhead and thereby improve efficiency. The question then arises: whether there exist more efficient initialization methods, such as Weakness 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RzpuLT1yW4", "forum": "oiz0QHejVj", "replyto": "oiz0QHejVj", "signatures": ["ICLR.cc/2026/Conference/Submission7305/Reviewer_24zq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7305/Reviewer_24zq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291038755, "cdate": 1761291038755, "tmdate": 1762919422137, "mdate": 1762919422137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the CLIP-Map model compression framework. Unlike existing approaches, CLIP-Map addresses the optimisation challenge of mapping matrices by leveraging Kronecker factorisation and learnable inter-layer linear combinations. The authors also introduce a diagonal inheritance initialisation technique. However, this paper also exhibits several significant limitations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses a practical and timely issue: large multimodal models incur substantial computational and storage overhead, which is critical for resource-constrained devices. Furthermore, the core approach of replacing selection and discard with mapping and combining pre-trained weights is reasonably justified. This method can preserve the original model's information, demonstrating particularly significant effects at high compression rates, whereas comparable methods may result in critical information loss. The authors' experiments further demonstrate that CLIP-Map achieves substantial performance gains under extreme compression ratios over the TinyCLIP baseline model, thereby validating its advantages."}, "weaknesses": {"value": "1) This paper's core idea is that mapping preserves more information than selection currently remains at an intuitive level, lacking rigorous theoretical support. For instance, the authors fail to analyse from a mathematical perspective why this mapping-based structure can more effectively maintain the knowledge fidelity of pre-trained models. 2) The experiments lack ablation studies. For instance, the paper provides no ablation experiments to distinguish the respective contributions of width and depth compression."}, "questions": {"value": "The current experiments appear to omit a key ablation study: how much do these two components contribute to the model's performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bgzP6vp5XQ", "forum": "oiz0QHejVj", "replyto": "oiz0QHejVj", "signatures": ["ICLR.cc/2026/Conference/Submission7305/Reviewer_TpdV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7305/Reviewer_TpdV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554420064, "cdate": 1761554420064, "tmdate": 1762919421654, "mdate": 1762919421654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mapping-based compression method. It utilizes Kronecker factorization to map pre-trained weights to a lower dimension to reduce model size. To train the mapping matrix, this paper also introduces Diagonal Inheritance Initialization to alleviate distribution shift issues and stabilize training. The effectiveness and efficiency of this method are demonstrated in extreme compression scenarios with high compression rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The methods proposed in this paper are technically sound, and both contributions offer relevant improvements for clip compression. Moreover, the experiments are comprehensive, with thorough exploration conducted on both classification and retrieval tasks. This task is also quite important."}, "weaknesses": {"value": "Can the compressed CLIP model be widely applied to downstream tasks? The experiments in this paper only evaluated retrieval and classification tasks, without assessing other tasks such as generation or some comprehension tasks. If the model can only improve performance in classification and retrieval but not be applicable to these other tasks, its applicability in a broader range of fields would be significantly reduced. It would be even better if comparisons with other compressed models on other tasks could be provided.\n\nDiagonal initialization is one of the main contributions of this paper, which is also experimentally validated. However, the improvement compared to other initialization methods is quite significant. The diagonal initialization method has been mentioned in other works (e.g., \"On the Parameterization and Initialization of Diagonal State Space Models\"), and it is not a novel algorithm. Therefore, its innovation seems limited."}, "questions": {"value": "see above .The depth compression part in Figure 3 could be made clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ir4VPwfNnf", "forum": "oiz0QHejVj", "replyto": "oiz0QHejVj", "signatures": ["ICLR.cc/2026/Conference/Submission7305/Reviewer_V2MN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7305/Reviewer_V2MN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620874729, "cdate": 1761620874729, "tmdate": 1762919421247, "mdate": 1762919421247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors studied the problem of compressing CLIP models efficiently while preserving performance. They proposed CLIP-Map, a mapping-based CLIP compression framework that uses learnable structured matrices to map and combine pretrained weights instead of simply selecting subsets of weights. The method leverages full-mapping with Kronecker factorization and a Diagonal Inheritance Initialization strategy to preserve original weight distributions. The compressed model is first learned through mapping while freezing the original CLIP, followed by a knowledge-distillation retraining stage using the original CLIP as a teacher. Experiments show that CLIP-Map outperforms selection-based compression approaches across various compression ratios, with especially strong performance under high compression settings"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-organized and easy to follow.\n- The proposed method is effective, and the use of structured matrix mapping is novel in the context of CLIP compression.\n- The experimental results are comprehensive and convincing."}, "weaknesses": {"value": "- This paper greatly violates ICLR's formatting guidelines (see suggestions below). The paper should not be accepted in its current form.\n- Equation (10) is not rigorous. The kronecker product of $F^{in}$ and $F^{out}$ initialized with Equation (9) is not an identity matrix (the diagonal entries are not all 1 and there are non-zero off-diagonal entries)"}, "questions": {"value": "- Questions\n  - What does the kronecker product of $F^{in}$ and $F^{out}$ look like after the first stage?\n  - In your experiments, is it possible to directly train the mapping matrices without decomposing them into kronecker products? If so, how does the performance compare with CLIP-Map?\n- Suggestions\n  - The font size in the tables is too small.\n  - The margin between headings and texts is small as well.\n  - I suggest the authors provide the average performance across 21 datasets in Table 2.\n  - The captions of tables should be placed above the tables.\n- Typos\n  - Line 70: using -> use\n  - Line 149: TinyCLIPWu et al. (2023) -> TinyCLIP (Wu et al., 2023)\n  - Line 150: other -> another\n  - Line 278: leading -> leading to\n  - Figure 3: the subscripts of $F^{in}$ and $F^{out}$ seems incorrect (e.g., there are three $F_{2, 3}^{out}$ in $F^{out}$)\n  - Line 420: The bold texts are a paragraph itself, which is inconsistent with previous bold texts that are part of a paragraph.\n  - Line 466: to provide -> provides"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iKFfWd5kGq", "forum": "oiz0QHejVj", "replyto": "oiz0QHejVj", "signatures": ["ICLR.cc/2026/Conference/Submission7305/Reviewer_dmgx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7305/Reviewer_dmgx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972964219, "cdate": 1761972964219, "tmdate": 1762919420427, "mdate": 1762919420427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}