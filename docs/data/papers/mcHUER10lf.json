{"id": "mcHUER10lf", "number": 19100, "cdate": 1758293568157, "mdate": 1759897060677, "content": {"title": "AUPO - Abstracted until proven otherwise: A reward distribution based abstraction algorithm", "abstract": "We introduce a novel, drop-in modification to Monte Carlo Tree Search's (MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an automatic action abstraction algorithm that solely relies on reward distribution statistics acquired during the MCTS. Thus, unlike other automatic abstraction algorithms, AUPO requires neither access to transition probabilities nor does AUPO require a directed acyclic search graph to build its abstraction, allowing AUPO to detect symmetric actions that state-of-the-art frameworks like ASAP struggle with when the resulting symmetric states are far apart in state space. Furthermore, as AUPO only affects the decision policy, it is not mutually exclusive with other abstraction techniques that only affect the tree search.", "tldr": "A detection of approximately value-equivalent state-action pairs by comparing their empirical layerwise reward distributions", "keywords": ["Artificial Intelligence", "Abstractions", "Sequential Decision-making"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/549028974e64dbe40522bcd3a27a43d59d08e7d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper describes an abstraction mechanism based on the reward distributions generated by different actions at the root of MCTS search. Different actions whose reward distributions at different depths are not statistically distinguishable are combined into a single node. A 2 level action selection first selects the abstract node that scores the highest, and then selects an action based on the standard MCTS scoring from the selected abstract node. Empirical results show that AUPO outperforms well on most of the benchmark planning domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a general and simple approach that seems to work well across a range of domains. \nThe paper is well written and the results are thoroughly evaluated. AUPO seems to outperform MCTS in most domains."}, "weaknesses": {"value": "The title \"Abstraction until proven otherwise\" is not very informative. Consider a more informative name for the method and the title, perhaps \"reward-distribution guided abstarctions\" or something like that.  \n\nThere is no comparison to any abstraction methods although several have been discussed, e.g., AS-UCT, OGA-UCT etc. Admittedly some of these methods have stronger requirements such as knowing the transition model and may be inapplicable in some cases. While the comparison may not be totally fair, it is still useful to see what the relative improvements are. \n\nThe authors make the point that their method only works at the root level and hence can be combined with other abstraction techniques that work on the trees. It would be good to see if such combinations are practical or introduce additional complexities.  \n\nThe method is only applicable for dense reward settings.\n\nMinor comments:\nL 035. \"costly retrained\" -> \"expensively retrained\"\nL 048. Fix \"exist that ....exist\" \nL 104. \"lack behind\" -> \"lag behind\"\nL 143. \"agent\" is used instead of policy, possibly because it is stochastic. Agent is an overused term. I prefer policy or stochastic policy. \nL 212. \"R_{d,i} must be R_{d,k}\"\nL 213. \"soft abstractions\" How do you deal with them since each action should go into exactly one cluster?\nL 267.\"(machines 1-2,5-9)\" -> \"(machines 1-2,4-9)\"?\nL 338. \"C \\sigma\" A little more explanation of what role it plays in MCTS will be useful here.\nL 345. \"using the return filter SF\" is repeated twice. \nL 357. Stray comment: \"Do reduce the amount of visuals\" \nL 430. Please give one line explanations of pairings score and relative improvement scores in the text."}, "questions": {"value": "Can you compare the approach to one or two other abstraction techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LbtM5orAUM", "forum": "mcHUER10lf", "replyto": "mcHUER10lf", "signatures": ["ICLR.cc/2026/Conference/Submission19100/Reviewer_5vV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19100/Reviewer_5vV5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588016020, "cdate": 1761588016020, "tmdate": 1762931124024, "mdate": 1762931124024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AUPO, an action abstraction algorithm which can be added to the decision policy of MCTS. The core principle of AUPO is to group actions at the root node where the layerwise statistics of the reward distributions at all layers are similar. The algorithm then selects actions by choosing the highest-value abstract action, then the highest-ground-value within that abstract action. Experiments are conducted on 14 simple environments from IPPC and the planning literature, which show that AUPO commonly outperforms the baselines of normal MCTS and AUPO with random abstractions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The algorithm avoids issues with comparable baselines like needing full transition probabilities or DAGs. It does appear to produce performance increases, although I have some concerns here (see weaknesses). I also like that the algorithm is conceptually simple."}, "weaknesses": {"value": "- I'm concerned about the evaluation practices used to compare AUPO to the baselines. In particular, around \"we considered the best AUPO performance when varying the parameters…\", it sounds like AUPO was optimized over 5+ hyperparameters, and we're taking the maximum, but the baselines may not have had the same treatment. How do I know that the performance gains of AUPO aren't just coming from more tuning?\n- The settings are a bit toy, and I'd be curious to know if AUPO is effective in more complex applications of MCTS, e.g. board games. Perhaps AUPO is not applicable here due to the sparse reward condition.\n- The quality of the writing is a weaker point, some sections are quite dense/verbose.\n- \"These environments were deliberately chosen as they … feature value-equivalent sibling actions, dense rewards, two theoretically necessary requirements for AUPO to yield any performance increase in the first place.\" This makes me concerned that AUPO is not very general. Can the authors say more about what kinds of environments have value-equivalent siblings? I'd like to understand how restrictive this is.\n- The zero padding used when trajectories terminate early is questionable to me – I worry this would skew the reward distributions a lot / prevent good abstractions from being formed."}, "questions": {"value": "- \"Note that this induces a soft-abstraction where it is possible that for three actions (a, b, c), a is grouped with b, b is grouped with c but a is not grouped with c.\" How is action selection handled in this case?\n- I am concerned that the random baseline is too easy of a comparison, but I might be missing something. Can the authors say more about why they did not compare to at least one of the baselines from the literature, such as ASAP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g9S25LFM9c", "forum": "mcHUER10lf", "replyto": "mcHUER10lf", "signatures": ["ICLR.cc/2026/Conference/Submission19100/Reviewer_uAv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19100/Reviewer_uAv6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990124004, "cdate": 1761990124004, "tmdate": 1762931123418, "mdate": 1762931123418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Monte Carlo tree search uses state/action abstraction to reduce search complexity in order to improve the depth of search along promising paths of the search tree. This paper introduces an abstraction algorithm to be used during decision time after search to select the optimal action."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Originality: The algorithm they introduce appears to be using a novel technique."}, "weaknesses": {"value": "This paper lacks any significant contribution. The main contribution the authors tout is an abstraction method to be used during decision time after MCTS has been performed. I do not really understand the motivation for this --- the main reason abstractions are used during search is that the search space can be very large. Abstractions allow one to reduce the search space by aggregating similar states and actions into abstracted states and actions. This reduces the branching factor at the each layer of tree affording one the ability to build a deeper tree for more accurate outcome estimates and share information between similar states and actions. This paper introduces an abstraction algorithm for use *after* search is completed, which makes no sense. When making decisions, one desires more refinement after search, not less.\n\nEven considering this to be used as a selection policy after tree search, the procedure makes no sense. It effectively performs an extra, more-computationally expensive iteration of search giving it unfair advantage over other selection policies. This makes comparisons against other selection policies unfair.\n\nThey also compared their selection policy against a single baseline selection policy: selecting the action with the highest value. I take that to mean they select the action with the highest estimated average. This is only one such selection process among many and one that is not reliably a good selection policy either. It is more common to select the action with the highest sample count or the highest lower confidence value. These policies are less vulnerable to the effects of variance in the outcomes.\n\nI have a few gripes with their evaluation:\n- The authors state the report the results of the parameter settings for AUPO that were the best. That is, it reads as if they ran the algorithm with varying parameter values and reported the results for this parameter sweep. Firstly, did they do that provide this courtesy to MCTS?\n- Secondly, this is improper practice. Once the parameter sweep is done, you have run the algorithm on a separate test set using the best parameters.\n\nAdditionally, the paper is not well-written. There are several lines that make no sense and their are a number of terms and notation used without being defined or explained."}, "questions": {"value": "- Line 85: \"... their transition probabilities to the node groups of the previous layer also lie within a threshold.\" : Surely, the probabilities *from* the previous layer, right?\n\n- Line 178: \"Though consequently the actions $a_1, \\dots, a_k$ are value-equivalent they suffer from an overestimation bias in the decision policy that worsens exponentially with increasing $k$.\" : Are you missing a comma?\n\n- You have to define or explain $\\mathbb{E}$ and $\\mathbb{P}$.\n\n- Trajectories are defined in two different ways.\n\n- Line 266: rebooting should also include computer 4"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "This submissions contains parts that are identical to parts in other submissions."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0fLVrYzvlb", "forum": "mcHUER10lf", "replyto": "mcHUER10lf", "signatures": ["ICLR.cc/2026/Conference/Submission19100/Reviewer_MVsv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19100/Reviewer_MVsv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209545483, "cdate": 1762209545483, "tmdate": 1762931122849, "mdate": 1762931122849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}