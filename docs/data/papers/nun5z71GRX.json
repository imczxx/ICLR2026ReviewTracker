{"id": "nun5z71GRX", "number": 474, "cdate": 1756741647737, "mdate": 1759898258762, "content": {"title": "GPS: General Per–Sample Prompter", "abstract": "LLMs are sensitive to prompting, with task performance often hinging on subtle,\nsometimes imperceptible variations in phrasing. As a result, crafting effective\nprompts manually remains challenging and time-consuming. Recent automatic\nprompting methods mitigate this difficulty but face three key limitations: (i) for\neach new task, they require large datasets to train good prompts; (ii) they rely on\ncostly optimization loops that may take hours; (iii) they typically produce a single\ntask-level prompt that does not adapt to the individual input problem to be solved.\nWe propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen\ninput, improving performance across diverse tasks. The prompter is trained with\nreinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.\nEmpirically, GPS demonstrates competitive performance: we attain second best\nresults among baselines on text simplification, third best results on summarization\nand on-par results on classification, while not training on any of these tasks, in\ncontrast to the baselines. For in-domain prompting, we obtain sota on GSM8K.\nOur work shows the potential of a novel and effective paradigm for automatic\nprompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Code and data will be\nreleased upon acceptance.", "tldr": "We present an automatic prompting method that does not need access to training data for the task at hand, is sample specific and delivers results comparable automatic prompting methods trained on each task.", "keywords": ["Large Language Models", "Prompt Engineering", "Reasoning", "Automatic Prompt Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7c52617c94a4fee3a03e35263aa9bde9e40be5d.pdf", "supplementary_material": "/attachment/6b9d19f70e0e1d900be53969812a893d7dac53f4.zip"}, "replies": [{"content": {"summary": {"value": "General-per-sample prompter is a trained model for generating task- and sample- specific prompts for novel samples and tasks. This improves on prior optimization approaches by dynamically generating sample-specific prompts and generalizing beyond in-distribution training tasks. The model is trained using RL with a novel regularizer intended to prevent reward hacking that might result from the prompter providing the answer directly to the LLM. Experimental results are presented showing the prompter's capability to generalize across tasks, and ablations are performed on the effects of varying regularization, decoding, and per-sample prompting methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "GPS adapts to the task and generates a unique prompt for each sample.\nGPS aims to generalize to unseen tasks, making it more versatile.\nA novel regularization approach prevents reward hacking, which could generalize to other RL settings.\nThe prompter is decoupled from the evaluator, enabling a smaller model to boost the performance of a large model."}, "weaknesses": {"value": "In general the experimental results show that the method is competitive but PRL seems to be a stronger method overall- the authors should discuss the relative advantages of their approach compared to PRL. Was PRL evaluated on unseen tasks?\nTraining tasks are largely reasoning tasks while test tasks are mainly classical NLP tasks, underscoring the need for ablations to understand whether GPS generalizes well to more complex tasks. The GSM8K results are encouraging but the absence of a holdout reasoning task is a liability."}, "questions": {"value": "How did you choose the tasks you trained on? Did you perform any cross-validation by mixing the training tasks?\n\nExplain the distinction from PRL and the benefits? Is PRL trained directly on the test tasks, explaining the superior performance? \n\nIs it necessary to calibrate r_{alignment} so that task rewards are balanced?\n\nHave you performed any ablation over N in the decoding phase?  Compare with a baseline best-of-N prompting scheme?\n\nIt took me some time to figure out that GPS-J and GPS-SR-0.1 correspond to the two regularization methods- be sure to indicate this clearly in section 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PrrrTCUn1i", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Reviewer_3EL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Reviewer_3EL1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794638477, "cdate": 1761794638477, "tmdate": 1762915527403, "mdate": 1762915527403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments (4/4)"}, "comment": {"value": "References:\n\n[1] N. Lambert et al., “Tülu 3: Pushing frontiers in open language model post-training,” arXiv:2411.15124, 2024.\n\n[2] Muennighoff, Niklas, et al. \"s1: Simple test-time scaling.\" Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.\n\n[3] Lightman, Hunter, et al. \"Let's verify step by step.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[4] Shao, Zhihong, et al. \"Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\" arXiv preprint arXiv:2402.03300 (2024).\n\n[5] Guo, Daya, et al. \"Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\" arXiv preprint arXiv:2501.12948 (2025)."}}, "id": "ApbAsf6lk2", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763668950087, "cdate": 1763668950087, "tmdate": 1763668950087, "mdate": 1763668950087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GPS (General Per-Sample Prompter), a reinforcement-learning–trained prompt generator that, at inference, produces a unique prompt per input for an unseen task. Two regularization schemes (an LLM “Judge” that penalizes answer leakage and a sample-swap scheme) aim to prevent the generator from embedding solutions in the prompt. Inference uses Minimum Bayes Risk (MBR) selection over multiple candidate prompts/evaluator outputs. Trained on math/logic/programming tasks, GPS is evaluated out-of-domain on summarization (SAMSum), simplification (ASSET), and classification (SST-2/-5, MR, CR, AG News, TREC, SUBJ). Results show 2nd–3rd best performance on summarization/simplification and competitive classification averages, plus strong in-domain GSM8K, with ablations for regularization, MBR, per-sample prompting, backbone (Qwen vs Llama), and evaluator scale."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem setup and the paper is well-motivated.\n- Method simplicity and plausibility: two-LLM setup (generator/evaluator), GRPO optimization, and explicit reward decomposition (format/structure/accuracy). \n- Broad evaluation and ablations: out-of-domain generalization (trained on reasoning; tested on summarization/simplification/classification), backbone swap (Qwen→Llama), effect of regularization probability, per-sample vs no-per-sample, and MBR on/off."}, "weaknesses": {"value": "- Effectiveness:\n   - Underperforms PRL on most tasks. The central claim is that a task-agnostic, per-sample prompter should be at least competitive with task-specific prompt RL (PRL). Yet across the main tables (summarization, simplification, multi-task classification), GPS typically lags PRL. This weakens the paper’s core effectiveness claim.\n   - Ask for a controlled, cost-normalized head-to-head. Please report a training budget comparison against PRL, including GPU hours, latency and token cost. \n\n- Novelty / Positioning\n\n   - “First general-purpose per-sample prompter” is not well established. Instance-level prompt optimization is not new, and per-sample demonstration selection (retrieval-augmented ICL) is widely studied. The paper should more clearly delineate how GPS differs from (i) instance-dependent prompt tuning/selection and (ii) retrieval-based ICL that adapts the context per input.\n\n   - Some other works generate per-instance keywords such as \"https://arxiv.org/abs/2302.11520\" is also considerred as per-sample PO.\n\n- Missing Baselines:\n\n  - Include some demonstration-selection baselines, since they are also per-sample strategies.\n\n- Additional Evidence that would Strengthen the Paper:\n\n   - What do optimized prompts look like?\n\n   - Judge validation: How reliable is your Judge?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqmefTQCTV", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Reviewer_TVfW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Reviewer_TVfW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948439714, "cdate": 1761948439714, "tmdate": 1762915527259, "mdate": 1762915527259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments (3/4)"}, "comment": {"value": "**2. Choice of mathematical, coding and commonsense reasoning tasks.**\n\nRegarding the choice of meta-tasks (math, logical reasoning and coding), we intentionally train GPS on verifiable reasoning problems, rather than directly on summarization or classification data, for two reasons. First, these domains naturally provide binary or programmatic rewards (exact numeric answer, multiple-choice correctness, unit tests), which are crucial for stable RL/RLVR-style training. This design is consistent with recent work on “reasoning models,” where reinforcement learning or RL with verifiable rewards on math / logic / coding data leads to broad gains on downstream benchmarks beyond the exact training distribution [1–4]. For example, Tülu 3 [1] applies RLVR on a mixture of verifiable tasks (GSM8K, MATH, and instruction-following data such as IFeval) and reports consistent improvements across diverse benchmarks such as MMLU, GSM8K and HumanEval, even when evaluation tasks differ from the training. Similarly, Muennighoff et al. [2] show in s1: Simple test-time scaling that optimizing on roughly one thousand carefully curated, hard reasoning problems is enough to obtain a model whose test-time–scaled policy transfers across multiple reasoning benchmarks (MATH500, AIME24, GPQA Diamond), outperforming much larger baselines under comparable compute. Work on verifier-based training for math (e.g., Let’s Verify Step by Step) further demonstrates that using verifiable, step-level feedback for multi-step mathematical reasoning significantly improves performance on challenging problems in the MATH benchmark compared to outcome-only supervision [3]. Finally, recent reasoning-focused RL models such as DeepSeekMath and DeepSeek-R1 are trained heavily on math, yet show strong results on a suite of heterogeneous reasoning benchmarks, including competition math, GSM8K, MATH-500, Math Odyssey, LiveCodeBench, and general knowledge benchmarks such as MMLU and GPQA [4,5]. Our setup is in the same spirit: by training GPS on difficult, verifiable reasoning and coding tasks, we encourage it to learn general prompt-construction and reasoning patterns that transfer to unseen NLP tasks (summarization, simplification, classification), even though those tasks never appear during training.\n\nWe agree that, in principle, performance will depend on the choice of meta-tasks and reward design; this is also emphasized in recent work on RL with verifiable rewards and process-supervised feedback [1,3]. Exploring larger and more diverse meta-task collections is therefore an interesting extension, but our current results already show that a reasoning-only training mixture can yield non-trivial gains on out-of-domain NLP benchmarks."}}, "id": "PMtFg61Rip", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669011268, "cdate": 1763669011268, "tmdate": 1763669011268, "mdate": 1763669011268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework called GPS, to customize the prompt for the inputs to the large language models (LLMs). The customization focuses on the instance-level and is done by a general prompt generator without finetuning on any data from the targeted downstream tasks. More specifically, a general prompt generator is trained via GRPO on the prompt generation task with the data from mathematical, logical, and programming tasks. Two regularization methods are developed to discourage the generator from inserting the answer to the input into the prompt template and hack the rewards. Experimental results on several NLP tasks show that the proposed method can achieve comparable performance with the existing prompt customization methods finetuned on the downstream tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper are listed as follows.\n1. The paper is well motivated. The prompt design can affect the performance of the LLM and a general-purpose generator can reduce the human effort in prompt design significantly.\n2. The two regularization methods are reasonable and reflect deep insights from the authors in the prompt customization task.\n3. It is great to see that the authors provided a detailed ablation analysis in their methods, providing more insights into their method."}, "weaknesses": {"value": "The weaknesses of the paper are listed as follows.\n1. The experimental results are not convincing enough. I illustrated this point as follows:     \na. The evaluated downstream tasks are relatively naïve. More complicated tasks should be evaluated to show the effectiveness of GPS such as commonsense reasoning, information extraction and long-context understanding.     \nb. The performance of in-domain performance in the math, logical reasoning and coding is also important to understand whether GPS is underfitting to the meta-tasks.       \nc. It is not clear why Table 1 and Table 2 did not include the baseline named NI for the comparison. Besides, according to Table 3, the training-based GPS does not show a clear advantage over the NI, a baseline that designs the prompt by humans.    \n2. The writing of the paper can be improved to make it easier to follow. For example, the contribution can be summarized into three key points, focusing on technical novelty, methodology, and evaluation. Secondly, since the training pipeline is based on PRL, the authors should highlight the difference between GPS and PRL in the related work section. Thirdly, in the method section, it would be better if the authors could formulate the prompt generation problem as an optimization problem to clearly define the objective, constraints, the learnable parameters, input and output to the problem.    \n3. According to the proposed training pipeline, the general-purpose prompt generator is not model-agnostic and thus limits its impact. For example, the generated prompt may be overfit to the model family of the generator. So when the target LLM is not in the same family as the generator, the generated prompts may be suboptimal. The experiments do not provide enough evidence to address this concern since the authors did not compare with the existing baselines in their ablation study on cross-model performance.\n4. Some important technical details are not clear. I raised the questions in the Questions section of the review."}, "questions": {"value": "My questions are listed as follows.\n1. What is the average prompt length GPS generated for each evaluated task? Will it generate a much longer prompt than the baselines and incur much higher latency for online inference?\n2. What is the latency for a single sample inference with GPS? Will the introduction of an LLM-based prompt generator cause heavy overhead on the latency?\n3. Could you provide more explanation about why picking math, logical reasoning and coding tasks as the meta-tasks to train the GPS? Why not the other tasks? Is the performance of GPS sensitive to the choice of meta-tasks and the relevant training dataset?\n4. What are the differences between GPS and IPT or IDPG mentioned in the related work? Why not compare GPS with them in the experiments?\n5. For equation (1), is there any scaling factor for each reward term?\n6. In the proposed GRPO training, how to define a group when calculating the group average advantage?\n7. In the proposed GRPO training, what is the decoding strategy of prompt generator and evaluator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "txQR9UydH0", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Reviewer_hbtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Reviewer_hbtD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962146005, "cdate": 1761962146005, "tmdate": 1762915527152, "mdate": 1762915527152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments (2/4)"}, "comment": {"value": "References:\n\n[1] W. Shi, Y. Chen, S. Bian, X. Zhang, K. Tang, P. Hu, Z. Zhao, W. Lu, X. Du. No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization. NeurIPS, 2025.\n\n[2] He, Zhiwei, et al. \"Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning.\" arXiv preprint arXiv:2504.11456 (2025).\n\n[3] Lightman, Hunter, et al. \"Let's verify step by step.\" The Twelfth International Conference on Learning Representations. 2023.\n\n[4] Yang, Hang, et al. \"Llm-medqa: Enhancing medical question answering through case studies in large language models.\" arXiv preprint arXiv:2501.05464 (2024)."}}, "id": "w1Jktm52It", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669038729, "cdate": 1763669038729, "tmdate": 1763669038729, "mdate": 1763669038729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GPS, a general-purpose, per-sample prompting framework. GPS generates unique prompts for each input without task-specific fine-tuning. It is trained via RLVR across diverse reasoning and programming tasks, with two regularization techniques (judge and sample regularization) to prevent label leakage. The model demonstrates strong out-of-domain generalization on summarization, simplification, and classification, and on GSM8K."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of per-sample prompting is quite important and well-motivated. \n- Method is evaluated across diverse domains—reasoning, summarization, simplification, and classification—with thorough ablations on regularization, decoding (MBR), and per-sample prompting."}, "weaknesses": {"value": "- Given that the prompt generator model is RL-tuned. A fair comparison would be to compare to fully finetune the model on these tasks, with the same utility reward. Would that approach directly outperform the indirect optimization over the prompts? and what would the generalization performance be in that setting?\n- I'd want to see a comparison with more recent prompt optimization approaches like in reflective prompt optimization GEPA."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3p630FInIV", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Reviewer_gdUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Reviewer_gdUi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140365409, "cdate": 1762140365409, "tmdate": 1762915526999, "mdate": 1762915526999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments (1/4)"}, "comment": {"value": "**1. Comparison with GRACE and additional benchmarks:**\n\nWe thank the reviewers for suggesting comparison with more recent methods and benchmarks. Below, we compare GPS with GRACE [1] (NeurIPS 2025) and extend our evaluation to several additional tasks.\n\nFor a fair comparison, we reran GRACE using Qwen2.5-7B-Instruct as both prompt generator and evaluator, instead of the stronger models used in the original paper (DeepSeek-R1 for prompt generation and DeepSeek-V2-0324 for evaluation). We evaluate GRACE and GPS on all benchmarks already included in the paper, as well as on several new benchmarks added specifically for this rebuttal. The GRACE numbers reported here are lower than those in the original GRACE paper, which reflects the difference in underlying model strength rather than the prompt optimization method itself.\n\nWe also evaluate GPS on three additional benchmarks:\n\n- MATH500 [3] and DeepMath [2] are large-scale, challenging benchmarks for mathematical reasoning.\n\n  - MATH500 is out-of-distribution for GPS. It was not used for training.\n\n   - DeepMath is in-domain, samples from this dataset were used during training. Training samples do not overlap with test data.\n\n- MedQA [4] is a large-scale, multiple-choice medical QA benchmark based on professional board-exam-style questions. MedQA is also in-domain. We evaluate on the test split not seen during training.\n\nBelow we summarize the results.\n1. Mathematical Reasoning tasks (DeepMath, Math500, GSM8K):\n\n|MATH500| |\n|-|-|\n\n|Method| Accuracy|\n|-|-|\n|APE|31.53±1.04|\n|DE|34.20±1.39|\n|GA|40.13±1.39|\n|GRACE|33.20±1.60|\n|PRL|44.40±1.40|\n|**GPS-SR-0.1**|**31.80±1.60**|\n|**GPS-J**|**34.20±0.80**|\n\n|DeepMath| |\n|-|-|\n\n|Method| Accuracy|\n|-|-|\n|APE|15.47±0.45|\n|GA|18.63±2.37|\n|DE|16.10±0.00|\n|GRACE|15.05±0.16|\n|PRL|21.58±0.22|\n|**GPS-SR-0.1**|**20.98±0.08**|\n|**GPS-J**|**21.40±0.25**|\n\nDeepMath results above differ slightly from Figure 3 in the paper because we use a smaller maximum generation length (i.e., a lower token budget) for all methods.\n\n\n|GSM8K||\n|-|-|\n\n|Method|Accuracy|\n|-|-|\n|MI|78.20|\n|APE|83.43±1.98|\n|GA|81.62±1.38|\n|DE|79.52±0.45|\n|GRACE| 82.37±1.82|\n|PRL| 86.15±0.55|\n|**GPS-SR-0.1**|**87.55±0.42**|\n|**GPS-J**|**84.45±0.93**|\n\n  2. MedQA (domain base task):\n\n|Method|Accuracy|\n|-|-|\n|APE|45.66±0.97|\n|GA|51.95±1.61|\n|DE|51.76±0.16|\n|GRACE|52.26±0.16|\n|PRL|53.34±0.11|\n|**GPS-J**|**54.92±0.14**|\n|**GPS-SR-0.1**|**53.31±1.47**|\n\n  3. Classification tasks:\n\n|Method|CR|MR|SST-5|AG's News|SST-2|TREC|Subj|Avg|\n|-|-|-|-|-|-|-|-|-|\n|MI|87.25|87.40|52.31|82.29|92.70|69.20|57.95|75.59|\n|NI|91.50|90.85|51.90|83.43|95.77|66.60|68.10|78.31|\n| APO    | 93.48 ± 0.24 | 89.97 ± 1.37 | 53.94 ± 0.29 | 83.73 ± 0.31 | 93.71 ± 0.25 | 71.30 ± 1.90 | 69.80 ± 5.96 | 79.42 |\n| APE    | 92.87 ± 0.02 | 89.90 ± 0.94 | 49.37 ± 5.66 | 82.58 ± 1.20 | 91.23 ± 0.66 | 77.07 ± 1.61 | 73.92 ± 1.39 | 79.56 |\n| GA     | 92.75 ± 0.40 | 90.45 ± 0.72 | 53.76 ± 1.13 | 82.24 ± 1.00 | 94.65 ± 1.04 | 79.20 ± 2.83 | 74.93 ± 3.12 | 81.14 |\n| DE     | 93.38 ± 0.19 | 89.98 ± 0.24 | 55.25 ± 0.37 | 82.18 ± 1.04 | 93.29 ± 0.34 | 76.47 ± 0.38 | 73.08 ± 4.95 | 80.52 |\n| GRACE  | 90.92 ± 1.15 | 89.60 ± 1.51 | 53.96 ± 0.93 | 82.34 ± 0.39 | 93.61 ± 0.53 | 72.53 ± 8.62 | 73.92 ± 3.05 | 79.55 |\n| PRL    | 92.83 ± 0.24 | 91.27 ± 0.05 | 56.21 ± 0.15 | 84.36 ± 0.08 | 96.32 ± 0.04 | 77.07 ± 2.36 | 76.90 ± 0.95 | 82.14 |\n| **GPS-SR-0.1** | **90.50 ± 0.38** | **88.70 ± 0.05** | **55.14 ± 1.13** | **84.21 ± 0.34** | **92.98 ± 0.19** | **68.20 ± 0.20** | **65.10 ± 0.28** | **77.83** |\n| **GPS-J** | **90.65 ± 0.05** | **89.15 ± 0.38** | **55.16 ± 0.36** | **84.04 ± 0.02** | **94.25 ± 1.20** | **72.80 ± 0.60** | **64.20 ± 2.25** | **78.61** |\n\n\n  4. Summarization task:\n\n|Method|ROUGE-1|ROUGE-2|ROUGE-L|\n|-|-|-|-|\n|MI|32.76|10.39|28.97|\n|APE|37.12±2.02|12.97±0.74|33.32±1.68|\n|GA|39.69±1.76|14.47±1.00|35.84±1.63|\n|DE|33.91±4.04|12.53±1.47|31.05±3.79|\n|GRACE|40.61±0.54|14.65±0.53|35.86±0.54|\n|PRL|42.47±0.83|16.17±0.24|37.73±0.36|\n|**GPS-SR-0.1**|**40.03±0.11**|**14.36±0.13**|**35.91±0.19**|\n|**GPS-J**|**38.08±0.74**|**13.07±0.44**|**34.09±0.61**|\n\n  5. Simplification task:\n\n|Method|SARI|\n|-|-|\n|MI| 43.77|\n|APE| 45.33±0.83|\n|GA| 46.25±0.47|\n|DE| 45.79±0.35|\n|GRACE| 50.21±0.18|\n|PRL| 52.26±3.51|\n|**GPS-SR-0.1**|**52.09±0.22**|\n|**GPS-J**|**48.10±0.66**|\n\nWe want to stress again that we do not claim that our method will be able to outperform prompt generators that train from scratch on each new task they are tested on, as already evidenced by the comparison against PRL and EvoPrompt. This would be highly surprising, since GPS has only been trained out of distribution. What our work shows is that we can get significant prompt improvements by a general prompt generator that is pre-trained once and then functions for new tasks without any new training. Comparisons against existing prompt generator methods are certainly informative, but are ultimately not comparable in that our method does not need training for new tasks."}}, "id": "2BRpbWA22Q", "forum": "nun5z71GRX", "replyto": "nun5z71GRX", "signatures": ["ICLR.cc/2026/Conference/Submission474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission474/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission474/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669115748, "cdate": 1763669115748, "tmdate": 1763669115748, "mdate": 1763669115748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}