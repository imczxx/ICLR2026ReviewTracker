{"id": "O4Oy7NsSwG", "number": 4772, "cdate": 1757763541444, "mdate": 1759898014330, "content": {"title": "Topology and geometry of the learning space of ReLU networks: connectivity and singularities", "abstract": "Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.", "tldr": "", "keywords": ["learning dynamics", "topology", "neural networks", "ReLU networks", "geometry", "symmetry", "loss landscape", "gradient", "singularity", "connectedness"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8fd131677d720bc81f15ed0002e438f4cc9a201e.pdf", "supplementary_material": "/attachment/6aff6d52254a623419256a14dc28a70e3c2832a3.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents their study on the function spaces parameterized by polynomial neural networks (i.e., those whose activation functions are polynomial). There are two main contributions: identifiability and singularity of functions in the neuromanifold (i.e., functions representable by neural networks). For the former, the authors show that for generic functions in neuromanifold, the set of parameters realizing these functions is at most finitely many or singleton, for Multi-Layer Perceptrons (MLP) and Convolutional Neural Networks (CNN) architectures respectively. For the latter, they characterize singularities as functions realized by sparse subnetworks and links this discovery to the sparsity bias of MLPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper are generally well-written and the results are well-presented. While I do not dive into the proof, their results look sound to me. Two contributions are mathematically interesting and suggest further following work."}, "weaknesses": {"value": "Several points deserves to be further polished:\n1. Since most architectures use ReLU, I find that it is better to connect the current results to the ReLU cases (authors did admit this limitation in section 5).\n2. The bound on the degree of the activation in Theorem 4.1 is vacuous in the dimensions of the neural network architecture. Hence, I am not sure if this result reflects what we truly observe in practice.\n3. If I understand it correctly, the definition of critically exposed implies that there exists a positive probability that mappings $u$ admit a weight in a critically exposed set as critical points of the training dynamics (provided that we have sufficiently data). However, since we are unable to quantify this probability, they might be negligible and might vanish when dimension increases. I am not sure if we can use this notion to explain the so-called ``bias towards sparse subnetworks'' as in the paper."}, "questions": {"value": "1. In section 3.2, the link between optimization on the parameter space and on the neuromanifold is rather hand-waving. I wonder if there is a real relation between these two (under suitable conditions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4bYzYMcFjn", "forum": "O4Oy7NsSwG", "replyto": "O4Oy7NsSwG", "signatures": ["ICLR.cc/2026/Conference/Submission4772/Reviewer_aTae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4772/Reviewer_aTae"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734599990, "cdate": 1761734599990, "tmdate": 1762917563717, "mdate": 1762917563717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the (dis)connectedness and the existence of singularities in the parameter space of ReLU neural networks whose architectures admit a DAG computational graph representation. The authors then leverage these results to study whether these elements can exert impact on the dynamics of gradient flow (GF) of standard training algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I generally like the paper. Here are several remarkable points:\n1. The conservative law for ReLU networks in the DAG case is very elegant.\n2. Theorem 1 makes a very nice connection to flow problem in graph theory.\n3. Theorem 2 - Proposition 5 to 7 provide a very clean picture on the dynamics of gradient flow in the existence of singularities"}, "weaknesses": {"value": "1. I have a hard time distinguishing what are the main contributions and what are already proved in the literature. Authors might want to re-organize the section 2, and credit properly all the results (theorems, propositions, definitions) if they are ever taken/inspired by previous works.\n2. Do the author forget to define the notion of stable by forward/backward edges in the announcement of Theorem 1? Otherwise, I believe that Theorem 1 needs rephrasing to be easier to understand."}, "questions": {"value": "1. Do Proposition 6 - 7 imply that singularities are truly rare? It seems to me that the limit of GF can still be a singularity (or a sparse subnetwork). If the GF dynamics does not bias towards sparse subnetworks, do you have any idea which points are preferable for the convergence of GF?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4bYzYMcFjn", "forum": "O4Oy7NsSwG", "replyto": "O4Oy7NsSwG", "signatures": ["ICLR.cc/2026/Conference/Submission4772/Reviewer_aTae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4772/Reviewer_aTae"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734599990, "cdate": 1761734599990, "tmdate": 1763044557752, "mdate": 1763044557752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the feed-forward ReLU networks defined over directed acyclic graphs, examining the (dis)connectedness of the parameter space and the existence of singularities within it. The conservation laws under gradient flow are identified. Due to the disconnectedness of certain parameter configurations, certain singularities are unreachable, reducing the expressivity of ReLU networks at initialization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is relatively well-written and polished. Illustrative figures are provided to accompany the theoretical results and aid understanding.\n- The theoretical formulation is clean.\n- The result on the disconnectedness of the parameter space is somewhat surprising. The implication of losing expressivity at initialization seems interesting.\n- Some numerical experiments are conducted to validate theoretical results."}, "weaknesses": {"value": "- I am wondering whether the disconnected case occurs in fully-connected ReLU networks or not, since the example network given in Figure 2(d.1) does not look like a fully-connected network. If the disconnection only occurs in networks that are not fully connected, then the statement in line 358 may be inaccurate: \"the expressivity can be reduced to the extent that they lose their universal approximation capability\"; because ReLU networks that are not fully connected are not universal approximators to begin with. Please feel free to correct me if I have misunderstood your results.\n- In line 423, the authors state that: \"given a random initialization, the probability of $\\mathcal H_G(c)$ having singularities is zero.\" I trust that this statement itself is correct. However, it doesn't necessarily mean that the gradient flow/descent cannot go near singularities. It's quite common that ReLU networks can have saddle-to-saddle dynamics, in which the gradient flow path passes near a sequence of fixed points [1]. In those cases, even though the dynamics from random initialization never puts the parameters exactly in an invariant set, going near those fixed point is still a very prominent, if not the most prominent, trait of the learning dynamics. If I didn't misunderstand the result, the paragraph \"singularities are rare\" should probably come with more nuance or caveat -- \"probability of having singularities being zero\" doesn't mean that learning dynamics doesn't go near singularities.\n- The conservation laws arising from symmetries are also studied in [2]. I am wondering how their results relate to yours results in \"local conservation laws under gradient flow\" in line 160.\n- It might be useful to also discuss the limitation of studying gradient flow in place of SGD. Because the quantities that obey conservation laws under gradient flow can actually be time-varying in SGD [3,4].\n\n[1] Boursier et al. \"Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs.\" NeurIPS 2022.\n\n[2] Ziyin. \"Symmetry induces structure and constraint of learning.\" ICML 2024.\n\n[3] Liu et al. \"Noise and fluctuation of finite learning rate stochastic gradient descent.\" ICML 2021.\n\n[4] Chen et al. \"Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks.\" NeurIPS 2023."}, "questions": {"value": "Is there a particular reason to use the uncommon notation of double angle brackets $《》$? I struggled to understand it from a short inline definition given in line 177. I also didn't know if this notation is essential for reading and understanding the main results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L6BjbXTkBb", "forum": "O4Oy7NsSwG", "replyto": "O4Oy7NsSwG", "signatures": ["ICLR.cc/2026/Conference/Submission4772/Reviewer_Fprd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4772/Reviewer_Fprd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833588265, "cdate": 1761833588265, "tmdate": 1762917563439, "mdate": 1762917563439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the properties of the parameter space of ReLU networks, notably in order to decide whether this space is connected and/or contains singularities, which are relevant questions to consider when targeting an optimally trained network, or to prune the network without losing performance/expressiveness, respectively.\n\nThe authors consider the framework of Directed Acyclic Graphs (DAGs), which is more general than layered architectures,and focus in this paper on properties of homogeneous activation functions, in particular here the ReLU activation function.\n\nThey show a complete characterization of the connectedness of the learning space under GF with any given initialization, analysing to this end the role of bottleneck vertices in the network (that is, vertices with only one out-going arc, or only one in-coming arc) and the balance conditions (invariant under GF once the initialization is done) on related sets of vertices.\n\nMoreover, the authors study singularities, namely parts of the learning space where part of the network stops contributing to the computation. They prove a link between the existence of such singularities and the already mentioned balance conditions, and that even when the conditions are gathered, a GF algorithm will not stumble upon a singularity in finite time. \nThe authors circumvent this impossibility to favor \"self-pruning\" by using regularization, and provide numerical experiments showing which regularization helps driving the model towards singularities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Provides a sound and thorough theoretical analysis of the connectivity of learning space for ReLU-activated DAGs Networks trained under GF after arbitrary initialization.\n\nTheoretical analysis of the conditions of existence of singularities, and of the possibility to reach them, complemented with experimental results on tools to reach these singularities in practice."}, "weaknesses": {"value": "The results on connectivity might be achievable with simpler tools and less technicality.\n\nThe experimental part on connectivity does not bring anything to the discussion. \n\nThe introduction of some notions and symbols is lacking."}, "questions": {"value": "p3, discussing on re-scaling: Do you assume here and in the rest of the paper that all biases are 0?\n\np4, top of the page: do you have any other requirement on $\\\\ell$, other than it being differentiable? For instance $\\\\ell(x,x)=0$ ?\n\np5, Definition 1: $\\\\theta^2$ is the vector obtained from $\\\\theta$ by squaring each individual element? Or do you here implicitly use some other product?\n\np5, Proposition 3: the point of view of network flows can be obtained in a simpler way as what is done in Appendix A.3. Indeed, since the source and sinks have unconstrained flows, it would suffice to initialize all edge weights with 1, and then correct the balance for each node $u$ with a simple edition of the weights along a path from an arbitrary source to an arbitrary sink going through $u$. Does the algebraic point of view give, in some way, more insight for this paper?\n\np6, Theorem 1: I think the proof could take a shortcut (following the idea of the precedent remark, and the intuition-providing  text at the beginning of page 7: first prove that if the conditions are not satisfied, it is unfeasible to satisfy the responsible set of vertices, and if it is, show that fixing first the edge weights incident to $Anc(v)$ (or $Desc(v)$) to satisfy the local constraints, and then construct the rest of the solution greedily without editing any edge incident to $Anc(v)$ (which is then possible by definition of this set, anything outside is on at least one path from source to sink avoiding $Anc(v)$). \nFor the trivial case where the deleted $e$ does not correspond to a bottleneck, the result is immediate.\nThen, the Proposition 4 directly yields the result.\nIs there a reason for taking the long and more technical way?\n\np6, Figure 2d: the figure is a good illustration of the proved theorem. I don't understand however, what the additional experiments on real data (Appendix A.9.1) bring to the paper, since it needs no further empirical demonstration that the space is disconnected. As I am less acquainted with the experimental side, could you indicate what I am missing here?\n\np8, Proposition 6: is the converse known to be true/false? \n\np9: When and why is self-pruning interesting to have? I understand why one wants to self-prune when the initialization was made such that some singularity exist, but is there an advantage in how expressive the network can be when initialized with a reachable singularity, versus when initialized such that none can be attained by GF?\n\n\nTypos and Suggestions:\n\np3, Symmetries of ReLU networks: $\\\\sigma$ is not introduced, which could be done by adding \"the activation function\" in front.  Moreover, the formulae for ReLU and Leaky ReLU are both wrong: ReLU: $\\\\sigma(z)=\\\\max\\\\{z, 0\\\\}$ and Leaky ReLU: $\\\\sigma(z)=\\\\max\\\\{z, \\\\gamma z\\\\}$.\n\np3, Local conservation laws under gradient flow: the variables $d$ and $e$ are clear from context but should be defined nonetheless. \n\np6, Definition 2: prefer the use of \"...with $V^-_B$, $V^+_B$ the sets..., respectively.\"\n\np6, Figure 2c: (ii) this case is misleading, since it is not obvious without the text that the case (iii) can \"override\" it. It also is technically not true, since there could be a completely independent vertex $v'$ in the network making the space disconnected. Maybe find an alternate formulation meaning roughly \"a priori connected\".\n\np7, Corollary 2: unless some intermediate layer has a single neuron! It might not be an interesting case, but the soundness of the corollary requires excluding it.\n\np7: \"Concretely, it means that the balance condition ... will forbid sign switches ...\" This is the important intuition behind this section, it would be valuable to highlight it more, and potentially to merge it with the previous paragraph. \n\n\n\nI am open to updating my rating of the paper, depending on the answers provided by the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8LyWSQ16cV", "forum": "O4Oy7NsSwG", "replyto": "O4Oy7NsSwG", "signatures": ["ICLR.cc/2026/Conference/Submission4772/Reviewer_j6Nf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4772/Reviewer_j6Nf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926511526, "cdate": 1761926511526, "tmdate": 1762917563091, "mdate": 1762917563091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies invariant sets for gradient flow training of DAG-based ReLU architectures and singularities within those invariant sets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very well written and provides some insights on properties of the training dynamics."}, "weaknesses": {"value": "To me the results seem to be relatively minor and easy extensions of previous results. The authors suggest that formulating these conservation laws with the use of the incidence matrix of the DAG gives significant new insight. But as far as I can see, the main insight is that there a singularities when parts of the graph become disconnected, which does not seem to be surprising."}, "questions": {"value": "On one hand, singularities could be a concern, because they cannot be escaped once reached. On the other hand you suggest that they may be desirable in the sense that they can be seen as the model performing some automatic pruning during training (and indeed you suggest that one may want to induce singularities intentionally). May question is whether there could not be a worry that inducing singularities prematurely limits the model and prevents it from later converging to more favorable solutions which require all neurons (or at least some of the prematurely pruned ones)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QDl0LSwCbp", "forum": "O4Oy7NsSwG", "replyto": "O4Oy7NsSwG", "signatures": ["ICLR.cc/2026/Conference/Submission4772/Reviewer_pzFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4772/Reviewer_pzFJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000237969, "cdate": 1762000237969, "tmdate": 1762917562657, "mdate": 1762917562657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}