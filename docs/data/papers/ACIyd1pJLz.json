{"id": "ACIyd1pJLz", "number": 7177, "cdate": 1758010714175, "mdate": 1762919599069, "content": {"title": "Sieve Attention: Fusing Context-Aware Filtering and Sequential Allocation For Long Sequence", "abstract": "Transformer-based language models struggle with long-context generalization, a problem often rooted in their attention mechanisms. Existing solutions often face a trade-off: sparse attention mechanisms excel at identifying globally relevant content but are permutation-invariant and rely on brittle positional encodings, while sequential mechanisms are inherently order-aware but can be 'short-sighted,' failing to attend to distant yet crucial information. To resolve this dichotomy, we propose Sieve Attention, a novel, two-stage attention mechanism that unifies content-based filtering with sequential allocation. Sieve Attention first employs α-entmax to 'sieve' the entire context, selecting a small candidate set of content-relevant tokens. Subsequently, it applies a sequential, stick-breaking process exclusively on this pre-filtered set to allocate attention with an intrinsic recency bias, thereby eliminating the need for external positional encodings. We theoretically prove that this design allows Sieve Attention to overcome the mutual limitations of its predecessors, demonstrating both immunity to local distractors and inherent order-sensitivity. Extensive experiments on long-context language modeling and retrieval benchmarks show that Sieve Attention significantly outperforms established baselines in length extrapolation and in-context learning. Our work presents a new path toward building more robust long-context models by holistically integrating global content analysis and local sequential reasoning directly within the attention mechanism.", "tldr": "", "keywords": ["sparse attention", "long-context", "long-context generalization", "long-context extrapolation", "length generalization", "length extrapolation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7ef6640a658739a2b1e5f0e8f1e61ad6b730446f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Sieve Attention, a new Transformer attention mechanism for handling long sequences. It combines content-based filtering using α-entmax with a sequential, recency-biased allocation step, removing the need for positional encodings. This design lets models focus on relevant information while preserving order sensitivity. Experiments show Sieve Attention outperforms existing methods in long-context modeling."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written."}, "weaknesses": {"value": "1. Limited Novelty: The idea to \"first select, then attend\" is not new for sparse attention. The authors can revisit the classic Reformer [1] paper, which first select the most relevant contents with hash projection, and then do recency-biased attention with sliding window. What's more, the time complexity of Reformer is strictly sub-quadratic with respect to the sequence length, which is better than the authors' sieve attention for long context;\n\n2. Safety Concern: The motivation to have recency bias in attention does not align with some of the most important LLM capabilities, e.g. instruction following. For example, instructions on safety should never be forgotten throughout the generation process. However, if we apply the authors' sieve attention, owing to its recency-biased nature, it might be vulnerable to attack strategies like injecting misleading information in the middle of the context.\n\n3. Limited Evaluation: The current evaluation largely focuses on synthetic tasks. The authors are expected to evaluate and report their results across more settings, e.g.\n\n    - Full benchmark results of Ruler across 13 subtasks;\n    - At least one real-world long-context benchmark (e.g. LongBench [2]), since the context length in Table 2 tasks are not very long.\n\n[1]. Reformer: The Efficient Transformer. ICLR 2020.\n[2]. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. ACL 2024."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UUWZ9yD8oR", "forum": "ACIyd1pJLz", "replyto": "ACIyd1pJLz", "signatures": ["ICLR.cc/2026/Conference/Submission7177/Reviewer_FTWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7177/Reviewer_FTWn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760845282495, "cdate": 1760845282495, "tmdate": 1762919338030, "mdate": 1762919338030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "NirLQUPQCg", "forum": "ACIyd1pJLz", "replyto": "ACIyd1pJLz", "signatures": ["ICLR.cc/2026/Conference/Submission7177/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7177/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762919597926, "cdate": 1762919597926, "tmdate": 1762919597926, "mdate": 1762919597926, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a sparse attention mechanism that combines α-antmax and stick-breaking attention. The main idea is to use α-antmax to compute attention scores, then select the top-k tokens to achieve sparsity, and finally apply stick-breaking attention to these tokens. Experimental results show that this method explicitly outperforms traditional softmax and stick-breaking attention in terms of extrapolation capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores an attention mechanism based on the combination of sparse α-antmax and stick-breaking, and conducts thorough experiments. Compared to the baseline, it achieves improvements in both in-domain and extrapolation settings."}, "weaknesses": {"value": "1. Compared to stick-breaking attention, the improvement is relatively marginal: From Figure 5, I observe that the extrapolation curve is slightly better overall than that of stick-breaking attention, but the enhancement does not appear to be particularly significant.\n2. In terms of methodology, the approach lacks novelty and is essentially a combination of existing methods: The overall idea is based on sparse α-antmax and stick-breaking attention, which lacks deeper insights.\n3. Missing references: Some relevant length generalizable attention works are missing, such as https://arxiv.org/abs/2305.16300, https://arxiv.org/abs/2410.01651."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aUQZ9IBeDu", "forum": "ACIyd1pJLz", "replyto": "ACIyd1pJLz", "signatures": ["ICLR.cc/2026/Conference/Submission7177/Reviewer_FXWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7177/Reviewer_FXWg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465854606, "cdate": 1761465854606, "tmdate": 1762919337595, "mdate": 1762919337595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sieve Attention, a two-stage attention mechanism designed to address the limitations of standard softmax-based attention in long-context generalization. The method combines a sparse content-based filtering stage (using α-entmax) with a sequential allocation stage (using a stick-breaking process) to improve focus and reduce reliance on positional encodings. The authors provide theoretical analysis and experimental results on synthetic and real-world benchmarks to demonstrate the effectiveness of their approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The two-stage design of Sieve Attention is intuitively appealing and aligns with the goal of decoupling content-based relevance from sequential prioritization.  \n- Extensive experiments on both synthetic and real-world benchmarks (e.g., MQRAR, RULER) are conducted, showing consistent improvements over strong baselines.  \n- The hardware-efficient implementation and ablation studies add practical value and credibility to the proposed method."}, "weaknesses": {"value": "1. The core components of Sieve Attention—α-entmax and the stick-breaking process—are not novel. Both have been previously introduced and studied in the literature [1,2]. The combination of these two existing techniques does not constitute a significant conceptual contribution.  \n2. The claim that the entropy of the final attention distribution is strictly lower than that of the candidate distribution relies on the assumption that the sigmoid function σ becomes saturated (i.e., σ(z) → 1). In practice, this saturation is not guaranteed, and the entropy reduction may not hold universally. This limits the generality of the theoretical claim.  \n3. While the authors propose a hardware-efficient implementation, Sieve Attention still underperforms FlashAttention in terms of throughput. Since FlashAttention is an optimized implementation of standard softmax attention without altering its mathematical form, the lower throughput of Sieve Attention suggests higher inherent computational complexity, which may limit its practicality for large-scale deployment.\n4. The related works should include discussion and comparison with the broader landscape of sparse attention mechanisms designed for long-context modeling (e.g., [3-9])."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j7px0GBub9", "forum": "ACIyd1pJLz", "replyto": "ACIyd1pJLz", "signatures": ["ICLR.cc/2026/Conference/Submission7177/Reviewer_V5V7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7177/Reviewer_V5V7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791507230, "cdate": 1761791507230, "tmdate": 1762919337050, "mdate": 1762919337050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Sieve Attention, a attention mechanism designed to address the long-context generalization limitations of the standard Transformer. The proposed method decouples the attention process into two stages: (1) a content-based filtering stage using α-entmax to select a sparse set of candidate tokens, and (2) a sequential allocation stage using a stick-breaking process on this candidate set to assign final weights with a recency bias. The authors argue that this design eliminates the need for external positional encodings and provides superior length extrapolation. The paper includes theoretical analysis and empirical evaluations on synthetic tasks, language model pretraining, and long-context benchmarks to support these claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies and motivates the core challenges in long-context modeling: attention dispersion from dense softmax and the brittleness of positional encodings.\n- The two-stage design of Sieve Attention is explained clearly and illustrated effectively with diagrams.\n- The discussion on a hardware-efficient implementation, including online α-entmax and in-SRAM sequential allocation, demonstrates a consideration for practical deployment."}, "weaknesses": {"value": "- The core technical components of Sieve Attention are not novel. Both the α-entmax sparse activation function [1] and the stick-breaking process for attention [2] are existing methods. The primary contribution of this work is their specific combination. While this combination is non-trivial, it does not constitute a significant conceptual leap.\n- The claim that the entropy of the final distribution \\(H(a_j)\\) is strictly less than that of the candidate distribution \\(H(c_j)\\) is not fully substantiated. The analysis relies on the scenario where \\(\\sigma(z_{i_l,j}) \\to 1\\) for a recent token, causing weights for earlier candidates to vanish. However, it does not account for the saturation properties of the sigmoid function. In practice, if multiple candidates have high, saturated logits, the resulting distribution \\(a_j\\) may not be significantly more concentrated than \\(c_j\\), and the entropy reduction may be minimal. The analysis would benefit from a more rigorous treatment of this dynamic.\n- The condition for filtering out a distractor token—\\((\\alpha-1)z_d \\leq \\tau(z_j)\\)—is presented in an idealized, noise-free setting. The proposition assumes that a single high-scoring relevant token \\(t_f\\) can raise the global threshold \\(\\tau(z_j)\\) sufficiently to filter all distractors. In realistic scenarios with complex, multi-modal score distributions and numerous distractors, this global threshold may not be as effective, and the theoretical guarantee may not hold. An analysis incorporating noisy score distributions would strengthen this claim.\n- The paper acknowledges that Sieve Attention's throughput is lower than the highly optimized FlashAttention. Given that FlashAttention is an implementation of standard Softmax attention, this performance gap suggests that the two-stage Sieve Attention algorithm has a higher inherent computational and/or memory complexity. While the complexity is argued to scale with the candidate set size \\(s_j\\), the overhead of the filtering stage and the sequential allocation on non-contiguous data appears non-negligible in practice. This raises concerns about its efficiency as a drop-in replacement for large-scale training and inference.\n\n---\n\n[1] Sparse sequence-to-sequence models. ACL 2019\n[2] Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study. 2024"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mt5EEoSTmx", "forum": "ACIyd1pJLz", "replyto": "ACIyd1pJLz", "signatures": ["ICLR.cc/2026/Conference/Submission7177/Reviewer_PXeG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7177/Reviewer_PXeG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7177/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874629667, "cdate": 1761874629667, "tmdate": 1762919336599, "mdate": 1762919336599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}