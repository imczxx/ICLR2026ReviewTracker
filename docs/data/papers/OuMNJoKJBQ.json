{"id": "OuMNJoKJBQ", "number": 15113, "cdate": 1758247931471, "mdate": 1759897327755, "content": {"title": "Alignment-Weighted DPO:  A principled reasoning approach to improve alignment", "abstract": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce **Alignment-Weighted DPO**, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.", "tldr": "", "keywords": ["Reasoning", "LLM alignment", "DPO"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ef7b0b2f4c5cf5234137b4223addf951c682c7f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors identify that current alignment techniques such as SFT, RLHF, and DPO rely on superficial refusal heuristics rather than deep reasoning, making LLMs vulnerable to jailbreak attacks. Through causal intervention (neuron deactivation), they show that alignment performance is independent of reasoning ability. To address this, they:\n- Construct a Chain-of-Thought (CoT) fine-tuning dataset that pairs safety-critical and general prompts with step-by-step reasoning.\n- Propose Alignment-Weighted DPO (AW-DPO), which decomposes model outputs into reasoning and final-answer segments and assigns distinct preference weights to each.\n- Demonstrate that AW-DPO yields fine-grained, reasoning-aware optimization, outperforming standard DPO and SFT baselines on safety benchmarks (e.g., SorryBench, WildJailbreak) while maintaining utility"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:**\nThe paper introduces a reasoning-aware alignment framework, Alignment-Weighted DPO (AW-DPO), which innovatively decomposes model outputs into reasoning and response segments and assigns adaptive preference weights. This idea extends traditional DPO by integrating fine-grained, causal reasoning signals—a novel conceptual and methodological contribution. The use of causal neuron intervention to empirically verify the superficiality of current alignment mechanisms is also original.\n\n**Quality:**\nThe work demonstrates strong empirical rigor. The authors validate their claims across multiple model families (LLaMA, Mistral) and diverse benchmarks (SorryBench, WildJailbreak, MMLU). The causal probing experiments and sensitivity analyses add robustness to their conclusions. The methodological formulation of AW-DPO is mathematically grounded, and comparisons against recent strong baselines (e.g., STAIR-DPO, SAFECHAIN) are fair and well-controlled.\n\n\n**Significance:**\nThis work makes a meaningful contribution to the intersection of reasoning and alignment, an emerging and critical area in safe LLM development. By showing that reasoning-based alignment leads to more principled refusals and stronger jailbreak robustness without major utility loss, the paper provides a promising direction for future research on safe reasoning optimization. The open-sourced CoT dataset further enhances reproducibility and potential community impact."}, "weaknesses": {"value": "- While the proposed Alignment-Weighted DPO (AW-DPO) introduces an intuitive weighting between reasoning and response segments, the paper lacks formal justification or theoretical analysis on how these weights affect convergence, stability, or optimization dynamics in preference learning.\n\n- The causal probing analysis in Figure 1 may be limited, as the base model used is not a dedicated reasoning model. Consequently, the observation that alignment is independent of reasoning ability might be incomplete; reasoning could indeed influence alignment in state-of-the-art Large Reasoning Models (LRMs).\n\n- The paper does not compare performance against recent jailbreak methods such as GCG, GPTFuzzer, BOOST, or PAIR, which represent strong baselines in safety evaluation. Including these would make the results more comprehensive and credible.\n\n- The experimental evaluation relies primarily on structured benchmarks like SorryBench and WildJailbreak, which, while diverse, may not capture adaptive or compositional jailbreak behaviors. I recommend incorporating JailbreakBench or other in-the-wild adaptive attack datasets to strengthen the empirical robustness claims.\n\n- Finally, I suggest extending experiments to include Large Reasoning Models (LRMs) such as DeepSeek-R1 or Qwen3, to examine whether AW-DPO remains effective when applied to models with explicit reasoning capabilities."}, "questions": {"value": "- What does the scaling factor α in Table 4 represent?\n- Is it possible to evaluate safety metrics at the level of individual reasoning traces? How does AW-DPO efficiently identify and mitigate unsafe reasoning paths within the model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yIlb9YMyvV", "forum": "OuMNJoKJBQ", "replyto": "OuMNJoKJBQ", "signatures": ["ICLR.cc/2026/Conference/Submission15113/Reviewer_mqeq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15113/Reviewer_mqeq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534145934, "cdate": 1761534145934, "tmdate": 1762925436785, "mdate": 1762925436785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that current LLM safety alignment is superficial, relying on shallow heuristics rather than deep reasoning. This leaves models vulnerable to jailbreaks. To prove this, the authors conduct a causal intervention, deactivating reasoning-critical neurons. They find this action degrades reasoning ability, but leaves the model's alignment performance largely unaffected, supporting their hypothesis.\n\nTo address this, the authors create a CoT fine-tuning dataset to teach models reasoned refusals. They propose Alignment-Weighted DPO, a method that assigns different DPO preference weights to the \"reasoning\" and \"final-answer\" segments of a response. This allows for more targeted updates. The authors perform experiments across multiple benchmarks and show that AW-DPO consistently improves safety robustness while maintaining the model's general utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work has a clear hypothesis and strong motivation. The preliminary experiment provides empirical evidence for this hypothesis, showing that alignment and reasoning are separable.\n- The dataset used in this work will be open-sourced, which is a valuable contribution to the community.\n- The authors conducted extensive empirical evaluation, which demonstrates that the AW-DPO method achieves strong safety performance across multiple model families and sizes. The method demonstrates significantly improved safety performance while largely maintaining utility on the MMLU benchmark.\n- The authors compared AW-DPO against a reasonable set of baselines, including standard SFT/DPO methods and several alignment techniques."}, "weaknesses": {"value": "- In Section 3, the authors conclude that alignment is \"largely independent of deep reasoning,\" which is an overstatement. The probing tasks used to define reasoning and alignment are simple binary classifications. This experiment shows that representations for these two specific tasks are separable, but this does not exclude a more complex, non-linear relationship or the possibility that robust alignment (beyond simple prompt detection) requires reasoning.\n- The presentation of this work is rather poor: almost all tables seem to have a wrong column name (Model Name), and the captions for tables and figures are vague and non-informative. A lot of time is wasted trying to match a table/figure with its corresponding paragraph to find crucial information, such as which models the experiments were done on. In Section 5.7, the authors' whole narration is based on results that are only available in the appendix. The name of the method is not consistent in the paper; sometimes it is \"AW-DPO\" and sometimes \"+Safety DPO\".\n- In Section 5.3, the authors claim that \"merely improving general reasoning ability is insufficient for achieving better performance on alignment-specific tasks\" based on results from only one model family. This hypothesis may not be true in general.\n- The method seems to require that the model explicitly output a step-by-step reasoning trace that can be segmented. This is a potential limitation not discussed in the paper. The presented method may not be applicable to models that reason implicitly or to standard (non-CoT) model outputs.\n- The AW-DPO pipeline requires an LLM judge to score the harmfulness of the reasoning and response segments separately. The authors don't specify which LLM was used as the judge or how its reliability for this fine-grained, two-part scoring was validated. The performance of the presented method is highly dependent on the quality of these scores.\n- The presented method depends on decomposing outputs into \"reasoning\" and \"response\" segments. In the main paper (Section 4), there is not much information about how this segmentation is practically achieved. The implementation details of this part are crucial and should not only be available in the appendix."}, "questions": {"value": "- Please fix the inconsistent method naming (\"AW-DPO\" vs. \"+Safety DPO\") used throughout the paper.\n- Please correct the confusing \"Model Name\" column in the tables. \n- Please improve the vague captions for tables and figures to make them more informative on their own.\n- Please move key results (e.g., for Section 5.7) from the appendix to the main paper to support the core narrative.\n- Which specific LLM was used as the judge for the AW-DPO pipeline?\n- How was the LLM judge prompted to provide separate harmfulness scores for the reasoning ($h_{rs}$) and response ($h_{rp}$) segments?\n- What validation was performed to ensure the LLM judge's scores are accurate and reliable?\n- How are outputs practically segmented into \"reasoning\" and \"response\" parts? This crucial detail must be in Section 4, not just the appendix.\n- Can you discuss the method's limitation of requiring explicit CoT outputs and its inapplicability to standard, non-CoT models?\n- Can you justify the strong causal claim in Section 3, given that the probing tasks were simple binary classifications?\n- Can you justify the general claim in Section 5.3 that \"improving general reasoning... is insufficient\" for alignment, as it's based on only one model family?\n\nI am willing to increase my score if the most important points are solved, particularly the critical questions regarding the validation of the LLM judge and overstated phrases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Pf8NsfXMIS", "forum": "OuMNJoKJBQ", "replyto": "OuMNJoKJBQ", "signatures": ["ICLR.cc/2026/Conference/Submission15113/Reviewer_N5bQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15113/Reviewer_N5bQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604006557, "cdate": 1761604006557, "tmdate": 1762925435791, "mdate": 1762925435791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes enhancing LLM safety by grounding refusals in reasoning rather than superficial heuristics.  Through causal neuron intervention, the authors show that current alignment methods (SFT, RLHF, DPO) rely on shallow refusal patterns independent of reasoning ability.  To address this, they introduce a CoT fine-tuning dataset with reasoning-based safety prompts and propose Alignment-Weighted DPO, which decomposes model outputs into reasoning and response parts, assigning different optimization weights based on their harmfulness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reasoning-based alignment: Introduces AW-DPO, which shifts alignment from surface refusals to reasoning-grounded safety decisions, improving interpretability and robustness."}, "weaknesses": {"value": "1. The experimental section lacks a PPO baseline using the same LLM judge for reward modeling, which is essential to prove that AW-DPO’s advantage comes from its weighted formulation rather than simply leveraging LLM-based supervision.\n\n2. The paper compares AW-DPO with DPO but omits discussion of the additional computational cost introduced by multi-candidate generation, per-segment harmfulness scoring, and dynamic weight calculation.\n\n3. The weighting relies heavily on a single LLM’s scoring reliability; fluctuations in prompt design or model drift could cause unstable or biased training outcomes."}, "questions": {"value": "1. Would AW-DPO still outperform if a PPO baseline using the same LLM judge for rewards were included?\n\n2. How much extra computation does AW-DPO add compared to standard DPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3vlMQER3M4", "forum": "OuMNJoKJBQ", "replyto": "OuMNJoKJBQ", "signatures": ["ICLR.cc/2026/Conference/Submission15113/Reviewer_uxXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15113/Reviewer_uxXE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914033668, "cdate": 1761914033668, "tmdate": 1762925434550, "mdate": 1762925434550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to improve reasoning about alignment, such as why a harmful query should be refused, by weighting the reasoning part (chain of thought, or CoT) differently from the final response during DPO training. The weights are determined by using an LLM-as-judge to decide the relative harmfulness of the two components. For example, the model might output a safe response after reasoning that indicates the query is not harmful, or output a harmful answer after reasoning that the query is unsafe. The paper first motivates this approach by conducting an experiment where linear probing is used to identify the attention heads with representations that are most predictive of either whether a query is harmful or an answer to a CommonsenseQA question is correct or incorrect. Deactivating the heads identified on the reasoning task does not affect alignment significantly. Further, the heads associated with identifying safe versus unsafe tend to be in earlier layers of the model. The paper argues that this means the model uses shallow heuristics to decide whether to answer a query or not. Experiments show that alignment improves without significantly hurting utility after this weighted DPO training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is simple and easy to implement.\n\n2. The results indicate that it can give a nice boost to alignment without sacrificing utility, although not with perfect consistency."}, "weaknesses": {"value": "1. I am skeptical of the motivating experiment. The jump from the linear probing results to the interpretation that reasoning about alignment is shallow seems unsound. How do we know that the heads that are predictive of the answers to commonsense QA reasoning are used in all types of reasoning and that there are no other reasoning heads?\n\n2. The LLM judgements used to weight the learning objective are not carefully studied. How correlated are they with human judgements of safety? Combined with the issue above, it leaves me unconvinced that the method is working for the reason that is claimed."}, "questions": {"value": "1. What happens if you repeat the linear probing experiments on the model trained with AW-DPO? Do the harmfulness heads move to later layers? Does knocking out the \"reasoning\" heads undo the gains on alignment tasks? If so, that would be a step to improving the soundness of the paper.\n\n2. In multiple places, the paper says that related work fails to consider \"utility trade-offs\" when constructing their datasets. This seems vague. What does it mean, and it what way is utility considered during the construction of this data? I see that utility is evaluated (which is great), but that seems different from considering it during data construction.\n\n3. On line 102, Wei 2021 is cited as a reference of using RL for alignment, but that paper considers neither RL nor alignment.\n\n4. The text in the figures are so small that it is basically impossible to read on paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ll6us2r0RS", "forum": "OuMNJoKJBQ", "replyto": "OuMNJoKJBQ", "signatures": ["ICLR.cc/2026/Conference/Submission15113/Reviewer_5LUh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15113/Reviewer_5LUh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022991625, "cdate": 1762022991625, "tmdate": 1762925433775, "mdate": 1762925433775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}