{"id": "iNg5KL7eTC", "number": 5455, "cdate": 1757911522520, "mdate": 1759897973676, "content": {"title": "Meta-UCF: Unified Task-Conditioned LoRA Generation for Continual Learning in Large Language Models", "abstract": "Large language models are increasingly deployed in settings where newtasks arrive continuously, yet existing parameter-efficient finetuning (PEFT) methods either bloat linearly with the task horizon or sacrifice deep adaptation, leaving catastrophic forgetting unresolved. We aim to achieve memory-constant, on-the-fly adaptation for a frozen LLM facing an unbounded stream of tasks. To this end we propose Meta-Unified Contrastive Finetuning(\\textsc{Meta-UCF}), which encodes each task into a lightweight layer-normalised mean embedding and feeds it to a single hypernetwork that instantly generates rank-r LoRA updates for every transformer layer; a meta-contrastive coupled with orthogonality objective further steers task embeddings into near-orthogonal directions, preserving past knowledge without inner-loop gradients. On four benchmark streams—Std-CL 5, Seq-GLUE 7, Long-CL 15 and TRACE-8—Meta-UCF raises average accuracy by up to 2.2 pp and cuts forgetting by 13% relative to the strongest LoRA baseline, while using the parameters of a single adapter. By decoupling continual learning from parameter growth, Meta-UCF provides a practical path toward scalable, low-resource lifelong language modelling.", "tldr": "", "keywords": ["LLM", "Fine-Tuning", "Continual fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79b3ef5f3a708f16f1fa1a3205bad3ee28aba2c2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a continual learning framework named Meta-UCF. Its core idea is to utilize a hypernetwork to dynamically generate LoRA parameters for new tasks, conditioned on a task embedding derived from a small support set. The method aims to solve the problem of linear parameter growth with the number of tasks found in existing methods, and it combines contrastive and orthogonality losses to maintain model performance while mitigating catastrophic forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Meta-UCF generates task parameters via a single, shared hypernetwork, which keeps the total model parameter count constant as tasks increase. Compared to methods that add new modules for each task, this is an advantage in terms of scalability, especially for scenarios requiring continuous learning of new tasks.\n\n2. The paper conducts experiments under various task sequence settings (short, long, and heterogeneous sequences) and compares against multiple baselines. The ablation studies and sensitivity analyses are also quite thorough, validating the effectiveness of the framework's components."}, "weaknesses": {"value": "1. The \"on-the-fly\" generation of LoRA parameters mentioned in the paper implies that a forward pass through the hypernetwork is required before processing a new task. This introduces additional inference overhead. The paper lacks a latency comparison against methods that load static LoRA modules. Without this data, it is difficult to assess the practical cost of the framework.\n\n2. The framework claims to solve the memory growth problem, but this primarily refers to model parameters. The algorithm itself relies on a replay buffer to construct embedding vectors for old tasks. The size of this buffer either grows with tasks or is constrained by a fixed budget, which somewhat undermines the claim of \"constant memory\"。 \n\n3. The entire adaptation process relies on the task embedding extracted from a small number of samples (32 by default). If these samples are not representative, the resulting task embedding will be biased, which in turn will affect the model's performance on that task. This is a potential point of failure, but the paper lacks a discussion of its robustness.\n\n4. The final loss function includes multiple weighting hyperparameters. Although the paper states that these parameters are kept fixed across all experiments, determining these values is a non-trivial challenge in itself. A discussion on the difficulty and sensitivity of tuning these hyperparameters is recommended.\n\n5. According to the ablation study (Table 2b), removing the dynamic bias calibration module results in only a 0.6% drop in accuracy on Std-CL 5. The improvement from this module is limited. The paper needs to provide a stronger motivation for its inclusion or demonstrate its value on a dataset known to require bias calibration.\n\n6. The orthogonality loss (L_orth) is calculated on the CLS hidden states from the query set. The paper should explain why the query set was chosen over the support set and how this loss, which acts on output representations, works in conjunction with the contrastive loss (L_ctr) that acts on task embeddings."}, "questions": {"value": "Please see \"weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Null."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dHAYHFog9b", "forum": "iNg5KL7eTC", "replyto": "iNg5KL7eTC", "signatures": ["ICLR.cc/2026/Conference/Submission5455/Reviewer_ehuo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5455/Reviewer_ehuo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410130231, "cdate": 1761410130231, "tmdate": 1762918072727, "mdate": 1762918072727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Meta-Unified Contrastive Fine-Tuning, a novel approach to continual learning in LLMs. The method addresses the challenges of catastrophic forgetting and parameter growth by proposing a task-conditioned hypernetwork that generates low-rank LoRA updates dynamically for each task. Meta-UCF incorporates a meta-contrastive objective and orthogonality constraints to ensure task embeddings are near-orthogonal, preserving past knowledge without requiring additional adapters for new tasks. Experimental results demonstrate state-of-the-art performance across four benchmarks, with improvements in accuracy and reductions in forgetting compared to prior methods. The approach is computationally efficient, scalable, and adaptable to diverse backbone architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The task-conditioned hypernetwork and the integration of meta-contrastive and orthogonality objectives are innovative contributions that address key limitations of existing methods.\nThe experimental results are comprehensive, covering multiple benchmarks and baselines. The inclusion of theoretical analysis (e.g., expressivity bounds, PAC-Bayes generalization) adds depth to the work.\nThe paper is well-organized, with clear explanations of the methodology, experiments, and findings."}, "weaknesses": {"value": "While the benchmarks are diverse, the paper does not evaluate Meta-UCF on real-world deployment scenarios (e.g., latency-critical applications or domain-specific tasks).\nAlthough the sensitivity analysis shows robustness, the reliance on specific hyperparameter configurations (e.g., orthogonality and contrastive weights) may pose challenges for practitioners.\nThe theoretical analysis assumes idealized conditions (e.g., compact task embeddings, independent task distributions), which may not fully capture the complexities of real-world task streams."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ptzk8ZQ1VD", "forum": "iNg5KL7eTC", "replyto": "iNg5KL7eTC", "signatures": ["ICLR.cc/2026/Conference/Submission5455/Reviewer_TwVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5455/Reviewer_TwVA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688689219, "cdate": 1761688689219, "tmdate": 1762918072452, "mdate": 1762918072452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Meta-UCF proposes a unified continual learning method for LLMs that uses a single hypernetwork to generate task-specific LoRA parameters from lightweight task embeddings derived from a small support set, without inner-loop updates or replay; task interference is mitigated via contrastive separation and orthogonality constraints in the embedding space, while keeping the backbone frozen and trainable parameters fixed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Meta-UCF maintains a fixed set of trainable parameters regardless of the number of tasks, eliminating the need for task-specific adapters or replay buffers.\n\n2. It effectively reduces task interference through contrastive learning and orthogonality constraints on task embeddings, without requiring inner-loop adaptation or data replay."}, "weaknesses": {"value": "1. While parameter efficiency is emphasized, the paper omits analysis of the whole cost introduced by the hypernetwork and embedding generation pipeline, which is crucial for real-world deployment.\n\n2. The paper’s formatting is inconsistent and unclear, with poorly structured tables and ambiguous notation that reduce readability.\n\n3. Experiments assume relatively benign task sequences, with no analysis of performance under extreme domain shifts or adversarial task orderings that better stress-test continual learning robustness."}, "questions": {"value": "Although this paper addresses continual learning, I am curious, following the discussion in [1], about how the proposed method performs on standard LLM tasks beyond continual learning, such as mathematical reasoning, code generation, and instruction following.\n\n[1] HFT: Half Fine-Tuning for Large Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G5aI8b2StF", "forum": "iNg5KL7eTC", "replyto": "iNg5KL7eTC", "signatures": ["ICLR.cc/2026/Conference/Submission5455/Reviewer_KhxK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5455/Reviewer_KhxK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990865998, "cdate": 1761990865998, "tmdate": 1762918072144, "mdate": 1762918072144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents META-UCF, a new LoRA-based adaptation module intended to improve generalization across tasks while keeping parameter growth small, and reports strong results against diverse baselines.  I basically creates \"add-ons\" for each new task instead of storing a separate add-on bank per task. Training encourages tasks to stay distinct, which helps the model remember old skills while learning new ones. Across several benchmarks, this approach nudges accuracy up and reduces forgetting compared to strong baselines. The gains hold under different settings, suggesting the method is fairly robust rather than overly tuned."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper's idea is simple and well motivated. The figures, though simple, are intuitive and convey the point well. \\\nS2. Consistent gains in AA with lower forgetting across four continual streams versus strong LoRA and prompt baselines.  \\\nS3. The method has the potential to be the next \"SOTA\" with some fixes in writing."}, "weaknesses": {"value": "W1. Section 3.3 clarity & missing intuition. The meta-objective is presented mostly as equations without enough narrative to make the design choices legible (e.g., how the positive view ​is constructed, why InfoNCE is preferred here, and how the orthogonality penalty interacts with the contrastive term during optimisation). The paper defines multiple notations (omega) and aggregates losses but gives little intuition for stability/plasticity trade-offs beyond symbol lists. Even a short paragraph walking through one training step with concrete shapes would help. As written, 3.3 is hard to follow for readers not already steeped in contrastive notation.\n\nW2. Experimental depth & statistical evidence. Main results are reported as averaged metrics per stream (AA/FR/BWT), but the tables omit confidence intervals or statistical tests against strong baselines; the text only notes averaging over three seeds, which is thin for claims of \"state-of-the-art\". Per-dataset/task breakdowns (especially for TRACE-8’s diverse domains) and order-sensitivity analyses are absent from the main paper, limiting interpretability of where gains come from. Without significance testing, improvements are hard to judge.\n\nW3. Compute/memory reporting is not actionable. While the method argues constant-size adapters and gives a big-O generator complexity plus one throughput as compared to last layers vs all, the paper lacks concrete parameter counts for the hypernetwork vs adapter banks, end-to-end training wall-clock vs baselines, and detailed memory footprints during training/inference."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QKIVEE3e6V", "forum": "iNg5KL7eTC", "replyto": "iNg5KL7eTC", "signatures": ["ICLR.cc/2026/Conference/Submission5455/Reviewer_JLyJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5455/Reviewer_JLyJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5455/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762911818133, "cdate": 1762911818133, "tmdate": 1762918071881, "mdate": 1762918071881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}