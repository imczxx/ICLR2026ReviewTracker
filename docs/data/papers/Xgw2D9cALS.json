{"id": "Xgw2D9cALS", "number": 8121, "cdate": 1758065768566, "mdate": 1759897805692, "content": {"title": "ConvRec-R1: Training LLM-based Conversational Recommender Systems with Reinforcement Learning", "abstract": "Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://anonymous.4open.science/r/ConvRec-R1-5615.", "tldr": "", "keywords": ["Conversational recommender system", "Reinforcement learning with Verifiable Reward"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f49c7eba6ad107e16f9324a2470bd21867e7568.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces R1 style reinforcement learning and self improvement techniques into conversational recommender systems. It first develops a remap reflect adjust pipeline to improve data quality, and then introduces a ranking GRPO reinforcement learning objective with reward shaping techniques for performance improvement. Experiments demonstrate that it can improve the performance of LLM-based conversational recommender system."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation of the paper is clear.\n2. The proposed method shows performance improvement.\n3. Case studies show that after RL, the ranking output of the model is better, demonstrating that rank-GRPO is functioning properly."}, "weaknesses": {"value": "1. It will be better if the method can be tested on more datasets. Current only one dataset is used.\n2. It will be better if other aspects of conversation quality of the framework such as helpfulness and informativeness, other than the ranking accuracy, can be evaluated.\n3. Some claims can be more rigorous. For example, authors claim GRPO is fundamentally misaligned with tasks with rank-style outputs, but the proposed method is still a simple extension of GRPO. If GRPO is fundamentally misaligned, the proposed framework might be something very different from GRPO."}, "questions": {"value": "Is the method senstive to the predefined prompt templates or is it robust to different templates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Joqw464ckC", "forum": "Xgw2D9cALS", "replyto": "Xgw2D9cALS", "signatures": ["ICLR.cc/2026/Conference/Submission8121/Reviewer_1bEF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8121/Reviewer_1bEF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761356639108, "cdate": 1761356639108, "tmdate": 1762920099441, "mdate": 1762920099441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ConvRec-R1, a two-stage framework for training LLM-based conversational recommender systems. It combines a supervised fine-tuning pipeline (Remap-Reflect-Adjust) with a new reinforcement learning method (Rank-GRPO) that optimizes recommendations at the rank level. The authors present experiments on REDDIT-V2 that demonstrate solid improvements over baselines, achieving near-GPT-4 performance with smaller open-source models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper addresses an important and emerging problem i.e aligning LLMs for conversational recommendation, which has practical utility for the industry and is a relevant problem for the community.\n2) The authors provide code, data, and detailed implementation notes, making the work easy to reproduce and build upon.\n3) The proposed rank-level GRPO is a generally useful method for ranking tasks. It's well motivated, includes gradient analysis, and performs reasonably well in practice.\n4) The experimental section is detailed and comprehensive, covering different model scales, baselines, and both SFT and RL stages."}, "weaknesses": {"value": "1) The core ideas, namely supervised fine-tuning plus RL alignment, primarily extend existing GRPO and RLHF frameworks, without introducing a fundamentally new paradigm.\n2) The approach is tightly focused on conversational recommendation and may not generalize well to broader LLM alignment or other ranking tasks, which could limit the paper's impact.\n3) The performance improvement over strong prompting baselines (e.g., CRAG) is modest given the added training complexity, and under off-policy settings the method can perform even worse, suggesting limited robustness and stability.\n4) All experiments are conducted on the REDDIT-V2 dataset, leaving open the question of how well the method generalises to other domains or item catalogs.\n5) The work lacks online or human evaluation, making it unclear whether the improvements translate to better real-world user experience."}, "questions": {"value": "1) The paper only uses the REDDIT-V2 dataset without sufficient justification. For a research paper, relying on a single dataset is insufficient to demonstrate generality. Why weren't other conversational recommendation datasets considered?\n\n2) ConvRec-R1 performs worse than CRAG under off-policy evaluation, despite its more complex training. What causes this drop? reward misalignment, data shift, or instability in Rank-GRPO? A clearer analysis would strengthen the paper's claims.\n\n3) The paper reports results on models only up to 3B parameters. Have the authors tested or considered larger LLMs to see whether the proposed Rank-GRPO continues to scale effectively with model size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PRHuL0dQ7f", "forum": "Xgw2D9cALS", "replyto": "Xgw2D9cALS", "signatures": ["ICLR.cc/2026/Conference/Submission8121/Reviewer_uEVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8121/Reviewer_uEVq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862166220, "cdate": 1761862166220, "tmdate": 1762920099103, "mdate": 1762920099103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ConvRec-R1, a two-stage approach to prevent LLMs from generating out-of-catalog items and to avoid sharp drops in ranking quality. Stage 1 performs supervised fine-tuning (SFT) on high-quality synthetic data produced by GPT-4o; Stage 2 applies Rank-GRPO, an extension of GRPO tailored for ranking optimization. Experimental results demonstrate that ConvRec-R1 achieves strong performance on the evaluated benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated.\n\nThe paper is readable."}, "weaknesses": {"value": "1. **Performance on larger models (e.g., 7B) is unclear.**\n    \n    Please provide experimental results or discussions on how the proposed method scales to larger backbones (e.g., 7B parameters). This will help verify whether the observed improvements generalize across model sizes.\n    \n2. **Baselines in Table 1 are insufficient.**\n    \n    Table 1 should include more **post-training baselines** specific to LLM-based recommender systems, rather than comparing only SFT or SFT + GRPO. Incorporating recent LLM-agent or ranking-enhanced recommender baselines would make the comparison more convincing.\n    \n3. **A more detailed ablation study is needed.**\n    \n    Please include ablation experiments isolating the effects of key components, such as **without remap**, **without reflect**, and **without SFT**. These results would clarify each module’s contribution to the overall performance."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FvPpo5UOnu", "forum": "Xgw2D9cALS", "replyto": "Xgw2D9cALS", "signatures": ["ICLR.cc/2026/Conference/Submission8121/Reviewer_NBhB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8121/Reviewer_NBhB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899446028, "cdate": 1761899446028, "tmdate": 1762920098483, "mdate": 1762920098483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces ConvRec-R1, a two-stage framework for training LLM-based conversational recommender systems. The first stage uses a novel \"Remap-Reflect-Adjust\" data distillation pipeline to create a high-quality, catalog-aware dataset for SFT. The second stage introduces Rank-GRPO, a new RL algorithm tailored for ranking tasks. Rank-GRPO reframes the RL update unit to the \"rank position\" level, addressing fundamental flaws in standard GRPO like non-causal credit assignment. Experiments show the framework improves recommendation quality (Recall and NDCG) and allows smaller open-source LLMs to outperform larger, zero-shot models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tRank-GRPO is a main contribution. The manuscript clearly identifies the core weaknesses of applying standard GRPO to ranking tasks and proposes an new solution by re-framing the problem at the rank level. The technical solutions are well-motivated and supported by theoretical analysis.\n2.\tThe \"Remap-Reflect-Adjust\" pipeline is a significant engineering contribution that provides a sophisticated solution to the critical data scarcity problem in this domain. It is quite practical for researchers and practitioners aiming to deploy LLM-based CRS.\n3.\tThe experimental design is robust. The results clearly show the value of each stage of the framework and validate the superiority of Rank-GRPO over its baselines. The finding that a well-aligned 3B model can outperform a much larger zero-shot model is certainly relevant."}, "weaknesses": {"value": "1.\tThe manuscript utilizes an “LLM as a judge” in its data pipeline but does not adequately discuss or account for known biases of this paradigm, such as position or verbosity bias, which could affect the quality of the SFT dataset.\n2.\tAll experiments are conducted on a single dataset in the movie domain. The manuscript would be stronger with a discussion on the potential challenges of applying the framework to other domains like e-commerce or music.\n3.\tThe manuscript notes that the model's outputs tend to drift out-of-catalog during RL training. This practical limitation deserves more prominent discussion in the main text, including the final out-of-catalog rate and potential mitigation strategies.\n4.\tThe primary results table (Table 1) omits a direct on-policy comparison with GSPO, a relevant and stronger baseline discussed elsewhere in the manuscript. Including this would make the evaluation of Rank-GRPO's advantage more complete.\n5.\tThe multi-step SFT data pipeline involves a lot of components and hyperparameters. The manuscript lacks an analysis of how sensitive the final data quality is to these choices, which would be valuable for reproducibility and practical application."}, "questions": {"value": "1. Can you elaborate on measures taken to mitigate potential biases (e.g., position bias) from the LLM-as-a-judge used in the \"Reflect\" step?\n2. What is the final out-of-catalog recommendation rate on the test set, and what are potential strategies to better enforce catalog constraints during the RL phase?\n3. Why was the GSPO baseline omitted from the main on-policy results in Table 1, given it is a key point of comparison?\n4. How sensitive is the model's performance to the new penalty hyperparameters (ϵ_u, ϵ_o) introduced in Rank-GRPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yg3YAhTiRS", "forum": "Xgw2D9cALS", "replyto": "Xgw2D9cALS", "signatures": ["ICLR.cc/2026/Conference/Submission8121/Reviewer_o3UH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8121/Reviewer_o3UH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8121/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088158782, "cdate": 1762088158782, "tmdate": 1762920097969, "mdate": 1762920097969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}