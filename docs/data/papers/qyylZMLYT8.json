{"id": "qyylZMLYT8", "number": 6176, "cdate": 1757956381334, "mdate": 1759897931695, "content": {"title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design", "abstract": "We propose a general-purpose approach for improving the ability of large language models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED).\nThis enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments.\nOur approach, which we call BED-LLM (Bayesian experimental design with large language models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated (and then estimated) in a principled way using a probabilistic model derived from the LLM's predictive distributions and provide detailed insights into key decisions in its construction and updating procedure. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20 Questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.", "tldr": "", "keywords": ["Experimental Design", "Bayesian", "BED", "EIG", "Information Gain", "LLM", "Conversational", "clarification questions", "input ambiguities", "uncertainty"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00741fa72dc2b4e5da041c7d73224071f5fc1f8d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a Bayesian Experimental Design (BED) inspired algorithm that enables LLMs to ask informed questions to a user or an external source. The approach leverages the internal belief probabilities of the LLM to estimate the prior and conditional distributions over queries and answers, allowing it to select optimal questions based on Expected Information Gain (EIG). EIG is computed as the difference in entropy between the posterior and prior beliefs, capturing the expected reduction in uncertainty after conditioning on the queried question and its potential answers.The algorithm proposes several strategies to construct and update the belief distributions through prompting and sampling from the underlying LLM. Notably, it uses an external LLM to generate potential questions candidates rather than a fixed set of known questions. Experiments on both the “20 Questions” and a preference elicitation benchmark show modest improvements compared to “Naive” and “Split” baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important challenge in information seeking for LLM, aiming to formalize how LLMs can ask meaningful and informative questions.\n2. The proposed framework, based on Bayesian Experimental Design (BED) and Expected Information Gain (EIG), is theoretically well-motivated and offers strong interpretability and soundness for modeling information-seeking behavior.\n3. The authors provide detailed explanations of their assumptions and offer clear justifications for these assumptions"}, "weaknesses": {"value": "The paper would benefit from improvements in clarity and presentation, which would enhance its accessibility to readers. \n\n1. The overall structure of the paper is not well organized and is difficult to follow. The methodology section is dense and provides minimal examples to clarify key ideas. The authors should consider adding illustrative visuals or concrete examples to make the approach more understandable. \n\nSome claims and assumptions are insufficiently motivated/addressed, and the arguments supporting them appear insufficient or not well justified. The author should refrain from making claims and general conclusion without strong supporting evidence. \n\n2. Several claims in the paper are not sufficiently supported by experiments or reasoning. For instance, the authors state that the ``approach often fails to appropriately incorporate the information from $h_{t-1}$, leading to a belief distribution inconsistent with past observations,'' yet no examples, analyses, or ablations are provided to substantiate this claim. \n3. Some simplified assumptions are concerning. In particular, replacing $p(y \\mid \\theta, x)$ with $p_{\\text{LLM}}(y; \\theta, x)$ when constructing the joint distribution $p(y, \\theta; x)$ raises concerns about correctness and validity. In appendix A.1 the author's justification for not updating likelihood $p_{\\text{LLM}}(y; [\\theta, x_t])$  is \"$\\theta$ capture all the required information to predict y|x\" which also lacks evidence to support the claim.  \n\nThe paper lacks sufficient specification of experimental details and design choices, and provides limited baseline comparisons or ablation analyses. The two baselines included in the paper are not representative of existing approaches, offering little context for interpreting the reported results. Moreover, since the proposed method requires multiple LLM calls to generate candidate hypotheses and questions, reporting computational costs such as token usage or inference time would be essential to contextualize performance. Finally, the lack of detailed descriptions or illustrative examples for the baselines is concerning make it difficult to assess the fairness of the comparisons.\n\n4. The paper does not discuss computational costs such as token usage, inference time, or scalability. Including such details would provide important context for assessing the practicality of this approach beyond toy problems.\n5. Prompt based methods such as Reflexion, ReAct, and CoT should have been included for a more meaningful evaluation. \n6. The experimental setup is not clearly explained. For example, the paper does not define what the `\"Naive'' baseline represents, and the \"Split'' baseline is only vaguely described. The comparison to standard LLM prompting methods is also limited---For the preference elicitation task, including only the ``Naive'' baseline is insufficient.\n7. The paper lacks ablation studies analyzing the impact of different design choices. Without these analyses, it is difficult to determine which components of the proposed approach actually contribute to the reported performance improvements.\n\nI think the paper is well motivated and the core idea is well-justified. The paper itself requires substantial improvement in clarity, organization, and experimental rigor. The presentation is difficult to follow, several design choices and assumptions are not well motivated, and key claims lack sufficient empirical support or ablation analysis."}, "questions": {"value": "1. The paper uses $p_{\\text{LLM}}(y; \\theta, x)$ as an approximation of $p(y \\mid \\theta, x)$. Is there evidence demonstrating that this approximation is sound? Specifically, have you provided empirical validation from your own experiments or cited related work that supports this substitution? A clearer justification seems necessary, given that this approximation is central to the computation of the joint probability $p(y, \\theta, x)$.\n\n2. In line 159, you state that \"autoregressive sequential rollouts often lead to more nuanced and diverse behavior than repeated static likelihood queries.'' Could you elaborate on why having ``more nuanced and diverse behavior'' is important in a BED framework? What advantage does this bring in relation to your work?\n\n3.  In Appendix~A.1, you mention that \"we choose not to update the likelihood model as more data is gathered,'' meaning $h_{t-1}$ is not used to update the likelihood. The justification provided---\"for many problems, our beliefs on $\\theta$ capture all the required information to predict $y|x$''---seems inconsistent with your prior, which does depend on $h_{t-1}$. Wouldn’t it therefore be more accurate to include $h_{t-1}$ in your experiments rather than exclude it? I’m not fully convinced by the argument that doing so is \"not necessary.''\n\n4.  The paper claims that one advantage of the proposed method is that, unlike prior work, it is not restricted by pre-specifying allowable entities: \"In general, these previous works have also required restrictions on the space of allowable hypotheses $\\theta$, whereas we do not tell the LLM this set of target entities, so the space of $\\theta$ is bounded only by what the LLM can generate.'' However, your approach still requires $\\theta$ when computing the EIG. Is the key difference that prior work assumes $\\theta$ is explicitly given, while your approach allows $\\theta$ to be generated by the LLM? \n\n5. Could you provide details on how the \"Naive'' and \"Split'' baselines are implemented? The current manuscript lacks sufficient description of their setup. Additionally, how does the number of possible answers (20 vs. 500) affect performance? Does it influence your results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cyfQeAjGQp", "forum": "qyylZMLYT8", "replyto": "qyylZMLYT8", "signatures": ["ICLR.cc/2026/Conference/Submission6176/Reviewer_4sXe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6176/Reviewer_4sXe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842107291, "cdate": 1761842107291, "tmdate": 1762918519354, "mdate": 1762918519354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose BED-LLM, a method designed to improve the information gathering ability of large language models (LLMs). The main idea of the paper is to select questions that maximize the expected information gain (EIG), enabling the model to acquire relevant information more efficiently than existing approaches. The method requires no parameter updates or fine-tuning and can be easily used with any pretrained LLMs. Through experiments on the 20 Questions game and film preference elicitation tasks, the authors demonstrate that maximizing EIG significantly enhances an LLM’s ability to ask informative and discriminative questions. These results indicate that EIG is an effective metric for improving information gathering in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tBED-LLM goes beyond simply identifying questions that elicit new information. It also evaluates whether each question is contextually coherent and whether it contributes to determining the final answer. By incorporating these considerations, the method effectively avoids asking irrelevant or redundant questions, guiding the model toward more meaningful and goal-oriented information gathering.\n\n-\tThe method is model-agnostic. It can be used with any existing LLMs without any parameter updating or fine-tuning, which greatly enhances its practicality and generalizability.\n\n-\tThe authors decompose BED-LLM into four distinct components (Entropy, Data-Estimation, ICL Beliefs, Implicit Maximization)  and perform ablation experiments to isolate the contribution of each method. This experimental design enables a quantitative assessment of how each component contributes to the model’s information-gathering performance, offering empirical evidence of the method’s internal effectiveness."}, "weaknesses": {"value": "-\tThe experimental evaluation is relatively limited in scope. The 20 Questions setting, while well-structured, represents a simplified and highly constrained environment that may not adequately capture the ambiguity and uncertainty inherent in real-world information-gathering tasks. Moreover, the chosen topics—animals, celebrities, and things—are narrow in domain and may not generalize well to more complex or abstract scenarios. This raises questions about the method’s robustness and applicability across diverse contexts.\n\n-\tThe paper does not clearly define the termination criterion for the questioning process. In the 20 Questions experiment, the interaction terminates either when the model exhausts all twenty turns or when only a single hypothesis remains and the model correctly identifies it.\nIn contrast, in the film preference elicitation setup, the model proposes a list of films at each turn, which is then evaluated against the user’s preferences, but the stopping condition for this process is not clearly specified. As a result, it remains unclear how and when BED-LLM determines that sufficient information has been gathered to conclude the interaction."}, "questions": {"value": "The BED-LLM approach appears to be computationally heavy, as it generates multiple candidate questions, estimates their expected information gain (EIG), and updates a belief distribution at each turn. It would be helpful to understand how the computational cost compares to baseline methods, such as Naive or Split, in terms of inference-time overhead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ssrhW3WoLH", "forum": "qyylZMLYT8", "replyto": "qyylZMLYT8", "signatures": ["ICLR.cc/2026/Conference/Submission6176/Reviewer_Ah5k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6176/Reviewer_Ah5k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979456085, "cdate": 1761979456085, "tmdate": 1762918518888, "mdate": 1762918518888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BED-LLM, a framework that integrates BED principles into LLMs to enable adaptive and information-efficient query generation. The core idea is to model belief over hypotheses and select the next query by maximizing an approximate EIG, implemented through an LLM-mediated sampling and likelihood estimation process. The formulation adapts Bayesian inference perspectives into a framework for LLM-driven reasoning. The experiments, however, are limited to small, synthetic tasks and do not provide sufficient evidence that the proposed method yields general improvements in interactive reasoning or adaptive planning. The contribution is conceptually promising and well-articulated, but the empirical validation is too narrow to establish practical impact."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Presents a clear and well-motivated connection between Bayesian experimental design and LLM-based information gathering.\n- Offers a unified probabilistic framing that clarifies the relationship between prompting, belief updates, and adaptive query selection.\n- Demonstrates measurable gains over naïve baselines on synthetic tasks, suggesting that EIG-guided prompting can improve efficiency in principle."}, "weaknesses": {"value": "- Limited scope of evaluation. Although the paper is positioned as a general framework for intelligent and adaptive information gathering with LLMs, the experiments are confined to small, toy-scale domains (20-Questions and synthetic movie recommendation). These tasks are useful for illustration but are quite far from the complex, noisy, multi-step reasoning settings the paper emphasizes introduction emphasizes. As a result, the evaluation does not convincingly demonstrate that BED-LLM generalizes beyond simple controlled environments.\n- Missing planning and reasoning baselines. The paper compares primarily against naïve or ablated versions of its own approach, omitting stronger baselines such as reinforcement learning, Monte Carlo tree search, or other uncertainty-driven planning methods that have been explored for adaptive query generation in LLMs. Without these comparisons, it is difficult to attribute observed gains to the proposed Bayesian-experimental-design formulation rather than to prompt structure or candidate filtering.\n- Lack of validation for expected information gain (EIG) approximations. The core methodological claim is that BED-LLM uses approximations of expected information gain to guide question selection. However, there is no empirical or analytical evaluation of the accuracy or stability of these approximations. The paper reports only downstream success metrics, leaving unclear whether the observed performance improvements arise from better EIG estimation or unrelated modeling choices. A more direct analysis, for e.g., comparing true vs. approximated EIG, or assessing consistency under varying priors, would greatly strengthen the argument."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eg7Lb3wj1Z", "forum": "qyylZMLYT8", "replyto": "qyylZMLYT8", "signatures": ["ICLR.cc/2026/Conference/Submission6176/Reviewer_T2zx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6176/Reviewer_T2zx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762188776082, "cdate": 1762188776082, "tmdate": 1762918517450, "mdate": 1762918517450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BED‑LLM. It treats multi‑turn question asking as sequential Bayesian experimental design. At each step, the model makes several candidate multiple‑choice questions. It then picks the one with the highest expected information gain (EIG) and asks it. After the answer, it updates a simple belief over possible hypotheses by sampling, filtering for consistency and renormalizing. The method uses a prior–likelihood setup and a low‑variance EIG estimator based on the model’s logits. It avoids the common mistake of using only predictive entropy. The authors show gains on 20 Questions and movie‑preference tasks, including when the questioner and answerer are different models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear problem formalization and principled objective. The paper cleanly frames multi‑turn question selection as sequential Bayesian experimental design (BED) and optimizes incremental expected information gain (EIG) in a concrete loop: generate candidate questions -> estimate EIG  -> select the best question -> update the belief state. \n- Rigorous treatment of uncertainty (beyond predictive entropy). The authors explicitly show that predictive entropy ≠ EIG and that dropping the expected likelihood entropy term selects ambiguous or irrelevant questions. They provide an intuitive example and ablations (“Entropy” baseline) demonstrating real performance gaps. \n- Sensible modeling choice. It uses a prior–likelihood factorization that puts uncertainty in the answer space y (small and enumerable) rather than the often much bigger hypothesis space $\\theta$. Practically, questions are written as multiple‑choice so probabilities and entropies are reliable.\n- Belief update that fixes in‑context issues. Instead of relying on raw in‑context updates (which can be inconsistent and overconfident), it samples hypotheses, filters them against the history, retains consistent ones across turns, and reweights uniformly. Removing this step hurts performance in ablations, which shows it matters."}, "weaknesses": {"value": "- No compute time accounting. Each turn can be heavy: sample at least N=15 hypotheses (with up to three generate‑and‑filter rounds), score M=15 candidate questions, etc. The paper doesn’t report tokens, wall‑clock, or cost curves to show the overhead vs. baselines.\n- Sensitive to API log‑probabilities. The EIG estimator and the filtering step assume access to stable token‑level probabilities. Some APIs limit this. \n- The algorithm optimizes the next question’s EIG only. The paper itself notes that sequential BED can be suboptimal and points to policy‑based alternatives, but it doesn’t compare against them here.\n- Depends on multiple‑choice answers. The method works by keeping the answer space small and enumerable, so it doesn’t directly handle open‑ended text or continuous answers. This is by design to make probabilities and entropies reliable, but it narrows where the approach applies."}, "questions": {"value": "- Why threshold 0.2, and why uniform reweighting? How sensitive are results to these, and what happens with temperature scaling?\n- For free‑text or continuous y, what EIG approximations or discretizations are viable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B2IuxY66HI", "forum": "qyylZMLYT8", "replyto": "qyylZMLYT8", "signatures": ["ICLR.cc/2026/Conference/Submission6176/Reviewer_Jn86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6176/Reviewer_Jn86"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6176/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762534347912, "cdate": 1762534347912, "tmdate": 1762918516249, "mdate": 1762918516249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}