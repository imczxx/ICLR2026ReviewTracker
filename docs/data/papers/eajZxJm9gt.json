{"id": "eajZxJm9gt", "number": 16468, "cdate": 1758264896314, "mdate": 1763732996542, "content": {"title": "INRCT: An End-to-End Framework for Encoding and Generating Implicit Neural Representation", "abstract": "Current diffusion models based on implicit neural representations (INRs) typically adopt a two-stage framework: an encoder is first trained to map signals into a latent INR space, followed by a diffusion model that generates latent codes from noise. This design requires training and maintaining two separate models, introducing compounded reconstruction errors through the latent-to-data mapping and often leading to increased system complexity. In this work, we propose INRCT, a unified and end-to-end training generative framework for modality-agnostic INR modeling. Instead of operating in the latent space, INRCT performs diffusion directly in the data space by training a single INR hyper-network as a denoiser. Given noisy observations at different noise levels, INRCT predicts the INR for the corresponding clean signal, which is then rendered into data space for supervision. Our training objective coherently integrates a generation loss and a reconstruction loss to jointly support INR generation from noise and INR encoding from real signals within a single model. Extensive experiments on multiple benchmark datasets demonstrate that INRCT achieves superior generation and reconstruction performance compared to existing two-stage generative INR methods, while significantly improving the model efficiency and simplifying model design.", "tldr": "We propose INRCT, a unified and end-to-end training generative framework for modality-agnostic INR modeling, supporting both INR encoding and generation in a single model.", "keywords": ["Implicit Neural Representation", "Diffusion Models", "Consistency Training"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ed82dbe8d460eef6567a98f75c814009772fe43c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents INRCT, an end-to-end, modality-agnostic generative framework that unifies implicit neural representation (INR) encoding and generation using a single model via consistency training. The core innovation is to replace the standard two-stage latent diffusion pipeline with a hyper-network trained as a denoiser, performing consistency-based few-step diffusion directly in data space and outputting INR parameters. This single model supports both INR encoding (signal-to-INR) and INR generation (noise-to-INR), with a cross-modal loss that combines generation and reconstruction objectives. Experiments on standard image and 3D generative modeling benchmarks demonstrate improvement over prior two-stage diffusion-based INR methods in both generation quality, reconstruction fidelity, and efficiency, and show qualitative flexibility including multi-resolution and single-image-to-3D generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Instead of using two separate models like previous works, INRCT achieves both INR encoding and generation within a single, end-to-end trainable hyper-network, simplifying the overall architecture and reducing compounded error from decoupled stages.\n\n2. The method is versatile—demonstrating applicability across images and 3D NeRF scenes. The cross-modal training objective extends consistency training to multiple domains.\n\n3. Experiment results (Table 1, Table 2, and Table 3) show that for both image (CIFAR-10, CelebA-HQ) and 3D object (SRN Cars) tasks, INRCT matches or exceeds the FID, IS, PSNR, and other relevant metrics of two-stage approaches, while also being faster (Table 6, Table 8). Ablation analysis (Table 5) substantiates the contribution of each component."}, "weaknesses": {"value": "1. While the proposal to unify encoding and generation is well-motivated, most core technical elements (such as probability flow ODE, consistency loss, hyper-networks for INR generation) are adaptations of previous works (Song et al., 2023; Geng et al., 2024; Dupont et al., 2022a; You et al., 2023; Park et al., 2024). The direct application of these ideas is potentially incremental and should be somewhat more deeply contrasted in the method.\n\n2. As acknowledged in Section D, experiments are only performed on low-resolution (CIFAR-10, $32\\times32$, CelebA-HQ $64\\times64$) and fairly simple NeRF scenes. The method’s claimed scalability and flexibility to more complex generative tasks remains speculative. \n\n3. Although the method is proposed as scalable (L938), the authors themselves note that training in high-dimensional data space is still a substantial challenge. A more concrete discussion (or at least an experiment with larger-scale images or more complex NeRFs) would be welcome."}, "questions": {"value": "1. Can the authors elaborate, with more quantitative estimates or empirical evidence, on the memory and runtime consequences of applying INRCT to higher-resolution or more complex domains? Could hybrid latent-data approaches (as hinted in Section D) be unified with INRCT?\n\n2. Why is the combination $\\mathcal{L}{\\text {gene }}$, $\\mathcal{L}{\\text {recon }}$, and $\\mathcal{L}_{\\text {diff }}$ superior to standard consistency loss or other combinations? Are there settings where the diffusion loss, in particular, might hinder convergence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "I83sXeEzW4", "forum": "eajZxJm9gt", "replyto": "eajZxJm9gt", "signatures": ["ICLR.cc/2026/Conference/Submission16468/Reviewer_wcDN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16468/Reviewer_wcDN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761118975463, "cdate": 1761118975463, "tmdate": 1762926577770, "mdate": 1762926577770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely appreciate the valuable feedback provided by the reviewers. We are happy that the paper received two positive and encouraging assessments. However, we were surprised by one reviewer’s significantly lower overall rating, which appeared inconsistent with the content of the written comments. To further strengthen the manuscript, we decide to withdraw the submission."}}, "id": "bes8uAMhjL", "forum": "eajZxJm9gt", "replyto": "eajZxJm9gt", "signatures": ["ICLR.cc/2026/Conference/Submission16468/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16468/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732994592, "cdate": 1763732994592, "tmdate": 1763732994592, "mdate": 1763732994592, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces INRCT (Implicit Neural Representation Consistency Training), a novel end-to-end framework for encoding and generating signals using Implicit Neural Representations (INRs).  \n- The main goal of the method is to unify INR encoding and generation into a single end-to-end trainable model, avoiding the limitations of existing two-stage approaches that rely on separate encoding and diffusion models.  \n- INRCT uses a hyper-network to directly map noisy observations to the INR parameters of the clean signal.  \n- It leverages consistency training from diffusion models to enable few-step generation.  \n- The model is trained with a combined objective:  \n  - Generation loss: Enforces consistency between any two adjacent noisy points in the PF-ODE.  \n  - Reconstruction loss: Enables accurate INR encoding from clean signals at the boundary time-steps.\n  - Diffusion loss at smaller time-steps for stabilizing the training.\n- Evaluated on CIFAR-10, CelebA-HQ, and SRN Cars datasets for image and 3D NeRF generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written.  \n- First end-to-end framework for INR encoding and generation.  \n- Modality-agnostic: applicable to 2D images and 3D NeRFs.  \n- Supports any-resolution image generation and single-image-to-3D NeRF synthesis.  \n- Outperforms existing INR-based generative models (e.g., Functa, mNIF, DDMI) in both generation quality (FID, IS) and efficiency (NFE, latency).  \n- End-to-End Training: Eliminates error accumulation from separate encoding and diffusion stages.  \n- Efficiency: Few-step (even one-step) generation reduces inference time significantly.  \n- Achieves better or competitive FID, IS, and PSNR scores across datasets.  \n- Combines multiple losses and a phased training scheduler for stable convergence.  \n- Ablation studies examining each objective term is provided."}, "weaknesses": {"value": "- Scalability: Tested only on low-resolution images and simple 3D scenes.  \n- Still lags behind some discrete image generation methods (e.g., CT) in FID on CIFAR-10.  \n- The paper claims generalization to any resolution. Yet when examining the results in Figure 4b, it is obvious that the method does not generalize well beyond the training resolution (64\\$\\times\\$64).   \n- Mapping between data space and INR space introduces optimization challenges that does not worth the effort.  \n- The paper does not present any evaluation of method on \\$NFE>2\\$."}, "questions": {"value": "- As far as I can tell there is no proper definition of \\$\\\\{c_i\\\\}\\$ (the input of the INR function) in the paper.  \n- How does the method perform when examined on scenarios where \\$NFE>2\\$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DwJ4iLVHv4", "forum": "eajZxJm9gt", "replyto": "eajZxJm9gt", "signatures": ["ICLR.cc/2026/Conference/Submission16468/Reviewer_Cpyd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16468/Reviewer_Cpyd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660848372, "cdate": 1761660848372, "tmdate": 1762926577235, "mdate": 1762926577235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces INRACT, an end-to-end generative framework using Implicit Neural Representations (INRs). INRACT operates diffusion on raw data and leverages consistency training to achieve fast inference. Given a source and a target observation set, the framework first encodes noisy source data into INR representation, then uses the INR function of the target to decode it back to raw data domain. This whole stack acts as the denoiser and subjects to consistency training. In addition, reconstruction is added as an auxiliary loss to stablize training and achieve better result. The paper shows improved performance compared to other INR based methods, although the performance still falls short compared to non-INR based diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper explores a very different approach for generative modelling which uses INR and diffusion in the raw data domain."}, "weaknesses": {"value": "Scalability seems to be an issue. It's unclear whether this approach can scale to larger models or high resolution data. \n\nThe paper doesn't convincingly show the benefits of operating through INR representation."}, "questions": {"value": "1. For diffusion loss at small time-steps (line 300), is the implementation equivalent to extending the reconstruction loss for $T_q(M_{G_θ(O^s_t,t)}) \\quad \\forall t < t_{\\epsilon}$ ?\n\n2. There is inconsistency in using $s$ and $q$ as subscript or superscript. E.g. eq 12 use q as subscript while line 301 uses as superscript.\n\n3. Since the consistency model parameterization (eq 3) doesn't fully satisfy the need of INRACT (the need of adding reconstruction loss), could the authors comment on whether there are other parameterizations that might work out better for INRACT?\n\n4. What is the concrete form of $w(t)$ used in training?\n\n5. Could the authors provide more discussion about CelebA-HQ samples from INRACT? While authors argue that the samples are more \"realistic and diverse\" (line 421), it feels to me that they are less coherent globally and often look less natural.\n\n6. In table 5, it seems that adding reconstruction loss alone at $t=0$ has a worse performance than not adding it at all. Could the authors comment on this?\n\n7. Could the authors discuss more on the issue of scalability? It seems to me that using latent or lower resolution as source would defeat the purpose of diffusion on raw data, as both result in loss of information.\n\n8. Could the authors provide some experimental results on sampling speed and high resolution generation performance compared against diffusion or consistency model? The authors advocate that one of INR's benefits is the flexibility of generating samples at larger resolution, but how does it perform compared to interpolation a diffusion/consistency model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6KMHuxDJRX", "forum": "eajZxJm9gt", "replyto": "eajZxJm9gt", "signatures": ["ICLR.cc/2026/Conference/Submission16468/Reviewer_9wZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16468/Reviewer_9wZ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16468/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762115717468, "cdate": 1762115717468, "tmdate": 1762926575045, "mdate": 1762926575045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}