{"id": "uTderO7iyk", "number": 21062, "cdate": 1758313314892, "mdate": 1759896944308, "content": {"title": "LangPert: LLM-Driven Contextual Synthesis for Unseen Perturbation Prediction", "abstract": "Predicting cellular responses to previously unseen genetic perturbations remains a fundamental challenge in computational biology, with broad applications in understanding gene function, disease mechanisms, and therapeutic development. Despite advances in computational approaches, developing models that generalise effectively to novel perturbations continues to be difficult. Large Language Models (LLMs) have shown promise in biological applications by synthesizing scientific knowledge, but their direct application to high-dimensional gene expression data has been impractical due to numerical limitations. We propose LangPert, a novel hybrid framework that leverages LLMs to guide a downstream k-nearest neighbors (kNN) aggregator, combining biological reasoning with efficient numerical inference. We demonstrate that LangPert achieves state-of-the-art performance on single-gene perturbation prediction tasks across multiple datasets.", "tldr": "We propose a hybrid model, LangPert, that combines LLMs with kNN to predict unseen gene perturbation effects with state-of-the-art performance", "keywords": ["Large Language Models", "Genomics applications", "Cellular perturbation prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b640b596bff67c345fbc85a3885b5bd1ce7aa11.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors leverage a pretrained LLM to predict cellular responses to unseen genetic perturbations. Given a set of train perturbation labels, they prompt the LLM to output a set of $k$ functionally related neighbors for a test perturbation $x$. The final prediction for $x$ is the average of these neighbors' gene expression vectors. The authors evaluate their method using two perturbation datasets and show that their approach outperforms prior works."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors perform cross-validation for all experiments and also include a data-scaling analysis, where models are compared across different sizes of the train set.\n- The analysis of how different LLMs affect the performance of LangPert is insightful and useful for the community."}, "weaknesses": {"value": "- A significant limitation of this study is the absence of direct, simpler baselines that would be essential for validating the paper's central hypothesis. To isolate the LLM's contribution, the authors should have compared LangPert to simpler kNN aggregators guided by structured biological knowledge, such as a Gene Ontology similarity search or PPI graphs (e.g., SPACE embeddings [1]).\n- The authors evaluate performance by calculating metrics on the top 20 DE genes. While it is true that this has been employed by other works such as GEARS, performance on this small subset may not be representative of the model's accuracy across the transcriptome. Furthermore, this metric is asymetric as DE genes are computed in the ground truth, not the model's predictions, hence, it does not properly penalize \"false positives\". I'd recommend including metrics computed on the full gene set in the appendix to provide a more complete assessment of performance.\n- The authors claim that the LLM identifies the appropriate similarity criterion based on the context, however, there is no quantification of this claim. Only a few examples are given and the paper does not correlate the type of reasoning strategy with predictive accuracy for different classes of genes. Without this quantitative validation, the claim remains anecdotal.\n\nOverall, given the number of methods being published in this space, the paper's benchmarking needs to be more rigorous by including other key foundation models and simpler baselines.\n\n[1] https://academic.oup.com/bioinformatics/article/41/9/btaf496/8250101"}, "questions": {"value": "- Can the authors comment on the possibility of information leakage in the LLM's training data? The data is from 2022, with many papers discussing this specific dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "w7itpVk5LQ", "forum": "uTderO7iyk", "replyto": "uTderO7iyk", "signatures": ["ICLR.cc/2026/Conference/Submission21062/Reviewer_7FLC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21062/Reviewer_7FLC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712181212, "cdate": 1761712181212, "tmdate": 1762940633313, "mdate": 1762940633313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LangPert, a hybrid framework that integrates large language models (LLMs) with a k-nearest neighbors (kNN) aggregator to predict transcriptional responses to *unseen* genetic perturbations. The core idea is to decouple reasoning and computation: LLMs identify biologically related perturbations by reasoning over literature and gene-function context, while kNN performs the numerical aggregation of expression responses. LangPert is evaluated on Perturb-seq datasets (K562 and RPE1 cell lines) and demonstrates state-of-the-art performance across MAE, MSE, and correlation metrics, outperforming models such as scGPT, GEARS, and GP+LLM. The authors further analyze how LLMs perform context-dependent reasoning, adapting their similarity criteria based on biological context (e.g., pathway, complex, or functional process), and show that stronger LLMs consistently yield better results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Novel hybrid architecture:** Combining LLM-guided reasoning with a numerical aggregator is a clear and effective way to address the high-dimensional limitations of LLMs.  \n- **Well-motivated and positioned:** The paper clearly identifies the shortcomings of prior VAE- and transformer-based models (e.g., scGPT, GEARS) that cannot generalize to unseen perturbations.  \n- **Strong empirical performance:** Across both K562 and RPE1 datasets, LangPert achieves consistent improvements in MAE and MSE over prior methods, while maintaining strong correlation metrics.  \n- **Interpretability and biological faithfulness:** The reasoning examples (e.g., MTOR, EIF3E, PSMD11) show biologically plausible relationships, indicating that LangPert’s outputs are consistent with real biological organization.  \n- **Comprehensive evaluation across LLMs:** The comparison across multiple backbones (Claude, OpenAI o1, DeepSeek, Llama) establishes the generality and robustness of the approach.  \n- **Good clarity and presentation:** The paper is well structured and clearly written."}, "weaknesses": {"value": "- **Limited novelty in aggregation:** While the LLM-guided kNN idea is creative, the aggregation component (simple averaging) is rudimentary. More sophisticated probabilistic or weighted methods could potentially improve performance, as the authors also acknowledge.  \n- **Dependence on proprietary LLMs:** The method relies on large closed-weight models (Claude, OpenAI o1), which limits reproducibility and accessibility. The reported results for open-weight LLMs indicate a notable performance drop.  \n- **Evaluation limited to single-gene perturbations:** Since performance appears largely dependent on the LLM’s knowledge base rather than the aggregation method, it remains unclear whether similar reasoning holds for multi-gene or combinatorial perturbations. Such settings could amplify LLM limitations (e.g., hallucination, reasoning instability).  \n- **Lack of prompt and context ablation:** The study would benefit from an analysis of how prompt structure and contextual cues (e.g., cell-line-specific information) influence results. Because K562 and RPE1 are well-studied lines with abundant literature coverage, the LLM’s prior exposure may strongly influence outcomes.  \n- **Uncertainty quantification missing:** The results are presented as point estimates, though biological perturbations are inherently noisy. Uncertainty may stem both from the LLM’s retrieval of neighbors and from numerical aggregation; disentangling these sources would make the framework more informative and reliable."}, "questions": {"value": "1. How sensitive is LangPert’s performance to the number of neighbors $(k)$ and the specific prompt formulation? Were these tuned per dataset or fixed globally?  \n2. Have you quantitatively validated whether LangPert’s selected gene subsets overlap with known biological interaction networks (e.g., STRING, Reactome)?  \n3. Given that performance depends heavily on LLM reasoning, would incorporating literature-derived embeddings (as in GP+LLM) into the aggregation step improve or destabilize predictions?  \n4. How well would LangPert generalize to combinatorial perturbations involving multiple genes, and does the reasoning quality of LLMs degrade in these more complex contexts?  \n5. Since fine-tuned open-weight models such as TxGemma still lag behind frontier LLMs, do the authors see potential in retrieval-augmented strategies or hybrid pipelines (e.g., combining smaller LLMs with domain-specific retrieval) to narrow the performance gap without full reliance on closed models?\n6. Beyond predictive accuracy, how reproducible are the LLM-derived reasoning traces? For instance, do repeated runs with identical prompts yield consistent gene sets, or is there notable stochasticity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8WW2KLm4sL", "forum": "uTderO7iyk", "replyto": "uTderO7iyk", "signatures": ["ICLR.cc/2026/Conference/Submission21062/Reviewer_4QA6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21062/Reviewer_4QA6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738513848, "cdate": 1761738513848, "tmdate": 1762940633008, "mdate": 1762940633008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a core problem in perturbation biology:\nGiven a gene knockout (or other genetic perturbation) that the model has never seen before, can we predict that perturbation’s full transcriptional effect (i.e. the differential expression vector across thousands of genes in that cell type)? \n\nThe authors propose LangPert, a hybrid method that combines:\n1. An LLM “reasoner” — given a novel perturbation (e.g. knock out gene X) and a library of previously tested perturbations, the LLM selects a biologically relevant subset of training perturbations that it thinks will behave similarly. This uses biological prior knowledge (literature, pathway membership, complex membership, feedback loops, etc.). \n\n2. A numerical aggregator (kNN-style) — the model then averages the actual measured expression shifts for those selected neighbors to produce a prediction for the unseen perturbation. No high-dimensional generation is done by the LLM itself.\n\nThe authors evaluate LangPert on large Perturb-seq datasets (K562 and RPE1 cell lines, >1000 perturbations each). They compare against:\nscGPT fine-tuning, GEARS (a GNN using gene–gene priors), GP+LLM (Gaussian Process using LLM-derived gene embeddings)\nA surprisingly strong baseline: the mean non-control response (just average observed shifts from other perturbations)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong empirical results vs. tough baselines\nThe paper does not cherry-pick easy baselines. It compares against GEARS (graph prior), GP+LLM (Gaussian Process + LLM-derived embeddings), and even the “non-control mean,” which is known to be obnoxiously competitive in this domain. LangPert consistently wins on MAE and MSE and usually wins or ties on correlation, across two very different cell lines. \nThis is the most convincing kind of win: beating “dumb but annoyingly good” baselines.\n2. Generalizes to unseen perturbations\nMost older VAE-style perturbation models just learn embeddings for perturbations they’ve actually seen. That means they basically can’t extrapolate to a new gene. LangPert can, by construction, because it never needed to learn a continuous latent for that gene — it just reasons about which known genes are most similar and aggregates their measured effects. \nThat’s exactly the scientific use case: “I haven’t assayed gene X yet — predict it anyway.”"}, "weaknesses": {"value": "1. Heavy dependence on external LLMs\nPerformance depends on using a strong LLM (e.g., Claude 3.5 Sonnet). That raises:\nReproducibility concerns under double-blind / future access restrictions.\nFairness concerns: do labs without frontier API access get worse science?\nThe paper does explore smaller LLMs, which is good, but reproducibility and openness will almost certainly come up in ICLR discussion. \n\n2. No explicit handling of uncertainty\nThe kNN-style averaging will always return some answer, even if the LLM’s “neighbors” are a bad match. There’s no uncertainty score, confidence interval, or abstain option. That’s risky in experimental design, where false confidence can waste wet-lab time.\nThis feels solvable (bootstrap over neighbors, entropy over LLM-selected sets, etc.) but it’s not addressed.\n\n3. Biological scope is still single-gene, steady-state transcriptomics\nThe paper evaluates only single-gene perturbations in two immortalized human cell lines (K562, RPE1). No combos (A+B knockouts), no time series, no protein-level phenotypes, no spatial interactions. That’s fine scientifically (still a hard task), but the Discussion gestures toward “broad cellular systems modeling,” which is a little ahead of where the method has actually been tested."}, "questions": {"value": "1. Reasoning vs Retrieval\nSince LangPert doesn’t fine-tune the LLM, how much of the gain comes from retrieval-style reasoning versus latent biological knowledge inside the pretrained LLM?\nDoes the LLM’s reasoning produce genuinely novel pairings or mostly recover known relationships (protein complexes, shared pathways)?\n\n2. LLM Prompting and Stability\nHow sensitive are results to prompt phrasing, temperature, or the number of retrieved “neighbor” perturbations (k)?\nHave you evaluated reproducibility under re-prompting the same LLM (variance across runs)?\n\n3. Metrics and Evaluation\nWhy limit evaluation to the top 20 DE genes? Does LangPert still outperform baselines genome-wide?\nAre results consistent across genes with small but biologically relevant changes (e.g., transcription factors)?\nHow do errors vary by gene function or perturbation type (TF vs enzyme vs chaperone)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BvH1f0g8oN", "forum": "uTderO7iyk", "replyto": "uTderO7iyk", "signatures": ["ICLR.cc/2026/Conference/Submission21062/Reviewer_jUhm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21062/Reviewer_jUhm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802762466, "cdate": 1761802762466, "tmdate": 1762940632701, "mdate": 1762940632701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework, LangPert, that uses Large Language Models (LLMs) with k-nearest neighbors (kNN) for predicting cellular responses to unseen genetic perturbations.\nInstead of directly modeling gene expression vectors, the model uses the LLM to identify biologically relevant perturbations in the training set and aggregates their corresponding expression profiles using kNN. Finally, they show that LangPert outperforms baselines such as GEARS and scGPT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It is interesting to see how LLMs could perform as good and in some cases better than models used in this domain."}, "weaknesses": {"value": "- The contribution and findings of this paper is more suitable for a workshop paper rather than a full conference paper. \n- This paper does not include a systematic study of why the LLM makes certain biological choices.\n- The LLMs used in this paper were all trained (or fine-tuned) on large portions of the public internet, biomedical papers, and open-access databases e.g. they might have even been trained on datasets derived from the same cell lines (e.g., K562, RPE1) or papers describing the very Perturb-seq results used in evaluation. So when LangPert asks, “Which genes are similar to MTOR in K562?”, the LLM may be recalling previously reported associations rather than reasoning biologically de novo. This is a very critical problem.\n- Some baselines in this domain such as sclambda and PRESAGE are missing in the evaluation setting."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cc61Up4X1Q", "forum": "uTderO7iyk", "replyto": "uTderO7iyk", "signatures": ["ICLR.cc/2026/Conference/Submission21062/Reviewer_mbmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21062/Reviewer_mbmS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21062/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938749274, "cdate": 1761938749274, "tmdate": 1762940632421, "mdate": 1762940632421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}