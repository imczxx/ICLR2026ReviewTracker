{"id": "KsxSp2LMlo", "number": 6299, "cdate": 1757965394593, "mdate": 1759897924015, "content": {"title": "Red-Teaming NSFW Image Classifiers as Text-to-Image Safeguards", "abstract": "Not Safe for Work (NSFW) image classifiers play a critical role in safeguarding text-to-image (T2I) systems. However, a concerning phenomenon has emerged in T2I systems -- changes in text prompts that manipulate benign image elements can result in failed detection by NSFW classifiers -- dubbed \"*context shifts*.\" For instance, while a NSFW image of *a nude person in an empty scene* can be easily blocked by most NSFW classifiers, a stealthier one that depicts *a nude person blending in a group of dressed people* may evade detection.  How to systematically reveal NSFW image classifiers' failure against context shifts? \n\nTowards this end, we present an automated red-teaming framework that leverages a set of generative AI tools.  We propose an **exploration-exploitation** approach: First, in the *exploration* stage, we synthesize a diverse and massive 36K NSFW image dataset that facilitates our study of context shifts. We find that varying fractions (e.g., $4.1$% to $36$% nude and sexual content) of the dataset are misclassified by NSFW image classifiers like GPT-4o and Gemini. Second, in the *exploitation* stage, we leverage these failure cases to train a specialized LLM that rewrites unseen seed prompts into more evasive versions, increasing the likelihood of detection evasion by up to 6 times. Alarmingly, we show **these failures translate to real-world T2I(V) systems**, including DALL-E 3, Sora, Gemini, and Grok, beyond the open-weight image generators used in our red-teaming pipeline. For example, querying DALL-E 3 and Imagen 3 with prompts rewritten by our approach increases the chance of obtaining NSFW images from $0$ to over $44$%.", "tldr": "We present a systematic framework to automatically red-team NSFW image classifiers (which are often deployed as text-to-image system safeguards) against image context shifts.", "keywords": ["red-teaming", "NSFW classification", "generative AI", "robustness", "adversarial attacks", "LLM", "T2I"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7aed3d2483bcb8bbfd18c4bfdb7742efbfda2c52.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an automated red-teaming framework to expose vulnerabilities of NSFW image classifiers when facing context shifts. The framework follows an exploration–exploitation paradigm. In the exploration stage, the authors create a 36K NSFW dataset. In the exploitation stage, they fine-tune a LLM rewriter that learns from these failure cases to craft new prompts that further evade detection. Finally, the authors show that fine-tuning classifiers on red-teamed data improves robustness against such attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed exploration–exploitation pipeline, along with the synthetic NSFW dataset, could be valuable for future research.\n2. The paper explores the vulnerabilities of NSFW image classifiers from a new perspective, context shifts—benign visual elements that can hide unsafe content. \n3. The proposed method proves effective against commercial T2I models."}, "weaknesses": {"value": "1. The choice of baselines seems weak. The authors should include comparisons with more advanced red-teaming methods like SneakyPrompt[1], ART[2], etc, and human-written red teaming datasets i2p[3], Adversarial Nibbler[4], etc.\n2. The paper lacks sufficient discussion on how NSFW content was verified during dataset construction. Were all the images verified automatically or manually? It seems unreasonable to assume that all generated images are inherently NSFW.\n3. The idea of fine-tuning an LLM to generate red-teaming prompts against LLMs or T2I models is not novel—multiple previous works like MART[5] or ART[2] already employ supervised, RL or multi-round adversarial LLM setups for this purpose.\n\n[1]Yang, Yuchen, et al. \"Sneakyprompt: Jailbreaking text-to-image generative models.\" 2024 IEEE symposium on security and privacy (SP). IEEE, 2024.\n\n[2]Li, Guanlin, et al. \"ART: automatic red-teaming for text-to-image models to protect benign users.\" Advances in Neural Information Processing Systems 37 (2024): 91184-91219.\n\n[3]https://huggingface.co/datasets/AIML-TUDA/i2p\n\n[4]Quaye, Jessica, et al. \"Adversarial nibbler: An open red-teaming method for identifying diverse harms in text-to-image generation.\" Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. 2024.\n\n[5]Ge, Suyu, et al. \"Mart: Improving llm safety with multi-round automatic red-teaming.\" arXiv preprint arXiv:2311.07689 (2023)."}, "questions": {"value": "1. Since the rewriter model is fine-tuned only on nude and violent scenes, can it generalize to rewriting other NSFW types, such as drug abuse or weapons?\n2. In Section 4.1, when constructing the NSFW dataset, how to determine whether the generated images are NSFW or not, through automatic filtering or manual verification?\n3. How does the method handle borderline or ambiguous cases, such as artistic nudity or implicit sexual content, during dataset construction?\n4. In Figure 12, when testing Nano Banana, why is the word “nude” rewritten as “nuuuude”? Is this due to keyword escaping? If “nude” were simply replaced rather than rewritten through a learning-based approach, would the attack against Nano Banana and other t2i models still be effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4BPguZsuWt", "forum": "KsxSp2LMlo", "replyto": "KsxSp2LMlo", "signatures": ["ICLR.cc/2026/Conference/Submission6299/Reviewer_tUh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6299/Reviewer_tUh4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495817995, "cdate": 1761495817995, "tmdate": 1762918601771, "mdate": 1762918601771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a concept called context shift in which a NSFW image can be missed by a safety classifier if surrounded by benign content. To test current safety classifiers against this issue, authors propose a new exploration-exploitation red teaming approach. In the exploration stage, authors create a diverse NSFW image dataset eligible for context shift. In the exploitation stage, they use the failures found in the exploration stage to train a LLM that rewrites prompts into stealthy version to evade detection by the classifier. Authors perform experiments to show the effectiveness of the approach against various models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and is easy to follow.\n2. The paper studies an important and timely problem.\n3. The paper performs a comprehensive set of experiments."}, "weaknesses": {"value": "1. While the paper studies an important problem, it lacks technical rigor. The novelty of the work is also not significant.\n2. It is not clear how it is verified if an image is indeed NSFW based on the image generated by the T2I model. I think a human in the loop approach is necessary here. However, in that case the approach might not be as scalable. I wonder if authors have some ideas on this.\n3. Some large scale human verification is needed that are unbiased and preferably done by not the authors of the paper to not bias the outcomes.\n4. The training of the rewriting LLM can be expensive perhaps there might be more efficient and clever ways to achieve context shift which might be interesting to explore. \n5. The studied baselines are really simple and naive. Authors could have used more sophisticated and better baselines to compare their work to (e.g., using current SOTA red teaming approaches for T2I models)."}, "questions": {"value": "It is not clear how it is verified if an image is indeed NSFW based on the image generated by the T2I model. I think a human in the loop approach is necessary here. However, in that case the approach might not be as scalable. I wonder if authors have some ideas on this."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1lFwHTAfNw", "forum": "KsxSp2LMlo", "replyto": "KsxSp2LMlo", "signatures": ["ICLR.cc/2026/Conference/Submission6299/Reviewer_H1be"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6299/Reviewer_H1be"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677302565, "cdate": 1761677302565, "tmdate": 1762918600387, "mdate": 1762918600387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the robustness of NSFW image classifiers that act as safety mechanisms in T2I systems. It identifies a key vulnerability termed *context shifts*, where the inclusion of benign visual elements can cause unsafe images to be misclassified as safe. To systematically study this issue, the authors propose an automated red-teaming framework based on an exploration–exploitation paradigm. The approach leverages large language models and generative image models to synthesize diverse NSFW datasets and to learn prompt rewriting strategies that expose classifier weaknesses. The study further demonstrates that these vulnerabilities can transfer to commercial T2I and text-to-video systems, highlighting potential real-world safety risks. The authors also explore preliminary mitigation strategies, showing that fine-tuning on red-teamed data can improve classifier robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper addresses a timely and practically important research topic.\n+ The experiment is comprehensive, spanning multiple classifier families, black-box evaluation, and transfer across generators and commercial deployments.\n+ The paper is generally clear and well organized."}, "weaknesses": {"value": "- Most discoveries are made with one primary generator and one captioner, then transferred to a second generator. This leaves open whether the rewriter has overfit to generator/captioner characteristics (style priors, texture biases). Incorporating real-image benchmarks and mixed real/AI distributions (as in UnsafeBench) and swapping captioners (e.g., BLIP/LLaVA vs. GPT) would better establish external validity.\n- Following the former point, the rewriter may face similar issues: it is trained on captions generated by a single powerful MLLM (the paper uses GPT-4o as the captioner). That captioner encodes its own priors about what is salient or benign/unsafe, and those priors may overlap with the decision boundary of the target classifiers (or the deployed safeguards). As a result, the rewriter may be exploiting captioner–classifier shared biases rather than learning generalizable evasive strategies.\n- For Step in Section 4.1, the paper assumes that the LLM-generated extensions appended to unsafe seed prompts introduce only benign contextual elements, yet it provides no explicit mechanism to verify this assumption. Without filtering or auditing, some extensions may inadvertently add new unsafe or borderline content, confounding the interpretation of context shifts. In such cases, classifier misclassifications could stem from changes in NSFW intensity rather than the presence of neutral context. Incorporating automated text-level moderation, constrained generation templates, or human annotation to confirm benignity would make the causal link between context and classifier failure more credible.\n- In Section 5.3, the paper reports success rates of the jailbreak against commercial T2I systems, but omits comparisons with existing attack frameworks such as SneakyPrompt [1], PGJ [2], or MMA-Diffusion [3]. These works similarly target deployed generative systems and quantify jailbreak success and efficiency under real-world constraints. Without benchmarking against them, it remains unclear whether the proposed LLM-based rewriting offers superior effectiveness or simply reproduces known vulnerabilities under a different pipeline.\n- The empirical design does not fully isolate whether benign contextual modifications are the true cause of classifier failures. Stronger controls (e.g., counterfactual tests where only a single benign element varies) would better establish causality and rule out confounds from generator stochasticity or style variation, a known issue in semantic adversarial evaluation [4].\n\n## References\n- [1] Yuchen Yang et al. SneakyPrompt: Jailbreaking Text-to-image Generative Models. IEEE S&P 2024.\n- [2] Yihao Huang et al. Perception-Guided Jailbreak Against Text-to-Image Models. AAAI, 2025.\n- [3] Yijun Yang et al. MMA-Diffusion: MultiModal Attack on Diffusion Models. CVPR 2024.\n- [4] Dan Hendrycks et al. Natural Adversarial Examples. CVPR 2021."}, "questions": {"value": "- How do you verify that the LLM-extended prompts truly introduce **benign** rather than additional unsafe content? Did you apply any automated moderation or human auditing to confirm that the added contextual elements do not alter NSFW intensity?\n- Given that both dataset creation and the rewriter training rely on a single generator (SDXL) and a single captioner (GPT-4o), how do you ensure that your findings are not specific to these components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wCXEukmJP1", "forum": "KsxSp2LMlo", "replyto": "KsxSp2LMlo", "signatures": ["ICLR.cc/2026/Conference/Submission6299/Reviewer_vEpg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6299/Reviewer_vEpg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6299/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899444194, "cdate": 1761899444194, "tmdate": 1762918599893, "mdate": 1762918599893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}