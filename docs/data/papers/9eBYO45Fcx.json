{"id": "9eBYO45Fcx", "number": 21851, "cdate": 1758322676484, "mdate": 1759896899873, "content": {"title": "Towards Robust Out-of-Distribution Generalization for Deep Neural Networks with Tailored Data Regularization", "abstract": "Out-of-Distribution (OOD) generalization remains both a fundamental challenge and an often-overlooked aspect of modern machine learning—especially in the context of Deep Neural Networks (DNNs), which are highly expressive yet prone to overfitting under distributional stress. Classical learning theory highlights the role of regularization in managing the bias-variance trade-off—particularly important for complex models with higher VC dimension. In this work, we explore stochastic data regularization techniques—such as random transformations and noise injection—applied not only as isolated strategies but also organized through a Scheduling Policy framework using a Curriculum Learning-based approach. By progressively increasing input difficulty during training, the scheduling aligns model capacity with task complexity, promoting more robust generalization. We also propose a novel statistical procedure to assess the consistency of performance estimates across cross-validation folds, mitigating miscoverage effects in confidence interval estimation. Altogether, our findings highlight the importance of a tailored data regularization, where the selection, combination, and scheduling of perturbations become key to achieving OOD robustness in DNNs.", "tldr": "", "keywords": ["deep neural networks", "noise injection", "out-of-distribution", "regularization", "data augmentation"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f9f3bce93945d41b381cfa2e5b3178ab137c149.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper systematizes probabilistic data regularization methods aimed at improving DNN robustness to perturbations and empirically evaluates a curriculum-inspired scheduling approach. It further proposes a new severity metric for corruptions based on the KL divergence between internal representations of an autoencoder, and introduces an evaluation protocol intended to overcome the miscoverage issues inherent to standard K-fold CV. Experiments on CIFAR-10 and CIFAR-10-C with three model families show that stronger regularization yields better generalization under the proposed evaluation, and that the proposed scheduling approach outperforms other baselines only for the relatively small model (ResNet20)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Systematization of probabilistic data regularization: The paper organizes existing common regularizers into three application modes—selective application, combination, and curriculum-style scheduling of regularization strength—an organization that seems practically useful.\n- KL-based difficulty measure for corruptions: The paper quantifies perturbation distance from the training distribution using KL divergence computed over an autoencoder’s internal representations.\n- Toward a more essential evaluation protocol: The proposed protocol attempts to address coverage reliability rather than focusing solely on average performance."}, "weaknesses": {"value": "- Limited novelty: Robustness via data regularization has a long history, and its effectiveness is widely known. While a systematic combination and evaluation of heterogeneous regularizers is less common, the paper does not demonstrate performance gains large enough to make this viewpoint a substantial contribution on its own.\n- Narrow experimental scope: Validation is restricted to CIFAR-10/CIFAR-10-C. Demonstrating sizable improvements on broader datasets (e.g., CIFAR-100, ImageNet) would make the claims more solid.\n- Unconvincing results for scheduling: The curriculum-style regularization shows benefits primarily for ResNet20. On WideResNet-28-10 and CCT, the scheduling approach is not clearly superior—and sometimes underperforms—non-scheduling variants. As a result, the main takeaway appears to be simply that “using regularization helps,” rather than that curriculum-style scheduling is consistently better."}, "questions": {"value": "- Performance appears roughly correlated with training epochs; the success of scheduling on ResNet20 might be due to longer training rather than the curriculum mechanism in itself. What happens under a fixed-epoch comparison?\n\n- Beyond F1, incorporating calibration metrics (e.g., ECE, Adaptive ECE) would make the evaluation more compelling.\n\n- Some figure/table references seem incorrect:\n  - Line 211: Figure 3 → Figure 1\n\n  - Line 318: Figure 5 → Figure 3\n\n  - Line 401: Figure 5 → Figure 4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6X1raOCZUa", "forum": "9eBYO45Fcx", "replyto": "9eBYO45Fcx", "signatures": ["ICLR.cc/2026/Conference/Submission21851/Reviewer_oidv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21851/Reviewer_oidv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760517734615, "cdate": 1760517734615, "tmdate": 1762941956517, "mdate": 1762941956517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a data augmentation framework for OOD generalization. The proposed framework comprises three ingredients: selection policies that stochastically select data transformations, combination policies that combine selected data transformations sequentially, and a scheduling policy that serves as a curriculum learning framework. The proposed method is evaluated on CIFAR-10-C."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "There is little technical contribution in this paper. In the actual implementation, \"selection policy\" uses RandAugment as is, \"combination policy\" combines RandAugment and Gaussian/Salt-and-Pepper noise injection, and \"scheduling policy\" is a standard curriculum learning setup based on pre-defined early stopping criteria. Moreover, adding the final scheduling policy results in little performance gains on CIFAR-10-C, which is itself a small-scale OOD generalization benchmark that should not be used alone for benchmarking. I think the paper at its current stage is far from the bar of ICLR."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "22v0JnUAWA", "forum": "9eBYO45Fcx", "replyto": "9eBYO45Fcx", "signatures": ["ICLR.cc/2026/Conference/Submission21851/Reviewer_yQiq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21851/Reviewer_yQiq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737398468, "cdate": 1761737398468, "tmdate": 1762941956197, "mdate": 1762941956197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a tailored data augmentation strength strategy to improve OOD classification of deep networks in OOD cases. The paper evaluates two convolutional networks on CIFAR-10C."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**S1.** The paper is tackling a relevant problem: improving OOD classification performance.\n\n**S2.** The paper is well-written and easy to follow. **"}, "weaknesses": {"value": "**W1.** Lack of novelty: The methodology presented here strongly resembles the curriculum learning strategy from [1], which is not even cited in the paper despite its significant similarity (see the next weakness).\n\n[1] EfficientNetV2: Smaller Models and Faster Training. ICML 2021  \n\n**W2.** Missing related work: The related work section is extremely poor, with as few as 8 references, considering the importance of topics such as out-of-distribution categorization/curriculum learning, and its wide popularity within the field.\n\n**W3.** Missing previously published methodology: The analysis of section 3.2 (Algorithm 2) to characterize distance among ID and OOD samples does not employ previously published and well-known approaches for this task [2].\n\n[2] OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization. CVPR 2021\n\n**W4.** Narrow experimental setup: The empirical tests are performed on a single dataset (CIFAR10) and corresponding corrupted versions (CIFAR10C), not meeting expectations from an empirical perspective for a conference paper at ICLR.\n\n**W5.** Figures' layout (pages 7 and 8) is not suitable for a conference of the ICLR level."}, "questions": {"value": "I appreciate the authors’ effort and recognize the relevance of the topic.\n\nHowever, the main concerns (limited novelty, missing literature coverage, and lack of usage of previously published methodology) reflect substantive rather than clarificatory issues. Therefore, I do not have specific questions for rebuttal that would likely alter my assessment.\nIn this form, I see the paper as more suitable for a workshop or a minor conference rather than ICLR main conference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JRMG0N5fVO", "forum": "9eBYO45Fcx", "replyto": "9eBYO45Fcx", "signatures": ["ICLR.cc/2026/Conference/Submission21851/Reviewer_NLBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21851/Reviewer_NLBW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927310961, "cdate": 1761927310961, "tmdate": 1762941955853, "mdate": 1762941955853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of out-of-distribution (OOD) generalization in deep neural networks through a modular framework for stochastic data regularization. The framework combines selection, combination, and scheduling policies to dynamically adapt data perturbations during training. By organizing noise injection and random transformations under a curriculum learning scheme, the method gradually increases perturbation strength to align model capacity with task difficulty. The paper also introduces a miscoverage-based statistical analysis to quantify the stability and reliability of performance across cross-validation folds. Experiments show that the proposed regularization scheduling improves OOD robustness and generalization stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and organized.\n2. The proposed framework is comprehensive.\n3. The author has taken a wide range of experiments to verify this method."}, "weaknesses": {"value": "1. The frameworks seems as a combination of some known ideas, such as RandAugment, noise injection, and curriculum learning, which shows limited novelty.\n2. Then experiments mainly focus on CIFAR-10, could you verify this idea on more diverse datasets and tasks?\n3. Can the proposed framework integrate or compare against adversarial training or spectral regularization techniques?\n4. How sensitive is the scheduling policy to hyperparameters like the stage count or early stopping patience?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oB1vz93dst", "forum": "9eBYO45Fcx", "replyto": "9eBYO45Fcx", "signatures": ["ICLR.cc/2026/Conference/Submission21851/Reviewer_H32E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21851/Reviewer_H32E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966143664, "cdate": 1761966143664, "tmdate": 1762941955514, "mdate": 1762941955514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}