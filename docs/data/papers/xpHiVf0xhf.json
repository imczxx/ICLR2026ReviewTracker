{"id": "xpHiVf0xhf", "number": 16772, "cdate": 1758268547591, "mdate": 1759897220279, "content": {"title": "Learning Better Certified Models from Empirically-Robust Teachers", "abstract": "Adversarial training attains strong empirical robustness to specific adversarial attacks by training on concrete adversarial perturbations, but it produces neural networks that are not amenable to strong robustness certificates through neural network verification. On the other hand, earlier certified training schemes directly train on bounds from network relaxations to obtain models that are certifiably robust, but display sub-par standard performance. Recent work has shown that state-of-the-art trade-offs between certified robustness and standard performance can be obtained through a family of losses combining adversarial outputs and neural network bounds. Nevertheless, differently from empirical robustness, verifiability still comes at a significant cost in standard performance. In this work, we propose to leverage empirically-robust teachers to improve the performance of certifiably-robust models through knowledge distillation. Using a versatile feature-space distillation objective, we show that distillation from adversarially-trained teachers consistently improves on the state-of-the-art in certified training for ReLU networks across a series of robust computer vision benchmarks.", "tldr": "We show that knowledge distillation from adversarially-trained teachers improves the state-of-the-art in deterministic certified training for ReLU networks.", "keywords": ["Certified Training", "Adversarial Robustness", "Knowledge Distillation", "Neural Network Verification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33c91793cc82ca6ab21060ce2be1f257197121e0.pdf", "supplementary_material": "/attachment/2a46567ba020b47f425e047595db5e54e7186314.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies improving the accuracy-robustness tradeoff in certified training via transferring knowledge from an adversarially trained teacher. While the idea is simple, this work proposes clean and computationally efficient ways to combine the soft-label of the teacher model into the training loss. Basically, they regularize the IBP bounds in the feature space to be close to the feature of the adversarial model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea is clean and effective. The proposed algorithm is efficient and integrates nicely into one of the SOTA certified training algorithm. The paper is well-written. The improvement on the clean accuracy is significant, with nontrivial improvement on the certified robustness."}, "weaknesses": {"value": "The algorithm is heavily dependent on CC-IBP, one of the SOTA algorithms. It is not clear how to combine the idea of knowledge transfer into other SOTA algorithms, even similar ones such as MTL-IBP. This might limit the extension of the idea, while at a simple glance all certified training algorithms should benefit from knowledge transfer.\n\nSome related work regarding certified training is missing: [1] studies the optimization difficulty of precise convex relaxations; [2] proves there exists no network encoding a simple function that may be found via certified training where a single convex relaxation such as IBP can provide precise bounds; [3] further shows that there exists certain network encoding every piecewise linear functions that may be found via certified training where layerwise multi-neuron relaxations can provide precise bounds.\n\n[1] https://arxiv.org/abs/2403.07095\n\n[2] https://arxiv.org/abs/2311.04015\n\n[3] https://arxiv.org/abs/2410.06816"}, "questions": {"value": "Besides the points raised in the weakness, please address the following technical questions.\n\n1. In Line 200, the loss controls the  deviation between the feature of the student model on the worst-case input and the **clean** feature of the teacher model. Therefore, it seems the teacher model only needs to be good at the clean features. Could the authors then show why an adversarially robust teacher might be necessary in good performance (if this is the case by intuition and the default method). Experimental comparisons are required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "j3ISBesloQ", "forum": "xpHiVf0xhf", "replyto": "xpHiVf0xhf", "signatures": ["ICLR.cc/2026/Conference/Submission16772/Reviewer_DdPg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16772/Reviewer_DdPg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760899582210, "cdate": 1760899582210, "tmdate": 1762926816696, "mdate": 1762926816696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The problem this work is trying to solve is to improve the standard performance of a certifiably robust model, so as to match an adversarially trained model. The authors propose the CC-DIST that consists of two terms. The first term is an interpolation between the upper bound (loss at the adversarial example) and the lower bound (certified loss) of the adversarial loss. The second term is a distillation loss between the adversarial bound (with interpolation) of the internal representation of the student model, and the clean representation of the teacher model. The authors tested their method on some commonly used datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Section 2 is a nice description of the background\n2. The experiment section includes a number of experiments"}, "weaknesses": {"value": "I think there are three main weaknesses: presentation, motivation, and experiments.\n\n## 1. Presentation\nThe presentation could be greatly improved. Even though I am pretty familiar with the subject, I feel overwhelmed by the notations in this manuscript that look very similar to one another. I cannot see why it is necessary to add CC as a subscript before everything, and those $\\theta_h$, $\\theta_h^t$ etc. are all very confusing.\n\nI think the idea is pretty simple. Basically there are two ways to get bounds: the empirical bound obtained from an adversarial example, and the certified bound obtained from IBP. So why not just use $h(x_{adv})$ to denote the empirical bound, $h$ with upper and lower bars to denote the certified bound, and use $h_t$ for the student model? The current notations are very hard to read.\n\nFor section 3, I think that the idea can be better demonstrated with diagrams. For example, the definition of the interpolations, the distillation loss, etc. I also don't see why Lemma 3.3 is important.\n\nRegarding the experiment section, there are an unnecessarily large number of compared methods, but some important parts are missing (such as how $\\alpha$ is selected), which I will explain in more detail later.\n\n## 2. Motivation\nThe motivation of this work is to give certified robust models the same standard accuracy as adversarially trained models. First, this is very confusing because adversarially trained models have much lower accuracy than vanilla training. Second, I cannot see how the different components in the proposed method fulfill this motivation. For example, why interpolating between the empirical and the certified bounds? Why distilling from a teacher model guarantees that the student model will have both better standard and certified adversarial performances, instead of both worse?\n\nIn other words, I find the connection between the method and the motivation very weak, and the motivation itself is questionable. I am not even sure if the central problem studied in this work is a valid one.\n\n## 3. Experiment\nFinally, the experiment section does not show strong evidence that the proposed method is doing anything interesting. Surely Table 1 shows that the proposed method, as the authors tested it, has higher standard and certified accuracy than the compared methods. But there are several issues.\n1. The reported results are not significantly better\n2. The experiment setting is very unclear. For example, I cannot find in the paper how $\\alpha$ is selected in the experiments, which seems to me to be very important\n3. The results for previous methods are reported in prior work, and it is unclear whether the improvement of the proposed method comes from a different evaluation setting. The authors themselves noted that for several methods, evaluation from later work could attain a higher accuracy, which shows that a small difference in the evaluation experiment setting could make a difference.\n\nIn summary, I lean towards rejecting this work."}, "questions": {"value": "1. How do you select $\\alpha$ in your experiments?\n2. Why do you interpolate between the empirical and certified bounds? Are you following the prior work by De Palma et al.? What does this trick buy us?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tlo0bowSGx", "forum": "xpHiVf0xhf", "replyto": "xpHiVf0xhf", "signatures": ["ICLR.cc/2026/Conference/Submission16772/Reviewer_S6Q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16772/Reviewer_S6Q6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786382079, "cdate": 1761786382079, "tmdate": 1762926816365, "mdate": 1762926816365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper seeks to bridge the gap between empirical and certifiable robustness by distilling knowledge from adversarially trained teachers into certifiably trained students through CC-Dist, a feature-space distillation objective tightly coupled with an expressive certified training loss, CC-IBP. The method forms convex combinations of adversarial features and interval bound propagation (IBP) feature bounds, demonstrating that the resulting distillation objective upper-bounds the worst-case feature-level risk and varies continuously and monotonically with the parameter α. This same parameter coherently regulates both the certified loss and the distillation target. Implemented with auto_LiRPA and verified post hoc using OVAL with α–β–CROWN, CC-Dist consistently improves both standard and certified accuracies over pure CC-IBP on CIFAR-10, TinyImageNet, and ImageNet64, achieving state-of-the-art trade-offs for ReLU networks. Teacher–student analyses further show that while teachers exhibit higher standard and empirical robustness but negligible certifiable guarantees, CC-Dist students achieve strong certification with competitive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work achieves a tight theoretical coupling between feature-space distillation and expressive certifiable training (CC-IBP). The distillation target is constructed as a convex combination of adversarial features and their IBP lower and upper bounds, and the authors prove that the distillation term upper-bounds the worst-case feature-level risk while varying continuously and monotonically with the interpolation parameter α. Under the affine classifier assumption, α jointly regulates both the certified loss and the distillation target, thereby unifying empirical discriminability and certifiable stability in a single coherent and interpretable framework.\n\n2. The proposed method exhibits consistent improvements across multiple vision benchmarks and perturbation radii, and teacher–student comparisons reveal a clear division of strengths. Teachers demonstrate higher standard and empirical robustness but weak certifiability, whereas students achieve significantly stronger certificates while maintaining competitive accuracy. Moreover, curve analyses of distillation strength show stable trends in training dynamics and performance metrics as hyperparameters vary, supporting the overall soundness of the design.\n\n3. The method also demonstrates strong generality and practical deployability. The two endpoint cases of distillation (α = 0 / 1) remain effective in several settings and can be readily transferred to other certified training methods (e.g., SABR). The implementation is built on widely used community toolchains, facilitating reproducibility and extension. The paper further notes that under large perturbation settings, specialized 1-Lipschitz architectures offer superior performance, suggesting a promising direction for extending CC-Dist to such structures."}, "weaknesses": {"value": "1. In scenarios with large perturbation radii, such as ε = 8/255 on CIFAR-10, the ReLU-based CC-Dist has yet to surpass specialized 1-Lipschitz architectures such as SortNet. This suggests that, although the approach advances the accuracy–certificate frontier in most settings, it has not fundamentally bridged the structural gap in large-ε regimes. Moreover, both theoretical and experimental analyses mainly focus on ReLU activations and IBP, while the adaptation and potential benefits across other activation functions or architectures, such as strictly Lipschitz-constrained networks, remain to be systematically validated.\n\n2. The method relies on high-quality adversarial teachers, which introduces additional training costs. Achieving certifiable guarantees also requires substantial computation and time on branch-and-bound verifiers. The paper reports that the increased certified accuracy of CC‑Dist models is accompanied by higher verification runtime. Due to computational cost, teacher certification is evaluated only on a subset of the test set, suggesting that the overall training and verification pipeline remains expensive, particularly for larger datasets and more complex models.\n\n3. The study focuses primarily on ℓ∞ threat models and visual classification benchmarks, with limited evaluation under other paradigms such as ℓ₂ or semantic-level perturbations, or in tasks such as detection and segmentation. Although the shared parameter α between the distillation and the certified loss is theoretically motivated, its sensitivity and robustness across datasets, architectures, and training schedules are not comprehensively analyzed, and β is mostly assigned a fixed empirical value. Furthermore, the affine-head assumption improves interpretability but limits applicability to more complex nonlinear or attention-based heads, leaving open questions regarding broader generalization and complete ablation."}, "questions": {"value": "1. It would be valuable to include preliminary results or a clear plan for extending CC-Dist to 1-Lipschitz architectures (e.g., SortNet) under the ε = 8/255 setting, in order to better assess whether the observed performance gap primarily stems from methodological factors or architectural differences.\n2. A more systematic ablation of α/β and teacher strength across datasets, architectures, and multiple random seeds would help quantify the sensitivity of the method to hyperparameters and teacher quality, thereby reinforcing the robustness and generality of the conclusions.\n3. Could you quantify how tight the IBP feature bounds used in the distillation loss are (e.g., per‑layer bound gaps vs. approximate worst‑case features) and whether looseness concentrates in deeper layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hm15mVjd8O", "forum": "xpHiVf0xhf", "replyto": "xpHiVf0xhf", "signatures": ["ICLR.cc/2026/Conference/Submission16772/Reviewer_LhHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16772/Reviewer_LhHm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924210563, "cdate": 1761924210563, "tmdate": 1762926816005, "mdate": 1762926816005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CC-Dist, a certified training methods for provable robustness. It augments CC-IBP with a feature-space knowledge distillation objective to achieve a better trade-off between the clean accuracy and the provable robust accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "++ I generally like the idea: although adversarial training will not generally obtain provably robust models, these may not be problem of adversarial training but the current verifier cannot certify their robustness. By incorporating the adversarially trained models into provable robust learning in a smooth way can make the best of both sides.\n\n++ The algorithm is straightforward and the manuscript is well written: compared with CC-IBP, it only adds one single hyper-parameter which is not very sensitive, making the proposed CC-Dist easy to adopt on top of existing algorithms.\n\n++ The experiments are relatively comprehensive and convincing."}, "weaknesses": {"value": "1. The gaps between the proposed method and baselines are relatively small on Table 1, so running the experiments for multiple times and report the performance variance would be better.\n\n2. PGD-40 is utilised as the metric for empirical robustness. However, the most reliable empirical robustness evaluation scheme is AutoAttack in RobustBench. It would be better to use AutoAttack instead.\n\n3. The scope is a bit limited, as the results and discussions are demonstrated for $l_\\infty$ bounded perturbations only and based on the architecture CNN-7 with ReLU as the only activation functions. I am not sure if the same method can be generalizable to deeper neural networks, activation functions and perturbation types.\n\nIn general, I think this work is a meaningful contribution to the community. It can be improved if the concerns above can be properly addressed.\n\nMinor: there are some additional related literature:\n\n* AutoAttack as the reliable empirical evaluation metric: \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks\" (2020)\n\n* When discussing linear network relaxations for provable robustness, there is a series of works utilising geometric properties to derive the loss function: \"Provable robustness of relu networks via maximization of linear regions.\" (2019), \"Training Provably Robust Models\nby Polyhedral Envelope Regularization\" (2023)\n\n* Randomized smoothing is also an important series of provable robustness: \"Certified Adversarial Robustness via Randomized Smoothing\" (2019)"}, "questions": {"value": "First some questions about the weakness:\n\n1. What is the variance of the performance? Is it big? In addition, please use AutoAttack as the metric for empirical robustness for a more reliable evaluation.\n\n2. Is this method extendable to boarder architectures, other activation functions and perturbations beyond $l_\\infty$ bounded ones?\n\nIn addition, I have additional questions:\n\n1. In Appendix D.2.4, the author mentioned \"tuning was carried our on the evaluation sets.\" Isn't it unfair?\n\n2. Why do you choose the features before final affine layer for alignment in knowledge distillation? Can you choose an earlier layer? You can still use IBP or CROWN-IBP to derive the output bound to calculate the original loss $^{CC}\\mathcal{L}^{\\mathcal{C}_\\epsilon}_{f_\\theta}$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RY1RKIYFXO", "forum": "xpHiVf0xhf", "replyto": "xpHiVf0xhf", "signatures": ["ICLR.cc/2026/Conference/Submission16772/Reviewer_1hxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16772/Reviewer_1hxv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762437600241, "cdate": 1762437600241, "tmdate": 1762926814070, "mdate": 1762926814070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}