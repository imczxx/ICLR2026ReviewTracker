{"id": "qVFbnfVmuu", "number": 15938, "cdate": 1758257375753, "mdate": 1763192948192, "content": {"title": "On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study", "abstract": "Counterfactual reasoning has emerged as a crucial technique for generalizing the reasoning capabilities of large language models (LLMs). By generating and analyzing counterfactual scenarios, researchers can assess the adaptability and reliability of model decision-making. Although prior work has shown that LLMs often struggle with counterfactual reasoning, it remains unclear which factors most significantly impede their performance across different tasks and modalities. In this paper, we propose a decompositional strategy that breaks down the counterfactual generation from causality construction to the reasoning over counterfactual interventions. To support decompositional analysis, we investigate 11 datasets spanning diverse tasks, including natural language understanding, mathematics, programming, and vision-language tasks. Through extensive evaluations, we characterize LLM behavior across each decompositional stage and identify how modality type and intermediate reasoning influence performance. By establishing a structured framework for analyzing counterfactual reasoning, this work contributes to the development of more reliable LLM-based reasoning systems and informs future elicitation strategies.", "tldr": "In-depth understanding of LLMs' counterfactual reasoning eligibility", "keywords": ["Counterfactual Reasoning", "Large Language Models (LLMs)", "Experimental Study"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7c2447c8b52e4751613ebdfcaf5a5d9c4dd2075.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Large language models typically struggle on counterfactual reasoning tasks, however it is unclear *why* they struggle, particularly at which parts of such reasoning. This submission proposes a benchmark to evaluate different aspects in LLMs' causal reasoning.They isolate that LLMs struggle to properly identify causal variables and find the counterfactual mediator and outcome even when given the variables, their relationships and the intervention to perform."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The approach is original; I have never seen any attempt at explaining the difficulties of LLMs in causal reasoning.\n\n- The division into subtasks looks sound to me.\n\n- Diversified source of tasks."}, "weaknesses": {"value": "- The benchmark seems to impose the strict presence of four variables : covariates, treatment, mediator, outcome. However, the mediator could be absent from the context, as in the example:  “A person is running a marathon and collapses.” l.467-469. It is unclear why LLMs should explicitly,identify “Dehydration” as a mediator, and not other sources.\n\n- It is unclear what prompts are given to LLMs : from my understanding, it is the \"CRASS Prompt\" given in Appendix A, but it seems to apply to all datasets?\n\n- Typo? \"the primary challenge lies not in\" l.390-391 : it should be \"lies in\", right?"}, "questions": {"value": "- Can you answer/address the above weaknesses?\n\n- In the datasets, are the examples *only* made up of the $X,T,M,Y$ variables, or are any other variables present such that the model must extract the relevant variables *among other options*?\n\n- Did humans or LLMs annotate the paper's benchmark dataset? Which of these scored LLMs' answers, or was this done through another method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y58uSFR7bi", "forum": "qVFbnfVmuu", "replyto": "qVFbnfVmuu", "signatures": ["ICLR.cc/2026/Conference/Submission15938/Reviewer_fFFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15938/Reviewer_fFFZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761262489256, "cdate": 1761262489256, "tmdate": 1762926151937, "mdate": 1762926151937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes three key contributions:\n\n- It introduces a **decompositional framework** for evaluating counterfactual reasoning in LLMs, breaking it into four stages: causal variable identification, causal graph construction, counterfactual intervention identification, and outcome reasoning.\n- It constructs a **comprehensive multimodal benchmark** using 11 datasets across text, image, code, and symbolic modalities, enriched with causal annotations and graphs.\n- It provides an **in-depth evaluation and improvement strategy**, identifying key bottlenecks (especially in mediator reasoning and cross-modal tasks) and proposing tool augmentation and advanced prompting techniques to enhance LLM performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Systematic and Granular Evaluation Framework:** The paper's primary strength is its decompositional approach, which breaks down the complex task of counterfactual reasoning into four distinct, measurable stages. This allows for a much more precise diagnosis of *where* and *why* LLMs fail, moving beyond a monolithic \"pass/fail\" assessment to identify specific bottlenecks, such as the particular difficulty with implicit mediators.\n\n2.  **Comprehensive and Multimodal Benchmark:** The authors construct a substantial and diverse benchmark by curating 11 datasets spanning various modalities (text, image, code, symbols) and tasks. This breadth ensures that the findings are not limited to a single domain and provides a robust testbed for evaluating the generalizability of LLMs' counterfactual reasoning abilities.\n\n3.  **Actionable Insights and Proposed Solutions:** The paper goes beyond mere identification of problems by proposing and evaluating concrete strategies for improvement. The demonstration that tool-augmentation can alleviate modality-specific issues and that advanced prompting can help with implicit reasoning provides valuable, actionable directions for future research aimed at enhancing LLM reasoning capabilities."}, "weaknesses": {"value": "I think there are 2 brief weaknesses of the paper:\n\n*   **Potentially Artificial Evaluation:** The benchmark relies on pre-annotated causal structures, which may not reflect the challenge of inferring causality from raw, unstructured data.\n\n*   **Surface-Level Diagnosis:** The analysis identifies performance bottlenecks but offers a high-level explanation (e.g., \"working memory\") without deeply investigating the underlying architectural mechanisms in LLMs that cause these failures."}, "questions": {"value": "The benchmark relies on curated causal structures. To what extent do you believe the performance on these structured tasks generalizes to real-world scenarios where causal graphs are not provided and must be inferred from raw data? Did you run any experiments on \"raw\" data without this scaffolding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9wuo67Bp2W", "forum": "qVFbnfVmuu", "replyto": "qVFbnfVmuu", "signatures": ["ICLR.cc/2026/Conference/Submission15938/Reviewer_BrYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15938/Reviewer_BrYZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889129133, "cdate": 1761889129133, "tmdate": 1762926151601, "mdate": 1762926151601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines counterfactual reasoning in LLMs across a range of modalities and models.\nThe paper shows that by decomposing the reasoning scenario to causality construction and counterfactual intervention, the reasoning ability of LLMs under counterfactual scenarios can be greatly improved.\nBy leveraging tools that can help the model identify the causal variable, the performance can be further improved."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall methodology of decomposing the counterfactual reasoning process is novel and the experiments show this really helps.\n2. The experiments cover a wide range of dataset design specifically for counterfactual reasoning.\n3. The final proposed method seems to be easy to adopt for any LLM for reasoning."}, "weaknesses": {"value": "1. The experiments covers many datasets, but it lacks comparison on model scale, for example, Qwen 3 provides models across different scales, it could make the paper stronger if some results are shown there.\n2. The NER tools are designed to use Bert like models, however, would it be possible that the tools are instantiated by another model using different prompts?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iYa8xAk9Q9", "forum": "qVFbnfVmuu", "replyto": "qVFbnfVmuu", "signatures": ["ICLR.cc/2026/Conference/Submission15938/Reviewer_Ndd6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15938/Reviewer_Ndd6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908654616, "cdate": 1761908654616, "tmdate": 1762926151210, "mdate": 1762926151210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a decomposition of counterfactual reasoning into smaller sub-tasks: causal variable identification, causal graph construction, counterfactual identification, and outcome reasoning. The authors evaluate the performance of different LLMs in performing these sub-tasks across a variety of datasets, identify variable identification and counterfactual identification as a weak point, and propose strategies to improve it."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It has been observed repeatedly that LLMs perform worse when answering counterfactual queries relative to factual ones. The paper’s attempt at understanding why in a more fine-grained fashion is a significant problem.\n\nExperimental evaluations are comprehensive in terms of the number of models and the variety of datasets they consider."}, "weaknesses": {"value": "It is not clear how the performance on the four sub-tasks relate to the end-to-end performance. Establishing this relation is important when interpreting performance on these sub-tasks as decomposition.\n\nFor instance, the paper concludes that LLMs are generally better at Task 1 than Task 2 conditioned on correct results from Task 1 (which is supported by experiments and indeed seems to be the case). However, it could still be the case that starting from the inputs of Task 1 and directly querying for the outputs of Task 2 is as easy as performing Task 2 conditioned on correct results from Task 1. If, for instance, processing X, Y, Z variables implicitly is somehow easier than outputting them explicitly.\n\nWhile the above is probably unlikely to be the case, it is a crucial validation to perform in order to confirm that Task 1 and Task 2 (conditioned on Task 1 results) behave as intended. Evidence provided in the paper does not provide support for attributing poor end-to-end Task-1 inputs to Task-2 outputs performance to poor Task 1 performance. \n\nI want to emphasize that this is a general consideration for all sub-tasks, the entire chain, not just Tasks 1 and 2 specifically.\n\nThe paper doesn’t discuss the related work to a sufficient depth. In particular, other methods of evaluating counterfactual reasoning and whether there have been attempts at decomposing this process into smaller steps in evaluation is not discussed.\n\nThe four sub-tasks that are identified, while generally sound, are arbitrary. Their choice is not motivated.\n\nWhile evaluations in Section 4.1 are interesting, they are not exactly actionable. They help identify which sub-tasks are the weak points to be improved (which arguably all a decomposition is supposed to do) and the paper suggests improvement for these weak points. However, the suggestions take the decomposition as a given. When the goal is to answer a question end-to-end, starting from the given of Task 1 and providing the outputs of Task 4, the paper does not explore the impact of going through Tasks 1, 2, 3, and 4 as a pipeline rather than direct queries. Moreover, it does not explore whether the task specific improvements in Section 4.2 lead to a comparable end-to-end gain as well."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KtGwcjz5Du", "forum": "qVFbnfVmuu", "replyto": "qVFbnfVmuu", "signatures": ["ICLR.cc/2026/Conference/Submission15938/Reviewer_aJkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15938/Reviewer_aJkt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996174867, "cdate": 1761996174867, "tmdate": 1762926150746, "mdate": 1762926150746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}