{"id": "yD0JI9BaVS", "number": 16579, "cdate": 1758266328390, "mdate": 1759897231624, "content": {"title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "abstract": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities.\nHere, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. Our optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt.", "tldr": "An automatic method for mining prompts that cause the TTI model to generate biased outputs, targeting open-set and specific biases.", "keywords": ["Bias", "Open-set Bias", "Diversity", "Text-to-Image models", "Genetic algorithms"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d3401c705420d7a8237e879dc10c6f6ccccdcfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MineTheGap, a framework for automatically mining text prompts that cause TTI models to generate biased outputs. The method employs a genetic algorithm to iteratively search the prompt space, guided by a novel, open-set bias score. This score quantifies bias by comparing the distribution of images generated by the TTI model for a given prompt to a reference distribution of plausible interpretations. This reference distribution is approximated by generating a set of diverse \"textual variations\" of the original prompt using LLMs. A large gap between the generated image distribution and the textual variation distribution indicates a high degree of bias. The authors validate their bias score by demonstrating its correlation with real-world occupational statistics."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**1. Novel Bias Score for Open-Set Scenarios**\n\nThe paper introduces a novel and clever bias score that moves beyond traditional, axis-based evaluation. By quantifying bias as the distributional gap between generated images and LLM-generated textual variations, the method provides a flexible and powerful way to measure bias in open-set scenarios without needing predefined categories like gender or race. This is a significant methodological contribution to the field of fairness evaluation.\n\n**2. An Automated Framework for Proactive Bias Mining**\n\nIt proposes a complete, automated framework called MineTheGap for mining problematic prompts from the vast text space. The use of a genetic algorithm to search for and optimize prompts that expose model weaknesses is a sound approach. This shifts the paradigm from passively detecting bias on a given set of prompts to actively discovering a model's inherent vulnerabilities.\n\n**3. Strong Validation of the Proposed Metric**\n\nA key strength of this work is the validation of its proposed bias score. The authors demonstrate that their metric's ranking of occupational bias shows a strong and positive correlation (Spearman's ρ = 0.72) with real-world labor statistics, even outperforming a notable prior method."}, "weaknesses": {"value": "**1. Unproven Assumption of the Bias Score**\n\nThe paper's entire bias metric rests on a strong and unproven assumption: that an LLM's \"textual variations\" can serve as a valid proxy for the true, human-expected distribution of a prompt's meaning. This is a fundamental weakness, as the LLM used to generate this reference distribution is itself inherently biased and has a limited scope of creativity. While the authors briefly acknowledge this in the conclusion, the fact that the \"ground truth\" is actually another potentially flawed model significantly undermines the claimed objectivity of the bias score.\n\n**2. High Computational Cost and Lack of Practicality Analysis**\n\nThe framework appears to be computationally expensive. The genetic algorithm requires generating N images and N text variations, followed by an N x N similarity calculation, for every candidate prompt in every iteration. The paper provides no discussion or estimate of the actual computational resources (e.g., GPU hours) for this process, making it difficult to assess the practical viability and scalability of the method as a real-world auditing tool.\n\n**3. Lack of Sensitivity Analysis for the Bias Score's Hyperparameters**\n\nThe proposed bias score is dependent on key hyperparameters, notably the number of generated samples and the percentile choice. The paper does not provide any sensitivity analysis to demonstrate how the ranking of biased prompts would be affected by different choices of these hyperparameters. This is a significant omission that makes it difficult to assess the robustness and stability of the core metric.\n\n**4. Insufficient Baselines for the Prompt Mining Framework**\n\nWhile the paper validates its bias score against prior work, the genetic algorithm itself lacks a rigorous comparison against simpler baselines. For instance, it is unclear how much the complex evolutionary process outperforms a more straightforward approach, such as a large one-shot random sampling of prompts ranked by the same bias score. Without such a comparison, the added value of the genetic algorithm over simpler search strategies is not well-established."}, "questions": {"value": "**1.** The validity of the proposed bias score hinges on the assumption that an LLM's output can serve as a proxy for the true human-expected distribution. Given that any LLM is itself inherently biased, could the authors provide a stronger justification for this core assumption or discuss how the choice of LLM might influence the discovered biases?\n\n**2.** The paper lacks any analysis of the computational cost, which appears to be a significant concern for the method's practicality. Could the authors provide an experiment or at least an estimate of the resources (e.g., total GPU hours) required for a full mining run on a single model and comment on the framework's scalability for real-world auditing?\n\n**3.** The stability of the bias score is dependent on key hyperparameters. Could the authors provide a sensitivity analysis experiment to demonstrate how the ranking of biased prompts is affected by variations in these crucial parameters, in order to validate the metric's robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mCqn1RfSyh", "forum": "yD0JI9BaVS", "replyto": "yD0JI9BaVS", "signatures": ["ICLR.cc/2026/Conference/Submission16579/Reviewer_MP9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16579/Reviewer_MP9j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760563462178, "cdate": 1760563462178, "tmdate": 1762926658282, "mdate": 1762926658282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a genetic-algorithm framework that mines prompts likely to expose bias in text-to-image models. It scores each prompt by comparing a batch of generated images to LLM-produced text variations in a shared embedding space, using a bias.-score that depends on coverage and relevance which is then used in the algorithm to identify prompts that induce biases. Experiments are performed across SD 1.4, SD 2.1, SD 3, and FLUX.1 models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach goes beyond fixed biased and tackle an important and challenging problem  automatic mining of prompts that induce biases.\n2. The experimental results are robust, spanning SD 1.4/2.1/3 and FLUX models, which suggests the approach is architecture-agnostic."}, "weaknesses": {"value": "1. The bias score considered cannot be considered a parity metric. it doesn’t check how often each group appears, only whether every text considered has at least one matching image (coverage) and every image matches some text (relevance). I think there is a loophole here.  For example, with the texts “female doctor,” “female doctor in ER,” “male doctor,” and “male doctor in office,” a batch containing three male office portraits and one female ER photo will still “pass” both coverage and relevance where each row has a single good match, and each image has a matching text. The score would be high (suggesting low bias) despite an obviously skewed distribution (3/4 male). I believe that this metric may underreport demographic imbalance (or any other imbalances) compared to parity-style measures (e.g., those used in OpenBias) that directly assess group proportions.\n\n2. This is again related to the first point. In Section 4.2,  the authors force demographic sensitivity by constructing text variations via a gender×race Cartesian product, which makes the score responsive to demographic omissions. But if the text variations are randomly generated without explicitly targeting gender or race, as in the “doctor” variations listed in Appendix C.3, gender bias may not surface as a “missed visual concept.” With only a small number of female-specific texts and some female images, the evaluation can still show good coverage and relevance, effectively missing a known demographic skew. Hence, this method has limitations in detecting certain known biases, especially representational imbalances, unless demographic attributes are explicitly and sufficiently encoded in the text set. In the “unknown-bias” setting, where no ground truth exists, this makes it hard to assess reliability and the reported results may not faithfully reflect real-world bias.\n\n3. It would have been stronger if the paper had included a user study for the unknown-bias detection setting, to measure how well the method’s findings align with human judgments."}, "questions": {"value": "1. What happens if you use a parity metric  as in OpenBias instead of the bias score introduced in the paper?\n2. Could the authors report the missed visual concepts for the prompt “a doctor” using the textual variations from Appendix C.3? Given the known biases in this case, this will provide a validation the above points.\n3. The method is dependent on multiple hyperparamaters. Would the same set of hyperparameters work for other cases where prompts are more complex?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VOvNN2kAey", "forum": "yD0JI9BaVS", "replyto": "yD0JI9BaVS", "signatures": ["ICLR.cc/2026/Conference/Submission16579/Reviewer_Bze9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16579/Reviewer_Bze9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760976667272, "cdate": 1760976667272, "tmdate": 1762926657201, "mdate": 1762926657201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The paper introduces MineTheGap, an automatic prompt mining method to uncover instructions that would lead to bias in Text-to-Image (TTI) models' generations.\n  * The method uses a genetic algorithm and a LLM to iteratively refine a pool of prompts, enabling the discovery of both known and previously unseen open-set biases.\n* The author proposes a novel bias score that ranks biases according to their severity to assist with the optimization process.\n  * The bias measurement method compares the distribution of generated images to distributions of textual variations to capture deviations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The automatic prompt mining technique is novel in TTI model bias evaluation.\n* The bias measurement method by ranking prompts based on bias score is also novel and seems to be effective from the visual examples."}, "weaknesses": {"value": "My concerns about this paper is mainly regarding its problem formulation, experiment design and empirical results. While the core methodology is novel, the overall experiment design and result analysis lacks systematic rigor. The current presentation makes it difficult to fully assess the efficacy and generalizability of the proposed approach across diverse scenarios and TTI architectures.\n\n* **Ambiguity in Bias Definition and Scope**: The paper does not provide a clear, formal definition of \"bias\" within the context of Text-to-Image (TTI) models, nor does it explicitly delineate the categories or dimensions of bias the proposed framework is designed to detect. Clarifying the theoretical boundaries of the framework is essential.\n* **Reliance on Un-Benchmarked Prompt Initialization**: The methodology's dependence on LLM-sampled initial prompts and a self-designed prompt refinement paradigm without a clearly stated problem definition (i.e., the specific types of bias targeted for exploration) introduces an element of subjectivity. A more robust approach would involve starting with prompts derived from existing, established bias benchmarks to provide a grounded point of comparison.\n* **Questionable Scalability to Complex Prompts**: The reliance on overly simple, short prompts (limited to eight words) raises significant doubts about the method's effectiveness on more recent and powerful TTI models capable of processing longer, more complex, and semantically sophisticated instructions. The paper should include a thorough analysis of how the method performs with the scaling of prompt length and complexity.\n* **Narrow Validation of Bias Score and Lack of Human Grounding**: The validation of the new bias score is primarily demonstrated on a constrained and simple task (gender bias in \"portrait of profession\" prompts) using only a Spearman rank correlation. It remains unclear how the score generalizes to other domains of bias or more complex prompts. Furthermore, the absence of a human validation study to verify the perceptual relevance of the bias score is a notable oversight.\n* **Insufficient  Quantitative Results**: While the paper is proposing a new bias score metric, the authors provide limited quantitative results on different TTI models or bias dimensions. The current experimental section relies heavily on qualitative examples and visual demonstrations. Without showing the bias in TTI models that your method is successful in capturing, I don't think the main contribution can be justified.\n* **Insufficient Model Evaluation**: The authors only conducted experiments on a limited set of models, omitting tests on several state-of-the-art TTI models, such as the publicly available Qwen-Image or leading closed-source models like DALL-E and GPT-Image-1. A comprehensive audit requires testing against the current frontier of TTI models.\n* Additionally, the image examples are small and hard to see, especially in Figure 2."}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ylzOU8r86c", "forum": "yD0JI9BaVS", "replyto": "yD0JI9BaVS", "signatures": ["ICLR.cc/2026/Conference/Submission16579/Reviewer_EUCN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16579/Reviewer_EUCN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963246843, "cdate": 1761963246843, "tmdate": 1762926656742, "mdate": 1762926656742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MineTheGap, a framework for automatically discovering biased prompts in text-to-image (TTI) models. The method employs a genetic algorithm guided by a bias score that measures the extent to which a model’s generated image distribution deviates from the expected semantic diversity of a given text prompt. This expected diversity is approximated through LLM-generated textual variations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an interesting idea that moves beyond traditional bias detection toward bias discovery, introducing an automated framework for mining prompts that reveal hidden or emergent biases in text-to-image models.\n- Demonstrates the framework across multiple TTI models (Stable Diffusion 1.4–3 and FLUX), validating the metric through correlation with real-world statistics and showing model-specific bias discovery."}, "weaknesses": {"value": "- The method relies heavily on LLMs to generate prompts and mutations, making its performance dependent on the quality and biases of the chosen LLM. Although the authors acknowledge this limitation, they do not analyze how different LLMs might affect the results or the stability of the mining process.\n- The proposed metric depends on CLIP embeddings, which can carry demographic biases of their own. Because it measures similarity in this embedding space, it may wrongly treat harmless stylistic or compositional differences as bias. For example, two images of doctors in different lighting or art styles might be judged as “biased” simply because their embeddings differ.\n- The choices of hyperparameters are not conceptually motivated or empirically ablated, such as the 20% percentile (α), number of iterations, and population size. Although ablations are included, they do not address parameter sensitivity or convergence behavior. Similarly, the construction of the initial population is unclear: it depends on LLM-generated prompts, yet the paper does not examine how variations in this initial set influence the outcomes. The role of random sentence injection is also insufficiently studied, leaving its contribution to exploration unexplained. Moreover, the mutation diversity relies entirely on the LLM’s ability to generate varied prompts. The paper does not verify whether these mutations are genuinely diverse or merely superficial rephrasings. For example, if the prompt “a cat on a chair” repeatedly mutates into “a kitten on a chair” or “a cat sitting on a chair,” the process would appear active but contribute little semantic diversity, potentially skewing the biased search results."}, "questions": {"value": "- L11: \"Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous.\" What are these aspects?\n- L51: \"Prior approaches are limited to quantifying bias along specific axes and dividing each axis into a finite set of options, regardless if these are predefined as is typical in studies of sociodemographic biases, or proposed on the fly using LLMs.\" How is generating biases on the fly restrictive, especially in [1]? The authors argue that generating biases on the fly, as in [1], is restrictive because it limits the exploration to predefined bias axes. However, this claim appears inconsistent with their own approach as they also rely on predefined prompt templates containing specific bias structures (Appendix C.1). This reliance effectively mirrors the same constraint they critique, making their argument somewhat counterintuitive.\n- L121: \"These images (shown in (c)) were generated using SD 3 with textual variations of the prompt that specify gender, race, style, and surroundings.\" How is this process implemented? It appears to involve manual intervention, which raises concerns about scalability and automation.\n- L159: \"The optimization begins with an initial population of b diverse prompts, designed to broadly cover the space of possible biases. Ideally, this population approximates a uniform sample from the space of valid prompts, P.\" What is the method’s dependency on the initial prompt set? This should be quantitatively analyzed. Also, when the authors mention a “uniform sample,” do they mean sampling from a uniform probability distribution over the prompt space ( P )?\n- L175: \"We then generate mutations for each of the top-ranked sentences and add random sentences to form the next population.\" The effect of adding random sentences should be systematically analyzed, including the degree of randomness introduced and its impact on the search process. It would also be useful to examine whether these random samples act as soft or hard negatives, and how each type influences optimization behavior.\n\n[1] Aditya Chinchure, Pushkar Shukla, Gaurav Bhatt, Kiri Salij, Kartik Hosanagar, Leonid Sigal, and Matthew Turk. Tibet: Identifying and evaluating biases in text-to-image generative models. In European Conference on Computer Vision, pp. 429–446. Springer, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hNyzfGXnCM", "forum": "yD0JI9BaVS", "replyto": "yD0JI9BaVS", "signatures": ["ICLR.cc/2026/Conference/Submission16579/Reviewer_iZmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16579/Reviewer_iZmd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035054382, "cdate": 1762035054382, "tmdate": 1762926656283, "mdate": 1762926656283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}