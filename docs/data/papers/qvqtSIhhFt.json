{"id": "qvqtSIhhFt", "number": 15035, "cdate": 1758247090459, "mdate": 1759897334166, "content": {"title": "OpenDataBench: Real-World Benchmark for Table Insight Generation and Question Answering Over Open Data", "abstract": "The promise of Large Language Models (LLMs) for data analysis is hindered by benchmarks that inadequately reflect real-world complexities, including multiple large tables and external knowledge. Moreover, they mainly focus on fact retrieval via Question Answering (QA) and overlook the critical task of exploratory insight generation. To address these gaps, we introduce OpenDataBench, a benchmark built from governmental open data capturing these practical challenges. It features two types of tasks: multifaceted Table QA tasks that require answering complex decomposable questions with either text or graphs, and Table Insight tasks that challenge models to generate expert-level findings from exploratory data analysis.\nWe evaluate state-of-the-art LLMs and our proposed agentic solution on OpenDataBench. Our experimental results indicate that even top-performing models struggle with both tasks. This highlights a significant gap between current model capabilities and the demands of realistic data analysis. OpenDataBench serves as a rigorous benchmark for advancing research on LLM-driven data analysis systems capable of addressing both reactive question answering and proactive insight discovery. Code and sample data are available at https://anonymous.4open.science/r/opendatabench-8AFA/", "tldr": "A challenging benchmark for table insight generation and question answering over open data accompanied by proposed specific LLM agents", "keywords": ["Data Analytics Benchmark", "LLM Agent", "LLM Evaluation", "Table Question and Answering", "Table Insight Generation", "Open Data"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54068c76a1ee07f18c3722ee03cf59f01d1f4033.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on the challenge of evaluating LLMs on real-world data analysis, where existing table reasoning benchmarks fail to capture the complexity of large, multi-table datasets and the open-ended nature of insight generation.\nTo address this, the authors construct OpenDataBench, a benchmark built from governmental open data featuring tasks for multifaceted table QA and Table Insight Generation, and introduce two corresponding agentic solutions, including an Answer Agent and an Insight Agent. Experiments show that even state-of-the-art models like GPT-4o and Gemini 2.5 perform poorly on these tasks, revealing a significant gap between current LLM capabilities and realistic data-analysis demands."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a real and important challenge that current LLM benchmarks for table reasoning do not reflect the complexity and messiness of real-world data analysis.\n\n2. The idea of building a realistic benchmark from government open data is solid and practical, offering a credible way to capture large-scale, multi-table, and heterogeneous data scenarios often encountered in real applications."}, "weaknesses": {"value": "1. The novelty is limited. The benchmark mainly extends existing table QA and insight-generation setups to larger, real-world datasets without introducing fundamentally new tasks or evaluation methods.\n\n2. The proposed agents (Answer Agent and Insight Agent) are largely incremental combinations of existing techniques like code generation, self-correction, and reflection, offering engineering value but little conceptual innovation.\n\n3. The evaluation results mainly confirm that current LLMs struggle with large and messy tables, rather than revealing new insights into how to overcome these challenges."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no ethics concerns"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DPu7XvG1Gl", "forum": "qvqtSIhhFt", "replyto": "qvqtSIhhFt", "signatures": ["ICLR.cc/2026/Conference/Submission15035/Reviewer_SPHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15035/Reviewer_SPHH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535582974, "cdate": 1761535582974, "tmdate": 1762925361516, "mdate": 1762925361516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenDataBench, a benchmark designed to evaluate table understanding capabilities of language models using real-world open data from governmental sources. The benchmark addresses two limitations of existing table reasoning benchmarks: lack of real-world complexity (large-scale, multi-table datasets with external knowledge) and narrow task scope (focusing only on question answering while neglecting insight generation). OpenDataBench features two main tasks: Table QA (answering complex decomposable questions with text or visual outputs) and Table Insight (generating expert-level findings from exploratory analysis). The authors also propose two agentic solutions: an Answer Agent with fail-safe modules for Table QA and an Insight Agent using graph-based exploration for Table Insight."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 The proposed dataset represents a good advancement over existing benchmarks that use small, clean tables.\n\n2 The benchmark formalizes both reactive (Table QA) and proactive (Table Insight) data analysis tasks. \n\n3 The paper employs a four-stage pipeline (question generation, scoring, answer generation, human verification) with multiple LLM judges and human experts."}, "weaknesses": {"value": "1 The use of LLM-based evaluation for insight generation, while practical, introduces subjectivity. The quantitative metrics may not fully capture insight quality, but provides limited validation of the evaluation methodology's reliability.\n\n2  The multi-agent approaches involve complex pipelines with multiple LLM calls, but the paper lacks analysis of computational requirements, inference latency, or cost considerations, which are important factors for real-world deployment."}, "questions": {"value": "please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LQxswo40dz", "forum": "qvqtSIhhFt", "replyto": "qvqtSIhhFt", "signatures": ["ICLR.cc/2026/Conference/Submission15035/Reviewer_n5Wt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15035/Reviewer_n5Wt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626887254, "cdate": 1761626887254, "tmdate": 1762925360441, "mdate": 1762925360441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark, OpenDataBench, which consists on two tasks: multi-faceted table question-answering and table insights. The former assesses factual reasoning over composable questions (with multiple subquestions), while the latter, challenges models to provide expert-level insights , which can require in-depth analysis. The authors leverage a filtering technique, human annotators  and LLM as a judge in the construction of the data for both tasks. The tasks are based on publicly available complex, heterogeneous datasets that contain large tables. The authors propose a table serialization technique to bypass the need to pass the full table to the LLM. The authors also provide two agents for each task which outperform existing models and agents. The authors also include error analysis and an ablation wrt the proposed table serialization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a complex enough benchmark that can be leveraged to assess agentic workflows. This is an active area of research and it is important to have access to realistic benchmarks for evaluation."}, "weaknesses": {"value": "- The paper is missing precise implementation details for the new proposed agents"}, "questions": {"value": "- Add one liner to explain how different types of variables are handled in the main paper so there’s no need to go to the appendix to understand the full details.\n- It would be good to add more clearly the percentage of discarded data points and questions as well as describe how many times you require to prompt an LLM to provide a better notion of how expensive the dataset creation is.\nEven though there is a section that discusses error analysis, one particular failure mode that is not discussed in detail is what happens if there are questions that use multiple tables and the agent fails to fetch all the relevant tables? \n- Typically, SQL is the tool of choice to extract information from one or more tables. Is it the case that the python code can leverage libraries to perform SQL queries on the tables?\n- It would be interesting to add to the ablation study if instead of naively passing the first 10 rows, what if we pass the table schema and a number of rows? Is that equivalent to the proposed feature serialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Idxql1peVV", "forum": "qvqtSIhhFt", "replyto": "qvqtSIhhFt", "signatures": ["ICLR.cc/2026/Conference/Submission15035/Reviewer_32SQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15035/Reviewer_32SQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932776337, "cdate": 1761932776337, "tmdate": 1762925360047, "mdate": 1762925360047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new benchmark targeting two tasks -- table question answering over large tables, and table insight generation.\nThe source tables are collected from public government datasets, and question-answer pairs are generated with LLM followed by human verification, keeping only QA pairs with low execution-result agreement for LLM generated code. Table Insights are extracted from publicly written report based on the tables. Baselines together with error analysis are also provided.\nMain contributions of this paper include\n1. Created a benchmark for QA over large tables\n2. While not the first to create a insights generation benchmark, the insights are extracted from organic sources (human written reports) rather than planted into tables\n\nI think the motivation for this work is clear and if the idea is executed properly this can be an important contribution, but some sections (dataset construction/eval metrics/baseline) lacks sufficient detail to judge soundness of the work, so for now I'm recommending reject."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This benchmark addresses current limitations/shortcomings in existing table-based QA\n2. The insights are collected organically from human written reports, which seems more representative of human interests compared to existing benchmark\n3. The answers are not limited to textual format but also covers data visualization/chart generation"}, "weaknesses": {"value": "1. Missing some details in dataset construction/eval metrics/baseline design & analysis\n\n    a. Unclear to me how insights are extracted from the human written reports. Also through prompting?\n\n    b. For table QA, is your naive baseline (feeding first 10 rows to LLM in single turn) generating code to produce answer or just prompted to generate output? \n\n    c. baseline analysis -- what's the proportion of answer agent needing to go back to revise the generated code? how many iterations do you allow answer agent to run before stopping\n\n    d. Insight agent -- the pipeline seems similar to the data construction stage for your table QA benchmark generation (except human verification)? how do you determine what questions to keep and what's the stopping criteria? how do you rank the generated questions? \n\n    e. Insight generation metrics -- the proposed G-eval based score compares generated insights against 'ground-truth' insights, but original G-eval score is comparing summary against source article. Also, G-eval reports four separate scores (Coherence, Consistency, Fluency, Relevance). Why was only one combined score reported? How are those different aspects combined to get a single score? I think the description of the proposed g-eval inspired metrics does not have sufficient detail.\n\n    f. Analysis: as I understand, the answer are generated using LLM generated code, and the answer agent is also prompted to generate code, even though the dataset construction employs multiple LLM providers to mitigate bias, is there any chance that the ones that are answered correctly also happens to be generated with the same model? i.e. in the successfully answered portions, say by Gemini, any chance they just happened to be questions whose answers were already generated by Gemini?\n\n\n \n\n2.  Missing some reference\n    a. How does this dataset compare to DataBench (Grijalba et al 2024) -- this also seems to be targeting QA over large tables\n\nJorge Osés Grijalba, L. Alfonso Ureña-López, Eugenio Martínez Cámara, and Jose Camacho-Collados. 2024. Question Answering over Tabular Data with DataBench: A Large-Scale Empirical Evaluation of LLMs. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 13471–13488, Torino, Italia. ELRA and ICCL."}, "questions": {"value": "1. Seems like question types are limited to those that are answerable with python code, as the dataset construction process with python execution for answer generation seems to guarantee that. What's the justification behind limiting it to these types of questions? How are you prompting the LLM to generate questions?\n\n2. I appreciate the authors acknowledged the potential subjectivity of insights and their attempt to address it. I'm curious if the authors have conducted analysis on annotator agreement for the generated insights\n\n3. \"After executing the code from all four models, we measured the answer consensus to filter out questions that yielded unanimous agreement across all four LLMs. Such instances were deemed to indicate a low level of analytical complexity, making them unsuitable for a benchmark to challenge state-of-the-art models.\" => \n\n    a. how do you measure answer consensus? \n\n    b. what is the justification for the claim that 'unanimous agreement' indicates 'low level of analytical complexity' -- seems like this is just to intentionally construct an adversarial set that is hard for current systems, not so much representative of true distribution/human interests? \n\n    c. if the executed code has no agreement, how do you decide which answer to keep? \n\n4. Human verification stage -- \"During this stage, annotators also filtered out questions for qualitative reasons,\nsuch as not being insightful, being too ambiguous to permit a definitive answer, or requiring\nexternal knowledge that was unavailable.\" => what is considered 'insightful'?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zXcWIIB7f7", "forum": "qvqtSIhhFt", "replyto": "qvqtSIhhFt", "signatures": ["ICLR.cc/2026/Conference/Submission15035/Reviewer_UM5Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15035/Reviewer_UM5Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950211125, "cdate": 1761950211125, "tmdate": 1762925358934, "mdate": 1762925358934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}