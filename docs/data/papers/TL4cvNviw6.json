{"id": "TL4cvNviw6", "number": 7313, "cdate": 1758015356356, "mdate": 1759897860467, "content": {"title": "DeRaDiff: Denoising Time Realignment of Diffusion Models", "abstract": "Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback–Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to \"reward hacking\". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce _DeRaDiff_, a _denoising-time realignment_ procedure that, after aligning a pretrained model once, modulates the regularization strength _during sampling_ to emulate models trained at other regularization strengths—_without any additional training or fine-tuning_. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse-step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed-form update under common schedulers and a single tunable parameter, $\\lambda$, for on-the-fly control. Our experiments show that across multiple text–image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, by enabling very precise inference-time control of the regularization strength, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.", "tldr": "", "keywords": ["alignment", "diffusion models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b60342edb1ab286577674e15f17c683fad6df41d.pdf", "supplementary_material": "/attachment/f55ff112fca7f4af71c12b4c64ce87342201929f.zip"}, "replies": [{"content": {"summary": {"value": "DeRaDiff introduces a denoising-time realignment procedure for diffusion models that allows practitioners to modulate KL regularization strength on the fly during sampling without any retraining or fine-tuning, by interpolating between a pretrained reference model and a single aligned anchor model through a closed-form Gaussian mixture applied at each denoising step, controlled by a scalar parameter lambda between 0 and 1. The method extends decoding-time realignment from language models to continuous latent diffusion processes, providing an efficient alternative to the expensive practice of training separate models for each regularization strength. Experiments on SDXL and Stable Diffusion 1.5 show that DeRaDiff accurately approximates models aligned from scratch across a wide range of regularization strengths, with mean absolute errors below 0.5% on metrics like PickScore, HPS v2, and CLIP, while reducing computational costs by up to 90% when exploring multiple regularization strengths. Additionally, DeRaDiff can undo reward hacking artifacts by increasing lambda to simulate stronger regularization, offering a practical and scalable solution for hyperparameter exploration in alignment of text-to-image diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "While the idea of decoding-time realignment was recently introduced for language models, the paper non-trivially lifts it to the continuous, iterative denoising process of diffusion: it derives a closed-form Gaussian mixture per timestep, handles scheduler-specific posteriors, and exposes a single scalar λ that maps to any effective KL strength β/λ. This extension is not straightforward—marginalizing over latent trajectories is intractable—so the authors provide a principled step-wise approximation backed by a new theoretical result. DeRaDiff is therefore the first inference-only knob for alignment strength in diffusion, removing the need for expensive sweeps.\n\nAlignment cost is a practical bottleneck for large diffusion models; DeRaDiff removes it by turning a multi-training sweep into a single pass plus sampling-time tuning. This lowers the barrier for researchers and practitioners to explore fine-grained alignment, potentially accelerating RLHF-style workflows, multi-reward composition, and personalised generation. The technique is architecture-agnostic (any scheduler admitting Gaussian posteriors) and complementary to existing alignment objectives (DPO, DDPO, etc.). By demonstrating that careful posterior interpolation can mimic full retraining, the work also hints at broader implications for efficient model merging and test-time adaptation beyond vision.\n\n\nThe submission is technically sound. The derivation (appendix) carefully justifies the Gaussian form, states regularity conditions (λ ∈ [0,1], positive variances), and warns against extrapolation (λ > 1). Empirical coverage is unusually broad: two model families (SDXL, SD-1.5), three human-centric metrics (PickScore, HPS v2, CLIP), 500 prompts spanning two datasets, and six regularization strengths. Error magnitudes are small (<0.5 % of metric means), confidence intervals are supplied, and ablations show stability inside the convex regime. Compute savings are measured in actual GPU-hours and EFLOPs rather than vague “speed-ups”. Minor gaps—no user study on λ control, limited diversity in prompts—do not weaken the overall thoroughness."}, "weaknesses": {"value": "The paper trains anchors only at β ∈ {500, 1000, 2000, 5000, 8000, 10 000}.  \n   - Fig. 5 shows that PickScore saturates after β ≈ 2000; the interesting “knee” region where human appeal rises fastest (β ≈ 100–1500) is sampled very coarsely.  \n   - Because DeRaDiff can **interpolate** (λ < 1) but can only **weakly extrapolate** (λ > 1) before instability, a user who wants to explore β < 500 cannot do so with the supplied β = 500 anchor.  \n   \n\n\n   The authors never run a β-sweep **guided** by DeRaDiff. The experiment pipeline (§5.1) still assumes the user already knows which β-values to test.  \n   \n\n   Fig. 3 and Table 6 show visible degradation once λ ≳ 2.5. Many users will nevertheless try λ > 1 to obtain stronger alignment; at present the paper offers no guard-rail except a verbal warning.  \n   \n\n   PickScore, HPS v2 and CLIP give nearly identical curves (Fig. 5). The experiment therefore does **not** show that DeRaDiff can trade off **aesthetics vs. prompt fidelity**, a key reason one tunes β in practice.  \n   \n\n   All results use Euler-A, 50 steps. Many deployment pipelines use DPM-Solver-12 or 20 steps for speed. The closed-form update assumes the scheduler is **linear in the Gaussian sufficient statistics**; this is only approximately true for some solvers.  \n  \n   A simpler baseline is to take the aligned β = 500 and β = 2000 checkpoints and linearly interpolate their **weights** (θ-ref + λ(θ-2000 − θ-ref)). This costs zero extra inference time and is already used in “model soups” work."}, "questions": {"value": "The ReAlignment problem is a notion that has only recently surfaced in the LLM literature.  \nAs the first work that imports this term into the generative-Art domain, the current manuscript cannot assume that most readers are already familiar with it.  \n\nTo discover what “ReAlignment” actually means, one is forced to consult *Decoding-time ReAlignment of Language Models*.  \nTherefore, I believe the paper should be revised to:  \n\n- Give a concise, self-contained definition of ReAlignment up front,  \n- Explain how the generative-Art setting changes the problem."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5f5yZITqGl", "forum": "TL4cvNviw6", "replyto": "TL4cvNviw6", "signatures": ["ICLR.cc/2026/Conference/Submission7313/Reviewer_kfW9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7313/Reviewer_kfW9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618418797, "cdate": 1761618418797, "tmdate": 1762919427423, "mdate": 1762919427423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an inference-time realignment of diffusion models, to be able to emulate models trained under other regularization scenarios, without extra training or fine-tuning. Inspired by DeRa (from the realm of LLMs), the authors derive a closed-form approximate posterior sampling. The authors claim and briefly discuss the reduced computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is reasonably well-written with a coherent narrative. \n- The idea of extending DeRa (from LLMs) to diffusion models is an interesting angle. \n- The results are rather promising, and align with the core claims."}, "weaknesses": {"value": "- I believe only Fig. 7 for comparison across base, aligned and realigned models is too little, also not discussed in necessary level of detail. In my eyes this should be established with more qualitative results and further elaboration across the images and models.  \n- The paper would benefit from a thorough proof-read. Few typos, and styling inconsistencies can be seem across the document. \n- Maybe (pareto-front) reward vs divergence plots can help establish the core message from a different angle, that just looking CLIP or HPS."}, "questions": {"value": "- Fig 6 (b) is hard to read and interpret. Can't this be done differently? or at least elaborated better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G3lliKWD8Q", "forum": "TL4cvNviw6", "replyto": "TL4cvNviw6", "signatures": ["ICLR.cc/2026/Conference/Submission7313/Reviewer_Jger"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7313/Reviewer_Jger"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744963850, "cdate": 1761744963850, "tmdate": 1762919426896, "mdate": 1762919426896, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author presents a classifier‑free guidance–like formulation designed for reward alignment in diffusion models (DDPM).\nThe experimental results support the formulation's validation."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper makes a worthwhile contribution with a clear presentation and credible theoretical \nsupport.\n\n## Presentation: ~95th percentile\n\nThis paper presents coherence, and most of the idea is clearly addressed. I would like to thank you for saving me a lot of time reviewing your work.\n\n## Soundness: ~75th percentile\n\nTheorem 1 underpins the soundness of the paper. Although I have not examined every minute detail, the derivation appears to be correct.\n\n## Contribution: 40th~70th percentile\nThis method seems novel to me, although I’m not sure if something similar already exists in the literature. It builds a scalable way to tune between the anchor model and the aligned model.\n\n## Note\nI hope the AC is aware that the rating is calibrated using percentiles to reduce evaluation noise effectively."}, "weaknesses": {"value": "## Soundness\n\nI would have preferred to see additional comparisons between your method and other approaches applied to similar problems. Nonetheless, the absence of such comparisons does not undermine the validity of your \nclaim.\n\n## Presentation\n\n1. Presenting the denominator in Equations (3)–(6) as a partition function has both advantages and \ndisadvantages. While it clarifies the interpretation, the repeated form of the \nequations feels redundant. If the repetition is intentional, please justify it explicitly; otherwise, \nconsider consolidating the expressions to avoid unnecessary duplication.\n2. Algorithm 1 appears to be a verbatim transcription of your Python implementation. For readers who \nare more comfortable with Python than with pseudocode, it would be clearer to relocate the algorithm to the appendix and present the actual Python source code there. This approach preserves the practical \nrelevance of the code while keeping the main manuscript concise.\n\n## Contribution\nRegarding a [similar work](https://arxiv.org/abs/2505.18547) working on score-based SDE in the existing literature, it would be helpful to acknowledge it and clarify how your paper differs.\n\nIt is known that DDPM, DDIM, and any score‑based SDE can be reformulated as Karra’s SDE [1] in a \nbidirectional manner. Consequently, these paradigms are theoretically equivalent, although the conversion \nis not trivial. Thus, even if your work is considered concurrent or subsequent, there remains room for a \nmeaningful contribution. Besides, many contemporary studies were developed without awareness of this \nequivalence.\n\n[1] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in neural information processing systems 35 (2022): 26565-26577."}, "questions": {"value": "1. It seems odd to see the thermodynamic variable $\\beta$ placed in the denominator, since $\\beta$ \nis usually regarded as the inverse temperature $\\tau = 1/\\beta$.\n2. I was confused in Line 145 that you cited Song et al. (2020) without any SDE formulation in your paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRWmMQnTKz", "forum": "TL4cvNviw6", "replyto": "TL4cvNviw6", "signatures": ["ICLR.cc/2026/Conference/Submission7313/Reviewer_ziCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7313/Reviewer_ziCu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752210981, "cdate": 1761752210981, "tmdate": 1762919426574, "mdate": 1762919426574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel method called DeRaDiff, which performs per-step denoising realignment. Aligning models to human preferences from scratch is often very time-consuming and computationally expensive. DeRaDiff, on the other hand performs the alignment on the fly during inference by modulating the alignment strength by a parameter $\\beta$. The experimental evaluation shows that the method has comparable results to models aligned from scratch."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper shows that DeRaDiff has a closed-form solution.\n- The training-free inference-time alignment method saves computational costs\n- The method is able to undo reward hacking, eliminating the need for realignment"}, "weaknesses": {"value": "- When evaluating human alignment a real-world human study would have been nice\n- Since the pre-trained reference model is mixed with the aligned model, this could lead to biases reappearing in the output of DeRaDiff."}, "questions": {"value": "Q1: Did you observe whether biases are propagated from the reference model to the output of DeRaDiff?  \nQ2: In the experiments the reward-hacked model had a $\\beta$ of 500. Did you test it with even more reward-hacked models that have even a lower $\\beta$?  \nQ3: Does the method still work if the reference model and the aligned model have different architectures (e.g. DiT and U-Net models)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are no concerns."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3YggVIb43a", "forum": "TL4cvNviw6", "replyto": "TL4cvNviw6", "signatures": ["ICLR.cc/2026/Conference/Submission7313/Reviewer_CgVt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7313/Reviewer_CgVt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833239327, "cdate": 1761833239327, "tmdate": 1762919426194, "mdate": 1762919426194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}