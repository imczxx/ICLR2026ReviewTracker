{"id": "ttMLNXBWKY", "number": 14888, "cdate": 1758245119820, "mdate": 1759897343467, "content": {"title": "SkillFactory: Self-Distillation for Learning Cognitive Behaviors", "abstract": "Reasoning models leveraging long chains of thought employ various cognitive skills such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when base models exhibit these skills, a reasoning model trained by reinforcement learning (RL) can learn to leverage it. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may contain errors, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory initialization helps a model post-RL to generalize to harder variants of the task; (2) cognitive skills are indeed used by the model; (3) the presence of these skills allows for opportunities like budget forcing (driving a model to think longer) that our baselines lack.", "tldr": "We present SkillFactory, a pipeline for priming Language Models with cognitive reasoning skills that enhance reinforcement learning and improves downstream performance.", "keywords": ["Large Language Models", "Reasoning", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91a405346e50da5184375c0b2d18887514284671.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SkillFactory, a self-distillation method that trains small models to learn reasoning skills like reflection and retrying by rearranging their own outputs into structured “silver” traces. After supervised fine-tuning and reinforcement learning, the model shows improved reasoning and generalization on Countdown and OOD tasks without relying on larger teacher models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and clearly organized, making it easy to follow.\n\n2. The data generation process is detailed, combining multiple prompting strategies to self-elicit diverse cognitive skills in small models as well as to ensure diversity.\n\n3. The experiments are comprehensive, covering both in-domain and out-of-distribution evaluations, with detailed length analysis and ablation studies that strengthen the empirical support."}, "weaknesses": {"value": "1. While the idea to equip models with cognitive skills is clear, the current formulation focuses on a small, pre-defined set of tagged skills (e.g., retry and reflection). As these skills are incorporated through explicit templates and tags, it limits the generalization to other cognitive behaviors beyond these predefined patterns.\n\n2. SkillFactory involves both SFT and RL stages, whereas most baselines rely on only one of these training efforts (except STaR). The training effort is not consistent across these methods which leads to unfair comparison.\n\n3. Although SkillFactory’s main strength emerges after the RL stage, its SFT performance remains notably below the R1-distilled baseline (Table 1). While R1-distilled traces are expected to yield stronger results even after RL, the paper does not report such comparisons (both in-domain and OOD), making it difficult to assess how close SkillFactory’s trajectories come to matching teacher-distilled SFT data."}, "questions": {"value": "1. Countdown-3arg seems to be a relatively easy dataset, with performance saturating quickly after training. It seems that SkillFactory assumes the base model is at least sufficiently capable to generate several correct answers during sampling. Would the method still be effective if the base model struggles to produce correct outputs on more challenging datasets?\n\n2. The experiments are conducted using a single model (Qwen2.5-1.5B-Instruct) trained on one dataset (Countdown). Have the authors trained on harder datasets or larger models to assess its generalizability?\n\n3. How is the number of reflection/retry steps determined when composing a SkillFactory trajectory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xuy8VyA9zM", "forum": "ttMLNXBWKY", "replyto": "ttMLNXBWKY", "signatures": ["ICLR.cc/2026/Conference/Submission14888/Reviewer_3nnB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14888/Reviewer_3nnB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896103535, "cdate": 1761896103535, "tmdate": 1762925234173, "mdate": 1762925234173, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for teaching language models cognitive reasoning skills without requiring stronger teacher models. The key insight is that models can learn these skills from rearranged \"silver\" traces constructed from their own outputs. The method involves three stages: (1) sampling diverse solutions and reflections from a base model, (2) rearranging these into structured traces with explicit skill markers using tags, and (3) supervised fine-tuning followed by reinforcement learning. SkillFactory achieves substantial improvements on harder variants and better generalization to out-of-distribution tasks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of creating structured \"silver\" training data by rearranging a model's own outputs is impactful. \n- The paper proposes a sound training pipeline that shows compelling generalization evidence to other tasks. Experiments are done in extensive settings. (RL Only, STaR, BOLT, R1 Distillation)."}, "weaknesses": {"value": "- The entire study uses only Qwen2.5-1.5B-Instruct. Larger models (7B+) may already exhibit these skills naturally. In addition, larger models might have better performance on some baselines’ settings (such as reinforcement learning). The generalizability of conclusions in this paper beyond 1.5B parameters is highly uncertain.\n- Training exclusively on Countdown 3-arg (where solutions are easy to verify but hard to find) is an ideal scenario for reflection/verification skills. The paper lacks discussion of how SkillFactory's effectiveness changes for tasks where verification itself is difficult, subjective, or computationally expensive (e.g., creative writing, legal arguments)."}, "questions": {"value": "- Have you tested with at least one larger model (e.g., Qwen2.5-7)? This is critical to understand whether SkillFactory remains beneficial at practical scales where models may already exhibit some skills naturally. \n- Have you considered an experiment where SkillFactory is trained on a mixture of tasks (e.g., Countdown + GSM8K + Multiplication) rather than just Countdown? This would clarify whether the approach is domain-general or specific to search-like problems, which could change my assessment of the method's practical utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Btt36xMzHA", "forum": "ttMLNXBWKY", "replyto": "ttMLNXBWKY", "signatures": ["ICLR.cc/2026/Conference/Submission14888/Reviewer_pNtV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14888/Reviewer_pNtV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957692568, "cdate": 1761957692568, "tmdate": 1762925233806, "mdate": 1762925233806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a self-distillation framework that constructs silver SFT traces by sampling multiple responses from a base model (without instruction following fine-tuning), forming a long context-style SFT data with reflections. The proposed method conducts experiments that use these silver SFT data as a warm-up of the RL stage. Experiments show the model trained with warm-up data can have better OOD performance on harder variants of toy and real tasks, such as Countdown, 3-digits multiplication."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and well-organized. \n2. Related work is comprehensive, and the authors carefully position SkillFactory relative to RL-only, distillation from stronger models, and self-distillation methods."}, "weaknesses": {"value": "1. The title \"SkillFactory\" suggests a broad capability to learn diverse cognitive skills, but the implemented pipeline focuses on retrying and reflection. As L171 states, the proposed method has three steps: sampling diverse solutions, generating reflections, and assembling structured traces. It does not convincingly involve a wide variety of skills beyond long CoT with explicit verification and retry. \n2. Several prior works adopt a similar idea of sampling multiple attempts and assembling them into longer, structured traces with reflections, such as injecting \"wait\" to force the model to think more or combine multiple traces. The author discussed these methods in the related works, and makes the claim that \"SkillFactory is similar to these methods, but focuses on generating data entirely from the base model and highlights that structure is key for the generalization of consistent skill use.\" Firstly, I appreciate that the author made a clear statement of the relation against prior works. However, this motivation may be insufficient for a top-tier conference paper. Why does a similar idea purely rely on the base model to make a new method?\n3. The main results take the Qwen2.5-1.5B-Instruct and focus on toy tasks, like Countdown, digit multiplication, and Letter. This limits the strength of the claims about general-purpose reasoning skill acquisition. Results on widely accepted long-CoT benchmarks, such as AIME/AMC/GPQA-Diamond with 7B size models, would substantially improve the paper."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zi3zI6AcOr", "forum": "ttMLNXBWKY", "replyto": "ttMLNXBWKY", "signatures": ["ICLR.cc/2026/Conference/Submission14888/Reviewer_KXRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14888/Reviewer_KXRf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985566311, "cdate": 1761985566311, "tmdate": 1762925233404, "mdate": 1762925233404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a synthetic data generation technique for self-distilling reasoning behaviors into language models. The authors show that their method, SkillFactory, helps LLMs learn these behaviors by training on the game of countdown. they also show transfer to other domains like Common Sense QA, GSM8k etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and the method is straightforward.\n- The generalization performance of the model trained on 3 dig countdown is impressive.\n- The method matches or surpasses distillation from stronger models.\n- The paper also has good ablations and baselines."}, "weaknesses": {"value": "- The biggest weakness of the paper is that all experiments are with the qwen2.5-1.5B model. Adding a model from another family and a different model size would help show the generalizability of the method.\n- I have some questions about circularity: How do you reconcile necessity of the method if behaviors are already present in the pretraining data? “skills surface less consistently.” If the model can generate correct solutions and reflections (required for silver traces), why can't RL alone elicit these behaviors?\n- If the reasoning behaviors aren’t present in the pretraining data, how does the model produce those?\n- The paper mentions silver traces \"may contain errors\" but provides no analysis of error rates, types, or impact on learning."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QmJA8lTkjJ", "forum": "ttMLNXBWKY", "replyto": "ttMLNXBWKY", "signatures": ["ICLR.cc/2026/Conference/Submission14888/Reviewer_VLtu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14888/Reviewer_VLtu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14888/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043786547, "cdate": 1762043786547, "tmdate": 1762925232943, "mdate": 1762925232943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}