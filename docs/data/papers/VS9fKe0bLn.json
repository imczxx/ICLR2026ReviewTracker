{"id": "VS9fKe0bLn", "number": 19563, "cdate": 1758297288301, "mdate": 1759897032349, "content": {"title": "Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT", "abstract": "We introduce Guard Vector, a safety task vector computed as the parameter difference between a guardrail model (Guard Model) and a same-architecture pretrained language model. Composing this vector with a target language model yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware approach that combines prefix-based training and evaluation with a classifier that produces a single-token output. With this composition alone, TGM improves classification quality over established Guard Models across standard safety suites and enables language extensibility to Chinese, Japanese, and Korean, requiring neither additional training nor target language labels. It also demonstrates model portability across two widely used public guardrail backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM preserves classification quality under streaming by aligning the behavior between prefix inputs and full-text inputs. The single-token output design increases throughput and reduces latency. Together, these components reduce data and compute requirements while promoting streaming-aware evaluation practices, thereby contributing to a more responsible AI ecosystem.", "tldr": "We propose Guard Vector, a task-vector composition that transfers safety to target language models from public weights with no extra training or labels. A streaming-aware prefix SFT preserves parity under streaming and improves efficiency.", "keywords": ["Large Language Model", "Guardrail", "Safety alignment", "Cross-lingual safety", "Task-vector composition", "Streaming-aware prefix SFT"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/50fa0884b3c9b1778d537c54cdfd6255a5aab264.pdf", "supplementary_material": "/attachment/b897dc71a6ad48fe484482200379d7eab31a1317.zip"}, "replies": [{"content": {"summary": {"value": "This work presents an experimental study on constructing a target guard model by combining a continued pretraining model for a specific language with a Guard Vector, obtained through the parameter difference between a Guard Model and its corresponding pretrained model. The study demonstrates that safety behaviors can be effectively transferred through parameter vector migration. Additionally, the authors explore a prefix-based SFT strategy to reduce inference latency in streaming mode while maintaining parity in classification accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Provides strong empirical evidence that safety behaviors can be transferred without additional training data, merely through parameter vector manipulation.\n- Proposes an interesting and practical prefix-based protocol that enables low-latency inference with a single-token prefix, achieving performance parity in streaming mode."}, "weaknesses": {"value": "- Readability issues: The abstract introduces multiple ideas simultaneously, making the central focus difficult to grasp. The Evaluation Metrics section (lines 276–293) requires reformatting for clarity. In Table 6, the F1 delta arrows are ambiguous and may lead to misinterpretation.\n- Dependency on resources: The approach relies on the availability of open-source guard and pretrained models. Moreover, the continued pretraining on a language-specific corpus limits the demonstrated cross-lingual generalization.\n- Limited task and language coverage: The experiments focus solely on classification tasks within a narrow set of languages."}, "questions": {"value": "- Could the authors evaluate both Gemma and Llama models on each of the test datasets presented in Table 6 to enable direct comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7rhWbbdmJM", "forum": "VS9fKe0bLn", "replyto": "VS9fKe0bLn", "signatures": ["ICLR.cc/2026/Conference/Submission19563/Reviewer_zsJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19563/Reviewer_zsJr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775084895, "cdate": 1761775084895, "tmdate": 1762931444990, "mdate": 1762931444990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Guard Vector, a task-vector style method to transfer safety/guardrail behavior from an English guard model (e.g. Llama Guard 3) into a non-English continual-pretrained (CP) model of the same architecture, and then makes this target guard model streaming-aware via prefix SFT with single-token classification. The goal is to (i) get guardrails “beyond English” without extra target-language labels, and (ii) make guardrails actually usable in production streaming settings, where early detection + low latency matter. The paper evaluates on Korean datasets plus a helpfulness (all-SAFE) set, and even shows portability to Gemma via ShieldGemma."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of task-vector composition (Guard Vector) is a highly practical and novel approach for cross-lingual safety alignment. It effectively transfers safety behaviors to Chinese, Japanese, and Korean with significant F1 score gains over baselines, all without requiring any additional training or target language labels. This drastically lowers the barrier to deploying guardrails in diverse language environments."}, "weaknesses": {"value": "1. Limited Technical Depth in Task Vector Composition: The paper defines the Guard Vector simply as the parameter difference and the composition as a simple addition (Equations 2 and 3). While the results are good, there is no in-depth analysis of why this simple linear composition works so effectively in the cross-lingual setting, especially compared to more complex vector merging techniques like TIES-Merging or other task arithmetic methods. The exclusion of LayerNorm parameters is mentioned but the motivation is only briefly cited from prior work. A deeper analysis of the parameter space and vector alignment would strengthen the technical novelty.\n\n2.  Comparison baselines need to be tighter: They compare to LG3, Kanana, ShieldGemma-origin variants, but comparing against: (i) a strong multilingual safety-tuned LLM (e.g. XLM-R-based or mT5-based safety classifier), (ii) a simple translate-then-classify baseline (translate to English → Llama Guard 3) would strengthen the claims.\n\n3. Lack of failure case analysis: While the paper carefully compares offline vs. streaming regimes and even surfaces an important negative result (full-text SFT collapses under streaming), it does not provide a dedicated error/failure analysis section."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kP0PCNkfNQ", "forum": "VS9fKe0bLn", "replyto": "VS9fKe0bLn", "signatures": ["ICLR.cc/2026/Conference/Submission19563/Reviewer_bZ3s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19563/Reviewer_bZ3s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948773302, "cdate": 1761948773302, "tmdate": 1762931444508, "mdate": 1762931444508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to transfer safety behaviors between large language models without retraining. It defines a ''Guard Vector'' as the parameter difference between a safety-aligned guard model and its base pretrained model, then adds this vector to another model’s weights to create a Target Guard Model that inherits safety traits. To enable real-time moderation, the authors introduce Streaming-Aware Prefix Supervised Fine-Tuning, which trains on partial prefixes for early risk detection, and a single-token classifier to reduce latency. Experiments on English and Korean datasets show large gains in F1 score, faster throughput, and nearly identical offline versus streaming performance, while also demonstrating transfer across architectures (Llama↔Gemma) and languages (CJK)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed Guard Vector approach that computing a parameter delta between a safety-tuned model and its base pretrained model, then composing that delta with another model in a target language. The authors also introduce prefix SFT to adapt the resulting guardrail for streaming, enabling early risk detection with a single-token classifier. The experimental section is extensive, covering Korean, Japanese, and Chinese setups, and the results are consistently strong across Llama and Gemma. \n1. The paper has a good structure and logic, and the appendix provides more experimental details.\n2. A new Composition idea to define a clear and reproducible task-vector arithmetic framework for transferring guardrail behaviors.\n3. Good Experimental design with quantitative metrics and comprehensive analysis."}, "weaknesses": {"value": "The Guard Vector idea, although effective, largely repurposes existing task-vector and model-merging techniques with limited theoretical or analytical novelty. Some important design choices, such as omitting LayerNorm parameters, fixing τ = 0.5, and using 100-character prefix intervals—are presented without justification or sensitivity studies. \n1. Despite mentioning Chinese and Japanese results, the analysis and datasets are minimal; nearly all in-depth experiments centre on Korean.\n2. The monotonic SAFE→UNSAFE assumption excludes self-correcting responses, potentially biasing the dataset and overestimating early detection accuracy.\n3. Full-text SFT collapses under streaming (e.g., −15.23 to −26.65 F1 in Table 2), but the paper does not provide more analyse  results."}, "questions": {"value": "1. Why 100 characters instead of token-based scheduling, and how sensitive is TTD to this granularity?\n2. Could you explain why throughput per-token barely improves despite a single-token classifier? Does this indicate that latency reduction comes mostly from fewer decode loops rather than actual compute efficiency (Table 4)?\n3. When composing ShieldGemma → Korean Gemma 2 IT, F1 improves +10.6 pp. Did you consider whether this gain persists if the Gemma CP model is multilingual rather than Korean-specific? Otherwise, improvements might be due to data domain alignment, not guard vector transfer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uXGqnNHMcW", "forum": "VS9fKe0bLn", "replyto": "VS9fKe0bLn", "signatures": ["ICLR.cc/2026/Conference/Submission19563/Reviewer_CJqQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19563/Reviewer_CJqQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762622153386, "cdate": 1762622153386, "tmdate": 1762931443858, "mdate": 1762931443858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}