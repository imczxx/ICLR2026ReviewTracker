{"id": "3LGjvzYgdt", "number": 9716, "cdate": 1758136422929, "mdate": 1759897702679, "content": {"title": "Enhancing Zero-Shot VLM Reward Models Through Structure-Aware Fine-Tuning", "abstract": "Designing effective reward functions remains a major bottleneck in Reinforcement Learning (RL). Recent work uses large foundation Vision-Language Models (VLMs) as zero-shot reward models, computing text–observation similarity to bypass manual reward engineering. Although promising, these rewards are noisy, brittle, and misaligned with ground-truth objectives. We introduce Structure-Aware Fine-Tuning (SAFT), a lightweight, LoRA-based method that adapts frozen VLM reward models online using simple structural priors. SAFT enforces invariances and proportionality in the reward signal via augmentations and auxiliary losses, yielding smoother and more consistent reward landscapes. Experiments across classic control and robotic manipulation tasks show faster policy convergence, substantially improved alignment with ground-truth rewards, and elimination of the extensive human annotation effort that Preference-based Reinforcement Learning (PbRL) would otherwise require. These results establish structure-aware fine-tuning as a simple path toward stable, text-conditioned reinforcement learning.", "tldr": "Reward design in RL is hard. VLMs offer zero-shot rewards but are noisy and misaligned. SAFT fine-tunes them with structural priors, yielding smoother rewards, faster learning, better alignment, and less need for human annotation.", "keywords": ["Reinforcement learning", "VLM", "Zero-shot rewards", "Reward alignment", "Text-conditioned policies", "Structure-aware fine-tuning (SAFT)", "RLHF", "CLIP", "Contrastive augmentation", "Lipschitz regularization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3133929472669702e5cab06e72517b22df129251.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Structure-Aware Fine-Tuning (SAFT) to improve zero-shot VLM reward models used in RL. A CLIP-style VLM provides dense rewards via image–text cosine similarity; SAFT inserts small LoRA adapters in the image encoder (text frozen) and fine-tunes them online during policy learning using simple structural priors. Two auxiliary, ground-truth-free objectives are introduced: Contrastive Augmentation Loss (CAL), enforcing reward invariance under task-preserving transforms, and Reward Lipschitz Regularization (RLR), encouraging proportional reward changes for nearby observations. SAFT is evaluated on CartPole, MountainCar, Reach, and ReposeCube; reported outcomes include faster policy convergence, reduced EPIC distance to ground truth, and large reductions in preference labels versus a PbRL baseline."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Clear high-level goal. Improve VLM reward reliability for RL without GT at policy time.\n\nVery interesting idea. Structure-aware fine-tuning (SAFT) that encodes geometric/temporal invariants; plausible inductive bias."}, "weaknesses": {"value": "As currently written, this paper lacks sufficient information to be reproduced or reviewed. The central issue is that the authors do not specify what loss function is used for pre-training the VLM for experiments in sections 4.2 and 4.3. The language of section A.2 (and in general, the rest of the paper) is highly ambiguous and extremely lacking in clearly-articulated details. Two sensible interpretations can be identified:\n\n- Interpretation 1: The pre-training loss is the dense ground truth (GT) rewards the authors manually engineer for each task (which, incidentally, is only explicitly defined for some environments; but is not for the CartPole one, notably; also compromising reproducibility). \n- Interpretation 2: The pre-training loss is their CAL/RLR objectives (which also require per-environment manual work, but are at least unsupervised).\n\nLanguage by the authors would suggest the first interpretation (\"This pretraining is performed online by regressing the VLM reward predictions toward the true task rewards.\"), which at least we know to apply to section 4.3; however, whether it also applies to results in 4.2 is unclear, as the grammar of this paragraph is poor.\n\nIf the method the authors used for their results is Interpretation 1 (\"pre-training loss = dense GT rewards\"), this would imply full leakage of the manually designed ground truth to the VLM, and invalidate all evidence the paper presents in support of their method and make most of their statements false, given the problem they purport to solve: avoiding the expensive and manual design of reward functions and replacing them with reward signals from vision-language models. At that point, in their experiments, all they show is that the VLM becomes, at best, a way to map from pixel observations to the state space, which is, granted, an interesting avenue of research, but not related to the claims of the paper, and requiring entirely different evaluation baselines. This would not, in principle, refute the validity of the method (it could still be a good method), but it would fully invalidate any evidence in support of it. Of course one would expect to find that such a method would \"substantially improve alignment with ground-truth rewards\", since the authors are training on the evals!\n\nIf, in contrast, Interpretation 2 is true (\"pre-training loss = CAL/RLR objectives introduced by the authors\"), then the baseline is just an ablation of the author's own method; claims should be framed as \"online SAFT improves over offline SAFT-style pretraining\" and the like; and, in any case, never \"over zero-shot VLM\", which is the central framing of the paper. There is nothing zero-shot about pre-training a VLM per-task, using per-task knowledge. Only if the pre-training were cross-task or, being per-task, used a loss function that depended in no way on knowledge about the task, would it make sense to talk about \"zero-shot\". Either way, the absolute non-negotiable baseline needed if Interpretation 2 is true would be to compare the results with a _frozen_ VLM (no pre-training). Assuming the results turned out to be positive (I find this plausible), I would still reject the vast majority of the claims and language used in the draft. A full re-framing of the results would be required to pass academic integrity standards, though could be still a very valuable contribution to the literature. Knowledge of geometric invariants about tasks is an extremely strong assumption when trying to jump from toy CartPole examples to general-purpose robotics tasks. The strength of VLMs for reinforcement learning is that knowledge about these invariants _emerges_ during pre-training due to the downstream generalization capabilities of foundation models. Any attempt to improve on this method should either (1) be comparably general or (2) clearly frame and explain the specific extent of applicability of the additional assumptions introduced. For example, for the latter, geometric invariants are cooked into physics-informed neural networks regularly, because the relationship between the assumptions made and the problem they solve is self-evident. To make the author's method valuable, they would have to either (1) significantly adapt their language, to make it clear to readers the obvious caveats of the approach instead of burying it at the end, or (2) provide a serious discussion of the extension capability of their method, which could well be, for all I know, widely extensible to more complex tasks, subject to evidence of it being provided.\n\nThe paper has a few other smaller issues, but those are inconsequential relative to the aspects outlined above.\n\nIf the authors were to entirely remove any per-task pretraining, updated all their experiments, and the results were still very strong (subject to seriously improving the writing with more clarity on reproducibility, added source code, etc.), my estimation of the paper's quality would significantly change; I'd rate it a 6 or 8, depending on specifics."}, "questions": {"value": "Pretraining loss. Precisely what objective was used to “minimally pretrain” the VLM until it can solve tasks (GT regression? CAL/RLR? something else)? Provide the equation and targets. \n\n4.2/4.3 pretraining budget. Total rollouts/steps, stopping criteria (“until it solves” is not reproducible), and any early-stopping or validation signals.\n\nGT reward formulas. Give explicit mathematical definitions for each environment’s “true task reward” used in §4.3 pretraining (e.g., MountainCar “distance from right side,” Reach’s top-down projection). Include any shaping terms.\n\nFrozen VLM baseline across all tasks. IIUC, you show off-the-shelf ViT-B-16 only for CartPole (2 seeds). Please add all 4 envs with ≥5 seeds.\n\nCAL specifics used in main runs. exact counts/sampling for positives/negatives per step (p, n), any temporal distance thresholds for “soft negatives,” and the β value used in main results (not just ablation).\n\nRLR normalization details. in Eq. (6), clarify how $\\tilde{o}_i = {o_i}/{\\bar{o}}$ is implemented for images (per-pixel mean over window? per-channel? scaling range?), and how pairs are batched when you “backpropagate once per rollout.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BOM6oI0ZYP", "forum": "3LGjvzYgdt", "replyto": "3LGjvzYgdt", "signatures": ["ICLR.cc/2026/Conference/Submission9716/Reviewer_fyXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9716/Reviewer_fyXP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761434200702, "cdate": 1761434200702, "tmdate": 1762921215858, "mdate": 1762921215858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SAFT (Structure-Aware Fine-Tuning), a method for fine-tuning a Vision-Language Model (VLM) to produce more reliable reward signals (i.e., similarity scores) conditioned on visual observations and language-based task descriptions. The authors propose two self-supervised objectives, Contrastive Augmentation Loss (CAL) and Reward Lipschitz Regularization (RLR), to fine-tune the LoRA adapters of the VLM alongside policy training. The proposed method is evaluated on both classical control tasks (CartPole and MountainCar) and robotic manipulation tasks (Reach and ReposeCube), showing improvements over the original VLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed self-supervised objectives for fine-tuning VLMs seem novel and well-motivated, helping stabilize reward estimation in environments whose visual observations differ from the VLM's pretraining data.\n- The descriptions and visual illustrations of the two objectives (CAL and RLR) are clear and make their roles in improving reward stability easy to understand."}, "weaknesses": {"value": "- While the proposed objectives for fine-tuning VLMs are conceptually well-motivated by structural properties (symmetry and smoothness), the experiments are conducted only on simple environments that seem particularly suited to the proposed techniques. This raises concerns about the generalization ability and practical applicability of SAFT in more diverse or realistic settings. Prior works such as RoboCLIP [1] and VLM-RMs [2] demonstrated results in more complex environments (e.g., humanoid control or robot arm manipulation), yet it remains unclear whether SAFT can perform effectively in such settings or whether crafting augmentations in these environments would be as straightforward. \n\n- Among closely related works (RoboCLIP, VLM-RMs, FuRL), only VLM-RMs is included for comparison. Additional comparisons, especially with FuRL, which also fine-tunes VLMs, are necessary to more comprehensively evaluate the effectiveness of the proposed method.\n\n- The paper mentions that states differing only by viewpoint changes or background noise (L199–L202) should have identical reward values, however, no experiments involving corresponding augmentations or transformations are provided to support this claim.\n\n- Lack of analysis for different window sizes $W$.\n\n- In Section 4.2, it is unclear how the paper performs \"minimal pretraining\" of the VLM such that the base VLM model can solve the tasks (L295–L297). This point remains ambiguous even after reading the Appendix (L740–L742). Which loss function is used for fine-tuning at this step?\n\n- The paper claims that the proposed method substantially improves alignment with ground-truth rewards; however, as shown in Figure 6, among the four environments, only CartPole shows a noticeable match to ground-truth reward performance.\n\nMinor comments:\n- The EPIC distance should be introduced in Section 4.3 rather than Section 4.4 to provide better context for L375–L377 and Figure 7.\n\n\nReferences:\n\n[1] Sontakke, Sumedh, et al. \"Roboclip: One demonstration is enough to learn robot policies.\" NeurIPS 2023.\n\n[2] Rocamonde, Juan, et al. \"Vision-language models are zero-shot reward models for reinforcement learning.\" ICLR 2024.\n\n[3] Fu, Yuwei, et al. \"FuRL: Visual-language models as fuzzy rewards for reinforcement learning.\" ICML 2024."}, "questions": {"value": "1. How does SAFT perform in more complex settings, as evaluated in RoboCLIP or VLM-RMs? How does SAFT compare to RoboCLIP and FuRL?\n\n2. Could you clarify how the base VLM is pretrained such that it can solve tasks without SAFT?\n\n3. Why does the Goal-Baseline Regularization baseline (VLM-RMs) perform worse on CartPole? In VLM-RMs's paper, this baseline showed perform well on CartPole, and also evidenced in [4].\n\n4. Could you provide an analysis for different window sizes $W$?\n\n5. Is there any factor that controls the strength of RLR during fine-tuning?\n\n\nReferences:\n\n[4] Wang, Yufei, et al. \"RL-VLM-F: Reinforcement learning from vision language foundation model feedback.\" ICML 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i0vQEe7Wej", "forum": "3LGjvzYgdt", "replyto": "3LGjvzYgdt", "signatures": ["ICLR.cc/2026/Conference/Submission9716/Reviewer_dAd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9716/Reviewer_dAd2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651652044, "cdate": 1761651652044, "tmdate": 1762921215430, "mdate": 1762921215430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Structure-Aware Fine-Tuning (SAFT), a lightweight method for improving zero-shot Vision-Language Model (VLM) reward models in reinforcement learning. SAFT leverages LoRA-based fine-tuning with augmentation and auxiliary losses. The method is evaluated on classic control and robotic manipulation tasks, demonstrating improved sample efficiency, reduced reliance on human preference labels, and better alignment with ground-truth rewards compared to baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Combines contrastive learning and Lipschitz regularization in a novel way for fine-tuning VLM reward models;\n2. Reduces the need for costly human feedback and enables smaller models to perform comparably to larger ones."}, "weaknesses": {"value": "1. The method assumes the base VLM has a reasonable initial understanding of the task; it may fail if the VLM is fundamentally misaligned.\n2. Lack of comparisons on more complex tasks, such as some of the evaluation environments used in [1]."}, "questions": {"value": "The authors claim that their proposed method is more robust to visual variations compared to previous approaches [1][2]. Can results on more complex tasks be provided to support this claim? Additionally, would the prior assumptions underlying Lipschitz Regularization remain valid in such tasks? For example, high-dimensional human-robot tasks from or visual RL tasks with background perturbation in [1].\n\n[1] Rocamonde, et al. VISION-LANGUAGE MODELS ARE ZERO-SHOT REWARD MODELS FOR REINFORCEMENT LEARNING. ICLR, 2024.\n[2] Fu, et al. FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning. ICML, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VJws9Cjv7E", "forum": "3LGjvzYgdt", "replyto": "3LGjvzYgdt", "signatures": ["ICLR.cc/2026/Conference/Submission9716/Reviewer_WVB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9716/Reviewer_WVB3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992018351, "cdate": 1761992018351, "tmdate": 1762921214500, "mdate": 1762921214500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Structure-Aware Fine-Tuning (SAFT), a lightweight LoRA-based method that enhances zero-shot vision-language reward models for reinforcement learning. Instead of relying on human feedback or ground-truth rewards, SAFT enforces structural priors—invariance and proportionality—through two auxiliary objectives: the Contrastive Augmentation Loss (CAL) and Reward Lipschitz Regularization (RLR). These objectives impose smoothness and consistency on the VLM-derived reward landscape. Experiments across classic control and robotic manipulation tasks demonstrate significant gains in sample efficiency, alignment with ground-truth rewards, and reductions in human annotation effort."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem the paper aims to address is highly important. Adjusting VLM-based rewards to better suit downstream tasks without labeled supervision represents a very promising research direction.\n\n2. In terms of performance, the results across the four evaluated tasks effectively support the authors’ claims.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "I am somewhat concerned that the proposed structure-aware component may be task-specific, tailored to each environment. For more complex or diverse tasks, such manually designed structural rules might not generalize well and could easily become ineffective."}, "questions": {"value": "I also share a concern about whether this algorithm can be successfully applied to more realistic embodied intelligence scenarios, rather than being limited to simulated environments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IX6IbQpxu8", "forum": "3LGjvzYgdt", "replyto": "3LGjvzYgdt", "signatures": ["ICLR.cc/2026/Conference/Submission9716/Reviewer_kTvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9716/Reviewer_kTvJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9716/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012954319, "cdate": 1762012954319, "tmdate": 1762921214239, "mdate": 1762921214239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}