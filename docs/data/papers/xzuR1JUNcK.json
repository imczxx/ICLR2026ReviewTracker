{"id": "xzuR1JUNcK", "number": 16506, "cdate": 1758265329944, "mdate": 1759897236422, "content": {"title": "Stabilizing Heterogeneous Federated Learning via Feature Decorrelation and Bidirectional Alignment", "abstract": "Data heterogeneity poses a major challenge in federated learning, leading to significant degradation in global model performance. Prior studies have shown that heterogeneity induces dimensional collapse and biased classifiers, which hinder the learning of both feature extractors and classifiers. To tackle these issues, existing approaches apply feature decorrelation to mitigate dimensional collapse and adopt a synthetic classifier with a projector to reduce classifier bias. However, these decorrelation methods fail to prevent small singular values from collapsing to zero, slowing the mitigation of dimensional collapse. Besides, the synergy among the feature extractor, projector and synthetic classifier is overlooked, leading to divergent optimization across clients. To overcome these limitations, we propose FedBlade, a federated learning framework with bidirectional alignment and feature decorrelation. Our feature decorrelation method accelerates the mitigation of dimensional collapse by yielding exponential gradients, while the bidirectional alignment method enhances synergy among model modules and ensures consistency across clients. Extensive experimental results demonstrate that FedBlade outperforms relevant baselines and achieves faster convergence of the global model.", "tldr": "This paper proposes FedBlade, a federated learning framework with bidirectional alignment and feature decorrelation.", "keywords": ["federated learning", "data heterogeneity", "dimensional collapse", "feature alignment"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3db0700e028929f0c18eb3549275358614ebf3bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- Connects the the non-IID problem in Federated Learning (FL) to the collapse of singular values. While the connection of the singular values and the collapse that happens in non-IID FL is well documented in prior work, the authors make a point that previous studies do not focus on preventing collapse of the smaller singular values.\n\n- Introduces a new decorrelation method of minimizing the negative log-determinant of the representation correlation matrix. They argue that this approach prevents the collapse of the of the smaller singular values, which expands the useable feature space. With the expanded space, they argue that their alignment method improves performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors do well in emphasizing the need to focus on the small singular values.\n- The design of the decorrelation loss is quite clever. I don't think I've seen the determinant being used for a decorrelation loss, but this makes sense when we're focusing on the smaller singular values.\n- Decent results in terms of performance and speedup."}, "weaknesses": {"value": "- The main problem I have with this paper is the lack of transparency for the efficacy of the method. The authors' central claim is that their method increases effective rank, as their loss more harshly penalizes the small singular value. Since the authors claim this, they MUST also provide the effective rank for other methods, so readers can truly see that their method is effective not only in increasing the effective rank, but also the performance (which they do show). From Figure 4, they only show ablations of their own method. I could not find the effective rank for other methods.\n- I also think the authors have missed two key citations [1], [2]. [1] introduced the idea of keeping the classifier fixed, because the classifier leads to much bias. [2] made connections to the singular values, debiasing, and increasing feature space.\n- The core idea of this paper is also quite similar to that of [2]. FedUV uses Uniformity to increase the feature space, and Variance to debias the classifier. I feel this paper needs to frame their paper in a different light to highlight their novelty (likely the decorrelation loss).\n\n- I feel the name of 'bidirectional alignment' is quite poor. It's alignment to the projector and the output of the encoder. There's no 'bi-directional' anything going on. Maybe 'bi-focused'? I'm sure there would be a better name. The goal is the convey that you are targeting two places in the network, not two directions.\n\n[1] Fedbabu: Towards enhanced representation for federated image classification, ICLR 2022.\n[2] FedUV: Uniformity and Variance for Heterogeneous Federated Learning, CVPR 2024"}, "questions": {"value": "1. Can the authors provide more transparency into the effective rank for other methods? They should also provide their rationale backed by as much data as they can provide regarding this point.\n2. Can the authors frame their method in a more novel light, by first citing the very relevant papers mentioned above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JDVwWg8Mub", "forum": "xzuR1JUNcK", "replyto": "xzuR1JUNcK", "signatures": ["ICLR.cc/2026/Conference/Submission16506/Reviewer_BH9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16506/Reviewer_BH9Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761000416023, "cdate": 1761000416023, "tmdate": 1762926599974, "mdate": 1762926599974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedBlade, a federated learning framework addressing label skew via two main components: (1) LDDecorr, a log-determinant based feature decorrelation regularizer that produces exponential gradients to impose stronger penalties on small singular values, thereby mitigating dimensional collapse better than FedDecorr; and (2) PBA, a prototype-guided bidirectional alignment mechanism that aligns the feature extractor and projector with a shared ETF classifier through global prototypes. The whole method builds upon FedETF and FedDecorr, aiming to enforce neural collapse and reduce dimensional collapse, under label skew FL."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and intuition of this work are clear and sound, which provides a well-articulated motivation for addressing dimensional collapse and classifier bias in federated learning. \n\n2. This work has solid theoretical grounding. Theoretical analysis connecting the log-determinant term to spectrum isotropy and effective rank is insightful.\n\n3. Using global prototypes as bridges between modules (extractor–projector–ETF classifier) is a natural and elegant idea that improves interpretability.\n\n4. The paper is well-structured, and the presentation is clear and easy to follow."}, "weaknesses": {"value": "1. The proposed method is an incremental improvement over FedETF and FedDecorr, mainly combining a modified decorrelation loss and a prototype-based alignment. The novelty is moderate.\n\n2. The improvement is limited and often small, especially under full client participation.\n\n3. The ablation results are somewhat puzzling: on CIFAR-100, LDDecorr improves accuracy while PBA reduces it; on Tiny-ImageNet, the opposite trend occurs. This suggests dataset-specific sensitivity or complex interactions between the two modules that are not well explained in the paper.\nInterestingly, when both components are applied together, the overall performance improves significantly and consistently across datasets, implying that LDDecorr and PBA may complement each other in a deeper way. For instance, LDDecorr might enhance the representation isotropy that PBA relies on for effective alignment, while PBA may in turn stabilize the projection space required for LDDecorr to operate effectively. A more detailed and theoretical analysis of this potential synergy would strengthen the paper’s understanding of why the two modules jointly work better than each alone.\n\n4. Although LDDecorr theoretically reduces computational cost via Cholesky decomposition, there exists a bad case:  on Tiny-ImageNet with α=0.5, where FedDecorr converges faster. Authors should explain why.\n\n\n5. The experimental scope is narrow. Experiments use only MobileNetV2 and three vision benchmarks. Testing on additional architectures or non-vision datasets would strengthen the generality claim.\n\n6. Lack of cost metrics. No wall-clock, FLOPs, or communication-overhead analysis is provided to substantiate claims of improved efficiency."}, "questions": {"value": "1. The improvements under full client participation seem limited. Could the authors provide a detailed analysis of why FedBlade’s advantages diminish in this setting?\n\n2. The ablation trends are inconsistent across datasets (Table 3). Can the authors explain why LDDecorr and PBA sometimes have opposite effects? Is this due to prototype drift, sensitivity to γ/β, or data distribution differences?\n\n3. Could the authors show convergence curves or wall-clock time comparisons for α=0.5 to better support the claim of “faster convergence”?\n\n4. How sensitive are the results to hyperparameters β, γ, and τ?\n\n5. Does PBA introduce additional communication overhead when aggregating global prototypes, and if so, how significant is it?\n\n6. The paper mentions that FedBlade “enforces neural collapse.” Could the authors provide quantitative evidence (e.g., NC1–NC4 metrics) to substantiate this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o5L5wOiEeS", "forum": "xzuR1JUNcK", "replyto": "xzuR1JUNcK", "signatures": ["ICLR.cc/2026/Conference/Submission16506/Reviewer_CBFg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16506/Reviewer_CBFg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833906459, "cdate": 1761833906459, "tmdate": 1762926599096, "mdate": 1762926599096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-pronged approach to dealing with label skew in Federated Learning for natural image classification. The first component modifies the regularization term introduced in FedDecorr (which encourages different dimensions of representations to be uncorrelated) from the Frobenius norm of the correlation matrix to the negative log-determinant of the same. The second component consists of two sub-components: (i) the projector & frozen ETF classifier head first proposed in FedETF that leverages the neural collapse phenomenon to reduce classifier bias (ii) global class prototypes (like in FedProto) that both the projector and main network are aligned with via separate alignment losses. Experiments are conducted on CIFAR10/CIFAR100 and TinyImageNet, under Dirichlet non-IIDness with alpha values 0.05, 0.1, 0.5, using an untrained MobileNetV2, against 8 baseline methods. There is also an ablation experiment of the components, and sensitivity analysis of the introduced hyper-parameters."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper benefits from a clear motivation, namely mitigating dimensional collapse and addressing classifier bias at the same time. Figure 1 helps build intuition to this effect. The idea itself is interesting even if its not that novel (see W1). The experimental setup for the most part falls in line with similar papers, and the range of compared baselines is pretty diverse. The paper is generally well-written, with good pacing, and the target problem is an important one."}, "weaknesses": {"value": "I appreciate the authors' efforts in tackling the important problem of heterogeneity in federated learning. However, I have some concerns that I hope can be addressed to strengthen the contribution:\n\n1. Theoretical novelty and experimental depth: The paper presents an interesting combination of previous work, but I found myself wanting more depth in certain areas:\n* The approach builds upon FedDecorr (replacing the Frobenius norm with log-determinant) and FedETF (incorporating prototypes similar to FedProto/FedNH [3]). Given this foundation, I believe the paper would benefit from either theoretical justification for why this combination is effective or more comprehensive empirical validation.\n* The log-determinant technique for matrix rank minimization is well-established [1], so it would be helpful to clarify the specific novelty here.\n* While the methodology section presents various formulas, some appear to go unused in the final approach. Additionally, formal analysis (e.g., convergence guarantees or theoretical properties) would strengthen the contribution.\n* The experimental scope could be broadened to include more diverse heterogeneity scenarios (e.g., domain shift, real-world datasets, less extreme α settings). I also noticed the model backbone doesn't leverage recent FL research showing that pre-trained models and architectures without BatchNorm can significantly mitigate non-IID performance issues.\n\n2. Related work positioning: The paper would benefit from a broader contextualization within the federated optimization literature:\n* While the focus on dimensional collapse and classifier bias is valuable, the heterogeneity challenge in FL has been extensively studied from multiple angles. I'd suggest considering [2] as well, which shares conceptual similarities.\n* Recent work has demonstrated that BatchNorm usage and non-pretrained models contribute significantly to performance degradation under non-IID data. Acknowledging this literature and clarifying when/where the proposed approach is most advantageous would help readers understand the method's positioning. Happy to provide some starter references on this if needed.\n\n3. Experimental design considerations: I have several concerns about the experimental setup that might affect the interpretation of results:\n\n* The choice of E=5 local epochs for 100 rounds is interesting. Since higher E under non-IID conditions often degrades performance, could you provide an ablation study (similar to FedDecorr's analysis) or justification for this choice versus E=1? I also noticed in Figures 3, 4, and 7 that algorithms haven't converged at T=100. Would a lower E with longer training change the conclusions and result ranking?\n\n* Under full participation (Table 6), improvement is marginal, with baselines outperforming or performing within the variance range of the proposed method, despite being simpler. Could you discuss these results?\n* I noticed FedDecorr's $\\beta$ is set to 10 here, while their paper used 0.1 under similar settings. Could you clarify this 100x difference to ensure fair comparison?\n* Regarding Figures 5 and 6: the 10% Y-axis increments make the sensitivity appear much lower than it really is, can the authors provide a rendering where the increments are more in line with the performance difference between the proposed method and baselines (e.g. 2% increments)? This would help contextualize the sensitivity better. If test set tuning was used for these hyperparameters, as it appears to be the case, this might disadvantage simpler baselines with fewer hyperparameters. Can the authors comment on this?\n\n## Minor corrections\n- Typo line 108 Relate Work -> Related Work\n- Typo line 237 definded -> defined\n\nI'm happy to discuss these points further and would be open to reconsidering my assessment if these concerns can be addressed.\n\n## References\n\n[1] Fazel, M., Hindi, H. and Boyd, S.P., 2003, June. Log-det heuristic for matrix rank minimization with applications to Hankel and Euclidean distance matrices. In Proceedings of the 2003 American Control Conference, 2003. (Vol. 3, pp. 2156-2162). IEEE.\n\n[2] Guo, Y., Tang, X. and Lin, T., 2023, July. Fedbr: Improving federated learning on heterogeneous data via local learning bias reduction. In International conference on machine learning (pp. 12034-12054). PMLR.\n\n[3] Dai, Y., Chen, Z., Li, J., Heinecke, S., Sun, L. and Xu, R., 2023, June. Tackling data heterogeneity in federated learning with class prototypes. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7314-7322)."}, "questions": {"value": "1. The authors present accuracy averaged over the last 10 rounds (10% of training). Can they explain the reasoning behind this decision?\n2. I'd appreciate access to the code, which is mentioned by the authors, to better understand the implementation details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4XEO4Feyq9", "forum": "xzuR1JUNcK", "replyto": "xzuR1JUNcK", "signatures": ["ICLR.cc/2026/Conference/Submission16506/Reviewer_1H5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16506/Reviewer_1H5i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167014821, "cdate": 1762167014821, "tmdate": 1762926598368, "mdate": 1762926598368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Data heterogeneity in federated learning, particularly label skew, poses a significant challenge by causing dimensional collapse (where features become low-rank) and classifier bias, ultimately degrading global model performance. To address these issues, this paper proposes FedBlade, a novel framework integrating two key components: LDDecorr and PBA. LDDecorr is a feature decorrelation method that maximizes the log-determinant of the feature correlation matrix; this yields exponential gradients that, unlike previous linear methods, apply an \"infinite penalty\" to small singular values, effectively and rapidly mitigating dimensional collapse. PBA (Prototype-guided Bidirectional Alignment) enhances the synergy between the model's feature extractor, projector, and a fixed ETF classifier by using global prototypes as a common reference, ensuring these modules are aligned and consistent across clients. Extensive experiments show that FedBlade outperforms existing baselines in accuracy and achieves substantially faster convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes LDDecorr, a novel feature decorrelation method. Unlike previous approaches like FedDecorr that use linear gradients, LDDecorr uses a log-determinant formulation to produce exponential gradients ($\\nabla_{\\lambda_{i}} = -1/\\lambda_{i}$). This is a significant strength because it imposes an \"infinite penalty\" on small singular values, preventing them from collapsing to zero much more effectively and accelerating the mitigation of dimensional collapse.\n\n2.  The paper identifies a key weakness in prior work (like FedETF) that uses fixed classifiers, which lacks synergy between the feature extractor, projector, and classifier. Its second component, PBA (Prototype-guided Bidirectional Alignment), directly solves this. It uses global prototypes as a common \"bridge\" to simultaneously align the feature extractor with the prototypes and the projector with the fixed classifier, ensuring all model parts work together coherently.\n\n3. FedBlade consistently outperforms a wide range of relevant baselines (including FedAvg, FedDecorr, and FedETF) in final accuracy, especially on complex datasets like CIFAR-100 and Tiny-ImageNet. Furthermore, it shows FedBlade achieves substantially faster convergence."}, "weaknesses": {"value": "1. Feature decorrelation method (LDDecorr) is sensitive to the decorrelation strength,. \n\n2.  The proposed PBA method requires an extra communication step in each round. Clients must compute and send their local prototypes to the server, and the server must aggregate the global prototypes and send them back to the clients. This adds to communication costs, a key bottleneck in federated learning.\n\n3. The FedBlade framework adds computational overhead on both the client and server. Clients must perform extra calculations for the new loss terms: $\\mathcal{L}_{LDDecorr}$ which involves a determinant calculation and the two PBA losses. The server also has the new task of aggregating all client prototypes."}, "questions": {"value": "1. What are the limitations of the proposed approach? \n\n2.  How do the computational costs of LDDecorr and the communication costs of PBA scale, especially with high-dimensional feature spaces and a large number of classes? At what point do these added costs make FedBlade less practical than faster, simpler methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MHvCXTCjN7", "forum": "xzuR1JUNcK", "replyto": "xzuR1JUNcK", "signatures": ["ICLR.cc/2026/Conference/Submission16506/Reviewer_BbJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16506/Reviewer_BbJv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762221060656, "cdate": 1762221060656, "tmdate": 1762926597888, "mdate": 1762926597888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}