{"id": "EbonLCjxKp", "number": 22590, "cdate": 1758333192889, "mdate": 1763676822994, "content": {"title": "Soft Instruction De-escalation Defense", "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control) - a simple yet effective multi-stage sanitization pipeline designed for tool-augmented LLM agents. Our approach begins by unconditionally rewriting incoming data to neutralize any potential instructions by masking, rephrasing, or removing them. To detect attacks against the rewriter itself, we inject known canary instructions before this process; if these instructions survive, we conclude the rewrite was compromised. To account for the imprecision of LLMs, we apply multiple independent rewrite passes. Finally, a detection module inspects the full text and smaller chunks of the output for any residual instruction-like content. If imperative instructions remain, the agent halts to ensure security. This defense-in-depth strategy, combining unconditional rewriting, canary checking, and chunk-based detection, makes successful attacks significantly more difficult than bypassing a single detection model.", "tldr": "", "keywords": ["Prompt Injections", "defense", "tool-use"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/322fc94e9699a7344dc75c26a0773e0de94aa563.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a lightweight defense mechanism for LLM-based Agents, designed to resist Prompt Injection attacks.\nThe authors introduce an iterative and lightweight purification approach, which gradually rewrites potential instructions to neutralize malicious behavior.\nThis design avoids two common weaknesses in previous methods:\n\n1. vulnerability to adversarial rephrasing, and\n2. overly aggressive detection that causes frequent false positives.\n\nThrough evaluations on multiple LLMs, the authors demonstrate that SIC can reduce the Attack Success Rate (ASR) to zero across all tests while preserving the Agent’s original task performance—showing the clear effectiveness of the method.\nAdditionally, the work emphasizes parallelization, and the reduced computational cost makes it well-suited for real-world deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tBy employing a preprocessing module or introducing an LLM-as-a-Judge component, the method avoids modifying internal model parameters, making it friendly to black-box models.\n2.\tThe multi-round strategy is more robust than a one-shot approach, and experimental results strongly demonstrate the effectiveness of this iterative mechanism.\n3.\tThe method is efficient and parallelizable, while maintaining the original task performance, which is crucial for its practical applicability."}, "weaknesses": {"value": "1.\tSome detector or rewriter designs rely on external LLMs. Has the paper considered the scenario where the attack itself targets these external LLMs? Could this lead to delayed defense response or even worse cascading failures?\n2.\tThe performance of both the rewriter and the detector depends heavily on the quality of their prompt templates. Combined with the first concern, how does the framework ensure the robustness and diversity of these templates under adversarial conditions?\n3.\tRegarding the choice of the auxiliary LLM, is there any ablation study or selection criterion provided? How do the alignment quality and model size of the auxiliary LLM affect the performance and latency of the SIC framework?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "76Dc4BpT7X", "forum": "EbonLCjxKp", "replyto": "EbonLCjxKp", "signatures": ["ICLR.cc/2026/Conference/Submission22590/Reviewer_F1H1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22590/Reviewer_F1H1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649885093, "cdate": 1761649885093, "tmdate": 1762942293430, "mdate": 1762942293430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily investigates the performance of different defense mechanisms against various instruction injection attacks, with a focus on analyzing the robustness of the SIC method across multiple models and tasks. The article demonstrates that SIC can reduce attack success rates to 0% under various attack types while causing only minimal degradation in utility. Through ablation studies, it explores the impact of rewriting and chunking mechanisms, explaining how the chunking mechanism helps reduce attack success rates while also noting the potential false positive issues it may introduce."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The SIC method maintains 0% ASR across various attack types and models, demonstrating significant robustness.\n\n2. The comparative analysis system is comprehensive, including different models and existing defense methods.\n\n3. Ablation experiments are included, explaining the underlying reasons why the chunking mechanism reduces attack success rates."}, "weaknesses": {"value": "1. The analysis of false positive sources is relatively brief, only mentioning \"instruction-like statements,\" lacking more specific classification or mitigation strategies.\n\n2. The experiments primarily focus on plaintext prompt injection, lacking validation against more covert multi-turn or cross-modal attacks.\n\n3. There is insufficient detailed evaluation of defense overhead, such as computational resource consumption and response latency.\n\n4. In multilingual environments, can SIC still effectively identify and intercept instruction injections?"}, "questions": {"value": "see Weakness Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f8iVYXXsLc", "forum": "EbonLCjxKp", "replyto": "EbonLCjxKp", "signatures": ["ICLR.cc/2026/Conference/Submission22590/Reviewer_Eo7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22590/Reviewer_Eo7B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896174766, "cdate": 1761896174766, "tmdate": 1762942293045, "mdate": 1762942293045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Soft Instruction Control (SIC), a defense method designed to counter indirect prompt injection attacks in LLM agents. SIC performs iterative instruction detection and rewriting to “soften” and sanitize input data, effectively preventing malicious instructions from being activated during the agent’s execution phase. Experiments conducted on the AgentDojo benchmark, covering various models and attack scenarios, demonstrate that SIC can significantly reduce the attack success rate (ASR) to 0% while maintaining high task utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SIC combines iterative rewriting and detection to establish a “soft control” defense mechanism. This design balances defense effectiveness with performance and offers strong deployment feasibility.\n- Extensive evaluations were conducted on the AgentDojo benchmark, covering various models and attack scenarios. Comparisons with other defenses, such as MELON and PI-GUARD, further validate the effectiveness of SIC."}, "weaknesses": {"value": "- The paper presents a theoretical analysis of SIC’s latency but lacks experimental comparisons with other defense methods. This limitation makes it difficult for readers to fully assess SIC’s overall performance across the “security–utility–efficiency” trade-off. It is recommended to include detailed latency comparison experiments among different defense methods to further strengthen the practical validation of the approach.\n- The paper does not specify the exact auxiliary LLM model used in the SIC method. Since the performance of SIC may depend on the capability of the auxiliary model, it is recommended to include ablation studies to quantitatively illustrate the direct impact of the auxiliary model’s performance on SIC’s defense effectiveness."}, "questions": {"value": "- Do the results of the adaptive attack reported in Section 6 (ASR = 60% in the Slack scenario) generalize to other tasks and models? Could more experimental results be provided?\n- What is the defense cost associated with the auxiliary model used in SIC? Would training or fine-tuning a lightweight model separately reduce the defense cost or enhance the defense effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1jLnz20KL0", "forum": "EbonLCjxKp", "replyto": "EbonLCjxKp", "signatures": ["ICLR.cc/2026/Conference/Submission22590/Reviewer_m4mo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22590/Reviewer_m4mo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922979521, "cdate": 1761922979521, "tmdate": 1762942292816, "mdate": 1762942292816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Soft Instruction Control (SIC), an iterative sanitization defense against prompt injection attacks in tool-augmented LLM agents. The core idea is to repeatedly rewrite untrusted input to remove imperative instructions, inject canary instructions to detect rewriting failures, and use chunked classification to verify no malicious instructions remain. The method is evaluated on AgentDojo benchmark across multiple models (GPT-4o, Qwen3-32B, Kimi-k2, GPT-4.1-mini) and achieves 0% ASR on standard attacks while maintaining reasonable utility. However, under adaptive attacks (Nasr et al., 2025), the defense achieves only 60% ASR, revealing three primary failure modes: embedded executable workflows, authority-styled language, and partial-failure narratives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Combining rewriting, canary detection, and chunked classification creates defense-in-depth that is harder to bypass than single-layer approaches.\n2. Testing across multiple SOTA models (GPT-4o, Qwen3-32B, Kimi-k2, GPT-4.1-mini) and three task domains demonstrates generalizability on standard benchmarks\n3. Unlike many security papers, the authors conduct worst-case analysis and clearly document three failure modes with concrete examples\nStrong performance on standard attacks: Achieving 0% ASR on AgentDojo attacks while maintaining 50-55% utility is impressive compared to baselines"}, "weaknesses": {"value": "1. The paper assumes white-box access (Section 3) but the adaptive attack reveals the defense relies on assumptions (e.g., \"instructions are imperative\") that adversaries can trivially violate. The threat model should explicitly state what adversarial capabilities are not covered.\n2. Section 4.2 claims \"latency remains small\" but provides no actual measurements. For production systems processing thousands of requests, the cost of R+1+k LLM calls per input could be prohibitive."}, "questions": {"value": "1. Why is there no comparison with CaMeL (Debenedetti et al., 2025)? This is the primary related work and claimed inspiration. What are the trade-offs between SIC's soft approach and CaMeL's formal decomposition in terms of both security and utility?\n2. What happens if an attacker discovers the specific canary text? Have you tested with randomized canaries or multiple diverse canaries? How does performance change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VeuvABIadB", "forum": "EbonLCjxKp", "replyto": "EbonLCjxKp", "signatures": ["ICLR.cc/2026/Conference/Submission22590/Reviewer_CRvw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22590/Reviewer_CRvw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22590/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989731074, "cdate": 1761989731074, "tmdate": 1762942292591, "mdate": 1762942292591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}