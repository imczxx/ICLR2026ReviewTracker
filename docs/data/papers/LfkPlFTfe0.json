{"id": "LfkPlFTfe0", "number": 12317, "cdate": 1758207037075, "mdate": 1759897517790, "content": {"title": "Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy", "abstract": "Human-object interaction (HOI) synthesis is crucial for applications in animation, simulation, and robotics. However, existing approaches either rely on expensive motion capture data or require manual reward engineering, limiting their scalability and generalizability. In this work, we introduce the first unified physics-based HOI framework that leverages Vision-Language Models (VLMs) to enable long-horizon interactions with diverse object types — including static, dynamic, and articulated objects. We introduce VLM-Guided Relative Movement Dynamics (RMD), a fine-grained spatio-temporal bipartite representation that automatically constructs goal states and reward functions for reinforcement learning. By encoding structured relationships between human and object parts, RMD enables VLMs to generate semantically grounded, interaction-aware motion guidance without manual reward tuning. To support our methodology, we present Interplay, a novel dataset with thousands of long-horizon static and dynamic interaction plans. Extensive experiments demonstrate that our framework outperforms existing methods in synthesizing natural, human-like motions across both simple single-task and complex multi-task scenarios. For more details, please refer to our project webpage: https://vlm-rmd.github.io/.", "tldr": "We propose a unified physics-based HOI framework that leverages VLM-guided spatio-temporal reasoning to automatically generate goal states and reward functions, enabling long-horizon interactions with diverse object types.", "keywords": ["Human-Object interaction", "Character animation", "Human motion generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/673c7133f8617e90b97b8d4bb2bd283af92259f4.pdf", "supplementary_material": "/attachment/1c3e4ede9cabd98de72583ac8b344d37a5c47a22.zip"}, "replies": [{"content": {"summary": {"value": "For HOI synthesis, this paper’s main contributions are twofold: (1) using a VLM as a high-level planner and (2) proposing RMD as a mid-level interface between planning and control. The method requires neither task-specific motion-capture demonstrations nor manual reward engineering, and supports long-horizon, multi-stage interactions with static objects, dynamic objects, and articulated objects. The authors also introduce InterPlay, a dataset containing thousands of long-horizon interaction plans for systematic evaluation. Experiments show consistent gains over strong baselines (e.g., AMP, InterPhys, UniHSI, TokenHSI) in completion rate and precision across single-task and multi-task settings. Ablations further demonstrate that VLM visual input and the fine-grained RMD representation are key to the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Comprehensive ablations and analysis. The paper isolates component contributions via ablations and alternative VLMs. Removing visual input or coarsening RMD degrades performance, underscoring the importance of VLM spatial grounding and fine-grained RMD guidance. A user study in the appendix aligns with quantitative metrics, indicating more natural and plausible motions.\n\n* Thorough empirical validation. InterPlay spans diverse scenes and task types. Metrics (e.g., Completion Rate, Precision) are appropriate, and comparisons against representative baselines cover static, dynamic, and multi-task scenarios. Results show sizable improvements (often tens of percentage points in completion) and higher motion fidelity, supporting generalization and robustness."}, "weaknesses": {"value": "* Planner reliability. While GPT-4V outputs are structured, the planner remains a black box. Erroneous sub-plans could hinder training or misguide rewards (e.g., incorrect reasoning about dynamic object trends). The paper does not quantify or correct such errors. Moreover, cross-phase physical causality is only implicitly encoded via RMD; the VLM may be pattern-matching rather than reasoning about outcomes, which could break in highly dynamic tasks (e.g., throwing/catching).\n\n* RMD’s bipartite modeling misses self-contact and intra-object contact. By construction, RMD only encodes relations between human parts and object parts. Many everyday interactions require human–human self-contact and articulated object self-contact (e.g., folding a clamshell laptop). Such intra-agent and intra-object constraints are not representable with a strictly bipartite graph, which can lead to inaccurate guidance or reward shaping for these tasks.\n\n* Task scope. Extremely dynamic, high-precision, or fast-response scenarios are not covered; it is unclear whether RMD + GPT-4V planning scales to these regimes.\n\n* Writing. The related-work narrative around “using a planner” vs. “the representation that the planner outputs” is not sufficiently disentangled; these are distinct axes and should be contrasted more clearly."}, "questions": {"value": "* Physical causal reasoning. The method relies on VLM-imagined motion trends. In highly dynamic settings (e.g., projectile interactions or multiple moving objects), can GPT-4V reason causally? Can RMD be extended to handle exogenous object motion (not initiated by the agent)? If objects move autonomously or unexpected events occur (e.g., a rolling ball), can the framework revise the plan online?\n\n* Planner stability. Did you observe VLM hallucinations (e.g., incorrect part segmentation, anomalous RMD weights)? What filtering or correction mechanisms were used? How sensitive are plans to prompt phrasing? Please provide observations and mitigation strategies for planner stability.\n\n* Open resources. Please confirm the release timeline for the InterPlay dataset and the codebase (including prompt templates and exemplar plans)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gfP19jTIzp", "forum": "LfkPlFTfe0", "replyto": "LfkPlFTfe0", "signatures": ["ICLR.cc/2026/Conference/Submission12317/Reviewer_iGBJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12317/Reviewer_iGBJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887353870, "cdate": 1761887353870, "tmdate": 1762923243858, "mdate": 1762923243858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of existing Human-Object Interaction (HOI) synthesis methods—such as reliance on expensive motion capture data, manual reward engineering, and poor support for long-horizon/dynamic interactions—by proposing a unified physics-based framework. The core innovation is VLM-Guided Relative Movement Dynamics (RMD), a fine-grained spatio-temporal bipartite graph representation that encodes part-level relationships between humans and objects (e.g., stationary contact, approaching/separating motion). RMD enables Vision-Language Models (VLMs, e.g., GPT-4V) to automatically generate goal states and reward functions for reinforcement learning (RL), eliminating manual tuning. The framework supports interactions with static, dynamic, and articulated objects, and the authors introduce InterPlay, a novel dataset of thousands of long-horizon HOI plans. Experiments demonstrate that the method outperforms baselines (e.g., InterPhys, TokenHSI, UniHSI) in both single-task and long-horizon multi-task scenarios, producing more natural, task-aligned motions as validated by quantitative metrics and a user study."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Captures fine-grained spatio-temporal dynamics between human and object parts, addressing the limitations of coarse contact-based (e.g., chain-of-contacts) or kinematic-only representations. This enables modeling of continuous interaction dynamics (e.g., carrying an object) rather than discrete events.\n\n2.  Leverages VLMs to translate high-level instructions and scene context into structured RMD plans, which are directly mapped to goal states and composite rewards. This eliminates labor-intensive manual reward engineering, a major bottleneck for scalable HOI.\n\n3. Uniquely supports static, dynamic, and articulated objects, as well as long-horizon multi-task sequences (e.g., pick up → carry → place → sit). Most prior methods are limited to single-task or static-only interactions.\n\n4.  Fills a critical gap in existing HOI datasets by providing long-horizon, context-rich interaction plans, enabling systematic evaluation of multi-task HOI synthesis.\n\n3. Outperforms baselines across key metrics (completion rate, sub-step precision) in both single and multi-task settings. The user study further confirms superior motion naturalness and task alignment."}, "weaknesses": {"value": "1. The framework only supports single-agent interactions, ignoring multi-agent collaboration (e.g., two people moving a sofa) or social dynamics—key for real-world applications like assistive robotics or collaborative environments.\n\n2. The VLM planner may struggle with extremely complex long-horizon tasks requiring deep hierarchical planning (e.g., multi-step cooking with ingredient prep, cooking, and serving), as noted in the paper’s future work.\n\n3. Object part decomposition depends on the VLM’s implicit judgment, with no explicit evaluation of decomposition accuracy or robustness to novel/unfamiliar objects (e.g., specialized tools)."}, "questions": {"value": "1. The edge weight categories (0=stationary, 1=approaching, 2=separating, 3=unstable) are heuristic—what empirical or theoretical basis supports this granularity? Could a learned weight space or more granular categories (e.g., varying speed of approach) improve interaction naturalness?\n\n2. How does the VLM handle part decomposition for novel objects with ambiguous functional parts (e.g., a multi-purpose tool)? Is there a fallback mechanism, and how is decomposition quality evaluated?\n\n3. For tasks more complex than those in InterPlay (e.g., \"tidy the living room, wash dishes, and prepare coffee\"), how does the framework scale? Would integrating chain-of-thought prompting (mentioned in future work) quantitatively improve plan coherence and task completion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBiD30dYKE", "forum": "LfkPlFTfe0", "replyto": "LfkPlFTfe0", "signatures": ["ICLR.cc/2026/Conference/Submission12317/Reviewer_7XuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12317/Reviewer_7XuJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914049728, "cdate": 1761914049728, "tmdate": 1762923243487, "mdate": 1762923243487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified physics-based HOI framework that uses a VLM (GPT-4V) to automatically construct both goal states and reward functions from a structured representation called Relative Movement Dynamics (RMD). RMD is a part-to-part bipartite graph over human and object components with discrete relative-motion labels (e.g., approach, separate, stationary ). The VLM planner outputs a sequence of RMD steps plus spatial anchors; the RL policy (PPO) then executes them with a composite task reward (human/object destination + RMD consistency) and a style prior. A new InterPlay dataset supports long-horizon static, dynamic, and articulated interactions. Experiments show large gains in completion, sub-step ratio, and precision, with ablations that isolate the value of VLMs, multi-part modeling, and the RMD features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The RMD abstraction gives the VLM a concrete “language” to plan in, and gives RL a direct mapping from that plan to goals and rewards. This avoids per-task reward tuning and makes long-horizon transitions feel principled instead of different parts being glued together. This is a meaningful step toward building more general-purpose simulated agents.\n- I appreciate the completion definition (do the interaction and return to a neutral position). It addresses what usually breaks in HOI: recovery and chaining. The method’s advantage in hybrid / multi-task settings (Table 2) is meaningful.\n- The Interplay dataset, including both static and dynamic objects and requiring multi-object sequences, is a welcome addition to the community."}, "weaknesses": {"value": "- In the demo video opening door scene, it is actually very hard to see the agent using its hand/stump.\n- The system leans heavily on GPT-4V with prompts.  What is the failure case for GPT-4V that may impact training and learning agents?\n- There are two compute costs here: (i) planner inference (VLM) and (ii) training/execution. It would help to see end-to-end wall-clock and per-episode runtime (planning + control), and how that compares to baselines.\n- The naturalness of the human motion ultimately leans on known motion-style priors (AMP-like discriminator), which means out-of-distribution motion may not be feasible."}, "questions": {"value": "- What is the empirical error rate of the VLM planner on InterPlay, and how does task performance degrade?\n- The stage transition uses a fixed and hand-picked 0.9 threshold. How sensitive are completion and precision to this value, and did you try progress-based or confidence-weighted switching that might be less brittle in cluttered or dynamic cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "- What is the empirical error rate of the VLM planner on InterPlay and how does task performance degrade?\n- The stage transition uses a fixed and hand-picked 0.9 threshold. How sensitive are completion and precision to this value, and did you try progress-based or confidence-weighted switching that might be less brittle in cluttered or dynamic cases?"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RvKeZcI7Ny", "forum": "LfkPlFTfe0", "replyto": "LfkPlFTfe0", "signatures": ["ICLR.cc/2026/Conference/Submission12317/Reviewer_xkXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12317/Reviewer_xkXT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972993042, "cdate": 1761972993042, "tmdate": 1762923243200, "mdate": 1762923243200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for synthesizing physics-based human-object interactions (HOI) by leveraging Vision-Language Models (VLMs) to automatically generate goal states and reward functions. The core contribution is the Relative Movement Dynamics (RMD) representation—a fine-grained spatio-temporal bipartite graph that models part-level relationships between human and object components during interactions. This allows the VLM to reason about motion dynamics and generate semantically grounded, interaction-aware plans without manual reward engineering. The authors also contribute the InterPlay dataset, which includes long-horizon static and dynamic interaction tasks in diverse indoor scenes. Experiments in both single-task and multi-task settings demonstrate that the proposed method outperforms existing approaches in terms of completion rate, motion naturalness, and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe introduction of Relative Movement Dynamics is a conceptually elegant and technically sound way to bridge high-level task instructions with low-level motion control. By modeling interactions as a bipartite graph of human and object parts with explicit motion trends, the method captures both spatial and temporal aspects of interaction in a unified manner.\n2.\tThe method achieves state-of-the-art performance across multiple metrics (completion rate, sub-step precision) in both single-task and long-horizon multi-task scenarios. The improvements are especially notable in dynamic and hybrid interaction settings, where prior methods often fail."}, "weaknesses": {"value": "1. The InterPlay dataset introduced in the paper includes articulated objects not found in current interaction datasets, but it does not specify what types of articulated objects are included or what kinds of interactions are involved.\n2. The 6D pose of objects is crucial. How does the method prevent issues like unintended object rotation caused by inaccurate 6D pose estimation during interaction?\n3. The construction of Graph B seems to be only briefly discussed in the paper, yet I believe the connectivity of this graph and the selection of edge weights would directly impact the generated motion outcomes."}, "questions": {"value": "1. The paper claims the contribution of being the \"first unified physics-based HOI synthesis framework leveraging the powerful world knowledge of VLMs,\" but it may not actually be the first work to use VLMs for physics-based HOI. This statement could be somewhat biased."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fDRRC30Ehs", "forum": "LfkPlFTfe0", "replyto": "LfkPlFTfe0", "signatures": ["ICLR.cc/2026/Conference/Submission12317/Reviewer_WtPe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12317/Reviewer_WtPe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978012020, "cdate": 1761978012020, "tmdate": 1762923242858, "mdate": 1762923242858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}