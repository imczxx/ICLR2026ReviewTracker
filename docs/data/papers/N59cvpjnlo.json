{"id": "N59cvpjnlo", "number": 16887, "cdate": 1758269958351, "mdate": 1759897213178, "content": {"title": "POLLINATOR: OPTIMAL MATCHMAKING IN AN INTELLIGENCE MARKETPLACE", "abstract": "The rapid growth of the intelligence marketplace has created an abundance of Large Language Model (LLM) producers, each with different cost–performance tradeoffs, making optimal selection challenging and resource-intensive. We present POLLINATOR, a novel router that integrates a frugal, data-efficient predictor with an online dual-based optimizer. The predictor combines graph-based semi-supervised learning with an Item Response Theory (IRT) head, reducing training cost by up to 49% while improving predictive accuracy over prior state-of-the-art. The optimizer formulates matchmaking as a strongly convex problem, which allows efficient dual-to-primal conversion for real-time serving. Extensive experiments demonstrate that POLLINATOR delivers superior cost–performance tradeoffs: achieving 0.43%-1.5% gains at 71%-93% of the cost of state-of-the-art router, 3-5% gains at only 1.9-3% of the cost of the best individual producer,\nand up to 10.6% higher accuracy at just 0.3-35.7% of the cost on challenging real-world benchmarks such as BFCL-V3 and MMLU-Pro. Finally, the interpretability of learned query difficulties and model abilities demonstrates POLLINATOR’s effectiveness for dynamic and cost-efficient intelligence matchmaking.", "tldr": "POLLINATOR efficiently routes queries to the right LLM by combining graph-based prediction with dual optimization, improving both accuracy and cost.", "keywords": ["LLM selection", "Model Routing"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d858dc6cdd312778155a8d7b461a7ce33d73895.pdf", "supplementary_material": "/attachment/09c16f5d0b8c60fad4c64ae840f67c1cc95334ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an elastic model selection framework that dynamically allocates large models to optimize performance and cost. The idea is interesting and practically valuable for large-scale deployment. However, the paper lacks clarity on how dynamic model switching is feasible in real settings, given that invoking multiple cloud or local services incurs significant latency and token overhead. Moreover, the method’s handling of prediction–optimization mismatch is insufficiently analyzed, and scalability issues related to operational and inference costs are not well quantified. Overall, the paper presents a promising direction but requires deeper technical explanation and broader empirical validation to be fully convincing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely problem in efficient large-model deployment by proposing an elastic selection mechanism to balance accuracy and cost. Its idea of dynamically routing tasks to different models based on predicted utility is conceptually appealing and reflects practical system needs. The formulation is well-motivated, and the integration of optimization principles into model selection demonstrates good technical insight."}, "weaknesses": {"value": "The main limitation of the paper lies in the practicality and scalability of its proposed elastic model routing mechanism. Although the idea of adaptively selecting large models is appealing, the paper lacks a concrete analysis of how such routing can be implemented efficiently in real deployment scenarios. Frequent switching between models or services would inevitably introduce latency, overhead, and token costs, yet these factors are not sufficiently quantified or discussed.\n﻿\nMoreover, the framework’s predictive component relies heavily on estimated performance and cost metrics without a clear strategy for handling uncertainty or prediction errors. The absence of robustness tests, sensitivity analysis, and ablation studies weakens the empirical evidence. Additionally, operational constraints such as service rate limits, model drift, or dynamic pricing are not modeled or evaluated, raising concerns about the system’s stability and reliability under real-world conditions."}, "questions": {"value": "1. How does the proposed elastic model selection framework manage the latency and token overhead introduced by dynamically invoking multiple large models across cloud or local services?\n﻿\n2. What mechanisms are used to mitigate prediction–optimization mismatch in the estimated performance and cost metrics ? Are calibration or uncertainty-aware strategies incorporated during training or inference?\n﻿\n3. Can the authors provide additional experiments or analysis to evaluate scalability, particularly in terms of end-to-end latency, system throughput, and robustness under dynamic model pricing or service availability changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2jjLy1r9eD", "forum": "N59cvpjnlo", "replyto": "N59cvpjnlo", "signatures": ["ICLR.cc/2026/Conference/Submission16887/Reviewer_toZC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16887/Reviewer_toZC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138097708, "cdate": 1761138097708, "tmdate": 1762926922352, "mdate": 1762926922352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of optimal LLM selection in the intelligence marketplace in which numerous LLMs with varying cost-performance tradeoffs make it difficult for users to  select the right model according to the request.  The proposed method is a novel router integrating a predictor and an online optimizer, aiming to balance inference performance, cost savings, and real-time serving capability for different scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses a critical challenge in the intelligence marketplace. This is a timely problem of practical importance. \n\nIt combines GCN) with IRT for prediction, as well as convex optimization for online serving. This integration results into significant performance improvement over existing methods.\n\nAuthors have conducted comprehensive experiments that cover a total of 14 datasets. Results demonstrate clear advantages."}, "weaknesses": {"value": "Somehow the modeling comes with oversimplified cost and constraints. The paper only models cost based on token counts, did not take into account other practical factors such as hardware deployment overhead or regional pricing differences, which plays an important role in real-world applications. Other common industrial constraints are not considered in the proposed approach e.g., minimum LLM usage volume commitments. \n\nThe paper emphasizes on engineering implementation, it lacks in-depth theoretical analysis and comparisons with RL-based dynamic routing methods.  Also, it assumes static LLM abilities, with no mechanism to adapt to LLM updates e.g., iterative model fine-tuning or sudden shifts in query types."}, "questions": {"value": "Is the proposed method sensitive to the hyper parameter selection ?  \nWhat are some alternative methods for cost optimization in LLM deployment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u3za3XaePg", "forum": "N59cvpjnlo", "replyto": "N59cvpjnlo", "signatures": ["ICLR.cc/2026/Conference/Submission16887/Reviewer_ex1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16887/Reviewer_ex1Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877606189, "cdate": 1761877606189, "tmdate": 1762926921974, "mdate": 1762926921974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents POLLINATOR, a system for optimal LLM matchmaking in the Intelligence Marketplace. The framework includes two key components: a) Predictor, a data-efficient prediction module that integrates a GCN with an IRT head to improve accuracy while reducing training cost; b) Optimizer, a strongly convex, dual-based optimization module that supports efficient online routing and enforces safety constraints during model selection. Experiments across multiple benchmarks show consistent improvements in cost–performance trade-offs over existing routing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The integration of GCN and IRT for performance prediction is innovative, effectively capturing task–model relationships while offering interpretability via parameters representing model ability and query difficulty.\n- The optimization is elegantly formulated as a strongly convex program, enabling closed-form solutions for utility allocation. This avoids costly iterative optimization and supports real-time decision-making with solid theoretical grounding.\n- Extensive experiments on 14 diverse benchmarks demonstrate consistent gains in both accuracy and cost efficiency compared with strong baselines."}, "weaknesses": {"value": "- The paper mentions potential mismatch between predicted and actual values but does not quantify its impact on constraint satisfaction or budget adherence. A sensitivity analysis would strengthen the evaluation.\n- Although Algorithm 1 is well-designed, the paper lacks runtime and complexity analysis. The scalability of the approach for large model pools (tens or hundreds of models) remains unclear."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iOQpEojf7p", "forum": "N59cvpjnlo", "replyto": "N59cvpjnlo", "signatures": ["ICLR.cc/2026/Conference/Submission16887/Reviewer_NoyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16887/Reviewer_NoyV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894237939, "cdate": 1761894237939, "tmdate": 1762926921626, "mdate": 1762926921626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel router designed to select the optimal LLM producer for a request, balancing cost and performance in real-time. The system integrates a frugal predictor that combines graph-based semi-supervised learning with an Item Response Theory head. This is coupled with an online dual-based optimizer that formulates the matchmaking as a strongly convex problem for efficient, real-time serving. Experiments demonstrate superior cost-performance tradeoffs, achieving performance gains at a fraction of the cost  across various in-domain and out-of-domain benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel approach that leverages GCN with an IRT head, reducing the training cost significantly. \n- The dual serving scheme allows online optimization and offers better cost-performance tradeoff.\n- Empirical results show that the method achieves superior performance gain at a lower cost."}, "weaknesses": {"value": "- It's unclear how the GCN helps improve the router. The ablation study does not show any particular trend on the graph neighbour size (tab. 4), questioning its role. What if we remove the GCN part (similar to size 1), or simply apply k NN averaging? \n- The experimental setting is not presented well. How do you create the Performance-First/Cost-First/Balance setting? The paper should include a thorough comparison across the whole spectrum of performance and cost (see Figure 4 in RouterBench for example). Also, the oracle result where we choose the best and cheapest LLM for each sample should be reported for reference.\n- The results look strange. How can we achieve lower costs than the cheapest model (Tab. 1 Cost-First, Tab. 2 Balanced/Cost-First, and Tab. 3)?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BLoyvErPEj", "forum": "N59cvpjnlo", "replyto": "N59cvpjnlo", "signatures": ["ICLR.cc/2026/Conference/Submission16887/Reviewer_hrhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16887/Reviewer_hrhM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966118575, "cdate": 1761966118575, "tmdate": 1762926921124, "mdate": 1762926921124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}