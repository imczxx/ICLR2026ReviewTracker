{"id": "EUJ33R3LwL", "number": 12839, "cdate": 1758210736187, "mdate": 1759897482002, "content": {"title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in Brain", "abstract": "Recent voxel-wise multimodal brain encoding studies have shown that multimodal Transformer models exhibit a higher degree of brain alignment compared to unimodal models in two distinct settings: when subjects are engaged in unimodal stimuli and when they are exposed to multimodal stimuli. Notably, this alignment is achieved even when these Transformer models are not trained on brain data. More recently, a new class of models, namely instruction-tuned multimodal models has emerged, demonstrating strong zero-shot performance across a variety of tasks. These models offer a promising direction for capturing task-specific representations that align closely with brain activity. However, prior work evaluating the brain alignment of multimodal large language models (MLLMs) has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigate the brain alignment, i.e., measuring the degree of predictivity of neural activity using instruction-specific embeddings from six video and two audio MLLMs as participants engage in watching naturalistic movies (video included with audio). Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform in-context learning multimodal models (by 9%), non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Specifically, our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in enhancing the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both systems.", "tldr": "", "keywords": ["brain encoding", "fMRI", "multimodal instruction-tuned models", "Video LLMs", "Audio LLMs", "multi-modal stimuli", "Transformers", "interpretability", "pretrained multimodal video-audio LLMs"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/acc8c8ac30acbccb467c5e978d42ee919f3c4fbe.pdf", "supplementary_material": "/attachment/49ab1d4935f3414f3a623e7dfe27b3e5fbea8c46.zip"}, "replies": [{"content": {"summary": {"value": "This study uses instruction tuned MLLMs to model multimodal human brain activity during audiovisual movie viewing. They find that instructive tuning improves brain predictivity of both video and audio MLLMs. Instruction tuning improves video model predictivity across the brain, but audio improvements are limited to auditory cortex. The results also show how different types of instruction tuning differentially improve prediction across cortex, revealing known functional preferences across the brain. Finally, the models also reproduce the layer-wise hierarchy, with early layers most predictive of sensory regions, and later layers predicting higher-level brain areas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This an interesting and timely paper showing novel and potentially more interpretable ways to improve modeling of multimodal brain activity.\n\nThe modeling results reproduce known feature tuning and cotical hierarchies\n\nThis study seems to open the door to many future applications for both neuroscience and neuroengineering.\n\nThe paper is comprehensive and clearly written."}, "weaknesses": {"value": "While the results are interesting and novel it is not entirely clear what is at stake - is the main goal of this work to better understand the brain or for future applications (if so what)? It is easy to imagine future benefits of this work, particularly with the differential effects for different types of instruction tuning, but it’s not entirely clear how to move beyond mapping already known brain function.\n\nOverall the figures are quite clear but it is hard to keep track of the different models/instructions. I wonder if Fig 2 could be labeled not only with model name but also with model type (video, audio, instruction-tuned, not insturction tuned, etc). The color coding already seems to obey this. \n\nRelatedly, it is hard to differentiate the colors in Fig 3 (this may be unavoidable given the large number of categories). A table or graphic with the different instructions, their task grouping, and (coloring in this way) may help.\n\nAs a smaller point - the intro lays out two types of multimodal brain model (for unimodal versus multimodal stimuli). While these are clearly different applications, the summary suggests they are largely similar (multimodal better than unimodal models). This section could be streamlined and I don’t think is entirely accurate, as the benefits of multimodal modeling are usually minimal for unimodal stimuli."}, "questions": {"value": "How should we interpret the finding that video models are the most predictive of language and auditory regions?\nHow can we disentangle the role of more training / data versus the specific contribution of the instruction tuning?\nHow does cross subject predictivity compare to model predictivity shown in Fig 2?\nAre the whole brain results in all cortical voxels? Given some movies are repeated it may make sense to restrict this to only reliable voxels"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a4MUmWP7A7", "forum": "EUJ33R3LwL", "replyto": "EUJ33R3LwL", "signatures": ["ICLR.cc/2026/Conference/Submission12839/Reviewer_yTV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12839/Reviewer_yTV4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857659956, "cdate": 1761857659956, "tmdate": 1762923637157, "mdate": 1762923637157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates the brain alignment capability of instruction-tuned multimodal large language models (Instruction-Tuned MLLMs) under naturalistic movie stimuli, providing the first cross-modal analysis of the relationship between model representations and neural activations across both visual and auditory modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work is represents a good step to compare brain alignment between visual/audio modalities and large multimodal language models under naturalistic movie stimuli, conducting extensive experiments across multiple datasets and baselines to ensure robustness and generality of the findings."}, "weaknesses": {"value": "1. Limited originality. The main contribution of this paper lies in expanding the data dimension, without introducing any innovation in algorithmic design, model architecture, or theoretical analysis. Overall, the work appears to be an incremental extension of prior unimodal studies.\n2. the authors claim that instruction tuning improves brain alignment, the paper does not explain why task instructions better capture neural task differentiation. The cognitive mechanism underlying this effect, whether due to changes in attention allocation, semantic structuring, or task-specific feature learning remains unclear.\n3. The current evidence for the effect of instruction tuning on brain alignment is based solely on cross-model comparisons, which suffer from confounding factors such as architectural differences, pretraining data, and parameter scale. To establish causal validity, the authors are strongly encouraged to design a self-controlled experiment comparing the same model before and after instruction tuning. This setup isolates the single variable of “whether instruction tuning has been applied,” allowing for a direct measurement of its pure effect. Such an experiment would not only answer whether instruction tuning improves brain alignment, but also how it reshapes internal representations, e.g., identifying which layers show the largest improvement in alignment, whether changes occur in semantic versus perceptual layers, and whether the representational distribution becomes more consistent with cortical activation patterns. Furthermore, by comparing model activations for the same instruction (e.g., “Describe this video”) before and after instruction tuning, the authors could directly demonstrate how tuning transforms the model from producing task-agnostic representations to generating task-specific ones that align with relevant brain regions, thereby providing strong empirical evidence for the paper’s core claim that task instructions guide functional specialization in the brain. Therefore, causal verification is required through self-controlled experiments.\n4. Experimental design lacks rigor. To validate “instruction tuning” within a neuroscience or cognitive science framework, experiments should systematically test semantically similar or equivalent instructions, comparing the resulting activation patterns and brain alignment. The current finding, that different task instructions (e.g., “video description” vs. “spatial understanding”) activate distinct brain regions, does not yet prove that the model captures semantic-level task distinctions rather than merely reacting to superficial textual differences.\nTo strengthen the argument, the authors should conduct a semantic similarity robustness test. Specifically, they could construct a dataset of instruction pairs with known semantic similarity (e.g., “Describe this scene” vs. “Narrate what happens in the video”) and compute both (a) the semantic similarity between instructions using text embedding models, and (b) the similarity of internal model representations (e.g., cosine similarity between layer vectors).\nA strong positive correlation between these measures would demonstrate that instruction tuning enhances the model’s sensitivity to fine-grained semantic distinctions.\nAdditionally, clustering analysis of activation patterns induced by various instructions could reveal whether the model organizes tasks into conceptual categories resembling human cognition, e.g., grouping “object recognition” and “scene recognition” as perceptual tasks, and “inferring intentions” and “summarizing narratives” as social reasoning tasks. Such analyses would verify the model’s semantic precision and conceptual structuring ability, significantly enhancing the work’s interpretability and contribution to cognitive neuroscience.\n5. The paper does not provide or cite empirical findings showing that instructions themselves lead to differential neural activations in humans. Since the authors claim that instruction tuning improves brain alignment, they should explain the corresponding neural or representational mechanism. For example, whether task instructions alter attention distribution, facilitate semantic decomposition, or modulate higher-order reasoning representations in ways analogous to human cognition.\n6. The compared multimodal large language models (MLLMs) differ in architecture size, pretraining corpus, and task scope. The paper should clarify how these factors were controlled to ensure fair and scientifically valid comparisons.\n7. The paper emphasizes a “hierarchical correspondence” between model layers and brain regions, yet it lacks supporting evidence such as a correlation matrix or significance testing. The authors should verify whether this pattern reflects genuine layer-wise alignment or is driven by model size or random effects. Furthermore, it would be informative to test whether the same hierarchical pattern persists when replacing task instructions with random natural language prompts.\n8. Weak performance of audio models remains unexplained. The paper should analyze whether this weakness stems from modality misalignment, instruction design, or feature fusion mechanisms, and discuss how these factors influence cross-modal representation learning.\n9. Unjustified task categorization. The division of 13 tasks into 5 categories lacks statistical validation. The authors should clarify whether this categorization is subjective or data-driven, ideally supporting it with quantitative clustering or similarity analysis.\n10. The voxel-wise mapping via ridge regression may oversimplify the relationship between model representations and neural activity by assuming linearity. The authors should consider nonlinear methods (e.g., kernel regression or neural mapping) to explore potential higher-order relationships and verify robustness."}, "questions": {"value": "See weeknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3EOvGQXexc", "forum": "EUJ33R3LwL", "replyto": "EUJ33R3LwL", "signatures": ["ICLR.cc/2026/Conference/Submission12839/Reviewer_i7h9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12839/Reviewer_i7h9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880740164, "cdate": 1761880740164, "tmdate": 1762923636866, "mdate": 1762923636866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the alignment between instruction-tuned multimodal large language models (IT-MLLMs) for video and audio and human brain activity. Using fMRI data from participants watching naturalistic movies (video with audio) , the authors employ a voxel-wise encoding methodology to compare representations from six video and two audio IT-MLLMs against non-instruction-tuned, in-context learning, and unimodal models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a crucial and timely question at the intersection of foundation models and neuroscience: Do instruction-tuned models, which are becoming the dominant paradigm, process information in a way that is more aligned with the human brain? This work provides strong affirmative evidence for the video modality.\n- The use of naturalistic, multimodal stimuli (movies with audio)  is a significant strength. This is a major step beyond previous work that often relied on unimodal stimuli (static images or text), allowing for a more ecologically valid assessment of multimodal processing."}, "weaknesses": {"value": "- The paper finds that audio IT-MLLMs (Qwen-2.5-Audio, Kimi-Audio) provide only limited gains, significantly underperforming their video counterparts. The discussion acknowledges this and attributes it to potential differences in training data or objectives. However, the analysis could more deeply explore why this is the case. Is the audio stream simply less informative for this stimulus, or are current audio MLLMs genuinely less \"brain-like\" in their representations?\n- The paper groups ICL models (Qwen-2.5-Omni, InternVL)  as a baseline. While IT-MLLMs do outperform them, the conceptual line between a model following an in-context prompt and a model following an instruction it was tuned on is slightly blurred. A more direct discussion on what specific properties instruction-tuning adds beyond the zero-shot task-following capabilities of ICL models would strengthen the argument.\n- Miss some good video LLM works, e.g., Kimi-VL and Seed1.5 VL."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e6bXPH35Ox", "forum": "EUJ33R3LwL", "replyto": "EUJ33R3LwL", "signatures": ["ICLR.cc/2026/Conference/Submission12839/Reviewer_qdXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12839/Reviewer_qdXK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12839/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975321694, "cdate": 1761975321694, "tmdate": 1762923636584, "mdate": 1762923636584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Responses to Reviewers qdXK, i7h9 and yTV4:"}, "comment": {"value": "*We are grateful to all reviewers for their strong positive feedback, time and their constructive suggestions, which will further strengthen the impact of our work.*\n\n**CQ4. Correlation Between Instruction Semantics and Model Representations (Reviewers: qdXK, i7h9 and yTV4)**\n\nWe sincerely thank the reviewer for this constructive suggestion to strengthen our experimental validation. Based on reviewer’s suggestion (**reviewer i7h9**), we have conducted a comprehensive semantic similarity robustness study using the two settings reviewer recommended. \n\n**Semantic Similarity Measurement:** To validate that our task-specific instruction set contains semantically similar pairs (as requested by the reviewer), we perform following:\n* Using 13 video task-specific instructions, we first computed pairwise semantic similarity using two independent text embedding models (all-MiniLM-L6-v2 and MPNet). Both models produced highly consistent semantic similarity matrices (Pearson r = 0.94 between MiniLM and MPNet embeddings).\n* Captures fine-grained semantic relationships between instruction texts.\n* Example pairs: \"Describe the video\" vs. \"Caption the video\" (high semantic similarity).\n\n**Appendix T, Fig 28.** shows pairwise semantic similarity between 13 instruction prompts computed using MiniLM embeddings (left) and MPNet (right). \n   * From Fig.28 (left), we observe that the semantic similarity ranges from 0.15 to 0.85 (mean: 0.42 ± 0.18), with multiple high-similarity pairs identified (>0.60):(Action Recognition vs. Video Understanding (0.68), Video Understanding vs. Visual Question Answering (0.65), Object & Scene Recognition vs. Action Recognition (0.68)). \n   * We also observe low semantic similarity pairs: (Commonsense Reasoning vs. Most others, Commonsense Reasoning vs. other tasks, Spatial Understanding vs. Emotional/Narrative). \n   * This confirms that our instruction set contains \"semantically similar or equivalent instructions\" requested by the reviewer for testing fine-grained task distinctions.\n\n**Instruction-tuned Model Internal Representations:** To measure how the model internally processes different task-specific instructions, we extracted language hidden states from the Qwen2.5-VL-7B-Instruct model across all processing layers. We perform the following:\n* Extracted language hidden states across all 29 layers for each task-specific instruction\n* Used the same video input with varying instructions to isolate instruction effects\n* Analyzed three key layers: Early (Layer 5), Middle (Layer 14), and Late (Layer 25)\n* While we present detailed results for three representative layers, we computed correlations and clustering metrics across all 29 layers to ensure findings are not artifacts of specific layer selection.\n\n**Appendix T Fig. 29** shows pairwise similarity among the 13 instruction prompts computed using three key layers. From Fig.29, we observe that layer-specific differentiation patterns emerge: \n   * In early layers, relatively uniform representation similarity suggests that initial encoding stage has limited task specialization, \n   * In middle layers, increased variation in similarity patterns indicates evidence of task-specific transformations, and \n   * In later layers, pattern similar to early layer but with subtle differences implying possible refinement of task-specific representations.\n\n* To identify natural groupings in how the model processes instructions, we performed hierarchical clustering on Layer 14 representations (the layer showing maximum differentiation): \n   * Cluster 1 - Perceptual/Recognition Tasks: (Object & Scene Recognition, Visual Question Answering, Global Appearance), \n   * Cluster 2 - Descriptive/Understanding Tasks: (Video Understanding, Video Captioning, Narrative Understanding), \n   * Cluster 3 - Temporal/Action Tasks: (Action Recognition, Temporal Ordering, Linking Events), and \n   * Cluster 4 - Reasoning/Analytical Tasks: (Commonsense Reasoning, Video Reasoning, Emotion & Sentiment Analysis, Spatial Understanding). \n* This implies that the functional organization is not explicitly programmed but emerges from instruction tuning, suggesting that the model has learned task-specific processing strategies.\n* While we observe clear structure in model representations, a critical question remains: Do these patterns reflect semantic similarity or functional task requirements? To answer this, we next compare model representations with semantic similarity computed from text embeddings.\n\nWe have added this discussion in **Appendix T** of the revised paper."}}, "id": "BWDR42eM1m", "forum": "EUJ33R3LwL", "replyto": "EUJ33R3LwL", "signatures": ["ICLR.cc/2026/Conference/Submission12839/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12839/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12839/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763617497606, "cdate": 1763617497606, "tmdate": 1763621928816, "mdate": 1763621928816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}