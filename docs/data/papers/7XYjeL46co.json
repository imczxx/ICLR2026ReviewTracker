{"id": "7XYjeL46co", "number": 12214, "cdate": 1758206377487, "mdate": 1759897524894, "content": {"title": "MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers", "abstract": "Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present \\textbf{MCP-SafetyBench}, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains—browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.Our benchmark is available at https://anonymous.4open.science/r/MCP-SafetyBench-5738.", "tldr": "", "keywords": ["MCP", "Safety Benchmark", "MCP Attack"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd6534c5f02acc78b5c5a45723b76a49125d1b2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MCP-SafetyBench, a new benchmark for evaluating the safety of LLM agents operating within the Model Context Protocol (MCP) framework. The benchmark is built on real-world MCP servers  and covers five distinct domains: browser automation, financial analysis, location navigation, repository management, and web search. It introduces a taxonomy of 20 MCP-specific attack types spanning server, host, and user sides, with a particular focus on server-side threats. The authors evaluate several leading proprietary and open-source LLMs , measuring both Task Success Rate (TSR) and Attack Success Rate (ASR)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Problem Timeliness: The paper addresses a timely and critical problem. As LLM agents become more integrated with external tools via protocols like MCP, understanding their security vulnerabilities is of paramount importance.\n- Comprehensive Taxonomy: The proposed attack taxonomy, detailed in Appendix A, is comprehensive and well-structured . It clearly categorizes threats across the server, host, and user layers, which is a valuable contribution for the security community.\n- Sound Evaluation Methodology: The dual-evaluator framework, which separately measures task success and attack success, seems a sound methodology. It allows for a nuanced understanding of agent behavior, distinguishing between failing a task and being compromised by an attack."}, "weaknesses": {"value": "- Unclear results: In the end of introduction, it is *The primary finding is a negative correlation between task performance and safety, suggesting a “safety-utility trade-off” where more capable models are often more vulnerable to manipulation.*, while in the experiment, it is *We find no clear correlation between task success (TSR) and attack resistance (ASR)*. The results seem contradictory.\n- Misleading “Real-World” Claim: The paper’s emphasis on “real-world” integration is potentially misleading. While the tasks and servers are based on MCP-Universe , the attacks themselves are synthetically generated using templates and an LLM (Cursor). This benchmark evaluates defense against known, templated attacks, not novel, “in-the-wild” exploits (quite hard to collect though). This limits the generalizability of the findings on model robustness.\n- Counter-intuitive SOTA Performance: The reported Task Success Rates (TSR) for SOTA proprietary models are exceptionally low (e.g., GPT-5 at 15.92%, Claude-4.0-Sonnet at 10.20%). This is deeply counter-intuitive and contradicts public knowledge of these models’ capabilities. The paper provides no explanation for this. This finding casts significant doubt on the validity of the task evaluation or the overall experimental setup. Are the underlying tasks from MCP-Universe simply too difficult for even the best models?\n- Shallow Analysis of the “Safety-Utility Trade-off”: The paper’s main conclusion that “We find no clear correlation between task success (TSR) and attack resistance (ASR)”  is a shallow observation. a) The paper lacks qualitative case studies or deeper analysis to explain why this trade-off occurs, a well-explained case study will help. b) This trade-off may be an artifact of the benchmark’s design. As illustrated in Figure 2, a “safe” agent that detects the parameter poisoning (ticker -> TSLA) and refuses to execute would be penalized by Etask as a Fail. The benchmark does not appear to reward “safe refusal,” thus conflating “utility” with “unsafe obedience.”\n- The presentation could also be improved: The paper’s presentation hinders the assessment of its contributions: The excellent attack taxonomy (Appendix A), a key contribution, is relegated to the appendix and should be in the main body. Besides, There are numerous formatting errors, such as the consistent misuse of \\citep vs. \\citet throughout the manuscript, which detracts from the paper’s quality."}, "questions": {"value": "- What is the result of this benchmark evaluation exactly? \"No clear correlation\" or \"negatively correlated\"? Can you explain why in quantitative and qualitative analysis?\n- Can the authors please explain the extremely low Task Success Rates for models like GPT-5 and Claude-4.0-Sonnet, which are counter-intuitive? How are the TSR and ASR calculated exactly?\n- How does the benchmark evaluate a “safe refusal”? If an agent identifies a server-side attack (e.g., parameter poisoning in Fig. 2) and refuses to call the tool, is this marked as a Task Success or Task Fail by Etask? Does the “safety-utility” trade-off simply reflect a penalty for being cautious?\n- Given that the attacks are synthetically generated, how can the authors be sure they are representative of realistic adversarial strategies, rather than just artifacts of the templates used?\n- The analysis in Appendix A.2 concludes that models like GPT-4.1/4o have “spiky” defense profiles (strong against some attacks, weak against others). Does this conclusion hold for the newer flagship models evaluated (e.g., GPT-5, Gemini-2.5-Pro, Claude-3.7-Sonnet)? Or do these newer models show a different, perhaps more uniform, vulnerability profile?\n\nI would be happy to raise my rating if these questions are addressed convincingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9rtL1thxnO", "forum": "7XYjeL46co", "replyto": "7XYjeL46co", "signatures": ["ICLR.cc/2026/Conference/Submission12214/Reviewer_TQA6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12214/Reviewer_TQA6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546939848, "cdate": 1761546939848, "tmdate": 1762923161643, "mdate": 1762923161643, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MCP-SafetyBench, is a comprehensive benchmark introduced to evaluate LLM safety when interacting with MCP servers in multi-step, multi-server settings. The authors first consolidate and extend prior (narrowly focused) work, establishing a unified taxonomy of 20 attack types spanning three threat surfaces:\n- MCP server-side: tool poisoning variants, function overlapping, preference manipulation\n- host-side: intent injection, data tampering, identity spoofing\n- user-side: malicious code execution, credential theft, retrieval-agent deception)\n\nThe resulting MCP-SafetyBench is built off the MCP-Universe benchmark and consists of 245 realistic tasks across five domains (browser automation, financial analysis, location navigation, repository management, web search), each paired with exactly one attack instantiation and dual evaluators measuring both task success and attack success.  Evaluation of 13 leading open-source and proprietary models reveals that all LLMs are vulnerable to MCP-targeted attack, with Attack Success Rates ranging from 29.80% to 48.16%.  Furthermore, a negative correlation between task performance and defense robustness emerges, indicating no model achieves both strong capability and security. The results show host-side and user-side attacks consistently achieve high success rates above 70%, while models exhibit \"spiky\" defense profiles; highly resistant to explicit exploits like command injection but vulnerable to semantic threats like function overlapping. The benchmark demonstrates that widely-used LLMs face escalating vulnerabilities as task complexity and MCP-server interactions increase."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well written and does well to condense the (now substantial) body of MCP security works, spanning previous safety benchmarks and MCP-targeted attacks.  The paper also does well to situate its contributions relative to previous work.  The benchmark itself is robust (particularly compared to existing benchmarks, which are substantially smaller in either scope or automated use), with the multi-turn capabilities enabling some of the more complicated and subversive MCP attacks released over the past year (e.g., RADE).  The resulting portfolio of attacks is comprehensive and stands to provide a systematic tool to understand defense strategies for MCP-enabled agents.  The large evaluation of popular open- and closed-source models and subsequent analysis are also major contributions that illuminate the current vulnerable state of existing models under agentic workflow attacks."}, "weaknesses": {"value": "As a benchmark paper, the manuscript lacks novelty wrt nascent attacks/exploits.  However, the achievement of such a comprehensive benchmark and timeliness given the ever-growing adaption of MCP-powered agents greatly outweigh this."}, "questions": {"value": "\"GPT-4.1 (?)\" <- missing ref\n\nFor the following:\n\n> o4-mini achieves the highest TSR (21.22%) but also a high ASR (48.16 %),\n\n> Models with stronger tasksolving and tool-use abilities generally show higher ASRs, suggesting that heavy optimization for tool use\nand execution may make them more prone to indiscriminate instruction following and manipulation\n\nIt can also be noted that newer proprietary models also undergo safety alignment against more contemporary/emergent attacks.  Thus, o4-mini is likely to be more susceptible to prompt injection attacks compared to a recent frontier model which was underwent extensive safety alignment to resist such attacks, e.g., GPT-5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x4RqBpfALw", "forum": "7XYjeL46co", "replyto": "7XYjeL46co", "signatures": ["ICLR.cc/2026/Conference/Submission12214/Reviewer_L6g8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12214/Reviewer_L6g8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884981116, "cdate": 1761884981116, "tmdate": 1762923161161, "mdate": 1762923161161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for evaluating the safety risks of large language models within the model context protocol. It reviews existing MCP safety benchmarks and presents a clear taxonomy of MCP tasks. The proposed benchmark builts on a prior MCP benchmark, incorporating additional modifications related to attacks. Experiments across multiple models show mixed patterns and inconsistent trends in the safety risks associated with using MCP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a comprehensive MCP safety benchmark that covers a wide range of attack types.\n\n2. The paper proposes a clear and compact taxonomy of MCP vulnerabilities, which is crucial for safety benchmarking.\n\n3. The paper evaluates a wide range of open-source and proprietary models\n\n4. The paper is well-written."}, "weaknesses": {"value": "1. The paper does not sufficiently justify or elaborate on the task selection process. It’s unclear why the task source (MCP-Universe) is trusted, why these five domains are focused, how specific tasks are chosen, and whether the selection introduces systematic bias. \n\n2. The experimental settings (token/runtime/budget limits, temperature, number of turns, number of repetitions) are not clearly stated or motivated.\n\n3. The empirical analyses lacks the rigor expected of a benchmark paper. For example, the following claims are not supported by rigorous statistical evidence (e.g., hypothesis testing) \n\n    - “Models are especially vulnerable in Financial Analysis and Repository Management”\n\n    - “whether a model is open-source or closed-source does not systematically determine its robustness”\n\n    - “reasoning and non-reasoning models have broadly similar attack success rates”"}, "questions": {"value": "1. Table 2: How were attack types determined. Are they intended to be a comprehensive set of possible attacks? If so, please justify the coverage.\n\n2. Line 189: How do you ensure tasks mirror real-world applications beside being multi-turn? Multi‑turn structure alone does not guarantee realism. \n\n3. Line 190: How do you ensure reproducibility? \n\n4. Line 222: Why was MCP-Universe benchmark selected as the only data source?\n\n5. Line 90, Line 230-234: How were tasks selected? Why do you select those five domains? In what sense are they representative? Please discuss potential selection bias.\n\n6. Line 280: Why focus on disruption and stealth attack? What other attack classes did you consider, and why were they excluded?\n\n7. Line 323: The citation for GPT-4.1 appears malformed. Please correct it.\n\n8. Section 4.1: How do you configure the models / agents? Minimally, please report token/runtime/budget limits, temperature, number of turns, and number of repetitions.\n\n9. Table 4: What are the variances of TSR and ASR for each model/domain? Can you report uncertainties (e.g., standard error or confidence interval) for each estimate?\n\n1. Figure 5 is difficult to read due to overlapping lines and similar colors (e.g., GPT-4o and DeepSeek v3.1). Please improve readability.\n\n11. Line 328: Please standardize the model name: DeepSeek V3 or DeepSeek V3.1 (as shown in Figure 5)\n\n12. Line 375: Why models are especially vulnerable in Financial Analysis and Repository Management? It seems Gemini-2.5 has a high score on Location Navigation. Can you support your conclusion with rigorous statistical analysis (e.g., hypothesis testing)\n\n13. Line 397-398, Line 404-405, Line 409-410: Similar, please support these conclusions with rigorous statistical analysis, not only illustrative examples? This is especially important because the trends in Figures 6-7 are not visually clear.\n\n14. Line 409-410: What are explicit exploits and semantic threats? Why do Command Injection and Function Overlapping fall into these categories? The discussion in Appendix A.2 does not seem to resolve this. Please define the terms precisely and justify the categorizations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6PgzWMNiCq", "forum": "7XYjeL46co", "replyto": "7XYjeL46co", "signatures": ["ICLR.cc/2026/Conference/Submission12214/Reviewer_TNH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12214/Reviewer_TNH4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961291471, "cdate": 1761961291471, "tmdate": 1762923160751, "mdate": 1762923160751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for evaluating the safety risks of large language models within the model context protocol. It reviews existing MCP safety benchmarks and presents a clear taxonomy of MCP tasks. The proposed benchmark builts on a prior MCP benchmark, incorporating additional modifications related to attacks. Experiments across multiple models show mixed patterns and inconsistent trends in the safety risks associated with using MCP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a comprehensive MCP safety benchmark that covers a wide range of attack types.\n\n2. The paper proposes a clear and compact taxonomy of MCP vulnerabilities, which is crucial for safety benchmarking.\n\n3. The paper evaluates a wide range of open-source and proprietary models\n\n4. The paper is well-written."}, "weaknesses": {"value": "1. The paper does not sufficiently justify or elaborate on the task selection process. It’s unclear why the task source (MCP-Universe) is trusted, why these five domains are focused, how specific tasks are chosen, and whether the selection introduces systematic bias. \n\n2. The experimental settings (token/runtime/budget limits, temperature, number of turns, number of repetitions) are not clearly stated or motivated.\n\n3. The empirical analyses lacks the rigor expected of a benchmark paper. For example, the following claims are not supported by rigorous statistical evidence (e.g., hypothesis testing) \n\n    - “Models are especially vulnerable in Financial Analysis and Repository Management”\n\n    - “whether a model is open-source or closed-source does not systematically determine its robustness”\n\n    - “reasoning and non-reasoning models have broadly similar attack success rates”"}, "questions": {"value": "1. Table 2: How were attack types determined. Are they intended to be a comprehensive set of possible attacks? If so, please justify the coverage.\n\n2. Line 189: How do you ensure tasks mirror real-world applications beside being multi-turn? Multi‑turn structure alone does not guarantee realism. \n\n3. Line 190: How do you ensure reproducibility? \n\n4. Line 222: Why was MCP-Universe benchmark selected as the only data source?\n\n5. Line 90, Line 230-234: How were tasks selected? Why do you select those five domains? In what sense are they representative? Please discuss potential selection bias.\n\n6. Line 280: Why focus on disruption and stealth attack? What other attack classes did you consider, and why were they excluded?\n\n7. Line 323: The citation for GPT-4.1 appears malformed. Please correct it.\n\n8. Section 4.1: How do you configure the models / agents? Minimally, please report token/runtime/budget limits, temperature, number of turns, and number of repetitions.\n\n9. Table 4: What are the variances of TSR and ASR for each model/domain? Can you report uncertainties (e.g., standard error or confidence interval) for each estimate?\n\n1. Figure 5 is difficult to read due to overlapping lines and similar colors (e.g., GPT-4o and DeepSeek v3.1). Please improve readability.\n\n11. Line 328: Please standardize the model name: DeepSeek V3 or DeepSeek V3.1 (as shown in Figure 5)\n\n12. Line 375: Why models are especially vulnerable in Financial Analysis and Repository Management? It seems Gemini-2.5 has a high score on Location Navigation. Can you support your conclusion with rigorous statistical analysis (e.g., hypothesis testing)\n\n13. Line 397-398, Line 404-405, Line 409-410: Similar, please support these conclusions with rigorous statistical analysis, not only illustrative examples? This is especially important because the trends in Figures 6-7 are not visually clear.\n\n14. Line 409-410: What are explicit exploits and semantic threats? Why do Command Injection and Function Overlapping fall into these categories? The discussion in Appendix A.2 does not seem to resolve this. Please define the terms precisely and justify the categorizations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6PgzWMNiCq", "forum": "7XYjeL46co", "replyto": "7XYjeL46co", "signatures": ["ICLR.cc/2026/Conference/Submission12214/Reviewer_TNH4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12214/Reviewer_TNH4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961291471, "cdate": 1761961291471, "tmdate": 1763770390525, "mdate": 1763770390525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MCP-SafetyBench, a benchmark for evaluating the safety of LLM agents interacting with real-world Model Context Protocol (MCP) servers. The motivation is clear: while MCP has enabled flexible tool use through standardized APIs, it also opens a new attack surface that existing benchmarks overlook. The authors provide a unified taxonomy of 20 attack types across server, host, and user levels, implement them over five realistic domains (financial analysis, browser automation, navigation, repository management, and web search), and evaluate a broad range of state-of-the-art open- and closed-source LLMs. The results demonstrate worrying trends—especially the high success rates of stealth attacks and compounding vulnerabilities in multi-turn, multi-server setups—highlighting the gap between task performance and true safety robustness.\nThe benchmark’s strengths lie in its realistic integration with operational MCP servers and its execution-based evaluation that produces deterministic ground truth rather than subjective annotations. The paper is also carefully contextualized within the growing literature on MCP security (e.g., SafeMCP, MCPTox, MCP-AttackBench) and provides a much broader and systematic taxonomy. I particularly appreciated the dissection of attack success by type in Table 5, which reveals nuanced weaknesses—models that resist syntactic attacks like command injection often remain vulnerable to semantic ones like function overlapping. The empirical findings are credible and reproducible, with open-sourced tasks and clear experimental methodology."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "See above"}, "weaknesses": {"value": "### Weakness\n\nThe paper could better situate its contributions within broader LLM red-teaming and safety evaluation frameworks. For example, \"Operationalizing a Threat Model for Red-Teaming LLMs\" offers a structured way to define adversarial capabilities and goals, which could help formalize the threat assumptions behind MCP-SafetyBench. Similarly, works like Red-Teaming for Generative AI: Silver Bullet or Security Theater? and Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In provide complementary perspectives on how functional or agentic evaluations translate to real-world safety guarantees. Drawing these connections would not only strengthen the theoretical framing but also help readers see how MCP-SafetyBench fits into the evolving ecosystem of AI safety benchmarks.\nOverall, this is an excellent and timely contribution. It exposes real vulnerabilities in emerging MCP-based agentic workflows and provides the community with a much-needed benchmark for systematic safety evaluation. I encourage the authors to expand the discussion of defense strategies—e.g., dynamic tool vetting, cross-server provenance tracing, or adaptive sandboxing—and to better articulate what constitutes “safe” behavior in a multi-agent MCP context. With those clarifications, MCP-SafetyBench could become a foundational resource for future work on secure and trustworthy LLM tool-use.\n\n\n### Suggestions for Improvement\n- Contextualization. Frame the benchmark’s attacker goals/capabilities using a general red-teaming threat-modeling framework (e.g., comprehensive LLM red-team taxonomies that introduced function-calling/tool attack families), and connect to broader agent safety evaluations beyond MCP\n- All results use ReAct; robustness may be agent-policy dependent. Re-run a subset with a planning-heavy or tool-gating agent to test claim generality or provide some discussion.\n- Some other ablations to report that would be valuable contribution to the community: (i) Vary #servers and cross-server hops; (ii) swap benign vs malicious manifests (“shadow/decoy” tools) to measure false positives; (iii) cap attacker budget (edits/characters/calls) and show ASR-vs-budget curves, etc. The current paper as it stands doesn't provide any discussion or ablations in main section of the paper."}, "questions": {"value": "See comments above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XyTnbxBwqf", "forum": "7XYjeL46co", "replyto": "7XYjeL46co", "signatures": ["ICLR.cc/2026/Conference/Submission12214/Reviewer_aJQS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12214/Reviewer_aJQS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966775067, "cdate": 1761966775067, "tmdate": 1762923160257, "mdate": 1762923160257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}