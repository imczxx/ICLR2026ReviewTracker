{"id": "SNVu84wBhH", "number": 4135, "cdate": 1757609098664, "mdate": 1759898051398, "content": {"title": "SoCo: Progressive Spectrum Optimization for Large Language Model Compression", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, yet prohibitive parameter complexity often hinders their deployment. Existing singular value decomposition (SVD) based compression methods equate singular values with component importance, an assumption that often fails to correlate with downstream task performance. In this work, we introduce SoCo (Singular spectrum optimization for large language model compression), a novel framework that learns to rescale SVD components. Concretely, we employ a learnable diagonal matrix to assign importance scores and introduce Progressive Spectrum Optimization, a principled strategy that operates in a single, continuous training run. Inspired by heuristic optimization, this process guides the learnable scores through distinct functional phases—from an initial exploration of the solution space, through an oscillation refinement, to a final, decisive sparsification—thereby navigating the complex optimization landscape to balance compression and performance. Thanks to this adaptive process, SoCo prunes components based on their learned importance, rather than a fixed order. More importantly, amplified scores on preserved components allow them to compensate for the information loss from pruning. Experimental evaluations across multiple LLMs and benchmarks demonstrate that SoCo surpasses state-of-the-art methods in large language model compression.", "tldr": "We propose SoCo, a data‑driven SVD‑based compression framework that learns to rescale and sparsify the singular spectrum via a three‑stage optimization, yielding superior compression–performance trade‑offs for large language models.", "keywords": ["Large Language Model", "Model Compression", "Singular Value Decomposition", "Data‑Driven Optimization", "Low‑Rank Approximation", "Singular Spectrum Rescaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/844eb8a3e2921e8ab1094120c2bb72f914630822.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SoCo (Singular spectrum optimization for large language model Compression), a novel compression framework for large language models. Unlike traditional SVD-based methods that simply truncate components by singular value magnitude order, SoCo employs a learnable diagonal matrix to reassess the importance of SVD components, drawing inspiration from heuristic optimization algorithms. The method optimizes importance scores through three phases within a single training run: compression-driven exploration, oscillatory refinement, and decisive sparsification, effectively avoiding local optima that direct optimization can easily fall into. Unlike existing methods, SoCo not only prunes components based on learned importance but also amplifies the scores of preserved components to compensate for pruning loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- SoCo shows substantial improvements over state-of-the-art methods, with particularly notable gains at higher compression ratios.\n- SoCo is evaluated on multiple LLMs across diverse architectural families showing consistent effectiveness across different model architectures."}, "weaknesses": {"value": "- The paper lacks comparison with quantization-based compression methods and doesn't explore how SoCo could be combined with quantization in a comprehensive compression pipeline. Since quantization is a common complementary approach to model compression, this represents a significant gap in the evaluation.\n- The main results compare SoCo (which involves training) against baselines without any fine-tuning, creating potentially unfair comparisons. While the authors try to address this concern in Section 4.3 by comparing against LoRA-enhanced baselines, this analysis is insufficient: it only evaluates a single model on one dataset's perplexity, lacks details about LoRA fine-tuning settings, and doesn't discuss the computational overhead comparison between different methods.\n- As a model compression method, the paper doesn't report inference speedup or throughput (tokens/sec) on real hardware under different settings (batch sizes, sequence lengths, etc.), which is crucial for evaluating the practical benefits of compression in deployment scenarios."}, "questions": {"value": "1. How does SoCo interact with quantization techniques, and can these compression methods be effectively combined in practice?\n2. Could the authors provide more comprehensive comparisons with fine-tuned baselines beyond the limited analysis in Section 4.3? Specifically, what are the LoRA fine-tuning settings, computational overhead comparisons, and results across multiple models and datasets rather than just single model perplexity on C4?\n3. What inference speedup and throughput improvements does SoCo achieve on real hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lWU6piDLhy", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_QwSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_QwSQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760865702719, "cdate": 1760865702719, "tmdate": 1762917192298, "mdate": 1762917192298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoCo, a novel framework that learns to adaptively rescale and prune the singular spectrum of weight matrices in LLMs. Instead of directly truncating singular values in fix order, SoCo employs a learnable diagonal matrix of importance scores that are optimized end-to-end through a Progressive Spectrum Optimization process. This optimization proceeds through three phases: exploration, oscillatory refinement, and decisive sparsification, guided by differentiable objectives combining compression ratio, performance preservation (KL divergence), and sparsity regularization. Experiments show that SoCo consistently outperforms SOTA SVD-based methods and remains efficient in both trainable params and runtime."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed SoCo framework makes the SVD-based pruning process learnable, making SVD pruning more versatile and accurate on different models and tasks, and the insights proposed are of high value to subsequent related work.\n2. The Progressive Spectrum Optimization (PSO) is conceptually elegant, breaking training into distinct phases that progressively stabilize and polarize importance scores. The combination of differentiable compression and KL-based alignment is effectively balances compression with accuracy. The PSO also jointly optimizes decomposition and rescaling in an end-to-end way, improving global convergence.\n3. The authors conduct comprehensive experiments, including comprehensive comparisons with similar SVD-based methods and other pruning methods on various models and tasks, and design corresponding ablation experiments for almost every component (1-2-3 phases, deviation term, functions). These experiments strongly validated the advanced nature of SoCo and the effectiveness of its design.\n4. The authors also provide detailed hyperparameters and experimental settings, and deploy SoCo on real devices to test the actual acceleration performance, which improves the reproducibility and application value of the work."}, "weaknesses": {"value": "1. It’s not fully explicit how the trainable deviation $d$ is applied to $W'$, more implementation specifics (for example, are there similar constraints on $d$ during PSO) would aid reproducibility for diverse codebases.\n2. The proposal of PSO mentioned in 3.3 seems intuitive. Is there any further formal theoretical guarantee, such as the existence or convergence of the final performance?\n3. Comparison with non-SVD pruning methods is limited. Table 4 only shows the performance on PPLs, without comparing performance on multiple downstream tasks like other results. It would be helpful if the authors could provide more detailed results and analysis."}, "questions": {"value": "1. Training largely relies on WikiText-2 as the calibration corpus; it is small and stylistically narrow relative to real deployment. How sensitive are results to calibration data scale/domain? \n2. Have you tried to design different thresholds for different layers (the activation distributions of network layers with different depths are significantly different)? Any benefit to adaptive thresholds vs preset?\n3. Is SoCo compatible with commonly used model quantization methods, such as GPTQ? Can subsequent LoRA fine-tuning further restore performance after SoCo pruning (especially at high compression ratio)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pQMmoIuPWE", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_a4Qo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_a4Qo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761155549596, "cdate": 1761155549596, "tmdate": 1762917191990, "mdate": 1762917191990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoCo, a framework for compressing large language models (LLMs) by learning to rescale and prune SVD components in the model's weight matrices. Rather than solely truncating the smallest singular values as in standard SVD-based compression, SoCo introduces learnable importance scores (via a diagonal matrix) and proposes a progressive spectrum optimization procedure inspired by heuristic optimization algorithms like simulated annealing. The process iteratively explores, refines, and sparsifies the importance assignments, aiming to achieve a better trade-off between model size and downstream task performance. Empirical results across a range of LLMs and benchmarks consistently show improved perplexity and task accuracy over several SVD-based and pruning baselines, especially at high compression ratios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Sufficient Experiments: The experiments include a wide variety of settings, multiple models, and strong SVD/Pruning baselines. Figure 1 and Table 2 clearly show that SoCo outperforms all tested baselines, especially at aggressive compression rates.\n2. Principled Approach and Insightful Design: The paper identifies and addresses a core limitation of classical SVD-based compression, the disconnect between singular value ordering and real downstream task importance. The proposed introduction of a learnable diagonal score matrix is a conceptually sound way to endow SVD compression with task-awareness."}, "weaknesses": {"value": "1. A more comprehensive analysis is required to elucidate why SoCo attains such outstanding performance. Although the paper suggests that the ordering of singular values may not necessarily correlate with downstream task performance, Figures 3, 4, and 9 reveal that SoCo not only learns to prune the SVD components within the model’s weight matrices but also to rescale these components and reallocate pruning ratios across layers. Additional analytical experiments are necessary to quantify the contribution of each component to SoCo’s overall performance and to empirically validate the claim that the ordering of singular values may not directly correspond to downstream task performance.\n2. The paper uses specified training steps rather than any criteria to determine when to transition from Phase 1 and Phase 2 to Phase 3, and whether to end Phase 3 training. As these standards can significantly influence model performance, where overtraining may cause overfitting and insufficient training may lead to suboptimal convergence, the authors should provide explicit guidelines or empirical criteria for deciding when to switch training phases and when to stop training.\n3. Lines 202-204 mention that SoCo introduces a trainable deviation term $d$ after transforming $W^’$. However, the paper does not explain why this term is necessary. What motivates the inclusion of $d$. Does SoCo encounter any critical issues that require introducing additional trainable parameters to mitigate them?"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6cCJshTq5N", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_dvHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_dvHK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878426180, "cdate": 1761878426180, "tmdate": 1762917191613, "mdate": 1762917191613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SoCo, a SVD-based compression method to prune less important singular components. Previous methods typically perform truncation based on the magnitude of singular values, neglecting the importance of the singular vectors themselves in relation to the loss. To address this, SoCo presents a three-phase training framework that keeps the singular vectors fixed while rescaling the singular values. This framework incorporates constraints on compression rate and applies a sparsity penalty to the singular values, resulting in effective compression of LLMs.  Experiments conducted on various models demonstrate that the proposed method outperforms existing SVD-based compression baselines as well as pruning compression methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The core idea of directly optimizing the loss to assess the importance of singular vectors is sound.\n3. The paper conducts comprehensive experiments across various models and baseline methods."}, "weaknesses": {"value": "1. SoCo requires fine-tuning with three phases, which contributes to its overall complexity, as well as making the approach difficult to reproduce.\n2. Some details of the fine-tuning process are lacking:\n- How does Phase 3 ensure the compression rate, given that the loss function does not impose any constraints on it?\n- Certain training stages involve two different losses that are simply summed in the paper. When switching models or training datasets, the relative magnitudes of these losses may change, potentially leading to a degradation in SoCo's performance.\n- The rationale for dividing the training into three phases could be re-evaluated; for instance, a single training objective could be developed to address the requirements of all three phases.\n3. For the generation task, experiments were conducted only on TruthfulQA, which undermines the persuasiveness of the results.\n4. In the main experiments, comparing SoCo with methods that do not require training is unfair. The content of Table 6 should be extended for more comprehesive comparison in the main experiments.\n5. The paper does not report on efficiency, including training cost and the acceleration during inference.\n6. Compared to quantization, the SVD-based truncation method alone has limited model compression capability, which makes such methods less significant than quantization methods in real-world scenarios. Therefore, it is important to see how this method can be combined with quantization methods to achieve effective compression."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6cCJshTq5N", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_dvHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_dvHK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878426180, "cdate": 1761878426180, "tmdate": 1763100685603, "mdate": 1763100685603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SoCo, a framework for compressing large language models (LLMs) by learning to rescale and prune SVD components in the model's weight matrices. Rather than solely truncating the smallest singular values as in standard SVD-based compression, SoCo introduces learnable importance scores (via a diagonal matrix) and proposes a progressive spectrum optimization procedure inspired by heuristic optimization algorithms like simulated annealing. The process iteratively explores, refines, and sparsifies the importance assignments, aiming to achieve a better trade-off between model size and downstream task performance. Empirical results across a range of LLMs and benchmarks consistently show improved perplexity and task accuracy over several SVD-based and pruning baselines, especially at high compression ratios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Sufficient Experiments: The experiments include a wide variety of settings, multiple models, and strong SVD/Pruning baselines. Figure 1 and Table 2 clearly show that SoCo outperforms all tested baselines, especially at aggressive compression rates.\n2. Principled Approach and Insightful Design: The paper identifies and addresses a core limitation of classical SVD-based compression, the disconnect between singular value ordering and real downstream task importance. The proposed introduction of a learnable diagonal score matrix is a conceptually sound way to endow SVD compression with task-awareness."}, "weaknesses": {"value": "The paper does not specify any criteria for determining when to transition from Phase 1 and Phase 2 to Phase 3 training, or when to terminate Phase 3. As these standards can significantly influence model performance, where overtraining may cause overfitting and insufficient training may lead to suboptimal convergence, the authors should provide explicit guidelines or empirical criteria for deciding when to switch training phases and when to stop training."}, "questions": {"value": "1. A more comprehensive analysis is required to elucidate why SoCo attains such outstanding performance. Although the paper suggests that the ordering of singular values may not necessarily correlate with downstream task performance, Figures 3, 4, and 9 reveal that SoCo not only learns to prune the SVD components within the model’s weight matrices but also to rescale these components and reallocate pruning ratios across layers. Additional analytical experiments are necessary to quantify the contribution of each component to SoCo’s overall performance and to empirically validate the claim that the ordering of singular values may not directly correspond to downstream task performance.\n2. Lines 202-204 mention that SoCo introduces a trainable deviation term $d$ after transforming $W^\\prime$. However, the paper does not explain why this term is necessary. What motivates the inclusion of $d$. Does SoCo encounter any critical issues that require introducing additional trainable parameters to mitigate them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qJ8zLE3lp", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_jYZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_jYZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894664765, "cdate": 1761894664765, "tmdate": 1762917191319, "mdate": 1762917191319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SoCo to compress LLMs by learning to rescale and prune SVD components of the weight matrices at different stages. The learnable importance scores are optimized by heuristic optimization algorithms. The SoCo framework iteratively compresses, refines, and sparsifies the importance assignments with Progressive Spectrum Optimization strategy. Experiments across different models and compression levels show that SoCo consistently outperforms prior SVD-based and pruning methods in perplexity and downstream task accuracy, especially under high compression ratios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea is interesting. It dynamically identifies the singular vectors to retain at each layer, and the subsequent rescaling of the preserved singular values in Stage 3 further enhances the performance of the compressed model.\n2. Extensive experiments over a wide range of compression ratios and model architectures demonstrate that SoCo consistently outperforms existing approaches across diverse settings."}, "weaknesses": {"value": "The proposed method is complex including the tuning of multiple hyperparameters, such as the threshold of compression ratio, lambda parameters in Equation (4), the trainable deviation term $d$ in line 203, and the loss weight of L_{inc} and L_{dec}. The training procedure shown in Table 1 may face the training stability issue as the transition among different conditions are not smooth. \n\nThe evaluations are conducted on pretrained LLMs with the wikitext-2 as the training data. However, instruct LLMs or aligned LLMs are expected to be compressed in real applications. The method are expected to be evaluated with diverse training data (tasks like sft) and  LLMs (e.g. more recent instruct/aligned LLMs instead of LLMs in the year 2023) on diverse downstreams generation tasks (like math reasoning, alpaca-eval, etc.)\n\nIn addition, more analyses of the compressed LLMs are expected to make it clear why this framework can work."}, "questions": {"value": "1. As shown in Table 1, will and should the loss item $L_{inc}$ be considered as the  Conditions for different phrases? what is the loss curve for L_{inc} in your experiment?\n2. As shown in Equation (7), do you consider to select the ground truth labels as the training references? what is the difference?\n3. In the experiments, what is your motivaion or consideration to select these pretrained LLMs?\n4. Line 203, what is the shape of the item d? what is your consideration of this design? will the structure or model be different on other tasks or datasets?\n5. The evaluations on more recent LLMs and diverse reasoning generative tasks are expected.\n6. More recent references and baselines are expected to be incorporated in this paper. Only one Qwen-2.5 technique report paper is listed in 2025, others are all published one year ago."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4qJ8zLE3lp", "forum": "SNVu84wBhH", "replyto": "SNVu84wBhH", "signatures": ["ICLR.cc/2026/Conference/Submission4135/Reviewer_jYZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4135/Reviewer_jYZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894664765, "cdate": 1761894664765, "tmdate": 1763103306635, "mdate": 1763103306635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}