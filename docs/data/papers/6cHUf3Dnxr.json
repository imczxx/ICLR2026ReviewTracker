{"id": "6cHUf3Dnxr", "number": 20560, "cdate": 1758307392961, "mdate": 1759896971279, "content": {"title": "HAIPR: A High-Throughput Affinity Prediction Framework", "abstract": "Computational prediction of protein binding affinity is a cornerstone of modern drug development, accelerating tasks from lead optimization to de novo protein design. However, progress is often hampered by evaluation practices, such as Random Cross-Validation (RandomCV), that can substantially overestimate model generalization on real-world tasks and lacking experimental validation. To address this, we introduce HAIPR, a unified framework that standardizes the entire modeling pipeline from training and optimization to inference, providing an initial selection of algorithms, robust evaluation protocols and curated benchmark datasets. By extending the BindingGYM benchmark and implementing more realistic, biologically meaningful data splits, our framework reveals that model performance on these challenging tasks is substantially lower than suggested by RandomCV. We systematically compare classical machine learning approaches, such as Support Vector Regression (SVR) on protein language model (pLM) embeddings, with parameter-efficient fine-tuning (PEFT) of pLMs. Our results show that SVR can be competitive in low-data regimes and less prone to model collapse, while PEFT methods offer clear advantages as dataset size and problem complexity increase. Furthermore, we analyze the minimum data requirements for reliable prediction and demonstrate that even modestly sized models can achieve performance that rivals the experimental reproducibility between state-of-the-art affinity assays, highlighting a critical ceiling for in silico prediction. Code and pre-computed embeddings are made available.", "tldr": "We provide a unified framework for end-to-end in-silico protein binding affinity maturation based on deep mutational scanning data.", "keywords": ["binding-affinity prediction", "deep mutational scanning", "high-throughput screening"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d901949103e75b2b3a16510ca9f152f33a458d3a.pdf", "supplementary_material": "/attachment/b04ab4d897875033f913a27adf135421f0183462.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces HAIPR, a unified framework for training, evaluating and performing inference for single‑complex binding‑affinity prediction using deep mutational scanning (DMS) data. The authors argue that standard random cross‑validation (RandomCV) inflates performance estimates because train and test variants are drawn from similar distributions. To address this, they extend the BindingGYM benchmark with five combinatorial DMS datasets and propose two evaluation splits that preserve all available data but better mimic real‑world generalisation: Leave‑One‑Mutation‑Out (LoMo), where all variants containing a particular mutation are withheld, and Out‑of‑Distribution (OOD) splits, where affinity labels are binned and one bin is held out for testing. The HAIPR pipeline includes data preprocessing, configurable split generation, hyper‑parameter optimisation, and a simple Predictor/Generator interface to support arbitrary models and sequence generators."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Predicting how mutations affect binding affinity is a key challenge in drug discovery and protein engineering. The paper clearly shows that RandomCV can overestimate model performance because many mutations appear in both train and test sets. The authors make a strong case that better splits are needed to reflect real-world generalisation, where the goal is to predict unseen mutations.\n\n- The proposed LoMo and OOD splits address this issue effectively. LoMo tests generalisation to unseen mutation sites, while OOD splits hold out affinity ranges. Both maintain full dataset size, unlike single-mutant filtering. Figures 4 and 5 show these splits reveal much lower performance than RandomCV, highlighting the gap between conventional benchmarks and realistic use cases.\n\n- The authors evaluate SVR and PEFT models across multiple ESM sizes (8 million to 15 billion parameters) on 21 datasets.\n\n- The authors implement a genetic algorithm that uses an ensemble of fine‑tuned ESMC‑300M models to explore sequence space and fold promising candidates with BOLTZ‑2.\n\n- The paper is generally well‑written and includes informative diagrams."}, "weaknesses": {"value": "- The paper does not clearly explain how OOD bins are defined. It's unclear if they are based on equal width, equal counts, or another rule. The LoMo split assumes that each mutation can be held out without hurting diversity in the training set. It would help to report how many variants are used in train and test per LoMo split, and how sensitive the results are to the binning method. Even a small table in the appendix with sample counts per split would improve clarity and reproducibility.\n\n- Figure 2-B’s caption needs more explanation. As it stands, the figure is hard to understand without extra effort.\n\n- Section 3.3 should describe what the model inputs and outputs are. Explaining this clearly would help readers in machine learning better understand the setup. Skipping these details makes the method harder to follow.\n\n- The models considered are a standard SVR with radial‑basis kernel and a PEFT variant of pLM fine‑tuning. No new architectures or task‑specific losses are proposed, and no structural models or graph neural networks are evaluated, despite recent successes in geometry‑aware affinity prediction. The demonstration that SVR can be competitive in low‑data regimes is interesting but incremental; similar conclusions can be noted in the LoMo and OOD splitting strategy. As such, the methodological contribution lies mainly in designing the evaluation pipeline rather than new methods.\n\n- The models used are standard SVR with RBF kernel and a PEFT-based fine-tuning of pLMs. There are no new architectures, no custom loss functions, and no comparison with structure-based models like GNNs, which are increasingly common in affinity prediction. While it's useful to show that SVR works well in small data settings, this is not a new insight. Similar conclusions can be noted in the LoMo and OOD splitting. The main contribution lies in the evaluation pipeline, not in model design.\n\n- The PEFT experiments use fixed DoRA rank and dropout values, without tuning. The paper notes that PEFT sometimes collapses, causing missing results.\n\n- Only Spearman correlation is reported. While useful, practical applications often care about identifying top binders or predicting absolute affinities. Including other metrics like mean squared error or top-k classification accuracy would give a fuller picture."}, "questions": {"value": "I appreciate the effort and care in this work. In my view, it is better suited for a bioinformatics workshop at ICLR than for the main track. The paper does not introduce a novel idea or method that would justify main-track acceptance. But I understand, the binding affinity problem is important. A practical pipeline for realistic evaluation is valuable for the community. If the authors address the weaknesses listed above or clarify points I may have misunderstood, I am willing to raise my score to 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4lXZgW43Lt", "forum": "6cHUf3Dnxr", "replyto": "6cHUf3Dnxr", "signatures": ["ICLR.cc/2026/Conference/Submission20560/Reviewer_xb4f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20560/Reviewer_xb4f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761609797122, "cdate": 1761609797122, "tmdate": 1762933975178, "mdate": 1762933975178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HAIPR, a framework that standardizes training, evaluation, and inference for predicting binding-affinity changes of protein–protein complex (PPC) variants. This paper states that conventional RandomCV overestimates generalization performance. To address this issue, they proposed a dataset and introduced a novel splitting method, OOD and LoMo, to evaluate models under screening scenarios that better reflect practice. They also compare SVR and parameter-efficient fine-tuning (PEFT; DoRA) on top of pLM embeddings to analyze trade-offs across dataset size and task difficulty. Additionally, they examine the dependency of sample size to both RandomCV and proposed splitting method. They also analyzed the effect of focus-on/off input settings, and they demonstrate a high-throughput design pipeline that combines a genetic algorithm with structure prediction (BOLTZ-2)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes an evaluation protocol that addresses RandomCV’s tendency of overestimating model performance by introducing OOD and LoMo splits.\n- This paper also integrates a GA and BOLTZ-2-based sequence generation method, demonstrating practical downstream utility."}, "weaknesses": {"value": "- The evaluation relies heavily on Spearman correlation; therefore, it would be better to include other evaluating metrics other than Spearman correlation.\n- The proposed split is not compared against Contig or Modulo splits under identical conditions. It would be better to show the comparison result using quantitative metric.\n- There was no comparison of proposed sequence generation algorithm and conventional optimizers (e.g., greedy, random mutational search) under the same settings.\n- LoMo split might be inefficient for context-aware models. When a model leverages adjacent residues to predict binding affinity, omitting the mutated residue has minimal impact, which might undermine the purpose of the split. \n- The title can be read as a general molecular evaluation framework; it should explicitly state its PPI/PPC focus to avoid ambiguity and enhance title clarity. \n- OOD split should consider distance between embeddings, which was not considered in this paper. It would be better if showing distances between embeddings."}, "questions": {"value": "- Is there a reason for choosing SVR as the machine-learning baseline rather than models such as Random Forest or XGBoost? \n- The explanation of LoMo split is unclear. Since there are combinatorial library dataset containing multiple mutations, did you mean excluding all the specific mutations at specific site or at all sites?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F7Ot34VpEg", "forum": "6cHUf3Dnxr", "replyto": "6cHUf3Dnxr", "signatures": ["ICLR.cc/2026/Conference/Submission20560/Reviewer_gzwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20560/Reviewer_gzwm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809524567, "cdate": 1761809524567, "tmdate": 1762933974785, "mdate": 1762933974785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HAIPR, a unified framework for high-throughput affinity prediction on protein–protein complexes (PPCs) using DMS data, which is particularly helpful improving the given protein's affinity. Its main technical pieces are: (i) standardized evaluation splits—Leave-one-Mutation-out (LoMo) and label-binned OOD—intended to avoid RandomCV optimism; (ii) a curated benchmark (extending BindingGYM with five combinatorial datasets to 21 PPCs); and (iii) baselines comparing SVR on ESM embeddings with PEFT (DoRA) fine-tuning, plus a genetic-algorithm demo with Boltz-2 folding checks. The results reinforce that RandomCV overestimates performance, LoMo/OOD are more realistic, focus-on vs. focus-off context yields only small differences for sequence PLMs, and PEFT can outperform SVR but is training-sensitive and collapses more often."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Evaluation realism.** Defines LoMo (hold out all samples containing a mutation token) and label-binned OOD (hold out an affinity bin), addressing RandomCV optimism and avoiding single-mutant filtering losses seen in contig/modulo.\n- **Benchmark expansion.** Expands BindingGYM with five combinatorial datasets (total 21 PPCs) and documents their characteristics for split design."}, "weaknesses": {"value": "- **Limited novelty.** The core contribution is split design and packaging; modeling components (ESM embeddings+SVR, DoRA-PEFT) and DMS curation build on existing lines (ProteinGym/BindingGYM). Scientific novelty is modest for a top-tier venue.\n- **Framework description is high-level.** Interfaces (Predictor/Generator), dataset registry, and exact split manifests are not specified in enough detail to guarantee drop-in applicability although the authors choose this as their main contribution. If authors can give an example of using new model with their framework in the level of the code, it would be more helpful to understand the paper's strength.\n- **Underpowered design-loop evidence for a \"unified\" framework.** Although HAIPR is motivated by accelerating enrichment for protein design, the paper demonstrates only one generator (a genetic algorithm) on a single dataset, without head-to-head enrichment against diverse generators (e.g., RFdiffusion/ProteinMPNN-based loops) or standardized enrichment metrics (e.g., top-k hit rate per iteration, best/median affinity gain, sample complexity under LoMo/OOD). As a result, the practical acceleration claim remains weak.\n- **Multi-chain input treatment is ad-hoc.** Focus-off concatenates chains with a separator token (not native to ESM training), yet shows little benefit; the work does not compare against chain-wise embedding+fusion or structural models in a controlled way. This weakens conclusions about \"structural context\"."}, "questions": {"value": "1. **Scatter vs. density panels.** In Fig. 4, are the prediction scatter plots drawn on train+test or test-only data? A legend/footnote clarifying this would help interpret calibration and the apparent distribution mismatch.\n2. **PEFT collapse diagnostics.** What proportion of runs collapsed per model? Why does the model collapse frequently? Are there any expected reasons for the issue, such as noisy data labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qabuTODiyZ", "forum": "6cHUf3Dnxr", "replyto": "6cHUf3Dnxr", "signatures": ["ICLR.cc/2026/Conference/Submission20560/Reviewer_EGdD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20560/Reviewer_EGdD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962951641, "cdate": 1761962951641, "tmdate": 1762933973979, "mdate": 1762933973979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes HAIPR, a high-throughput framework for protein–protein binding affinity prediction. It extends the BindingGYM benchmark by adding new combinatorial deep mutational scanning datasets and provides standardized evaluation protocols for assessing model generalization. The authors argue that common practices such as Random Cross-Validation can lead to overly optimistic results. To address this, they propose two alternative evaluation schemes, Leave-One-Mutation-Out and Out-of-Distribution splits. The framework supports both classical machine learning methods, such as Support Vector Regression, and parameter-efficient fine-tuning of protein language models. The authors further analyze the effects of sample size and evaluate the feasibility of in silico screening for variant design."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a relevant issue in computational biology for fair and reproducible evaluation of protein affinity prediction models. The motivation is clear, and the idea of systematically comparing data splits to assess generalization is useful. The experiments are extensive, and the results effectively illustrate the overestimation caused by Random Cross-Validation. The inclusion of minimal data size analysis and the availability of a unified interface for benchmarking may benefit future research in this area."}, "weaknesses": {"value": "The novelty of the work is limited. The framework mainly combines existing datasets, standard evaluation strategies, and previously available models into one framework. The proposed data splits are incremental extensions rather than fundamentally new evaluation concepts. The analysis provides limited mechanistic or theoretical insight into model behavior, and the experimental findings are largely confirmatory rather than revealing new patterns."}, "questions": {"value": "Besides using SVR and PEFT, have the authors considered freezing all parameters of the pLMs and training only a lightweight MLP head on top of the frozen embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A9k3VOYDyV", "forum": "6cHUf3Dnxr", "replyto": "6cHUf3Dnxr", "signatures": ["ICLR.cc/2026/Conference/Submission20560/Reviewer_FrK5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20560/Reviewer_FrK5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20560/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141640103, "cdate": 1762141640103, "tmdate": 1762933973511, "mdate": 1762933973511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}