{"id": "Bq0CAUrMCC", "number": 12362, "cdate": 1758207286897, "mdate": 1763543187165, "content": {"title": "Structured Transformer Circuits Pruning", "abstract": "Transformers become ubiquitous across vision and language tasks, but their depth and parameter count often far exceed what is needed for a given downstream application, leading to unnecessary compute and memory overhead. Existing layer‐pruning techniques either require multiple retraining cycles, rely on continuous relaxations that never fully deactivate blocks, or depend on architecture‐specific analyses. We introduce STCP, a model‐agnostic, single‐pass pruning framework that learns binary gates over each block’s multi-head self-attention (MHSA) and MLP sub-layers in a pretrained transformer.  We optimize gates while also injecting noise and introducing an $L_1$ penalty: this allows us to escape from local minima, and to find sparser circuits. We validate STCP on both image classification and NLP tasks with large pretrained models, showing good trade-offs in terms of complexity and performance. The code will be made publicly available upon acceptance of the article.", "tldr": "", "keywords": ["Compression", "Pruning", "Transformer", "Efficiency"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee28a3ffe25f2452e7383e70ec76c5ac32f0e190.pdf", "supplementary_material": "/attachment/cdb2acf1d5e4897ed63319d9a59a3dc2ab7f02bc.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Structured Transformer Circuits Pruning (STCP), a structured pruning method to improve the efficiency of Transformer models by removing entire Multi-Head Self-Attention (MHSA) and MLP sub-layers within each block. They apply a binary gate to each sub-layer to decide whether to turn it on or off. This gate is implemented using the non-differentiable Heaviside step function, and it employs the Straight-Through Estimator (STE) trick to allow gradients to flow during backpropagation. To help the gates effectively explore on and off states during fine-tuning, Gaussian noise is injected. Simultaneously, an L1 regularization penalty is applied to encourage the gate values to move toward zero. After training, gate parameters $g$ are ranked, and sub-layers are removed sequentially until performance drops just below a threshold. Experiments across various Vision (CLIP, ViT) and NLP (BERT, RoBERTa) tasks demonstrated that the method is highly robust in terms of its performance-to-compression ratio compared to existing methods, validating the effectiveness of the proposed methodology."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is well-motivated by applying a binary gating mechanism to structured pruning. This directly targets real-world computational and memory overheads, which is often not achieved by less-structured pruning.\n\n2. STCP is a single-pass pruning framework. This makes it significantly simpler and more practical than prior approaches that typically rely on complex, multi-stage, iterative prune-and-finetune cycles.\n\n3. The framework demonstrates broad applicability. It is model-agnostic and shows strong, consistent performance across diverse architectures (e.g., CLIP, ViT, BERT, ROBERTa) and different domains, including both Vision and NLP tasks.\n\n4. The method proves to be highly robust. Extensive experiments show that it can prune a significant number of sub-layers while maintaining performance very close to, or in some cases even slightly exceeding, the original dense model."}, "weaknesses": {"value": "1. The final pruning stage is a greedy approach that ranks gate values and removes sub-layers sequentially. This method does not guarantee finding the optimal combination of remaining sub-layers.\n2. The process requires manual intervention. The user must define an acceptable performance drop threshold to determine when to stop removing layers, which is not an automated part of the framework.\n3. The paper lacks sufficient references and comparisons to other established differentiable gating mechanisms used for pruning (e.g., Gumbel-Softmax).\n4. The overall writing, structure, and presentation require further polish and refinement. This includes insufficient detail in the Method section (Sec 3), redundant sentences (e.g., “The gates’ values and the possibility for each gate to flip after training for the CLIP model trained on the CIFAR-10 are presented in Appendix B.” is duplicated on pages 7 and 8), and imprecise statements (e.g., “Thus, any gate with $|g|$ small has roughly a 50% chance to be on or off, …” on page 4)."}, "questions": {"value": "Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sZAp7YMvmr", "forum": "Bq0CAUrMCC", "replyto": "Bq0CAUrMCC", "signatures": ["ICLR.cc/2026/Conference/Submission12362/Reviewer_qoVD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12362/Reviewer_qoVD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559305452, "cdate": 1761559305452, "tmdate": 1762923274573, "mdate": 1762923274573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes and evaluates a procedure for removing entire components of transformer models. The scheme adds \"gates\" to the model and then jointly optimizes parameters of the gates and model weights to identify components that can be removed. Numerical experiments are provided to illustrate the efficacy of the scheme."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The manuscript provides an interesting method for shrinking the size of transformer models while retaining performance. Moreover, in the realm of methods that remove entire components it seems to outperform other methods when no fine-tuning is allowed."}, "weaknesses": {"value": "The main weakness of this work is its narrowness in terms of how it presents the comparative landscape. While I agree with the manuscript that there are good computational reasons to preference removal of entire components within a model vs. unstructured pruning methods (in terms of practical speedup), it does seem that the competitive landscape is ultimately larger than just methods that can remove entire components. For example structured pruning methods (i.e., those that remove entire neurons) can be effective for MLPs since they retain the computational structure of the layer just at a smaller size (as opposed to, e.g., introducing unstructured sparsity). \n\nThe literature is too numerous to name, but, e.g., [Kwon, Woosuk, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. \"A fast post-training pruning framework for transformers.\" Advances in Neural Information Processing Systems 35 (2022): 24101-24116.] and [van der Ouderaa, Tycho FA, Markus Nagel, Mart Van Baalen, and Tijmen Blankevoort. \"The LLM Surgeon.\" In The Twelfth International Conference on Learning Representations.] \n\nOne reason to bring this up is that a cursory comparison of results with Kwon et al. above and https://arxiv.org/pdf/2406.00061 suggests that, e.g., BERT models can have a substantial fraction ~40% of the parameters removed in a structured matter while retaining performance similar to the proposed method when only 2 layers are removed. (Thought it is hard to make an explicit comparison across papers because it seems the pretrained baseline may differ?)\n\nIn addition, quantization is also a very effective strategy for reducing both storage and computational time. While, presumably, a pruned model could then be quantized, it is not clear that these procedures are orthogonal: perhaps an aggressively pruned model resists quantization in a way that makes it better to quantize the larger model than prune then quantize. While it is unreasonable to burden this manuscript with answering that rather broad question, it does illustrate that a broader range of experiments are likely needed to help situate the proposed procedure in the broader landscape. I consider comparisons with structured pruning somewhat essential, quantization less so.\n\nLastly, while I am sympathetic to the ease with which one ask for \"more experiments,\" I do think that newer, larger models (particularly for NLP tasks) should be considered to understand both the generality and scalability of the method. Perhaps (smaller) Llama models or similar would be useful here to provide at least a few points of comparison with other modern methods (e.g., LLM surgeon or similar)\n\nSome assorted minor comments/weaknesses:\n\n- While the proposed method does not have an explicit \"fine tuning\" step, the weights are updated with the gates (assuming the pseudo code is followed). (In fact, presumably this is why removing a layer can increase performance.) As such, it seems a bit unfair to completely omit fine tuning from other methods; better would be to control for compute and balance that (or, report compute to make it easier to assess the relative expense of methods)."}, "questions": {"value": "- While the method is articulated as \"single-pass,\" presumably once a certain layer is removed that could change the optimal $g$ parameters for the other layers significantly, is there any value to thinking about the method via sweeps in which at each step 1 or a few layers are removed and then the gates are retrained? \n\n- The $\\ell_1$ penalty seemingly drives the $g$ towards 0, but that corresponds to at 50/50 gate (if I interpreted things correctly) and not an \"off\" gate, were other regularization strategies or penalties explored?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "44OoliuyMt", "forum": "Bq0CAUrMCC", "replyto": "Bq0CAUrMCC", "signatures": ["ICLR.cc/2026/Conference/Submission12362/Reviewer_TxxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12362/Reviewer_TxxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867271157, "cdate": 1761867271157, "tmdate": 1762923274204, "mdate": 1762923274204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Structured Transformer Circuits Pruning (STCP), a single-pass pruning method that inserts learnable binary gates into each MHA and MLP sub-layer of a pretrained transformer. During fine-tuning, Gaussian noise and an L1 regularization encourage some gates to turn off, allowing the model to identify unimportant sub-layers. The method is evaluated on both vision and language transformers (CLIP, ViT-H14, BERT, RoBERTa) and reports competitive results compared with Shortened-Taylor and Joint Layer Drop."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. All training details are included, and the experimental setup is consistent across datasets.\n2. The experiments span both image and text domains and include ablation studies, FLOP/memory/time tables, and comparisons to multiple baselines.\n3. STCP achieves competitive or slightly better results than dense baselines in several small-scale benchmarks, which is pretty impressive."}, "weaknesses": {"value": "1. The proposed gating+L1 framework and mask-training approach have been well-studied before (e.g., SNIP, GraSP, MaskLLM, LLM-Eraser). The contribution on this side feels incremental and more engineer-like.\n2. Because each sub-layer requires storing gates, masks, and gradients throughout training, the memory cost can easily exceed that of normal fine-tuning. The paper shies away from this overhead and never reports peak GPU memory. For large-scale models, such methods quickly become infeasible.\n3. The pruning criterion relies on validation accuracy to decide the pruning threshold, implying that the whole fine-tuning and ranking process must be repeated for each new task, which is far from the claimed \"model-agnostic\" in practice.\n4. All results are on CIFAR-10, Tiny-ImageNet, SST-2, and QNLI with medium-size transformers. There is no evidence that STCP works for modern-scale LLMs or vision foundation models.\n5. While FLOPs and latency are reported for the inference stage, memory usage and training cost vs. accuracy trade-offs are not included, which are often the true bottlenecks for structured pruning."}, "questions": {"value": "1. Could the authors provide the training cost, including Memory usage, Training Time, and FLOPs for baseline methods and STCP? If this is too time-consuming, at least I want to see the comparison between STCP and the dense model, i.e., regular SFT.\n2. Could the authors show that their approach is really \"model-agnostic\"? Modern Structured pruning methods like LLM-Pruner evaluate their pruned model on several tasks instead of one.\n3. Could the authors try to perform STCP on a larger-scale model like Llama3-1B/Qwen2.5-0.5B? They are not so large and are on the same model size compared to what the authors have used in their paper (ViT-H14/Roberta, etc.)\n\nI would be happy to raise my scores if the author could provide these details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xC9AiDyN1v", "forum": "Bq0CAUrMCC", "replyto": "Bq0CAUrMCC", "signatures": ["ICLR.cc/2026/Conference/Submission12362/Reviewer_3kWs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12362/Reviewer_3kWs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973336461, "cdate": 1761973336461, "tmdate": 1762923273740, "mdate": 1762923273740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pruning framework named \"Structured Transformer Circuits Pruning\" (STCP). The method aims to learn binary gates for MHSA and MLP sub-layers within Transformer blocks through a single fine-tuning pass. Its core mechanism is claimed to combine Gaussian noise injection and L1 regularization to remove redundant sub-layers without iterative retraining. The authors report that this method outperforms SOTA (Shortened-Taylor, Joint Layer Drop) on models like CLIP, ViT, and BERT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper well written, the presentation of the idea is clear for most part. \nThe results discussions in main text and appendix seems quite multi-dimensional and comprehensive with a lot of datasets, ablation and hyperparam discussions.\nThere is also a limitation section which worths encouraging."}, "weaknesses": {"value": "1. The evaluation setup seems to have unfairness issue. One of the key advantages of this paper seems to be very cheap in finetuning compared to similar structured pruning methods like LLM-Pruner and RECAP and only requires single pass. But it is confusing that experimental detail also mentioned you still require 10000 steps to train, and there is no training cost comparison with those pruning methods. So I feel the main selling point of this paper becomes questionable. \n\n2. Although the authors put efforts to include a lot of dataset and tasks in experiment, a key problem is that there are no any SOTA pruning methods to compare with in your main results, only a few vanilla strategies like weight pruning and gradient pruning. not sure how the performances position in the latest landscape. \n\n3. The compression rates achieved are also underwhelming for models in main results (CLIP and ViT), i.e. up to 17/64=26.56% total pruning rate, which is quite conservative compared to SOTAs like LLM-Pruner, OBC [1]. \n\n\n[1] Frantar, Elias, and Dan Alistarh. \"Optimal brain compression: A framework for accurate post-training quantization and pruning.\" Advances in Neural Information Processing Systems 35 (2022): 4475-4488."}, "questions": {"value": "1. Does the single-pass referred to pruning steps or finetuning steps? If it's pruning stage, then it kind of defeats the whole motivation claimed in this paper, because SOTA methods like LLM-pruner also only requires single-pass for pruning stage if using taylor criterion to collect gradient. If it's finetuning step, what's the 10000 training steps mentioned in Section 4.1 refers to?\n2. Related works mentioned some SOTA pruning baselines like LLM-Pruner, but none were compared in the experiment results. It is concerning if this meets ICLR standard."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1WjpK8AThq", "forum": "Bq0CAUrMCC", "replyto": "Bq0CAUrMCC", "signatures": ["ICLR.cc/2026/Conference/Submission12362/Reviewer_6Svc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12362/Reviewer_6Svc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762850066196, "cdate": 1762850066196, "tmdate": 1762923273448, "mdate": 1762923273448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}