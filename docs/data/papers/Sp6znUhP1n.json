{"id": "Sp6znUhP1n", "number": 18793, "cdate": 1758290937951, "mdate": 1763632695049, "content": {"title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs", "abstract": "Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval.\nWe present SynthKGQA, a LLM-powered framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models.\nWe apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers  with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.", "tldr": "A new framework for generating high-quality, complex Knowledge Graph Question Answering synthetic datasets with ground-truth subgraphs, enabling better training and benchmarking of GraphRAG solutions", "keywords": ["knowledge graphs", "augmented LLMs", "GraphRAG", "RAG", "benchmarks", "synthetic datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66423f4b820e661f2843ec3a36868ecff47d7346.pdf", "supplementary_material": "/attachment/6467cb4dd9864f7ff4a315c21f1251023a5a508a.zip"}, "replies": [{"content": {"summary": {"value": "They introduce SynthKGQA, a framework for generating high-quality synthetic knowledge-graph QA datasets from any KG, supplying the full set of facts in the KG to reason over for each question. They demonstrate that, besides enabling more informative benchmarking of KG retrievers, data created with SynthKGQA also helps train better models. They apply SynthKGQA to Wikidata to generate GTSQA, a new dataset meant to test zero-shot generalization of KG retrievers across unseen graph structures and relation types, and benchmark popular KG-augmented LLM methods on it."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. They proposed SynthKGQA, a new framework that enables scalable creation of KGQA datasets using LLMs.\n\n2. Using this framework, they introduced GTSQA, a new KGQA dataset."}, "weaknesses": {"value": "The framework proposed in this paper, called SynthKGQA, which utilizes an LLM, is a method that has already been used in other KGQA studies [1, 2]. If the framework claimed as the main contribution of this paper has already been introduced in previous works, it is difficult to regard this paper’s contribution as significant. The paper should provide a detailed explanation of how this approach differs from those existing methods.\n\n[1] Ronak Pradeep, Daniel Lee, Ali Mousavi, Jeffrey Pound, Yisi Sang, Jimmy Lin, Ihab Ilyas, Saloni Potdar, Mostafa Arefiyan, and Yunyao Li. 2024. ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA Datasets with Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 1176–1206, Miami, Florida, US. Association for Computational Linguistics.\n\n[2] Dammu, Preetam Prabhu Srikar, Himanshu Naidu, and Chirag Shah. \"Dynamic-kgqa: A scalable framework for generating adaptive question answering datasets.\" Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2025."}, "questions": {"value": "As noted in the Weaknesses section, the approach of using an LLM to extract subgraphs and then leveraging them to construct a KGQA dataset has already been introduced in other papers. A detailed explanation is needed of how this paper’s method differs from prior work. Could you provide such an explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lNTNrVarav", "forum": "Sp6znUhP1n", "replyto": "Sp6znUhP1n", "signatures": ["ICLR.cc/2026/Conference/Submission18793/Reviewer_EYwG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18793/Reviewer_EYwG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760814163405, "cdate": 1760814163405, "tmdate": 1762999993486, "mdate": 1762999993486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the framework SynthKGQA to generate KGQA (Knowledge Graph Question Answering) benchmarks with the help of an LLM. The framework works as follows: a subgraph is randomly selected from the KG and it is passed to an LLM, which generates a question where the entities involved in the question and its answer are within the sampled subgraph (referred to as ground-truth subgraph). The framework is then used to generate a new dataset from Wikidata, designed to test zero-shot generalization abilities of KG retrievers to unseen question graph structures and unseen relation types. The authors also claim that training KG retrievers with questions generated from ground truth subgraphs produces better models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper highlights that by using ground-truth subgraphs it is possible to train better KG retrievers (Table 3).\n\nThe description of the generation framework is clear and well-summarized by Figure 1."}, "weaknesses": {"value": "The authors briefly mention concurrent works in the main text, relegating a more detailed comparison to Appendix E. These are the most relevant works to this paper and should be discussed in more detail within the main text. The two concurrent works mentioned in the paper use ground-truth subgraphs to generate question-answers; hence, it seems that the differences between the proposed approach and the most recent concurrent work primarily lie in aspects like the usage of all seed entities, SPARQL validation by directly discarding some data, and the addition of some questions with unseen graph structures. The main text should highlight that the paper is introducing neither the usage of ground-truth subgraphs to generate queries, nor the usage of SPARQL to validate them.\n\nFurthermore, the authors do not provide a comparison of existing methods on concurrent benchmarks, nor do they highlight how leaderboard rankings might change when using their new benchmark."}, "questions": {"value": "I understand that previous frameworks have limitations regarding hallucinations and in validating the generated questions. Why not just fix the existing generation frameworks and their related datasets (i.e., by removing incorrect questions/answers)? Why is it necessary to propose the new GTSQA dataset and a new framework for generation?\nWhen existing methods are evaluated on the new benchmark, is there a significant change in the leaderboard compared to their rankings on established benchmarks?\nIn Lines 369-371, you mention that KG retrievers are usually trained on datasets that do not have ground-truth answer subgraphs, but in Lines 1556-1558, you mention that the concurrent works Dynamic-KGQA and KGQAGen do generate questions from ground-truth answer subgraphs. Are the datasets generated in such works not used for training? Why?\nAdditionally, the paper's objective to generate questions from a KG using an LLM should be stated explicitly in the abstract. The term \"generated\" is used, but it is not clear that this generation involves an LLM until Figure 1 and Line 81, and this is only made fully clear when concurrent works are introduced (Line 116)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xFA3FfGLDn", "forum": "Sp6znUhP1n", "replyto": "Sp6znUhP1n", "signatures": ["ICLR.cc/2026/Conference/Submission18793/Reviewer_F3Vj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18793/Reviewer_F3Vj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633124416, "cdate": 1761633124416, "tmdate": 1762999993718, "mdate": 1762999993718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a core problem in KG-RAG: the lack of ground-truth reasoning subgraphs in existing benchmarks, which forces models to train on flawed \"Shortest Path\" (SP) heuristics. The authors introduce `SynthKGQA`, a novel \"subgraph-to-question\" generation framework that uses an LLM to propose questions, answers, and reasoning paths, then programmatically validates them using SPARQL queries to ensure factual consistency. Using this framework, they create `GTSQA`, a new 32k-question benchmark designed to test zero-shot generalization by partitioning graph structures and relation types between train and test splits. The paper's key contribution is demonstrating that training models on `GTSQA`'s precise \"Ground-Truth\" (GT) subgraphs yields 5-20% higher end-to-end accuracy than training identical models on the traditional SP heuristic."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addresses a Core Problem with a Verifiable Solution:** The paper accurately identifies the central bottleneck in KG-RAG evaluation and training: the lack of ground-truth subgraphs. The proposed `SynthKGQA` framework provides a sophisticated and powerful solution. By using an \"LLM-propose + SPARQL-validate\" loop, it programmatically guarantees the factual consistency of the generated (question, SPARQL, answer, subgraph) tuples. Its 0.47% validation failure rate is far lower than alternative generation methods.\n2. **GTSQA: A Benchmark Designed for Generalization:** The `GTSQA` dataset is a significant contribution in itself. Instead of a simple random split, the authors have meticulously designed a **three-dimensional zero-shot generalization challenge** by deliberately partitioning answer nodes, relation types, and graph isomorphism types between the train and test sets. This design allows for the genuine evaluation of a retriever's structural and semantic generalization capabilities, rather than an LLM's memorization.\n3. **Definitive Proof of a Flawed Training Paradigm:** The paper's most impactful contribution lies in Section 6. It provides the first quantitative, undeniable evidence that (1) the \"Shortest Path\" (SP) signal, long used as a heuristic for training, is an extremely poor and misleading proxy (low overlap, high misdirection); and (2) **models trained using \"Ground-Truth\" subgraphs (provided by GTSQA) as supervision significantly outperform identical models trained on SPs** (by 5-20% EM Hits). This finding offers a new, superior direction for the KG-RAG training paradigm."}, "weaknesses": {"value": "1. **Severe \"Closed-Loop Evaluation\" and Synthetic-to-Real Generalization Gap**\n\nThis paper's most critical limitation is its \"closed-loop\" evaluation. While Section 6 effectively demonstrates a \"synthetic-to-synthetic\" gain (training on GTSQA improves performance on GTSQA), the paper completely lacks the most crucial experiment: demonstrating \"synthetic-to-real\" generalization. It fails to show if a model trained on GTSQA outperforms an SP-trained model on a real-world, human-created benchmark (e.g., WebQSP, CWQ). Without this proof, the practical utility of GTSQA as a training resource is unverified. Furthermore, the GTSQA question distribution (e.g., high enrichment of multi-hop, non-redundant questions) likely differs significantly from real-world user queries, questioning its external validity.\n\n2. **Systematic Biases from the Data Generation Pipeline**\n\nThe SynthKGQA framework (Fig 1) replaces human template bias with \"LLM generation bias.\" Its \"subgraph-to-question\" flow is the reverse of real-world user intent (\"question-to-subgraph\"), which likely introduces systematic biases in linguistic patterns and semantic focus. More critically, the pipeline imposes hard structural constraints (e.g., \"tree-shaped\" and \"$\\le$6 edges\"), which systematically excludes complex, real-world queries involving cycles, aggregation, or longer reasoning chains. This limits the evaluated generalization to \"intra-tree\" generalization. The test set's enforced non-redundancy and enrichment of tail relations further skews its distribution away from real-world scenarios.\n\n3. **\"Strawman\" Comparison Against the \"Shortest Path\" (SP) Baseline**\n\nThe paper's core argument in Section 6, that GT-supervision beats SP-supervision, constitutes a \"strawman\" argument. SP is a known, deeply flawed heuristic. The paper fails to demonstrate that its GT signal is superior to stronger, more modern heuristics (e.g., subgraphs derived from PPR, Steiner tree approximations, or simple agent-based exploration). Furthermore, the claim itself rests on a weak statistical foundation, with Table 3 reporting only the mean of three runs, lacking rigorous statistical significance analysis (e.g., variance, confidence intervals).\n\n4. **Ambiguous \"Ground-Truth\" Definition and Inconsistent Evaluation Scopes**\n\nThe paper's claim of providing \"Ground-Truth Subgraphs\" is ambiguous. The \"ground-truth\" $\\mathcal{G}$ is merely an LLM-proposed, sufficient subgraph (Step 2), not a verified minimal or optimal reasoning path (e.g., App A.3 only checks for redundant seeds, not redundant triples). Furthermore, the paper admits (App A.4) that the full_answer_subgraph (from the SPARQL CONSTRUCT) can be larger than the LLM-proposed G. The recommendation to use the former for evaluating retrievers but the latter for classifying complexity introduces inconsistent evaluation scopes and potential for misaligned optimization.\n\n5. **Limitations and Fairness Issues in Evaluation Setup**\n\nThe evaluation setup suffers from several limitations. First, all end-to-end (E2E) and ablation experiments rely on a single LLM (GPT-4o-mini) as the final reader, making the conclusions highly dependent on this specific model's behavior, with no robustness checks on other models (e.g., Llama, Mistral). Second, the paper (App C.1) correctly notes that many SOTA retrievers require \"question-specific graphs\" (k-hop + PPR pruning). This creates a new fairness issue: the reported baseline performance is now contingent on this specific (and potentially optimal) graph preprocessing setup. Finally, while the paper highlights high \"GT triple recall\" as a predictor for EM ($r=0.95$), it simultaneously shows that retrieval precision is universally poor (F1 < 30%). This indicates that simply training for recall on GT subgraphs does not solve the core RAG challenge of noise control and precision."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NZfxACyxFq", "forum": "Sp6znUhP1n", "replyto": "Sp6znUhP1n", "signatures": ["ICLR.cc/2026/Conference/Submission18793/Reviewer_vG9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18793/Reviewer_vG9L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913042477, "cdate": 1761913042477, "tmdate": 1762999993510, "mdate": 1762999993510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a core problem in KG-RAG: the lack of ground-truth reasoning subgraphs in existing benchmarks, which forces models to train on flawed \"Shortest Path\" (SP) heuristics. The authors introduce `SynthKGQA`, a novel \"subgraph-to-question\" generation framework that uses an LLM to propose questions, answers, and reasoning paths, then programmatically validates them using SPARQL queries to ensure factual consistency. Using this framework, they create `GTSQA`, a new 32k-question benchmark designed to test zero-shot generalization by partitioning graph structures and relation types between train and test splits. The paper's key contribution is demonstrating that training models on `GTSQA`'s precise \"Ground-Truth\" (GT) subgraphs yields 5-20% higher end-to-end accuracy than training identical models on the traditional SP heuristic."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Addresses a Core Problem with a Verifiable Solution:** The paper accurately identifies the central bottleneck in KG-RAG evaluation and training: the lack of ground-truth subgraphs. The proposed `SynthKGQA` framework provides a sophisticated and powerful solution. By using an \"LLM-propose + SPARQL-validate\" loop, it programmatically guarantees the factual consistency of the generated (question, SPARQL, answer, subgraph) tuples. Its 0.47% validation failure rate is far lower than alternative generation methods.\n2. **GTSQA: A Benchmark Designed for Generalization:** The `GTSQA` dataset is a significant contribution in itself. Instead of a simple random split, the authors have meticulously designed a **three-dimensional zero-shot generalization challenge** by deliberately partitioning answer nodes, relation types, and graph isomorphism types between the train and test sets. This design allows for the genuine evaluation of a retriever's structural and semantic generalization capabilities, rather than an LLM's memorization.\n3. **Definitive Proof of a Flawed Training Paradigm:** The paper's most impactful contribution lies in Section 6. It provides the first quantitative, undeniable evidence that (1) the \"Shortest Path\" (SP) signal, long used as a heuristic for training, is an extremely poor and misleading proxy (low overlap, high misdirection); and (2) **models trained using \"Ground-Truth\" subgraphs (provided by GTSQA) as supervision significantly outperform identical models trained on SPs** (by 5-20% EM Hits). This finding offers a new, superior direction for the KG-RAG training paradigm."}, "weaknesses": {"value": "1. **Severe \"Closed-Loop Evaluation\" and Synthetic-to-Real Generalization Gap**\n\nThis paper's most critical limitation is its \"closed-loop\" evaluation. While Section 6 effectively demonstrates a \"synthetic-to-synthetic\" gain (training on GTSQA improves performance on GTSQA), the paper completely lacks the most crucial experiment: demonstrating \"synthetic-to-real\" generalization. It fails to show if a model trained on GTSQA outperforms an SP-trained model on a real-world, human-created benchmark (e.g., WebQSP, CWQ). Without this proof, the practical utility of GTSQA as a training resource is unverified. Furthermore, the GTSQA question distribution (e.g., high enrichment of multi-hop, non-redundant questions) likely differs significantly from real-world user queries, questioning its external validity.\n\n2. **Systematic Biases from the Data Generation Pipeline**\n\nThe SynthKGQA framework (Fig 1) replaces human template bias with \"LLM generation bias.\" Its \"subgraph-to-question\" flow is the reverse of real-world user intent (\"question-to-subgraph\"), which likely introduces systematic biases in linguistic patterns and semantic focus. More critically, the pipeline imposes hard structural constraints (e.g., \"tree-shaped\" and \"$\\le$6 edges\"), which systematically excludes complex, real-world queries involving cycles, aggregation, or longer reasoning chains. This limits the evaluated generalization to \"intra-tree\" generalization. The test set's enforced non-redundancy and enrichment of tail relations further skews its distribution away from real-world scenarios.\n\n3. **\"Strawman\" Comparison Against the \"Shortest Path\" (SP) Baseline**\n\nThe paper's core argument in Section 6, that GT-supervision beats SP-supervision, constitutes a \"strawman\" argument. SP is a known, deeply flawed heuristic. The paper fails to demonstrate that its GT signal is superior to stronger, more modern heuristics (e.g., subgraphs derived from PPR, Steiner tree approximations, or simple agent-based exploration). Furthermore, the claim itself rests on a weak statistical foundation, with Table 3 reporting only the mean of three runs, lacking rigorous statistical significance analysis (e.g., variance, confidence intervals).\n\n4. **Ambiguous \"Ground-Truth\" Definition and Inconsistent Evaluation Scopes**\n\nThe paper's claim of providing \"Ground-Truth Subgraphs\" is ambiguous. The \"ground-truth\" $\\mathcal{G}$ is merely an LLM-proposed, sufficient subgraph (Step 2), not a verified minimal or optimal reasoning path (e.g., App A.3 only checks for redundant seeds, not redundant triples). Furthermore, the paper admits (App A.4) that the full_answer_subgraph (from the SPARQL CONSTRUCT) can be larger than the LLM-proposed G. The recommendation to use the former for evaluating retrievers but the latter for classifying complexity introduces inconsistent evaluation scopes and potential for misaligned optimization.\n\n5. **Limitations and Fairness Issues in Evaluation Setup**\n\nThe evaluation setup suffers from several limitations. First, all end-to-end (E2E) and ablation experiments rely on a single LLM (GPT-4o-mini) as the final reader, making the conclusions highly dependent on this specific model's behavior, with no robustness checks on other models (e.g., Llama, Mistral). Second, the paper (App C.1) correctly notes that many SOTA retrievers require \"question-specific graphs\" (k-hop + PPR pruning). This creates a new fairness issue: the reported baseline performance is now contingent on this specific (and potentially optimal) graph preprocessing setup. Finally, while the paper highlights high \"GT triple recall\" as a predictor for EM ($r=0.95$), it simultaneously shows that retrieval precision is universally poor (F1 < 30%). This indicates that simply training for recall on GT subgraphs does not solve the core RAG challenge of noise control and precision."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NZfxACyxFq", "forum": "Sp6znUhP1n", "replyto": "Sp6znUhP1n", "signatures": ["ICLR.cc/2026/Conference/Submission18793/Reviewer_vG9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18793/Reviewer_vG9L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913042477, "cdate": 1761913042477, "tmdate": 1763731799152, "mdate": 1763731799152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}