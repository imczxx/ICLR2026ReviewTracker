{"id": "km8rXu0RnF", "number": 9162, "cdate": 1758113575605, "mdate": 1759897740205, "content": {"title": "LOGIT: Learning to Generate Gradients for Federated Learning with Arbitrary Client Unavailability", "abstract": "Federated learning (FL) enables distributed clients to collaboratively train a shared model while keeping data local. However, in practice, unreliable transmissions, power constraints, and client mobility induce intermittent client unavailability, which biases gradient aggregation and impedes convergence. To address this issue, we propose LOGIT, a gradient-generation framework that learns client-specific gradient trajectories to reconstruct missing updates on the server when clients drop out. Specifically, LOGIT conditions a lightweight generator on each client’s gradient history and the current-round updates from available clients, producing surrogate gradients for unavailable clients and preserving statistical diversity across participants. We further derive a tighter convergence bound and show that LOGIT converges at a rate of $\\mathcal{O}(1/\\sqrt{T})$, where $T$ is the number of communication rounds. Our experimental results on public datasets validate the effectiveness of LOGIT, demonstrating consistent superiority over baselines, particularly in scenarios with high data heterogeneity and client unavailability.", "tldr": "", "keywords": ["Federated learning", "Gradient generation", "Client availability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/263166e5ad326928c7ceada5b565ceb3eddef898.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **LOGIT**, a gradient-generation framework for federated learning (FL) under arbitrary client unavailability. The method aims to reconstruct missing client gradients on the server by learning client-specific gradient trajectories conditioned on historical and current updates from available clients. The authors claim that LOGIT enables unbiased surrogate gradients for dropped clients, derives a tighter convergence bound with rate $\\mathcal{O}(1/\\sqrt{T})$, and empirically outperforms existing methods on public datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper trys to address an important and practical problem in federated learning, namely client unavailability. The idea of reconstructing unavailable gradients using a generator network represents an interesting attempt to improve learning continuity. The paper is also clearly written and well-organized, making the methodology easy to understand."}, "weaknesses": {"value": "The proposed methodology requires the server to collect gradients from individual clients, which makes it incompatible with secure aggregation and differential privacy mechanisms, as both rely on hiding individual client updates. Moreover, the approach assumes that each client’s gradient trajectory can be effectively learned, but in practice, gradients tend to diminish and fluctuate dynamically during training. It is unclear how the proposed GCN-based generator can reliably capture and predict such evolving gradient patterns.\n\nIn addition, the data available for training each client’s GCN is extremely limited. For example, in a conventional federated learning system with 100 clients, where 10 clients participate per round and the total number of rounds is 100, the server can only obtain about 10 gradient samples per client. Under such conditions, it is questionable whether the GCN can learn meaningful gradient dynamics or generalize well. This may also require significantly more communication rounds, further increasing the training cost.\n\nIn the main experiments, the number of clients is set to 10, which is too small to validate the effectiveness of the proposed algorithm. \n\nThe methodology also lacks scalability, as the complexity of training the GCN grows linearly with the number of clients, and the model size may need to increase with the dimensionality of the gradients. \n\nWith Assumption 3, the analysis becomes nearly identical to that of conventional FedAvg. \n\nOverall, the GCN component appears heuristic and insufficiently justified."}, "questions": {"value": "What's the reason for you to choose GCN as the generator?\n\nAccording to existing studies on FedAvg, this algorithm is already quite robust to partial client participation. Therefore, it is unclear whether it is truly necessary to recover the gradients of each client using such a heuristic methodology. \n\nGiven the limited number of communication rounds typically available in federated learning, it is doubtful that one can reliably identify the dynamic gradient patterns of individual clients. The feasibility of accurately modeling these trajectories under practical constraints remains highly questionable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5fNJj51BPP", "forum": "km8rXu0RnF", "replyto": "km8rXu0RnF", "signatures": ["ICLR.cc/2026/Conference/Submission9162/Reviewer_PCbU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9162/Reviewer_PCbU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633690094, "cdate": 1761633690094, "tmdate": 1762920842613, "mdate": 1762920842613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LOGIT, a framework to address client dropouts in Federated Learning (FL) by generating surrogate gradients. A server-side Gradient Generation Network (GGN) is trained for each client. When a client is unavailable, its GGN generates a substitute gradient based on its own gradient history and information from currently available clients."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of using a generative model to create surrogate gradients for missing clients is novel and provides a new perspective on tackling the client unavailability problem.\n* The paper provides a complete convergence analysis, establishing a rate of O(1/√T) and offering insights into how factors like participation probability and gradient staleness affect convergence.\n* The method is validated on three public datasets (CIFAR-10, CIFAR-100, IMAGENETTE), which helps demonstrate its effectiveness across different tasks."}, "weaknesses": {"value": "* There are minor typos in the paper, such as the repeated \"denote denote\" and the redundant sentence \"the cosine similarity. denotes the cosine similarity.\" on page 4.\n* The server is required to store the most recent gradient for each client. This could lead to prohibitive memory overhead in large-scale FL scenarios with many clients, as each stored gradient has the same size as the global model.\n* The main experiments are conducted with only N=10 clients, which may not be representative of large-scale FL. It is also unclear if the experiments assume all 10 clients are selected each round (full participation) before dropouts occur. The performance under a more common partial participation setting is not explored.\n* The client availability of p=0.5 in the main experiments is an extreme setting. Figure 3(a) shows that the performance difference between p=0.5 and p=0.9 is not substantial, which might suggest that the impact of client dropouts is not as significant as claimed, and the proposed method's benefits may be marginal in more realistic scenarios.\n* The baselines are limited to FedAvg and two methods based on reusing stale gradients (MIFA, WS). The paper should include comparisons with stronger baselines mentioned in the related work, such as FedProx or SCAFFOLD, which are designed to handle statistical heterogeneity and may inherently be more robust to biased aggregation from dropouts."}, "questions": {"value": "* According to Algorithm 1, the GGN is trained and used online from the very beginning. In the early rounds, the GGN is not well-trained and may generate poor-quality gradients, which could be detrimental to the main model's convergence. Have the authors investigated the effect of this \"cold start\" problem?\n* The convergence analysis seems to assume a bounded error for the GGN. Does this analysis fully account for the nested optimization process where the GGN is being trained online on a non-stationary target, as the client gradients change while the global model evolves?\n* Could the authors provide learning curves (accuracy vs. communication rounds) for the main experiments? This would offer a clearer view of the training dynamics and convergence speed compared to just reporting the final accuracy or rounds to target.\n*  Regarding the \"Client-specific coordinatewise GGN,\" the paper states the same small network is applied to every coordinate. Does this mean for each client n, there is one single small network Φ(·; θn) that processes each of the d gradient coordinates independently? Even if so, the server compute of O(Kd) could still be substantial for models with very high dimensionality (d). Is it practically feasible for very large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gz6T0MCz4Q", "forum": "km8rXu0RnF", "replyto": "km8rXu0RnF", "signatures": ["ICLR.cc/2026/Conference/Submission9162/Reviewer_1uDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9162/Reviewer_1uDW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847017271, "cdate": 1761847017271, "tmdate": 1762920842253, "mdate": 1762920842253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper processes LOGIT, a novel framework for FL that addresses client unavailability caused by communication failures or dropouts. LOGIT introduces a server-side Gradient Generation Network (GGN), which learns client-specific gradients from historical updates and current active clients to generate surrogate gradients for missing clients. This LOGIT shows a superior accuracy and communication efficiency across multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper addresses a very practical issue in FL, which is the possibility of broken communication between the server and clients that results in missing gradient updates in some rounds. By focusing on this intermittent connectivity problem, the paper tackles a common real-world challenge that can bias gradient aggregation and impede convergence in FL.\n2. The authors design a comprehensive set of ablation studies to evaluate LOGIT under various realistic conditions. In particular, they systematically vary the communication delay, the fraction of successful communications, and the number of participating clients. By examining these factors, the experiments convincingly demonstrate the effectiveness and robustness of LOGIT from multiple perspectives, highlighting that it can preserve model performance even when communication is delayed or sporadic."}, "weaknesses": {"value": "1. It is unclear how accurate the surrogate gradient generator in LOGIT is over the course of training. The paper would benefit from an analysis of whether the generator's outputs become more precise as training progresses. For instance, monitoring the average error between the actual gradients and the generated surrogate gradients would illustrate the generator's fidelity, Additionally, since the surrogate gradients are likely of lower quality in the eraly traiing phase, the authors could clarify if any techniques (e.g., a warm-up period or special training schedule) are used to mitigate the impact of initially inaccurate surrogate updates.\n2. The generator's design seems misaligned with the goal of accurate gradient prediction. According to Eq. 3, the loss function $L_{MSE}$ focuses the generator's output to be close to the input gradient. In other words, the current design encourages the generator to reproduce the most recent stable gradient rather than predicting the unseen new gradient. This raises concerns that LOGIT's generator might be merely outputting a slightly adjusted version of the previous gradients, instead of leveraging the client's gradient trajectory to forecast the missing update (as a sequential model would). Clarification on how the generator accounts for temporal patterns would strengthen the work.\n3. The paper does not clearly explain how $\\lambda$ is chosen in Eq. 3 to balance the two loss objectives. Is this $\\lambda$ fixed or tuned for each client, and does its optimal value depend on the degree of data heterogeneity across clients? Moreover, the formulation of the generator's loss as $L_{MSE} - L_{Align}$ is unusual, since subtracting $L_{Align}$ is equivalent to maximizing the alignment term. The authors should clarify the rationale behind subtracting this $L_{Align}$ rather than adding it. \n4. Several notational and presentation issues could be addressed to improve clarity. \n- L 223, the term \"client $n'$\" appears without definition.\n- L 195, the notation $g_n(x_t), k \\in  \\mathcal{A}_t  \\backslash  \\{n\\}$ is confusing, since $k$ is not defined in $g_n(x_t)$.\n- L 244, the summation is from $i=1$ to $i=I-1$ for $x_{t, i-1}^n$, whereas Algorithm 1 L8 suggests it should be $i=0$ to $i=I-1$ for $x_{t, i}^n$.\n- L 364, the set notation is missing braces.\n- It would be helpful to unify and clearly explain the notation for superscript and subscript."}, "questions": {"value": "Refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y0Waq1LZ3T", "forum": "km8rXu0RnF", "replyto": "km8rXu0RnF", "signatures": ["ICLR.cc/2026/Conference/Submission9162/Reviewer_rUQc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9162/Reviewer_rUQc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160715119, "cdate": 1762160715119, "tmdate": 1762920841913, "mdate": 1762920841913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for federated learning with temporarily unavailable clients.\nIn the proposed approach, the gradients that are missing from clients are reconstructed based on the previously observed gradients from this client and the gradients observed from other clients.\nA theoretical analysis is performed, showing a $O(1/\\sqrt{T})$ convergence rate, and scaling with the \"gradient staleness\" (i.e., maximal time between reception of consecutive gradients from a client).\nTo reduce the complexity of the proposed method, it is applied to the gradient coordiante-per-coordinate, avoiding the need to consider correlations between coordinates.\nExperimental results show the relevance of the method on multiple classical problems, demonstrating the the proposed method exhibits improved performance in comparison with other existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of generating missing gradients in cases where clients are unavailable is interesting and, to my knowledge, original.\n2. The proposed methods exhibits strong empirical results in the case where clients are unavailable following a Bernoulli distribution, outperforming other baselines while reducing the number of communications.\n3. Theoretical convergence guarantees are provided, proving that the method converges despite gradient staleness, although (as expected) the time between successive computation of the gradients slows down the training."}, "weaknesses": {"value": "1. Assumption 3, that the gradient computed by the proposed algorithm is unbiased, seems overly strong. In my understanding, this essentially assumes that the model used for generating gradient is unbiased, which is very likely not the case given the setting used for generation.\n2. While the coordinate-wise scheme for gradient generation significantly reduces the computational cost, it may create strong bias in the generated gradients, which may prevent convegence in complex scenarios.\n3. To estimate unavailable models, the server needs to store the previously received gradient for every client, which may have a prohibitive cost when the number of clients is large.\n4. Experiments consider a limited setting where clients participate following a fixed Bernoulli distribution: this is a restrictive framework, where clients have a fixed, pre-determined behaviour, and contrasts with the claim that the method work in very general settings."}, "questions": {"value": "1. Does Assumption 3 indeed mean that the gradient generation algorithm is unbiased? If so, is it possible to replace this assumption by a milder one, allowing to prove that the generation process itself is unbiased?\n2. It seems that the loss function proposed for generating the gradients in Eq. (3) depends on the iteration count $t$ through the sampled client set $\\mathcal{A}_t$: this seems contradictory with the fact that the loss $\\mathcal{L}_{GNN}$ does not depend on $t$: is this loss really the considered loss?\n3. In 3.3, authors claim that \"gradients are $\\ell_2$-normalized when fed to the GCN, while raw when aggregated in fedavg: what is the rational behind this? Doesn't it result in inconsistent gradient updates?\n\nThere are a few typos:\n- in 3.1: the variant of FedAvg that is presented seems to be using a single local step, while the opposite is claimed\n- in Eq. (3), $\\mathcal{L}_{GNN}$ seems to be $\\mathcal{L}_{GCN}$: if not, why call it \"GNN\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5papK8vbxT", "forum": "km8rXu0RnF", "replyto": "km8rXu0RnF", "signatures": ["ICLR.cc/2026/Conference/Submission9162/Reviewer_oTnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9162/Reviewer_oTnz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762366080154, "cdate": 1762366080154, "tmdate": 1762920841442, "mdate": 1762920841442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}