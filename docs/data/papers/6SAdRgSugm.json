{"id": "6SAdRgSugm", "number": 17964, "cdate": 1758282496676, "mdate": 1759897142362, "content": {"title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks. This substantial token decrease directly translates to a significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering a practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications.", "tldr": "LLMs using Chain-of-Thought (CoT) for everything is wasteful. We built AdaCoT, a smart system that teaches LLMs when to use CoT based on clear principles, saving compute and improving user experience without sacrificing performance on hard tasks.", "keywords": ["Adaptive Reasoning", "Chain-of-Thought", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a1c0f9d5350a5a75cc8c1b6e126f9eb2d6b89c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the significant computational overhead of CoT, which is often wastefully applied to simple queries that do not require complex reasoning. The authors propose AdaCoT, a framework that enables LLMs to adaptively decide whether to invoke CoT for a given query. The core idea is to frame this adaptive decision as a Pareto optimization problem, seeking to find an optimal balance between two competing objectives: maximizing model performance (accuracy) and minimizing reasoning cost (CoT usage). This trade-off is managed via PPO where the reward function includes penalty coefficients ($\\alpha_1$ for missing CoT, $\\alpha_2$ for overusing CoT). By varying these coefficients, the authors can train a spectrum of models that trace the Pareto frontier.\n\nA key technical challenge identified is \"decision boundary collapse,\" where the adaptive policy reverts to always (or never) using CoT when trained on domain-specific data (e.g., math problems). To solve this, the authors introduce Selective Loss Masking (SLM), a technique that preserves the adaptive decision-making capability. Experiments show that AdaCoT models can define a superior Pareto frontier compared to \"Full CoT\" or \"No CoT\" baselines, achieving, for instance, 62.8% average score with only a 53.3% CoT rate, approaching the 65.0% score of the \"Full CoT\" model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Framing the adaptive reasoning problem as a Pareto optimization task is an elegant and principled approach. It moves beyond simple heuristics and provides a formal framework for navigating the complex trade-off between performance and computational cost.\n\n- The results are compelling from an efficiency standpoint. The framework demonstrates a clear ability to substantially reduce CoT usage while maintaining high performance. The production traffic results (Table 2), showing CoT rates as low as 3.18% and token reductions of 69%, are particularly strong evidence of the method's practical utility."}, "weaknesses": {"value": "- The core of the method relies on a binary \"CoT vs. No CoT\" decision. This is a brutal simplification of the reasoning problem. The true spectrum of reasoning is continuous: simple queries need 0 steps, moderate queries need a few steps, and complex queries need many steps. The proposed framework cannot distinguish between a 3-step reasoning chain and a 30-step one; it only decides whether to generate anything in the <think> tags. This binary design is a fundamental flaw that the rest of the method must compensate for. The authors even acknowledge this limitation in Section 4.1.\n\n- A direct consequence of the flawed binary design is the method's reliance on manually tuning the $\\alpha_1$ and $\\alpha_2$ penalty coefficients to trace the Pareto frontier. This is not a \"learned\" trade-off but a brute-force sweep. This makes the method impractical for real-world deployment. How would a practitioner set these $\\alpha$ values for a new task or data distribution without re-running this entire, expensive experimental sweep? A more sophisticated design would ideally learn the optimal reasoning length, not require manual tuning of arbitrary penalties.\n\n- SLM as a Symptomatic Fix (a \"Patch\"): The \"decision boundary collapse\" problem and the SLM solution feel like an engineering patch for a problem created by the initial design. The model should collapse to a 100% CoT rate on a math dataset, as this is the optimal policy for that data. The SLM mechanism essentially prevents the model from learning the correct policy for that domain in order to preserve the \"adaptive\" behavior from a previous training stage. This suggests the multi-stage RL pipeline and binary reward are not robust, requiring this extra mechanism to hold them together.\n\n- **Crucial Missing Experimental Baselines**: This is the most significant weakness of the paper. The paper claims to present a \"Pareto-Optimal\" solution but fails to compare it against any other competing reasoning efficiency methods. The related work (Section 5) correctly identifies other SOTA approaches (e.g., pruning, summarization, RL with brevity rewards). However, the experiments only compare AdaCoT against its own non-adaptive (Full CoT, No CoT) and non-RL (Adaptive SFT) ablations. The Pareto frontier plot in Figure 1 is misleading because it's missing the most important data points: where would other methods from the literature fall on this graph? Without this comparison, it is impossible to know if AdaCoT is truly \"Pareto-optimal\" or if it is dominated by other, simpler efficiency techniques. The claim of \"pioneering adaptive triggering\" (Abstract) is too strong when the work is not benchmarked against its peers.\n\n- The title is incorrect as default template title."}, "questions": {"value": "- Is SLM truly necessary if the reward structure was more robust? What happens if you simply remove the $P_{over}$ penalty during the RL-Math stage? Wouldn't the model learn to use CoT (as required by the task) and then re-learn to be frugal when $P_{over}$ is re-introduced in the RL-General stage? SLM feels like it's fighting the RL agent's correct adaptation.\n- How sensitive is the framework to the initial SFT data labeling (e.g., the 67% CoT-required ratio)? To deploy AdaCoT in a new, specialized domain (e.g., legal, medical), would a user need to re-create the entire SFT dataset and re-run the full 2-stage RL pipeline, including the expensive $\\alpha$-coefficient sweep?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eoVZ15l9Dq", "forum": "6SAdRgSugm", "replyto": "6SAdRgSugm", "signatures": ["ICLR.cc/2026/Conference/Submission17964/Reviewer_moGN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17964/Reviewer_moGN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463791547, "cdate": 1761463791547, "tmdate": 1762927759984, "mdate": 1762927759984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AdaCoT which addresses the inefficiency of always using chain-of-thought (CoT) reasoning by enabling a model to adaptively decide when to invoke CoT. The paper frames this as a Pareto optimization problem balancing accuracy and cost, and addresses it using RL with rewards weighted by different coefficients. AdaCoT achieves significant token savings without substantially sacrificing performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Addresses a Practical Efficiency Problem. AdaCoT directly tackles selective reasoning, reducing unnecessary CoT usage, which is highly relevant for large-scale or latency-sensitive applications.\n\n2.  Selective Loss Masking (SLM) Innovation. Introduces SLM to stabilize RL training, preventing the trigger policy from collapsing to always CoT or always skip CoT. Ensures robust learning of the binary decision.\n\n3.  Empirical Validation. Tested on both real-world production traffic and reasoning benchmarks."}, "weaknesses": {"value": "1. The \"Pareto optimization problem\" is realized only through linear scalarization rather than by using an explicit algorithm to compute the Pareto front.\n\n2. The label is derived from manually designed principles and relies on the capabilities identified by the internal LLM."}, "questions": {"value": "1. For lines 233-235, could you clarify how the CoT necessity label is manually determined?\n\n2. When exploring the 1D Pareto front, equation (5) involves two free parameters, $\\alpha_1$ and $\\alpha_2$. How should these two parameters be chosen to effectively cover the entire 1D Pareto front?\n\n3. The authors argue that fully autonomous CoT trigger learning suffers from difficulty in measuring the counterfactual benefits of reasoning. Could this issue be mitigated by running the model twice: once with CoT and once without, and calculate the performance difference. That would double the running times of model, however it increases the reward accuracy. \n\n4. Minor issue: the paper title is not updated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VuAzE7kfum", "forum": "6SAdRgSugm", "replyto": "6SAdRgSugm", "signatures": ["ICLR.cc/2026/Conference/Submission17964/Reviewer_Pnuk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17964/Reviewer_Pnuk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709640207, "cdate": 1761709640207, "tmdate": 1762927759339, "mdate": 1762927759339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the practical question of when to trigger Chain-of-Thought (CoT) at inference time to balance quality and cost. It formulates the problem as Pareto optimization, maximizing task accuracy while minimizing the CoT triggering rate, and optimizes a linearly-scalarized objective with PPO, using asymmetric penalties for under-using and over-using CoT. To prevent decision-boundary collapse during multi-stage RL, the authors propose Selective Loss Masking (SLM) that masks the loss on the “decision token” right after `<think>` in fragile stages, preserving a healthy triggering distribution. Experiments across many public benchmarks and production traffic show strong accuracy–cost trade-offs (clear Pareto curves), significant token savings in the wild, and no loss of peak accuracy under an “always reason” setting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This work focuses on the practical and important issue of adaptive Chain-of-Thought (CoT) triggering under tight efficiency constraints, where current systems tend to over-use CoT (wasting tokens/latency) or under-use it (hurting accuracy), and multi-stage RL can collapse the decision boundary to “always/never reason,” skewing usage and degrading robustness.\n\nThis work proposes a tightly coupled framework for Pareto-optimal adaptive CoT, combining a scalarized performance–cost objective with asymmetric penalties (to regulate under/over reasoning), PPO-based policy learning (to sweep the frontier), and Selective Loss Masking (SLM) (to stabilize multi-stage training).\n\nThis work conducts multiple sets of experiments to verify the effectiveness and extensibility of the proposed method."}, "weaknesses": {"value": "(i) The contribution of this work reads primarily as a careful systems integration tailored to CoT triggering rather than a new theoretical principle, and the separation from adjacent paradigms, such as selector/gating approaches, early-exit/anytime inference, or logit/entropy-based stabilizers, remains somewhat blurred, making it hard to pinpoint what portion of the gains stems from genuinely new ideas.\n\n(ii) The ablation does not include a systematic sensitivity analysis of key hyperparameters that govern the accuracy–cost trade-off and training stability (e.g., scalarization weights, penalty coefficients, and masking schedule). As a result, the robustness of the reported gains and the controllability of the Pareto frontier across seeds, datasets, and domains remain unclear, and it is difficult to disentangle performance attributable to the method from potential tuning artifacts.\n\n(iii) In Related Work, this paper does not analyze in detail how the method addresses concrete failure modes of existing approaches (e.g., over-/under-triggering of CoT or decision-boundary collapse), nor does it map individual components (Pareto scalarization, asymmetric penalties, SLM) to these gaps.  This obscures the source of improvements and blurs the conceptual distinction from adjacent lines such as selector/gating, early-exit/anytime inference, and reasoning-length compression.\n\n(iv) Has some loopholes, e.g.,\n\n  - Format error, e.g., The title of this article uses the sample provided by the template and has not been modified to the title of this article.\n\nPlease correct the grammatical mistakes and polish them if possible."}, "questions": {"value": "Please see 'weakness', which simply can be summarised as:\n\n(i) Please articulate the core conceptual novelty beyond linear scalarization with PPO and the SLM masking trick, and clarify which portion of the gains stems from genuinely new ideas.\n\n(ii) Please provide a systematic sensitivity analysis of the key hyperparameters that control the accuracy–cost trade-off and training stability (e.g., ($\\lambda_P$, $\\lambda_T$, $\\alpha_1$, $\\alpha_2$) and the SLM masking schedule), including variance across random seeds/datasets and the resulting changes to the Pareto frontier.\n\n(iii) Please expand the Related Work (or an analysis section) to explicitly map each component—Pareto scalarization, asymmetric penalties, and SLM—to concrete failure modes in prior approaches (e.g., over-/under-triggering of CoT, decision-boundary collapse), and please substantiate these links with targeted evidence or case studies.\n\n(iv) Please fix the presentation/formatting issues and please polish the writing: update any placeholder elements (e.g., the template title)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "I93p72ns9W", "forum": "6SAdRgSugm", "replyto": "6SAdRgSugm", "signatures": ["ICLR.cc/2026/Conference/Submission17964/Reviewer_en53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17964/Reviewer_en53"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829070863, "cdate": 1761829070863, "tmdate": 1762927758849, "mdate": 1762927758849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes adaptive chain-of-thought (CoT), in which large language models (LLMs) adaptively decide whether to invoke CoT reasoning. The intuition is that CoT is often time- and resource-consuming, and that it is often not needed for simple queries. AdaCoT is formulated as a Pareto optimization problem of maximizing accuracy while minimizing computational overhead, and the paper proposes to use proximal policy optimization (PPO) to allow LLMs to decide whether to invoke CoT on a given query. In training the PPO algorithm, the paper employs selective loss masking to ensure robustness, and experiments demonstrate that the learned PPO algorithm can successfully reduce the number of CoT invocations without degrading performance on complex tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper proposes an interesting idea that makes intuitive sense. The experiments (Figure 1) also seem to validate that a CoT triggering rate of around 60% achieves essentially the same average score as always invoking CoT.\n\n+ The experiments include evaluation on fifteen different benchmark datasets, although Table 4 reporting the results on these datasets is borderline unreadable due to the small font used. AdaCoT has substantially lower triggering rates but comparable performance to full CoT, with better scores than reinforcement learning baselines that do not use CoT."}, "weaknesses": {"value": "--AdaCoT’s training process requires labels of whether to use CoT on a set of queries, which the paper proposes to obtain from from another model. Why not use this model as the discriminator for whether to invoke CoT? If invoking this model is too expensive, why not use a form of knowledge distillation to learn a smaller discriminator model instead of reinforcement learning?\n\n--It’s not clear why reinforcement learning is used to decide whether to invoke CoT. Reinforcement learning is typically useful when there are meaningful transition dynamics or reward coupling between states, but in this case the transitions seem to be determined by the incoming queries, and there is no reward coupling. Moreover, the reinforcement learning framework is under-specified. What are the actions and rewards? How often is the selective loss masking used; is it used in every training iteration (in which case, why have the decision token loss at all)?\n\n--The paper does not include a comparison with models that aim to optimize reasoning length, even though such models would also likely reduce inference time and computational overhead. Wouldn’t a reasoning length of 0 in such models be equivalent to not using CoT, and thus wouldn’t such models be a generalization of the proposed AdaCoT framework?"}, "questions": {"value": "1) In the first sentence of Section 5.1, two “internal…mixture-of-experts” models are introduced as the “base model”s. Are these the LLMs using CoT, or the models generating the CoT labels? What does “internal” mean?\n\n2) Figure 1 shows a fairly small range of CoT invocation rates. Thus, it’s not clear that the proposed weighting of different reinforcement learning rewards will really allow AdaCoT to traverse the Pareto front.\n\n3) Figure 2 is discussed in the first paragraph of Section 4.2, but I could not find a second figure anywhere in the paper’s main body or appendix.\n\n4) This is not really a question, but the paper title is not included in the draft (which has the title “Formatting Instructions for ICLR 2026 Conference Submissions”).\n\nPlease see also “Weaknesses” above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NNKeOnUWod", "forum": "6SAdRgSugm", "replyto": "6SAdRgSugm", "signatures": ["ICLR.cc/2026/Conference/Submission17964/Reviewer_xvjU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17964/Reviewer_xvjU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17964/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146821578, "cdate": 1762146821578, "tmdate": 1762927758320, "mdate": 1762927758320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}