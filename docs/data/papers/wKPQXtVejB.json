{"id": "wKPQXtVejB", "number": 19653, "cdate": 1758297996717, "mdate": 1759897027942, "content": {"title": "Explainable Evidential Clustering", "abstract": "Unsupervised classification is a core problem in machine learning. Because real-world data are often imperfect, non-additive frameworks, such as evidential clustering, grounded in Dempster-Shafer theory, explicitly handle uncertainty and imprecision. These frameworks are particularly well suited to high-stakes decisions, which tend to require both interpretability and cautiousness. However, while decision-tree surrogates have enabled transparent explanations for hard clustering, explainability for evidential clustering remains largely unexplored. We address this gap by formalizing representativeness, a utility-based criterion that captures decision-makers' preferences over explanation misassignments, and introducing evidential mistakeness, a loss function tailored to credal partitions. Building on these foundations, we propose the Iterative Evidential Mistakeness Minimization (IEMM) algorithm, which learns decision-tree explainers for evidential clustering by optimizing representativeness under uncertainty and imprecision. We provide theoretical conditions for effective explanations in both hard and evidential settings and show how utility parameters can be set to reflect different decision attitudes. Experiments on synthetic and real-world datasets demonstrate that IEMM improves the performance of existing methods by producing representative and preference-aligned explanations of evidential clusterings, supporting cautious, transparent analysis in the presence of imperfect data.", "tldr": "We developed a framework for understanding cautious explanations and their desirable properties; We proposed an interpretable and cautious algorithm for explaining evidential clustering.", "keywords": ["Explainability", "Cautiousness", "Unsupervised Classification", "Evidential Clustering", "Decision Trees", "Interpretable AI", "XAI"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06d7085e16f33bf1b9da44b4fe3057d639cd0de3.pdf", "supplementary_material": "/attachment/4e5ccf7da4a59c64df20b16ca483bdbaae4a5df2.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Explainable Evidential Clustering, a framework that extends decision-tree‚Äìbased explanations to clustering models that account for uncertainty. The authors formalise two new theoretical constructs: Evidential Representativeness, a utility-based criterion that quantifies how well an explanation reflects decision-maker preferences, and Evidential Mistakeness, a loss function capturing representativeness errors in evidential settings. The paper provides theoretical analysis, algorithmic formulation, and experiments on small synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an underexplored problem on explainability for evidential clustering, where model outputs include uncertainty and imprecision. Its main originality lies in adapting the decision-tree explainer paradigm to this evidential setting and introducing formal definitions. \n\nThe methodological quality is solid in its theoretical formulation, with proofs, definitions, and algorithmic description. The paper is well written and structured.\n\nIn terms of significance, the paper‚Äôs contribution is relevant for research on explainability under uncertainty and could serve as a useful foundation for future work in evidential learning. However, its empirical and comparative evaluation is limited, relying on small datasets and a single older baseline, which restricts its practical and experimental impact. Overall, the work is conceptually original and clearly presented, but its experimental and empirical depth remains a key weakness."}, "weaknesses": {"value": "The paper‚Äôs main limitation lies in its narrow and weak empirical validation. All experiments are conducted on small, classical datasets (e.g., Iris, Wine, Diabetes, and simple credal datasets), which do not convincingly demonstrate scalability or generalizability. The evaluation includes only one baseline (IMM, 2020), making it difficult to assess whether the proposed method actually advances the state of the art. \n\nThe methodological novelty is also limited since IEMM is largely an incremental adaptation of IMM to an evidential setting. \n\nIn terms of clarity and framing, while the theoretical exposition is rigorous, the paper occasionally leans too heavily on formalism. It does not provide concrete real-world examples where evidential clustering explanations would offer tangible benefits over simpler alternatives. The related work section is not critical or up-to-date, omitting recent methods for explanations for clustering models."}, "questions": {"value": "1. The related work overview lists some studies but does not critically analyse methodological differences or advances since 2020. Could the authors expand the discussion to include more recent developments (2021‚Äì2025) in explainable clustering and evidential learning?\n2. Why is the experimental comparison limited to IMM (2020)? Have you considered including other, more recent interpretable clustering baselines?\n3. The paper motivates cautiousness in high-stakes domains (e.g., healthcare), but the evaluation does not include such real-world data. Can the authors provide a realistic use case demonstrating IEMM‚Äôs interpretability or decision support value?\n4. The paper evaluates explanation quality only through the proposed metrics, which measures fidelity to the evidential clustering under a chosen utility. It is not clear why no human-centred or standard explanation quality measures were included. How can one ensure that higher representativeness actually corresponds to more understandable or useful explanations for end users?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OLXI0u0pDV", "forum": "wKPQXtVejB", "replyto": "wKPQXtVejB", "signatures": ["ICLR.cc/2026/Conference/Submission19653/Reviewer_3JY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19653/Reviewer_3JY5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761331049542, "cdate": 1761331049542, "tmdate": 1762931504193, "mdate": 1762931504193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to generate interpretable, decision-tree-based explanations for evidential clustering. This is a clustering paradigm that models uncertainty and imprecision using Dempster-Shafer theory. The authors introduce new utility-based notions of representativeness and evidential mistakeness, and design the Iterative Evidential Mistakeness Minimization (IEMM) algorithm to greedily optimize decision trees for explaining credal (i.e., evidential) partitions. Theoretical analysis generalizes previous results for hard clustering to the evidential setting and provides utility-based frameworks reflecting different decision-maker attitudes towards caution and error. Experimental validation on synthetic and real-world datasets demonstrates that IEMM delivers more utility-aligned, representative explanations than adapted baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles the largely unexplored challenge of explainability for evidential clustering, a problem that is of interest in high-stakes and risk-averse domains.\n\n2. It reconstructs IMM‚Äôs logic for hard cluster explainers, formalizes representativeness in the evidential regime, and proves that minimizing evidential mistakeness yields maximal representativeness under a stakeholder utility.\n\n3. The use of a stakeholder-specific utility to mediate caution versus specificity is conceptually and practically meaningful."}, "weaknesses": {"value": "1. The DSClustering paper (Hovhannisyan, 2025) recently proposed a system that also leverages Dempster‚ÄìShafer theory to generate interpretable, rule-based cluster descriptions and to communicate uncertainty to end users. Given this development, the authors of the current submission risk slightly overstating the claim that ‚Äúno one has addressed interpretability in evidential clustering.‚Äù While their approach remains distinct, the paper should avoid implying exclusivity in combining Dempster‚ÄìShafer reasoning with interpretability. Instead, the authors should explicitly acknowledge DSClustering as a concurrent but methodologically different effort, emphasizing that their work addresses the post-hoc explanation of existing evidential clusterings.\n\n2. The experiments rely only on small, low-dimensional tabular datasets (Iris, Wine, Diabetes). The absence of high-dimensional, noisy, or real-world examples (e.g., medical or industrial data) limits the demonstration of scalability and generality. Similarly, the baseline (IMM with collapsed labels) cannot express ambiguous metaclusters, making IEMM‚Äôs superiority somewhat tautological under the chosen utility. Including stronger baselines, such as CART-style trees trained to maximize expected utility directly, or rule-based interpretable evidential clustering methods, would provide a more compelling comparison.\n\n3. The introduction of a stakeholder-specific utility function U(A,B) is conceptually strong and central to the paper‚Äôs stakeholder-aligned framing. However, the paper does not specify how this utility is to be elicited, parameterized, or grounded in user preferences. In practice, different stakeholders, such as clinicians, regulators, or engineers, may have distinct preferences and tolerance levels for ambiguity or misclassification.\n\n4. The exposition in Sections 3‚Äì4 is dense and symbol-heavy, which obscures the paper‚Äôs otherwise logically theoretical structure. Definitions of representativeness and evidential mistakeness are presented abstractly before any motivating examples, forcing readers to decode complex notation (mass functions, focal sets, utilities) without an intuitive grounding. This could be improved by introducing a running toy example early in the section, visually showing how cautious explanations evolve with different parameters before formal derivations. The paper is overall good and will consider improving the score after clarifications in the rebuttal.\n\nReference:\n\nInterpretable Clustering Using Dempster‚ÄìShafer Theory, Hovhannisyan 2025."}, "questions": {"value": "1. How large can the focal set family |F| become in practice, and do you prune focal sets before running IEMM?\n\n2. Are shallow axis-aligned trees essential for your theoretical guarantees, or could oblique or deeper trees achieve higher representativeness without sacrificing interpretability?\n\n3. How do you envision practical elicitation of the stakeholder utility  ùëà(ùê¥, ùêµ)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGNL5plqJP", "forum": "wKPQXtVejB", "replyto": "wKPQXtVejB", "signatures": ["ICLR.cc/2026/Conference/Submission19653/Reviewer_Re2y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19653/Reviewer_Re2y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567029856, "cdate": 1761567029856, "tmdate": 1762931503679, "mdate": 1762931503679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an approach for explaining evidential clustering by training a surrogate decision tree whose leaves are labeled with the \nfocal elements of the credal partition. To construct the decision tree, the IMM algorithm used for explaining centroid-based methods is \nappropriately extended and the Iterative Evidential Mistakeness Minimization (IEMM) method is proposed and evaluated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. It seems to be the first approach to explain evidential clustering.\nS2. Several novel notions and definitions are included (e.g. cautious explainer)\nS3. The proposed IMM extension is well-formulated."}, "weaknesses": {"value": "W1. Evidential clustering has not been widely accepted, especially in real applications \n(compared for example to fuzzy or probabilistic clustering methods). Hence an explainer specialized to that framework has limited reach.\nW2. If the number of focal elements is large, it seems difficult to interpret the results."}, "questions": {"value": "Q1. Are there approaches that build decision trees to explain fuzzy or probabilistic clustering solutions \ntaking into account uncertainty in cluster membership? If yes, they should be included in the comparison.  \nQ2. It would be nice to present the trees obtained for the real datasets considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XqvGS6vw3q", "forum": "wKPQXtVejB", "replyto": "wKPQXtVejB", "signatures": ["ICLR.cc/2026/Conference/Submission19653/Reviewer_uywB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19653/Reviewer_uywB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650051385, "cdate": 1761650051385, "tmdate": 1762931503276, "mdate": 1762931503276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Explainable clustering is a problem that has received considerable theoretical attention in recent years. The setup is the following: Given a dataset and its (hard) cluster assignments, can we find an axis-aligned partition of the data that has competitive performance to the original clustering. This line work started with the IMM algorithm (Moshkovitz et al, 2020), and there has been multiple extensions of the problem setting and methods, but the main focus of this line of work has been on the theoretical guarantees (the so-called price of explainability, and its variations).\n\nThe present paper generalises the problem and IMM algorithms to the setting of evidential clustering. In evidential clustering, each data instance is mapped to a probability mass function over the power set of all clusters. This generalises hard clustering, Bayesian clustering (where only singleton clusters have non-zero probabilities), and categorical clustering (where each data is mapped to a single subset of clusters), as illustrated in Figure 5.\n\nIn explainable evidential clustering (as posed in the present work), one is given a data set and corresponding mass functions and the goal is to find axis-aligned decision tree based partition of the data such that each data is mapped to a subset of clusters. The authors propose a generalised notion of mistakes, using the mass functions, and extend the IMM algorithm of Moshkovitz et al to obtain an IEMM algorithm, which they empirically evaluate on Gaussian mixtures and few small UCI datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The evidential clustering framework is quite interesting, which I believe already provides more interpretability than hard clustering (since the mass function encodes relevant uncertainty in the clusters). The problem of fitting axis-aligned decision trees provides an additional layer of explainability to the problem. \n\nThe problem of explainable evidential clustering is also mathematically interesting (at least to those working on explainable clustering) because the criteria from clustering changes from k-means/k-median cost to a more complex setting, where the uncertainty information/mass functions are available."}, "weaknesses": {"value": "The main drawback of the paper is that the problem is not well-formulated. A few important missing pieces are noted below\n- The paper proposes IEMM as an approach for explainable evidential clustering without precisely stating the problem (that is, what do we want to minimise while ensuring explainability). One can contrast this with IMM and the corresponding line of work, where the explainable k-means clustering is posed as a problem of achieving low k-means cost while ensuring explainability. As a result IEMM is a heuristic that does not precisely minimise a well-defined loss. This is of the same flavour as hierarchical clustering heuristics, which did not have any sound theoretical basis until Dasgupta's seminal works in 2010s.\n- A natural consequence of above is that there are no theoretical guarantees for the proposed IEMM algorithm, which is in contrast to all existing works on explainable clustering that I am aware of. The appendix includes some theoretical results on representativeness, but they do not provide any formal guarantees on the performance of IEMM.\n- More fundamentally, the IEMM takes mass functions as input for each data point, but returns only a subset of clusters and not a mass function. Hence, there is loss of information after one approximates the evidential clusters for decision trees (in other words, this method does not seem relevant for explainable Bayesian clustering).\n- The experiments are too limited and focus only on simple data, where explainability does not have any consequence. I would be fine with limited experiments if there was strong theoretical contribution. In particular, the explainable clustering literature has been primarily of theoretical interest (with limited use in practice). However, without a strong theory, the paper needs to demonstrate the practical impact of such an approach, where explaining clusters with axis-aligned trees would matter.\n- The overall presentation is quite poor, and not written for a general machine learning audience. A large number of concepts are introduced, but not used much in the paper. This makes the paper quite hard to read. I believe the presentation can be significantly simplified by making it more direct. For example, I feel the notion of representativeness is presented in a quite complicated way, making it too formal to even understand how it is useful. The same can be said about the setup, where one can get a better idea only after reading the appendix."}, "questions": {"value": "There are no specific questions. This paper needs to be significantly reworked and rewritten. Please see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ODg8ibsQzT", "forum": "wKPQXtVejB", "replyto": "wKPQXtVejB", "signatures": ["ICLR.cc/2026/Conference/Submission19653/Reviewer_q3cB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19653/Reviewer_q3cB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19653/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656314194, "cdate": 1761656314194, "tmdate": 1762931502894, "mdate": 1762931502894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}