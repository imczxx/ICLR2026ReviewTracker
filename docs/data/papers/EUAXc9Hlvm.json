{"id": "EUAXc9Hlvm", "number": 13037, "cdate": 1758212945362, "mdate": 1759897469621, "content": {"title": "Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning", "abstract": "Recent time-series foundation models exhibit strong abilities to predict physical systems. These abilities include zero-shot forecasting, in which a model forecasts future states of a system given only a short trajectory as context, without knowledge of the underlying physics. Here, we show that foundation models often forecast through a simple parroting strategy, and when they are not parroting they exhibit some shared failure modes such as converging to the mean. As a result, a naive context parroting model that copies directly from the context scores higher than leading time-series foundation models on predicting a diverse range of dynamical systems, including low-dimensional chaos, turbulence, coupled oscillators, and electrocardiograms---and at a tiny fraction of the computational cost. We draw a parallel between context parroting and induction heads, which explains recent works showing that large language models can often be repurposed for time series forecasting. Our dynamical systems perspective also ties the scaling between forecast accuracy and context length to the fractal dimension of the underlying chaotic attractor, providing insight into previously observed in-context neural scaling laws. By revealing the performance gaps and failure modes of current time-series foundation models, context parroting can guide the design of future foundation models and help identify in-context learning strategies beyond parroting.", "tldr": "Time series foundation models can be beaten by simple parroting strategies in forecasting dynamical systems", "keywords": ["time series", "foundation models", "dynamical systems", "forecasting", "chaos", "physics", "scientific machine learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7081dbd63cfdc37fa10c91a6835c677c9d92a63.pdf", "supplementary_material": "/attachment/9241b369b23f29749af7c32f880eb3b34b56a3b2.zip"}, "replies": [{"content": {"summary": {"value": "# Review for Context Parroting\n\n## Summary\n- This paper presents parroting as a baseline for foundation models in scientific ML. The authors demonstrate that a simple algorithm outperforms foundation models in zero-shot forecasting of chaotic dynamical systems. They perform several analysis, comparing short- and long-time forecasts and motivate the power scaling law in LLMs with respect to one-step error and context length. While the technical contribution is simple, it does demonstrate as a useful baseline for comparing foundation models on forecasting chaotic systems.\n\n## Recommendation\n- Accept given the below feedback and questions are addressed. Particularly, provide a clean repository of code for reproducability (should not be too much effort) and add more comparisons (more models like Moirai and/or LLMTime as well as the classical methods described in the paper, and maybe even comparing sampling strategies)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n- The necessary dynamics backgrounds are explained well and sufficient detail (eg. Lyapunov time)\n- The parroting method is clearly explained.\n- The computational experiment for comparing models is explained clearly.\n- The evaluation metrics sMAPE and KL divergence are clearly explained.\n- The results are compelling and clearly laid out."}, "weaknesses": {"value": "## Weaknesses\n- The reproducability is lacking. There should be a GitHub page with code that is easily run to recreate the figures and experiments.\n- The authors mention classical forecasting methods like Simplex projection and S-map forecasts and connect it to context parroting, however, these classical methods are not used in any comparisons."}, "questions": {"value": "## Questions\n- Can you clarify the difference between fractal dimension, correlation dimension, and scaling coefficient? In the paper it's not clear why fractal dimension and correlation dimension are used separately when they appear to mean the same thing, this makes it slightly confusing.\n- I'm curious how different sampling strategies affect LLM results (eg. beam search, top-p, top-k, etc..). Please clarify if sampling strategy will affect results and how so.\n- I would like to see how off-the-shelf LLMs like Llama perform on this task too (see Gruver (2023)) [[link]](https://proceedings.neurips.cc/paper_files/paper/2023/file/3eb7ca52e8207697361b2c0fb3926511-Paper-Conference.pdf). Was this considered?\n- Classical methods are mentioned and a connection is drawn to parroting, why not just implement them and compare the results? How do those methods compare with parroting?\n- Why not also test multivariate foundation models like Moirai?\n\n## Feedback\n- Elaborate on the parenthetical 'can we estimate the \"fractal dimension\" of a language' and the potential applications there (line 099). Consider moving this to the conclusion (section 6).\n- Provide a direct citation of Takens':\n```\n@inproceedings{takens2006detecting,\n  title={Detecting strange attractors in turbulence},\n  author={Takens, Floris},\n  booktitle={Dynamical Systems and Turbulence, Warwick 1980: proceedings of a symposium held at the University of Warwick 1979/80},\n  pages={366--381},\n  year={2006},\n  organization={Springer}\n}\n```\n\n- Cite the Chronos Time-MoE and TimesFM models in \"Models.\" (line 196)\n- Axes of figure 2 right need to be explained more. What do the widths mean? (line 275)\n- Elaborate more on why Chronos has better correlation dimension that parroting.\n- Line 383, \"This is equivalent to embed\" to \"this is equivalent to embedding\"\n- Figure 4, line 371, \"because the use of the\" -> \"because of the\"\n- Line 379, \"Why do...?\", remove this sentence. Looks unprofessional, just state the answer to the question.\n- Please please please provide code to the broader community! None of this is proprietary and there should be a GitHub."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Swk6Msf6T", "forum": "EUAXc9Hlvm", "replyto": "EUAXc9Hlvm", "signatures": ["ICLR.cc/2026/Conference/Submission13037/Reviewer_rUtK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13037/Reviewer_rUtK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760470262517, "cdate": 1760470262517, "tmdate": 1762923771739, "mdate": 1762923771739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates context parroting, a simple “copying” strategy that mimics motifs from past context, as a surprisingly strong baseline for zero-shot forecasting in scientific machine learning (SciML). The authors show that this trivial method can outperform several state-of-the-art time-series foundation models (Chronos, Chronos-Bolt, TimesFM, Time-MoE) on forecasting chaotic and physical systems. They connect this behavior to induction heads in transformers and to in-context neural scaling laws, deriving a geometric explanation that ties scaling exponents to the fractal dimension of chaotic attractors. The work suggests that many current foundation models rely heavily on parroting-like mechanisms, calling for benchmarks and architectures that go beyond this trivial solution."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strong empirical evidence: Extensive benchmarking across 135 chaotic systems and multiple real-world datasets (e.g., ECG, Kuramoto oscillators) convincingly supports the claim that context parroting is competitive or superior to foundation models.\n\nClarity and interpretability: The algorithm is simple, well-motivated, and connected to classic nonlinear dynamics methods (Takens’ embedding, simplex projection).\n\nInsightful theoretical link: The explanation of in-context scaling laws through the fractal dimension of attractors is elegant and potentially generalizable beyond SciML.\n\nCritical perspective: The paper exposes fundamental limitations of current time-series foundation models—especially their tendency to “regress to the mean” and fail to utilize context effectively.\n\nRelevance and timeliness: Given the explosion of foundation models for science, a “simple but tough-to-beat” baseline is highly valuable."}, "weaknesses": {"value": "Scope of comparison: While the benchmarks are extensive, all compared models are pre-2025 foundations. It’s unclear whether next-generation models (e.g., fine-tuned or physics-aware transformers) would still underperform.\n\nLimited theoretical rigor: The link between fractal dimension and scaling exponent is compelling but heuristic; a more formal derivation or empirical validation of α ≈ 1/d₍cor₎ across systems would strengthen the claim.\n\nPotential overstatement: The claim that parroting “outperforms all foundation models” might be dataset-dependent—some tasks (e.g., short-context nonstationary regimes) still favor Chronos.\n\nComputational fairness: The comparison omits any fine-tuning or adaptation of foundation models; parroting benefits from zero training cost but might not scale as easily to multivariate or stochastic systems.\n\nNo ablation on motif length (D) or distance metrics: Although the authors mention robustness to D, a systematic analysis would clarify when parroting breaks down."}, "questions": {"value": "How does context parroting perform on nonstationary or regime-shifting time series (e.g., climate or economic data)?\n\nCould a hybrid “parrot + predictor” model be constructed to retain parroting’s simplicity while addressing its limitations?\n\nHave you tried comparing against autoregressive baselines (ARIMA, GP kernels, etc.) to quantify parroting’s advantage beyond foundation models?\n\nDoes the observed scaling α ≈ 1/d₍cor₎ persist under noise or partial observability?\n\nCould the parroting mechanism be explicitly detected within transformer attention patterns (e.g., induction heads over repeated motifs)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DmwiuaMFei", "forum": "EUAXc9Hlvm", "replyto": "EUAXc9Hlvm", "signatures": ["ICLR.cc/2026/Conference/Submission13037/Reviewer_2L3R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13037/Reviewer_2L3R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761374911957, "cdate": 1761374911957, "tmdate": 1762923771317, "mdate": 1762923771317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that simple parroting - the process of identifying the best-matching patterns in history and repeating them as forecast - could beat state of the art time-series foundation models in predicting the evolutions of systems that are deterministic and chaotic. It then observes that simple parroting results in a scaling law between number of time steps and error in prediction quality, which is characteristic of LLMs. Finally, the paper speculates that such in-context parroting mechanism could potentially explain the in-context neural scaling of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written. It makes a convincing case for context-parroting as a simple yet non-trivial baseline for zero-shot forecast of chaotic systems.\n- The paper is also very topical as it attempts to elucidate the mechanisms underlying in-context learning of foundation models, though not full achieving a mechanistic interpretation of foundation models.\n- The theoretical analysis bridging the algebraic coefficient of the power law to the embedding dimension of the time series is very insightful, though only speculative."}, "weaknesses": {"value": "Limited generality. The paper claims to explain the in-context neural scaling law reported in Liu et al. 2024 [1]. However, the in-context neural scaling law of that paper encompasses discrete, stochastic systems such as Markov chains with randomly generated transition matrices, chaotic systems such as the logistic map injected with noise, as well as deterministic ODE systems such as the Lorenz system. On the other hand, this paper (apart from section 5.3) only investigates deterministic systems governed by ordinary differential equations (ODEs). It is unclear whether context parroting could work on stochastic systems, and if so, whether it would result in similar scaling law. The paper defers the theoretical analysis of context parroting on stochastic system to future works, a valid decision in my opinion since the paper is already quite substantial and self-contained as it is. However, I still think the authors should clarify that they are only addressing the in-context neural scaling law that specifically results from learning deterministic chaotic systems governed by ODEs. \n\n[1]: Toni JB Liu, Nicolas Boullé, Raphaël Sarfati, and Christopher J Earls. Llms learn governing principles of dynamical systems, revealing an in-context neural scaling law. arXiv:2402.00795, 2024."}, "questions": {"value": "- The theoretical result that the power law coefficient $\\alpha$ should equal $\\frac{1}{d_{correlation}}$ is quite nice. However, the experimental support for this turns out to be quite weak (Fig. 11). Could the authors speculate on why?\n- In Figure 5, context-parroting with more delayed embeddings correspond to curves with higher loss. This is counter-intuitive to me since more delayed embedding should result in better matching of patterns, at least when the context length is sufficiently long. However, this is not what we see in the left panel of Figure 4. Could the authors comment on why? The caption to figure 4 states \"Larger D can be more accurate for longer forecasting horizons\". Could the authors extend on the context length further to see whether algorithms with large D eventually outperform those with lower D?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RFZe5FpfB1", "forum": "EUAXc9Hlvm", "replyto": "EUAXc9Hlvm", "signatures": ["ICLR.cc/2026/Conference/Submission13037/Reviewer_PSB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13037/Reviewer_PSB6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626430064, "cdate": 1761626430064, "tmdate": 1762923770784, "mdate": 1762923770784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes context parroting, a phenomenon observed in recent time series foundation models (FMs), as a tough-to-beat baseline for zero-shot forecasting of physical dynamical systems. The manuscript boils context parroting down to an efficient algorithm that directly copies from the context and is on par with or better than current foundations models such as Chronos. The method is used to explain recent in-context neural scaling laws and connected to the induction heads theory of LLMs. In that, context parroting highlights current failure modes of time series foundation models with the aim to improve future model design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and structured and hence easy to follow.\n2. Boiling down the algorithm behind context parroting to a lightweight routine which can be used as a baseline comparison is a great contribution to the field. \n3. Using and comparing the method to existing ideas and routines from dynamical systems theory is great way to gain a more mechanistic understanding of the underlying principles of current time series FMs."}, "weaknesses": {"value": "The paper should discuss recent work introducing DynaMix [1]. It is a FM for time series based on RNNs and MoE and shows promising performance in forecasting chaotic dynamics from limited context without context parroting. This work also addresses the finding of [2], and shows that while context parroting games short-term forecast metrics, it only produces cyclic patterns in the long-term, rendering the method unable to truly produce the chaotic dynamics underlying the system hinted by the context. An analysis that [1] includes and this paper misses, in my opinion, is one that evaluates invariant measures for long autoregressive roll-outs e.g. by estimating the empirical max. Lyapunov exponent $\\lambda_{max}$ for trajectories pushing $T \\rightarrow \\infty$. [1] clearly shows that Chronos (which is context parroting) only produces $\\lambda_{max} \\approx 0$, indicating cycles (albeit high-order and complex at times). The manuscript should include a thorough discussion on these results and ideally include DynaMix as a comparison FM.\n\n**References**:\n\n[1] Hemmer, Christoph Jürgen, and Daniel Durstewitz. \"True zero-shot inference of dynamical systems preserving long-term statistics.\" arXiv preprint arXiv:2505.13192 (2025).\n\n[2] Zhang, Yuanzhao, and William Gilpin. \"Zero-shot forecasting of chaotic systems.\" arXiv preprint arXiv:2409.15771 (2024)."}, "questions": {"value": "1. “Despite the theoretical correspondence, however, numerically it is challenging to accurately estimate the scaling coefficient α due to noise in the data.” I am confused, in case of controllable benchmark data such as datasets derived from *dysts*, how is noise a problem? Can’t we simply generate data w/o noise?\n2. “Moreover, even when restricted to parroting, Chronos can in principle dynamically choose the optimal embedding dimension D for each individual time series, giving it an advantage over parroting algorithms with a fixed D.” Can the authors explain how Chronos would do this?\n3. Could the authors include an experiment that shows whether the estimated max. Lyapunov exponent converges to the ground truth for increasing context lengths, where $\\lambda_{max}$ is estimated from predicted trajectories only?  And of course predicting for much longer than 10 Lyapunov times, say 100-1000, to really probe the long-term climate of the forecasts of the introduced context parroting algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8yBxvDgN94", "forum": "EUAXc9Hlvm", "replyto": "EUAXc9Hlvm", "signatures": ["ICLR.cc/2026/Conference/Submission13037/Reviewer_xP7T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13037/Reviewer_xP7T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931002131, "cdate": 1761931002131, "tmdate": 1762923769968, "mdate": 1762923769968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}