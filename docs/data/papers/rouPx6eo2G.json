{"id": "rouPx6eo2G", "number": 885, "cdate": 1756821890991, "mdate": 1759898237438, "content": {"title": "One-Step Score-Based Density Ratio Estimation: Solver-Free with Analytic Frames", "abstract": "Score-based density ratio estimation is essential for measuring discrepancies between probability distributions, yet existing methods often suffer from high computational costs. We propose One-Step Score-Based Density Ratio Estimation (OS-DRE), an analytic and efficient framework that eliminates the need for numerical solvers. Our approach is based on a  spatiotemporal decomposition of the time score function, where its temporal component is represented with an RBF-based (radial basis function) analytic frame. This transforms the intractable temporal integral into a closed-form weighted sum, enabling OS-DRE to estimate density ratios with only one function evaluation while preserving high accuracy. Theoretical analysis provides a rigorous truncation error bounds, ensuring provable accuracy with finite bases. Empirical results show that OS-DRE achieves competitive performance while completing density ratio estimation in a single step, effectively resolving the long-standing efficiency–accuracy trade-off in score-based methods.", "tldr": "A solver-free, one-step score-based method for density ratio estimation.", "keywords": ["one step", "score-based", "density ratio estimation", "solver-free", "analytic frame"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4022741f2f7d16e74c72708644c535a1f2539a02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Score-based Density Ratio Estimation (DRE) methods have garnered attention in recent years. However, previous methods typically model the time scores themselves using neural networks, requiring numerical integrations to obtain the resulting density ratios. The work proposes to resolve the computational challenges by an analytic alternative, which has theoreticall bounded approximation errors. Empirical evidence are provided to demonstrate the good performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The analytical framework is attractive, as both the time scores and the density ratios are tractable.\n2. The theoretical analysis on the framework is interesting."}, "weaknesses": {"value": "1. The emprical experiments are generally smaller scale.\n2. The authors proposed to use the neural network to predict the spatial coefficients in the density ratio. Perhaps a crude alternative is to model the density ratio itself using a neural network and obtain the time score by differentiating; e.g. [1] essentially uses CTSM [2] to train the model, and [3] proposed a method that is in principle applicable to any-step. Are there practical benefits of the proposed framework apart from computational efficiencies during training?\n\n[1] Learning normalized image densities via dual score matching, Guth et al.\n\n[2] Density Ratio Estimation with Conditional Probability Paths, Yu et al.\n\n[3] Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment, Chen et al."}, "questions": {"value": "See also the Weaknesses. I am curious about the expressitivity of the proposed framework. For instance, is the proposed method well-behaved in higher dimensional problems, e.g. when training an EBM on MNIST?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vkMuQrq1yf", "forum": "rouPx6eo2G", "replyto": "rouPx6eo2G", "signatures": ["ICLR.cc/2026/Conference/Submission885/Reviewer_bFyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission885/Reviewer_bFyf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761141690716, "cdate": 1761141690716, "tmdate": 1762915636464, "mdate": 1762915636464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal is to estimate the ratio of two densities $p_0$ and $p_1$ from their samples. To do so, the authors use the time score identity\n\n$\\log \\frac{p_1(x)}{p_0(x)} = \\int_0^1 \\partial_t \\log p_t(x) dt$.\n\nwhere the time score $\\partial_t \\log p_t(x)$ is first estimated, then integrated. While related literature focuses on the first part --- designing sample-efficient estimators of the time score [1, 2] --- the authors instead focus on the second part which is solving the integral with as few queries as possible to the time score because it is approximated using a neural network that is expensive to evaluate.\n\nThe authors propose a specific parameterization of the time score model\n\n$\\log p_t(x) \\approx \\sum_{k=1}^{K} h_k(x) g_k(t)$\n\nthat decouples the $x$ and $t$ variables and where the $g_k$ can be integrated in closed-form. This way, the time score can be integrated in one-step which is computationally efficient. \n\nThe authors show this leads to good performance. \n\n[1] Choi et al. Density Ratio Estimation via Infinitesimal Classification. AISTATS 2022.\n\n[2] Yu et al. Density Ratio Estimation with Conditional Probability Paths. ICML 2025."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Estimating the ratio of two densities $p_0$ and $p_1$ using intermediate densities $(p_t)_{t \\in [0, 1]}$ has been gaining traction in recent years [1, 2, 3, 4]. While most works focus on the *statistical efficiency* of the time score estimator, the authors instead focus on the *computational efficiency* of its integration. This is an original idea. The text is clear. The experiments are diverse and encouraging. \n\n[1] Rhodes et al. Telescoping Density-Ratio Estimation. NeurIPS 2020.\n\n[2] Choi et al. Density Ratio Estimation via Infinitesimal Classification. AISTATS 2022.\n\n[3] Yu et al. Density Ratio Estimation with Conditional Probability Paths. ICML 2025.\n\n[4] Williams et al. High-Dimensional Differential Parameter Inference in Exponential Family using Time Score Matching. AISTATS 2025."}, "weaknesses": {"value": "## Minor concerns\n\nThis is more of a comment, which the authors can disregard if they wish. Personally, I did not find Figure 1 helpful to understand the authors' method. Actually, I found the text very clear and clearer even than the figure.\n\n## Main concerns\n\nMy main concerns on the evaluation procedure. The authors' main claim is that their method should be *faster*.\n\nSpecifically, the authors use the same time-score estimator as in prior work [1], so we do not expect a difference in estimation error a priori.\nHowever, they employ a different integration scheme (closed-form) compared to previous works that used numerical integration or ODE solvers.\nHence, as a reader I would expect to see experiments in the main text that illustrate a reduced computational cost. \n\n**Concern 1: showing results on computational gain in the main text**. It would be nice to see results in the main text that quantify the computational gain. For example, the authors can plot the computational complexity on the Y axis (wall clock time or NFE) as a function of the approximation error of the integral chosen by the user on the X axis. We would expect an increasing function for other methods, and a near-zero constant function for the authors' method. \n\n**Concern 2: distinguishing between estimation error and integration error**. The large differences between methods in Figure 2 is a bit surprising to me. In my experience, such a difference is usually due to estimation error in the time-score model rather than from integration error. Yet, the authors use the same estimation procedure as DRE-∞, so one would not expect major differences in estimation accuracy, unless their particular parameterization introduces them. Can the authors comment on this?\n\n**Concern 3: clarifying Figure 3**. My understanding is that Figure 3 illustrate the ratio of two Gaussian densities. So in Figure 3.b., while the authors write \"CIFAR-10-C-Style Corruption\", they are not actually using CIFAR images. If my understanding is correct, then mentioning CIFAR is quite misleading and I would ask that the authors remove it. More generally, Figure 3 lacks quite a bit of explanation (I had to go through the appendix to understand it better). For example, how do you compute the KL in Figure 3c?"}, "questions": {"value": "I overall believe this could be a strong paper. If the authors address my three concerns detailed in the \"Weaknesses\" section, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4HBiQ12FmW", "forum": "rouPx6eo2G", "replyto": "rouPx6eo2G", "signatures": ["ICLR.cc/2026/Conference/Submission885/Reviewer_Syar"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission885/Reviewer_Syar"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419419092, "cdate": 1761419419092, "tmdate": 1762915636340, "mdate": 1762915636340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a way of learning density ratio estimates with score-based algorithms without needing to run a ODE solver. Instead, they consider an analytic expression for bridging the densities. This is done using a sequence of RBF approximations, with theoretical analysis of the convergence rate. Good empirical performance is demonstrated in a range of benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper makes a foundational contribution, resolving a major limitation of score-based DRE methods. This is an important family of models and removing the need for ODE solvers is a very clear contribution that opens up opportunities for both more accurate and more efficient methods. The authors provide sufficient theoretical guarantees and demonstrate the method well, which clear improvement over previous DRE methods in NLL and other measures.\n\nI appreciate the effort of validating multiple kernel choices, and the broad evaluation in general, covering also complementary aspects like continual learning."}, "weaknesses": {"value": "No notable weaknesses, but I acknowledge that I have not checked the theoretical proofs in detail and would likely have missed possible technical inaccuracies.\n\nOne thing that could be more clearly communicated is computational efficiency. You seem to quantify the efficiency only in terms of the function evaluations (NFE) yet make claims about drastic speed improvement. It would be good to somehow quantify this also in terms of wall-clock speed. Also, in Tables 1 and 3 you consider fairly scarce choice of NFE for the comparison methods -- why not plot the accuracy as a function of NFE, to make it clear what kind of NFE (if any) would be sufficient for reaching similar accuracy with methods based on ODE solvers?"}, "questions": {"value": "Fig 2 shows nicely how DRE with NFE=2 is blurry. How would DRE with large NFE look like? It would be better to show this, as it is not hurting your contribution in any way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YsfVKHvwaK", "forum": "rouPx6eo2G", "replyto": "rouPx6eo2G", "signatures": ["ICLR.cc/2026/Conference/Submission885/Reviewer_Lm7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission885/Reviewer_Lm7P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811014326, "cdate": 1761811014326, "tmdate": 1762915636218, "mdate": 1762915636218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of computing $\\log r(x)$, where $r(x)$ is a density ratio. The core goal of this paper is to perform density ratio estimation. Density ratio is a fundamental task in ML and statistics and is used to quantify discrepancies between two probability distributions. Recent works on DRE using continuous score-based methods proposed that instead of computing the ratio between $p_0$ and $p_1$, one can compute the path integral of the $\\delta_t \\log p_t(x)$ along time as a measure for $log$-density-ratio. However, computing this integral can be expensive. In this paper, the authors propose a one-step score-based density ratio estimation which computes an approximation to the path integral by first constructing a separable basis expansion of $\\delta_t \\log p_t(x)$ and then approximating using a finite sum. Note the finite sum corresponds to the coefficients of the basis function of $x$ and one would still need to integrate the basis due to $t$. However, the authors correctly observe that for standard orthonormal bases (e.g., Fourier, Legendre), all basis elements except the constant function have zero integral over $[0,1]$. To get around computing a vacuous solution, the authors relax the strict orthogonality condition for the bases of $t$, and instead use frames (discretization) of the function of $t$. Then they evaluate score as a double over this discretization and the coefficients due to $x$. Given the discretization leads to locally continuous functions in $t$, one can efficiently compute $\\log r(x)$. The authors further show that these bases can be precomputed given we can assume that the function of $t$ can be expressed using specific variants of the RBF kernel. One can then learn the parameters of the function in $x$ using a simple neural network. Using multiple experiments on synthetic and real world data, the paper demonstrates that this procedure leads to significant improvements when estimating DRE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work correctly identifies that estimation of an integral over dual variable can be expressed effectively using basis expansion and then uses the properties of specific functions to approximate this basis expansion effectively. This results in an analytic solution which is nice! Although I have not gone through all the main proofs minutely, I believe the theoretical claims made in this paper. \n\n- Reducing the core task of multiple function evaluations to a single forward pass in a neural network is also significant as it eliminates the need for expensive numerical solvers or iterative integration.\n\n- The study of $g_k(t)$ using RBF functions make the algorithm much more \"user friendly\", and applicable to a wide domain of applications (as RBF is often the main choice of kernel functions).\n\n- Beyond theoretical contributions, the authors demonstrate the superior performance of their method empirically on several synthetic and real world datasets."}, "weaknesses": {"value": "- I am a bit confused about the general applicability of the class of the methods in general. Observe that the primary assumption of these algorithms is that there is an integrable path from $p_0(x)$ to $p_1(x)$. Is that generally true for applications in the wild? For high-dimensional and multimodal data, this assumption might fall apart, limiting the scope of application of such methods. I don't understand that claim that these methods alleviate \"density-chasm\". E.g., when there is minimal overlap of the supports, how do you even compute its $log$? I think it becomes close to undefined. Wouldn't using measures like Wasserstein be more effective then for resolving the \"density-chasm\" problem?\n\n- While indeed the method works (demonstratively) when the target function belongs to a Sobolev space of lower smoothness, I am unsure, if it is even efficient for other function classes?\n\n- For Proposition 4.1 a core assumption is denseness. So what happens in case of long tailed distributions?\n\n- There are two stages of approximation -- 1) basis expansion is approximated using $K$ terms, and 2) the integral of $g_k(t)$ in $[0,1]$ is approximated using discretization (infinitely many discrete frames). But then when you write Equation (8) you write it as an equality without the coefficients $c$?"}, "questions": {"value": "Please see the limitations section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XxxbgJAqT2", "forum": "rouPx6eo2G", "replyto": "rouPx6eo2G", "signatures": ["ICLR.cc/2026/Conference/Submission885/Reviewer_LTmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission885/Reviewer_LTmZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938612928, "cdate": 1761938612928, "tmdate": 1762915636112, "mdate": 1762915636112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper contributes a new algorithm for continuous score-based density ratio estimation (DRE) that is computationally much more efficient than previous algorithms in this category at comparable or better accuracy.\n\nContinuous score-based DRE methods avoid the \"density chasm\" problem but suffer from the computational cost of numerical integration of the predicted time score (= partial derivative of log density ratio w.r.t time) along the path (in time) from one distribution to the other. This paper instead proposes to approximate the time score by a finite-term expansion of the form $s_t^{\\theta}(x, t) = \\sum_{k=1}^{k=K} h_k^{\\theta}(x)g_k(t)$, where $\\\\{g_k(t)\\\\}$ have closed form expressions for their time integral. The idea is that the time score prediction model parameters $\\theta$ can be trained to predict the $K$ functions $\\\\{h_k^\\theta(x)\\\\}_{k=1}^{k=K}$ instead of $s^\\theta()$. Since $\\\\{g_k(t)\\\\}$ are analytically integrable, their method avoids numerical integration in this fashion by just evaluating the model prediction only once.\n\nThe expansion for time score is analogous to expansions over Hilbert space of space x time functions, such as Karhunen-Loeve, which separates out space and time components, but with a few differences. For their method to work, they need the time integrals for $\\\\{g_k(t)\\\\}$ to not vanish (usually the case for may common choices, such as Fourier expansion). At the same time, their application does not depend on orthogonality of $\\\\{g_k(t)\\\\}$. This allows them to choose so called \"frames\" of Hilbert spaces for the time component (i.e., the space of functions of time on domain $[0, 1]$) for these time functions. In particular, they go with RBFs over time, and show that RBFs (under mild conditions i.e. strictly positive definite, e.g. Gaussian) qualify as frames, and that these RBFs can approximate well if the true time score and these RBFs satisfy some technical conditions---the approximation error diminishes as $O(K^{-\\beta})$ where $\\beta$ parametrizes the Sobolev space to which the time score is assumed to belong.\n\nThe main technical contribution of the paper are the finite-term expansion of time score in terms of frames over time, and the follow-up approximation result with RBFs as frames, with rigorous proofs for all claims. They also provide a section at the end that compares their algorithm to previous continuous time score DRE methods empirically."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a novel, rigorous, and well-substantiated contribution to the subject of DRE. Major strengths of the paper:\n\n- Novelty: Separating out space and time components in a frame-based expansion where the time functions have closed form integrals, thereby giving a compuationally efficient approximation, is a simple but new and effective idea.\n\n- Rigour: The key technical insight this paper provides is perhaps that continuous time score functions (under some technical assumptions) can be well approximated by a finite-term expansion in terms of RBF over time. The paper and its appendix provides rigorous proofs of each step leading to this result (Theorem 3.4, Propositions 4.1, and 4.2).\n\n- Experimental validation: The paper evaluates their novel method and compares it to previous continuous score-based DRE methods for a wide variety of datasets, real world as well as synthetic.\n\n- Presentation: The paper is written clearly, with sufficient background on previous work and motivation, and with brief explanations while deferring the detailed proofs to the appendix, all of which ensures smooth flow of exposition, while allowing the reader to verify the proofs separately."}, "weaknesses": {"value": "No major flaws. A few comments however:\n- It seems to me that the paper could do with a little more discussion on how the parameter $K$ (number of terms in the expansion) affects accuracy and computation cost. The paper does point out that large values of $K$ can lead to overfitting. But it seems to me that large values of $K$ may be necessary for fitting more complex time score functions in practice (the Sobolev space assumption notwithstanding).  Then the the time score prediction model (neural network) may need more parameters to fit a large number ($K$) of functions, and the cost of training and inference would also scale as a function of $K$ as a result. \n\n- While the paper presents a number of experimental results for their method (OS-DRE) and earlier ones, it would have been good to  compare OS-DRE to earlier methods, notably DRE-$\\infty$ (that they are perhaps closest to, in setup, losses etc.), on a real world dataset where DRE-$\\infty$ was also tested in the paper that originally proposed the latter (Choi et al 2022). For example, this paper does compare DRE-$\\infty$ and OS-DRE on real world datasets (from Grathwohl 2018), but these datasets were not used in the Choi et al 2022 paper as far as I can tell. Same for TRE.  etc. As someone not (previously) familiar with continuous score-based DRE literature, I perhaps could also have posed this in the \"Questions\" section of the review, but it would be good to have a clarification or update from the authors."}, "questions": {"value": "None, except see also the 2nd comment in the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VCmLaknZR3", "forum": "rouPx6eo2G", "replyto": "rouPx6eo2G", "signatures": ["ICLR.cc/2026/Conference/Submission885/Reviewer_RMmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission885/Reviewer_RMmF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission885/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981378821, "cdate": 1761981378821, "tmdate": 1762915635954, "mdate": 1762915635954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}