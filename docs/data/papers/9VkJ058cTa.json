{"id": "9VkJ058cTa", "number": 14053, "cdate": 1758227671288, "mdate": 1759897393390, "content": {"title": "POEMetric: The Last Stanza of Humanity", "abstract": "Large Language Models (LLMs) can compose poetry, but how far are they from human poets? In this paper, we introduce POEMetric, the first comprehensive framework for poetry evaluation, examining 1) basic instruction-following abilities in generating poems according to a certain form and theme, 2) advanced abilities of showing creativity, lexical diversity, and idiosyncrasy, evoking emotional resonance, and using imagery and literary devices, and 3) general appraisal of the overall poem quality and estimation of authorship. We curated a human poem dataset - 203 English poems of 7 fixed forms annotated with meter, rhyme patterns and themes - and experimented with 30 LLMs for poetry generation based on the same forms and themes of the human data, totaling 6,090 LLM poems. Based on POEMetric, we assessed the performance of both human poets and LLMs through rule-based evaluation and LLM-as-a-judge, whose results were validated by human experts. Results show that, though the top model achieved high form accuracy (4.26 out of 5.00, with Gemini-2.5-Pro as a judge; same below) and theme alignment (4.99), all models failed to reach the same level of advanced abilities as human poets, who achieved unparalleled creativity (4.02), idiosyncrasy (3.95), emotional resonance (4.06), and skillful use of imagery (4.49) and literary devices (4.67). Humans also defeated the best-performing LLM in overall poem quality (4.22 vs. 3.20). As such, poetry generation remains a formidable challenge for LLMs.", "tldr": "The first comprehensive framework for LLM poetry evaluation", "keywords": ["Poetry evaluation metrics", "poetry generation", "large language models", "dataset and benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/418b1aaaabf1082a7ca3c67348416291988c47c6.pdf", "supplementary_material": "/attachment/19e77bfedb4062292922a022dbf451499fa210b5.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce a novel evaluation dataset and framework for formal verse poetry. They first collect 203 human-created poems which they annotate with meter, rhyme pattern, topic, and other styles. These annotations are then used to condition a wide range of large language models to generate style-conditioned poetry. To evaluate generated (and to a lesser degree also human) poems, the authors propose various evaluation schemes. E.g., rule-based evaluation of form, meter, and rhyme scheme; and LLM/human-as-a-judge evaluation of basic instruction-following abilities, advanced creative abilities, and general appraisal. The authors show that LLMs-as-a-judge correlate well with human assessments and that generated poetry generally follows stylistic constraints well but fails to reach the same mastery of language (use of literary devices, creativty, emotional resonance, ...) that humans achieve."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Poetry evaluation is an challenging research problem, especially for general-purpose large language models whose poetry generation performance is not well understood. The authors successfully investigate a gap and their findings will be useful for future research.\n* The authors promise to release their dataset artifacts which will be useful for future work.\n* The authors evaluate many different dimensions of poetry which give a broad overview of performance."}, "weaknesses": {"value": "* The scope of the paper is a bit limited. It would have been interesting to see how some of the uncovered gaps between humans and generated poems could be closed either by model training or more advanced prompting. At least some suggestions for future work would have been insightful."}, "questions": {"value": "* l.259 why would averaging with other LLMs introduce noise and bias? Wouldn't that just be model ensembling which is a common technique that usually works well? ByGPT5 (one of your cited papers) showed that general purpose LLMs (as of 2023) are bad at following stylistic constraints such as rhyme and meter. Do the authors have an intuition what changed with current LLMs which apparently now work much better?\n\nComments\n* The poems in Figure 3 are too small to read comfortably.\n* l480: ALthough -> Although"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XnxGPNmrb1", "forum": "9VkJ058cTa", "replyto": "9VkJ058cTa", "signatures": ["ICLR.cc/2026/Conference/Submission14053/Reviewer_xKLP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14053/Reviewer_xKLP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760993889462, "cdate": 1760993889462, "tmdate": 1762924537995, "mdate": 1762924537995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for poetry generation and evaluation using modern LLMs. The authors curate a new human dataset with rich metadata (title, topic, style, meter, etc) and use this to show that machine generated poetry is high quality in terms of the structural elements of poetry, but falls short on creativity, style and metaphor etc. \n\nOverall this is well motivated and executed, however I feel that the main contributions of the work are the very detailed user study, as the method itself is quite simplistic, and as such this isn't a good fit for ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper’s main contribution was a detailed analysis of poetry outputs, and a careful and detailed comparison between various LLMs versus humans. \n\nThe dataset and evaluation should be useful to others working in the area, the wide range of poem types and rich metadata would make it a valuable resource.\n\nThe method used for generating and evaluating poetry was well motivated, clearly presented and sound (although many details were pushed to appendices and attached code, the latter which I haven't inspected.)"}, "weaknesses": {"value": "As stated above, I don’t think ICLR is the right venue to properly assess the human elements of the work. \n\nThe language generation and evaluation aspects were solid, but this is more a data + prompting exercise than a scientific contribution. The real contributions of the paper were the detailed evaluation, which is reflected in the way the paper in written, and the split of materials between the main text and the appendices.\n\nAccordingly, I think this work would be better suited to other venues, e.g., computational linguistics, even CHI or more user-centric fields, where the reviewing process would be better able to judge the quality of the human study side of this work."}, "questions": {"value": "1. There’s a lot of algorithmic detail in the appendix, around how the dataset is created that isn’t fleshed out and reproducible (appendix B, page 16, most steps are loaded: Levenshtein distance with walk? how to check for repetition and consistency? what is align?). (Note code is provided separately, according to the statement, so probably ok.)\n1. The use of prompting seems a bit naive, as this is formatted as a user survey which may not be well handled by many LLMs. I expect that tailoring the prompt may improve the quality and robustness of model outputs. Consider k-shot prompting, providing examples of good vs bad outputs for each criteria, asking for ratings and rationales, versus ratings alone, etc.\n1. Is thinking on/off a factor? There was a brief comparison of pairs of models with and without thinking, but I think this was confounded by model size. For some models thinking can be turned on and off. The same holds true for search, which may be relevant to evaluation. \n1. Are the eval metrics closely correlated to one another? Perhaps you might distill down to a handful of metrics instead.\n1. Might the model be directed to generate more creative poetry with more detailed prompting, or an iterative approach where feedback is given to make the eval more human-like (e.g., RL train over key evaluation metric)? This would be beyond the scope of one paper, but would be worthy of discussion. \n1. The human expert survey for eval includes “Please do not check any online or offline resources while completing this survey; we are interested in your own direct and personal response!” – what about plagiarism of text and ideas from other sources? I note that there was no such directive given to the LLM as judge eval, is search thought to be useful here?\n1. Agreement between human experts - the PoA analysis just uses LLM vs human setting, not human vs human. It's common to hold-out one human and compare them against the remaining humans as an upper bound.\n1. Regarding confidence in predictions: LLMs are often overconfident, see the extensive work on calibration of LLMs. The discussion appears to rely on what is a rather unreliable signal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xx6oXcex2O", "forum": "9VkJ058cTa", "replyto": "9VkJ058cTa", "signatures": ["ICLR.cc/2026/Conference/Submission14053/Reviewer_cK2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14053/Reviewer_cK2s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542950667, "cdate": 1761542950667, "tmdate": 1762924537540, "mdate": 1762924537540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for evaluating poetry quality based on aspects of instruction following, creative ability, and general appraisal. The authors also construct a dataset, including 203 poems across 7 fixed forms, all collected and annotated. The paper examines multiple LLMs and uses LLM-as-a-judge to evaluate the poems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Evaluating creative domains such as poetry remains an underexplored yet meaningful challenge. This paper explores this issue by proposing a comprehensive evaluation framework encompassing three key aspects and ten specific subdimensions."}, "weaknesses": {"value": "- Given the general scarcity of poetry data, it is essential to clarify the rationale for selecting this particular dataset. In this paper, the dataset comprises 203 human-authored English poems covering 7 fixed poetic forms, accompanied by an equal number of 203 corresponding prompts. Each prompt specifies a combination of form, meter, rhyme, and theme. However, it remains unclear whether all 203 poems represent unique combinations of these attributes. Notably, the dataset appears imbalanced in terms of poetic form, being dominated by Ballads and Sonnets. Such imbalance could potentially affect the reliability of the overall evaluation. As reported in (Sonnet or not, bot? poetry evaluation for large models and datasets), large language models (LLMs) used as evaluators exhibit considerable variation in their judgments across different poetic forms, suggesting that the reported performance may also suffer from this concern. Also, LLMs tends to overestimate their own output. It is not clear whether Gemini-2.5-Pro performance is due to this effect or not. \n\n- Another major concern lies in the variation of agreement (PAo) between LLM-based evaluations and human judgments across different aspects. As illustrated in Figure 7, the overall mean scores appear consistent for human evaluators but vary substantially across different models and evaluation aspects. For example, poems generated by DeepSeek-R1 shows large discrepancies in lexical diversity and imagery between human and LLM evaluation, while GPT-4o exhibits inconsistencies across nearly all aspects between human and LLM evaluation. It would be necessary to examine the percentage agreement (PAo) for evaluations of poems generated by different models across various aspects. This could help determine whether LLMs themselves exhibit systematic biases toward certain models, which may further affect the reliability of the evaluation.\n\n- Details of the human evaluation are needed. Similar to Question 1, human evaluators assessed poems generated by both human authors and seven models, resulting in annotations for a total of 58 poems. However, additional clarification is necessary: do these 58 poems cover all 7 fixed poetic forms, or were they drawn from a random subset of the dataset? This distinction is important, as it directly relates to the concerns raised in Question 1 regarding form imbalance and its potential influence on evaluation outcomes.\n\n- The discussion of the theoretical background and motivation underlying the framework’s development is relatively limited. While the proposed metric places particular emphasis on creativity measurement, the related literature addressing this aspect of evaluation is largely absent. A more thorough engagement with prior work on the conceptualization and assessment of creativity would help strengthen the framework’s theoretical foundation (for example, Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations). In this context, the evaluation of general creative tasks is also highly relevant. Findings from other creative domains suggest that LLMs used as evaluators tend to exhibit bias toward their own outputs (e.g., How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs). This underscores the importance of human verification to ensure the credibility of the evaluation results in question 3."}, "questions": {"value": "1. Would form difference affect the credibility of metric? \n\n2. How reliable are LLMs in each evaluation aspects? \n\n3. How are the human annotated samples selected? \n\n4. Other questions see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L0J1ngISFN", "forum": "9VkJ058cTa", "replyto": "9VkJ058cTa", "signatures": ["ICLR.cc/2026/Conference/Submission14053/Reviewer_YnfA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14053/Reviewer_YnfA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940314168, "cdate": 1761940314168, "tmdate": 1762924537184, "mdate": 1762924537184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for the evaluation of poems. A new dataset of 203 English poems is created and experiments with 30 LLMs are presented, assessing human and LLM-generated poems. Results showed that LLMs fail to produce creative poems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* A new resource is created, which can be useful for the community.\n* Extensive benchmark with 30 LLMs, providing useful insights.\n* Interesting insights are drawn from the experimental analysis."}, "weaknesses": {"value": "* **Agreement not chance-corrected** is not measured by Cohen's Kappa, which is an established chance-corrected measure for such tasks. Percentage agreement of 0.66, with this in mind, is low and puts the findings of this work at stake. \n* **TTR is biased** to short texts (by design, the denominator increases linearly with the text length). Combined with the limited data statistics shared, the findings are at stake. For instance, in Figure 6, Llama could be producing shorter texts that would explain the higher TTR values. \n* **Definitions are missing** including what is creative and novel (Line 204), authentic (Line 040), famous (Line 470).\n* **Evaluation** is not well motivated: results with precision, recall and F1 are missing, though the metrics are mentioned in Appendix B.  \n* **Related work** is not properly studied and comparisons hinder the evaluation of this work. Existing evaluation frameworks (e.g., https://link.springer.com/chapter/10.1007/978-3-031-49011-8_1) could have been used to show case their limitations, and the same applies for the existing datasets. The need for another dataset/framework should be backed with empirical evidence. The same applies for the algorithm introduced (Line 145); if there is a lack of existing algorithms, this should be stated explicitly. A simple search, though, leads to one: https://arxiv.org/pdf/2406.18906."}, "questions": {"value": "A. In Line 467, the task is mentioned as authorship, but it is not really authorship attribution but authorship type (human vs. machine). Could the authors elaborate?\n\nB. In Figure 9, models are called \"thinking\". Do the authors mean models employing \"chain of thought\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e9kFN15AmB", "forum": "9VkJ058cTa", "replyto": "9VkJ058cTa", "signatures": ["ICLR.cc/2026/Conference/Submission14053/Reviewer_YAYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14053/Reviewer_YAYx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975990914, "cdate": 1761975990914, "tmdate": 1762924536615, "mdate": 1762924536615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}