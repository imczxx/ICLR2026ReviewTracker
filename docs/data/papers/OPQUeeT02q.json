{"id": "OPQUeeT02q", "number": 12459, "cdate": 1758207976625, "mdate": 1759897508777, "content": {"title": "Semi-Supervised Noise Adaptation: Transferring Knowledge from Noise Domain", "abstract": "Transfer learning aims to facilitate the learning of a target domain by transferring knowledge from a source domain. \nThe source domain typically contains\nsemantically meaningful samples (*e.g.*, images) to facilitate effective knowledge transfer. However, a recent study observes that the noise domain constructed from simple distributions (*e.g.*, Gaussian distributions) can serve as a surrogate source domain in the semi-supervised setting, where only a small proportion of target samples are labeled while most remain unlabeled. \nBased on this surprising observation, we formulate a novel problem termed *Semi-Supervised Noise Adaptation* (SSNA), which aims to leverage a synthetic noise domain to improve the generalization of the target domain. To address this problem, we first establish a generalization bound characterizing the effect of the noise domain on generalization, based on which we propose a Noise Adaptation Framework (NAF). Extensive experiments demonstrate that NAF effectively utilizes the noise domain to tighten the generalization bound of the target domain, thereby achieving improved performance. \nThe codes are available at https://anonymous.4open.science/r/SSNA.", "tldr": "Transferring knowledge from a structured noise domain to facilitate the learning in the target domain", "keywords": ["Semi-Supervised Noise Adaptation", "Noise Adaptation", "Semi-supervised Learning", "Distribution Alignment"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06abf4cd0bc9a6dc7f898b302e81371b1836b6f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel solution for Semi-Supervised Noise Adaptation (SSNA), and proposes a new Noise Adaptation Framework (NAF) that leverages a synthetic noise domain, that is, constructed from random distributions to improve generalisation in a semi-supervised target domain. The authors proof a theoretical generalisation bound inspired by domain adaptation theory and design a learning objective combining supervised, noise-domain, and alignment losses. Experiments on CIFAR-10/100, DTD, Caltech-101, and ImageNet-1K show consistent improvements over Empirical Risk Minimisation and compatibility with existing semi-supervised learning methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Interesting improvement of existing concept: The idea of transferring structure from a synthetic, non-semantic noise domain is novel and could inspire future research on structure-based generalisation.\n\nClear theoretical definition and framing: The authors connect the proposed method to domain adaptation bounds, offering a explanation for why structured noise might help generalisation.\n\nExtensive experiments: The empirical section is broad and thorough, showing improvements across multiple benchmarks.\nGood writing and reproducibility: The paper is well written, clearly organised, and accompanied by public code, which is appreciated."}, "weaknesses": {"value": "The proposed method is a extension of prior work \"Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adapta…\", with some new experiments and explanation, however the visualisation used for explanation is almost identical (i.e. Figure 1 of this paper vs Figure 4 of the referred paper)\n\nSSNA made a strong assumption on a one-to-one mapping between noise classes and target classes and a known number of target classes. This can be limited in the setting like imbalanced dataset. And the paper didn't not discuss the constraints or experiment with dataset as such.\n\nIn most of the experiment results, only the \"accuracy\" metrics are presented, this can be misleading if the dataset is imbalanced, and does not provide error type. Potentially need to provide other metrics like recall or f1.\n\nOnly random noise were discussed and experimented, would other type of noise like Gaussian, uniform, structured noise also applicable?\n\nThe paper does not compare to modern self-supervised or contrastive regularisation methods that might yield similar improvements without requiring a noise domain."}, "questions": {"value": "Overall, I'm still slightly confused on the method is trying to minimised the error on the randomly labelled noise, but there is no clear link to show reduce the error on noise lead to lower error on the real target domain? This can be regularisation by add \"noise\" to the training instead of really \"transferring\" learning as it claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61mSZ2hpTx", "forum": "OPQUeeT02q", "replyto": "OPQUeeT02q", "signatures": ["ICLR.cc/2026/Conference/Submission12459/Reviewer_GdmS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12459/Reviewer_GdmS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761521541063, "cdate": 1761521541063, "tmdate": 1762923340375, "mdate": 1762923340375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Unlike conventional transfer learning where the source domain consists of semantically meaningful data, SSNA leverages a noise domain generated from Gaussian distributions as a surrogate source domain to assist target-domain learning. The authors theoretically derive a generalization error bound for the target domain and propose the NAF, which tightens this bound by jointly minimizing the empirical risk on the target domain, the noise-domain risk, and the inter-domain distributional discrepancy. Experiments conducted on multiple benchmark datasets demonstrate that NAF achieves significant performance improvements compared with various existing methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces the SSNA problem, which treats the noise domain as a surrogate source domain. This setting holds potential application value in scenarios with privacy constraints or data scarcity.\n- In Section 4.1, the authors derive a generalization bound (Theorem 1), explicitly showing that the bound depends on three components: the empirical error on the target domain, the empirical error on the noise domain, and the H-divergence between them.\n- The code and parameter configurations are detailed in Appendix B, and an open-source link is provided to enhance reproducibility."}, "weaknesses": {"value": "- There are concerns regarding the assumptions and some conclusions of NAF. [1] demonstrates that deep neural networks trained on completely random labels or random inputs can fully memorize arbitrary input–output mappings. [2] similarly finds that during optimization, deep networks with structural priors such as convolutional weight sharing, batch normalization, and nonlinear mappings tend to enforce linearly separable clustering in feature space. \nConsidering these findings, the reviewer is concerned that the “discriminative structure formed in the noise domain” claimed by NAF may in fact result from pseudo-structural effects induced by the inductive bias of deep models like ResNet, rather than from the noise distribution itself.\n\n- There are also concerns about the way the noise domain is generated. The paper uses a Gaussian distribution, but in real-world settings, natural noise (e.g., Poisson or long-tailed distributions) often exhibits non-Gaussian characteristics. Under such conditions, can the proposed noise projector (g_n) and distribution alignment mechanism still effectively extract discriminative structures? Since non-Gaussian noise tends to have larger intra-class variance, forming compact clusters within each class may be difficult, potentially weakening the effectiveness of knowledge transfer.\n\n- Would the noise distribution parameters (e.g., variance σ, dimensionality d) affect generalization performance?\n\n[1] Understanding deep learning requires rethinking generalization, ICLR'17\n\n[2] Learning to See by Looking at Noise, NIPS'21"}, "questions": {"value": "This paper addresses a highly interesting and novel problem. The writing and presentation are excellent, and the figures and results are consistent with the claims. \n\nHowever, some issues may influence the final rating:\n\n- Are there any visualizations, experiments, or theoretical analyses demonstrating that the transferable discriminative structure in the noise domain of NAF does not originate from the network itself?\n\n- Would different types of noise affect the conclusions and experimental results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hNDaNFh7Nh", "forum": "OPQUeeT02q", "replyto": "OPQUeeT02q", "signatures": ["ICLR.cc/2026/Conference/Submission12459/Reviewer_Gj7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12459/Reviewer_Gj7X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540355613, "cdate": 1761540355613, "tmdate": 1762923339921, "mdate": 1762923339921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work propose a Noise Adaptation Framework (NAF), which introduces random Gaussian clusters as anchors to align unlabeled data in semi-supervised learning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- the idea is simple and easy to follow"}, "weaknesses": {"value": "__Major Concerns:__\n- theoretical justification\n  - the proof of Theorem 1 is missing, and the derivation can be problematic.\n  - rather than $d_{\\mathcal{H}\\Delta\\mathcal{H}}(P_n,P_t)$, it should be $d_{\\mathcal{H}\\Delta\\mathcal{H}}(D_n,D_t)$ that introduces the uncertainty.\n  - the right-hand side should include a source error, i.e., $\\epsilon_n(f)$\n  - please provide the proof for: $\\lambda\\leq \\hat{\\lambda}$ since apparently some terms related to model complexity is missing.\n  - $\\epsilon_t(f)$ should not limited to labeled target data, which contradicts  line 243. \n  - I strongly suggest the author check [1] proposing a theory for SSDA based on Ben-David (2010), which may help the derivation.\n- algorithmic design\n  - randomly sampled clusters from $\\mathcal{N}(0,I)$ can overlap\n  - the conditional discrepancy between randomly initialized Gaussian clusters and target features can be extremely large that cause substantial negative transfer.\n  - the assumption that target feature must follow mixture of Gaussian distribution is too strick\n- experiment results\n  - the performance is far below the SOTA SSL methods such as DST\n  - as for an increment to SSL, the comparison should be made between e.g., DST + LERM & DST + NAF\n\n\n__Minor Concerns:__\n- the code is not available from the provided link\n- too much subscripts such as $u,l,n,e,t$, which is confusing\n\n***\n[1] Learning Invariant Representations and Risks for Semi-supervised Domain Adaptation, CVPR 2021"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UOxzVg3fOP", "forum": "OPQUeeT02q", "replyto": "OPQUeeT02q", "signatures": ["ICLR.cc/2026/Conference/Submission12459/Reviewer_SonZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12459/Reviewer_SonZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545075171, "cdate": 1761545075171, "tmdate": 1762923339552, "mdate": 1762923339552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a transfer learning based theoretical framework that learn from the noise domain to help the target domain in a semi-supervised learning manner."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The author formulated a novel theoretical framework to upper-bound the semi-supervised domain adaptation, interestingly, the source domain is a noise domain, and this paper is well-motivated.\n2. The method origninated from the theretical framework is reasonable.\n3. The experiment is good."}, "weaknesses": {"value": "1. The underlying assumption of this paper is Gaussian distribution, so the author may discuss some other noise distribution settings.\n2. I think the success of the knowledge transfer from the source to target is, the class is actually seperable from the source domain, e.g., the author is actually transfering the seperable characteristic of the noise source domain to the target domain (from the Figs of the main paper). Therefore, one concern arises that, for the Gaussian which generates noise in the source domain, will the different sampling strategy would the performance drops significantly? And whether the distance of the sampled Gaussians would affect the performance.\n3. Did you experiemnt on the domain adaptation dataset? e.g., VisDA? Since your contribution is the knowledge transfer, why you only employ the CV tasks.\n4. From Fig.4, it seems the $\\mathcal{L}_n$ is the main contribution to lead to the performance improvement, but how you construct the label of the source noise domains? Could you be more clear on this point?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C2RFAZPEEw", "forum": "OPQUeeT02q", "replyto": "OPQUeeT02q", "signatures": ["ICLR.cc/2026/Conference/Submission12459/Reviewer_L2yW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12459/Reviewer_L2yW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985222296, "cdate": 1761985222296, "tmdate": 1762923337250, "mdate": 1762923337250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}