{"id": "y9FPxcxWyh", "number": 15080, "cdate": 1758247495785, "mdate": 1759897330246, "content": {"title": "Mitigating Conflicts in Multi-Task Reinforcement Learning via Progressively-Trained Dynamic Policy Network", "abstract": "Reinforcement learning is widely applied in various fields, including game playing, robotic control and autonomous driving. However, we find that, when trained for multi-tasking where there exist inter-task conflicts, the standard reinforcement learning algorithm may yield limited performance on individual tasks. To mitigate this, we introduce a dynamic policy network that incorporates diverse computational pathways of varying depths, along with gating modules that selectively activate the appropriate pathways for different tasks. This design, equipped with better flexibility, allows the network to achieve improved multi-task performance. Second, we propose a progressive training technique to mitigate the conflicts among tasks by leveraging proper training order and continual learning techniques. Using the dynamic policy network design and the progressive training technique, we successfully trained a policy capable of performing seven quadrupedal locomotion tasks and a policy that achieved an improved final average reward on ten MiniHack games.", "tldr": "", "keywords": ["reinforcement learning", "continual learning", "multi-task learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d890d0290df379a30c0cec68df560b82c86d039.pdf", "supplementary_material": "/attachment/12294dac63d3d98a370e2cf4fb9909a12e7d1f24.zip"}, "replies": [{"content": {"summary": {"value": "The paper deals with the problem of multi-task RL, where there exists inter-task conflicts. To mitigate\nthis, the paper introduces a dynamic policy network that incorporates diverse computational pathways of varying depths, along with gating modules. Moreover, a progressive training technique is developed to further mitigate the conflicts among\ntasks. The proposed method is demonstrated on quadrupedal locomotion tasks against some baseline methods."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- the investigated problem, i.e., multitask RL, is important."}, "weaknesses": {"value": "- The paper is poorly written and poorly structured. For instance, experimental results are presented in the introduction section. A lot of concepts are introduced without clear definition or description, e.g., backbone feature, action frequency, etc.\n\n- The proposed method is not well-motivated. For instance, section 3.3 progressive training is motivated using Figure 3, which lacks clear information or evidence. \n\n- The experimental results are weak, without clear evidence that the proposed method outperforms existing multitask RL methods."}, "questions": {"value": "- Is the dynamic policy network only applicable to multilayer perceptron architecture? what if the backbone policy network is transformer-based?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s0VACdrNS8", "forum": "y9FPxcxWyh", "replyto": "y9FPxcxWyh", "signatures": ["ICLR.cc/2026/Conference/Submission15080/Reviewer_odXW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15080/Reviewer_odXW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924862854, "cdate": 1761924862854, "tmdate": 1762925403845, "mdate": 1762925403845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of negative transfer, or inter-task conflict, in multi-task reinforcement learning (RL). The authors propose two main contributions:\n\n1. A novel network architecture that introduces \"auxiliary branches\" (shortcuts) and \"self-dilation\" pathways to the network's backbone. These branches create diverse computational pathways of varying depths. Gating modules are used to selectively activate these pathways, allowing the network to dynamically choose shallower paths for simpler tasks and deeper paths for more complex ones.\n2. A novel training technique designed to mitigate a specific hypothesized source of conflict: competition over low-frequency actions between simple \"atomic\" tasks and complex \"compositional\" tasks. The technique first trains on atomic tasks with astrong smoothness rewardto learn stable, low-frequency actions. It then trains on compositional tasks, reducing the smoothness reward and using a continual learning method (like EWC) to preserve the previously learned low-frequency foundations.\n\nThe authors evaluate their combined approach on a 7-task quadrupedal locomotion suite and 10 MiniHack navigation tasks. In both benchmarks, DPNet outperforms baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work is highly significant for multi-task and continual RL. The \"action frequency conflict\" hypothesis provides a new, concrete way to analyze and address negative transfer.\n- The empirical evidence demonstrates significant performance improvements over all baselines in multiple environments.\n- The paper is well-structured."}, "weaknesses": {"value": "- The motivation for the specific design of the processing module ($S$) and gating module ($G$) is unclear. The diagram (Figure 2) is highly reminiscent of a MoE layer, yet the paper uses an element-wise product rather than the standard gated summation. The paper would be stronger if it justified this specific design choice, perhaps with an ablation study comparing the performance of this module to a standard MoE layer.\n- The distinction between the \"Backbone Module\" ($B_i$) and the \"Processing Module\" ($S_{i,m}$) is poorly defined. From Figure 2, their primary difference appears to be that Processing Module has a shortcut path. It is unclear if they differ in internal architecture or parameter count. The paper should clarify the design rationale: Why is a separate Backbone Module necessary? Why do only Processing Module utilizes shortcut connection?\n- The comparison against multi-task RL baselines in Table 3 needs more implementation details for reproducibility and fairness. Furthermore, several baselines were adapted from different enviroments, and the paper should detail these adaptations. For example, how were the \"context-based representations\" from the original CARE paper implemented for the locomotion environment?\n- The most substantial performance gains appear to stem from the progressive training technique, not the DPNet architecture alone. As shown in Table 2, the full DPNet architecture (under the \"Pretrain\" setting) provides only a modest gain (8.0 Avg.TLR) over the MLP baseline (7.6 Avg.TLR). The significant jump to 9.2 Avg.TLR only occurs when progressive training is applied. This confounds the main results in Table 3, as all baseline architectures were also evaluated using this same progressive training technique. It is unclear if DPNet is a superior architecture or if progressive training is just a powerful technique that universally boosts all models. To properly isolate the architectural contribution, a new ablation study is needed comparing all architectures (DPNet, MoE, CARE, etc.) under the same baseline \"Pretrain\" setting."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X8xLrkzlhF", "forum": "y9FPxcxWyh", "replyto": "y9FPxcxWyh", "signatures": ["ICLR.cc/2026/Conference/Submission15080/Reviewer_QAfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15080/Reviewer_QAfV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981931317, "cdate": 1761981931317, "tmdate": 1762925403327, "mdate": 1762925403327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an approach to mitigating conflicts in multi-task reinforcement learning (RL) by combining a dynamic policy network (DPNet) design with a progressive training technique. The idea of the method is to incorporate multiple computational pathways by varying the depth + gating modules to allow the selection of suitable pathways for different tasks (of different complexity). Progressive training is a curriculum training that uses a specific training order + continual learning to train policies on harder tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The method shows significant improvement on the continual learning benchmark - MiniHack."}, "weaknesses": {"value": "1. [Writing] The writing quality is not up to the mark for a publication in a top ML conference. The flow of the paper is not sequential, and as a reader, I had to go back and forth between different sections of the paper to understand it. I believe that this paper needs a rewrite. Some of the issues that I've come across are mentioned below:\n\n\t- The \"Action Frequency\" section in the preliminary is very abrupt.\n\t- The paper lacks a preliminary section on continual learning, which I believe would be helpful for the reader as a context.\n\t- [Minor comment] Citations are formatted in a weird manner where it shows {last initial., et al} instead of {last name., et al}. This is a minor comment. \n\t- Some related works sections, like \"Compositional Networks for Multi-Task Learning\" and \"Continual Reinforcement Learning,\" lack contrast of DPNet with other existing works. The main job of the related works section is to *compare* and *contrast* the current work with prior works and contextualize it.\n\n2. [Claims of progressive training]: The progressive training that is claimed to be proposed in this work is not a new strategy. In fact, it has been addressed for quadruped locomotion in Eurekaverse [1], where an environment curriculum was adopted during policy training.\n\n3. [Experiments: Environment] Metaworld suite of tasks is a common testbed for multi-task RL, and comparison on them should be provided in the paper.\n\n4. [Experiments: Baselines] Specifically for the Quadruped tasks - it is unclear to me how much of the contribution is coming from DPNet and how much is from the curriculum training? I would like to see an experiment without DPNet and only the curriculum.\n\n5. The validity of the Quadruped experiments for Sim2Real transfer is unclear and hasn't been shown.\n\n----\n**References:**\n\n [1]  Eurekaverse: Environment Curriculum Generation via Large Language Models, William Liang et al., CoRL 2024"}, "questions": {"value": "6. How much of a reduction in training/inference speed is observed for policies trained with DPNet?\n\n7. [No experiment needed] How do DPNet class of models differ from gradient surgery type of methods like PCGrad [2]?\n\n----\n**References**\n\n[2] Gradient surgery for multi-task learning, Tianhe Yu et al., NeurIPS (2020)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rojf4ExBTw", "forum": "y9FPxcxWyh", "replyto": "y9FPxcxWyh", "signatures": ["ICLR.cc/2026/Conference/Submission15080/Reviewer_MAwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15080/Reviewer_MAwb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028175038, "cdate": 1762028175038, "tmdate": 1762925402838, "mdate": 1762925402838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}