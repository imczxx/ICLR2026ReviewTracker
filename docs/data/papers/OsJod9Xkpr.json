{"id": "OsJod9Xkpr", "number": 13634, "cdate": 1758220159157, "mdate": 1759897423456, "content": {"title": "Bi-Phase Training: Learning Efficiently in High Dimensions", "abstract": "Pre-trained foundation models have achieved remarkable generalization across a wide spectrum of downstream tasks. However, as models scale in size, the cost to pre-train models becomes prohibitively expensive. In this work, we introduce Bi-Phase Training (BPT), a novel parameter-efficient pre-training method designed to capture the expressiveness of fully parameterized models while drastically reducing the number of trainable parameters. BPT achieves this by combining constrained high-rank transformations using diagonal matrices with exploration of lower-dimensional subspaces through low-rank matrices, facilitating effective optimization within a reduced parameter space. We empirically demonstrate the effectiveness of BPT across various model scales, showing that it successfully matches the performance of standard pre-training on language models while achieving significant reductions in trainable parameters, such as a 66\\% reduction of trainable parameters for a 1.5B model. Furthermore, we conducted a comprehensive evaluation of 17 diverse downstream tasks, confirming that models trained with BPT maintained performance comparable to those trained with a fully parameterized standard method.", "tldr": "A novel parameter-efficient pre-training method that achieves similar performance to a fully parameterized model with significantly fewer trainable parameters.", "keywords": ["Pre-training", "Large Language Models", "Foundation Models", "Parameter efficient training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61d3ee50ca55387d95c6e594ab84e030e50aae9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose BPT (Figure 2), a parameter-efficient pretraining method by introducing trainable low-rank factors and diagonal factors. Specifically, similar to ReLoRA, BPT leverages trainable low-rank factors, which are refreshed and accumlated to the full size weight every N steps. The core innovation of BPT is that it improves the expressiveness of the trainable low-rank factors by wrapping the weight update within two additional trainable diagonal matrices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors improves the expressiveness of ReLoRA with moderate extra trainable parameters.\n- The authors provide an upper bound of the rank of the update.\n- The authors evaluate the performance of BPT on natural language understanding, commonsense reasoning, and reading comprehension tasks."}, "weaknesses": {"value": "**Concern 1. The performance gain over ReLoRA is inconsistent.**\n\nAs shown in Table 2, Table 3, and Table 4. The performance gain of BPT compared to ReLoRA is inconsistent and marginal. From my perspective, BPT is largely built upon ReLoRA with a few extra trainable parameters. I wonder if there are some more significant empirical evidence to support that this small modification is essential enough to overcome some critical issues or bad properties in ReLoRA.\n\n\n**Concern 2. Empirical evidence for compounding effect in Section 3.1.** \n\nTo my understanding, the most essential difference between BPT and ReLoRA is the compounding effect introduced in Section 3.1. I wonder if the authors can provide more empirical evidence to support this claim. Does ReLoRA fail to achieve the 'useful directions discovery' effect and 'optimization direction improvement' in practice? If so, why are the root causes of this issue and how does the introduced trainable diagonal matrices help to address this issue?\n\n\n**Concern 3. Comparison with other parameter-efficient methods.**\n\nAs shown in [1,2], various parameter-efficient methods have been proposed to improve training performance within low-rank structures. Many of them exhibit better pretrained PPL and memory consumption than ReLoRA. I wonder if the authors can provide a comprehensive comparison with these methods in terms of performance and computational efficiency.\n\n[1]. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., & Tian, Y. (2024). Galore: Memory-efficient LLM training by gradient low-rank projection. arXiv preprint arXiv:2403.03507.\n\n[2]. Zhu, H., Zhang, Z., Cong, W., Liu, X., Park, S., Chandra, V., Long, B., Pan, D. Z., Wang, Z., & Lee, J. (2024). APOLLO: SGD-like memory, AdamW-level performance. arXiv preprint arXiv:2412.05270."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tCFX4oF1bU", "forum": "OsJod9Xkpr", "replyto": "OsJod9Xkpr", "signatures": ["ICLR.cc/2026/Conference/Submission13634/Reviewer_jAWY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13634/Reviewer_jAWY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645174241, "cdate": 1761645174241, "tmdate": 1762924211833, "mdate": 1762924211833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bi-Phase Training (BPT), a parameter-efficient approach to pre-training large-scale foundation models. The method combines high-rank transformations using diagonal matrices with low-rank matrix updates to reduce the number of trainable parameters while preserving the performance of fully parameterized models. The authors empirically demonstrate that BPT achieves comparable performance to fully trainable models across a range of model sizes and tasks, with up to 66% fewer trainable parameters. The paper also provides theoretical insights into the rank dynamics of the proposed method and validates its efficacy on 17 diverse downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important and timely problem in the field—reducing the computational cost of training large models—making it potentially impactful if executed well.\n2. The authors rigorously evaluate BPT across multiple scales (100M to 1.5B parameters) and demonstrate its comparable performance on 17 downstream tasks, including natural language understanding, commonsense reasoning, and reading comprehension."}, "weaknesses": {"value": "1. The method is largely incremental, building directly on existing approaches like LoRA and ReLoRA. While BPT introduces diagonal matrix scaling, this addition feels more like an extension rather than a fundamentally new idea. The lack of ablation studies to isolate the impact of the diagonal matrices versus low-rank updates further weakens the claim of novelty.\n2. The experiments focus exclusively on language modeling tasks, leaving open questions about whether the method generalizes to other domains like computer vision, speech, or generative models. The authors themselves acknowledge this limitation, but it significantly reduces the broader impact of the work.\n3. The main experiment only compared the full parameter with ReLORA, without comparisons with methods such as HyperAdapt and LoRA.\n4. The paper emphasizes parameter reduction but does not provide sufficient analysis of computational trade-offs. For example, the impact on training time, convergence speed, and memory overhead is not discussed. This omission makes it difficult to assess the practical value of BPT."}, "questions": {"value": "1. Could you provide a more detailed explanation of how BPT fundamentally differs from prior methods such as LoRA, ReLoRA, and HyperAdapt? Specifically, what is the unique advantage of combining diagonal matrices with low-rank updates, and why is this combination necessary?\n2. While BPT reduces trainable parameters, how does it impact training time, convergence speed, and memory usage? Does the introduction of diagonal matrices and periodic merging introduce additional computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4nd0M7tEuv", "forum": "OsJod9Xkpr", "replyto": "OsJod9Xkpr", "signatures": ["ICLR.cc/2026/Conference/Submission13634/Reviewer_Xe6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13634/Reviewer_Xe6g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645305559, "cdate": 1761645305559, "tmdate": 1762924211323, "mdate": 1762924211323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new parameter-efficient approach for pre-training LLMs, which builds on prior parameter efficient finetuning methods such as LoRA and ReLoRA. They argue that low-rank update alone may be too restrictive for pre-training from scratch. So they update the weight metrix by \\Delta W = A (W + UV) B - W where A and B are trainable diagonal matrices and U and V are low-rank matrics. SImilar to ReLoRA, they periodically merge W + UV -> W and reinitalize U and V again to explore new subspaces. They show a theoretical bound for \\Delta W that can achieve high rank updates. Their experimental results on pretraining different sizes of LLM from scratch show that their proposed method achieves similiar or even lower perplexity than full parameter finetuning, and it achieve better downstream task performance than finetuning with ReLoRA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses paramter-efficient pre-training, which is less explored by previous study.\n2. They provide theoretical upper bound to show the expressiveness of the proposed approach.\n3. Empirical results on pretraining on different scales and tasks is non-trivial."}, "weaknesses": {"value": "1. The update is a composition of known ingredients such as diagonal scaling plus low-rank plus merging. The theory provided is not showing the convergence of optimization or generalization.\n2. Parameter efficiency makes sense for finetuning where compute and data are limited, but the motivation is less clear why we need to contraint parameter updates during pretraining, given that the scaling laws tie performance to total compute and parameter count.\n3. The paper emphasizes on fewer trainable params used in training but does not quantify FLOPS, wall time and actual memory used at training. The periodic merge overhead should also be analyzed."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yergeocBkP", "forum": "OsJod9Xkpr", "replyto": "OsJod9Xkpr", "signatures": ["ICLR.cc/2026/Conference/Submission13634/Reviewer_u8cQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13634/Reviewer_u8cQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783262404, "cdate": 1761783262404, "tmdate": 1762924210472, "mdate": 1762924210472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bi-Phase Training (BPT), a parameter-efficient pre-training method that combines high-rank diagonal transformations with low-rank subspace exploration. The goal is to match the performance of fully parameterized models while substantially reducing the number of trainable parameters.\nThe authors show that BPT achieves comparable pre-training and downstream performance in some settings with up to 66% fewer trainable parameters for a 1.5B model, demonstrating this on Qwen2.5 and OLMo architectures."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- they present an update rule that maintains high-rank capacity with few trainable parameters.\n- The writing is clear, making the paper easy to follow.\n- Preliminary experiments show some early-stage promise, suggesting the idea could be worth exploring further, however more comprehensive evaluation is needed."}, "weaknesses": {"value": "* No compute-efficiency report.\nThe author shows that the number of trainable parameters are reduced in some limited settings. However, while the number of trainable parameters is lower, actual savings in FLOPs or wall-clock time are not measured. This is unclear if this approach actually leads to performance benefits.\n\n* Evaluation tasks are outdated and non-expressive.\nThe 17 benchmarks largely reflect older GLUE-style tasks, which are no longer discriminative for large models. Stronger and more diverse benchmarks (reasoning, multilingual, math, code, instruction following) would give a clearer picture of generalization of current models. Some of the suggeted becnhmarks are GSM8K, MMLU, MMLU-Pro, Math-500, HUmaneval, MBPP, arc challenge ,... these benchmarks are currently widely used to compare large scale languuage models performances.\n\n* Models not trained to convergence.\nThe paper stops at early stages (10–60B tokens), far below full pre-training scale. The main results are for after training 22K steps which is rather too early to derive any conclusions. Conclusions about matching full training are therefore speculative. The “0.05 log-perplexity difference” after partial training is not sufficient evidence.\n\n* Missing ablations.\nThere is no clear ablation showing the contribution of the diagonal matrices vs. low-rank components."}, "questions": {"value": "- How far were the models from convergence when results were reported?\n- Can you show validation curves until training plateaus?\n- Does BPT’s advantage hold when trained for longer, or does it converge to a higher final loss?\n- Have you considered evaluating BPT on more challenging and expressive benchmarks beyond classical GLUE-style tasks—particularly those testing reasoning, math, and code generation (e.g., MMLU, MMLU-Pro, GSM8K, Math-500, RACE, HumanEval, MBPP)? These are now widely used for assessing LLM generalization and reasoning capabilities.\n- What are the real compute and memory savings (e.g., GPU hours, FLOPs)?\n- Could the diagonal scaling matrices be replaced by structured alternatives (e.g., block-diagonal, banded) to improve expressiveness further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SX58qbmIBr", "forum": "OsJod9Xkpr", "replyto": "OsJod9Xkpr", "signatures": ["ICLR.cc/2026/Conference/Submission13634/Reviewer_QwS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13634/Reviewer_QwS4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913262813, "cdate": 1761913262813, "tmdate": 1762924210004, "mdate": 1762924210004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}