{"id": "rMHZfCznhZ", "number": 8836, "cdate": 1758099630421, "mdate": 1759897760830, "content": {"title": "RLAP-CLIP: Continual Multimodal Learning with Prototype Adaptation and Difficulty-Aware Routing", "abstract": "Vision-language models, such as CLIP, achieve strong zero-shot performance through contrastive pre-training but face significant challenges in continual learning scenarios. When learning new tasks sequentially, current methods suffer from degradation in prototype quality due to passive averaging and underutilize their visual adaptation capabilities. We propose RLAP-CLIP, which addresses these limitations through three components. First, Reinforcement Learning-based Prototype Optimization (RLPO) formulates prototype construction as a reinforcement learning problem to actively optimize class separability rather than relying on simple averaging. Second, difficulty-aware cross-modal fusion uses a mixture-of-experts to route samples through specialized processing pathways based on complexity. Third, dual-modal prompting balances visual and textual adaptation. Experiments across eight datasets demonstrate consistent improvements, with RLAP-CLIP achieving average accuracy gains of 3.72-4.46 points and final accuracy improvements of 0.49-4.48 points over other methods, validating that RLAP-CLIP achieves state-of-the-art performance. Our source code is available at [RLAP-CLIP](https://anonymous.4open.science/r/197165541613026132779/RLAP-CLIP).", "tldr": "", "keywords": ["Continual Multimodal Learning; Prototype Optimization; Mixture-of-Experts"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3277f68e846e27efa43cf361ad5dbe8a96d0e45f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a reinforcement learning framework for continual image classification. The method, RLAP-CLIP, uses Reinforcement Learning–based Prototype Optimization (RLPO) to actively refine class representations and mitigate prototype (representation of each class or a embedding of class) degradation. It incorporates difficulty-aware cross-modal fusion to route samples by complexity and enhanced dual-modal prompting to balance visual and textual adaptation. Experiments across multiple image classification benchmarks show RLAP-CLIP outperforms prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The reinforcement learning–based prototype update method empirically outperforms prior work. Enhanced dual-modal prompting and difficulty-aware sample handling further contribute to the gains, as demonstrated in experiments.\n\n- The paper provides empirical and theoretical evidence of training stability—covering hyperparameter sensitivity and convergence—which is critical for reinforcement learning frameworks."}, "weaknesses": {"value": "- The method improves continual image classification by combining established components—prototype updates, reinforcement learning, learnable prompts, and routing. While the gains on benchmarks are clear, the pipeline is heuristic and tailored only for classification, making its generalization to other continual recognition tasks (e.g., retrieval, detection...) uncertain.\n\n- Key terminology should be clearly defined (e.g., “prototype,” “center-based exemplar selection”) to improve readability and understanding. Only objectives and architectures are only explained in the paper. Training and inference process are not explained. Please more clarify what are frozen or learnable weights in the framework."}, "questions": {"value": "- What is center-based example selection in the Figure 1. This part is not explained in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1dKV3TlTRW", "forum": "rMHZfCznhZ", "replyto": "rMHZfCznhZ", "signatures": ["ICLR.cc/2026/Conference/Submission8836/Reviewer_5YDU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8836/Reviewer_5YDU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572567950, "cdate": 1761572567950, "tmdate": 1762920606397, "mdate": 1762920606397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RLAP-CLIP, a framework for continual multimodal learning with vision-language models (e.g., CLIP). aiming to alleviate prototype quality degradation and asymmetric adaptation in sequential task learning. The method introduces (1) RLPO, a reinforcement learning-based approach to actively optimize prototype construction for better class separability; (2) a difficulty-aware, mixture-of-experts mechanism for dynamic sample routing; and (3) dual-modal prompting to balance textual and visual adaptation. Experiments on eight datasets across diverse domains and tasks demonstrate RLAP-CLIP outperforms strong baselines, especially in fine-grained and out-of-distribution settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Motivation and Illustration: Section 2 gives a nice layout of the prototype quality degradation issue and comparison of different prompting strategies in class separation, effectively highlighting the failure cases of existing continual learning methods for VLMs. \n\n2. Clear Presentation and Novel Designs: Section 3 gives a thorough introduction of different components of the RLAP-CLIP framework. Particularly, the RLPO module transforms prototype averaging into a reinforcement learning problem, with well-articulated mathematical objectives and proof. \n\n3. Comprehensive Experiments and Solid Results: RLAP-CLIP is benchmarked on a wide range of datasets, showing consistent improvement over state-of-the-art baselines. Additionally, a stepwise ablation is provided, attributing improvements to each module Hyperparameter sensitivity analysis is also conducted to verify the robustness of the framework."}, "weaknesses": {"value": "1. Lack of Details on Prototype Policy: The paper provides some equations but is somewhat vague regarding the policy network architecture itself, such as policy hyperparameters and normalization details.\n\n2. Limited Analysis of Scalability: While RLPO's theoretical soundness is established, discussion of potential computational bottlenecks (especially for large-scale, real-world continual learning) is lacking. For example, how does RLPO's policy network scale for hundreds/thousands of classes and when data distributions heavily shift?\n\n3. Limited Task Scenarios: The experimental setup follows class-incremental protocols with a fixed number of exemplars per class, but there is minimal exploration of memory constraints or more severe task shift scenarios (e.g., open-world settings). Additionally, the paper could benefit from presenting qualitative failure cases or edge conditions."}, "questions": {"value": "1. Can the authors clarify policy network architecture details for RLPO (exact layer sizes, normalization)? How does the policy adapt when exemplar set sizes are very small, or as class counts grow?\n\n2. Are there task types (e.g., language-driven tasks) where visual or dual-modal prompting hurts? Figure 2 and Table 2 suggest continuous improvement, but are there more fine-grained trade-offs?\n\n3. Could you contextualize the paper against more recent, related works that were not discussed, such as [1] which introduces a mixture-of-experts network for improved sample efficiency in visual RL, and [2], which proposes strategies for continual learning also using RL?\n\nReferences:\n\n[1] Huang, S., Zhang, Z., Liang, T., \"MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning\" (2025)\n\n[2] Liu, Z., Fu, G., Du, C., \"Continual Reinforcement Learning by Planning with Online World Models\" (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bg5qaqOAu0", "forum": "rMHZfCznhZ", "replyto": "rMHZfCznhZ", "signatures": ["ICLR.cc/2026/Conference/Submission8836/Reviewer_LqUL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8836/Reviewer_LqUL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839256627, "cdate": 1761839256627, "tmdate": 1762920605632, "mdate": 1762920605632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel reinforcement learning based framework, RLAP-CLIP, for improving vision-language models on continual image classification tasks. They introduce three different components 1) RLPO to mitigate prototype degradation in continual classification tasks, difficulty-aware cross-modal fusion for better cross-modality integration for training, and enhanced dual-modal prompting to resolve modality imbalance. The authors present results on a variety of image classification tasks to support their claims and also provide theoretical guarantees to strengthen them."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow, with each design choice for RLAP-CLIP clearly explained.\n2. The authors show a comparison with a variety of past approaches that strengthen their work.\n3. I also like that they clearly introduced the problem first by showing how forgetting increases in vision-language models as tasks increase and conventional averaging-based approaches are not ideal to resolve this."}, "weaknesses": {"value": "1. The effect of dual modal prompting in forming better compact clusters for each class is difficult to see. Can you provide some quantitative measure of how effective the correct cluster formation is?\n2. The idea of normalized advantages and comparing between intra-class and inter-class seems quite similar to GRPO [1] reward optimization. What is the novelty in RLPO, and how does it compare with this RL fine-tuning approach?\n3. How do other dual prompting compare with other parameter-efficient finetuning approaches like LoRA, Prefix tuning?\n4. Also, the proposed framework is limited to classification tasks, whereas other methods like C-CLIP work on a variety of different tasks, like retrieval and captioning. This limits the generalizability of RLAP-CLIP.\n5. Please add some qualitative examples from the datasets used for benchmarking that help understand the advantage of RLAP-CLIP better.\n\n[1] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"}, "questions": {"value": "Please refer to the questions raised above in the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hZQOzUIgMv", "forum": "rMHZfCznhZ", "replyto": "rMHZfCznhZ", "signatures": ["ICLR.cc/2026/Conference/Submission8836/Reviewer_vX6H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8836/Reviewer_vX6H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923191146, "cdate": 1761923191146, "tmdate": 1762920604711, "mdate": 1762920604711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RLAP-CLIP, a continual multimodal learning framework that (i) replaces passive prototype averaging with reinforcement-learning-based prototype optimization (RLPO), (ii) incorporates dual-modal prompting for visual and textual inputs, and (iii) employs difficulty-aware mixture-of-experts (MoE) routing. Experiments on eight datasets show consistent improvements in both average and final accuracy over strong CLIP-based baselines, with ablation studies confirming the contribution of each component."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The prototype drift argument is convincing, and the analysis figures clearly show how simple averaging causes performance degradation.\n\n- Framing prototype construction as a reinforcement learning weighting problem, where rewards promote intra-class cohesion and inter-class separation, offers an interesting new perspective. The use of KL regularization toward a reference policy provides reasonable stability.\n\n- The study demonstrates that visual prompts are important in continual learning and that dual-modal prompting outperforms text-only and visual-only approaches."}, "weaknesses": {"value": "- RLPO introduces a policy network and reward normalization, while MoE adds routing and a deeper hard path. However, the paper does not provide a clear comparison of training time, inference latency, or FLOPs and parameter counts against the baselines under the same hardware and memory conditions, which is crucial for evaluating methods in continual learning settings.\n- The paper focuses on class-incremental classification, However, it remains unclear how RLPO and MoE would perform in task-agnostic or open-world scenarios involving unknown classes, or multimodal image–text continual learning.\n- The results depend on exemplar buffers of 20 samples per class. Please evaluate performance under smaller memory budgets or exemplar-free settings (e.g., with synthetic replay) to demonstrate the robustness of RLPO."}, "questions": {"value": "- How do results change with 10/5/0 exemplars per class? Could RLPO operate with pseudo-exemplars (e.g., feature replay) instead of images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uKkYm2Ti4", "forum": "rMHZfCznhZ", "replyto": "rMHZfCznhZ", "signatures": ["ICLR.cc/2026/Conference/Submission8836/Reviewer_hrcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8836/Reviewer_hrcE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8836/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984777607, "cdate": 1761984777607, "tmdate": 1762920604260, "mdate": 1762920604260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}