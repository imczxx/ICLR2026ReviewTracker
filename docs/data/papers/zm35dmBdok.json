{"id": "zm35dmBdok", "number": 16674, "cdate": 1758267523789, "mdate": 1759897225884, "content": {"title": "FOLD: Fast Correct Speculative Decoding", "abstract": "Speculative decoding accelerates Large Language Model (LLM) inference by using a small, fast 'draft' model to propose tokens that a larger 'target' model then verifies in a single, parallel step. While this paradigm has become the standard for high-throughput inference, the community's focus has been almost entirely on a single metric: maximizing the acceptance rate of drafted tokens. We argue this is a critical oversight. The true bottleneck is not just acceptance, but the catastrophic computational cost of rejection. A single rejected token triggers a cascading failure, discarding all subsequent work and nullifying potential gains.\nWe introduce Fast cOrrect specuLative Decoding (FOLD), a framework that fundamentally reframes the problem from merely avoiding rejection to instantly recovering from it. FOLD transforms the verification step itself. Instead of a simple pass/fail check, our novel verifier uses an integrated Early Exit module to proactively generate high-probability alternative sequences in parallel. When the primary draft fails, FOLD doesn't discard the computation; it seamlessly pivots to a pre-computed, correct path. This turns a catastrophic failure into a minor course correction, salvaging the entire speculative branch. Extensive experiments show that by treating rejection as an opportunity for correction, not a point of failure, FOLD achieves up to a 4.09$\\times$ speedup over Auto Regression decoding, setting a new bar for inference efficiency.\nWe anonymously open-source our project at https://anonymous.4open.science/r/iclr26-fold.", "tldr": "We introduce FOLD(Fast cOrrect specuLative Decoding) to accelerate the inference speed of Large Language Models (LLMs) by fast correcting wrong token.", "keywords": ["speculative decoding", "early exit", "inference acceleration", "large language models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fae8942845b413566829584ce76e21281d214e19.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "FOLD is a parallel speculative decoding framework. At the start of a given step, there are possible unverified tokens left over from a previous step. The draft model continues generation from these unverified tokens, while the target model with an early stopping module generates alternatives to the unverified tokens. Next, the full target model continues with verification of the unverified tokens while the draft model continues generation from its own branch (building on top of the unverified tokens) and branches from the early stopped target model (which are alternatives to the unverified tokens). When the full target model finishes verification, the branch with the most fully verified tokens is selected for the next round. Note that that branch will have some tokens not yet verified by the full model (i.e., those generated by the draft model in this round). The authors build FOLD into the PEARL speculative decoding framework with Kangaroo’s early stopping module, and show speedups.\n\nI had difficulty understanding the presentation of the method, so the above is a best-effort summary of the work. If I have misinterpreted the work, please let me know."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea is an effective combination of Kangaroo’s method of early stopping as a draft model, PEARL’s method of parallelizing drafting and verification, and tree-based speculative decoding methods maintaining multiple drafts at once.\n- Results are strong, showing reasonably consistent speedups compared to baselines across a variety of tasks."}, "weaknesses": {"value": "The main weakness of the paper is its poor presentation, which makes it difficult to understand and evaluate. For example:\n- Lines 58-64 in the intro (“Specifically, FOLD extends … during Final Verify”) do little to explain what the four stages do and discuss concepts before they are introduced (e.g., what constitutes a “verification result”, where do branches come from, how are branches accepted).\n- Figure 1 was confusing and did not help me understand the method. The legend was confusing (What’s a “correct wrong draft token”?), and I could not understand where each token came from, nor what the target tokens were. The caption was insufficient, and I did not find any references to the figure in the paper.\n- The background was also insufficient. More discussion was needed of related works in Speculative decoding (e.g., the baselines presented in the results). More discussion of Pearl was also needed as it forms one of the core experiments of the paper (though that discussion could have been moved down to Section 4).\n- Section 3.1, which describes how FOLD works, was very difficult for me to understand.\n  - First, I think discussing the combination of Early Verify and Draft Correct when applying FOLD to Pearl (lines 126-129) this early only serves to confuse the reader.\n  - The description of each stage is also unclear. The inputs, models used, methods applied, and outputs from each stage are not at all clear. For example, during Early Verify, it is not clear what tokens are being early verified. On my first read I assumed it was the output tokens from Early Draft, now I think it is unverified tokens from a previous step, but I am still not sure.\n  - Figure 3 did not help either. On first read it is unclear how each token is obtained, how branches were built, why the target model re-generated input tokens, etc.\n- Similar issues persisted into Section 4. In particular, there was essentially no explanation or discussion for Figure 4, which is a complicated figure key to understanding how FOLD was integrated into PEARL.\n- More minor formatting issues:\n  - The notations paragraph in the Background section is way earlier than it needs to be and confusingly refers to Early Draft and Draft Correct before they are introduced. Consider integrating it into section 3.1.\n  - Also in that paragraph, “farward” should be “forward”.\n  - Close quotes are used when open quotes should be used\n  - Xu & McAuley, 2023a (line 97) and Xu & McAuley, 2023b (line 101) refer to the same paper, but are cited differently\n  - In Line 160, refer to “Figure 3”, not just “3”.\n  - Double comma in line 139 (“…draft tokens , , which means…”).\nThese are just some of the presentation issues I had with the paper. I do not believe the paper is ready for publication in its current state. Significant editing and revising would be necessary to change my opinion.\n\nAnd some comments on the results section:\n- The authors characterize FOLD as a “training-free method” (line 339), but FOLD requires the training of the Early Exit Module. There is a training cost argument to make here the Early Exit Module may be much cheaper to train than a draft model, but training is still required.\n- Authors should cite the benchmarks used.\n- Experiments are restricted to Llama 3 and Llama 2. Results on models from other model providers would strengthen the results by demonstrating generalizability.\n- No confidence intervals are provided for the speedup figures, making it difficult to contextualize the significance of the speedups."}, "questions": {"value": "- Could you clarify the FOLD architecture in detail, specifying the inputs and outputs to each model at each stage, and across steps?\n- What is the exact evaluation setup you used? Did all methods have access to the same computational resources?\n- Related to this, it seems that FOLD would require more resources at any given time as both the draft model and target model need to run in parallel. Does FOLD actually end up using more GPU hours or does the faster generation offset this additional cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cBBYENk52D", "forum": "zm35dmBdok", "replyto": "zm35dmBdok", "signatures": ["ICLR.cc/2026/Conference/Submission16674/Reviewer_RFZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16674/Reviewer_RFZF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773459298, "cdate": 1761773459298, "tmdate": 1762926730816, "mdate": 1762926730816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FOLD, a novel method that enhances the speculative decoding process. It refines the two fundamental phases of speculative decoding—draft and verify—into a more granular four-step procedure: early draft, early verify, draft correct, and final verify. This refined, four-stage design is central to the method's effectiveness. It enables a subset of the tokens produced during the initial early draft phase to be supplemented and corrected using partial information from the target model, thereby substantially improving the quality of the draft tokens. Empirical results demonstrate the efficacy of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The central idea of this paper is both novel and sound, and the experimental results validate the effectiveness of the proposed methodology.\n2. The paper is well-written, and the figures and tables are clear and easy to follow."}, "weaknesses": {"value": "1. I observe that all experimental results were derived using the 70B large-scale model. I am curious whether the proposed FOLD method remains applicable and effective at smaller model scales, such as 7B or 14B.\n2. The comparison between FOLD and other baseline methods (like vanilla SD) appears to be potentially unfair, as the latter seemingly did not utilize the tree draft mechanism while FOLD did. A portion of FOLD's observed speedup directly stems from the tree draft. To strengthen the persuasiveness of the results, I recommend the authors enable the draft tree mechanism for all compared methods and then re-evaluate the performance accordingly.\n3. Regarding the results presented in Table 2, I suggest the authors include the speed and the corresponding speedup data for Auto Regression decoding, similar to what is shown in Table 1. This would offer a more complete analysis and comparison of the results.\n4. The theoretical speedup of this work is contingent upon a meticulously designed parallel mechanism. However, the authors' open-sourced repository seems only to contain the debug code for FOLD, omitting the implementation of the crucial parallel acceleration component. I kindly request the authors to either provide or clearly indicate the implementation details and location of FOLD's core parallel mechanism within the existing open-source code."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MjcbU2M178", "forum": "zm35dmBdok", "replyto": "zm35dmBdok", "signatures": ["ICLR.cc/2026/Conference/Submission16674/Reviewer_R4Ma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16674/Reviewer_R4Ma"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971511148, "cdate": 1761971511148, "tmdate": 1762926730399, "mdate": 1762926730399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FOLD (Fast cOrrect specuLative Decoding), a novel framework that improves the efficiency of speculative decoding in Large Language Models by mitigating the high cost of token rejection. Unlike traditional methods that discard all computation after a rejected token, FOLD introduces a four-stage process—Early Draft, Early Verify, Draft Correct, and Final Verify—that proactively generates and verifies multiple alternative sequences using an Early Exit Module and Tree Attention. This allows the system to recover from errors without losing progress, turning rejection into correction. Compatible with existing approaches like Pearl, FOLD achieves up to a 4.09× speedup over standard auto-regressive decoding, marking a significant advancement in inference performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Reframes speculative decoding by treating token rejection as an opportunity for rapid correction instead of failure to improve efficiency.\n- Introduces a four-stage framework with an Early Exit Module and Tree Attention, enabling parallel correction and preserving useful computation.\n- Demonstrates up to a 4.09× speedup over standard auto-regressive decoding and is easily adaptable to existing methods like Pearl, proving both performance and practicality."}, "weaknesses": {"value": "- Some methodological details, such as hyperparameter choices, are insufficiently explained, making the approach harder to reproduce or tune.\n- The proposed Tree Attention mechanism, while innovative, may introduce additional memory overhead during inference, potentially limiting scalability on resource-constrained hardware. \n- The experiments focus on short-context datasets, leaving uncertainty about FOLD’s effectiveness and efficiency on longer or more complex tasks, which limits the demonstrated generalizability of the approach."}, "questions": {"value": "- On line 186, there is “γ ×K + 1 draft branches”, would that be “γ ×k + 1”? Can you explain more on this?\n- In the tree attention part, multiple branches will be generated, which increases the GPU memory usage/accesses at inference time. Considering the decoding stage in LLM inference, which is already memory-bound, will this tree attention further induce bottlenecks on limited bandwidth? Can you explain qualitatively or quantitatively?\n- GSM8K and HumanEval are used in the experiment section. Does FOLD also work on datasets with longer prompt lengths, such as arxivSummary or other similar datasets, to further demonstrate the effectiveness of FOLD under different workloads?\n- How does FOLD decide what values to be assigned for γ1 and γ2 at inference time? (i.e, does FOLD use fixed values for both across different prompts or does FOLD dynamically change values) For example, given two prompts of different difficulty levels (one is easy to answer/generate results, while the other is not), I assume the draft model in the Early Draft phase may be expected to generate different numbers of tokens (different γ1). Can you elaborate more on this point qualitatively or quantitatively?\n\n\nMinor typos:\nLine 88-89, farward -> forward\nLine 459, parameterkindirectly -> parameter k indirectly\nLine 461, differentkvalues -> different k values\nLine 461, parameterkincreases -> parameter k increases\nLine 464, increasingkto -> increasing k to"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtxur6OuHs", "forum": "zm35dmBdok", "replyto": "zm35dmBdok", "signatures": ["ICLR.cc/2026/Conference/Submission16674/Reviewer_qNND"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16674/Reviewer_qNND"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978358502, "cdate": 1761978358502, "tmdate": 1762926729737, "mdate": 1762926729737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}