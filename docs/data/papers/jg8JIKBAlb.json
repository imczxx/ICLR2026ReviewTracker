{"id": "jg8JIKBAlb", "number": 11283, "cdate": 1758195232363, "mdate": 1759897596233, "content": {"title": "MoSA: Mosaic Shared Adaptation of Large Language Models", "abstract": "We introduce MoSA, a new parameter-efficient fine-tuning (PEFT) method that replaces low-rank factorization with randomized, fine-grained sharing of weight updates. Each adapted weight matrix is constructed by broadcasting a small set of learned scalars over a fixed tessellation, a pre-defined group assignment of weight entries of the weight matrix, producing expressive changes under the same parameter budget as low-rank adaptation (LoRA). MoSA requires no architectural changes and can be merged into the base model for zero-overhead inference. Across diverse language understanding and generation tasks, MoSA matches or surpasses strong PEFT baselines under strictly matched budgets. Analyses and ablations indicate that non-local parameter sharing acts as an effective regularizer, and that grouping design and budget allocation govern the expressivity–efficiency trade-off. These results position MoSA as a simple, scalable alternative to LoRA.", "tldr": "", "keywords": ["Parameter-efficient fine-tuning", "Large Language Model", "Adaptation", "Transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c8e1f1db003ce0d9a95bc3210cd8477ae8db1c2.pdf", "supplementary_material": "/attachment/d51858b2b11d5869f4b5bf0f216300658ef04f06.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces MOSA, a novel PEFT method that replaces LoRA's low-rank constraint with randomized, fine-grained parameter sharing. MOSA learns a single scalar update for pre-defined, size-balanced groups of weights (tessellations), enabling high-rank updates with no architectural changes or inference overhead. This balanced sharing acts as a strong regularizer, allowing MOSA to consistently outperform strong baselines like LoRA and HiRA in commonsense reasoning, dialogue, and out-of-distribution math tasks under matched budgets. The method is extremely parameter-efficient, notably surpassing a LoRA r=32 baseline while using only an r=1 equivalent budget."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Superior Performance and Generalization: MOSA consistently outperforms strong PEFT baselines like LoRA, DoRA, and HiRA across diverse benchmarks, including commonsense reasoning and dialogue. Its advantage is particularly pronounced in out-of-distribution generalization, where it shows a superior ability to learn abstract reasoning principles.\n- Extreme Parameter Efficiency: The method is highly efficient, achieving strong results at a fraction of the parameter cost. It can surpass the performance of much larger, well-established baselines while using only a very small budget, suggesting its sharing mechanism is a more effective use of parameters.\n- Flexibility and Simplicity: It offers a simple alternative to LoRA by replacing the low-rank constraint with fine-grained parameter sharing. This design enables expressive, high-rank updates and provides highly granular budget control, as the number of trainable groups is not constrained by matrix shapes.\n- Inference and Training Efficiency: MOSA requires no architectural changes and can be losslessly merged into the base model, resulting in zero-overhead inference. For training, it introduces a highly optimized segmented-reduction backward kernel that significantly accelerates gradient computation compared to standard autograd and scales robustly."}, "weaknesses": {"value": "- `Confusing Terminology`: The term \"Mosaic\" fails to adequately reflect the method's core concept. I noted from the supplementary code that it was apparently previously named \"Group-Share Adaptation.\" Clarification on the reason for this change is crucial for understanding the method and the paper's authenticity.\n- `Limited Comparison`: The Commonsense Reasoning scenario employs a multi-task setup, yet the paper lacks a comparison against the wide range of existing MoE-based multi-task LoRA variants [1-3].\n- `Unclear Presentation`: The paper's presentation is poor. For instance, Table 1 (referenced in Section 2) includes methods like DoRA that are never introduced, making the paper appear disjointed. The methodology section is inundated with formulas that feature arbitrary and unprofessional subscript notation. Furthermore, Figure 1 consumes excessive space while failing to convey sufficient methodological detail. The clarity of the paper's presentation is essential, and I will determine whether to update the final score based on the revised version.\n\n[1] When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications\n\n[2] HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning\n\n[3] CoLA: Collaborative Low-Rank Adaptation"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G5At0kAi3j", "forum": "jg8JIKBAlb", "replyto": "jg8JIKBAlb", "signatures": ["ICLR.cc/2026/Conference/Submission11283/Reviewer_Vc9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11283/Reviewer_Vc9B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415453359, "cdate": 1761415453359, "tmdate": 1762922433996, "mdate": 1762922433996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSA (Mosaic Shared Adaptation), a new parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) that replaces LoRA’s low-rank decomposition with randomized, fine-grained parameter sharing. Instead of factorizing weight updates, MoSA partitions each weight matrix into a fixed number of groups (tesserae) and assigns a single learnable scalar per group, broadcast across all elements within it. This design enables high-rank expressivity under the same parameter budget as LoRA while maintaining full architectural compatibility and zero-overhead inference. The authors also propose an efficient segmented-reduction backward kernel that aggregates gradients per group without atomics, providing substantial training speedups. Empirically, MoSA is evaluated on commonsense reasoning (8 datasets), open-domain dialogue (ConvAI2), and mathematical reasoning (MetaMathQA→GSM8K) using Llama-2-7B and Llama-3-8B. Across all tasks, MoSA outperforms LoRA, DoRA, MoRA, and HiRA under matched parameter budgets (≈0.7–0.8% of model parameters).  A first-order theoretical analysis shows that balanced random tessellations minimize expected gradient discrepancy, formalizing why random grouping works as a regularizer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptual Novelty: MoSA breaks the low-rank assumption dominating PEFT (LoRA, DoRA) by proposing non-local, groupwise constant updates, a different structural prior.\n\n2. Strong Empirical Evidence: The paper benchmarks across three families of tasks (reasoning, dialogue, OOD generalization) and two strong base models (Llama-2-7B, Llama-3-8B), all under strict budget parity. Results are consistent and robust.\n\n3. Thorough Ablations: The study systematically probes grouping strategy, component selection, and budget scaling, reinforcing"}, "weaknesses": {"value": "1. Limited Theoretical Depth: The analysis in §3.1 (Theorem 1, in appendix) stops at first-order approximation. It does not address the *optimization dynamics* or convergence implications of groupwise sharing.\n\n2. Potential Overclaim on High-Rank Expressivity: While empirically strong, the claim that MoSA can realize “high-rank updates” under small budgets is not theoretically substantiated (no explicit rank analysis of ∆W(λ)).\n\n3. Lack of Comparison on Memory/Latency Trade-offs: The paper claims “zero-overhead inference,” but training-time memory and backward compute overhead are only partially reported (speed benchmarks but no GPU memory profiling).\n\n4. Missing Larger-Scale or Real-World Fine-tuning Tasks: Only medium-scale reasoning and dialogue datasets are used; domain adaptation tasks (e.g., summarization, instruction tuning) are absent.\n\n5. Presentation Minor Issues: The flattened notation and symbol reuse (e.g., Mh, Gh, g, π) are hard to follow for readers unfamiliar with vectorized representations."}, "questions": {"value": "1. Expressivity Justification: Can the authors provide empirical evidence (e.g., singular value spectra) to demonstrate that MoSA indeed produces higher-rank ∆W updates than LoRA or DoRA at the same budget?\n\n2. Gradient Projection Proof: Theorem 1 claims near-optimality of balanced tessellations; can the authors formalize this derivation in the main text and explain how it generalizes beyond first-order updates?\n\n3. Ablation Granularity: How sensitive is MoSA’s performance to the random seed of tessellation assignment? Is grouping deterministic per layer or re-sampled per run?\n\n4. Training Efficiency: What are the memory and time trade-offs versus LoRA for large-scale training (e.g., Llama-3-70B)? Is the backward kernel still efficient when scaling to multi-GPU distributed setups?\n\n5. Interoperability: Could MoSA be combined with quantization (like QLoRA) or low-rank initialization (hybrid approaches)? Would grouping interact with quantized layers?\n\n6. Generalization Analysis: Why does MoSA achieve such a large margin (+7% on GSM8K)? Any insights into whether the tessellation acts as regularization, or is this due to implicit gradient smoothing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kxJ8VrTWKg", "forum": "jg8JIKBAlb", "replyto": "jg8JIKBAlb", "signatures": ["ICLR.cc/2026/Conference/Submission11283/Reviewer_xciz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11283/Reviewer_xciz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507947409, "cdate": 1761507947409, "tmdate": 1762922433422, "mdate": 1762922433422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSA, a parameter-efficient fine-tuning method that replaces low-rank adaptation with fine-grained, randomized sharing of weight updates. Each weight matrix is adapted by broadcasting a small set of learned scalars over a fixed tessellation, enabling expressive updates without changing the model architecture and allowing zero-overhead inference. Experiments on diverse language understanding and generation tasks show that MoSA matches or exceeds strong PEFT baselines. Analyses indicate that non-local parameter sharing acts as a regularizer, and design choices in grouping and budget allocation control the expressivity–efficiency trade-off, positioning MoSA as a simple, scalable alternative to LoRA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is innovative and interesting.\n2. The paper is well written and easy to follow.\n3. The approach achieves strong empirical results, outperforming or equaling established PEFT baselines on diverse language modeling and generation benchmarks."}, "weaknesses": {"value": "1. The work lacks theoretical analysis to explain why the proposed fine-grained, randomized weight-sharing mechanism is effective, leaving the underlying principles largely empirical.\n2. The experiments are conducted only on the LLaMA series. Including evaluations on additional models, such as Qwen, would strengthen the empirical validation of the method.\n3. MoSA is implemented only for linear layers and cannot be directly applied to convolutional layers, which may limit its generality across different model architectures.\n4. How is the hyperparameter k controlled so that MoSA uses the same number of trainable parameters as LoRA with rank  r=32?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k7lYLFtLJ2", "forum": "jg8JIKBAlb", "replyto": "jg8JIKBAlb", "signatures": ["ICLR.cc/2026/Conference/Submission11283/Reviewer_xCoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11283/Reviewer_xCoM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839650930, "cdate": 1761839650930, "tmdate": 1762922433025, "mdate": 1762922433025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSA, a PEFT method that constructs an adapted weight matrix by broadcasting a small set of learned scalars over a pre-defined group assignment of weight entries. MoSA features high-rank expressivity, arbitrary granularity, and non-local parameter-sharing regularization. MoSA is also coupled with efficiency techniques like Segmented Reduction and pre-caching to speed up training.              Experiments demonstrate MoSA’s performance advantage over LoRA, DoRA, and high-rank methods like MoRA and HiRA under the same updated parameter budget."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel PEFT method, first grouping original weight entries, then fine-tuning them with scaling parameters.                   This design ensures a high-rank update during fine-tuning.\n2. MoSA’s realization is theory-grounded. The paper offers a theoretical view supporting its Balanced Random Tessellation design.\n3. Performance improvement over baselines is significant."}, "weaknesses": {"value": "1. Additional memory cost and latency. Since MoSA needs to precompute and cache (g, π, m, o) for each unique layer shape, it means MoSA must store more than two extra full-dimensional matrices, which are not present in methods like LoRA. Additionally, although the gather operation is optimized, it is still a slower operation than LoRA’s matrix multiply. This paper doesn’t include a direct speed comparison with LoRA.\n2. Lacks stronger baselines. Stronger methods (e.g., LoRA-GA, LoRA-pro, PiSSA, HD-PiSSA…) are not compared. DoRA, MiRA, and HiRA are not strong baselines.\n3. No updated rank analysis, which cannot support the paper’s high-rank expressivity claim."}, "questions": {"value": "1. Could the authors provide the speed and memory comparison with LoRA?]\n2. See other weaknesses above.\n3. Is MoSA more sensitive to the learning rate or not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V4mGI5VtYU", "forum": "jg8JIKBAlb", "replyto": "jg8JIKBAlb", "signatures": ["ICLR.cc/2026/Conference/Submission11283/Reviewer_JoHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11283/Reviewer_JoHz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980045793, "cdate": 1761980045793, "tmdate": 1762922432695, "mdate": 1762922432695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}