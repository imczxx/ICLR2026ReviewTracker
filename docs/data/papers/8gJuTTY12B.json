{"id": "8gJuTTY12B", "number": 19947, "cdate": 1758300872067, "mdate": 1759897011082, "content": {"title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch", "abstract": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. To quench this thirst, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch—an expansive reservoir of texts with rich and diverse private information—designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.2 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical records, legal documents, financial records, calendars, emails, meeting transcripts, and text-messages with a total of 44 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that recursively decomposes texts and applies targeted sanitization. Our compact sanitization models ($\\leq$4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B.", "tldr": "We present Privasis, the first million-scale synthetic dataset built from scratch with rich private information.", "keywords": ["synthetic data", "privacy", "sanitization"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc09cab4090407d2c671af97de0a2c009b41def5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a synthetic data set containing fake private data (names, dates, locations etc).   A second dataset is then generated by sanitising the first dataset, an LLM is then trained on this sanitised dataset and its ability to sanitise text evaluated on held-out data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is nicely written, and identifies the need for larger and more diverse datasets for privacy work."}, "weaknesses": {"value": "The data generation process lacks detail and justification (little evidence is given to support that the dataset is in fact representative of real-world datasets, and so useful).  The sanitisation process is is almost entirely ad hoc - there is a long history in the privacy literature of apparently reasonable ad hoc sanitisation methods being proposed and later shown to provide litte if any privacy, so this is not a minor weakness.  The evaluation of privacy, and indeed utility, is similarly ad hoc and unsatisfactory.  Since these issues are core to the contribution of the paper, and not easy to fix, I think this one must be a \"reject\"."}, "questions": {"value": "See my comments on the papers weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AO4jPwdVGw", "forum": "8gJuTTY12B", "replyto": "8gJuTTY12B", "signatures": ["ICLR.cc/2026/Conference/Submission19947/Reviewer_hrp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19947/Reviewer_hrp9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760955798593, "cdate": 1760955798593, "tmdate": 1762932119631, "mdate": 1762932119631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRIVASIS, a million-scale synthetic dataset containing diverse privacy-sensitive text records generated entirely from scratch using LLMs. The authors employ auxiliary control variables and diversity-preserving iterative refinement to create 1.2M records spanning medical, legal, financial, and communication domains with 44M annotated attributes. They construct PRIVASIS-SANITIZATION, a parallel corpus for text sanitization using a decomposition-based pipeline, and train compact models (≤4B parameters) that outperform frontier LLMs like GPT-5 on sanitization tasks. The work addresses a fundamental data scarcity problem in privacy research while avoiding real-world reference data dependencies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The auxiliary control variable approach combined with Vendi score-based diversity preservation is elegant and well-motivated. \n* With 1.2M records, 44M attributes, and coverage across 10 primary domain categories, PRIVASIS represents an orders-of-magnitude leap over existing privacy datasets. The hierarchical categorization into 42 subcategories and support for contextual, instruction-based sanitization beyond fixed PII categories addresses real limitations in current privacy-preserving approaches\n* The decomposition-based sanitization pipeline is well-designed for handling long documents, and the hierarchical evaluation framework (direct/inference/proximity leaks) provides nuanced assessment"}, "weaknesses": {"value": "* The paper omits several important text-to-text privatization baselines [1,2,3]. It would be beneficial to discuss these works and, if feasible, include them as additional baselines for comparison.\n* It would also strengthen the paper if the authors considered multiple threat models, such as the Static and Adaptive Attacker settings described in [4].\n\n* The pipeline relies heavily on specific LLM capabilities (GPT-OSS-120B for 62.6% of records) without thoroughly investigating how synthesis quality varies across model families or scales\n\n\n\nRefs\n\n[1] Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations .WSDM 2020\n\n[2] TEM: High Utility Metric Differential Privacy on Text, SIAM, 2023\n\n[3] Locally differentially private document generation using zero shot prompting, EMNLP 2023\n\n[4] The Limits of Word Level Differential Privacy, EMNLP 2022."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MIMDjgxEcH", "forum": "8gJuTTY12B", "replyto": "8gJuTTY12B", "signatures": ["ICLR.cc/2026/Conference/Submission19947/Reviewer_YerD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19947/Reviewer_YerD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815925040, "cdate": 1761815925040, "tmdate": 1762932118588, "mdate": 1762932118588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's primary contribution is a synthetic privacy dataset on the order of millions of observations (PRIVASIS). The records in the data span a wide range of domains, e.g., medical, legal, and financial documents, and attributes, e.g., names, birthdays, and medical conditions. To generate this data, the authors leverage US government data and LLM-based pipeline. This enables the development of realistic profiles which then serve as an initial generation input for the LLMs.\n\nAfter building PRIVASIS, the authors assemble a derivative corpus of sanitized documents (SANITIZATION). Using the combined records from PRIVASIS-SANITIZATION, the authors train small LMs to carry out the sanitization task and show that these models out-perform frontier models such as GPT-5."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Large and open source: As the authors point out, there are few large-scale open-source datasets for privacy tasks (there are some datasets developed with *proprietary* methods that are on the scale of 100k observations by AI4Privacy and the dataset of Selvam and Ghosh consists of 384,789 observations). PRIVASIS, at 4x the size of Selvam and Ghosh, arguably fills a need for large-scale open data.\n- Sampling Diversity: The authors build on existing literature which seeds synthetic data generation using realistic profiles (Yukhymenko et al, NeurIPS 2024; Selvam and Ghosh 2025 (PANORAMA)). Specifically, the addition of a refinement loop which revises low-quality samples until they exceed an acceptance threshold.\n- Evaluation of Sanitization Effectiveness: An area of the paper that I really like is how the authors evaluate the effectiveness of sanitization. While many papers focus on \"direct leaks,\" i.e., a model exactly outputting a sensitive string, the authors also explore more insidious leaks that are not as immediately obvious."}, "weaknesses": {"value": "## Privacy Safety\n- A central claim is that PRIVASIS is \"privacy-safe\" because it is conditioned only on public name databases. But, ultimately LLMs are going to output content that is comparatively likely to co-occur with the provided context. Given that the dataset is generated by sampling from LLMs, the extent to which the dataset is truly privacy-safe is going to be a function of the safety of the underlying models.\n- The current test of safety relies on sampling 100 profiles and querying (presumably Google but the paper doesn't specify) whether these profiles correspond to real people\n- Given that the main contribution of the paper is a **privacy-safe** dataset, this validation is not sufficient. If we think of the leakage rate as *r*, then a typical rule of thumb for sample size *n* is that both $nr \\geq 10$ and $n(1-r) \\geq 10$. If $r=0.01$, i.e., only 1 in 100 records in PRIVASIS corresponds to a real person, then the sample needs to be at least 1,000. I recognize that if PRIVASIS is truly safe then you'd have to sample infinite records to meet this rule of thumb, so I am not suggesting it literally, but simply that a validation of 100 records is far too small.\n\n## Sanitization Effectiveness\n- A secondary contribution of the paper is that small task-specific LMs can do a better job than larger generalist models. But the authors have not explicitly optimized the sanitization pipeline of these larger models. It is entirely possible that after optimizing this pipeline with something like GEPA, the larger models will outperform the small ones.\n\n## English/US-centric\n- One of the strengths of the paper is that the profiles are seeded with realistic information from the US SSN database. The flip side of this is that the data is US-centric and does not enable research on privacy beyond English-language records."}, "questions": {"value": "## Privacy Safety Evaluation\n- How exactly was this evaluation carried out?\n- Have you considered conducting a formal audit using membership inference attacks?\n\n## Diversity of Dataset\n- In Table 2, PRIVASIS seems to provide a material lift. But can you clarify the window size used for the MATTR metric? It would be nice to see a sweep of MATTR as a function of window size to understand how important this is.\n- Related to hyperparameter choices, the Vendi score requires specifying $q$, what value are you using and did you formally evaluate the quality of the samples as a result of $q$?\n\n## Sanitization Effectiveness\n- How is the eval pipeline for inference and proximity leaks calibrated? How is consistency in this eval maintained? Have you explored the extent to which there is inter-evaluator agreement across LLM judges? My understanding is that you have used only GPT-OSS-120B. Why not use the same models that were used to carry out the sanitization?\n- Since the data is US-centric, how effective are the sanitization evals at measuring whether the sanitizer is simply re-writing the content in another language? Or a cypher?\n- Considering the poor performance of the frontier models, do you have any hypotheses for why the models fail at seemingly simple tasks? Particularly given that OSS-120B is able to detect the leaks? Would subsequent calls to the sanitizing model asking it to check its work resolve these failures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uTfjFqGgTQ", "forum": "8gJuTTY12B", "replyto": "8gJuTTY12B", "signatures": ["ICLR.cc/2026/Conference/Submission19947/Reviewer_SgVS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19947/Reviewer_SgVS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859711905, "cdate": 1761859711905, "tmdate": 1762932117774, "mdate": 1762932117774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PRIVASIS, a large, fully synthetic corpus with about 1.16 million records and 44 million annotated attributes across domains such as medical, legal, and financial. It generates diverse, realistic, and privacy-safe data using control variables and refinement without relying on real individuals, and provides a paired PRIVASIS-Sanitization dataset for text sanitization. The authors design a hierarchical evaluation for direct, inference, and proximity leaks, and train PRIVA-Cleaner models that outperform large LLMs on standard privacy tests while preserving content, demonstrating scalable and ethical data generation for privacy research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a million-scale, synthetic, privacy-rich corpus spanning many domains, with fine-grained, multi-level sanitisation targets and instructions. The paper goes beyond prior PII-span datasets by supporting removal and graded abstraction across long records and arbitrary categories.\n\nThe method is strong. It uses controlled generation with rich attribute annotation and grouping, and a rigorous hierarchical evaluation that detects direct, inference, and proximity leaks while requiring retention for a complete success record. Test sets (vanilla vs hard) are clearly separated to stress different failure modes.\n\nThe paper is easy to follow with clear dataset statistics, domain distributions, and precise metric definitions."}, "weaknesses": {"value": "The paper trains only on PRIVASIS-Sanitization and evaluates on its own test sets, without comparing the same model trained on other datasets (e.g. NAP^2 and other datasets mentioned in this paper). This makes it unclear whether PRIVASIS’s advantage comes from its design or just data scale and domain match. For LLM, in many cases, smaller, high-quality datasets could yield similar or better results like indicated in LIMA: Less Is More for Alignment\n\nProfiles are seeded from the US Social Security baby-name database, which can tilt names/demographics and social context toward the US.\n\nThe pipeline relies on LLM-based steps (sensitivity weighting, chunk relevance, span extraction, instruction generation), and the evaluation also employs an LLM. Without robust cross-evaluator checks, scores may reflect evaluator bias."}, "questions": {"value": "Could the authors train the same model on other datasets (e.g., NAP) to show that PRIVASIS’s gains come from its design, data scale or domain fit?\n\nHave they tested whether smaller, high-quality datasets, similar to LIMA’s findings, could achieve comparable results?\n\nHow do they address potential US-centric bias from using the Social Security baby-name database?\n\nSince both generation and evaluation rely on LLMs, how do they control for evaluator bias or self-agreement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Ip3EEfRGm", "forum": "8gJuTTY12B", "replyto": "8gJuTTY12B", "signatures": ["ICLR.cc/2026/Conference/Submission19947/Reviewer_Cwn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19947/Reviewer_Cwn2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19947/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983387570, "cdate": 1761983387570, "tmdate": 1762932117244, "mdate": 1762932117244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}