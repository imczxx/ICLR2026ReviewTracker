{"id": "zJm9nmoahk", "number": 8443, "cdate": 1758083840112, "mdate": 1763550088262, "content": {"title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge", "abstract": "Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for scalable reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.", "tldr": "we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge.", "keywords": ["GraphRAG", "RAG", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d5aa47999b67ce9211da4fcf7f0048dd72efd0a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LLMs reason well but are constrained by static, incomplete parametric knowledge. RAG helps by fetching external info, yet current methods falter on knowledge-intensive tasks due to fragmented sources and weak structure modeling. They propose G-reasoner, which unifies graph and language foundation models for reasoning over diverse graph-structured knowledge. Its core is QuadGraph, a four-layer abstraction that standardizes heterogeneous sources into a single graph. They also introduce a 34M-parameter Graph Foundation Model (GFM) that captures both topology and text and integrates with LLMs. With mixed-precision training and distributed message passing for scalability, experiments on six benchmarks show state-of-the-art results, stronger LLM reasoning, better efficiency, and cross-graph generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental validation of the method was well conducted based on a variety of experiments.\n\n2. A general-purpose framework that can be extended to any data type was proposed.\n\n3. It achieved higher performance compared to other baselines.\n\n4. Despite its small size of 34M, it demonstrated efficient and strong performance."}, "weaknesses": {"value": "Overall, this is a well-written paper with solid experiments. However, one weakness is that each component of QuadGraph (such as the document-level hierarchy and the knowledge graph hierarchy) integrates existing concepts rather than introducing entirely new ones, which limits its novelty. The same observation applies to the GFM component as well."}, "questions": {"value": "There is no mention of this paper’s limitations. What are some of the limitations of this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TavNnAeSUT", "forum": "zJm9nmoahk", "replyto": "zJm9nmoahk", "signatures": ["ICLR.cc/2026/Conference/Submission8443/Reviewer_voRT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8443/Reviewer_voRT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760766531363, "cdate": 1760766531363, "tmdate": 1762920332942, "mdate": 1762920332942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes G-Reasoner, a GNN foundation model-based system for unified reasoning over graph-structured knowledge.  The G-Reasoner system contains three main modules. It first constructs a hierarchical heterogeneous QuadGraph, which contains Community, Document, Knowledge Graph, and Attribute four layers. Then it devises a query-dependent GNN as the graph foundation model for relevant context retrieval. Given a question q, the model predicts whether each of the node (can be entity, document, passage, community and attribute) contribute positively to answer the question. To mitigate the label scarcity issue, the authors proposed to leverage pre-trained language models for knowledge distillation. In order to support training on large-scale graph, the authors propose mixed precision training.  Finally, the LLM reasoning step receives top-k relevant node, and use them to augment the LLM for final answer generation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe concept of a Unified QuadGraph is novel and, to the best of the reviewer’s knowledge, is introduced here for the first time.\n\n2.\tThe proposed method G-Reasoner, outperforms competitive GNN/Graph Retriever-based methods in multi-hop question answering. \n\n3.\tThe prerequisite knowledge from sentence transformers shows to be useful in guiding the GNN. \n\n4.\tThe proposed mix precision training technique achieves higher throughput with lower GPU memory usage."}, "weaknesses": {"value": "1.\tG-reasoner does not propose new graph construction methods. Instead, it reuses graph construction techniques from three baseline methods, namely HippoRAG2, LightRAG, and Youtu-GraphRAG. \n\n2.\tG-reasoner is not compared with various RAG systems, including but not limited to Search-o1 [1], Search-R1 [2], RAG-Gym[3], R1-Searcher [4], Collab-RAG [5] and RAG-Star [6], ReSearch [7], IRCoT [8]. \n\n3.\tThis manuscript and its related baselines adopt an alternative evaluation setup than the commonly-adopted Multi-hop QA evaluation setting. The commonly-adopted setup constructs the retrieval corpus with  the full Wikipedia dump [2,3,4,6,7] or all supporting plus distractor passages from all training, validation, and test questions [8]. \n\nIn contrast, as stated in section 5.1, the manuscript follows the settings used in Gutierrez et al., 2024 (HippoRAG), which “*collect all candidate passages (including supporting and distractor passages) from our selected (the 1000) questions and form a retrieval corpus for each dataset*”.  This retrieval environment is overly idealized and does not reflect real-world conditions. \n\nThe reviewer understands that constructing a graph over the full text corpus is practically challenging. However, based on the existing experimental results, one cannot conclude that: (1) the proposed graph retrieval method outperforms iterative RAG approaches (or can achieve competitive performance), and (2) the proposed G-reasoner is robust to larger graphs and applicable to open-domain question answering.\n\n[1] Li et al., Search-o1: Agentic Search-Enhanced Large Reasoning Models\n\n[2] Jin et al., Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning\n\n[3] Xiong et al., RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation\n\n[4] Song et al., R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning\n\n[5] Xu et al., Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration\n\n[6] Jiang et al., RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement\n\n[7] Xie et al., Interleaved Reasoning for Large Language Models via Reinforcement Learning\n\n[8] Trivedi et al., Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"}, "questions": {"value": "1. The reviewer suggests the authors to add the following experiments:\n\na)\tSince the performance reported in Table 2 is not directly comparable to the multi-hop QA results in [1–8], re-evaluating some baselines on the current evaluation knowledge corpus (same subset of distractive passages) may help clarify the advantages and necessities of the proposed graph search over iterative dense/sparse document retrieval.\n\nb)\tScaling-up the scope for the document corpus used in evaluation. For example, the MuSiQue dataset only contains 25k questions. Using all supporting documents can be less challenging than the 2Wiki dataset. This may help in validating the effectiveness of the proposed method when being adapted to real-world scenarios.\n\n2. [Minor] Apart from providing references for graph construction solutions, the reviewer suggests the author to include a short description to introduce how we can construct the unified graph in this paper. \n\n3. For the HotpotQA, MuSiQue, and 2Wiki datasets, *performance can vary substantially under different evaluation settings* (such as open-domain, distractor, etc.)  To avoid confusion, apart from providing relevant references, the reviewer suggests the authors to explicitly state the retrieval scope for evaluation. It can be included in the appendices if the main sections do not have enough space.\n\nThe reviewer is willing to raise the score if the aforementioned issues are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2CoWKEbPQK", "forum": "zJm9nmoahk", "replyto": "zJm9nmoahk", "signatures": ["ICLR.cc/2026/Conference/Submission8443/Reviewer_jHd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8443/Reviewer_jHd3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761081726513, "cdate": 1761081726513, "tmdate": 1762920332553, "mdate": 1762920332553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes G-reasoner, a novel framework integrating graph and language foundation models for reasoning over graph-structured knowledge. The authors introduce QuadGraph, a unified graph abstraction, and a 34M-parameter Graph Foundation Model (GFM) powered by Graph Neural Networks (GNNs). The paper is theoretically sound and demonstrates strong experimental performance across six benchmarks, including QA datasets as well as domain-specific benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Novel Contribution:**\n\nThe QuadGraph abstraction is a promising innovation that unifies different types of graph structures, such as knowledge graphs and document graphs, into a single standardized representation. This is a key step toward addressing the challenge of generalizing reasoning models across diverse graph types.\n\nThe integration of GNN-based reasoning into a graph foundation model that leverages both graph topology and textual semantics is a novel approach that aligns well with current trends in deep learning and knowledge representation.\n\n**Strong Empirical Results:**\n\nG-reasoner consistently outperforms state-of-the-art methods, including graph-enhanced techniques like GraphRAG and HippoRAG, in multi-hop reasoning tasks across diverse domains.\n\nThe approach demonstrates strong cross-graph generalization, which is critical for real-world applications involving diverse knowledge domains.\n\n**Efficient Training and Reasoning:**\n\nThe use of mixed-precision training and a distributed message-passing mechanism allows the model to scale effectively across large datasets and graph structures, ensuring both efficiency and performance."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "**Model Complexity and Interpretability:**\n\nThe GFM model, while powerful, could benefit from further discussion on its interpretability. Complex models like GNN-based foundation models often suffer from interpretability issues, making it hard to explain the reasoning behind predictions. This is particularly important for high-stakes applications such as legal | medical | safety reasoning.\n\n**Lack of Comparison with State-of-the-Art GNN Methods:**\n\nWhile the model is compared with existing graph-enhanced RAG methods, there is no direct comparison with state-of-the-art GNN-only models or pure graph models that don’t involve retrieval-augmented generation. This comparison could provide clearer insight into the unique benefits of GFM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LE8EJRdX7n", "forum": "zJm9nmoahk", "replyto": "zJm9nmoahk", "signatures": ["ICLR.cc/2026/Conference/Submission8443/Reviewer_gBkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8443/Reviewer_gBkL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830967185, "cdate": 1761830967185, "tmdate": 1762920332209, "mdate": 1762920332209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces G-Reasoner, a framework designed to unify graph reasoning and language reasoning by integrating Graph Foundation Models (GFMs) with Large Language Models (LLMs). The core idea is that while LLMs are powerful reasoners, they lack structural awareness; conversely, graph structures effectively represent relational knowledge but are underutilized in LLM reasoning. To address this, the authors propose three components: QuadGraph, Graph Foundation Model (GFM),  and a LLM-enhanced reasoning.\nExperiments on six benchmarks show consistent improvement over state-of-the-art methods such as GraphRAG, HippoRAG, and GFM-RAG."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduce clear motivation and organization. The introduction frames the LLM-graph reasoning gap well. I also like the figures and tables that effectively summarize the pipeline and results.\n2. The paper proposes a four-layer unified graph schema, which is elegant and general enough to cover multiple graph types (KGs, document graphs, hierarchical graphs).\n3. The paper also conduct comprehensive evaluation. Benchmarks include both general QA datasets and domain-specific GraphRAG benchmarks. The ablations also clearly demonstrate component contributions. Efficiency experiments are also included."}, "weaknesses": {"value": "1. The paper has an incremental novelty. The core idea (combining GNN-based reasoning with LLM retrieval) closely follows existing works like GFM-RAG and GNN-RAG; the contribution feels like a well-engineered unification rather than a conceptual breakthrough.\n2. The overall methodological depth is limited. For example, the GFM architecture largely reuses standard DistMult-style message passing and MLP updates without introducing new graph learning techniques. The “QuadGraph” abstraction appears more of a schema design than a learnable innovation.\n3. The paper can benefits from some interpretability analysis or case-study. The paper shows gains but does not deeply analyze why G-Reasoner helps LLM reasoning or how graph structure contributes. A case-study with visualization of reasoning paths showing the predicted top-k scores from the graph will be great."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KPDmkoYPVt", "forum": "zJm9nmoahk", "replyto": "zJm9nmoahk", "signatures": ["ICLR.cc/2026/Conference/Submission8443/Reviewer_qUas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8443/Reviewer_qUas"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967050012, "cdate": 1761967050012, "tmdate": 1762920331806, "mdate": 1762920331806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thorough and thoughtful reviews of our paper. We are grateful for the positive feedback and constructive suggestions provided, which have helped us improve the clarity and quality of our work. We have carefully revised the paper according to the comments, and the edits have been highlighted in **BLUE**. We also provide a detailed response to each comment below. Here we highlight our major revisions, and respond to each reviewer below. We hope our responses can properly address your concerns.\n* **Novelty and Contributions** (Reviewer qUas, jHd3, voRT): We want to clarify that our main contribution lies in proposing a **unified framework** that enables scalable training and generalized reasoning over **diverse graph-structured knowledge**. We have added detailed discussions in the general response and in our response to Reviewer qUas and voRT.\n* **Interpretability Analysis and Case Studies** (Reviewer qUas, gBkL): We have added detailed path-based interpretability case studies in **Table 16 at Appendix D.5** of the revised paper, demonstrating how GFM traces multi-hop reasoning paths to explain its reasoning.\n* **Comparison with Advanced Multi-step RAG Systems** (Reviewer jHd3): We have added comparison results with advanced multi-step RAG systems (IRCoT, R1-searcher, Search-R1) in **Table 10 of Appendix D.2** of the revised paper, showing G-Reasoner’s superior performance.\n* **Evaluation on Larger Document Corpus** (Reviewer jHd3): We have conducted additional experiments on the full dev set of MuSiQue with an expanded corpus, and added the results in **Table 11,12 of Appendix D.2** of the revised paper, demonstrating G-Reasoner’s robustness in more realistic scenarios.\n* **Evaluation Settings and Graph Construction Details** (Reviewer jHd3): We have added detailed descriptions of the retrieval scope used in evaluation and graph construction process in **Appendix B, C.1** of the revised paper to clarify these aspects.\n* **Limitations and Future Work** (Reviewer voRT): We have added a dedicated section on Limitations and Future Work in **Appendix F** of the revised paper, discussing key limitations and potential future directions.\n\n## General Reply to the Novelty and Contributions\n\nWe sincerely thank the reviewers for their valuable feedback regarding the novelty and contributions of our work. We would like to respectfully clarify that our main contribution lies in proposing a **unified framework** that enables scalable training and generalized reasoning over **diverse graph-structured knowledge**. The effort to achieve such unification is non-trivial. This not only requires careful conceptual design of both the generalizable data abstraction (*QuadGraph*) and problem formulation (*GFM training and reasoning*), but also demands practical GFM architectures (*synergized reasoning over structure and semantics*), training strategies (*knowledge distillation for label sparsity*), and scalable engineering implementations (*mixed-precision training and distributed message passing*). These components work together to enable the unified framework that can generalize across different graph types and support versatile reasoning tasks.\n\nWe would also like to frame our contribution in a spirit similar to other foundational models. For example, **BERT** was built directly on standard Transformer blocks, yet it is widely recognized as a major conceptual contribution because it introduced a **unified pre-training framework** (MLM + NSP) that demonstrated the power of scaling. In a similar spirit, our contribution is not a new GNN layer, but a **unified and scalable training framework** for graph foundation models, enabled by our QuadGraph abstraction, that generalizes across diverse graph knowledge. More detailed discussions on the novelty of our work can be found in our response to Reviewer qUas.\n\nWe hope this clarifies our perspective on the paper's novelty."}}, "id": "BCrYDjh1EI", "forum": "zJm9nmoahk", "replyto": "zJm9nmoahk", "signatures": ["ICLR.cc/2026/Conference/Submission8443/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8443/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission8443/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763549841599, "cdate": 1763549841599, "tmdate": 1763549841599, "mdate": 1763549841599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}