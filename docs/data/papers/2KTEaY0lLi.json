{"id": "2KTEaY0lLi", "number": 22245, "cdate": 1758328297367, "mdate": 1759896877966, "content": {"title": "Learning Disentangled Multi-Agent World Model for Decentralized Control", "abstract": "World models enable learning policies via latent imagination, offering benefits such as history compression and sample efficiency.\nThe primary challenge in applying world models to multi-agent tasks is that modeling multi-agent dynamics in latent space requires integrating information from different agents, often creating spurious correlations between their latent states.\nExisting methods either reconstruct the observation for each agent or employ communication to maintain correlation during execution, failing to learn disentangled latent states that are crucial for effective decentralized control.\nTo address this, we present the Disentangled Multi-Agent World Model (DMAWM). The framework facilitates learning decentralized policies in the latent space through a novel architecture comprising independent agent modules and a shared environment module.\nDuring real-environment execution, agent modules independently process local information to form a factorized latent representation.\nThe environment module is then trained to mirror the factorized structure generated by the agent modules, effectively disentangling individual latent states from the interaction dynamics.\nConsequently, imaginary rollouts generated by the environment module more faithfully simulate decentralized execution dynamics, facilitating the transfer of policies from imagination to decentralized execution.\nOn three multi-agent reinforcement learning (MARL) benchmarks with both vector and visual observations, DMAWM outperforms existing model-based and model-free approaches in convergence speed and final performance, with additional visualization demonstrating its efficacy in capturing agent interactions.", "tldr": "", "keywords": ["model-based reinforcement learning", "multi-agent reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d86e76be336b29b5d9067480cc0d2c72e644a1f.pdf", "supplementary_material": "/attachment/532d7b98fb118b9b4a5d07f267e1211c9e1bd25e.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of applying world models to decentralized multi-agent settings. The authors propose the Disentangled Multi-Agent World Model (DMAWM), a framework designed to learn factorized latent representations. The architecture consists of independent agent modules that process local observations and a shared environment module that models their interactions during imagination. By training the environment module's joint prior to match the factorized posterior from the agent modules, the method aims to learn disentangled latent states. The effectiveness of DMAWM is demonstrated through experiments on SMAC, SMACv2, and Melting Pot benchmarks, where it outperforms existing model-based and model-free methods in sample efficiency and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper tackles an important problem: effectively training decentralized multi-agent policies in a sample-efficient manner using world models.\n2.  The proposed architecture, which separates independent agent modules from a shared environment module, is an intuitive approach to learning factorized representations and avoiding spurious correlations.\n3.  The proposed method consistently achieves superior performance compared to a range of model-free and model-based baselines, demonstrating impressive sample efficiency.\n4.  The paper includes a valuable ablation study that analyzes the contribution of the core components."}, "weaknesses": {"value": "1.  The primary contribution appears to be an adaptation of the DreamerV3 to the multi-agent domain by applying the Centralized Training with Decentralized Execution (CTDE) paradigm. The core components—a recurrent model, a representation model, and decoders—are standard in DreamerV1-3.\n2.  A central claim is that DMAWM learns \"disentangled\" latent states. However, the evidence provided is limited to a single visualization in Figure 4. This claim would be much stronger with more rigorous support, such as more diverse qualitative examples showing how the model handles specific agent interactions.\n3.  The related work section misses two recent model-based MARL algorithms[a-b]. A direct comparison would be crucial for accurately positioning DMAWM's contribution within the most relevant literature.\n\na. Liu, Qihan, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, and Chongjie Zhang. \"Efficient multi-agent reinforcement learning by planning.\" arXiv preprint arXiv:2405.11778 (2024).\nb. Ma, Chengdong, Aming Li, Yali Du, Hao Dong, and Yaodong Yang. \"Efficient and scalable reinforcement learning for large-scale network control.\" Nature Machine Intelligence 6, no. 9 (2024): 1006-1020."}, "questions": {"value": "1.  Could you please clarify the contribution of DMAWM beyond being a multi-agent adaptation of the Dreamer architecture?\n2.  The claim of learning disentangled latent states is a cornerstone of your work. Could you provide more compelling evidence to support this? For example, could you show latent space interpolations or provide visualizations from SMAC or SMACv2?\n3.  Why were a and b not included as a baseline? A comparison would seem essential to evaluate the state of the art.\n\na. Liu, Qihan, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, and Chongjie Zhang. \"Efficient multi-agent reinforcement learning by planning.\" arXiv preprint arXiv:2405.11778 (2024).\nb. Ma, Chengdong, Aming Li, Yali Du, Hao Dong, and Yaodong Yang. \"Efficient and scalable reinforcement learning for large-scale network control.\" Nature Machine Intelligence 6, no. 9 (2024): 1006-1020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jJWwklFYmL", "forum": "2KTEaY0lLi", "replyto": "2KTEaY0lLi", "signatures": ["ICLR.cc/2026/Conference/Submission22245/Reviewer_JXMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22245/Reviewer_JXMY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576013041, "cdate": 1761576013041, "tmdate": 1762942132685, "mdate": 1762942132685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model-based MARL algorithm. The proposed mutli-agent dynamics model consists of individual agent modules and one environment module to integrate all each agent’s information. Agent policies training is speeded up by using the imaginary trajectories from the learned dynamics model."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow;"}, "weaknesses": {"value": "- The experiment results only show the performance in 50k steps. How about the longer training results? Can the proposed method achieve the performance as good as the baselines? Currently, the comparison is very suspicious.\n- How accurate is the proposed dynamics model? It should be quantified.\n- Since you are still using centralized training and decentralized execution, why not learn a centralized dynamics model?"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1iAWFiEdGx", "forum": "2KTEaY0lLi", "replyto": "2KTEaY0lLi", "signatures": ["ICLR.cc/2026/Conference/Submission22245/Reviewer_zG7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22245/Reviewer_zG7f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761677633929, "cdate": 1761677633929, "tmdate": 1762942132423, "mdate": 1762942132423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DMAWM, a world-model-based multi-agent RL framework that factorizes dynamics into agent modules and a shared environment module. The model learns latent representations and trains decentralized policies through imagined rollouts. Experiments span SMAC, SMACv2, and Melting Pot, where the method reports improved sample efficiency over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Addressing latent imagination for multi-agent control is timely and important, and indeed world models provide compelling benefits for sample efficiency.\n\n2. The paper evaluates on diverse benchmarks, including visual observations, which strengthens empirical relevance.\n\n3. Visualization results are informative and helpful."}, "weaknesses": {"value": "#### **1. (Major) Implicit reliance on CTDE not acknowledged or contextualized**\n\nThe method implicitly relies on **centralized training** with latent state alignment, transformer-based global critics, and joint priors over agent latent, which is a classical **CTDE** setup. Yet, the paper **never explicitly acknowledges** this design choice or position itself within the broad literature of CTDE, nor discusses limitations imposed by CTDE on scalability.\n\n#### **2. (Major) Missing comparisons to closely related SOTA baselines** \n\nThe paper does **not** compare against nor cite several closely related and highly relevant approaches: \n- **LIMARL** (uses agent modules for latent inference and an environment module for state representation, and it is a decentralized execution approach)\n- **MA2E** (uses transformers for trajectories learning, and it is a decentralized execution approach)\n- **COLA** (latent-space coordination via contrastive objectives, and it is a decentralized execution approach) \n\nThese works directly target **latent representations, interaction modeling, and decentralized execution**, overlapping significantly with the stated contribution. This omission is significant and weakens the contribution claim. Thus the related work discussion and experimental baselines are incomplete.\n\n#### **3. Scalability concerns not addressed** \n\nTransformer-based interaction prediction over per-agent embeddings scales **quadratically** in the number of agents. No discussion or experiments address this problem. \n\n#### **4. Overfitting of latent dynamics acknowledged but unaddressed** \n\nThe paper admits the latent dynamics \"overfit to trajectories generated by the learned policies\" (Conclusion), but provides no mitigation strategies. This is a fundamental weakness for world models. \n\n#### **5. Lack of theoretical analysis**\n\nThe paper provides no theoretical analysis regarding the proposed disentanglement objective, latent factorization, or convergence properties of imagination-based policy learning.\n\n#### **6. High variance raises concerns about statistical significance**\n\nSome reported results show high variance across training runs (e.g., Figure 2, corridor), making it difficult to assess whether the observed improvements are statistically meaningful rather than due to stochasticity. Given the sample-efficiency claims, more rigorous reporting, such as confidence intervals or statistical tests, would be necessary to show significance.\n\n#### **Reference**\n\nKharrat, Salma, et al. \"Latent Inference for Effective Multi-Agent Reinforcement Learning under Partial Observability.\" EWRL 2025.\n\nKang, Sehyeok, et al. \"MA $^ 2$ E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder.\" ICLR 2025.\n\nXu, Zhiwei, et al. \"Consensus learning for cooperative multi-agent reinforcement learning.\" AAAI 2023."}, "questions": {"value": "1. Can the authors clearly state the extent to which DMAWM relies on centralized training of latent states and the critic, and how this differs in practice from standard CTDE architectures?\n\n2. Why are COLA, MA²E, LIMARL, and others omitted from discussion and comparison, given that all use latent representations and support decentralized execution?\n\n3. LIMARL explicitly factorizes dynamics into a local latent representation module and a global environment module. Can the authors comment on the similarity between LIMARL and DMAWM, and clarify the conceptual and architectural differences?\n\n4. How does the transformer-based interaction scale as the number of agents increases, both in computational cost and memory? What are the practical limits of this design?\n\n5. Some reported results (e.g., corridor in Figure 2) exhibit very high variance. Can the authors explain why such large performance fluctuations occur?\n\n6. If the latent dynamics overfit to policy-generated trajectories, as acknowledged, what prevents compounding model errors during imagination from degrading policy learning quality over long horizons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axVRXAGcBN", "forum": "2KTEaY0lLi", "replyto": "2KTEaY0lLi", "signatures": ["ICLR.cc/2026/Conference/Submission22245/Reviewer_kHuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22245/Reviewer_kHuN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905452696, "cdate": 1761905452696, "tmdate": 1762942132074, "mdate": 1762942132074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes Disentangled Multi-Agent World Model (DMAWM), a framework to learn decentralized policies using model-based techniques. The proposed architecture learns decentralized policies in the latent space, featuring independent agent modules and a shared environment module that jointly learn a factorized latent representation that captures interactions between the agents while disentangling individual agent latent spaces. The authors perform experiments over different scenarios/benchmarks, comparing the performance of the proposed method against other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper features a good discussion of related work in Sec. 2. The experimental protocol seems solid, with the authors performing multiple runs and reporting the standard deviation of the results. The experimental results seem reproducible. The experiments are also extensive, considering several different environments. The baselines selected in the empirical part of the work seem representative and diverse. The experimental results seem to be in favor of the proposed method. The authors also provide an ablation study. The work seems to feature a sufficient degree of novelty for publication, even though I am not very familiar with previous works."}, "weaknesses": {"value": "I feel like the clarity of the paper could slightly be improved, particularly in Sec. 4. I understand that the method proposed (DMAWM) features many components and that the authors seem to have tried to come up with a graphical way to explain how these different components interact (Fig. 1). Still, it took me a while to understand how all the pieces come together (and I read the section multiple times)."}, "questions": {"value": "- in Fig. 1 (b), why did you use $\\hat{I}$ instead of $I$?\n- \"Since no observation is available during imagination, the interaction predictor takes the joint deterministic state as input to model the interaction between agents instead.\" - could the authors clarify this sentence? Do you assume access to the underlying state during imagination? Shouldn't the imagination part be entirely done in a recursive manner by iteratively predicting latent states?\n- line 288, sentence \" After that, the interaction predictor samples the (...)\". As far as I understood, while the policies of each agent are decentralized in the sense that they only rely on local information for each of the agents, this interaction predictor runs in a centralized fashion, right? This is because it depends on the h's of all agents and, therefore, either the agents all have access to the interaction predictor and communicate with each other in order to sample the joint stochastic state, or there exists a central node sampling the joint stochastic state and broadcasting it to all agents. Is my reasoning correct?\n- Sec. 5: So, in practice, the policies of DMAWM are trained every 32 env. steps using 1024 parallel rollouts generated from the model. Do you also use this exact same rate for the other model-based baselines? If so, I think it is important to emphasize that this is the case in the document so that we can fairly compare the different model-based approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RnXwPSaT6d", "forum": "2KTEaY0lLi", "replyto": "2KTEaY0lLi", "signatures": ["ICLR.cc/2026/Conference/Submission22245/Reviewer_QkZz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22245/Reviewer_QkZz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929790699, "cdate": 1761929790699, "tmdate": 1762942131739, "mdate": 1762942131739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}