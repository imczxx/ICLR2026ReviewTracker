{"id": "zV5QoVWr05", "number": 12567, "cdate": 1758208656262, "mdate": 1759897501379, "content": {"title": "Towards the Explainability of Temporal Graph Networks via Memory Backtracking", "abstract": "Temporal graphs are ubiquitous in real-world applications such as social networks and finance, where Temporal Graph Networks (TGNs) achieve superior predictive accuracy. Understanding which historical events drive specific\nmodel predictions enhances trustworthiness of TGNs.  Existing explanation methods for TGNs overlook the memory module, the core component that records and updates node histories, leaving unexplored how past events shape memory dynamics and influence the current predictions. To address this challenge, we propose a framework that attributes TGNs predictions through the topology attribution tree and memory backtracking tree. The topology attribution tree captures neighbor influence, including the impact of their memory vectors. Then, we use the memory backtracking tree to quantify how historical events shape memory evolution. Our method satisfies a conservation principle, ensuring that the total contribution of events equals the model’s logits. Finally, we introduce optimization objectives to map logits to probabilities. Experiments on seven temporal graph datasets, spanning node property prediction and link prediction tasks, show that our method provides faithful explanations and consistently outperforms four state-of-the-art baselines.", "tldr": "", "keywords": ["Temporal graph networks", "memory backtracking tree", "explainability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc388f7e0bb500d4818563df909ef1c00e7ea14e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies explainability for Temporal Graph Networks. It introduces MemExplainer that attributes a target prediction to recent interactions and to stored node memories. The approach builds a topology attribution tree and then backtracks memory updates to earlier events. The authors claim a conservation-style property linking the final logit to the sum of selected event contributions. Experiments are shown across several temporal-graph tasks and datasets using figures to suggest improved fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work targets a real need for explanations that account for temporal memory in graph models.\n\n2. The pipeline is clear at a high level, with a split between topology attribution and memory backtracking and an emphasis on contribution conservation.\n\n3. The evaluation spans multiple datasets and both link prediction and node-property prediction, which increases practical relevance."}, "weaknesses": {"value": "1. The evaluation relies on model-internal fidelity measures that compare outputs before and after event selection. These measures can reward matching model behavior rather than faithfulness to the underlying phenomenon. There is no human study, no causal validation, and no task-utility assessment that would confirm usefulness beyond reproducing the model.\n\n2. Key ablations and robustness checks are missing. The paper does not ablate the memory-backtracking component or the topology-attribution component, so their individual contributions are not isolated. And there is little analysis of sensitivity to timestamp noise, to alternative memory-update modules, or to choices that control recent-event sampling and backtracking depth. Multi-seed statistics and significance testing are not reported.\n\n3. Computational cost is not reported. The method builds and traverses attribution structures over time, yet there are no numbers for run-time or peak-memory, or how these scale with graph size, event density, number of layers, or backtracking depth.\n\n4. Baseline and metric coverage could be stronger. Only a small set of explainers is considered, and some are adapted from static-graph settings.\n\n5. Lack of reproducibility details. Key implementation choices are not clearly described, including how node memories are built and updated, when event selection stops, and what optimizer is used to pick the event set. The paper does not mention code release or random seeds."}, "questions": {"value": "1. What are the typical run-time and memory costs per instance, and how do they change with graph size, number of sampled recent events, number of layers, and backtracking depth?\n\n2. Can you provide ablations that remove the memory-backtracking stage, remove the topology-attribution stage, and replace the event-selection objective with a simple top-$k$ rule to quantify each component’s contribution?\n\n3. How sensitive are the explanations to timestamp perturbations, to noisy features, and to alternative memory-update modules? Do the explanations transfer across different TGN variants?\n\n4. Do you have validation beyond model matching, such as human judgments or interventional tests that remove or inject events to confirm that selected events are truly explanatory?\n\n5. How is the optimization over the selected event set solved in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YA8Ed11pkG", "forum": "zV5QoVWr05", "replyto": "zV5QoVWr05", "signatures": ["ICLR.cc/2026/Conference/Submission12567/Reviewer_6Mxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12567/Reviewer_6Mxv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761079978362, "cdate": 1761079978362, "tmdate": 1762923419878, "mdate": 1762923419878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an explanation method for attributing the output of temporal graph neural networks to the important historical events. Specifically, the proposed method first applies the LRP on the embedding module to compute the topology attribution tree, which estimates the node memories’ contribution to node embeddings. Then, the proposed method further propagates it to the historical events by applying LRP on the memory updating module. The important events are finally selected by solving an optimization problem based on the obtained event contribution matrix."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The ideas of applying the two-step LRP to estimate the importance of historical events and formulating the selection problem as an optimization variant are natural.\n- The paper is generally well-written and easy to follow\n- The proposed method achieves better performance than the compared baselines."}, "weaknesses": {"value": "- The proposed method appears computationally intensive, as it requires building a topology attribution/memory backtracking tree and solving an optimization problem for each prediction. The authors should provide a clear complexity analysis and compare it with baseline methods. Additionally, a runtime comparison is needed to demonstrate practical efficiency.\n- Some parts of the paper need further clarification. 1)How is Equation (13) converted into Equation (14)? 2) How is the optimization of Equation (14) actually solved?\n- Minor issues. 1) It is better to denote the shape of the matrices/vectors when they first appear. 2) The superscript $t$ of $\\mathcal N_{u_k}^t$ in Line 159 should be $n$."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vZ8nC7M7eq", "forum": "zV5QoVWr05", "replyto": "zV5QoVWr05", "signatures": ["ICLR.cc/2026/Conference/Submission12567/Reviewer_UNHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12567/Reviewer_UNHa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475164660, "cdate": 1761475164660, "tmdate": 1762923419518, "mdate": 1762923419518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of explainability in Temporal Graph Networks (TGNs). It proposes a novel framework called MemExplainer to attribute the predictions made by TGNs by considering both spatial and temporal dynamics. The framework introduces two components: the topology attribution tree, which attributes predictions based on the contributions of recent events and their associated node memories, and the memory backtracking tree, which traces how past events influence memory evolution. The method ensures that the total contribution from events equals the model’s output logits, maintaining a conservation property. Extensive experiments on seven temporal graph datasets demonstrate that the proposed method outperforms several explainability approaches, showing improved fidelity and lower sparsity in the explanations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work proposes an innovative framework for explaining TGNs. The dual-attribution approach—using a topology attribution tree and a memory backtracking tree—gives a more comprehensive explanation of model predictions by accounting for both spatial and temporal influences. The conservation principle ensures that the sum of all event contributions exactly matches the model’s output logits, which helps guarantee the explanation’s faithfulness.\n2. The method shows strong performance across multiple benchmarks. It consistently outperforms existing explainers such as TGNNExplainer and TempME on both node property prediction and link prediction tasks.\n3. The paper is supported by solid theoretical foundations and practical design. For example, building both the spatial and temporal attribution components on Layer-wise Relevance Propagation (LRP) provides a consistent and theoretically justified framework, enhancing both reliability and interpretability."}, "weaknesses": {"value": "1. The paper's exploration of explainability for memory-based dynamic graph methods is commendable. However, it is questionable whether such methods can provide meaningful explanations on datasets with very high repetition rates, such as Wikipedia, Reddit, etc. This issue has been noted in recent research. For example, [1] points out a key limitation of these datasets: models can achieve good performance simply by remembering whether an edge has appeared before, making the learning task too easy. The strong performance of the EdgeBank model from [3]—which requires no training—on these datasets further supports this concern. Similarly, [2] analyzes attention weights in Transformer-based models and finds that models mainly rely on high-frequency, repetitive edges. This makes the explanations straightforward and intuitively clear, but also suggests they may not capture deeper reasoning for datasets with low repetition rate. Therefore, when evaluating dynamic graph models, using event repetition rate as a criterion for dataset selection might lead to more meaningful research.\n2. Although the method performs well on several datasets, its computational cost—especially from the memory backtracking process—could be an issue for large graphs with long temporal histories. The approach involves recursive backtracking and memory updates, which may become slow on very large-scale data. In addition, while the optimization for selecting important events is defined in Equation 14, the paper lacks details on how this optimization is implemented efficiently in practice, especially as the number of events increases. More information about convergence behavior, runtime, and computational trade-offs would help readers assess its practicality.\n3. While the framework is technically sound, the resulting explanations may not be easy to interpret. The tree structure (topology and memory) could be hard to understand without more explanation or visual examples. The paper would benefit from including concrete case studies or qualitative examples to show how specific predictions (on both low/high repetition rate) are explained and how users might apply these explanations in real-world scenarios.\n\nReference\n\n[1] TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics https://arxiv.org/abs/2502.02975v3\n\n[2] TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer https://arxiv.org/abs/2506.00431\n\n[3] Towards Better Evaluation for Dynamic Link Prediction https://arxiv.org/abs/2207.10128"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d5BzRdWwz7", "forum": "zV5QoVWr05", "replyto": "zV5QoVWr05", "signatures": ["ICLR.cc/2026/Conference/Submission12567/Reviewer_TxVo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12567/Reviewer_TxVo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571049660, "cdate": 1761571049660, "tmdate": 1762923419118, "mdate": 1762923419118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MemExplainer, an explainability framework for Temporal Graph Networks (TGNs). It decomposes the model's prediction into contributions from historical interaction events through a two-stage attribution process: the Topology Attribution Tree and the Memory Backtracking Tree."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Timely and meaningful problem: Explaining TGNs is crucial in domains such as finance, recommendation, and fraud detection; focusing on memory influence is insightful.\n\nComprehensive method: The topology + memory two-step structure is well-described with algorithms and clear relevance propagation logic."}, "weaknesses": {"value": "Unclear optimization in Eq. (14): The binary event-selection problem lacks details—whether solved exactly, heuristically, or via continuous relaxation—and its time complexity. Clarify the algorithm and provide runtime statistics.\n\nScalability and resource cost: Memory backtracking may explode for large event histories. The paper lacks complexity or runtime/memory analysis. Please add empirical resource tables or propose pruning strategies.\n\nBaseline implementation details: How were static explainers (GNNExplainer, PGExplainer) adapted to TGNs? What hyperparameters and seeds were used for TGNNExplainer and TempME？\n\nRobustness / statistical significance: Report averages ± std or confidence intervals for repeated runs, since some improvements are small.\n\nModel-component assumptions: The derivations assume a GRU updater. Discuss applicability to other TGNs (e.g., attention-based or transformer-style updaters) and whether the LRP rules transfer directly.\n\nLimited human-interpretability evaluation: Fidelity metrics show quantitative preservation, but user-level interpretability is not assessed."}, "questions": {"value": "How exactly is Eq. (14) optimized? What algorithm and complexity are used, and what are the runtime/memory statistics across datasets?\n\nPlease provide empirical runtime and memory data for memory-backtracking with different depths L.\n\nCan the approach generalize to non-GRU TGNs (e.g., attention updaters)? What modifications are required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9MZeDLaRYb", "forum": "zV5QoVWr05", "replyto": "zV5QoVWr05", "signatures": ["ICLR.cc/2026/Conference/Submission12567/Reviewer_Y5yW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12567/Reviewer_Y5yW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092442925, "cdate": 1762092442925, "tmdate": 1762923418630, "mdate": 1762923418630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}