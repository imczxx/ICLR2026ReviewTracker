{"id": "z3cFADf6zZ", "number": 971, "cdate": 1756825880262, "mdate": 1763635662450, "content": {"title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks", "abstract": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. \nExisting methods primarily focus on metrics related to generation quality and controllability. \nHowever, they often overlook the evaluation of downstream perception tasks, which are {\\bf really crucial} for the performance of autonomous driving. \nExisting methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). \nWhen we double the epochs in the baseline, the benefit of synthetic data becomes negligible.\nTo thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps.\nFinally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models.\nDream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. \nTo facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing.\nWe conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. \nThe code and dataset will be released.", "tldr": "", "keywords": ["Autonomous Driving", "Driving World Model", "Perception Tasks", "Synthetic Data"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0191346e40384ac38ee5010f2db6296d820db0f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper critically re-evaluates the utility of synthetic data generated by driving world models for downstream perception tasks. The authors argue that previous works often rely on an unfair evaluation protocol where models trained on hybrid (real + synthetic) data use twice the training epochs of real-data-only baselines. They demonstrate that when training epochs are matched, the benefits of existing synthetic data augmentation methods become negligible or even negative.\n\nTo address this, the paper introduces **Dream4Drive**, a novel framework for generating high-quality, 3D-aware synthetic data. Instead of relying on sparse conditioning signals like BEV maps, Dream4Drive first decomposes a real video into a set of dense, 3D-aware guidance maps (depth, normal, edge, etc.). It then renders 3D assets into these maps and fine-tunes a Diffusion Transformer-based world model to generate photorealistic, multi-view videos that incorporate these new objects. This approach allows for precise, instance-level control and ensures geometric and visual consistency.\n\nFurthermore, the authors contribute **DriveObj3D**, a large-scale 3D asset dataset tailored for driving scenarios, along with an automated pipeline for its creation. Through extensive experiments on detection and tracking tasks, the paper shows that augmenting the training set with a very small fraction (<2%) of data generated by Dream4Drive consistently and significantly improves perception performance, even under fair, epoch-matched comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Important and Timely Critique:** The central argument regarding the unfair evaluation of synthetic data is a significant contribution. By demonstrating that simply increasing training epochs on real data can match or exceed the performance of hybrid-data training, the paper forces the community to reconsider how the value of synthetic data is measured. This provides a strong and compelling motivation for the proposed work.\n2. **Comprehensive and Convincing Experiments:** The experimental validation is thorough and directly supports the paper's claims.\n    - The head-to-head comparisons under 1x, 2x, and 3x epoch settings provide clear evidence for the effectiveness of Dream4Drive over baselines.\n    - The ablation studies are insightful, systematically analyzing the impact of insertion position, distance, 3D asset source, and different components of the guidance maps. This provides a deeper understanding of what makes the synthetic data effective.\n    - The evaluation on both detection and tracking tasks, at multiple resolutions, demonstrates the general applicability of the generated data."}, "weaknesses": {"value": "1. **Limited Scope of Generated Corner Cases:** The paper defines \"corner cases\" primarily as the insertion of new objects into a scene. While this is an important class of long-tail events, it does not cover other critical scenarios, such as adverse weather conditions (heavy rain, snow, fog), unusual lighting (lens flare, low sun), complex multi-agent interactions, or environmental changes (e.g., road construction). The framework's applicability to these other types of corner cases is not explored.\n2. **Potential Bottlenecks in the Asset Generation Pipeline:** The `DriveObj3D` pipeline is a cascade of multiple sophisticated models (segmentation, multi-view generation, mesh generation). The final asset quality is contingent on the successful execution of every step. This pipeline may be brittle; for instance, a failure in multi-view generation could lead to an incomplete or distorted 3D mesh. A discussion on the robustness, failure modes, and potential need for manual curation of this pipeline would strengthen the paper.\n3. **Limited Technical Contribution.** While the specific application and the system-level design are effective in research application, the work does not introduce a new core generative modeling technique or a new research framework. For some reviewers, the novelty might be perceived as incremental, lying more in the clever combination of existing parts than in foundational innovation.\n4. **Computational Cost Analysis is Missing:** The proposed data generation process is multi-staged and involves fine-tuning a large Diffusion Transformer model. This suggests a significant computational cost. While the paper argues for fairness in terms of training *epochs*, a discussion on fairness in terms of total *compute* (cost of data synthesis + cost of training) would provide a more complete picture. For instance, how does the cost of generating 420 synthetic samples compare to the cost of training the perception model for an additional epoch?"}, "questions": {"value": "1. Regarding the `DriveObj3D` creation pipeline: Could you quantify the robustness of this pipeline? What is the approximate failure rate, and what are the common failure modes? How much manual filtering or intervention was required to curate the final high-quality dataset?\n2. The scene selection for insertion is described as choosing frames \"where no other vehicles appear along the insertion trajectory.\" How is this selection performed (manually or automatically)? Could this process introduce a bias, for example, by preferentially selecting less cluttered scenes, thereby limiting the complexity of the generated data?\n3. The paper highlights improved realism via shadows and reflections. Does the generative model also learn to synthesize more complex physical interactions? For example, does an inserted vehicle generate splashes when moving through a puddle, or create dust on a dirt road? To what extent can the model handle nuanced lighting effects beyond direct shadows, such as colored light from traffic signals reflecting on the vehicle?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XNRUbwVNji", "forum": "z3cFADf6zZ", "replyto": "z3cFADf6zZ", "signatures": ["ICLR.cc/2026/Conference/Submission971/Reviewer_yxwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission971/Reviewer_yxwm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583275330, "cdate": 1760583275330, "tmdate": 1762915650464, "mdate": 1762915650464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 1/2"}, "comment": {"value": "We would like to express our sincere gratitude to all the reviewers for their thoughtful and constructive feedback on our submission. We are delighted that our paper was recognized for **“identifying a key shortcoming in most generative simulators for autonomous driving data”** (reviewers dArx, yb3h, yxwm), **“high efficiency in data augmentation”** (reviewers dArx, FTf2, yb3h), **“3D-aware guidance maps enabling realistic, geometry-consistent, multi-view scene synthesis”** (reviewer FTf2), **“DriveObj3D fills a gap in open 3D asset resources for driving research”** (reviewers dArx, FTf2, yb3h), and **“comprehensive and convincing experiments”** (reviewer yxwm).\n\n------\n\n**We have revised our paper (with changes highlighted in blue). The updates are summarized as follows:**\n\n1. (For Reviewers dArx, FTf2, yb3h, yxwm). We have added a detailed description of the entire synthesis process for the 420 samples in Appendix A.2, including how the process is automated and the amount of manual effort involved.\n2. (For Reviewers dArx, yxwm). We have included additional ablation studies on scene insertion, trajectory insertion, and asset-category insertion in Appendix A.4.\n3. (For Reviewers yb3h, yxwm). We have added experiments on scaling the amount of OOD data in Appendix A.3.\n4. (For Reviewers FTf2, yb3h, yxwm). We have added quantitative evaluations using DriveObj3D metrics in Appendix A.3.\n5. (For Reviewer FTf2). We have streamlined the Introduction section, added more motivational connections in the Method section, adjusted the layout of Figure 4, and included additional relevant references.\n\n------\n\n**Regarding the reviewers' comments, we provide the following general responses.**\n\n> **GR1.** Detailed synthesis procedure and time-cost analysis for generating the 420 samples. \n\nWe begin by clarifying the construction of the 420 samples, then present a detailed, step-by-step account of the time required, and conclude by describing the operations carried out at each stage. \n\n- Construction of the 420 samples.\n\n  We randomly select seven annotated scenes and insert ten object categories (one asset per category). Each video contains 33 frames, from which we extract six key frames for downstream training, resulting in a total of 7 × 10 × 6 = 420 samples. \n\n- Detailed synthesis steps and time analysis. \n\n  - |                             Step                             | Automation（s） | Manual effort（s） |\n    | :----------------------------------------------------------: | --------------- | ------------------ |\n    | Scene selection & initial position annotation (MLLM-assisted) | 700×10          |                    |\n    |                      Scene verification                      |                 | 7×20               |\n    |                 Scene orientation annotation                 |                 | 7×15               |\n    |                       Asset selection                        |                 | 10×5               |\n    |                    Video generation (DiT)                    | 70×10×4         |                    |\n    |                            Total                             | 9800s ≈ 2.72h   | 295s               |\n\n  - Scene selection and initial position annotation. \n\n    An MLLM identifies scenes containing at least one collision-free straight-driving trajectory (front, rear, left, or right). For eligible scenes, it annotates the asset’s initial insertion position in ego-vehicle coordinates for the first frame (x, y, z). The full prompt is provided in Appendix A.2.\n\n    The nuScenes training set contains 700 videos, and each MLLM query takes about 10 seconds, totaling 700 × 10 seconds.\n\n  - Human verification of MLLM-filtered scenes. \n\n    Only seven scenes are needed. We manually verify the MLLM-generated initial positions, requiring ~20 seconds per scene, totaling 7×20 seconds.\n\n  - Scene orientation annotation. \n\n    Orientation is calibrated using *pyrender* python package. For each eligible scene, we render an initial asset-insertion using nuScenes intrinsics and extrinsics to determine the correct yaw angle. The first rendering takes 6 seconds, the correction ~3 seconds, and the second rendering another 6 seconds, for a total of approximately 15 seconds per scene. Because all assets are standardized, each scene only needs to be calibrated once, totaling 7×15 seconds.\n  \n  - Asset selection. \n\n    We require only one asset per object category, with no additional constraints. A random choice per category suffices (~5 seconds each), totaling 10×5 seconds.\n  \n  - Video generation. \n\n    We generate 70 videos for the 420 samples. Each video uses 10 inference steps; on an H200 GPU each step takes ~4 seconds, totaling 70 × 10 × 4 seconds.\n  \n  - **Summary.** \n\n    We adopt a partially automated pipeline. The full synthesis process for all 420 samples takes under 3 hours, with **approximately** **300 seconds of actual human labor**, which is effectively negligible."}}, "id": "9xXtJNhBqj", "forum": "z3cFADf6zZ", "replyto": "z3cFADf6zZ", "signatures": ["ICLR.cc/2026/Conference/Submission971/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission971/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission971/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763636065140, "cdate": 1763636065140, "tmdate": 1763636065140, "mdate": 1763636065140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dream4Drive, a framework for generating synthetic training data for autonomous driving perception tasks. The key insight is that previous work evaluated synthetic data unfairly by pretraining on synthetic data and then finetuning on real data (2x training epochs), obscuring true benefits. Dream4Drive decomposes videos into 3D-aware guidance maps (depth, normal, edge, object image, mask) and uses a fine-tuned diffusion transformer to render edited multi-view videos after inserting 3D assets. The authors contribute DriveObj3D, a dataset of 3D assets across driving scenarios. With only 420 synthetic samples (<2% of training data), they demonstrate improvements in detection and tracking at 1x, 2x, and 3x training epochs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a key shortcoming in most generative simulators for autonomous driving data: improvements in downstream task from training on both real and synthetic data have in the past been attributed only to the addition of synthetic data while ignoring that increasing training epochs improves performance even just using the base dataset (nuScenes).\n- The paper provides a large-scale dataset (DriveObj3D) along with a method to insert assets from the dataset in a natural manner to match scene lighting and shadows.\n- The results show some improvement over existing methods in downstream detection tasks while being sample efficient (only adding <2% data over the original dataset)"}, "weaknesses": {"value": "W1. The paper lacks novelty beyond the Epoch related finding. The actual data generation framework is a composition of existing models and methods rather than a technical contribution. While adding multiview images as conditions to generate a 3D mesh for 3D assets does improve asset quality over meshes generated from single view, it seems like an unsurprising improvement. Moreover, the actual improvement in scores from Table 5 compared to other 3D asset generation methods is relatively small, despite using multiview images vs Hunyuan3D's single view method.\n\nW2. The detection performance results using the method are marginal improvements over previous works. In Tables 1, 2, and 3, the relative improvement in metrics like mAP in the 2x and 3x Epochs are small, even over just using the original nuScenes dataset. In fact, this result shows that even the DriveObj3D falls prey to the core discovery the authors note about normalizing for # of Epochs trained; the improvement over training with only real data diminishes with increasing Epochs. The increase in performance in mAP vs Real data in Table 1 (2x Epochs) is <1% improvement despite including >1% increase in training data. \n\nW3. Appendix E acknowledges that \"automatically ensuring insertions are in drivable areas and avoid collisions remains an open challenge.\" This is a significant limitation, with manual effort required preventing this method from generating corner cases on a significant scale."}, "questions": {"value": "Additional questions:\n\nQ1: Given that naive insertion performs within 0.6-0.7 mAP of your full method (Tables 3-4), can you provide a compute or latency cost-benefit analysis? The qualitative improvement in image quality is clear (Figure 8) but the actual metric benefits less so.\n\nQ2: How much manual effort is required to select 420 valid insertion positions and trajectories? Can you quantify the human time required and discuss path to full automation?\n\nQ3: Building off Q2, do the authors have an evidence or intuition as to whether the approach will scale with more augmented scenes using their library? W2 suggests that the increase in training data doesn't necessarily correspond with a proportional increase in performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZQfdkrSssa", "forum": "z3cFADf6zZ", "replyto": "z3cFADf6zZ", "signatures": ["ICLR.cc/2026/Conference/Submission971/Reviewer_yb3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission971/Reviewer_yb3h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768874782, "cdate": 1761768874782, "tmdate": 1762915650252, "mdate": 1762915650252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Dream4Drive, a 3D-aware synthetic data generation framework designed to improve downstream perception tasks such as 3D object detection and tracking in autonomous driving. Dream4Drive employs dense 3D-aware guidance maps to edit real driving videos by rendering high-quality 3D assets, ensuring multi-view geometric consistency and photorealism. A new dataset, DriveObj3D, is introduced to provide diverse 3D vehicle and traffic assets for large-scale, geometry-consistent scene editing. Dream4Drive demonstrates that injecting fewer than 2% synthetic samples consistently improves detection and tracking metrics, outperforming existing augmentation baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Dream4Drive introduces a combination of dense 3D-aware guidance maps with generative video editing, enabling realistic, geometry-consistent, multi-view scene synthesis.\n\n2. Demonstrating that fewer than 2% synthetic samples can enhance real data training is a strong and practically relevant result, suggesting high efficiency in data augmentation.\n\n3. The proposed DriveObj3D fills a gap in open 3D asset resources for driving research, supporting reproducibility, and future extension in video-level generative modeling."}, "weaknesses": {"value": "1. The paper does not clarify how inserted 3D assets are constrained to remain physically valid: ensuring no collisions, adherence to drivable areas, and correct orientations, nor how occlusion layers between objects are resolved across depth, normal, and edge maps.\n\n2. There is no qualitative criterion for evaluating 3D asset realism or consistency, raising uncertainty about the reliability of the generated dataset and edited scenes.\n\n3. As shown in Table 3, the method’s performance fluctuates across epochs; in some settings, direct insertion outperforms Dream4Drive, and differences between methods are within statistical noise, suggesting the need for repeated trials with mean–variance reporting.\n\n4. The introduction is verbose, and the method section lists procedural steps without sufficient motivation or justification. Figures contain minor layout inconsistencies.\n\n5. Missing Related Work: SimGen (NeurIPS 2024)."}, "questions": {"value": "Please refer to the weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0Ykqghm0O", "forum": "z3cFADf6zZ", "replyto": "z3cFADf6zZ", "signatures": ["ICLR.cc/2026/Conference/Submission971/Reviewer_FTf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission971/Reviewer_FTf2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880755467, "cdate": 1761880755467, "tmdate": 1762915650110, "mdate": 1762915650110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily investigates the limitations of existing generation methods in downstream tasks, demonstrating that significant improvements can be achieved using only 2% of generated data. It also introduces DriveObj3D, a large-scale 3D asset dataset covering typical categories in driving scenarios and enabling diverse 3D-aware video editing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Identifies and analyzes the limitations of current generative methods in downstream tasks, showing that even a small amount (as little as 2%) of high-quality generated data can lead to significant performance gains.\n- Introduces DriveObj3D, a large-scale 3D asset dataset encompassing typical object categories in driving scenarios, which supports diverse and 3D-aware video editing applications."}, "weaknesses": {"value": "- The method relies on scene insertion and asset insertion that currently lack sufficient automation and are primarily dependent on manual effort."}, "questions": {"value": "This work demonstrates the effectiveness of out-of-distribution (OOD) scene data generation for downstream task training, focusing specifically on *scene-level* OOD—namely, inserting vehicles into originally empty scenes. However, the scope of OOD scenarios could be further expanded. For instance, one could explore modifying existing scenes by altering vehicle trajectories or swapping vehicle categories to investigate how such diverse OOD variations impact downstream task performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "izC6kwriC6", "forum": "z3cFADf6zZ", "replyto": "z3cFADf6zZ", "signatures": ["ICLR.cc/2026/Conference/Submission971/Reviewer_dArx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission971/Reviewer_dArx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004368883, "cdate": 1762004368883, "tmdate": 1762915649930, "mdate": 1762915649930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}