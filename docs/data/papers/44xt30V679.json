{"id": "44xt30V679", "number": 1882, "cdate": 1756956811878, "mdate": 1763176623671, "content": {"title": "Mitigating Jailbreaks with Intent-Aware LLMs", "abstract": "Despite extensive safety-tuning, large language models (LLMs) remain vulnerable to jailbreak attacks via adversarially crafted instructions, reflecting a persistent trade-off between safety and task performance. In this work, we propose \\textsc{Intent-FT}, a simple and lightweight fine-tuning approach that explicitly trains LLMs to infer the underlying intent of an instruction before responding. By fine-tuning on a targeted set of adversarial instructions, \\textsc{Intent-FT} enables LLMs to generalize intent deduction to unseen attacks, thereby substantially improving their robustness. We comprehensively evaluate both parametric and non-parametric attacks across open-source and proprietary models, considering harmfulness from attacks, utility, over-refusal, and impact against white-box threats. Empirically, \\textsc{Intent-FT} consistently mitigates all evaluated attack categories, with no single attack exceeding a $50\\%$ success rate—whereas existing defenses remain only partially effective. Importantly, our method preserves the model's general capabilities and reduces excessive refusals on benign instructions containing superficially harmful keywords. Furthermore, models trained with \\textsc{Intent-FT} accurately identify hidden harmful intent in adversarial attacks, and these learned intentions can be effectively transferred to enhance vanilla model defenses.", "tldr": "This work proposes a post supervised fine-tuning method that tunes a safety-aligned LLM at reasoning about the intentions of an instruction before responding, resulting in significant improvement in defense against adversarial jailbreak attacks.", "keywords": ["AI Safety", "Jailbreak", "LLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/408237f7ce3e54e6e374f06061cc027261b63f8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes \\textsc{Intent-FT}, a simple and lightweight fine-tuning approach that explicitly trains large language models (LLMs) to infer the underlying intent of an instruction before generating responses. The authors conduct comprehensive evaluations on both parametric and non-parametric attacks across open-source and proprietary models, demonstrating that the method consistently mitigates all evaluated jailbreaking attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation and method are clear, and the visualization is helpful.\n\n2. The authors provide sufficient evidence, including extensive experiments and ablation studies, to support the effectiveness of their proposed method. \n\n3. This paper is well-written, making it easy to follow."}, "weaknesses": {"value": "1. The relationship between intent perception and jailbreaking robustness has been extensively explored from both attack and defense perspectives. This paper appears to trivially transfer this perspective into a fine-tuning setting, raising concerns about its novelty and conceptual contribution.\n\n2. What is the difference between <intent> inference and the reasoning-based model (e.g., models that “think before answering”), such as GPT-5? As the reasoning model is inherent to be able to better extract the intent and show better jailbreaking robustness.\n\n3. In my opinion, white-box GCG and GCG-like attacks, which rely on gradient-based optimization, can almost always jailbreak a model given a sufficient attack budget. Therefore, it is unclear why the authors claim an ASR below 50% and do not provide further results."}, "questions": {"value": "Refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XXE84IKf2", "forum": "44xt30V679", "replyto": "44xt30V679", "signatures": ["ICLR.cc/2026/Conference/Submission1882/Reviewer_1TYP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1882/Reviewer_1TYP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760592478055, "cdate": 1760592478055, "tmdate": 1762915928669, "mdate": 1762915928669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple yet effective method for safety alignment, Intent-FT. Similar to IA (Zhang et al., 2024), Intent-FT encourages LLMs to infer and articulate the user's intent. The key difference is that it trains the model to generate intention CoTs enclosed between `<intent>` and `</intent>` tokens, using intention CoTs collected from a strong teacher model. Intent-FT demonstrates superior performance compared to relevant baselines in both safety alignment and utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is intuitive, and the method is simple and broadly applicable.\n\n- Although it introduces additional computational cost during fine-tuning, the overhead appears to be marginal (not sure), and the empirical results are strong."}, "weaknesses": {"value": "This paper presents a simple yet effective method that is appealing to me, but I believe it can be improved with the following recommendations:\n\n- Important experimental setup details are missing. For example, the fine-tuning appears marginal (100 samples, as noted in line 192), but key details such as the number of epochs/iterations, wall-clock fine-tuning time, and whether LoRA or full fine-tuning was used are not provided.\n\n- Presentation could be improved:\n  - The notation is confusing. Subscripts are sometimes used for indexing (e.g., $x_{1:T}$) and sometimes to indicate different instructions (e.g., $q_v, q_a$). Notations such as $q_v, q_a$ and $y_ρ^h, y_s, y_i$ are hard to distinguish.\n  - In line 345, the authors claim that Intent-FT optimizes the trade-off along the Pareto frontier. It would be clearer to include a plot illustrating this claim.\n  - Several important figures referenced in the main text (e.g., Figures 5, 7, 12) are placed in the Appendix. Although this may be due to page limits, including at least some of these figures in the main paper would help clarify the effectiveness of the method.\n\n- While the proposed idea is simple and broadly applicable, the experimental evaluation is limited. Although Llama and Qwen are representative models, demonstrating results on a wider variety of backbone architectures (e.g., Gemma) would better support claims of generalizability.\n\n- Intent-FT shows that a stronger teacher model can generate intention CoTs to train smaller LLMs and improve their safety alignment. This implies that the teacher model itself may also benefit from leveraging these intention CoTs (e.g., by appending them during inference after generating them, as described in L373–392). It would be helpful to quantify this improvement and use it as evidence for the quality and usefulness of the generated intention CoTs.\n\n- Related to the above point, if an LLM (e.g., the teacher model) is already capable of generating intention CoTs, is Intent-FT still necessary? In other words, does the specialized training introduced by Intent-FT provide additional benefits even for models that can already generate such CoTs, or is its utility primarily for weaker models?\n\n- Minor: The experimental results are reported from a single run. There are several instances where quotation marks are misused instead of the correct LaTeX quotes (e.g., L38, L87, L124)."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HSMOnv8Idw", "forum": "44xt30V679", "replyto": "44xt30V679", "signatures": ["ICLR.cc/2026/Conference/Submission1882/Reviewer_ysq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1882/Reviewer_ysq3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761464141047, "cdate": 1761464141047, "tmdate": 1762915927874, "mdate": 1762915927874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses LLMs’ safety-task performance trade-off and vulnerability to jailbreaks due to shallow alignment. It proposes INTENT-FT, an intent-aware fine-tuning method that trains LLMs to infer instruction intent first. Key results: all attack success rates (ASR) stay below 50%, over-refusal on benign prompts is reduced, and model utility is preserved across open-source (Llama, Qwen) and proprietary (GPT-4.1-mini) models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive evaluation: It tests both parametric (Harmful-FT) and non-parametric (PAIR, AA) attacks, covering open-source and proprietary models, which ensures robustness of results (e.g., Llama’s INTENT-FT reduces PAIR ASR to 19% vs Vanilla’s 88%). \n2. Targeted over-refusal mitigation: Unlike baselines (e.g., Safety-FT) that increase over-refusal on XSTEST, INTENT-FT lowers refusal rates for Llama and GPT-4.1, as it trains on both harmful and benign intent deduction. \n3. Intent transferability: INTENT-FT-generated intentions, when added as context to vanilla models, significantly reduce jailbreak ASR (e.g., GPT-4.1 achieves near-perfect deterrence), showing intent generalizability."}, "weaknesses": {"value": "1. Limited white-box attack coverage: It only tests Ablation and ActAdd white-box methods; other techniques like CipherChat or Autodan are unexamined, which may underestimate real-world threats—adding these stronger attacks could improve generalizability. \n2. Narrow dataset scale testing: The impact of D_I size is only tested up to 100 samples; larger D_I (e.g., 500+) or diverse datasets (e.g., industry-specific harmful prompts) are untested, leaving scalability unclear."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2naDqGblH1", "forum": "44xt30V679", "replyto": "44xt30V679", "signatures": ["ICLR.cc/2026/Conference/Submission1882/Reviewer_7yRw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1882/Reviewer_7yRw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831686525, "cdate": 1761831686525, "tmdate": 1762915927052, "mdate": 1762915927052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-tuning framework termed Intent-FT, which aims to enhance LLM’s ability to infer the underlying intent of an instruction before generating a response. By modelling intent understanding, this approach seeks to improve robustness against both parametric and non-parametric attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes an intent-aware lightweight fine-tuning framework (Intent-FT) that incorporates intent inference to strengthen model’s defense against jailbreak attacks.\n\n2. Compared to defense baselines, the experimental results show that the proposed Intent-FT consistently exhibits robust defense across different attack types and models."}, "weaknesses": {"value": "1. The paper's core contribution is the introduction of fine-tuning for identifying the intention behind a query. While I respect the idea, the contribution is relatively incremental and provides limited further insight into defense. \n\n2. Although the evaluation includes Adaptive Attack, some recent attack methods such as I-GCG [1] and DRL [2] are not considered. Including these would provide a more comprehensive assessment.\n\n[1] Jia, Xiaojun, et al. \"Improved Techniques for Optimization-Based Jailbreaking on Large Language Models.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Chen, Xuan, et al. \"When llm meets drl: Advancing jailbreaking efficiency via drl-guided search.\" Advances in Neural Information Processing Systems 37 (2024): 26814-26845.\n\n3. The current evaluation is limited to two open-source models. To demonstrate the general effectiveness and scalability of Intent-FT, it would be beneficial to include additional open-source models, such as Qwen-3-8B and Mistral-7B-Instruct-v0.3.\n\n4. The related work and experimental comparison could be enhanced by including and discussing recent defense methods like adversarial tuning [3] and RPO [4].\n\n[3] Liu, Fan, Zhao Xu, and Hao Liu. \"Adversarial tuning: Defending against jailbreak attacks for llms.\" arXiv preprint arXiv:2406.06622 (2024).\n\n[4] Zhou, Andy, Bo Li, and Haohan Wang. \"Robust prompt optimization for defending language models against jailbreaking attacks.\" Advances in Neural Information Processing Systems 37 (2024): 40184-40211.\n\n5. Fine-tuning typically involves balancing model robustness, generalization, and computational efficiency. This paper could be further improved by discussing the limitations and trade-offs associated with the proposed fine-tuning framework."}, "questions": {"value": "Please refer to the concerns highlighted in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wx3JwGiryY", "forum": "44xt30V679", "replyto": "44xt30V679", "signatures": ["ICLR.cc/2026/Conference/Submission1882/Reviewer_pSrk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1882/Reviewer_pSrk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1882/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934746976, "cdate": 1761934746976, "tmdate": 1762915926012, "mdate": 1762915926012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}