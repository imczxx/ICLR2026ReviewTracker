{"id": "h05AulYT7g", "number": 5506, "cdate": 1757916315010, "mdate": 1759897970584, "content": {"title": "Any-to-Bokeh: Arbitrary-Subject Video Refocusing with Video Diffusion Model", "abstract": "Diffusion models have recently emerged as powerful tools for camera simulation, enabling both geometric transformations and realistic optical effects. Among these, image-based bokeh rendering has shown promising results, but diffusion for video bokeh remains unexplored. Existing image-based methods are plagued by temporal flickering and inconsistent blur transitions, while current video editing methods lack explicit control over the focus plane and bokeh intensity. These issues limit their applicability for controllable video bokeh. In this work, we propose a one-step diffusion framework for generating temporally coherent, depth-aware video bokeh rendering. The framework employs a multi-plane image (MPI) representation adapted to the focal plane to condition the video diffusion model, thereby enabling it to exploit strong 3D priors from pretrained backbones. To further enhance temporal stability, depth robustness, and detail preservation, we introduce a progressive training strategy. Experiments on synthetic and real-world benchmarks demonstrate superior temporal coherence, spatial accuracy, and controllability, outperforming prior baselines. This work represents the first dedicated diffusion framework for video bokeh generation, establishing a new baseline for temporally coherent and controllable depth-of-field effects. Code will be made publicly available.", "tldr": "", "keywords": ["Computational photography"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e161b6fbf268ce405e4edab22917f00fe8bd43a4.pdf", "supplementary_material": "/attachment/0f93e3c71ed00bb25a32bccbe4b3f98fa2d1f28a.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors present a video bokeh rendering method based on diffusion model. They propose a one-step diffusion framework, and employ a multi-plane image representation to the focal plane as a condition. Also, they introduce a progressive training strategy for stability. Experiments on synthetic and real-world benchmarks show the performance of their methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors propose a one-step diffusion framework for video bokeh rendering, which exhibits an efficiency advantage in inference time.\n- In the third stage, fine tune the VAE decoder and introduce texture loss based on image gradients to improve high-frequency texture and edge clarity, which helps ensure the presentation of details of the focused subject.\n- Time consistency and video quality indicators are significantly better than the baseline mentioned in the paper."}, "weaknesses": {"value": "- Comparison with Video Bokeh Methods. The authors only compared their with the image bokeh method, but it needs to be compared with video methods, such as VBR [1].\n- Complex scenes. The authors employ a multi-plane image (MPI) representation, and this representation can bring challenges, such as whether to divide an object into two different layers. The authors should discuss this situation.\n- The robustness of this method. The authors should compare their full model with degraded depth maps and without degraded depth maps.\n- Missing details. TS (token selection) is not defined in formula (4); How M̄=[1,M] aligns a “global token” with 2D masks should be provided; Specify mask resolutions for each block and the exact interpolation strategy; clarify how “near-focus vs. wide-interval” masks are selected per layer.\n- Dataset and generalization. Both synthetic training and testing are based on the construction of \"planar disparity\" (where d is an affine function of x and y). This simplifies geometry but differs significantly from real-world scenarios.\n\n[1] Luo, Yawen, et al. \"Video bokeh rendering: Make casual videography cinematic.\" Proceedings of the 32nd ACM International Conference on Multimedia. 2024."}, "questions": {"value": "- Weighted overlap inference strategy. I wonder if doing weighted overlap only during inference without the same operation during training will affect inference performance.\n- The number of image plane N. I want to know whether to use the same N or different N for different scenarios. If different, how is the value of N determined.\n- Additional metrics. Add LPIPS for frame-level perceptual quality;  \n- Dynamic effects. I suggest the author provide video effects"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f66UEwKWn7", "forum": "h05AulYT7g", "replyto": "h05AulYT7g", "signatures": ["ICLR.cc/2026/Conference/Submission5506/Reviewer_cW3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5506/Reviewer_cW3t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447485001, "cdate": 1761447485001, "tmdate": 1762918099825, "mdate": 1762918099825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for adding controllable Bokeh in videos. They fine tune a video diffusion model to accept a video and a corresponding explicit scene geometry conditioning and output a depth-aware Bokeh video in a single diffusion step. The authors propose a multi-stage training strategy that facilitates robustness to noisy input geometry and high temporal bokeh consistency. The authors provide extensive evaluations showing state-of-the-art results for controllable video bokeh, with control of the focus plane and bokeh intensity."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe authors leverage the prior of a video diffusion model in a novel way, to perform temporally consistent, controllable video bokeh. \n2.\tThe authors showcase good bokeh results. They also provide a supplementary video with their results and comparisons to other methods, which is very important for the qualitative assessment of their claims for temporally consistent bokeh addition.\n3.\tThe authors show extensive quantitative evaluations, emphasizing their lead over other competing methods.\n4.\tThe authors provide several ablations for their training strategy choices."}, "weaknesses": {"value": "Major:\n1.\tThe authors do not provide limitations for their method. Are there any scenarios where the model fails to generate a good bokeh video? Maybe in videos with fast motion, such as a car race.\nMinor:\n1.\tFigure 1 is not referenced.\n2.\tSM figures 10,11 – red border not corresponding to zoom-in area."}, "questions": {"value": "Please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SudpaIq5c9", "forum": "h05AulYT7g", "replyto": "h05AulYT7g", "signatures": ["ICLR.cc/2026/Conference/Submission5506/Reviewer_7dEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5506/Reviewer_7dEg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667403918, "cdate": 1761667403918, "tmdate": 1762918099615, "mdate": 1762918099615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel one-step diffusion framework for generating temporally coherent and depth-aware video bokeh effects. The proposed approach enables video refocusing with explicit control over the focal plane and bokeh intensity, addressing the limitations of prior image-based and video-based bokeh rendering methods. The framework leverages a focal-plane-adapted multi-plane image (MPI) representation to guide the diffusion process, ensuring temporal smoothness and accurate depth-dependent blur transitions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Innovative Approach:**\n   The use of an MPI-guided conditioning mechanism in a one-step diffusion framework for video bokeh generation is both original and well-motivated. It effectively bridges the gap between static image refocusing and temporally coherent video refocusing.\n\n2. **Temporal Coherence and Depth Awareness:**\n   The focal-plane-adapted MPI representation efficiently balances detail preservation in focused regions and smooth transitions in defocused areas, improving visual consistency across frames.\n\n3. **Comprehensive Experiments:**\n   The paper includes thorough quantitative and qualitative comparisons with existing methods, demonstrating consistent improvements in temporal stability, spatial accuracy, and controllability."}, "weaknesses": {"value": "1. **Dependence on Depth Estimation:**\n   The method relies on pre-trained depth estimation models as input. In dynamic or complex scenes, depth errors may propagate into the final bokeh rendering. The paper would be stronger with a sensitivity analysis or ablation showing how depth inaccuracies affect output quality.\n\n2. **Computational Efficiency:**\n   While the results are impressive, the paper provides limited discussion on computational cost. Diffusion-based models are typically resource-intensive; more details on runtime, memory consumption, and scalability (e.g., potential for real-time use) would enhance the practical relevance.\n\n3. **Dataset Bias and Generalization:**\n   The primary evaluation uses synthetic datasets. Although this allows for controlled comparisons, such datasets may not fully reflect real-world complexities such as fast motion, varying lighting, or occlusions. Additional experiments on diverse real-world datasets would strengthen claims of robustness and generalizability."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vk0UAoyHBa", "forum": "h05AulYT7g", "replyto": "h05AulYT7g", "signatures": ["ICLR.cc/2026/Conference/Submission5506/Reviewer_Y63a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5506/Reviewer_Y63a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5506/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826173467, "cdate": 1761826173467, "tmdate": 1762918099303, "mdate": 1762918099303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}