{"id": "M72B8jb7cA", "number": 437, "cdate": 1756739574539, "mdate": 1759898261102, "content": {"title": "Dynamic Target Attack", "abstract": "Existing gradient-based jailbreak attacks typically optimize an adversarial suffix to induce a fixed affirmative response, e.g., ``Sure, here is...''. However, this fixed target usually resides in an extremely low-density region of a safety-aligned LLM’s output distribution conditioned on diverse harmful inputs. Due to the substantial discrepancy between the target and the original output, existing attacks require numerous iterations to optimize the adversarial prompt, which might still fail to induce the low-probability target response from the target LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking framework relying on the target LLM's own responses as targets to optimize the adversarial prompts. In each optimization round, DTA iteratively samples multiple candidate responses directly from the output distribution conditioned on the current prompt, and selects the most harmful response as a temporary target for prompt optimization. In contrast to existing attacks, DTA significantly reduces the discrepancy between the target and the output distribution, substantially easing the optimization process to search for an effective adversarial prompt.\n\nExtensive experiments demonstrate the superior effectiveness and efficiency of DTA: under the white-box setting, DTA only needs $200$ optimization iterations to achieve an average attack success rate (ASR) of over $87$% on recent safety-aligned LLMs, exceeding the state-of-the-art baselines by over $15$%. The time cost of DTA is 2$\\thicksim$26 times less than existing baselines. Under the black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target sampling and achieves an ASR of $85$% against the black-box target model Llama-3-70B-Instruct, exceeding its counterparts by over $25$%. All code and other materials are available here.", "tldr": "", "keywords": ["large language model", "jailbreak attack", "adversarial attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c38ccab63505b8c9c62e636a4f99967db38e7be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a jailbreak attack framework, called Dynamic Target Attack (DTA), aiming to elicit harmful responses from safety-aligned LLMs. The key observation is that existing gradient-based jailbreak methods, such as GCG and AdvPrefix, optimize for a fixed affirmative response, which may be inefficient. Instead, DTA iteratively samples multiple candidate responses directly from the LLM’s output distribution and recognizes the most harmful one as the intermediate target. Extensive experiments across multiple LLMs, various baselines, and configurations validate the effectiveness and efficiency of DTA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ A new LLM jailbreak attack method that dynamically samples high-density responses from the target model.\n\n+ The experiments under white-box attack scenarios are comprehensive, which demonstrate the strong performance of DTA."}, "weaknesses": {"value": "- The limited black-box experiments and inconsistent performance compared with baselines cast doubt on the generalizability of DTA.\n\n- Figure 1 and the motivation of DTA are not well-explained.\n\n- No visualizations are provided to illustrate the optimized jailbreak prompts or dynamically sampled target responses."}, "questions": {"value": "Below are my major concerns:\n\n1. In the introduction, the authors stated that the design of DTA is motivated by the observation that a fixed affirmative response $T_{fixed}$ lies in a highly low-density region of the LLM’s output distribution, which suggests it requires many iterations for optimizing the adversarial suffices for successful jailbreak prompts. This motivation is somewhat illustrated by Figure 1. However, I found it difficult to comprehend the notations and the statistics reported in the figure. How are the adversarial suffix S and S* defined or computed? If a harmful prompt is given as input to a safety-aligned LLM, isn’t it expected that the conditional probability of triggering a non-refusal response will be extremely low? From the figure, the highest log probability is -28.03, which is still very low even conditioned on P+S*. Does this suggest a failure case of jailbreaking? All of these are pretty confusing.\n\n2. Following the previous point, there is a gap between the motivation and the core advantages of the proposed method over baseline attacks. The primary motivation for diversifying the target response is to enhance jailbreak efficiency; however, most of the experimental sections focus on showing the improved jailbreak success rates. Only Section 4.5 discusses the advantages in terms of efficiency and iteration costs. This is disconnected from my perspective, which requires further discussion. I expect an in-depth clarification of the key insight into why DTA is (expected to be) more effective in terms of ASRs than baseline attacks.\n\n3. While DTA demonstrates decent improvements under white-box settings, the performance under black-box settings (Section 4.3) seems weak. For example, the first row of Table 2 (Llama-3.2-1B-it $\\rightarrow$ Llama-3-70B-it), DTA achieves only 30% attack success, which is significantly lower than that of AdvPrefix. I’m not convinced that Table 2 validates the conclusion that DTA is highly effective in black-box settings. The scalability of the black-box experiments casts doubt about the generalizability of DTA. I strongly recommend that the authors test the generalizability of DTA with respect to real black-box LLMs, such as GPT-4 and Claude, as well as stronger safety-aligned LLMs like Llama Guard. \n\n4. The performance of the considered black-box baseline attacks appears to be quite weak, as shown in Table 2 in Section 4.3. Why do ReNLLM, PAP, and TAP only achieve around 10% jailbreak success for Llama-3-70B-it? These results seem to contradict the respective original papers, which require further clarification. Besides, the authors may want to include stronger black-box baselines, such as AdvPrompter [1] and GASP [2].\n\n[1] AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs, https://arxiv.org/pdf/2404.16873\n\n[2] GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs, https://arxiv.org/pdf/2411.14133\n\nBelow are a few minor questions:\n\n1. There are no visualizations of the optimized adversarial prompt and target response by DTA. How “natural” are the optimized jailbreak prompts? What is the final target response returned by the proposed attack framework? The authors are highly suggested to provide a few examples to discuss these questions. It would be even better if the authors could showcase the intermediate optimization results to better illustrate the proposed algorithm.\n\n2. Table 3 implies that the best performance is achieved with M=200 and T=1. Why not choose this configuration in the main experimental tables since it achieves the perfect 100% jailbreak success?\n\n3. Equation 12 requires the computation of the gradient of the DTA loss. How is the gradient computed for the discrete suffix space?\n\n4. In Section 3.3, $r*$ is always truncated to a fixed-length response $r_{L*}$ to mitigate the influence of noisy tokens and reinforce early-stage control. How exactly is this step performed (e.g., the length of $r_{L*}$)? Are there ablation studies that support the importance of this step?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper proposes a new jailbreak attack method. However, it does not provide any statements regarding the negative impact of the work or ethical considerations, which may raise concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eaniWYkhub", "forum": "M72B8jb7cA", "replyto": "M72B8jb7cA", "signatures": ["ICLR.cc/2026/Conference/Submission437/Reviewer_LdQP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission437/Reviewer_LdQP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667046892, "cdate": 1761667046892, "tmdate": 1762915520568, "mdate": 1762915520568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DTA optimizes adversarial suffixes by dynamically changing the expected target of the attack, making optimization easier. It achieves high ASR while being more efficient than prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a subtle flaw in existing gradient-based jailbreak attacks (i.e. GCG): they optimize for a fixed, low-probability target. This work introduces DTA which samples candidate responses directly from the target LLM's own output distribution and dynamically selects the most harmful one."}, "weaknesses": {"value": "- The mathematical notation is inconsistent across sections. In Section 1, the paper introduces $T$ and $S$; in Section 2, it shifts to $r$ and $r^*$, which appear to represent similar concepts. In Section 3.4, $T$ reappears alongside $R_{\\text{initial}}$ and $R_S^1$. This inconsistency creates confusion about whether $T$ refers to the target response or the truncated response. The authors should maintain consistent notation throughout the paper.\n- The paper lacks clarity regarding the optimization strategy used in the DTA algorithm (Line 14). It is unclear whether the suffix updates employ a discrete optimization approach (GCG) or a continuous optimization method followed by inaccurate decoding. Section 4.1 mentions the use of Adam for optimization, but the paper lacks a lot of implementation details.\n- The reported results raise skepticism. GCG is known to perform strongly in white-box settings, particularly on models such as Vicuna-7B and Mistral-7B, as shown in prior work and the original GCG paper. The authors should justify the significant discrepancies in performance not only for GCG but also for other compared frameworks.\n- Several key methodological details are missing. The proposed approach relies on a \"high-entropy decoding strategy\", effectively controlled by the temperature hyperparam. The paper does not specify the temperatures used for the target LLM. Similarly, no information is provided regarding the truncation length used during decoding. An ablation study of the length of this truncation would be useful as well.\n- The success of DTA is highly dependent on the target LLM’s temperature. If the responses (or truncated targets), consistently yield safe completions, DTA would likely fail to produce successful jailbreaks. The discussion in Section 3.4 does not really answer this limitation.\n- Further transferability experiments on black-box models would be appreciated.\n\nMinor Issues:\n- L67: The expression should be $\\log p(T_{\\text{sampled}} \\mid P + S)$ instead of $S^*$.\n- L379: DDTA?"}, "questions": {"value": "Several limitations and questions have been listed in the weaknesses. I am willing to revise my scores positively if all of the above concerns are satisfactorily addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cziEk5deyA", "forum": "M72B8jb7cA", "replyto": "M72B8jb7cA", "signatures": ["ICLR.cc/2026/Conference/Submission437/Reviewer_A5ZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission437/Reviewer_A5ZE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761769153874, "cdate": 1761769153874, "tmdate": 1762915520243, "mdate": 1762915520243, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Dynamic Target Attack (DTA), a novel jailbreaking method for safety-aligned large language models. In contrast to existing approaches that optimize adversarial suffixes toward fixed, low-probability target responses (e.g., “Sure, here is…”), DTA iteratively samples candidate responses directly from high-density regions of the target model’s output distribution and dynamically selects the most harmful one as the optimization target. This strategy substantially reduces the mismatch between the attack objective and the model’s natural output distribution. Experimental results show that DTA achieves an attack success rate (ASR) exceeding 87% on state-of-the-art safety-aligned LLMs within only 200 iterations—outperforming current state-of-the-art baselines by more than 15% while being 2× to 26× faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses a timely and important topic in AI security research.\n\n2.The paper is well-written with clear presentation and logical flow.\n\n3.The experimental evaluation is solid and comprehensive."}, "weaknesses": {"value": "**W1: Scalability and Cost Trade-off Concerns**\n\nThe proposed method relies on sampling multiple outputs from the target LLM to select the most harmful potential response as the optimization target. A critical concern arises: for well-aligned models, limited sampling may be insufficient to generate explicitly harmful responses. However, conducting extensive sampling to increase the probability of obtaining harmful responses would incur substantial computational costs. How to achieve an effective trade-off between sampling adequacy and computational efficiency in practical deployment remains an unresolved challenge that warrants deeper investigation.\n\n**W2: Lack of Convergence Analysis for Dynamic Targeting**\n\nThe paper adopts a dynamic target optimization strategy, but it lacks rigorous analysis on how to mitigate potential non-convergence risks induced by repeatedly changing optimization targets. Specifically, the paper would benefit from fine-grained quantitative analysis demonstrating the convergence behavior and approximation effectiveness throughout the optimization process under dynamic target selection. Without such analysis, the stability and reliability of the proposed approach remain questionable.\n\n**W3: Insufficient Evidence for Fair Experimental Comparison**\n\nThe fairness of hyperparameter settings across different baseline methods lacks sufficient justification. While the authors claim to use hyperparameters from open-source implementations and default configurations, there is inadequate discussion on whether these settings are comparable across methods. Ensuring that different approaches are evaluated under consistent attack budgets (e.g., iteration counts, batch sizes, total forward passes) is crucial for fairly characterizing their effectiveness. The substantial variation in iteration budgets across methods (e.g., GCG: 1,000 vs. DTA: 200) raises concerns about whether comparisons are conducted under equivalent computational resources."}, "questions": {"value": "Q1: What are the specific hyperparameters used for each baseline method? Are the total computational budgets (e.g., total number of model forward passes) consistent across different methods to enable fair comparison?\n\nQ2: How does the numbers and parameters of sampling affect the selection of optimization targets and the final attack performance? An ablation study on this critical hyperparameter would strengthen the empirical analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nx9IcLGGQn", "forum": "M72B8jb7cA", "replyto": "M72B8jb7cA", "signatures": ["ICLR.cc/2026/Conference/Submission437/Reviewer_QhRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission437/Reviewer_QhRH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821750389, "cdate": 1761821750389, "tmdate": 1762915519968, "mdate": 1762915519968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a gradient-based method for jailbreaking safety-aligned LLMs. The core idea of the method (Dynamic Target Attack, DTA) is to dynamically sample multiple candidate responses from the model’s own output distribution. Compared with the original optimization that optimizes fixed, low-probability adversarial suffixes, this method is more efficient and effective. The authors conduct extensive experiments on two standard datasets and compared DTA with multiple baselines across 5 different open-sourced LLMs, and demonstrate the superior performance under both white-box and black-box settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea of DTA that using dynamically sampled, model-native responses as the optimization objective is novel. Compared with GCG-based methods which optimize fixed targets in a low-density distribution, DTA samples targets from higher-density regions, leading to significant efficiency improvements (as shown in Table 4).\n\n2. The authors provide thorough experimental results that compared DTA against six white-box and three black-box baselines across five LLMs on two benchmarks.\n\n3. The paper is clearly structured and includes sufficient implementation details. Algorithm 1 gives a clear overview of the methodological design."}, "weaknesses": {"value": "1. During the exploration stage (section 3.2), the dynamic sampling of harmful targets depends on the performance of the judge model (GPTFuzzer). If the judge model is limited or weak in detecting subtle or novel harmful content, the exploration process may fail to identify the most effective targets. It would be helpful if the authors could discuss or evaluate DTA’s performance when using different judge models.\n\n2. The paper would be stronger if the authors could provide a case study of optimized adversarial examples. It would be interesting to see concrete examples of harmful responses generated by DTA."}, "questions": {"value": "1. As mentioned in Weakness 1, how sensitive is DTA’s performance to the choice of judge model? Have the authors compared results using different judges?\n\n2. Have the authors evaluated the transferability of DTA on closed-source LLMs? According to Table 2, DTA shows strong transferability, and optimizing on stronger surrogate models (e.g., 8B vs. 1B) seems to have better results. In addition, why do the authors report only the $ASR_G$ results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tqPfxjEtZB", "forum": "M72B8jb7cA", "replyto": "M72B8jb7cA", "signatures": ["ICLR.cc/2026/Conference/Submission437/Reviewer_Z2xg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission437/Reviewer_Z2xg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission437/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762887278892, "cdate": 1762887278892, "tmdate": 1762915519832, "mdate": 1762915519832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}