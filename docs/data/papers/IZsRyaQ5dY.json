{"id": "IZsRyaQ5dY", "number": 2140, "cdate": 1756996253027, "mdate": 1759898167033, "content": {"title": "HebbGate: Local Reward‑Modulated Gating for Continual Learning", "abstract": "Neural networks that learn continually should acquire new tasks without revisiting old data and with small per-task overhead.  In parameter-isolation CL, existing approaches typically learn dense task masks via backpropagation, which couples mask learning to the backbone optimiser, adds training compute, and inflates\nmemory with extra mask parameters.\nWe introduce HebbGate, a parameter-isolation method for continual learning that uses local, reward-modulated gates in place of backpropagated masks. Crucially, each task adds just one scalar per channel (not per weight), keeping memory growth tiny and the masks interpretable. A simple utilisation penalty discourages reuse of over-popular channels, and a $\\kappa$-decay capacity warm-up lets new tasks explore larger masks before annealing to the target sparsity, mitigating order bias and improving forward transfer.\nFor class-incremental evaluation, we replicate each sample across all task-masks and perform one fused forward pass.\nHebbGate is equally effective when applied to a two‑layer MLP, a shallow CNN, or a gated ResNet‑18. On Permuted‑MNIST, Split‑CIFAR‑10, and CIFAR-100, it matches the accuracy of gradient‑based masking methods while keeping the parameter footprint essentially unchanged. HebbGate, therefore, offers a lightweight, transparent alternative for exemplar-free, single-head continual learning.", "tldr": "HebbGate applies a local, reward‑modulated Hebbian rule to create sparse task‑specific subnetworks, delivering fast continual learning with minimal extra memory.", "keywords": ["Continual Learning", "Incremental Learning", "Hebbian Plasticity", "Local Learning Rules"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4e15340ca47b7cbab5888e49c1c5ecf31a2e98e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a gating method for parameters based on a reward modulated Hebbian rule, in which one scalar is added per neuron instead of per weight. This helps keep memory growth sublinear and mask interpretable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. HebbGate is a sensible algorithm satisfying realistic desiderata in class-incremental learning.\n2. Extensive theoretical and empirical analysis are presented."}, "weaknesses": {"value": "1. No Mention or consideration of recent related works in continual learning, e.g., most prior work in Section 2 is from 2017-2019.\n2. The experimental setup is somewhat outdated. The experiments use CIFAR 10 and CIFAR 100 with AlexNet and ResNet.\n3. No recent continual learning baselines included (see Table 1).\n4. The empirical results are mixed, especially (1) EFC (only recent method included in baselines) outperforms HebbGate, (2) context (e.g., computational/memory overheard) for each baselines and HebbGate not presented.\n5. Only average accuracy is presented in the main paper whereas other metrics critical to continual learning such as backward transfer should be presented. This can be visually inferred from Figure 3 but only roughly.\n6. Conceptually, no discussion on how HebbGate applies to more recent transformer models."}, "questions": {"value": "1. How does HebbGate apply to transformers?\n2. Can HebbGate scale to more recent and challenging datasets, e.g., MTIL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vJhDculooc", "forum": "IZsRyaQ5dY", "replyto": "IZsRyaQ5dY", "signatures": ["ICLR.cc/2026/Conference/Submission2140/Reviewer_KhyT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2140/Reviewer_KhyT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872178980, "cdate": 1761872178980, "tmdate": 1762916046065, "mdate": 1762916046065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of continual learning with a parameter-isolation approach. The authors first identified several issues with existing parameter-isolation strategies, such as dependence on backpropagation, memory overhead, and task bias. To address these issues, the authors proposed HebbGate, which applied a scalar mask to each channel. The update strategy of the mask takes activation energy, usage penalty, and margin reward, which do not depend on backpropagation. The authors then validated their proposed method on a few backbone models across different evaluation benchmarks, showing the improved performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well motivated\n\nIt is very interesting to consider parameter isolation without backpropagation and heavy memory overhead.\n\n2. The presentation is clear\n\nI appreciate the efforts of the authors to clearly explain the details in every section: motivation, method, implementation and experiment.\n\n3. The proposed method is lightweight and effective"}, "weaknesses": {"value": "1. The motivation needs to be further justified\n\n2. The contribution for forward transfer is not clear\n\n3. The applicability to deeper networks is not clear\n\n4. The presentation can be improved\n\nPlease see the Question section below for details."}, "questions": {"value": "1. The motivation needs to be further justified\n\nOne of the drawbacks of existing parameter-isolation approaches, as claimed by the authors, is the bias of channels for early tasks (lines 48, 107-108). However, this is not well presented in the paper. Therefore, it is not a solid support to motivate the proposed method.\n\n2. The contribution for forward and backward transfer is not clear\n\nThe authors claimed that the proposed method improves transfer (lines 100, 114, 131,217). However, this is not supported by a quantified evaluation. There are metrics for evaluating forward and backward transfer. I would recommend that the authors include this metric if such a point is the main emphasis in the paper.\n\n3. The applicability to deeper networks is not clear\n\nCurrent experiments are with shallow neural networks. I believe parameter isolation is more interesting when the model's capacity is much larger and the complexity of the tasks grows as well. \n\n\n4. The presentation can be improved\n\nThe authors refer to many tables and figures in the appendix in the experiment section. While it is not forbidden to do so, it largely reduces the readability of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d8mTmHHxS3", "forum": "IZsRyaQ5dY", "replyto": "IZsRyaQ5dY", "signatures": ["ICLR.cc/2026/Conference/Submission2140/Reviewer_iWqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2140/Reviewer_iWqj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995446103, "cdate": 1761995446103, "tmdate": 1762916045780, "mdate": 1762916045780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HebbGate, a local, reward-modulated gating mechanism for continual learning based on a three-factor Hebbian update. Instead of the dense, backprop-trained task masks often found in parameter-isolation approaches, HebbGate uses one scalar per channel (rather than per weight), updating via local activation, a global margin reward, and a utilization-aware penalty. The method incorporates a k-decay schedule allowing new tasks to initially explore greater capacity before annealing to a target sparsity, helping balance transfer and forgetting. Experiments on Permuted-MNIST, Split-CIFAR-10, and Split-CIFAR-100, across several architectures, demonstrate competitive or superior performance to state-of-the-art exemplar-free methods, with lower memory and computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Parameter Efficiency: Each task adds only one scalar per channel, versus dense masks per parameter, resulting in negligible memory growth and making the approach scalable. \n2. Forward Transfer and Order Robustness: The k-decay schedule and utilization-aware gate initialization directly target the early-task monopoly problem seen in prior work, supporting more equitable channel budgets and improved forward transfer."}, "weaknesses": {"value": "1. Limited comparison with recent methods: Only one method from 2024 has been compared with, rest of the compared methods are older. This reflects poorly on the claim of achieving state-of-the-art performance.\n2. Poor Performance on task incremental learning for the CIFAR-100 dataset\n3. Poor Performance on ResNet for class-incremental learning on CIFAR-100. The approach seems to perform poorly on ResNet but better on AlexNet, which seems strange.\n4. The proposed method shows better performance on CIFAR-10. However, the total classes being 10, makes it a weak case for an incremental learning setup."}, "questions": {"value": "Is there any reason why the authors chose not to give results on more established ImageNet-100 and ImageNet-1000 datasets for class-incremental learning?\nDoes the proposed method work better, when the number of classes are less?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hDA3MTmhWE", "forum": "IZsRyaQ5dY", "replyto": "IZsRyaQ5dY", "signatures": ["ICLR.cc/2026/Conference/Submission2140/Reviewer_2hp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2140/Reviewer_2hp2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018932644, "cdate": 1762018932644, "tmdate": 1762916045337, "mdate": 1762916045337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}