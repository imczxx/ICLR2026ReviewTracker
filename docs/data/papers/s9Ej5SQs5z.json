{"id": "s9Ej5SQs5z", "number": 21327, "cdate": 1758316285394, "mdate": 1759896928260, "content": {"title": "Learning What to Remember for Non-Markovian Reinforcement Learning", "abstract": "Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize the classic T-Maze domain, which gives us direct control over the degree of non-Markovian dependencies in the environment. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards and achieve convergence in the stack management policy without excessive removal of important experiences.", "tldr": "Adaptive Stacking enables RL agents to efficiently handle long-term dependencies by learning what to remember, reducing memory and compute costs while preserving optimality.", "keywords": ["Reinforcement Learning", "Non-Markovian", "Partial Observation", "State abstraction", "Bounded Agents", "Memory"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a7c05404568fb3b169dff1dfbd82210d2ad23bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors concern themselves with a subset of POMDPs where a few distinct observations are sufficient to predict future value. In other words, POMDPs that admit \"keyframe\"-based approaches. Frame stacking is a simple and popular approach to POMDPs, where agents buffer the last $k$ observations and treat this as a Markov state. Such methods assume that the last $k$ frames are all that is necessary for value estimation, where in many cases this is not true. In this work, the authors propose to learn which $k$ frames to keep, rather than using the most recent $k$ frames. They do this by extending the agent action space to include an \"overwrite\" action: a discrete action that corresponds to the buffer index where the agent will store the most recent observation. \n\nThe authors provide a formal problem definition, introduce their method called \"Adaptive Stacking\", and then provide various theorems and proofs concerning the optimality of their approach. Finally, the perform various experiments evaluating their method. They report returns, regret, and other memory metrics on a T-maze task where optimal $k$ is known ahead of time. They find that their method outperforms others. Finally, they evaluate their method on more difficult tasks like XorMaze, Rubik's Cube, and POPGym."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a simple approach that performs quite well. The idea of intelligently selecting a buffer of keyframes is a good idea. Finally, the authors provide promising results across various experiments."}, "weaknesses": {"value": "## Writing and Contribution 2\nThe main shortcoming of this paper is its writing. The paper is generally cluttered and takes many words to say little. It feels as if the authors purposely introduce a lot of unnecessary math to motivate their **theoretical analysis** contribution. \n\nAlmost all theoretical contributions are based on these three facts:\n- The authors only concern themselves with $k$th order POMDPs, where the last $k$ observations are always a Markov state: $s_t = x_{t:t-k}$\n- Frame stacking $k$ of the most recent frames is therefore sufficient to solve $k$th order POMDPs (again, $s = x_{t:t-k}$)\n- The authors' method subsumes a regular frame stacking approach ($s = x_{t:t-k}$)\n\nI do not find value in any of the three theoretical contributions:\n- Proposition 1: $V(s_t) = V(x_{t:t-k})$, this is just the definition of $k$th order POMDP\n- Theorem 1: If standard framestacking is optimal, Adaptive Stacking can be too since it can learn to represent standard framestacking\n- Theorem 2: $V(s_t) > V(s'\\_t)$ implies $V(x_{t:t-k}) > V(x'\\_{t:t-k})$, again this is just the definition of $k$th order POMDP ($s = x_{t:t-k}$)\n- Theorem 3: Standard $Q$ learning converges to a fixed-point optimal policy (we can let $s = x_{t:t-k}$)\n\nBeyond the unncessary analysis, there is much related work in method section (4.3) that should be integrated into related work section (3).\n\n## Buffer Index Selection Methodology\nThe authors' method increases the dimensionality of the action space by $k$. In $Q$ learning, this means the size of the $Q$ table grows by $k$. For harder problems with large $k$, this can quickly become intractable. Furthermore, the proposed method is not content-aware. It cannot see what data current occupies a specific slot. Contrast this with the Differentiable Neural Computer [1] which both (1) decouples the $Q$ table growth from buffer size and (2) provides content-aware reading and writing via keys/queries. The authors should consider and compare their approach to such indexing methods."}, "questions": {"value": "> selective gating mechanism that retains task-relevant information while filtering out\nirrelevant inputs\n\nThe connection to computational psychology is tenuous. Humans do compress sensory information, but it is not clear that they use a keyframe based approach. Do they only consider $k$ distinct observations, or simply build a state from tiny bits of information from many observations? \n\n> This equation (2) shows that the agent’s value under compressed memory is an expectation over possible latent histories. When the memory stack discards critical observations, this conditional distribution becomes broader, increasing uncertainty. As such, it is clear that Adaptive Stacking can be seen as a form of state abstraction in which multiple histories are compressed together in the estimate of the value function.\n\nThis is vague, uninformative, and frankly incorrect. You should define \"compressed memory\", how does it differ from \"latent history\"? Equation 2 does not contain an expectation. You cannot prove that the value distribution becomes \"broader\". What does it mean for multiple histories to be compressed together?\n\n## References \n\n[1] Graves et al., Hybrid computing using a neural network with dynamic external memory"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bnu72BLy2M", "forum": "s9Ej5SQs5z", "replyto": "s9Ej5SQs5z", "signatures": ["ICLR.cc/2026/Conference/Submission21327/Reviewer_piza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21327/Reviewer_piza"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818201045, "cdate": 1761818201045, "tmdate": 1762941698081, "mdate": 1762941698081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Adaptive Stacking, a method for efficient memory use in RL under non-Markovian conditions. Unlike Frame Stacking, which keeps a fixed window of recent observations, Adaptive Stacking learns which past observations to retain or discard through a memory policy. The authors prove convergence under both Monte Carlo and TD learning and view the method as a structured state abstraction. Experiments on TMaze, XorMaze, VelocityOnlyCartPoleHard POPGym task , and Rubik's Cube show that it matches or surpasses Frame Stacking while using much less memory and computation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Interesting idea in promising area of memory-intensive RL:** Adaptive Stacking lets the agent decide what to keep in memory instead of using a fixed-size history window. This is a conceptually simple but powerful shift from passive to active memory control.\n2. **Theoretical analysis:** Theoretical analysis is rare among works on memory in RL.\n3. **Solid experimental results:** The experiments show consistent improvements over Frame Stacking across multiple environments. AS matches oracle performance with far smaller memory and achieves lower \"memory regret\",  demonstrating its ability to retain only useful observations."}, "weaknesses": {"value": "1. **Insufficient exposition of the method in the main text:** The central algorithm (Algorithm 1) is specified in the Appendix without any detailed description either in the main text or in the Appendix (which is optional reading). The main text provides only a fairly general rule for updating the frame stack (Equation 1) and a general diagram (Figure 1) without describing the details. To convey the proposed idea more clearly, more attention should be paid to the method in the main text (for example, by moving part of the theoretical section to the Appendix).\n2. **Reproducibility**: The authors did not attach the code to their submission and did not provide a more detailed description of the operations used in Algorithm 1. For example, how are the pop() and push() operations defined? Personally, after reading the text, the principle of the method remains unclear to me.\n3. **Limited experimental diversity:** The paper considers only relatively simple low-dimensional diagnostic tasks, while full-fledged 2D (e.g., Minigrid tasks) or 3D (e.g., ViZDoom-Two-Colors) tasks are not considered. Additionally, no continuous-control or high-dimensional sensory domains are evaluated, so the claimed scalability to \"big worlds\" (L068) remains speculative. The results on Rubik’s Cube and single POPGym task, while interesting, are not sufficient to demonstrate generality.\n4. **Clarity and presentation:** The exposition is extremely dense, with excessive theoretical framing relative to algorithmic intuition. The connection between theoretical abstraction (state compression) and the empirical AS implementation could be made explicit. Figure 1 and Appendix, Figure 6 illustrate the concept, but there is little narrative about how the agent learns which memory to drop. Overall, the readability of the work needs to be improved.\n5. **Limited Related Work:** The paper considers only agents without memory, RNN-based agents, and Transformer-based agents, but in the field of RL there is a whole class of algorithms designed to solve memory-intensive tasks (for example, using linear attention [1], memory tokens [2], SSMs [3], etc.)."}, "questions": {"value": "1. How are push() and pop() operations defined?\n2. The paper states that the proposed method is general and does not depend on the chosen RL policy, although experiments are only presented for q-learning and PPO. Where can I find the pseudocode for adaptive stacking+PPO? Is it possible to get a general idea of the pseudocode for an arbitrary RL policy to support the claims in the paper?\n3. What is the empirical overhead of learning over both $a_t$​ and ​ $i_t$? For large $k$, does the discrete selection make exploration unstable?\n4. How will the proposed method work on tasks that require rewriting information to memory or retaining information about multiple events (for example, if we are dealing with a multidimensional T-Maze, where we may have more than one cue-turn pair, or if some previously important information becomes obsolete over time in the environment and at a certain point we need to remember other information)?\n5. How does the proposed method behave on classic MDP tasks where we are dealing with states rather than observations? Are there any performance drops or additional computational overheads?\n6. If we consider a simple k-order MDP (e.g., Atari Pong/Breakout), are there any confusing observations in the Adaptive Stack that interfere with the internal estimation of the direction/speed of the projectiles?\n7. In tasks violating Assumption 4.1, how does AS behave empirically? Does it degrade gracefully, or can it catastrophically forget relevant histories?\n\nCertain elements of the work would benefit from additional clarification or evidence, which I look forward to seeing addressed in the rebuttal.\n\n[1] Pramanik, Subhojeet, et al. \"AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning.\" arXiv preprint arXiv:2310.15719 (2023).\n\n[2] Cherepanov, Egor, et al. \"Recurrent action transformer with memory.\" arXiv preprint arXiv:2306.09459 (2023).\n\n[3] Lu, Chris, et al. \"Structured state space models for in-context reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 47016-47031."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8vNSr6rXou", "forum": "s9Ej5SQs5z", "replyto": "s9Ej5SQs5z", "signatures": ["ICLR.cc/2026/Conference/Submission21327/Reviewer_DARz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21327/Reviewer_DARz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917842213, "cdate": 1761917842213, "tmdate": 1762941697743, "mdate": 1762941697743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the use of adaptive stacking memory, a discrete external memory mechanism controlled by a reinforcement learning agent, to address partially observable decision-making problems. The central idea is to allow the agent to selectively store observations by choosing memory operations as part of its action space.\n\nThe authors provide a theoretical analysis, establishing conditions under which both Monte Carlo and Temporal Difference learning methods can successfully learn to control this memory structure in partially observable environments.\n\nEmpirical results demonstrate that agents can, in practice, learn to use adaptive stacking to solve a range of partially observable tasks. The method is also compared to soft memory architectures such as LSTMs and Transformers. A key finding is that adaptive stacking achieves comparable performance while, in principle, requiring significantly fewer computational resources—suggesting it may be a more efficient alternative in certain settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of equipping reinforcement learning (RL) agents with external memory that they can manipulate through actions has a long history [1–5]. However, to the best of my knowledge, this is the first work to formally establish conditions under which such an approach can be guaranteed to work. While I find the empirical results less compelling—since similar conclusions have been reported in prior work [3–5], to some extent—the theoretical contributions represent a meaningful advancement. These results offer valuable insights into the broader research effort on memory-augmented RL.\n\nAlthough I remain skeptical that hard memory mechanisms alone will be sufficient to solve partially observable problems in the long term (as discussed in the weaknesses section), I consider this paper an important milestone in the development of memory-augmented RL methods.\n\nIn fact, I encourage the authors to consider generalizing their theoretical results to encompass a broader class of external memory architectures [4]. Such an extension could significantly enhance the impact of the work. From my understanding, this generalization may be relatively straightforward to achieve and would further strengthen the paper’s contributions.\n\n---\n\n**References**\n\n[1] Littman, M. L. (1993). An optimization-based categorization of reinforcement learning environments. From animals to animats, 2, 262-270.\n\n[2] Peshkin, L., Meuleau, N., & Kaelbling, L. (2001). Learning policies with external memory. arXiv preprint cs/0103003.\n\n[3] Zhang, M., McCarthy, Z., Finn, C., Levine, S., & Abbeel, P. (2016, May). Learning deep neural network policies with continuous memory states. In 2016 IEEE international conference on robotics and automation (ICRA) (pp. 520-527). IEEE.\n\n[4] Icarte, R. T., Valenzano, R., Klassen, T. Q., Christoffersen, P., Farahmand, A. M., & McIlraith, S. A. (2020). The act of remembering: A study in partially observable reinforcement learning. arXiv preprint arXiv:2010.01753.\n\n[5] Demir, A. (2023). Learning what to memorize: Using intrinsic motivation to form useful memory in partially observable reinforcement learning. Applied Intelligence, 53(16), 19074-19092."}, "weaknesses": {"value": "**Presentation and Relation to Prior Work**\n\nThe paper is somewhat challenging to follow due to its frequent references to the appendix. For instance, some figures discussed in the main body (e.g., Figure 7a) are located in the appendix, requiring readers to continually switch back and forth to fully grasp the content. This disrupts the reading experience and undermines the clarity and coherence of the presentation.\n\nAdditionally, the paper shares notable conceptual similarities with the work of Icarte et al. (2020). I encourage the authors to explicitly discuss these connections, clearly articulating both the similarities and the distinctions, to better situate their contribution within the existing literature.\n\n---\n\n**Limitations**\n\nThe central idea of using a \"hard memory\"—where the agent explicitly decides what to store based on its actions—presents a fundamental limitation. It is inherently difficult to determine, a priori, whether a particular observation should be stored in external memory. For example, suppose an agent enters a room and sees a picture. Should it store this observation? At that moment, there is no way to know whether this information will be relevant in the future. If, hypothetically, recalling the picture becomes crucial 1000 steps later, the agent must retain it for that entire duration. However, under a random exploration policy, the probability of retaining that memory diminishes rapidly, as each step introduces a chance of overwriting it. This poses a significant challenge for reinforcement learning agents, which rely on exploration to learn optimal behavior.\n\nAnother concern arises from allowing the agent to manipulate its external memory through actions, as this opens the door for the agent to \"fool itself\" into believing it has reached a favorable state when it has not. In the XorMaze domain, for example—assuming the agent observes only the color of its current location—the optimal policy would involve storing the color of the first room (e.g., green), then navigating to the second room and storing that color as well. If both rooms are green, the resulting memory state [green, green] would correctly lead the agent to the 'equal color' goal and a reward. However, due to the nature of temporal-difference (TD) learning, such an optimal policy is not stable. Instead, the agent may learn to store the first room’s color twice, thereby fabricating the high-value memory state [green, green] without actually visiting both rooms. This non-Markovian shortcut is locally optimal under one-step TD learning, as it transitions the agent from a low-value to a high-value state. As a result, learning to control external memory becomes complex and unstable—even in the tabular setting—raising concerns about the robustness and reliability of the learned policies.\n\nA more intuitive example involves an agent tasked with cleaning a room and then plugging itself in to receive a reward. If the agent stores a picture of a clean room, it may mistakenly believe the room is clean, even if it hasn't performed the cleaning task. Since the external memory is the only source of state information, the agent cannot distinguish between a memory of a clean room and the actual clean state. This ambiguity can lead to unstable learning in most POMDPs.\n\nI appreciate the authors' efforts to introduce assumptions (e.g., Assumption 4.1) that create settings where the proposed approach can succeed. However, the need for such assumptions underscores the core issue. It may simply be unwise to allow RL agents to directly control their memory.\n\nIn contrast, alternative approaches based on soft (differentiable) memory architectures—such as LSTMs or Transformers—do not suffer from the same issues. For example, when training an LSTM, the entire trajectory is taken into account, with gradients backpropagated from the end to the beginning. This enables the model to recognize temporally extended patterns that contribute to high returns by leveraging the full trajectory—giving it a clear advantage over hard memories. In adaptive stacking, the agent must decide in the moment whether storing a piece of information will be useful later, without knowing future outcomes. By contrast, LSTMs benefit from backward credit assignment, where the learning signal propagates from future rewards to earlier decisions. While LSTMs are known to suffer from vanishing gradients, this issue is largely mitigated in Transformer-based architectures, which further strengthens the case for using differentiable memory mechanisms in RL.\n\nIn short, there are good reasons to believe that soft memory mechanisms are better suited for handling partial observability than hard memory. That said, hybrid approaches combining both paradigms may offer promising directions for future work.\n\n---\n**Convergence of Temporal Difference Learning**\n\nTheorem 3 appears incomplete, as it does not reference Assumption 4.1. The more complete version—Theorem 5 in the appendix—states that Q-learning converges to an optimal policy \"if policies in $\\mathcal{M}_t$​ are value-consistent.\" Does this imply that every policy must be value-consistent? If so, the assumption seems overly strong and likely only holds in fully observable settings. For instance, in any partially observable domain, a policy that always erases memory would presumably not be value-consistent.\n\nIn fact, from my reading, the domains in Appendix C appear to be value-consistent for some policies, but not for all. If that interpretation is correct, Theorem 5 would not apply to those domains. Am I misunderstanding something? Ideally, the theorem would only require the optimal policy to be value-consistent. However, the proof seems to rely on the stronger assumption that all policies are value-consistent—which, as noted, is quite restrictive.\n\nAdditionally, the theorem assumes a “fixed exploratory policy.” It would be helpful if the authors clarified what this entails. Is it a fixed behavior policy that is also value-consistent? If so, the theorem might apply more broadly—but only under the assumption that the behavior policy already makes effective use of memory. In that case, the environment becomes effectively Markovian, and the result becomes less interesting, as it sidesteps the core challenge of learning how to use external memory effectively in the first place.\n\n\n---\n\n**Experiments**\n\nThe experimental section is limited to only four domains (T-Maze, XorMaze, Rubik’s Cube, and POPGym Cartpole), despite the fact that Appendix C lists many more domains where the proposed method should, in principle, be applicable. This narrow selection raises concerns about the generality of the empirical findings.\n\nMoreover, the comparison with alternative memory architectures—such as LSTMs or Transformers—is conducted only on the simplest domain (the passive T-Maze). This makes it difficult to assess the relative strengths of Adaptive Stacking in more complex settings. I also suspect that the LSTM baseline, which uses only the current observation, could perform significantly better with more extensive hyperparameter tuning. In my experience, increasing the batch size and using multiple agents can substantially improve PPO-LSTM performance in partially observable environments.\n\nAdditionally, it is unclear why Adaptive Stacking outperforms other memory mechanisms such as those proposed by Icarte et al. (2020) and Demir (2023), especially given that those approaches use much simpler memory action spaces (e.g., push or no-push). In contrast, Adaptive Stacking requires more complex decisions, such as selecting which memory entry to erase. According to Figure 27, its performance is substantially better, which seems surprising. One possible explanation is that the intrinsic motivation mechanism is influencing the results in unexpected ways. If that is the case, it might have been more informative to evaluate the method without intrinsic rewards.\n\nFinally, a central claim of the paper is that Adaptive Stacking is more efficient than alternative methods. However, the experimental section does not report computational time or memory usage. While this is a minor concern, including such metrics would strengthen the argument for the method’s practical advantages.\n\n---\n\n**References**\n\n[1] Icarte, R. T., Valenzano, R., Klassen, T. Q., Christoffersen, P., Farahmand, A. M., & McIlraith, S. A. (2020). The act of remembering: A study in partially observable reinforcement learning. arXiv preprint arXiv:2010.01753.\n\n[2] Demir, A. (2023). Learning what to memorize: Using intrinsic motivation to form useful memory in partially observable reinforcement learning. Applied Intelligence, 53(16), 19074-19092."}, "questions": {"value": "1. **Hard vs. Soft Memory Architectures**: Could the authors elaborate on the long-term benefits and trade-offs of using hard (discrete, agent-controlled) versus soft (differentiable) memory mechanisms for solving partially observable problems? As I mentioned earlier, I am inclined to believe that hard memory may be less effective in the long run, but I may be overlooking important advantages.\n2. **Clarification on Theorem 5**: Could you clarify the assumptions underlying Theorem 5? Specifically, does it require that the agent follows a fixed policy that is already value-consistent (i.e., one that makes effective use of memory)? Or does it assume that all policies in the policy class are value-consistent? Or both?\n3. **Scope of Theorem 5**: Does Theorem 5 imply that Q-learning will, in principle, converge to an optimal policy for all the domains listed in Appendix C, assuming arbitrary Q-value initialization and sufficient exploration (e.g., via $\\epsilon$-greedy policies in the limit)?\n4. **Performance of Adaptive Stacking**: Could the authors provide insight into why Adaptive Stacking outperforms the memory mechanisms proposed by Demir (2023)? Given that those approaches use simpler memory action spaces (e.g., push/no-push), it is surprising that Adaptive Stacking, which involves more complex memory control decisions, performs significantly better (as shown in Figure 27). One possible explanation might be the influence of the intrinsic motivation mechanism, but if that is the case, it would be helpful to see results without it.\n5. **Computational Efficiency**: One of the paper’s key claims is that Adaptive Stacking is more computationally efficient than soft memory architectures. Could the authors provide empirical evidence to support this claim, such as runtime or memory usage comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Un2ao21Zp", "forum": "s9Ej5SQs5z", "replyto": "s9Ej5SQs5z", "signatures": ["ICLR.cc/2026/Conference/Submission21327/Reviewer_8iuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21327/Reviewer_8iuy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945506601, "cdate": 1761945506601, "tmdate": 1762941697248, "mdate": 1762941697248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission proposes Adaptive Stacking (AS), a novel meta-learning algorithm that enables RL agents to learn which observations to retain in a fixed-size memory state rather than using standard frame stacking's FIFO policy. This is relevant for environments where only a relatively small subset of distant past observations are important for optimal decision-making. The authors provide convergence guarantees under a value-consistency assumption, analyzes computational benefits, and demonstrates the approach on T-Maze variants and other memory tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The theoretical derivation on convergence is well-constructed with formal definitions, assumptions and proofs. I appreciate the detailed discussion on value consistency assumption.\n2. The proposed AS framework is architecture-agnostic, demonstrating applicability to MLPs, LSTMs, and Transformers. The authors demonstrate practical benefits of AS over FS on multiple memory-based tasks.\n3. Generally clear writing with good motivation, with comprehensive appendix, detailed proofs and experiments.\n4. Detailed analysis and discussion on improvements in compute and memory (Table 1 and Appendix F)."}, "weaknesses": {"value": "1. The comparison with Demir (2023) is limited. Can you provide more comprehensive comparisons and justify when your action space design is preferable? It would be helpful to see more detailed comparison on all benchmarks against Demir 2023, which uses push/skip operations. Currently, only single comparison on TMazes is presented in the appendix (Figure 27). \n2. In addition, since a lot of the tasks considered are short memory (e.g. maze length 2-6), why aren’t RNN-based agents that could potentially learn full belief state estimation (e.g. RL2, or methods with predictive representation such as VariBAD) included as baseline methods to compare against? I understand they might struggle with very long memory, but since the memory length requirements in your tasks are not super long, these methods should be included to fully demonstrate when memory management is better than belief state learning.\n3. Most experiments are T-Maze variants (Figures 2, 3, 8-26), while POPGym shows marginal improvements with high variance, and Rubik's cube is interesting but under-analyzed. Since in the appendix a lot of tasks are said to satisfy the value consistency assumption, it would be more convincing to demonstrate the performance of AS on more diverse tasks.\n4. Currently there seems to be little discussions on practical consideration of choosing κ. Can you provide more details on how to choose κ, what happens when κ is chosen too small, and how does performance vary as κ changes?"}, "questions": {"value": "1. The generalization results (Figures 19-26) seem promising. Could you provide more discussion on why AS generalizes better?\n2. In the RNN-based agent section under related work, since the tasks you considered are POMDPs more generally, literature on methods such as RL2 (Duan et al, 2016; Wang et al, 2016) and methods extending RL2 with predictive representations such as VariBAD (Zintgraf et al, 2021) should also be cited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Am3ISqrnXU", "forum": "s9Ej5SQs5z", "replyto": "s9Ej5SQs5z", "signatures": ["ICLR.cc/2026/Conference/Submission21327/Reviewer_FrdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21327/Reviewer_FrdC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983433453, "cdate": 1761983433453, "tmdate": 1762941696828, "mdate": 1762941696828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A new framework, Adaptive Stacking, is proposed, allowing selective retention of observations in a fixed-size working memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A new framework for working with non-Markovian environments, which is a very important area of research.\n2. The proposed framework is theoretically well-founded.\n3. The paper is well-written and easy to read."}, "weaknesses": {"value": "1. The survey of related work overlooks important studies on recurrent models (FFM [1], SHM [2]) and transformer-based models addressing the limited context problem (HCAM [3], AGaLiTe [4]), including approaches that incorporate recurrence into transformers (RATE [5]). Additionally, work [6] investigates the impact of context length on successful memory-dependent task performance.\n2. The experiments are conducted on only a small number of vector-based environments, and for TMaze, a very short corridor length is used. Testing on a larger variety of environments (both vector-based and visual) is necessary, such as MiniGrid-Memory, MemoryGym [7], or VizDoom-two-colors [5].\n3. Results are missing on how the proposed framework performs in cases where using a frame stack is unnecessary, such as in MuJoCo [8] or similar environments. This is important to demonstrate that the proposed mechanism does not degrade performance in Markovian environments.\n4. There is no comparison with strong baselines, such as FFM, SHM, RATE, and AGaLiTe.\n\nI am willing to reconsider my evaluation if the mentioned shortcomings are addressed.\n\n**References:**\n1. Morad, Steven, et al. \"Reinforcement learning with fast and forgetful memory.\" Advances in Neural Information Processing Systems 36 (2023): 72008-72029.\n2. Le, Hung, et al. \"Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning.\" arXiv preprint arXiv:2410.10132 (2024).\n3. Lampinen, Andrew, et al. \"Towards mental time travel: a hierarchical memory for reinforcement learning agents.\" Advances in Neural Information Processing Systems 34 (2021): 28182-28195.\n4. Pramanik, Subhojeet, et al. \"AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning.\" arXiv preprint arXiv:2310.15719 (2023).\n5. Cherepanov, Egor, et al. \"Recurrent action transformer with memory.\" arXiv preprint arXiv:2306.09459 (2023).\n6. Cherepanov, Egor, et al. \"Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation.\" arXiv preprint arXiv:2412.06531 (2024).\n7. Pleines, Marco, et al. \"Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of Agents.\" Journal of Machine Learning Research 26.6 (2025): 1-40.\n8. Fu, Justin, et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020)."}, "questions": {"value": "1. How does the proposed approach perform compared to strong baselines?\n2. Can it be adapted, for example, to FFM or SHM, and would this improve the original models’ performance?\n3. Does the approach generalize to visual observations?\n4. How does the approach perform in Markovian tasks?\n5. How does the approach behave, or how does performance change, if the corridor length in TMaze is increased substantially, e.g., to 100 or 1000?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "31DmfBRo8g", "forum": "s9Ej5SQs5z", "replyto": "s9Ej5SQs5z", "signatures": ["ICLR.cc/2026/Conference/Submission21327/Reviewer_kFSE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21327/Reviewer_kFSE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21327/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990270611, "cdate": 1761990270611, "tmdate": 1762941696486, "mdate": 1762941696486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}