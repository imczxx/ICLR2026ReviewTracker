{"id": "63VXjOFiit", "number": 24557, "cdate": 1758357965578, "mdate": 1763753661174, "content": {"title": "The Price of Robustness:  Stable Classifiers Need Overparameterization", "abstract": "The relationship between overparameterization, stability, and generalization \nremains incompletely understood in the setting of discontinuous classifiers. We \naddress this gap by establishing a generalization bound for finite function classes that improves \ninversely with _class stability_, defined as the expected distance \nto the decision boundary in the input domain (margin). Interpreting class stability as a quantifiable notion \nof robustness, we derive as a corollary a _law of robustness_ for \nclassification that extends the results of Bubeck and Selke beyond \nsmoothness assumptions to discontinuous functions. In particular, any \ninterpolating model with $p \\approx n$ parameters on $n$ data points must be \n_unstable_, implying that substantial overparameterization is necessary \nto achieve high stability. We obtain analogous results for (parameterized) infinite function classes by analyzing a stronger robustness measure derived from the margin in the co-domain, which we refer to as the _normalized co-stability_. Experiments support our theory: stability increases with model size and correlates with test performance, while traditional norm-based measures remain largely uninformative.", "tldr": "We show that interpolating classifiers can only be stable, and thus generalize well, if they are sufficiently overparameterized.", "keywords": ["concentration inequalities", "isoperimetry", "robustness", "stability", "classification problems", "generalization", "overparameterization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/079f094cae36df85d19ef9f0392bfaad5d7c4ec8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper defines two robustness/stability surrogates for discontinuous classifiers and proves data‑dependent generalization bounds that tighten as stability increases: class stability $S(f)$ and normalized co‑stability $\\bar{S}^*$. Under a c-isoperimetry assumption on the data distribution (Def. 3, p. 4), the authors prove a Rademacher complexity bound for finite classifier classes, and an extension to infinite, parameterized classes via normalized co‑stability and parameter‑Lipschitz score maps. Experiments are conducted On MNIST and CIFAR‑10 with fully‑connected MLPs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Framing of robustness for discontinuous classifiers. Replacing Lipschitzness of $f$ with expected input‑margin and codomain margin is sensible and bridges a known gap in extending the Bubeck–Sellke robustness law to classification. The formalization via signed‑distance representation is clean.\n- The finite‑class bound (Theorem 4) uses a careful Lipschitz surrogate + isoperimetry argument, then sharpens by invoking the signed‑distance representation. The infinite‑class extension cleanly separates the roles of average confidence and smoothness."}, "weaknesses": {"value": "- Assumptions vs. practice gap. The isoperimetry requirement on $\\mu_X$ is strong and not stress‑tested. The paper states a manifold‑dimension interpretation, but no empirical probes of isoperimetry or concentration are provided, even in toy data. The external validity of the law depends on this.\n- Overparameterization claim feels over‑indexed to $nd$. The corollaries argue a necessity of $p≈nd$, but the experiments do not stress this scaling (e.g., sweeping $n$ and $d$ while reading off the stability needed), nor do they test architectures beyond MLPs. As written, the claim risks overreach."}, "questions": {"value": "- Assumption stress‑test. Can you empirically probe the isoperimetry assumption (e.g., concentration of Lipschitz functions) on MNIST/CIFAR embeddings, or supply a synthetic non‑isoperimetric counterexample where your bound degrades?\n- Is the theoretical claim / empirical phenomena general enough, to hold on other classifiers, such as SVM, random forest classifiers, etc (other than MLP)? Will one obtain similar experiment results with different network architectures?\n- typos: “Selke” should be “Sellke”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "zGrz6FmXDr", "forum": "63VXjOFiit", "replyto": "63VXjOFiit", "signatures": ["ICLR.cc/2026/Conference/Submission24557/Reviewer_qsC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24557/Reviewer_qsC1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793299343, "cdate": 1760793299343, "tmdate": 1762943120699, "mdate": 1762943120699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the \"universal law of robustness,\" which analyzes the relationship between overparameterization, stability, and generalization, to the domain of discontinuous classifiers. The authors prove that for any discontinuous classifier, overparametrization is necessary if one wants to robustly interpolate the data. The prior work by Bubeck and Selke (2021) established this law for high-dimensional Lipschitz functions, but their reliance on the Lipschitz constant is problematic for discontinuous classifiers. To address this, the authors leverage alternative stability measures: the input-space margin concept of \"class stability\" and a newly introduced output-space metric called \"normalized co-stability.\" By employing these two measures, they successfully extend the theoretical framework to the discontinuous case, providing a compelling explanation for why modern, heavily overparameterized models can achieve robust generalization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "A major strength of this paper lies in its successful extension of the theoretical framework beyond the original Lipschitz assumption to the more challenging domain of discontinuous classifiers. While the work is theoretical in nature, its findings have practical implications explaining why heavily overparameterized models, which are common in modern machine learning, can achieve robust generalization. Furthermore, the paper is well-written and organized."}, "weaknesses": {"value": "Its novelty feels somewhat incremental. At a high level, the core idea mirrors that of Bubeck and Selke (2021): the original work used the Lipschitz constant to ensure that for different inputs $x_i$ and $x_j$, the distance $\\|| x_i - x_j\\||$ is non-trivial (i.e., $\\Omega(1)$); this paper adopts a similar underlying principle.\n\nFor finite function classes, the authors use the \"class stability\" proposed by Liu and Hansen (2024) to derive an upper bound on the Rademacher complexity. However, as noted in Remark 5, this bound introduces an undesirable additional factor in certain regimes, which requires further assumptions to mitigate.\n\nFor the infinite function class case, the analysis is restricted to a specific function class of the form $\\mathrm{sgn}\\circ \\mathcal{G}$, where $\\mathcal{G}$ is Lipschitz. This raises questions about how broadly the results can be generalized to all discontinuous classifiers. The newly proposed \"co-stability\" measure, used to derive Theorem 13 and Corollary 15, is defined in terms of this Lipschitz constant of $\\mathcal{G}$. Consequently, the results feel analogous to the original idea by Bubeck and Selke, with the main difference being the consideration of a composed function class $\\mathrm{sgn}\\circ\\mathcal{G}$ rather than a fundamentally new approach.​​\n\nTypo:\npage 4: The first bullet in thm4: empirical Rademacher complexity -> Rademacher complexity"}, "questions": {"value": "Q1.  About $\\mathrm{sgn}\\circ \\mathcal{G}$. While it clearly covers classical models like SVMs, its application to modern deep learning architectures merits further clarification. Could you provide examples of overparameterized deep learning models that fit the definition of a discontinuous classifier in this paper, and perhaps more importantly, any that might not fit this structure?\n\nQ2. The original Bubeck & Selke (2021) paper required a lower bound on the data dimension $d$ relative to the level of robustness $\\varepsilon$ (Assumption 4 in their paper). This assumption appears to be absent in your work. Could you highlight why this dimensional lower bound is no longer necessary in your proof for discontinuous classifiers?\n\nQ3. The theoretical bounds you derive depend on the true data distribution, which is unknown in practice, raising the possibility that the bounds could be vacuous. While the experiments effectively show a correlation between the proposed stability measures and test performance, could you compute the actual upper bounds derived in the paper using empirical estimates? How do these computed bound values compare to the observed generalization error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D4BsjgANnz", "forum": "63VXjOFiit", "replyto": "63VXjOFiit", "signatures": ["ICLR.cc/2026/Conference/Submission24557/Reviewer_WFc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24557/Reviewer_WFc2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998308880, "cdate": 1761998308880, "tmdate": 1762943120373, "mdate": 1762943120373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends results on isoperimetry and robustness to discontinuous classifiers, instead of the regression setup from Bubeck & Sellke.\nTo this end the authors define a notion of class stability that measures the expected distance to the decision boundary within each class, and serves as a type of stability criterion in the classification case.\nFor a finite hypothesis class, a bound is derived for the Rademacher complexity that depends on the minimum class stability, the isoperimetry of the data distribution and of course the sizes of the class and dataset. The main conclusion from the bound is that the size of the hypothesis class needs to grow much larger than the size of the dataset in order to produce a good upper bound on the generalization error.\n\nThe results are extended to non-finite hypothesis classes using some additional conditions, and simulations are performed on MNIST and CIFAR10 to demonstrate the theory."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a nice addition to the literature on stability and interpolation. While it gives results that are in similar flavor to existing results, there are still original developments that can be of interest to the community.\nTo achieve their generalization, the authors discuss several notions of stability and how they are combined with assumptions on the hypothesis class, in order to obtain meaningful results. I think these derivations are easy-to-understand and clearly written, and so is the rest of the paper."}, "weaknesses": {"value": "The most apparent weakness of the paper is that it is somewhat incremental, and proves results that are in the spirit of Bubeck and Sellke. Since the technical tools developed in the paper are novel, I think that this is not a major drawback.\n\nSmall comment: the authors mention generalization also in the context of out-of-distribution generalization and refer to a few results on this problem (e.g. Zou et al. 24). Since the paper refers to these topics and mentions generalization as a whole and not just in-distribution (or adversarially robust) generalization, I think it should also make a distinction between this setting and the setting of robustness to other distribution shifts (like spurious correlations, covariate shift etc.). The settings are rather different, but there were works discussing overparameterization in the context of distribution shifts, giving both negative and positive results [1,2,3]. Since the two lines of work share some terminology, I think it'd be useful to shortly clarify the distinction.\n\n[1] Wald, Y., Yona, G., Shalit, U., & Carmon, Y. Malign Overfitting: Interpolation Can Provably Preclude Invariance. In The Eleventh International Conference on Learning Representations.\n\n[2] Hao, Y., Lin, Y., Zou, D., & Zhang, T. (2024). On the benefits of over-parameterization for out-of-distribution generalization. arXiv preprint arXiv:2403.17592.\n\n[3] Sagawa, S., Raghunathan, A., Koh, P. W., & Liang, P. (2020, November). An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning (pp. 8346-8356). PMLR."}, "questions": {"value": "No immediate questions come to mind, as the paper is written quite clearly. I will read the other reviews and see if questions come up from them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8jfmS1gu4t", "forum": "63VXjOFiit", "replyto": "63VXjOFiit", "signatures": ["ICLR.cc/2026/Conference/Submission24557/Reviewer_mVwX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24557/Reviewer_mVwX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762200496765, "cdate": 1762200496765, "tmdate": 1762943120126, "mdate": 1762943120126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}