{"id": "6YktIxJTJr", "number": 23609, "cdate": 1758346278602, "mdate": 1759896804735, "content": {"title": "SYNAPSE: Simulation Benchmark of Neuro-Adaptive Patient-Specific Evaluation for Episodic Decision-Making", "abstract": "Recent advances in time-series analysis, treatment outcome prediction, and reinforcement learning (RL) have demonstrated great potential to automate decision-making in healthcare. However, the high stakes nature complicates the deployment of such frameworks in practice, clinically, or in the long term. A major challenge is the absence of realistic benchmark environments that capture the sequential, patient-specific nature of various therapies, which could enable extensive offline testing, evaluation, and model selection prior to clinical adoption. To address this, we introduce the SImulation Benchmark of Neuro-Adaptive Patient-Specific Evaluation (SYNAPSE), in the context of adaptive deep brain stimulation (DBS), a treatment for managing the motor symptoms of Parkinson’s disease (PD). Specifically, SYNAPSE is constructed using real-world data collected from both clinical and at-home studies involving participants undergoing DBS therapy. It enables offline training and evaluation of different treatment strategies, reflecting both short- and long-term effects, as well as treatment outcome prediction capturing participants’ responses to a range of temporal dynamics. Additionally, it allows for the assessment of safety-critical constraints inherent to neurostimulation decision-making. By rigorously validating its realism against clinical data and supporting both short- and long-term decision-making, SYNAPSE offers clear guidance for future DBS policy development, as well as helps identify and address key challenges in advancing truly personalized neurostimulation therapies.", "tldr": "", "keywords": ["Simulator and benchmark", "Deep brain stimulation for Parkinson’s disease treatment", "healthcare reinforcement learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3af15704768d33ae50280af2f2bad10e1c89c44.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops a simulation for aDBS for Parkinsons. The approach uses clinical data to train models of human patients. It enables future aDBS policy to be tested in silico before deployment on real patients."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "aDBS is a very promising technology to improve the lives of patients. However, there do not currently exist many ways to effectively test new methods for aDBS control before deployment in the patient. This paper addresses this very important problem by using patient derived data  to create an aDBS simulation."}, "weaknesses": {"value": "Immediate rewards and ihr could be better explained when initially introduced.\n\nMore details about the actual modeling need to be included since this is the point of the paper. This shouldnt just be in the appendix. A diagram of the simulation architecture would also be helpful\n\nA discussion of how one would choose which environment to test a new policy for a given patient would be helpful. is there a way to quantify how close a new patient is to each simulation environment to know which would be best to use?\n\nMore detail about the heterogeneity of responses would be useful and differences in each environment would be useful\n\nRelated works section talking about other data-driven human simulations for medical applications would be useful to better contextualize this work\n\nThis is an unusual topic for ICLR. I would recommend briefly discussing \"Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation\"  in related works as this is the most similar work that has also been published in this community"}, "questions": {"value": "New control strategies be out of distribution of the policies that the simulation was trained on. Is there a way to quantify uncertainty in the simulation?\nWhat does turning off dbs mean? Is this just no stimulation?\nCan medication be modeled in the simulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2uBL3kEh2G", "forum": "6YktIxJTJr", "replyto": "6YktIxJTJr", "signatures": ["ICLR.cc/2026/Conference/Submission23609/Reviewer_5ezG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23609/Reviewer_5ezG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761351824033, "cdate": 1761351824033, "tmdate": 1762942733829, "mdate": 1762942733829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper develops a SImulation Benchmark of Neuro-Adaptive PatientSpecific Evaluation (SYNAPSE) for adaptive deep brain stimulation (DBS) in Parkinson’s disease. The paper includes offline training and evaluation of different treatment strategies, reflecting both short- and long-term effects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe developed simulator is critical to understand the impact of reinforcement learning on DBS and bridge the gap between evaluation and real-world clinical practice. \n2.\tThe paper includes a diverse set of modern RL algorithms across multiple reward definitions and time horizons."}, "weaknesses": {"value": "1.\tThe contribution of this paper needs clarification. The experiment and the developed dataset have significant overlap with the existing work [1]. The paper should explicitly articulate what the new components are provided in the newly proposed benchmark.\n2.\tThe dataset includes only five patients, which limits the generality and makes the term “benchmark” somewhat premature. While the authors acknowledge this limitation, a stronger justification is needed to argue why this sample is sufficient to model variability across the PD population.\n3.\tWhile standard RL metrics (e.g., episodic return) are used, there is a lack of discussion on how these results map to clinically meaningful outcomes.\n4.\tLearning Agent is not defined in Figure 2. Does it refer to an RL agent?\n\n[1] Gao, Qitong, Stephen L. Schmidt, Afsana Chowdhury, Guangyu Feng, Jennifer J. Peters, Katherine Genty, Warren M. Grill, Dennis A. Turner, and Miroslav Pajic. \"Offline learning of closed-loop deep brain stimulation controllers for parkinson disease treatment.\" In Proceedings of the ACM/IEEE 14th International Conference on Cyber-Physical Systems (with CPS-IoT Week 2023), pp. 44-55. 2023."}, "questions": {"value": "1.\tWhat are the key differences between SYNAPSE and the previous dataset/work [1]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xKVIeA2aq0", "forum": "6YktIxJTJr", "replyto": "6YktIxJTJr", "signatures": ["ICLR.cc/2026/Conference/Submission23609/Reviewer_zV95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23609/Reviewer_zV95"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879279736, "cdate": 1761879279736, "tmdate": 1762942733624, "mdate": 1762942733624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SYNAPSE, a domain-specific simulator and benchmark for adaptive deep brain stimulation (aDBS) in Parkinson’s disease (PD). The key claim is that SYNAPSE provides patient-specific environments learned from real clinical and at-home data, supporting offline/online RL, off-policy evaluation (OPE), human-feedback modeling, and policy transfer across a small “virtual cohort.” The authors report multiple fidelity metrics (trajectory/statistics alignment) and illustrative RL/OPE experiments across a small but longitudinal dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is an important problem. Safe RL for closed-loop neurostimulation is impactful and arguably underexplored compared to EHR-style decision making.\n\nI like that the authors model each participant as an environment... this seems like a potentially useful abstraction for transfer and personalization.\n\nThe authors include off policy evaluation (OPE), long-horizon outcomes, and a human-feedback component.\n\nThe benchmark is built from longitudinal aDBS deployments rather than purely synthetic toy dynamics."}, "weaknesses": {"value": "The presented results are primarily within-patient... the paper probably needs leave-one-patient-out (LOPO) analyses to show simulator fidelity and RL/OPE behavior on a held-out patient.\n\nCurrent metrics (e.g. latent space or marginal distribution closeness) are only loosely tied to policy improvement. I would think the paper should evaluate policy ranking agreement, value calibration, and/or regret vs. real logs or prospectively held-out traces.\n\naDBS has clear safety constraints (energy budgets, ramping/overshoot limits)... I did not see reports of constraint-violation rates or adverse-proxy statistics under learned policies.\n\nOffline RL and OPE comparisons feel thin. In particular, a comparison to prior benchmarks like EpiCare (Hargrave, Spaeth, Grosenick NeurIPS 2024) a recent, broad healthcare RL benchmark with a stronger OPE/offline RL evaluation suite (that seems adaptable here / is synthetic) would situate SYNAPSE more clearly (even if only to show why device-level control needs different methods/metrics). And such prior work on POMDPs should be cited here?\n\nMinor but probably should be corrected: \n- It seems that the author's backronyms spells \"SINAPSE\", but the authors use \"SYNAPSE\" or sometimes \"SYNAPSES\". \n- There's some real potential redundancy between Fig 1 and Fig 2"}, "questions": {"value": "What happens under LOPO? Authors should try training simulators on N−1 patients and report transition/reward fidelity, OPE calibration, and policy performance/regret on the held-out patient.\n\nHow do common OPE methods behave under coverage shift?\n\nWhich hard constraints are enforced in simulation, and what are the violation rates during training/rollout?\n\nProvide calibration plots and sensitivity to priors/likelihoods; does HF aid or mislead policy selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OTxclPdLu6", "forum": "6YktIxJTJr", "replyto": "6YktIxJTJr", "signatures": ["ICLR.cc/2026/Conference/Submission23609/Reviewer_e5BC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23609/Reviewer_e5BC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762201690761, "cdate": 1762201690761, "tmdate": 1762942733332, "mdate": 1762942733332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an environment (SYNAPSE) for accelerating RL research and benchmarking adaptive deep brain stimulation policies for Parkinson's disease."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper targets a key Reinforcement Learning problem in medical settings, and solving it would have substantial impact."}, "weaknesses": {"value": "1. The main weakness is training the transition dynamics on very limited data from a single individual for each environment while claiming them realistic. The simulator can only be accurate for states it has actually observed, leaving many counterfactuals where it will likely produce unreliable outputs. Only a model with foundation-model-level scale and data coverage could approach the fidelity needed for realistic simulation.\n\n2. Insufficient evaluation - Using t-SNE, predicted-action MAE, and reward EMD/AE to assess the transition dynamics is not a meaningful or rational evaluation strategy.\n\n3. Referring to the setup as a Human-involved MDP (HMDP) is redundant, as both $R$ and $R^H$ can simply be components of a single vector-valued reward function within a standard MDP.\n\n4. It is unclear which policies the authors actually trained. They state that all RL policies were trained with DDPG, yet later they compare PPO, A2C, and CQL agents as well.\n\n5. The paper appears confused about the differences between online, offline, and off-policy methods. For example, \na) in the Clinical Sessions section, the authors write: 'The RL controllers are trained using deep deterministic policy gradient (DDPG) with data collected from three other types of controllers, followed by finetuning with the latest data.' But DDPG is an online algorithm, meaning it learns through interaction with the environment, not from static datasets collected by other controllers. This makes the described training setup inconsistent with how DDPG is intended to operate.\nb) The authors claim that the simulator enables off-policy evaluation, but a simulator is not required for that. A simulator enables online training, not off-policy evaluation.\n\n6. In the 'Challenges for ML/RL in DBS' section, the authors claim that existing simulators lack patient-specific nuances seen in vivo. But SYNAPSE’s transition dynamics are trained on data from only five patients, which leads to the same limitation."}, "questions": {"value": "1. The authors note that existing DBS controllers have been validated through clinical testing. While we understand that those datasets are not public, such data is often available under Data Usage Agreements. Could the authors explore accessing any of these datasets to compare SYNAPSE’s predictions? This would significantly strengthen confidence in the simulator’s validity.\n2. In the 'At-Home Sessions' section, the authors state: 'When the participant chose to start a session, one of the three controllers was uniformly randomly chosen to start until the participant chose to end the session.' How safe is random policy selection in this context, and why was this strategy chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vDtoKIeGzf", "forum": "6YktIxJTJr", "replyto": "6YktIxJTJr", "signatures": ["ICLR.cc/2026/Conference/Submission23609/Reviewer_xdYr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23609/Reviewer_xdYr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23609/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763015355417, "cdate": 1763015355417, "tmdate": 1763015355417, "mdate": 1763015355417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}