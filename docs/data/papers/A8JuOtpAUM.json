{"id": "A8JuOtpAUM", "number": 4858, "cdate": 1757778642228, "mdate": 1763673558682, "content": {"title": "Fixed Aggregation Features Can Rival GNNs", "abstract": "Graph neural networks (GNNs) are widely believed to excel at node representation learning through trainable neighborhood aggregations. We challenge this view by introducing Fixed Aggregation Features (FAFs), a training-free approach that transforms graph learning tasks into tabular problems. This simple shift enables the use of well-established tabular methods, offering strong interpretability and the flexibility to deploy diverse classifiers. Across 14 benchmarks, well-tuned multilayer perceptrons trained on FAFs rival or outperform state-of-the-art GNNs and graph transformers on 12 tasks -- often using only mean aggregation. The only exceptions are the Roman Empire and Minesweeper datasets, which typically require unusually deep GNNs. To explain the theoretical possibility of non-trainable aggregations, we connect our findings to Kolmogorov–Arnold representations and discuss when mean aggregation can be sufficient. In conclusion, our results call for (i) richer benchmarks benefiting from learning diverse neighborhood aggregations, (ii) strong tabular baselines as standard, and (iii) employing and advancing tabular models for graph data to gain new insights into related tasks.", "tldr": "Fixed, non-trainable neighborhood aggregations yield interpretable tabular features on which MLPs match or outperform GNNs in node classification tasks, justified through Kolmogorov–Arnold representations.", "keywords": ["deep learning", "graph neural networks", "node classification", "kolmogorov-arnold representation", "tabular learning", "fixed aggregation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5090b2c31a35d587e1fbf98d4a796ac30078c4ad.pdf", "supplementary_material": "/attachment/001bb9844b3cd8f28ecda640a3a4c83de17d4bd6.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents arguments in favor of using feature-engineering based on multiple, fixed node feature aggregations, rather than having a GNN do the heavy lifting every time. It Is argued that working with tabular ML methods on pre-processed node features offers more interpretability and better training stability than using GNN and that this is supported by empirical experiments on 14 datasets. In addition, the authors introduce a theoretical contribution based on the KAT to motivate the particular feature extraction strategy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Challenging the status quo is always a good idea. This paper does an extremely well job at summarizing prior work that has explored ways of making GNNs simpler, untrained, or heterophily-specific, and then it proposes a solution that seems inspired by multi-aggregation techniques like PNA and concatenation of features like jumping-knowledge networks (JK-Net, Xu et al.). I would like to see more papers devoting so much attention to prior work before proposing a contribution. In this sense, I believe that motivating the FAF scheme by revisiting the KAT under the lens of neighborhood aggregation is an extremely interesting and novel direction. The message-passing architecture is not easy to control and simpler but still effective solutions may even help us reason about different architectural schemes that are more effective than what we use today."}, "weaknesses": {"value": "There are several aspects in the paper that in my opinion require improvements before being accepted at an A* conference. \n\nI think the introduction might benefit from stronger justifications when motivating the proposed technique. For instance, the second paragraph does not seem to necessarily raise the basic question in line 42, rather it could motivate any architectural study of GNNs as a whole. It would be better to find a stronger argumentation that is specific to the aggregation.\n\nThe introduction is also quite difficult to read due to some ambiguities. It is often unclear whether the authors mean “graph convolution” or simply the “aggregation operator” when referring to “learning the aggregation”. A statement like “we find that untrained aggregators yield useful features” is true for both GNNs and FAF if one refers to the permutation-invariant operator over neighbors, so I would like to suggest that the authors take more care in disambiguating these cases in the text. \n\nFAF reminds me of an untrained PNA followed by an MLP classifier. I would like to disagree, however, with the statement that FAF can bring more interpretability than current methods that provide an interpretation in the form of a subgraph. The example in Section 3.1 is very ad-hoc, and it does not transfer to a more convoluted scenario where features are continuous and their mixing with different aggregators could make no sense. The experiments in the paper in this direction are not convincing to me, unfortunately.\n\nSection 4 presents interesting theoretical results, which I could not check in detail due to the reviewing workload amidst my other commitments. Apologies for that. I think it is a fresh perspective of neighborhood that I had not heard of. At the same time, I believe there is a big gap between the theoretical result and the empirical experiments, because of the use of non-injective functions invalidates all the theoretical arguments. Please let me know if I misunderstood something here. \n\nIn this sense, the answer to the question of Section 4’s title seems to be a sound “yes”: we need to learn aggregators because we have no way of learning injective functions by repeated application of known fixed aggregators. The theoretical arguments, in my opinion, would make for a very nice paper if analyzed and expanded further, and the same could be said for the FAF method. I understand the reasoning behind the current paper’s structure, but in my opinion the paper does not do good justice to neither contribution, since FAF takes too big of a step compared to the theoretical arguments, which are missing some more qualitative analysis.\n\nI have seen a very interesting reference to GESN and following works. I am aware of that research line: GESN variants have achieved very good results on at least subset of these datasets. Being correctly mentioned as related work, in the sense that the untrained reservoir of GESN acts as an “alternative FAF”, though not supported by theory, I am wondering why the authors did not compare empirically with this family of methods.\n\nThe most concerning issue on my end remains the empirical evaluation. When one introduces a new class of models, parametrized by a set of hyper-parameters, the usual way of evaluating the empirical risk is by running a model selection, selecting the best hyper-parameters on a validation set, and then evaluating the best configuration (assuming only one in a hold-out data split) on the test set. What I see in Table 1 is instead an instance of hyper-parameter tuning of the hyperparameter R (the set of aggregators to use) on the **test set**. There should be a single line for the class of FAF models in each of these tables, representing the fact that the set R was tuned separately on the validation set for each dataset together with the other hyper-parameters. This is a grave mistake in my understanding. Similarly, all ablation analyses should refer to the validation set performance without looking at the test set, but it does not seem to be the case.\n\nOverall, I really like the perspective of the authors and I encourage them to revise the paper, possibly following some of the suggestions in this review. At the same time, I do not feel I can recommend acceptance of the paper in its current shape."}, "questions": {"value": "Questions:\n- I would like to ask the authors where in the paper they supported their statement of line 86 that learnability and numeric stability govern practical success in addition to expressiveness, and why the authors think that this was not clear in the past.\n- Can you easily extent your theoretical arguments to F>1, or are they limited to F=1?\n- The first two suggestions for future work (lines 358-9) look a bit underdeveloped and generic with respect to the authors’ contribution. Could you elaborate a bit better why they are relevant in this context?\n- Potential typo: $\\mathcal{X}$ may be undefined in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gv7vVNQPv3", "forum": "A8JuOtpAUM", "replyto": "A8JuOtpAUM", "signatures": ["ICLR.cc/2026/Conference/Submission4858/Reviewer_78XL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4858/Reviewer_78XL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760622756330, "cdate": 1760622756330, "tmdate": 1762917619184, "mdate": 1762917619184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that a combination of non-learnable feature aggregation a an MLP can provide performance competitive with GNNs on many classic datasets for node classification. First, the paper proposes concatenating features obtained from neighborhood aggregations with different radius with the original node features to essentially convert a node classification task to a tabular classification task. Then, the paper investigates some theoretical aspects of such aggregations, showing that it is possible to design aggregations that preserve all neighborhood information (but, as the paper admits, such aggregations are not very practical). Then the paper conducts experiments with simpler aggregations and shows that MLPs on top of such aggregations can often rival GNNs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The main idea is simple but useful, its empirical performance can sometimes be quite impressive.\n- Experiments are conducted on a vast range of datasets, strong baseline models with adequate hyperparameter search spaces are used.\n- The paper raises timely questions regarding the adequacy of standard node classification benchmarks for the evaluation of complex models."}, "weaknesses": {"value": "- The interpretability example with the minesweeper dataset (section 3.1) is wrong due to a misunderstanding of what the node features are. As described in [1], where the dataset was proposed, the node features use one-hot encoding for the number of neighboring mines, not binary encoding (see page 7 of [1], the Minesweeper paragraph). Due to this mistake, the explanation that the authors provide for how the model uses the features is entirely wrong. I do not consider this a serious issue, as it is just a minor example that does not affect the main points of the paper, but it needs to be fixed.\n\n- The theoretical contributions seem not particularly interesting. First, Section 4.1 relies on the assumption of feature orthogonality, which is not very realistic. Even bag-of-words features are not orthogonal (and bag-of-words is an extremely outdated technique in 2025, but that is the problem of standard graph ML benchmarks, not of the current paper), and other feature types are typically even further from being orthogonal. Then, section 4.2 proposes a theoretical construction that, as the authors themselves admit (which is commendable), is not very practical. This leads to the theoretical sections being rather disconnected from the experimental sections. This is partially alleviated by some interesting discussions in Section 4.3, but they are not fleshed out enough in my opinion. I suggest shortening Section 4.1 and giving more space to the discussions in Section 4.3 (perhaps by providing more evidence for the hypotheses) to improve the paper.\n\n- It is a strong point of the paper that it uses a lot of datasets for experiments and evaluates improved and well-tuned GNNs from [2] rather than weaker models that are often used as baselines in other works. However, this raises the question: if the authors use the codebase of [2] and also use almost the same hyperparameter search space, why are the reported results sometimes significantly different than those in [2]? For example, on the cora dataset, the reported result for GCN is 81.28, while [2] reports 85.10 (note that a similar results would make GCN rather than FAF4 the strongest model on cora in the current paper). There are similar discrepancies for some of the other datasets. What is the reason for them?\n\nI am willing to raise my score if my concerns are addressed.\n\n\n\n[1] A critical look at the evaluation of GNNs under heterophily: Are we really making progress? (ICLR 2023)\n\n[2] Classic GNNs are strong baselines: Reassessing GNNs for node classification (NeurIPS 2024)"}, "questions": {"value": "See weaknesses.\n\nSome other suggestions for paper improvement:\n\n- The paper does not discuss efficiency at all, but it could potentially be another strong point of the FAF approach: precomputing aggregations once and then training MLPs on top of them is much faster than training GNNs (graph aggregation is typically the slowest operation in GNNs). I suggest discussing it and possibly even providing training times.\n\n- The paper mentions a couple times that its results imply that the current benchmarks used in graph machine learning are inadequate for evaluating complex models. This could be discussed more and positioned within the recent line of works that also discuss and/or address this issue. Specifically, [3-5] discuss the problems with current benchmarks ([3] is briefly mentioned in the current work), and [5] proposes better datasets for node property prediction. Note that [5] additionally uses neighborhood feature aggregation (NFA) mechanism which is similar to a 1-hop version of FAF, and also shows that it can be a strong baseline. Even more recently, a concurrent work [6] also proposes better benchmarks for graph machine learning.\n\n[3] Graph learning will lose relevance due to poor benchmarks (ICML 2025)\n\n[4] No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets (ICML 2025)\n\n[5] GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data (NeurIPS 2025)\n\n[6] GraphBench: Next-generation graph learning benchmarking"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wdVskdjvQq", "forum": "A8JuOtpAUM", "replyto": "A8JuOtpAUM", "signatures": ["ICLR.cc/2026/Conference/Submission4858/Reviewer_fmA9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4858/Reviewer_fmA9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760653557468, "cdate": 1760653557468, "tmdate": 1762917618813, "mdate": 1762917618813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents Fixed Aggregation Features, a simple technique that combines neighborhood aggregations of different types (e.g., mean, max, std), applies them to neighbors at different hops, concatenates the obtained information with the original node features and trains an MLP on top of these representations. The experiments show that such a technique can provide results comparable to or even better than standard message passing networks when measured on classic graph ML datasets. Thus, the work introduces a previously overlooked class of strong graph ML baselines and questions the relevance of classic citation and co-authorship networks with bag-of-word node features for evaluating advanced graph ML models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. An introduction of non-learnable multi-hop feature aggregation as a preprocessing step is very simple, intuitive and practical approach to augment the original node features with graph-based information.\n\n2. A discussion of the problems in optimization procedure of GNNs and theoretical properties of FAF, which also explains why a standard MLP on top of simple graph-based aggregations can perform on par with standard GNNs.\n\n3. An extensive empirical study showing that FAF enables to achieve nearly the same results as GNNs, together with ablation study showing that even a single aggregation type in FAF can be sufficient for some datasets.\n\n4. A number of additional experiments showing that theoretically lossless Kolmogorov-Arnold aggregations are not so effective in practice, while more simple aggregations like mean or max should be preferred for constructing FAF."}, "weaknesses": {"value": "As the main weakness of current version, I see the choice of graph datasets for experiments. There is a recently introduced GraphLand benchmark [1] that provides both classification and regression tasks from industrial applications, includes both homophilous and heterophilous graph datasets, and contains rich heterogeneous tabular node features. Moreover, this work introduces Neighborhood Feature Aggregation (NFA) that seems to be a specific instance of FAF using mean, max and sum aggregations over 1-hop neighborhood. It might be very relevant for this particular study and thus should be discussed as related work.\n\nIt is very interesting to see whether the observations about the importance of the closest neighborhood hold and how the performance of simple MLP on top of FAF representations transfers to GraphLand datasets. I admit that this benchmark appeared very recently, but I would highly recommend to include the experiments using at least the RL (random low) data split, as it could help the authors not only investigate their hypothesis regarding the need for more relevant graph datasets, but also significantly strengthen their empirical study in general. If the authors manage to provide such additional results, I am ready to increase my score.\n\n[1] GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data, NeurIPS 2025"}, "questions": {"value": "1. A couple of comments regarding the Theorem 1 and its proof in Appendix A.2.\n\n- I am not sure that the term \"unique function\" in the formulation of this theorem is clear for me. As I understand, \"uniqueness\" means *preserving the information about elements of the original multiset and making it possible to know how many elements in a multiset have a particular feature value*. If this is true, I would ask the authors to expand the formulation and make it more explicit.\n\n- I also feel that some explaining comments about decomposing any multiset function $f$ in the mentioned form are missing. As I understand, any multiset function $f$ depends on the counters of unique elements observed in it. Since we preserve the whole information about multiset under the transformation $\\Phi$, we can restore these counters by using $\\Phi^{-1}$ and then apply the desired $f$ to them. If the reasoning is correct, I would ask the authors to add such comments in the proof of this theorem.\n\n- There seems to be a typo in the proof — it should be $\\mathbb{R}^{n_f}$ instead of $\\mathbb{R}^n_f$.\n\n2. Can the authors explain why they obtain lower metrics for GNN baselines than those presented in [2], if they use the same hyperparameter search space?\n\n[2] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tBHKqq8mfH", "forum": "A8JuOtpAUM", "replyto": "A8JuOtpAUM", "signatures": ["ICLR.cc/2026/Conference/Submission4858/Reviewer_6Pr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4858/Reviewer_6Pr9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761244015139, "cdate": 1761244015139, "tmdate": 1762917618228, "mdate": 1762917618228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the common view that neighborhood aggregation of GNNs must be learned, and introduces Fixed Aggregation Features (FAFs). FAFs are essentially non-trained feature aggregators (e.g. sum, mean) applied at fixed hops and transformed into tables.A Kolmogorov–Arnold analysis justifies this design, which appears to work empirically in many node classification datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- challenges the established view of needing learned aggregations in GNNs with convincing arguments; in this respect, it can be considered innovative.\n- the tabular representation is interpretable with standard tools (e.g. SHAP), which clearly adds value.\n- the experiments are adequate in both in width (14 datasets) and depth (large set of hyperparameters), the design is fair and reproducible\n- some insights (e.g. GNNs may overfit later aggregations) are definitely thought-provoking and might help designing better GNNs (or improve their benchmarks).\n- the paper is clearly written and easy to follow."}, "weaknesses": {"value": "- The way it is presented, the main finding of the paper appears to be confined to (transductive) node classification. A more broader characterization (e.g. an extension to graph classification or to inductive node-classification) would increase the significance of this work.\n\n- The work (and especially its recommendation on using FAF baselines and reassess benchmarks) is connected with previous work on properly benchmarking GNNs for graph classification. In particular, [1] proposes simple baselines and similarly argues that current graph datasets are often inadequate. These connections should be acknowledged in the Related Works section. \n\n[1] Errica et al. A Fair Comparison of Graph Neural Networks for Graph Classification. ICLR 2020"}, "questions": {"value": "No questions, besides the relatively minor weaknesses detailed above. I believe this paper is a very solid contribution to the field already as-is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jxM8KVJj8J", "forum": "A8JuOtpAUM", "replyto": "A8JuOtpAUM", "signatures": ["ICLR.cc/2026/Conference/Submission4858/Reviewer_Vmh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4858/Reviewer_Vmh1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4858/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558124983, "cdate": 1761558124983, "tmdate": 1762917617415, "mdate": 1762917617415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}