{"id": "Lla4eFlDJu", "number": 6795, "cdate": 1757996096407, "mdate": 1759897893125, "content": {"title": "PE-DyRA: Dynamic Rank Adaptation for Parameter-Efficient  Fine-Tuning via Importance-Aware Pruning and Expansion", "abstract": "As large language models grow in scale, full-parameter fine-tuning for downstream tasks incurs substantial computational and storage costs. Low-Rank Adaptation (LoRA) provides a parameter-efficient paradigm for model adaptation, but its fixed-rank allocation cannot adapt to the heterogeneous importance of different layers or the evolving requirements across training stages, resulting in either redundancy or insufficient capacity. In this paper, we introduce Dynamic Rank Adaptation via Importance-Aware Pruning and Expansion (PE-DyRA), a novel framework that dynamically allocates ranks through importance score-based pruning and expansion. PE-DyRA introduces three key innovations: 1) A parameter importance evaluation measure based on gradient information and input activations to enable more stable ranking; 2) A bidirectional rank adjustment mechanism that dynamically prunes and expands ranks based on importance, enabling flexible allocation and improved parameter utilization; 3)The PE-DyRA framework can be used as a paradigm to achieve better results on benchmark methods such as DoRA, PiSSA, and QLoRA. Extensive experiments demonstrate the effectiveness of PE-DyRA, surpassing baseline methods. Furthermore, theoretical analysis demonstrates that PE-DyRA has better parameter efficiency.", "tldr": "A novel framework that dynamically allocates ranks through importance score-based pruning and expansion.", "keywords": ["parameter-efficient fine-tuning", "large language model", "low-rank adaptation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/468b9dde0e1638e12d0463a77730e5cfee2cfc04.pdf", "supplementary_material": "/attachment/6561df24038a7a6bdd51ddeb1e714c5b875787dc.zip"}, "replies": [{"content": {"summary": {"value": "The method is a variant of LoRA that parameterize the adapter as AEB (instead of AB) in the form of SVD (similar to AdaLoRA) and then the ranks of the layers are reduced or increased based on gradients, weights, and input activations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Interesting and logical method\n* Good results on Llama 2,3"}, "weaknesses": {"value": "* Method is much slower in comparison to LoRA (Table 10)\n* Table 2: Did you copy the baseline results or run them yourselves? If the latter, what hyperparams have you used? What seeds? How many runs?\n* Table 2: You wrote that AdaLoRA got 88.90 but in their paper they say 89.31. It’s a big difference.\n* Table 2: There is no stdev. I think the bare minimum is stdev at least for your method, for consecutive seeds: 1,2,3. We need to see the seed sensitivity, as it can be something around half a point. This should be validated using the supplied code. \n* Table 2: In table 3, with Llama2, the method was compared against DoRa, and it \nwas the strongest competitor. For some reason, this competitor is omitted from this table.\nThis raises the suspicion that it was omitted here since it outperformed the proposed method?\n* Table 3: There is no information in the paper about the hyperparams of the baseline methods, and how they were selected. Hyper params are very important for the performance and the comparison should be fair. How can the reader know you invested the same energy/methodology in finding optimal hypers for baseline methods like you did for your method?\n* Table 3: Seed sensitivity or stdev is missing. We know from the original LoRA paper that seeds can have a significant effect (sometimes even 0.7), so it is also important on Llama, at least for your method.\n* Theorem, equation (9): How did you get to this assumption? Why is it logical? This assumption directly affects goal of the theorem.\n* Table 10: Per dataset, all methods run the same number of epochs? if not, total training time should be specified for fair comparison.\n* Line 918: It says: “substantially reduces runtime (e.g., 220.64s vs. 341.22s on SST-2)”, but Table 10 shows exactly the opposite numbers."}, "questions": {"value": "* abstract, line 24, space is missing after 3)\n* Line 46, space is missing before AdaLoRA\n* Table 2: Better to keep the same number of decimal places for readability (see 88.9 and 71.6).\n* Table 2, I suggest to put the model name (Deberta) in the table title, so that tables will be as self explanatory as possible and to avoid confusion. Table 3 mentioned the model type, but table 2 does not. Additionally, when you refer to the table, you should say something about the hyperparameters of the baseline models, as they are very important.\n* Introduction, line 53, “often neglecting input activations\": there are methods, like PRiLoRA that takes into account weights and activations like Wanda.\n* Table 3: keep the same number of decimal places in all places, for readability. \n* Table 5: Here you mix two decimal places with three decimal places.\n* Line 809: You show the hyperparams of your method, but say something about the hyperparams of the other methods for completeness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ym20lz8zdU", "forum": "Lla4eFlDJu", "replyto": "Lla4eFlDJu", "signatures": ["ICLR.cc/2026/Conference/Submission6795/Reviewer_joZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6795/Reviewer_joZZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760814968426, "cdate": 1760814968426, "tmdate": 1762919067970, "mdate": 1762919067970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PE-DyRA, a dynamic rank adjustment method for LoRA-based parameter-efficient fine-tuning. The approach introduces a rank allocator (core matrix) component and integrates gradient- and input-based importance estimation to guide bidirectional rank pruning and expansion."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of integrating input activations and gradient information into the importance estimation is conceptually interesting and may contribute to more stable rank evaluation.\n- The paper provides experiments across multiple benchmarks and some ablation study, suggesting PE-DyRA outperforms existing baselines."}, "weaknesses": {"value": "- The writing and technical presentation require major revision.\n\n  - Several symbols are inconsistent, ambiguous, or reused, making the paper hard to follow.\n  - Frequent inconsistent capitalization, and irregular precision across tables.\n\n  - The two importance metrics share the same notation S(⋅) without clear distinction or explanation. Moreover, the aggregation step described in Algorithm 1 does not appear in the main text.\n  - Key parameters such as $t_i,ΔT,t_f$ are never explained.\n\n  - Some expressions are not sufficiently academic\n    - e.g., “batch size × seq len” in Line 220; “, j = 1,…, d” in Line 222.\n\n- Reproducibility concerns: Many hyperparameters are not specified or ablated. The paper lacks error bars and variance statistics, preventing proper assessment of robustness.\n- Reported gains over existing methods (e.g., AdaLoRA, DoRA) are minor, while the proposed method introduces additional computation."}, "questions": {"value": "- Could the authors show the distribution of the adaptive weights in Eq. 5? Line 195 states that “the core matrix plays a more critical role”; does this imply that the other two components receive negligible weights? Are there performance comparisons in Appendix F?\n- How are the hyperparameters α, β, λ chosen? The paper provides no sensitivity analysis. What does “appropriately chosen” mean in practice?\n- Could the assumption in Eq. 9 be further justified or empirically supported?\n- How to compute the effective rank in this paper? This detail is not explained.\n- Lines 223–224 vaguely describe “per-feature energy” and the EMA process. Please specify these computations clearly.\n- In LLaMA-2/3, feed-forward and attention matrices differ in dimension. Thus, pruning or expansion could change the parameter count even if rank stays constant. Are the reported table values consistent with this difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bYqIJlzA8x", "forum": "Lla4eFlDJu", "replyto": "Lla4eFlDJu", "signatures": ["ICLR.cc/2026/Conference/Submission6795/Reviewer_KJpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6795/Reviewer_KJpm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678033396, "cdate": 1761678033396, "tmdate": 1762919067345, "mdate": 1762919067345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PE-DyRA, a novel framework for the parameter-efficient fine-tuning (PEFT) of large language models. The authors identify two key limitations in existing methods like LoRA:1)  The inefficiency of a fixed-rank allocation and 2)  The one-directional nature (either pruning or expansion) of current dynamic methods.\n\nTo address this, PE-DyRA proposes two main contributions:\n\n### a) A bidirectional rank adjustment mechanism\nAt scheduled intervals, the framework prunes the least important ranks (based on an importance score) and simultaneously reallocates (expands) that same rank budget to the most important layers. This maintains a constant total parameter budget while dynamically redistributing resources.\n\n### b) A composite importance score\nTo guide the adjustment, the paper proposes a new importance metric that combines two sources of information:\n\n1.  A **gradient-based** score, which (like AdaLoRA) uses the product of smoothed sensitivity and uncertainty.\n2.  A novel **input-based** score, which incorporates the magnitude of input activations (inspired by the Wanda pruning metric).\n\nThe method utilizes the same SVD-based parameterization ($\\Delta=AEB$) as AdaLoRA to enable rank-level adjustments. The authors provide extensive experimental results on NLU (GLUE), mathematical reasoning (GSM8K), code generation (HumanEval), and summarization (XSum) tasks, demonstrating that PE-DyRA consistently outperforms existing PEFT baselines. The paper also presents a theoretical analysis based on Pareto optimality to justify the superiority of a dynamic allocation strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides compelling results across a diverse set of tasks (NLU, reasoning, code, summarization), models (DeBERTa, LLaMA 2/3), and budgets. The fact that PE-DyRA consistently outperforms baselines, including its direct predecessor AdaLoRA, strongly supports the authors' claims.\n2. The ablation studies are thorough and effectively validate the two primary contributions. Table 5 clearly shows that the bidirectional strategy (\"PE-DyRA\") outperforms \"Prune-only\" and \"Expand-only\" variants. Table 6 demonstrates the measurable benefit of including the \"input-based\" importance component.\n3. The paper is generally well-written and well-organized. The core idea is motivated clearly in the introduction, and the overall framework is well-illustrated in Figure 2."}, "weaknesses": {"value": "The paper suffers from a lack of clarity in key areas, which would make reproduction extremely difficult.\n\n1.  The hyperparameter $\\alpha$ for balancing the gradient- and input-based scores ($S_{G_{i}} = \\alpha \\cdot S_{G_{i}}^{grad} + (1 - \\alpha) \\cdot S_{G_{i}}^{inp}$) is introduced in Algorithm 1 but its value is never specified anywhere in the paper.\n2.  The description of the $S^{inp}$ calculation in Section 3.2.2 is ambiguous, using inconsistent indexing ($S_{ij} = |W_{ij}| \\cdot \\|X_j\\|_2$ vs. $S_{ki}$ in the final formula).\n\nMeanwhile, the paper's novelty is somewhat limited by its heavy reliance on AdaLoRA (Zhang et al., 2023b). The core SVD-based parameterization ($\\Delta=AEB$) and the entire formulation for the gradient-based importance score (smoothed sensitivity multiplied by uncertainty) are inherited directly from this prior work. While the work is cited, the text could be clearer about the extent of this inheritance."}, "questions": {"value": "1. The paper's core mechanism is asymmetric: pruning is fine-grained (removing the worst $k$ ranks globally), while expansion is coarse-grained (adding $k$ ranks to the top $k$ layers based on average importance).What is the rationale for this asymmetric design? Why not expand based on rank-level metrics as well? Does this design not create a \"rank-churn\" problem, where a new rank added to a high-average-importance layer may itself be unimportant and quickly become a candidate for pruning in the next cycle?\n\n2. Algorithm 1 introduces a critical hyperparameter $\\alpha$ to balance the gradient-based and input-based importance scores: $S_{G_{i}} = \\alpha \\cdot S_{G_{i}}^{grad} + (1 - \\alpha) \\cdot S_{G_{i}}^{inp}$. The paper only states this is \"appropriately chosen\". This is insufficient for reproducibility. What value of $\\alpha$ was used for the experiments? Was this value fixed across all tasks and models, or was it tuned? Please provide an ablation study on the sensitivity of $\\alpha$, as it seems central to the paper's contribution of combining these two importance metrics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfhkNt1pxx", "forum": "Lla4eFlDJu", "replyto": "Lla4eFlDJu", "signatures": ["ICLR.cc/2026/Conference/Submission6795/Reviewer_Xg83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6795/Reviewer_Xg83"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811268052, "cdate": 1761811268052, "tmdate": 1762919066771, "mdate": 1762919066771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dynamic LoRA framework that adjusts rank allocation during training to improve parameter efficiency. It first computes an importance score for each LoRA component using the product of weight and gradient magnitudes, then refines this score by multiplying it with the corresponding input activation to better capture data-dependent importance insipred by model pruning. \n\nBased on these scores, PE-DyRA performs a bidirectional rank adjustment (prune-then-extend): pruning the least important k ranks and reallocating them to the most important layers, enabling adaptive capacity redistribution under a fixed parameter budget. Experiments on GLUE, GSM8K, MATH, HumanEval, and XSum show some improvements over previous dynamic LoRA variants such as AdaLoRA and IncreLoRA, demonstrating higher efficiency and better overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a more comprehensive importance evaluation by combining input activations with gradient-based scores, leading to a more accurate assessment of parameter significance.\n2. It proposes a bidirectional dynamic rank adjustment strategy that prunes less important ranks and reallocates them to critical layers, effectively improving parameter utilization and adaptability."}, "weaknesses": {"value": "1. The performance gains diminish as the rank increases, as shown in Table 13, indicating limited scalability of the proposed method.\n\n2. The contributions are largely incremental, as most key ideas, such as gradient-based importance scoring, activation-aware weighting, and dynamic rank allocation, have been explored in prior works. The novelty mainly lies in combining these existing elements rather than introducing a fundamentally new mechanism.\n\n3. The improvements are relatively small on benchmarks like GLUE and XSum, and the bidirectional reallocation offers only marginal benefits compared with one-sided rank adjustment methods (Table 5)."}, "questions": {"value": "1. How sensitive is the performance of PE-DyRA to the choice of the exponential moving average (EMA) coefficients in importance estimation?\n2. How is the update interval for rank adjustment chosen, and how does it affect convergence stability during training?\n3. How does PE-DyRA perform compared to baselines under extreme low-rank settings(e.g., $r=1/2$)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "veomihDYxR", "forum": "Lla4eFlDJu", "replyto": "Lla4eFlDJu", "signatures": ["ICLR.cc/2026/Conference/Submission6795/Reviewer_ke7J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6795/Reviewer_ke7J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6795/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842054048, "cdate": 1761842054048, "tmdate": 1762919066454, "mdate": 1762919066454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}