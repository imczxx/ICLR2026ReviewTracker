{"id": "778Yl5j1TE", "number": 21504, "cdate": 1758318328856, "mdate": 1763362740520, "content": {"title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management", "abstract": "We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address the challenge, we introduce \\emph{summarization-based context management} to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization (\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \\texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks \\texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond fixed context length limits.", "tldr": "", "keywords": ["RL fine-tuning; multi-turn tool use; summarization-based context management"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10c39467b7f1356121d2e937298acf09641e8c62.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new algorithm SUPO aimed to optimize Llm training in long-horizon multi-turn tasks. Token sequences of increasing length pose a challenge to existing RL pipelines given the significant computational effort in terms of rollouts and memory requirements. In SUPO,  a task-relevant summary of past interactions is identified according to a threshold rule. A policy gradient is estimated based upon token sequences alongside summarization calls. The goal is to jointly optimize multi-turn interactions and summarization strategies. The authors report the results of an empirical testbed on CodeGym (Du et al., 2025) and BrowseComp-Plus (Chen et al., 2025) wherein SUPO is shown to lead to improved success rates with similar or smaller working context length."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and relevant problem in training LLMs for long horizon multi-turn tasks.\n\n2. The empirical testbed suggests context summarization is an effective way to reduce computational burden."}, "weaknesses": {"value": "1. I could not understand the way the context summarization tool is trained. At times the text seems to indicate that the only relevant hyper-parameter is the threshold L. However, this hyper-parameter is only used to determine when a sequence of tokens is considered too long. I fail to see anywhere in the text a clear indication on how the actual summary (which is also a sequence of tokens) is produced."}, "questions": {"value": "1. Please describe in detail how a summary sequence of tokens is produced. \n\n2. There is a literature on how to aggregate states for reinforcement learning.  See for example, the paper by  Singh, Jaakkola and Jordan. In a sense, summarization can be seen as state aggregation. I would like the authors to expand upon the connection of the present work to this literature.\n\nhttps://papers.neurips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pKzbruzuZP", "forum": "778Yl5j1TE", "replyto": "778Yl5j1TE", "signatures": ["ICLR.cc/2026/Conference/Submission21504/Reviewer_mER9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21504/Reviewer_mER9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853391119, "cdate": 1761853391119, "tmdate": 1762941808869, "mdate": 1762941808869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of context length limitations in reinforcement learning (RL) fine-tuning for long-horizon multi-turn tool use with large language models (LLMs). The proposed SUmmarization augmented Policy Optimization (SUPO) algorithm formulates this as an end-to-end RL problem, optimizing both tool-use behaviors and summarization strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- SUPO is the RL-based approach to seamlessly integrate summarization as a core component of the training process, effectively breaking the context length barrier. \n\n- The algorithm demonstrates robust performance gains on both coding (CodeGym) and web search (BrowseComp-Plus) benchmarks."}, "weaknesses": {"value": "- The novelty of the paper is relatively weak. The innovation point of summarization-augmented is essentially no different from agent memory [1] [2].\n\n- The reinforcement learning idea of SUPO is also widely applied. There are many works combining reinforcement learning and agent memory [3]. The authors did not conduct comparisons in the experimental section. Comparing only with GRPO is limited.\n\n- The related work also mentions Memory-r1. It designs adding, updating, or deleting entries in an external memory base. I do not think, as the authors claim, that it lacks the capability for summarization and compression. Choosing appropriate add, update, and delete operations is precisely summarization and compression.  Moreover, adding summarization and compression is just a matter of prompting and does not constitute a significant improvement.\n\n\n[1] Zhang, Zeyu, et al. \"A survey on the memory mechanism of large language model-based agents.\" ACM Transactions on Information Systems 43.6 (2025): 1-47.\n\n[2] Wang, Zora Zhiruo, et al. \"Agent workflow memory.\" arXiv preprint arXiv:2409.07429 (2024).\n\n[3] Yan, Sikuan, et al. \"Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning.\" arXiv preprint arXiv:2508.19828 (2025)."}, "questions": {"value": "- In Fig 1, do the last three lines all start with s1?\n\n- Why in Equation (1) is it $v_{sum} \\subseteq s_t$? Shouldn't it be $v_{sum} \\supseteq s_t$?\n\n- Why in the related work does it say that compared to MemAgent, “Our framework further subsumes more general multi-turn tool using problems”? What advantages does SUPO have over MemAgent?\n\n- Can the authors compare with Memory-R1 empirically? Because the SUPO combining reinforcement learning and agent memory, but does not compare with related work, which weakens the credibility of the paper's contributions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1abEptb5TV", "forum": "778Yl5j1TE", "replyto": "778Yl5j1TE", "signatures": ["ICLR.cc/2026/Conference/Submission21504/Reviewer_t1mf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21504/Reviewer_t1mf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914440591, "cdate": 1761914440591, "tmdate": 1762941808667, "mdate": 1762941808667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the use of RL on LLMs that use summarization-based context management. That is, given that periodic LLM-generated summaries are produced before resetting the working context to the initial prompt, how can we train the LLM to handle this structure most effectively? \n\nThis is formalized as a summarization-augmented MDP with a transition rule that triggers summarization once a threshold L is exceeded, then starts the following segment from the prompt \\+ the summary. The authors derive a policy-gradient representation that decomposes the long rollout into a sum of gradients over summarized sub-trajectories, enabling drop-in use of standard policy-gradient methods. \n\nThey instantiate the framework as SUPO and evaluate on CodeGym and BrowseComp-Plus. SUPO improves success rates relative to GRPO without summarization under the same or smaller working context, as well as ablations, and it scales at test time as the number of summary rounds increases beyond those used during training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Precise problem framing & principled setup.** The summarization-augmented MDP cleanly captures context resets and explicitly shows how summaries enter the dynamics.  \n* **Practical algorithmic choices.** Overlong masking and the group-relative advantage are well-motivated and ablated.  \n* **Compelling empirical results.** Across two distinct multi-turn tasks, SUPO achieves higher accuracy with narrower working windows than GRPO, and promising test-time scaling."}, "weaknesses": {"value": "* **Theory positioning.** Theorem 2.2 appears to be a particular case of the standard policy-gradient theorem (partitioning the sum at summary indices), since the setting is still an MDP. The paper could be clearer that this is an immediate corollary rather than a fundamentally new theorem. Additionally, if this is the case, much of the theoretical discussion would benefit from being trimmed down for leanness.  \n* **Minor clarity/typos.** There are multiple scattered grammar issues that should be polished before camera-ready.\n\nOverall, the contribution seems limited, as the paper essentially uses a slight variant of GRPO to train the LLM in this summarization setting, without any significant conceptual or methodological contribution. I am open to changing my assessment of this based on the discussion with the authors."}, "questions": {"value": "1. In what precise sense does Thm. 2.2 go beyond the classic policy-gradient theorem? From the derivation, it seems to be a partition of the standard sum. Could you clarify whether any usual assumption (e.g., Markovian dynamics, stationarity) fails and thus requires this adaptation?  \n2. **Figure 5 variant definitions.** For each figure, please specify the max number of summaries/tokens (S).   \n3. Beyond the positive scaling in Figure 5, do you observe a point where additional summaries hurt due to information loss or drift? Could you provide any analysis of summary content quality (e.g., factual retention rate) as (S) grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fo9J5NFMaN", "forum": "778Yl5j1TE", "replyto": "778Yl5j1TE", "signatures": ["ICLR.cc/2026/Conference/Submission21504/Reviewer_BrV1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21504/Reviewer_BrV1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21504/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989968981, "cdate": 1761989968981, "tmdate": 1762941808392, "mdate": 1762941808392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}