{"id": "psLcyKiuCp", "number": 2135, "cdate": 1756995364610, "mdate": 1763074767169, "content": {"title": "Learning from Encoded Video Bytes", "abstract": "We present an approach to understand video from encoded bytes, e.g., mp4s. These compressed videos are 99\\% smaller than the RGB pixel representations which are currently commonly used for video understanding. Encoded videos are able to compress the pixels by taking advantage of the redundant information across the frames using special encoding, such as key frames and motion residuals to handle this. However, standard video understanding models do not take advantage of this significant compression already available for each video, and instead either heavily subsample the frames or only work on short segments of the video. Here, we present an approach to understanding video from encoded bytes directly. We note that simply applying existing models, e.g., Transformers or State-Space models, to video byte sequences does not work, both due to difficulty in handling very long video byte sequences and easy overfitting. To address these challenges, we design a State-Space model with sequence parallelism to handle very long byte sequences, reaching 15 Million tokens in training, and essentially unlimited tokens in inference. We also propose a multilevel SSM activation fusion that reduces sequence length, which we find also benefits video understanding. We evaluate on common video understanding and natural extension to video + audio understanding tasks and demonstrate competitive performance, illustrating, for the first time, the feasibility of learning from compressed video byte representations.", "tldr": "", "keywords": ["video understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2ff5bbde8303355e1416ad1c6c4e027210586f69.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work propose to process video at the compressed byte level for video understanding. They especially designed a State-Space model for handling long byte sequences, and a multilevel SSM activation fusion for sequence length reduction and content understanding. They conducted experiments on popular video and video+audio benchmarks and compared with previous works."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Learning from compressed bytes for video understanding is a very novel and inspiring idea. Consuming videos without decoding into RGB frames has great potential of resource-saving, and this direction can be further explored by the community.\n\n2. The work explored and contributed some study about the structure of the bytes and training knowledge of the bytes: data augmentation and pre-training are important for video byte model."}, "weaknesses": {"value": "1. Table 3 is only indicating the TFLOPS benefits of bytes over RGB frames, but not showing the accuracy benefits / params benefits / latency benefits about that. Model Bytes obviously doesn't show strong classification performance, but which is claimed in lines 347-348.\nQuestion: Do you have any comparisons with variable controlled? like, under the same pretraining setting / params level / TFLOPS level, if you intend to prove the accuracy benefits?\n\n2. In lines 344-345, \"we used 25 fps (far higher than previous works)\", if you claim that, can you show the fps number at the table? How many fps is used by others?\n\n3. In Table 4, what is the baseline model? what is indicated by the second column? Can you clearly show and compare the computation number and performance number in the table for a clear comparison and for reaching the conclusion? Same as in Table 5 (which is not surpassing baseline) and Table 6.\n\n4. In Figure 4, I don't think 'number of frames - accuracy' constructs a proper trade-off pair in video understanding area. I also cannot see clear benefits to accuracy of sampling more frames. Num frames is not the number indicating the efficiency. Basically, accuracy should be compared under the same num frame setting. I never saw 'acc-num frame' is used as a trade-off pair to convince. Usually num frame can be used to indicate model throughput under the same computation/time resources. But it is weird to be set as x-axis in Figure 4. \n\n5. In the right figure of Figure 4, I also cannot see obvious advantage of using compressed bytes over RGB frames, acc-runtime tradeoff."}, "questions": {"value": "See weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oRTB7lvhDH", "forum": "psLcyKiuCp", "replyto": "psLcyKiuCp", "signatures": ["ICLR.cc/2026/Conference/Submission2135/Reviewer_AS16"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2135/Reviewer_AS16"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749941242, "cdate": 1761749941242, "tmdate": 1762916044235, "mdate": 1762916044235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "K6tvwosrAE", "forum": "psLcyKiuCp", "replyto": "psLcyKiuCp", "signatures": ["ICLR.cc/2026/Conference/Submission2135/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2135/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763074766476, "cdate": 1763074766476, "tmdate": 1763074766476, "mdate": 1763074766476, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes, for the first time, training deep learning models on encoded video bytes as opposed to raw RGB frames. They claim that encoded video bytes are similar to RGB frames in the sense that they can be decoded to into RGB frames. Thus, the semantic mapping learned by deep models trained on RGB frames should also work with encoded bytes. Due to the increased sequence length of video bytes as opposed to RGB videos, they propose using state space models over transformers due to their linear complexity. They perform training on a large pre-training RGB video dataset with their \"Bytes\" model and show experiments on various downstream action recognition and long video understanding datasets."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of training deep learning models with bytes vs. conventional RGB frames is extremely interesting.\n2. Many experiments are provided to validate that training a deep learning model with video bytes is feasible."}, "weaknesses": {"value": "1. **Extremely limited novelty** Besides the research problem of training deep learning models with encoded video bytes, there is no additional novelty introduced in the paper. The proposed Bytes model is basically a standard Mamba model, with stacked Mamba layers and some dimensional pooling in-between layers. The idea of computing sequences without a hidden state and updating after the fact was already proposed in the original Mamba paper [1]. The rest of the methodology section simply discuss trivial engineering details such as embedding size and pooling structure.\n\n2. **Scattered/Incomplete Experiments** Many of the experiments and tables in the paper are incomplete or do not have a coherent message. In Table 1, why are multi-modal models used as a comparison? The main idea should be to compare simple, uni-modal models that were ideally pre-trained on the same dataset as the Bytes model for a fairer comparison. Instead, a random assortment of multi-modal models, pre-trained on all different datasets, are used as comparison. Furthermore, I question the validity of Table 3 as the only VideoMamba variant that has 28.4 TFLOPS was VideoMamba trained on K400 in a self-supervised manner, whereas the authors claim that the table denotes classification results.  Moreover, the fact that simply doubling the size of model leads to a 25% increase in performance is surprising, and yet there is no discussion whatsoever on that point. Further, Tables 4 and 5 show results on a relatively unknown dataset, with no meaningful baselines. These are just a few examples of the inconsistency/unclear narrative around the experiments (see below).\n\n3. **Unsupported Claims** There are many claims made throughout the paper which require further support to confirm - especially given the new domain of training on encoded bytes. For example:\n\n- \"Bytes that are far apart have no correlation, so attention is not needed to understand video bytes.\" (Lines 127-128)\n- \"Recent SSM models [...] still don't fully address some challenges such as [...]. We present a different approach which preserves better the sequence signal.\" (Lines 160-165). The authors simply use stacked Mamba layers, so how does their method preserve the sequence better? Does this mean the comparison with baseline Mamba in Table 9 is a single Mamba layer?\n\n- \"We observed that the model very easily overfits when trained on encoded videos\" (Line 265). Not only are none of these experiments shown, but does this also imply that without augmentations, the Bytes model would get 4% on any test data due to overfitting? Yet simply by applying augmentations to the RGB video, which in turn changes the byte string, the authors are then able to attain nearly 85% on Kinetics? This also further raises questions about if nearly \"half the string\" is changed when applying small augmentations in the RGB image, how are encoded bytes a reliable enough modality for deep learning models to perform video understanding? The fact that even image-level understanding is not tackled first before investigating video/long-video understanding leads me to believe there is a lot more experimentation required to validate the large claims made in this paper.\n\n[1] Gu, A., & Dao, T. (2024, May). Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling."}, "questions": {"value": "See weaknesses section. Overall, I think the research question is interesting, but the scope, narrative, experiments, and writing of the paper requires a lot of improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1giVTThlFc", "forum": "psLcyKiuCp", "replyto": "psLcyKiuCp", "signatures": ["ICLR.cc/2026/Conference/Submission2135/Reviewer_ZNXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2135/Reviewer_ZNXf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777192209, "cdate": 1761777192209, "tmdate": 1762916043699, "mdate": 1762916043699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel paradigm, learning video from encoded bytes. Compared to traditional RGB-based video models, the proposed State-Space model is capable of handling extra-long video tokens at training time. The model achieves competitive performance on video-only and video+audio understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Using video bytes as model input substantially reduces compute and storage, addressing a key bottleneck in scaling video models.\n\n2. The proposed multi-stage SSM, parallelism strategy, strong data-augmentation are combined to facilitate the model training.\n\n3. Compared to RGB-based video models, the proposed model yields competitive performance on stardard video understanding benchmarks."}, "weaknesses": {"value": "1. Despite not perfect, the authors should give some cases on the reconstructed RGB/raw bytes in the self-supervised learning setup.\n\n2. As data-augmentation is conducted in the RGB space, this raises a concern about efficiency. The decoding of video is usually time-consuming (e.g. via Decord), sometimes being the efficiency bottleneck of dataloader especially for long videos. It seems that the proposed decode-then-encode augmentation pipeline might also be affected.\n\n3. It would be better if the results on SSv2 are reported, as SSv2 focuses on fine-grained temporal dynamics."}, "questions": {"value": "1. As encoding video bytes shares some similar features with language modeling, is it possible to initialize the model (even partially) with LLM pre-trained weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RrKWEqaUd7", "forum": "psLcyKiuCp", "replyto": "psLcyKiuCp", "signatures": ["ICLR.cc/2026/Conference/Submission2135/Reviewer_qgTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2135/Reviewer_qgTF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832596494, "cdate": 1761832596494, "tmdate": 1762916043570, "mdate": 1762916043570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes training video understanding models directly on compressed video byte streams instead of decoded RGB frames. It introduces a multilevel SSM to shorten sequences, and a sequence-parallel state correction scheme that enables training on very long inputs. The method achieves competitive results on multiple benchmarks.\n\nThe combination of compressed video input and state-space modeling is a promising design for scaling to long video sequences. However, the model design remains insufficiently developed, lacking clear mechanisms tailored to compressed data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, Clear motivation. The paper is well motivated. While most VLMs struggle to scale to long videos and rely on compressing image tokens from raw pixels, this work instead processes compressed video streams directly, leveraging mature video compression techniques to achieve efficient and scalable long-sequence modeling.\n\n2, Enabling parallelism for SSMs. The model proposes a technique for parallelism for SSMs. The sequence-parallel + state-correction formulation (with explicit update equations) is effective and lets SSMs scale to 15M tokens in training.\n\n3, Competitive results. The paper evaluates the performance in action recognition and  VQA benchmarks  and achieves competitive performance with strong efficiency scaling, including results on Kinetics-400/600 and long-video QA tasks (e.g., 57.1% on ActivityNet-QA with an LLM head)."}, "weaknesses": {"value": "1, Insufficient Related work. The Related Works section lacks prior research on video compression techniques (e.g., H.264, DCT, motion vectors), state-space models like S4 and Mamba, and compressed-domain learning from JPEG or H.264 inputs. Including these would better contextualize the proposed method and clarify its novelty.\n\n2, Video Byte-based model design underexplored. The model design shows little difference from a standard image-input long-sequence model and lacks specific adaptations for encoded video byte inputs. While the paper focuses on the SSM architecture and long-sequence processing, it omits critical details on handling compressed video data, such as how temporal compression, quantization, and I/P/B frame dependencies are managed—especially since B-frames require both past and future frames while SSMs operate only on past states. It also remains unclear how performance would compare if the model used image-byte sequence as inputs instead of video bytes.\n\n3, Parallelism assumptions vs. selective SSMs. The state-correction derivation (Eq. 1–4) is presented in linear SSM form; since Mamba is input-selective, it would help to clarify when/why these corrections remain valid and how approximation error behaves across chunks.\n\n\nMinor: \n\n1, In abstract, the abbreviation of SSM is used without introduction.\n\n2, Texts in the figures are too small and hard to read."}, "questions": {"value": "1, In line 240, which parameter is $h_0$ in equation 3? Would the correction still be valid when C is input dependent? How does the correction handle multilevel SSM with pooling?\n\n2, How does the model encode the byte input? How does it handle I, B and P frame differently?\n\n3, What is the model performance on long video benchmarks, such as LVBench, MLVU, VUE-TR or Neptune? (Line 73-74)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "spl56sqGsd", "forum": "psLcyKiuCp", "replyto": "psLcyKiuCp", "signatures": ["ICLR.cc/2026/Conference/Submission2135/Reviewer_jVJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2135/Reviewer_jVJm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873070044, "cdate": 1761873070044, "tmdate": 1762916043423, "mdate": 1762916043423, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}