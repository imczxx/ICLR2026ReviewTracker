{"id": "Fjw3OqdjTj", "number": 13288, "cdate": 1758216061241, "mdate": 1759897448638, "content": {"title": "Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval", "abstract": "The widely used retrieve-and-rerank pipeline faces two critical limitations: they are constrained by the initial retrieval quality of the top-k documents, and the growing computational demands of LLM-based rerankers restrict the number of documents that can be effectively processed. We introduce Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations by directly retrieving documents according to reranker preferences rather than following the traditional sequential reranking method. Our method uses a greedy search on proximity graphs generated by approximate nearest neighbor algorithms, strategically prioritizing promising documents for reranking based on document similarity. Experimental results demonstrate substantial performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9 on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100 documents. Our analysis suggests that, given a fixed pair of embedding and reranker models, strategically selecting documents to rerank can significantly improve retrieval accuracy under limited reranker budget.", "tldr": "", "keywords": ["Vector Similarity Search", "Information Retrieval", "LLM reranker"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/236c1472ab212f42233d61d4b07512ce1c71e035.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method Reranker-Guided Search (RGS) to address two key limitations of the traditional ‚Äúretrieval-reranking‚Äù pipeline: 1) Its performance is constrained by the quality of the Top-K documents returned by the initial retrieval stage; 2) The computational cost of large language model (LLM)-based rerankers is prohibitively high, limiting the number of documents that can be processed. RGS bypasses sequential reranking by performing a greedy search directly on the document proximity graph generated by an approximate nearest neighbor (ANN) algorithm. This allows for strategic prioritization of more promising documents for reranking based on the reranker's preferences. Experiments on BRIGHT, FollowIR, and M-BEIR demonstrate that RGS achieves significant performance gains over sequential reordering methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Compared to the Retrieve-and-Rerank approach, reranking results are imperceptible to the retrieval process. In RGS, reranking results can help improve the retrieval process, thereby achieving performance optimization.\n 2. On three challenging benchmarks: BRIGHT, FollowIR, and M-BEIR, RGS achieved significant performance improvements over both the traditional method and SlideGAR. This demonstrates its effectiveness.\n 3. Experiments demonstrate that RGS is robust to noisy query embeddings. Even when query embeddings are of low quality, they can still identify relevant documents by leveraging the local structure of the document graph and guidance from the reordering module."}, "weaknesses": {"value": "1.  RGS requires less computational effort, but each step's result depends on the previous one, resulting in longer processing delays compared to traditional reranking methods. In retrieval systems, this may be unacceptable to users. And RGS requires pre-constructing a proximity graph for the document collection. For large-scale document repositories, building and storing such a graph structure incurs additional preprocessing costs and memory overhead.\n2. For an initial retrieval result that is completely irrelevant and isolated in the embedding space, RGS struggles to correct it through re-ranking into a completely different, highly relevant region. Does RGS possess a certain degree of corrective capability?\n3. Although the effectiveness of RGS was validated across three datasets, these datasets are relatively small compared to real-world datasets such as MSMARCO. RGS requires validation of its effectiveness on a larger scale and should supplement its efficiency assessment."}, "questions": {"value": "1. Given that RGS requires multiple steps to return its final search results, time delay must be considered as a key metric for evaluating RGS methods.\n2. Case studies can be used to highlight improvements and enhancements to RGS, as well as whether it has a corrective effect on irrelevant retrieval results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CZhLkmdsIG", "forum": "Fjw3OqdjTj", "replyto": "Fjw3OqdjTj", "signatures": ["ICLR.cc/2026/Conference/Submission13288/Reviewer_Y7xP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13288/Reviewer_Y7xP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908881017, "cdate": 1761908881017, "tmdate": 1762923961273, "mdate": 1762923961273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method is Reranker-Guided-Search (RGS), a novel technique that directly retrieves documents based on a reranker's preferences using a greedy search on a proximity graph, rather than following the traditional sequential retrieve-then-rerank pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-The proposed method aims at a critical retrieval bottleneck.\n\n-For effective reranking with limited budget, RGS strategically selects the most promising documents to rerank, achieving significantly better performance (as evidenced by the reported gains of 3.5-5.1 points) without increasing computational cost. \n\n-The method is empirically validated across multiple diverse benchmarks BRIGHT, FollowIR, M-BEIR, demonstrating its robustness and generalizability."}, "weaknesses": {"value": "-The proposed method is highly dependent on the embedding model. Compared with Retrieve-and-Rerank in Figure2, the proposed method is less sensitive to the choice of embedding model with budget more than 200. However, the highly dependency on the embedding model is still similar to the Retrieve-and-Rerank method.\n\n-Why the proposed method RGS is better for reasoning intensive search tasks? Which componets of RGS play a critical role for this advantage?\n\n-RGS claimed that the novel introduction of documentation similarity will prioritize more promising documents and improving complexity query-document relationships. However, SlideGAR has used such information to improve search performance. The difference between them needs to be clear."}, "questions": {"value": "-The paper focuses on the quality of initial top-k ranked results, and proposes a corresponding searching method to tackle the tradeoff between effectiveness and efficiency. The proposed metric rerank@k is used for evaluation in the experiments. But we need some traditional metric like ndcg@k for comparison in Table 1 and 2. Though NDCG@10 is also provided, the correlation between rerank@k and ndcg@k leaves unknown. Moreover, the reasonability of rerank@k is needed to be verified.\n\n-The paper must be self-contained. Some notations like q_t\\rightarrow c_i in Table 2 need to be explained.\n\n-A detailed analysis should be provided to explain why the proposed method RGS performs less better on multi-modality retrieval datasets than scientific and math reasoning datasets.\n\n-Time complexity of baseline methods retrieve-and-rerank and SlideGAR needs to be analyzed like section3.4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4977jlpln5", "forum": "Fjw3OqdjTj", "replyto": "Fjw3OqdjTj", "signatures": ["ICLR.cc/2026/Conference/Submission13288/Reviewer_BB8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13288/Reviewer_BB8G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094573348, "cdate": 1762094573348, "tmdate": 1762923960839, "mdate": 1762923960839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Reranker-Guided-Search (RGS), a graph-based retrieval pipeline that departs from the standard retrieve-then-sequential-rerank paradigm by using a greedy search on a proximity graph of document embeddings and letting an expensive LLM-based reranker steer which graph neighborhoods to expand. The method initializes from a small set of embedding-nearest seeds, iteratively expands the neighbors of high-ranked documents, and uses a sliding-window listwise reranker to keep the most promising candidates until a fixed reranker budget is exhausted. Empirically the authors evaluate RGS under a Reranker@ùëò protocol on three challenging benchmarks, BRIGHT, FollowIR, and M-BEIR, and report consistent improvements over retrieve-and-rerank and a recent SlideGAR baseline, especially in low-reranker-budget regimes. The paper also presents ablations on embedding quality, perturbations to embeddings, and an analysis suggesting final performance is often limited by reranker alignment rather than search coverage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important practical bottleneck: how to get more value out of a small number of expensive reranker calls. The idea of combining graph-based ANNS traversal with reranker guidance is natural but underexplored, and the RGS design is a clean, well-motivated instantiation of that idea: it leverages the clustering hypothesis and the bi-metric intuition to trade cheap embedding similarity for targeted reranker effort. The experiments are broad and focused on modern, hard benchmarks that stress reasoning and multi-modality rather than only classical TREC-style tasks; showing gains across BRIGHT, FollowIR, and M-BEIR strengthens the claim of generality. The Reranker@ùëò evaluation protocol is also a useful contribution because it isolates the real-world constraint of costly reranker usage and makes comparisons fairer."}, "weaknesses": {"value": "First, practical costs and latency tradeoffs are not fully quantified: report wall-clock latency, token usage, and parallelization implications (RGS is sequential by design) so practitioners can judge deployment feasibility. \nSecond,  novelty relative to GAR/SlideGAR is claimed but the differences are somewhat qualitative.\nThird, statistical significance and variance across random seeds or query subsets are not reported; adding confidence intervals would strengthen the empirical claims."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mSvaqfiNSA", "forum": "Fjw3OqdjTj", "replyto": "Fjw3OqdjTj", "signatures": ["ICLR.cc/2026/Conference/Submission13288/Reviewer_zg7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13288/Reviewer_zg7B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110365541, "cdate": 1762110365541, "tmdate": 1762923960377, "mdate": 1762923960377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents reranker guided search (RGS). The technique is summarized in Algorithm 1. Essentially, the approach iteratively decides which documents to retrieve next by (1) reranking the current set of retrieved docs, (2) using the top ranked doc (after reranking) to then decide which new docs to include, and repeating at step (1) until the budget is fulfilled. The results on multiple text and multi-modal datasets are strong."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "S1. Retrieve and rerank is becoming dated. Listwise is certainly an improvement over cross-encoder, but the design space for search is massive yet very under-explored for papers that report results on the top datasets, including the ones mentioned in this paper. It is great to see new creative works like RGS being applied, and with seemingly good results. That being said, it's hard to calibrate the strength of the results without more baselines.\n\nS2. The paper includes extensive results and thoughtful analysis. I include some questions about new analysis, and questions about baselines, but the work is already very informative without these.\n\nS3. The results with perturbation are particularly fascinating. The field at large has started to question the need for vector search given that reranking can be used to filter down an initial candidate set, and this is a result in that same spirit."}, "weaknesses": {"value": "W1. It is a major weakness that ReasonIR is not included as a baseline, nor other reranker alternatives like JudgeRank or Rank-R1. Basically, the results in Table 1-3 are very limited in their baselines, only using BGE embedding and not looking at many rerank alternatives.\n\nW2. Listwise rerankers are know to be very sensitive to order, for example see https://arxiv.org/abs/2306.17563 which shows that listwise rerankers in reverse or when re-run multiple times can have very different results. For this reason, it would be very helpful to show Recall@R in Tables 1-3. We can not tell otherwise if it is recall that is helping or a different ordering of results fed to the reranker. The text has \"the number [recall] is only 31% on BRIGHT [at top-100]\", which does suggest RGS is helping with recall, but it's not clear which datasets this holds for. Using Recall@R can even be used to compare all the methods without applying any reranking.\n\nW3. Intuitively, it's not clear why RGS should help with instruction following.\n\nW4. The benefits of RGS decline rapidly after budget hits 500. I actually wonder if it will start to get worse as the budget increases further. More importantly, it's not clear so clear whether 100 budget is needed, or much of the benefit can be achieved with only 50 or even 20.\n\nW5. Although time complexity of RGS is reported, there is no cost analysis between the different methods."}, "questions": {"value": "How does Recall@R compare using embedding-only vs embedding + proximity graph? Basically, what is the recall before listwise reranker applied?\n\n\"We believe that these benchmarks represent the most complicated retrieval tasks available, which can‚Äôt be solved by simple semantic similarity, and serve as a better testbed for evaluating the efficiency of different high-performing retrieval methods.\" There are a range of other tasks worth considering beyond this set. CRUMB (https://arxiv.org/abs/2509.07253) provides a good overview.\n\nIs it necessary to run the reranker each time at step 12 in the algo? Could the reranking simply be done at the end?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "86MG6LJ5lU", "forum": "Fjw3OqdjTj", "replyto": "Fjw3OqdjTj", "signatures": ["ICLR.cc/2026/Conference/Submission13288/Reviewer_nmG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13288/Reviewer_nmG8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111200712, "cdate": 1762111200712, "tmdate": 1762923959570, "mdate": 1762923959570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}