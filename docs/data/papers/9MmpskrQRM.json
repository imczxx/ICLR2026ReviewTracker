{"id": "9MmpskrQRM", "number": 18571, "cdate": 1758289182563, "mdate": 1763732394403, "content": {"title": "Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry", "abstract": "We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes $0$ once the number of training samples exceeds a certain threshold.\nUnderstanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators—parameters that achieve zero training error—have been observed to generalize effectively. In this study, under a teacher–student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.", "tldr": "", "keywords": ["generalization error", "interpolator", "algebraic geometry"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e77ca96935965c10e99b5bda95b67464cf2aacc2.pdf", "supplementary_material": "/attachment/f0db221735a58712e49dbbd07a8e7020321d2bc9.zip"}, "replies": [{"content": {"summary": {"value": "This paper proves that randomly sampled interpolators (parameters that fit the training data exactly) can achieve zero generalization error once the sample size passes a finite threshold determined by the model’s geometry. The authors make two strong assumptions: (i) the data are noiseless, and (ii) the true function is representable by the network (realizability). They upper-bound the strong sample complexity by $\\dim(\\Theta)-\\dim(\\overline{\\Theta})+2$, where $\\Theta$ is the parameter space and $\\overline{\\Theta}$ is the teacher-equivalence set (TES); the claimed result holds with probability 1 under a real-analytic student model and an absolutely continuous input distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The approach of upper-bounding the strong sample complexity through an algebraic-geometric lens is interesting. Similar conclusions could be formulated in function space (rather than parameter space) via Bézout-type arguments, so **the novelty appears limited**. I have a few concerns about the proofs (listed below) and would appreciate the authors’ clarifications."}, "weaknesses": {"value": "- **Scope/novelty**: The main result (Theorem 2) closely aligns with classical dimension-counting arguments in function space: viewing the parametric model’s image inside a function space, each exact data constraint $f(x)=y$ imposes an affine hyperplane, so intersecting with $k$ such constraints reduces dimension by **at most** $k$. This suggests a finite “strong sample complexity’’ comparable to the model (image) dimension under suitable **regularity/transversality**. \n\n- **Proposition 4**: I am not fully convinced by the tube-measure step as stated in the proof. For example, the set $\\overline{\\Theta}$ (TES) need not be a **smooth submanifold** and this causes a problem! This can occur, for instance, when the teacher is realizable by a highly overparameterized network, which is not uncommon. Moreover, **in Figure 1**, you correctly show TES is not a smooth manifold, which suggests that **you are aware** of the potential presence of non-smooth points. Note that Weyl’s tube formula in its classical form applies to smooth compact submanifolds (or, more generally, requires conditions like positive reach/curvature measures or a stratified argument with the top stratum dominating). Please **(a)** state assumptions ensuring smoothness/positive reach (or a Whitney stratification plus a reference that justifies the leading-order volume scaling for real-analytic sets), and **(b)** clarify how one gets equation (2) in line 768. As written, these technicalities are crucial for the $1 - O(\\varepsilon)$ probability conclusion.\n\n- **Theorem 2**: In the proof outline (lines 344–345), the reduction by $n$ is attributed to imposing $n$ equations. Are you assuming a **complete-intersection** or **transversality condition** so that each constraint generically reduces the dimension by $1$? My concern is that the data is **not generic** (since it is generated by the **true function without noise**), so the resulting equations **need not be generic**. \n\n- **Notation**: Line 125 says “squared norm’’ but writes $\\||\\cdot\\||$; please clarify whether you mean the Euclidean norm $\\||\\cdot\\||_2$ or the squared Euclidean norm $\\||\\cdot\\||_2^2$ (cf. line 150, which appears to use the squared norm).\n\nOverall, I find this line of research interesting. However, in its current state, the work does not meet the standards of a theoretical contribution suitable for ICLR, primarily due to **incomplete (and potentially wrong) proofs**. I am not fully convinced of their correctness, and several essential details appear to be missing. This raises doubts about the validity of the results in the level of generality claimed by the authors. Nevertheless, I remain open to discussion."}, "questions": {"value": "Can you clarify how you get equation (2) in line 768? Also can you clarify in general how one can compute the dimension of the true parameter set?\n\nLastly, can you explain my main concerns regarding the proofs of Theorem 2 and Proposition 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rMTAuRPYzj", "forum": "9MmpskrQRM", "replyto": "9MmpskrQRM", "signatures": ["ICLR.cc/2026/Conference/Submission18571/Reviewer_dTSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18571/Reviewer_dTSd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760534714462, "cdate": 1760534714462, "tmdate": 1762928283695, "mdate": 1762928283695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Under a teacher–student regression setting, the authors prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training examples exceeds a threshold determined by the geometry of the interpolator set in parameter space. Using tools from algebraic geometry—specifically real analytic sets—the paper introduces and rigorously analyzes the concept of strong sample complexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear and Well-Written Presentation:** The paper is generally well-written, and the results are clean.\n- **Innovative Theoretical Contribution:** The paper rigorously analyzes the generalization properties of interpolators using tools from algebraic geometry. The derived results are both elegant and insightful. This paper is solid.\n- **Empirical Validation:** Experimental results on synthetic regression tasks and the MNIST dataset back up the theoretical findings, showing that the predicted bounds on the strong sample complexity are consistent with observed generalization behavior."}, "weaknesses": {"value": "- **Strong & Restrictive Assumptions:** The analysis is carried out in a controlled, noiseless teacher–student setting, and the student model is assumed to be real analytic. In practical scenarios, these assumptions may not hold, e.g., the teacher model is a ReLU MLP.\n- **Limited Applicability to General Machine Learning Settings:** The theoretical results depend on the assumption that the teacher and student models belong to the same parametric function class—more precisely, Assumption 2 requires the teacher-equivalent set (TES) to be non-empty. In many real-world machine learning tasks, the 'teacher model' (not in the teacher-student model) typically has a larger or more expressive function class compared to the student model. Consequently, Assumption 2 may fail to hold, thereby limiting the applicability of the results to more general settings.\n\nOverall, I think this paper has a significant contribution to the teacher-student setting. It deserves to be published!"}, "questions": {"value": "In general machine learning settings, the generalization error is typically of the order $O(1 / \\sqrt{n})$, where $n$ is the sample size. Consequently, to achieve a generalization error smaller than a given threshold $\\epsilon$, the required $n$ often depends exponentially on the input dimension $d$. In contrast, the sample complexity to achieve zero generalization error presented in this paper is much lower. While we acknowledge that this work is situated within a teacher-student framework, it would significantly enhance the contribution of the paper if the authors could further discuss the following points:\n- (1) Can the results derived in this paper be extended to a more general machine learning setting beyond the teacher–student model?\n- (2) What is the relationship between the proposed sample complexity results and the traditional generalization bounds (e.g., the $O(1 / \\sqrt{n})$ rate) encountered in broader machine learning contexts?\n- (3) Is it possible to extend these results to non-analytic models, such as neural networks with ReLU activations? If not, what are the main obstacles that prevent such an extension?\n\nA detailed discussion addressing these questions would not only clarify the scope of the current results but also broaden the impact of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSL7N0sGI6", "forum": "9MmpskrQRM", "replyto": "9MmpskrQRM", "signatures": ["ICLR.cc/2026/Conference/Submission18571/Reviewer_Codj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18571/Reviewer_Codj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761318237535, "cdate": 1761318237535, "tmdate": 1762928283245, "mdate": 1762928283245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new theory to explain the good generalization error of neural networks in the interpolation regime and in the teacher-framework setting. More specifically, The authors assume that the data (both training and test) is generated by a teacher network, without noise. Then, the generalization error of student network is studied, by focusing on the weight vectors achieving zero training error (assuming these always exist). The paper is based on the analysis of the dimension of these interpolators set, by assuming they form real analytics manifold. Based on this analysis, the authors managed to get a bound on the sample complexity, effectively showing that if the number of data points is large enough, then he empirical interpolator set converges to the ``population'' interpolator set, and, hence, all interpolators of the training errors achieve zero test error (for $n$ large enough). The technique is also extended to near interpolators. Finally, the theory is supported by numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The authors propose a model-based analysis of the generalization error of interpolators in a teacher-student setting. This offers an interesting perspective on generalization, showing that perfect generalisation error can be reacher when there is structure in the data distribution and the model is compatible with it (in the sense that it can interpolate).\n- The paper uses tools from algebraic geometry, suggesting new links between this field and statistical learning theory\n- The literature review in the introduction seems quite complete"}, "weaknesses": {"value": "*Main weaknesses:*\n - The introduction put an emphasis on the models that \"employ an excessive number of parameters\". However, the proposed theory states that the strong sample complexity is bounded by the number parameters of the student network. This bound seems to suggest that not too much overparameterization is allowed in order to obtain zero generalization error. Even in Theorem 6, the derived sample complexity seems to be $k = O(\\sqrt{d_\\Theta})$, which allows some but not arbitrary overparameterization. While the bounds are very interesting, I think that the link with overparameterization should be discussed more.\n - Line 401, it is stated that the strong sample complexity is independent on the number of parameters, but it still depends on the with of the student network, this should be clarified.\n - In my opinion, the claim of the abstract to explain the generalization error of interpolators \"for general machine learning models\" is a bit strong, given that the study is restricted to the teacher-student setting, which imposes a lot of structure on the learning task that is not necessarily present in general. I think this point should be clarified. \n - The assumption of real analyticity might prevent some modern neural architectures to fall into this framework.\n\n\n*Other issues:*\n- Line 190, the uniform distribution on $\\widehat{\\Theta}_n$ is mentioned, but it is not clear to me in general that a set can be endowed with a uniform distribution. What is meant by uniform distribution should maybe be clarified."}, "questions": {"value": "- A lot of recent work also suggest that is an implicit bias of the learning algorithm towards well-generalizing solutions, can you discuss how compatible these findings are with your theory?\n- Do you think that your theory can be extended to ReLU networks, where the analyticity assumptions might fail?\n- Why is there a +2 in theorem 2? In the proof sketch, it seems to be a +1 (of course it is a very minor change)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yKTiyRA28u", "forum": "9MmpskrQRM", "replyto": "9MmpskrQRM", "signatures": ["ICLR.cc/2026/Conference/Submission18571/Reviewer_LkHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18571/Reviewer_LkHH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566984622, "cdate": 1761566984622, "tmdate": 1762928282295, "mdate": 1762928282295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the generalization performance of a random interpolator. Under certain technical conditions, the main of which being a sufficient number of samples, the authors prove that the latter generalization error equals exactly 0.  The authors use it to make a point that the implicit bias property, studied a lot in the recent literature, might not be crucial for explaining generalization for the over-parameterized models. The paper uses techniques from algebraic geometry in the proofs of its main results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The problem formulation addresses generalization - one of the cornerstones of modern machine learning. The approach is novel and interesting."}, "weaknesses": {"value": "The resulting bounds appear to be vacuous for the overparameterized models, even though analyzing the latter serves as the motivation in the introductions. To corroborate further, consider the simplest case where both the teacher and the student networks match each other and are given by $f(w, x) = wTx$, where $x, w \\in \\mathbb{R}^d$. Denote the frozen weight parameter of the teacher network by $w_*$. Take $x$ to be i.i.d. standard normal. Then, first of all, the only zero generalization error weight is $w = w_*$.  As such, the theorem can be true only if it's the only interpolator. This is indeed the case: if $n \\ge d + 2$, there are enough equations in the corresponding system to recover $w_*$ uniquely. **However,  $n \\ge d + 2$ corresponds to an under-parametrized scenario**, missing the point of the minimum l_2-interpolator framework.  And I suspect that the same would happen for the non-linear neural networks. To add more to it, it is known that, for highly over-parameterized settings, the generalization error of an interpolator can depend heavily on the choice of the interpolator, as demonstrated extensively, for example, in \"Stochastic Mirror Descent on Overparameterized Nonlinear Models: Convergence, Implicit Regularization, and Generalization\" by Azizan et al."}, "questions": {"value": "1. Do you have any examples where Theorem 2 is applicable to an over-parameterized problem?\n2. How do we sample an interpolator if the set of all interpolators is not compact, such as in the case of a linear student and a linear teacher? I think this is not addressed in the paper, as it just says that the parameter is sampled \"uniformly\". \n3. What exactly does experiment in Section 6.2 validate if we still use Adam, which likely has its own implicit bias, to find all interpolators we consider? Also, it is very easy to attain a near-zero accuracy on MNIST-10, but what would the experiment look like for more complicated datasets, such as cifar-10?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NHw4qmhpMI", "forum": "9MmpskrQRM", "replyto": "9MmpskrQRM", "signatures": ["ICLR.cc/2026/Conference/Submission18571/Reviewer_p2LG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18571/Reviewer_p2LG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18571/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738411806, "cdate": 1761738411806, "tmdate": 1762928281639, "mdate": 1762928281639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}