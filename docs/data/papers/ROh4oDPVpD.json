{"id": "ROh4oDPVpD", "number": 7534, "cdate": 1758026192861, "mdate": 1759897847523, "content": {"title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "abstract": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R1-Precision of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Our code will be released soon.", "tldr": "", "keywords": ["Text-to-Motion Generation", "Spatial-temporal-frequency Modeling", "Causal Learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/18ae06506e89c24adcf81fcf868670b30bfab916.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces TriC-Motion, a new diffusion-based framework for text-to-motion generation that unifies spatial, temporal, and frequency-domain modeling under a causal learning paradigm. Authors address two limitations of existing approaches: 1) the lack of an integrated multi-domain representation that captures temporal dynamics, spatial joint topology, and motion frequency characteristics simultaneously, and 2) the entanglement of motion-relevant and irrelevant cues that degrade generation quality. To overcome these issues, TriC-Motion introduces three key components: Tri-Domain Modeling Modules, Score-guided Tri-domain Fusion, and Causality-based Counterfactual Motion Disentangler. Experimental results on HumanML3D and SnapMoGen demonstrate good performance gains over state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The performance of the proposed method on both HumanML3D and SnapMoGen dataset surpasses most baseline methods, proving the effectiveness of modeling all three domains.\n+ The paper writing is clear and easy to follow.\n+ The exploration of causal intervention in motion generation domain is interesting and inspiring."}, "weaknesses": {"value": "- The main idea of modeling all three domains at the same time is less convincing and requires more intuitive explanation and theoretically grounded analysis.\n- The architecture is heavy in computing and complex in the architecture. Therefore, it is important to also compare the run time, computation cost and model size with baseline methods.\n- The training stability is also worrying due to the complex architecture. The loss weighting needs more deep analysis and sensitivity test."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OCh9Otmso7", "forum": "ROh4oDPVpD", "replyto": "ROh4oDPVpD", "signatures": ["ICLR.cc/2026/Conference/Submission7534/Reviewer_p1bB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7534/Reviewer_p1bB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472866588, "cdate": 1761472866588, "tmdate": 1762919634649, "mdate": 1762919634649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes TriC-Motion, a diffusion-based framework for text-to-motion generation that integrates spatial, temporal, and frequency domain modeling with causal intervention to ensure temporal consistency, spatial topology, and dynamic coherence.\nExperimental results show that TriC-Motion achieves an R1-Precision of 0.612 on the HumanML3D dataset, significantly outperforming existing methods and generating motions with superior realism, consistency, diversity, and text alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed method demonstrates strong text–motion consistency.\n\n2.The introduction of causal learning reduces the impact of irrelevant information on motion generation."}, "weaknesses": {"value": "1.Please provide t-SNE or other visualization analyses that disentangle motion-irrelevant and motion-relevant information to demonstrate the effectiveness of the proposed method.\n\n2.The paper’s joint temporal–frequency–spatial strategy improves text–motion alignment (R@1, R@2, R@3), but FID did not improve; therefore you cannot claim that generation quality has improved, and the conclusions stated in the abstract are not supported.\n\n3.What advantages does using DistilBERT for word-level and sentence-level feature encoding have compared to CLIP?\n\n4.The proposed TME, STM, HFA, and S-Fus are commonly used extraction and fusion strategies in the temporal–spatial–frequency domain and lack novelty.\n\n5.The methods compared in Figure 5 are all general approaches; it’s unclear whether the proposed method outperforms contemporary spatio-temporal modeling methods. If possible, provide qualitative comparisons (or quantitative comparisons if allowed) against methods from papers accepted to CVPR 2025, ICCV 2025, and NeurIPS 2025."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jGp96avLs2", "forum": "ROh4oDPVpD", "replyto": "ROh4oDPVpD", "signatures": ["ICLR.cc/2026/Conference/Submission7534/Reviewer_QW9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7534/Reviewer_QW9h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703906835, "cdate": 1761703906835, "tmdate": 1762919633166, "mdate": 1762919633166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TriC-Motion, a diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. It includes Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis, with a Score-guided Tri-domain Fusion and a Causality-based Counterfactual Motion Disentangler to expose motion-irrelevant cues and fuse valuable information across domains."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The first work that simultaneously integrates spatial, temporal, and frequency domains into a unified motion generation framework\n\n2. Introduces a causality-based counterfactual motion disentangler to expose motion-irrelevant cues and disentangle the real modeling contributions of each domain.\n\n3. Provides ablation studies indicating the effectiveness of each domain branch and the causal-intervention design."}, "weaknesses": {"value": "1. The paper uses a perceptual loss defined in the same motion–text embedding space used by the HumanML3D evaluator (the author could clear this point if I'm wrong). Using the same feature extractor for training and inference would inflate the performancve. The author could do an ablation study that removes this loss term to show that the R-precision gain is not from this loss term.\n\n2. No visualization results. Quantitative metrics in text-to-motion is proven to be fragile and sometimes misaligned with human judgment. For motion, demo videos are necessary. I don’t see any supplementary videos, which makes it hard to judge the actual quality.\n\n3. The metrics themselves aren’t convincing. Even if R-precision is SOTA, an R-precision much higher than the ground truth is not meaningful and doesn’t reflect visual quality. Even worse, FID is significantly poorer compared to current methods.\n\n4. Missing strong baseline results. Compare against recent, stronger works (MARDM [1], MotionStreamer [2], MotionLCM v2 [3], SALAD [4]) with both qualitative and quantitative results. Also consider including a human study.\n\n5. Minor typo. In the introduction you say R@1 is 0.573, which doesn’t match Table 1.\n\nReference\n\n[1] Rethinking Diffusion for Text-Driven Human Motion Generation\n\n[2] MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space\n\n[3] MotionLCM-V2: Improved Compression Rate for Multi-Latent-Token Diffusion\n\n[4] SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uI0TZ5ZZ1R", "forum": "ROh4oDPVpD", "replyto": "ROh4oDPVpD", "signatures": ["ICLR.cc/2026/Conference/Submission7534/Reviewer_7LNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7534/Reviewer_7LNK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969153733, "cdate": 1761969153733, "tmdate": 1762919632357, "mdate": 1762919632357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 'tri-domain + causal' framework for text-to-motion generation. Built on MDM, it models motion in parallel with TME (temporal encoding), STM (skeleton-topology GCN), and HFA (hybrid frequency analysis via DWT+FFT). A dual-score S-Fus module fuses motion/semantic signals, and TIJ injects text via cross-attention. Training uses CCMD counterfactual decomposition (factual + counterfactual branches) to suppress spurious cues."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves remarkable improvement on the R Precision metric.\n\n2. The paper is well-written, ensuring that its content is easily understandable for readers.\n\n3. It is the first time for casual learning to be used in text-to-motion generation, making significant contributions to the research community."}, "weaknesses": {"value": "My primary concern is the choice of baselines. Under the HumanML3D evaluation protocol, the evaluator is too weak: many recent methods already surpass the 'ground truth', making R-Precision on HumanML3D unreliable. Meanwhile, the FID gap to stronger methods is large (0.285 vs 0.033), so the proposed method shows no advantage on HumanML3D. Porting the approach to a MoMask baseline should not be difficult; the authors should adopt a more appropriate baseline; otherwise, it may look like trading motion quality for text consistency, which does not substantiate effectiveness.\n\nIn addition, the ablation should include more combinatorial settings to better demonstrate effectiveness.\n\nFinally, the paper lacks a demo video as supplementary material and a user study to subjectively assess text-motion alignment and motion quality."}, "questions": {"value": "Please kindly refer to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "daaxxcTAXT", "forum": "ROh4oDPVpD", "replyto": "ROh4oDPVpD", "signatures": ["ICLR.cc/2026/Conference/Submission7534/Reviewer_CRLx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7534/Reviewer_CRLx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994941149, "cdate": 1761994941149, "tmdate": 1762919631797, "mdate": 1762919631797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}