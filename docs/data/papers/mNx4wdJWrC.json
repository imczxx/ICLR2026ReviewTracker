{"id": "mNx4wdJWrC", "number": 18247, "cdate": 1758285597834, "mdate": 1759897116775, "content": {"title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks", "abstract": "The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. \nWe explore the conditions that contribute to over-memorization and discover that this issue is prevalent across various tasks, models, and fine-tuning methods, with prolonged training and large learning rates exacerbating the problem. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. In light of our findings on over-memorization, we offer recommendations for checkpoint selection and propose techniques such as checkpoint merging and memorization-aware reweighting to mitigate this effect.", "tldr": "", "keywords": ["over-memorization", "llm analyses", "llm understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e2245c0cca8baa350c640bd3943edd0f19413291.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper unveils the over-memorization phenomenon in the case of reasoning of LLM. The authors unveil the conclusion via conducting experiments on math datasets with three backbones and various lora variants. They try to propose a method to mitigate this problem."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written with clear storyline.\n2. The paper's conclusion is clear, with experiments supported."}, "weaknesses": {"value": "1. **Limited novelty and generalizability of the main conclusion**  \n   The central claim—that supervised fine-tuning (SFT) tends to memorize [2]—has been extensively discussed in prior work. The paper’s contribution would be stronger if it offered a more nuanced or novel perspective on this phenomenon.  \n\n   More critically, the experimental setup undermines the generalizability of the findings. The authors repeatedly fine-tune on a small dataset (e.g., 100K examples) for many epochs (e.g., 10), which is not representative of modern SFT practices. In real-world scenarios, large-scale reasoning datasets such as OpenR1 (220K), MetaMath (395K), or NuminaMath CoT (860K) are commonly used, often with only 1–3 epochs. Under such realistic conditions, the observed over-memorization may not occur, and the conclusions—and by extension, the proposed MAR method—may not hold. Without experiments on larger, more diverse datasets or with standard training protocols, the practical utility and robustness of MAR remain unconvincing.\n\n2. **Concerns about the formulation and theoretical grounding of MAR**  \n   The proposed MAR loss is defined as  \n   \\begin{equation}\n   \\mathcal{L} = \\sum_{t=1}^T \\big(1 - p_\\theta(y_t \\mid x, y_{<t})\\big) \\cdot \\ell(y_t),\n\\end{equation}\n   which is the inverse of the weighting scheme used in DFT [3] (where the weight is $p_\\theta(y_t \\mid x, y_{<t})$). While the intuition—down-weighting high-confidence tokens to mitigate memorization—is plausible, the paper lacks theoretical justification or empirical ablation to validate this design choice. In particular, it is unclear whether this inversion genuinely addresses over-memorization or merely shifts the optimization dynamics in a way that coincidentally improves test performance on narrow benchmarks. Robustness across architectures, datasets, and training regimes has not been demonstrated.\n\n3. **Triviality of the analysis on over-memorization conditions**  \n   The paper claims to “explore the conditions that contribute to over-memorization” (Line 17), but the analysis appears superficial. For instance, the PPL on GSM8K test set (human-written reasoning traces) and MetaMath QA (GPT-3.5-generated traces) is attributed to over-memorization. However, this gap is more naturally explained by the distributional mismatch between $p_{\\text{human}}$ and $p_{\\text{GPT}}$: SFT aligns the model with the training distribution ($p_{\\text{GPT}}$), so high PPL score on human-authored test data is expected—not surprising. Similarly, overfitting under excessive epochs, large learning rates, or increased model capacity is a well-known phenomenon in traditional supervised learning [1]. Framing these as novel insights into LLM fine-tuning overstates the contribution.\n\n\n\n## Reference \n\n[1] Smith L N. A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay[J]. arXiv preprint arXiv:1803.09820, 2018.\n\n[2] Chu T, Zhai Y, Yang J, et al. SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training[C]//Forty-second International Conference on Machine Learning.\n\n[3] Wu Y, Zhou Y, Ziheng Z, et al. On the generalization of sft: A reinforcement learning perspective with reward rectification[J]. arXiv preprint arXiv:2508.05629, 2025."}, "questions": {"value": "1. In Figures 2 and 4, Llama-3.1-8B exhibits more severe over-memorization compared to Mistral and Gemma under identical training settings. Could the authors hypothesize why this architecture is particularly prone to memorization? Is this due to architectural differences (e.g., RoPE vs. ALiBi, attention mechanisms), training data composition, or initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "545ipX93Uu", "forum": "mNx4wdJWrC", "replyto": "mNx4wdJWrC", "signatures": ["ICLR.cc/2026/Conference/Submission18247/Reviewer_XDrx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18247/Reviewer_XDrx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761295528450, "cdate": 1761295528450, "tmdate": 1762927977182, "mdate": 1762927977182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a practically important training-time regime in finetuning LLMs for reasoning tasks: test accuracy plateaus at a high level while test perplexity continues to increase. The authors document this regime across several model families (LLaMA, Mistral, Gemma), multiple adaptation methods (full FT, LoRA variants), and several task types, suggesting it is not an isolated artifact. They further probe downstream behaviors and show that models in this regime can exhibit consistent, though modest, degradation in OOD generalization, robustness, diversity, and privacy metrics, which implies that accuracy alone is an insufficient model-selection criterion. Finally, they discuss lightweight mitigations such as checkpoint selection/merging and a memorization-aware reweighting loss, although the latter needs clearer positioning relative to existing reweighting techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1、Practical training takeaway. The paper highlights a realistic failure mode in finetuning pipelines: validation perplexity may start to rise while task accuracy is still improving, so stopping purely on perplexity can prematurely discard useful checkpoints. This makes the work directly useful to practitioners who fine-tune reasoning-capable LMs.\n2、Multi-faceted behavioral probing. Beyond reporting the metric divergence, the authors systematically examine its downstream effects on OOD generalization, prompt robustness, diversity, calibration, and privacy, offering a broader picture of what this “high-accuracy, high-perplexity” phase might entail.\n3、Strong reproducibility. The paper provides detailed dataset construction, model/configuration descriptions, and finetuning hyperparameters for several adaptation methods, making it straightforward for others to re-run, stress-test, or challenge the reported phenomenon."}, "weaknesses": {"value": "1、Marginal Novelty of the Core Phenomenon: The paper's central concept of \"over-memorization\"—defined as rising test perplexity while test accuracy remains stable —is insufficiently distinguished from classical overfitting. The authors define classical overfitting as rising perplexity and decreasing accuracy, but the behavior they identify is arguably just a minor variant of this, where the task-specific metric (accuracy) is less sensitive or lags behind the loss metric (perplexity). The claim to be the \"first to uncover the phenomenon\"  is overstated.\n2、Confounding Experimental Comparison: The behavioral analyses in Section 5 are based on a flawed comparison. The \"normal model\" is defined as the checkpoint from epoch 3, while the \"over-memorized model\" is the checkpoint from epoch 10 of the exact same training run. This is not a comparison of two different states; it is a comparison of an undertrained model against a fully trained model. A valid study would compare two models trained to convergence with different hyperparameters (e.g., one with a low learning rate, one with a high) that achieve the same high in-domain accuracy but different perplexity levels.\n3、Significant Omission of Related Work for \"MAR\": The proposed \"Memorization-Aware Reweighting\" (MAR) mitigation technique appears to be a direct reimplementation of existing loss-weighting concepts, such as Focal Loss, which down-weight the loss contribution of easy (high-confidence) examples. The paper's formula $\\mathcal{L}_{MAR}=\\sum_{t=1}^{t}(1-p_{\\theta}(y_{t}|x,y_{<t}))\\cdot l(y_{t})$ is a clear example of this. The authors fail to cite, discuss, or compare their method to this extensive and highly relevant body of literature, which calls the novelty of this contribution into serious question."}, "questions": {"value": "1、The paper sometimes describes the phenomenon as distinct from classical overfitting (no accuracy drop), but later aligns it with overfitting-like generalization issues. Could you make the taxonomy explicit: is over-memorization an early phase of overfitting in overparameterized LMs, or a genuinely different failure mode?\n2、The current comparison (epoch 3 vs epoch 10 from the same run) may conflate undertraining with the proposed phenomenon. Please consider a matched-accuracy, different-training-condition comparison (e.g., low-LR-long vs high-LR-short) to isolate the effect.\n3、Since the explanation relies on “multiple valid reasoning paths,” could you analyze generated chains from the two checkpoints to show they indeed follow different but correct trajectories?\n4、MAR looks very close in spirit to focal-style reweighting. It would help to cite and position against this line of work, and ideally add a small comparison to show MAR is preferable in this setting.\nFlag For Ethics Review"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "82dcdRu3v7", "forum": "mNx4wdJWrC", "replyto": "mNx4wdJWrC", "signatures": ["ICLR.cc/2026/Conference/Submission18247/Reviewer_7ccS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18247/Reviewer_7ccS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892187409, "cdate": 1761892187409, "tmdate": 1762927976874, "mdate": 1762927976874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reports an empirical phenomenon in LLM finetuning for reasoning tasks that the authors call over-memorization: after early training gains, test perplexity rises while test accuracy stays high, and this coincides with reduced robustness, OOD generalization, and generation diversity. The effect is shown across learning rates, epochs, finetuning methods (LoRA, PiSSA, full FT), and models; the authors also propose two mitigations—checkpoint merging and a memorization-aware reweighting (MAR) objective—and offer checkpoint-selection guidance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear empirical pattern: Multiple plots/tables show rising test PPL without collapsing accuracy, across tasks and models. \n\nBreadth: Results cover math QA, code, and scientific QA; also Gemma/Mistral. \n\nPractical takeaways: Concrete checkpoint-selection advice (balance val-ACC with val-PPL) and evidence that it matters. \n\nRobustness/OOD/Diversity analyses: Over-memorized checkpoints are more brittle to neutral prompt preambles and underperform on OOD. \n\nLightweight mitigations: Checkpoint merging (explicit formula) and MAR are simple and effective."}, "weaknesses": {"value": "Causality vs. correlation. While learning rate and training time correlate with the effect, other confounds (batch size, data curriculum/order, decoding temperature during evaluation, regularization, LoRA rank, prompt templates) are not systematically ruled out. The methodological breadth is good, but ablations feel incomplete\n\nNovelty positioning could be sharper. The paper claims to be the first to uncover this specific phenomenon; related work (e.g., learning-dynamics perspectives) is noted, but the boundary between “over-memorization” and known overfitting/under-determination behaviors is not crisply formalized. A precise decision rule for when a checkpoint is “over-memorized” is missing.\n\nSignificance of OOD/robustness deltas. OOD drops (~2 points on average) and perturbation losses are suggestive but modest; there are no confidence intervals or statistical tests, and some test sets are small/sensitive. Please report variance across seeds and significance for Table 2/Table 3."}, "questions": {"value": "Could checkpoint merging be extended beyond linear two-point averaging (e.g., weight-space ensembling across more steps), and what are the failure cases?\n\nFor Table 2 and Table 3, please include CI/SE over seeds and explain whether differences are statistically significant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cYZGM2eKDU", "forum": "mNx4wdJWrC", "replyto": "mNx4wdJWrC", "signatures": ["ICLR.cc/2026/Conference/Submission18247/Reviewer_GHrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18247/Reviewer_GHrg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969601554, "cdate": 1761969601554, "tmdate": 1762927976507, "mdate": 1762927976507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces and investigates an “over-memorization” phenomenon that emerges when fine-tuning large language models on reasoning tasks: test accuracy stays high while test perplexity keeps rising. Using LLaMA‑3.1‑8B on the MetaMathQA dataset, the authors show that larger learning rates accelerate this effect, whereas smaller rates eventually produce the same outcome. They propose two mitigation strategies—checkpoint merging and memory-aware reweighting—and demonstrate that both improve performance on in-domain and out-of-domain evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically uncovers the “high accuracy, rising perplexity” over-memorization phenomenon during fine-tuning, verifies its prevalence across multiple learning rates and methods, and clearly distinguishes it from traditional overfitting.\n\n2. It conducts a comprehensive experimental investigation that quantifies over-memorization’s negative impact on robustness, out-of-distribution generalization, Best-of-N sampling, and privacy risk with rich metrics.\n\n3. The paper proposes practical mitigation strategies—checkpoint merging and memory-aware reweighting—that improve both ID and OOD performance, keep computational cost low, and are backed by ablation analyses for deployment guidance.\n\n4. It maintains methodological rigor and reproducibility by detailing hyperparameters, data preprocessing, and multiple random seeds, while supplying full implementation specifics in the supplementary materials."}, "weaknesses": {"value": "1. Lack of deep theoretical understanding: The paper’s central weakness is that it remains overly empirical. Its theoretical analysis of why over-memorization occurs is insufficient, offering only a superficial explanation via cross-entropy mechanics (Sec. 5). It provides no mathematical framework to predict when over-memorization will arise or to quantify its severity beyond empirical observation (Eqs. 3–4).\n\n2. Questionable evaluation setup: Given the broad scope of the topic, comprehensive experiments are crucial. However, the study relies primarily on MetaMath for training and focuses on math reasoning benchmarks such as GSM8K and MATH. MetaMathQA was introduced in 2023, when GSM8K scores were still below 70; today, GSM8K and MATH (grade-school and high-school math) are largely saturated and offer limited insight. More challenging math evaluations—e.g., AIME 2024/2025—should be included. In addition, the paper lacks verification on other reasoning domains (logical, commonsense, etc.; Sec. 4.1) and evaluates only a single backbone (LLaMA‑3.1‑8B), which limits generality; models like the Qwen series should be tested to support broader claims.\n\n3. Inconsistent and unclear mathematical formulations: Equation (2)’s memory-aware reweighting loss \\(L_{\\text{MAR}}\\) lacks sufficient theoretical grounding for the chosen weighting scheme, and Equation (1)’s checkpoint merging uses a fixed 1/2 coefficient without theoretical or empirical justification."}, "questions": {"value": "See “Weaknesses.” and consider the following advices：\n\nExpand experimental scope and introduce statistical validation: The evaluation should be extended to diverse tasks such as logical reasoning (e.g., LogiQA), commonsense reasoning (e.g., CommonsenseQA), and reading comprehension, and the claims should be validated across multiple model scales (1B, 3B, 7B, 13B+) and architectures (Mistral, Gemma, Qwen, etc.) to strengthen generality.\n\n\nI genuinely find this topic an interesting angle. However, the experimental analysis is currently too empirical; if the concerns above are thoroughly addressed, I would consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "utWGkt8sBa", "forum": "mNx4wdJWrC", "replyto": "mNx4wdJWrC", "signatures": ["ICLR.cc/2026/Conference/Submission18247/Reviewer_Y3sc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18247/Reviewer_Y3sc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762853356782, "cdate": 1762853356782, "tmdate": 1762927976186, "mdate": 1762927976186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}