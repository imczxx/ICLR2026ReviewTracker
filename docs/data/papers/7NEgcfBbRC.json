{"id": "7NEgcfBbRC", "number": 24292, "cdate": 1758355027753, "mdate": 1763101932330, "content": {"title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "abstract": "Neural Architecture Representation Learning aims to transform network models into feature representations for predicting network attributes, playing a crucial role in deploying and designing networks for real-world applications. Recently, inspired by the success of transformers, transformer-based models integrated with Graph Neural Networks (GNNs) have achieved significant progress in representation learning. However, current methods still have some limitations. First, existing methods overlook hardware attribute information, which conflicts with the current trend of diversified deep learning hardware and limits the practical applicability of models. Second, current encoding approaches rely on static adjacency matrices to represent topological structures, failing to capture the structural differences between computational nodes, which ultimately compromises encoding effectiveness. In this paper, we introduce LeDG-Former, an innovative framework that addresses these limitations through the synergistic integration of language-based semantic embedding and dynamic graph representation learning. Specifically, inspired by large language models (LLMs), we propose a language embedding framework where both neural architectures and hardware platform specifications are projected into a unified semantic space through tokenization and LLM processing, enabling zero-shot prediction across different hardware platforms for the first time. Then, we propose a dynamic graph-based transformer for modeling neural architectures, resulting in improved neural architecture modeling performance. On the NNLQP benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA while demonstrating the first successful cross-hardware latency prediction capability. Furthermore, our framework achieves superior performance on the cell-structured NAS-Bench-101 and NAS-Bench-201 datasets. The source code will be released publicly.", "tldr": "", "keywords": ["Neural Architecture Representation", "Dynamic Graph Attention", "Language Embedding", "Latency Prediction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fcdf5533dcf9759e7804fe466f287993fbd901b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses the insufficient information usage problem in designing neural architecture representation learning methods. It identifies two key limitations, which are the overlooked hardware information and the heterogeneity of the nodes in the architecture graph. To tackle these issues, the authors propose a novel architecture representation learning method that incorporates hardware information and utilizes a language template representation to capture the heterogeneity of nodes. Three different masks are applied to process the attention information. The proposed method is evaluated on several NAS benchmarks, demonstrating its effectiveness compared to existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the insufficient information usage problem in designing neural architecture representation learning methods. It identifies two key limitations, which are the overlooked hardware information and the heterogenety of the nodes in the architecture graph, which are well motivated."}, "weaknesses": {"value": "1. Although the language template representation part is novel in the NAS domain, it is still not a novel solution. In areas like graph foundation model or graph learning, related techniques that transform graphs into tokens by using LLMs have already been explored. \n2. There lacks an in-depth investigation of the proposed problem. The positive correlation between the hardware information and the resulting latency is trivial to find. It is better to provide more insights than only showing the performance improvements."}, "questions": {"value": "1. What are the specific reasons for using three masks (son, father, grandfather)? What about using more or less masks?\n2. Table 1 is not straightforward to read. Please clarify the meaning of baselines at the beginning of the section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3WwNtDQZGB", "forum": "7NEgcfBbRC", "replyto": "7NEgcfBbRC", "signatures": ["ICLR.cc/2026/Conference/Submission24292/Reviewer_x8ou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24292/Reviewer_x8ou"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557791219, "cdate": 1761557791219, "tmdate": 1762943030853, "mdate": 1762943030853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Area Chair and Reviewers,\n\nWe sincerely thank the Area Chair and all reviewers for their time, effort, and constructive feedback.\n\nTo briefly clarify a key misunderstanding (especially by Reviewer wbrn) about our novelty: LeDG-Former is a hardware-aware neural architecture representation framework that jointly encodes architectures and hardware platforms via language embeddings and a dynamic graph transformer. By tokenizing both network structures and hardware specifications into a shared LLM-based semantic space, and using a dynamic graph module that learns adaptive relations instead of fixed adjacency matrices, LeDG-Former yields more expressive and hardware-aware representations. Experiments on NNLQP and NAS-Bench-101/201 show that LeDG-Former sets new state-of-the-art results and, for the first time, achieves zero-shot latency prediction on unseen hardware platforms. \n\nOnce again, we are grateful for the reviews, which will greatly help us to strengthen and clarify this line of work.\n\nBest regards,\n\nThe Authors"}}, "id": "xyu7hOTwDI", "forum": "7NEgcfBbRC", "replyto": "7NEgcfBbRC", "signatures": ["ICLR.cc/2026/Conference/Submission24292/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24292/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24292/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097107206, "cdate": 1763097107206, "tmdate": 1763097107206, "mdate": 1763097107206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "4N1OmIj4EP", "forum": "7NEgcfBbRC", "replyto": "7NEgcfBbRC", "signatures": ["ICLR.cc/2026/Conference/Submission24292/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24292/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763101930388, "cdate": 1763101930388, "tmdate": 1763101930388, "mdate": 1763101930388, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper is interested in proposing an ML model that receives a neural architecture as input, and outputs information about it, including predicting latency of running the input neural architecture on specific hardware (useful for compiler/program optimization, without having to compile and run every program); or predicting the accuracy of the input architecture on classification tasks (useful for increasing accuracy, without having to train every hyperparameter configuration). The paper produces estimates that are better than other SoTA methods.\n\nI think the paper has a lot of good contributions, however, I will vote for rejection due to limited novelty (model is straightforward combination of others) and also inaccurate advertising (\"dynamic\" graph & generalizing to unseen opcodes). However, if paper revises this (e.g., removes word dynamic from everywhere, and admits that the novelty is in the application and not in the model invention), then I could upgrade my rating."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The paper sets state-of-the-art for a couple of problems where the input is some neural network architecture. This has broad use-cases in \"efficiency\" [running faster] and in neural architecture search [finding more accurate networks, without going through the training process].\n\n* The paper introduces a GNN architecture, which consists of combining known techniques \n\n* Paper combines LLMs with GNNs. This combination is an active area of research."}, "weaknesses": {"value": "## Major weaknesses\n\n* The model is attention model on multiple adjacency matrices, i.e., a straightforward combination of (MixHop, NGCN, or alike) with graph transformer archiectures.\n\n* The word dynamic should be completely eliminated from everywhere: title, section headers, abstract, and main text. The paper does *not* deal with dynamic graphs. It deals with static graphs. They only combine different adjacency hops from a fixed graph (like MixHop, etc). Dynamic implies that the neural network architecture is changing with time (which is the case, for \"dynamic neural networks\", which are not studied at all in this paper).\n\n* The computational complexity of the method (eq 6-8) seems to be quadratic -- due to term $Q K^\\top$. This is actually bad news, especially because graphs of neural networks can contain tens-of-thousands, up-to a couple of million nodes. \n\n* Last paragraph of Intro claims that prior methods are limited to single hardware optimization. This is false. While I am not an expert in the field, I happened to attend the oral of https://openreview.net/pdf?id=bpS4vaOg7q which also does training & inference on multiple hardwares.\n\n* \"The specialized language templates\" line 101 defeats the motivation to handle \"unseen node types\" [line 188]. Can the authors show experiments on generalizing to new op-codes? In any case, I personally dont think this is a big issue because I would assume the op-codes in neural architectures are merely a handful (less than 300?): conv, add, relu, einsum, etc. \n\n* Please mention the sizes of your matrices and vectors (parameters and inputs/outputs). For instance, the dimensionality of $W_1, W_2, W_3$ (of Eq.4) is not obvious. This is especially confusing because softmax usually returns one object (not 3). The model is the most important piece (IMO) and being exact is important.\n\n* The representation $f^r_{node}$ is not used after definition. Perhaps math bug?\n\n* The related section does not serve its purpose and can be cut by at least 70%. Why not just focus on GNNs applied on Neural Architectures?\n\n## Minor Weaknesses\n\n* \"model deployment\" (last words of 4th line of paper) is very out of place. I think the paper meant to say \"inference\"?\n\n* In many paragraphs, the paper writes \"idea1, idea2, idea3 (citation1, citation2, citation3)\". It is better to properly distribute as \"idea1 (citation1), idea2 (citation2), ...\"."}, "questions": {"value": "* Did you actually do the quadratic computation of $Q K^\\top$? If not, please detail in the paper how.\n\n* Please address other questions from the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2pQXk6ai8Y", "forum": "7NEgcfBbRC", "replyto": "7NEgcfBbRC", "signatures": ["ICLR.cc/2026/Conference/Submission24292/Reviewer_wbrn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24292/Reviewer_wbrn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845226729, "cdate": 1761845226729, "tmdate": 1762943030551, "mdate": 1762943030551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on neural architecture representation learning, which typically represents computational operations as nodes and data flow as edges. Although transformer–GNN-style models have successfully captured both local and global information in graphs, current designs still overlook hardware attribute specifications and positional variations among nodes, as well as their distinct neighborhood attention patterns. To address these issues, this work integrates large language models (LLMs) with dynamic graphs to better capture hardware attributes and improve flexibility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed framework effectively integrates neural architecture and hardware platform representations, advancing existing approaches.\n2. The proposed dynamic graph self-attention mechanism reduces the computational cost of full attention while maintaining flexibility for specific local structures.\n3. The experimental results appear strong and convincing."}, "weaknesses": {"value": "1. Is the predefined template sufficiently informative for any neural architecture design? The manually defined template may be limited by the user’s knowledge. Could an automated approach be developed to extract more aligned concepts from both the neural architecture and the hardware platform? Furthermore, how are the hardware platform descriptions obtained? Their quality may significantly affect overall performance.\n2. If the attended nodes are predefined by nearly two-hop parents, how can the model still benefit from the global information flow that the original graph transformer provides?"}, "questions": {"value": "My main concern is W1 as described alove. Besides, there is a typo: “Attributes Predcting” in Figure 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jGpo13545Y", "forum": "7NEgcfBbRC", "replyto": "7NEgcfBbRC", "signatures": ["ICLR.cc/2026/Conference/Submission24292/Reviewer_RKFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24292/Reviewer_RKFN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24292/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762110518213, "cdate": 1762110518213, "tmdate": 1762943030228, "mdate": 1762943030228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}