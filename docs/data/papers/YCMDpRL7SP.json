{"id": "YCMDpRL7SP", "number": 21775, "cdate": 1758321620079, "mdate": 1763704208518, "content": {"title": "Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)", "abstract": "Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. \nWe examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. \nWe identify a feature space---the stylistic feature space---that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. \nFurthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected.\nWe then seek to understand if stylistic detectors are inherently more robust. \nTo study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. \nWe show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style.\nHowever, as the number of samples available for detection grows, the human and machine  distributions become distinguishable.\nOverall, our findings underscore previous recommendations to avoid reliance on machine-text detection.", "tldr": "", "keywords": ["generation", "machine-text detection", "style transfer", "detector evasion"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42b7145e7eabf777d433a14869be19b09e7d62de.pdf", "supplementary_material": "/attachment/9cf798063acccca03ce2be403b2dcbacd57ca2e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a stylistic feature space that can be used to detect LLM-generated text, in a way that remains robust against LLMs specifically optimized to evade detection using existing optimization techniques. It then proposes a training recipe using SFT and DPO to fine-tune LLMs to both mimic the writing style of human-written text and avoid detection, and shows that text generated by such fine-tuned LLMs evades detectors that rely on stylistic features, but only when a limited number of generated samples are presented to the detectors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors study an interesting problem of machine-text detection with important implications for preventing misuse of LLMs. The idea of fine-tuning LLMs to both mimic human writing styles and avoid detection conceptually makes sense.\n- The authors evaluate various detectors and baseline attacks, showing comprehensive results."}, "weaknesses": {"value": "- Overall, I think the paper lack clarity. Please see Questions.\n- The evaluation setup in Section 2 is not clear. How is the optimization against each detector done? What parameters did you use when you optimized the LLMs following Nicks et al.? Are the parameters the same across all LLMs evaluated?\n- It’s good to have the AUROC(10) metric, but you should also report the usual AUROC score. Additionally, if the argument is that false-positives are costly in real-world scenarios, FPR of 10% is still quite high, what would the AUROC be for lower FPRs? (e.g., 1% FPR used in StyleDetect?)\n- Concerns about overclaiming: according to Figure 3, your approach is more easily detectable against FastDetectGPT and Binoculars (even when only a single sample is available for detection), which contradicts claims \"In contrast, our approach is more universally undetectable across all detectors tested\" and \"Our detector evasion approach is consistently harder to detect across all detectors.\" The fact that with 10 samples, FastDetectGPT and Binoculars reach nearly 0.8 AUROC(10) on text generated by your approach limits its applicability to a very few number of samples, it’d be more ideal if there isn’t such a “trade-off” between detectability by FastDetectGPT/Binoculars and StyleDetect. \n \nMinor presentation issues include:\n- Duplicate sentences in lines 267 and 270\n- Typo in table 1 caption \"variatns with -DPO-StyleDetect suffix\"\n- Grammar error in line 288 \"can best evades\", missing \"(2)\" in line 289\n- In Table 2 and Appendix E, it'd be nice to include the original human-written text to compare with the LLM-generated samples and show that your approach does better"}, "questions": {"value": "- Could you clarify the training and evaluation setup? Specifically, section 4.1 says that samples are drawn from Reddit, Amazon reviews, and Blogs datasets to construct the preference tuning dataset, but in line 300 you mention that your approach is not trained in \"Amazon and Reddit\" domains\n- Could you clarify what type of detector was trained for distinguishing between the outputs of your system and human-written text, before using this detector to construct the preference dataset (referred to in \"Avoiding Machine-Text Detectors\" part of Section 3)? How big is D_sup? How did you measure the level of \"human-like\" in generated outputs?\n- In section 4.1, when you generate 5 paraphrases for each sampled human-written text to construct the training dataset, do you evaluate the semantic similarity between the paraphrases and the original? What about the diversity of the paraphrases?  \n- In section 4.2, what's the difference between the prompting baseline and the first paraphrasing baseline, since both prompt gpt-4o-mini to paraphrase machine-generated text? \n- In section 4.3, you describe the inference process of your approach, since it is specific to your approach, it’s better to describe it in the method section instead. Additionally, how the inference process works is not entirely clear to me, what system prompt do you use for your style-aware paraphraser at each iteration? How many iterations do you do and how do you decide how many iterations to do?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Sa6M49vSqx", "forum": "YCMDpRL7SP", "replyto": "YCMDpRL7SP", "signatures": ["ICLR.cc/2026/Conference/Submission21775/Reviewer_3Y7h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21775/Reviewer_3Y7h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842027884, "cdate": 1761842027884, "tmdate": 1762941926663, "mdate": 1762941926663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors show that DPO methods which have been used to circumvent log-likelihood based detectors of machine generated text are ineffective against style-based detectors of machine generated text, even when the DPO method is explicitly designed to evade style-based detectors. They then show that an alternative paraphraser can be effective at evading style-based detectors.\n\nThere are essentially three findings:\n\n1) Rewriting a text to evade log-likelihood based detectors of machine generated text does not evade style-based detectors of machine generated text.\n2) Attempting to modify the DPO based methods for evading log-likelihood based machine generated text detectors by substituting a style based goal for the log-likelihood based goal is also ineffective at avoiding style based detectors. \n3) An alternative (simple but nice) method for evading style based detectors does work.\n\nI think point 1 is good to check, but the result is unsurprising. Point 2 was interesting to me and unintuitive, I would have liked to see more discussion of why this holds (a reason is given on page 1, but I'd have liked to see more justification). The strength of point 3, for me, really depends on how novel the method for evading style based detectors is, I think the authors should do a better job of explaining this.\n\nI think there is quite some chance I will change my review significantly during the discussion stage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed humaniser (as described in section 3.1) is an interesting, simple idea. \n\nThe observation that methods to evade log-likelihood based detectors do not evade style based detectors is not surprising, but it is good to see it rigorously shown (one would be very surprised if style based detectors were secretly exploiting artefacts which are visible to log-likelihood based detectors, but it is still good that the authors have ruled this out)."}, "weaknesses": {"value": "There are several minor weaknesses which I mention in the questions section below.\n\nThroughout the manuscript the authors use 'detectors' as a proxy for 'log-likelihood based detectors of machine generated text'. There are a lot of clever detectors incorporating different signals than pure log-likelihood (e.g. looking at patterns in log likelihood rather than average log-likelihood), I don't think you need to use all of these different detectors as baselines but I do think you should avoid writing as if average log-likelihood based detectors and style based detectors are the only options.\n\nI would have liked aspects of the experimental design to be more clearly stated (how many tokens were in each text, what parameters were used to generate machine text).\n\nHowever, I think my main criticism is that I struggled to latch onto which parts of the paper are novel and surprising enough to meet the very high bar of acceptance into ICML. It could be that your humaniser is a simple, elegant new idea which may have applications beyond text detection, my gut feeling is to be impressed by the idea, but if this is the case then I'd like the authors to explain to me how it relates to other paraphrasers (rather than just that the performance is better on one metric), and really make the case for the strength of this idea. It could be the case that the authors have a clever insight about the difference between evading detection methods which aggregate token level scores and evading those that don't, but I would like to see more justification and analysis of the following sentence (How-\never, while this approach defeats detectors that use token-level features of the predicted conditional\ndistributions (Ippolito et al., 2020; Mitchell et al., 2023; Bao et al., 2024; Hans et al., 2024), we\nshow that detectors that use writing style (Soto et al., 2024) remain robust to the distribution shift\nintroduced during optimization.)"}, "questions": {"value": "1) What is special about style that makes it immune to DPO optimisation against style detectors. You suggest on page 1 that the thing about writing style is that it is not present at token level? In that, style detectors are not an aggregate of token level scores. Do you know (or have good reason to think) that this is the reason that DPO optimisation doesn't work? If so, what about other detectors which do not aggregate token level scores, e.g. those based on patterns in log-likelihood (rather than averages of log-likelihood). \n\n2) The proposed humaniser (as described in section 3.1) is an interesting, simple idea. Could you put it in more context as to how it relates to previous paraphrasers. Has a similar idea been tried (successfully or otherwise) elsewhere? Is it generally seen as improving the quality of machine generated text to human eyes? I think this is the strongest idea in the paper, I would like to use the fact that 1) the humanizer is novel and 2) your humaniser could find applications outside of detecting machine generated text as points in your favour, but I'd like you to provide a little more evidence/context for me to do so.\n\n3) After I wrote the previous paragraph I searched for other paraphrase inverters. I found https://aclanthology.org/2025.findings-acl.227/ . Mitigating Paraphrase Attacks on Machine-Text Detection via Paraphrase Inversion\nRafael Alberto Rivera Soto, Barry Y. Chen, Nicholas Andrews. I think this should be cited, and I think it's the same essential idea for evading detection, could you comment.\n\n3) I don't think your claim in the 'main contributions' section that 'optimizing against such detectors does not reduce their performance' is accurate. What you are really saying is that the strategy for optimizing against log-likelihood based detectors does not translate directly to style-based detectors.\n\n4) 'These results suggest that the features indicative of writing style\nare distinct from those used by detectors that use features derived from the predicted conditional\ndistributions.' I think this claim is slightly too strong, I would replace 'features derived from the predicted conditional distributions' with 'log-likelihood'. \n\n5) I would like to know how long the typical texts you are dealing with are (this is useful info when looking at the effectiveness of detectors)\n\n6) What decoding strategies (temp, top-p, top-k etc.) are you using when you generate text from machine. Note,  until recently huggingface enabled top-k=50 by default so unless this has been explicitly controlled for, you may be using top-k generation.\n\n7) You write 'We find that our approach preserves the meaning of\nthe original text (similarity >0.85)'. Is this a fair conclusion? I have no idea how to interpret it, but note that it is worse than some of the alternative methods. \n\n8) I found it really hard to understand what is meant by 'sample size' in your article. I think the answer is that, when sample size is n, you compute for example the fast detect score of n separate human texts and then compute the mean fast detect score of those n human texts, compared to the mean fast detect score of n machine texts. This is quite an unusual setup, it's not necessarily bad because it's more or less equivalent to just increasing the number of tokens to help detectors, but because we're so primed to imagine you submitting one text for analysis I think you need to go to more effort to stress exactly what you're doing.\n\n9) I think the caption to figure 2 is confusing. When you say detection performance (lower is better), you mean that higher is better for the detector, but lower is better for your paraphraser. I would write something like 'a lower score indicates better performance of the rewriter'.\n\n10) Could you mention text lengths. There is a known problem with the way that Fast-Detect normalises by passage length which can artificially inflate or depress it's performance, see for example TempTest: Local Normalization Distortion and the Detection of Machine-generated Text section 8.2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8RgjDMlNLb", "forum": "YCMDpRL7SP", "replyto": "YCMDpRL7SP", "signatures": ["ICLR.cc/2026/Conference/Submission21775/Reviewer_ZPpT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21775/Reviewer_ZPpT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906928631, "cdate": 1761906928631, "tmdate": 1762941926332, "mdate": 1762941926332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors of this paper explore the importance of text style in the detection of AI-generated texts and study the effect of the attacks on detectors via direct optimization of the generating model. \nThey propose a method of paraphrasing machine-generated texts that ``enforces'' the style of a human author on them to avoid detection; they further strengthen it with direct preference optimization against a detector. Authors conduct a series of experiments on data from 3 domains against various detectors and compare the proposed approach against competitive solutions. Finally they perform analysis of the learned writing style representations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The results of the experiments conducted by the authors demonstrate that their approach outperforms other paraphrasing attacks at avoiding automatic detection.  \n\n- The experiments cover rather underexplored setup of detection AI-generated content by analyzing multiple text samples from the same author.\n\n- Limitations of the proposed method are well outlined."}, "weaknesses": {"value": "- The fact that existing methods for AI-generated texts are very brittle to paraphrasing is quite well-known; this paper does not provide any novelty in this regard. Essentially, the main contribution lies in applying existing methods to transfer the style of a human author to machine generated texts.\n\n- Main claims of the paper contradict each other: \"although LLMs can be optimized to defeat machine-text detectors, they remain identifiable by detectors that avail of writing style and that moreover, optimizing against such detectors does not reduce their performance\", and, at the same time, results of the experiments in Section 5 clearly show that optimizing against writing style-avail detectors via the proposed method clearly reduces their performance. \n\n- ``Since StyleDetect requires exemplars from the machine class, we provide 100 examples from the unoptimized LLM model`` (section 2, lines 139-140). By this procedure, StyleDetect is initialized with the information on the generating model and the text domain, while other methods (FastDetectGPT, Binoculars) are not; this makes the comparison somewhat unfair and makes the results in Table 1 less informative.\n\n- Only a limited range of data domains was covered by the evaluation (free-form texts from Amazon/Reddit/...). More diverse domains, like Wikipedia articles, poetry, news, or some Q&A would help better analyze the generability of the proposed method.\n\n- Certain important implementation details are omitted or moved to the appendices; it is somewhat difficult to grasp the exact pipeline of the proposed method. In particular, what is the detector used in the direct preference optimization of the proposed method (line 185)?"}, "questions": {"value": "Questions/Suggestions:\n\n- Please, clarify \"N\" in the axis labels on the plots.\n\n- In Tables 3 and 4 could you please provide standard deviations for the average semantic similarities?\n\n- Binoculars is a general method that leverages representations from a pair of related models to detect AI-generated texts. Which model pair/pairs were used in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SD8cyeWwPn", "forum": "YCMDpRL7SP", "replyto": "YCMDpRL7SP", "signatures": ["ICLR.cc/2026/Conference/Submission21775/Reviewer_fRsS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21775/Reviewer_fRsS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913949646, "cdate": 1761913949646, "tmdate": 1762941925919, "mdate": 1762941925919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether language models (LLMs) can evade AI-text detectors and whether such evasion eliminates all detectable signals. When LLMs are explicitly optimized against detectors (e.g., FastDetectGPT), their outputs retain identifiable stylistic patterns that distinguish them from human writing. Authors later introduce a style-aware paraphraser, trained via supervised fine-tuning and DPO, which mimics human writing styles to avoid being detected. The method improves over prior paraphrasing and prompting attacks, producing text that is nearly indistinguishable from human-written samples when judged individually, though detectability re-emerges when multiple samples from the same source are aggregated. The study concludes that style-based features remain robust against current optimization attacks, and reliable AI-text detection may only be feasible when based on multiple documents rather than single samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides strong empirical evidence that style-based detection methods remain robust even after preference tuning aimed at fooling detectors.\n - The style transfer experiments are valuable and demonstrate practical potential for improving the naturalness and “humanization” of LLM-generated text, particularly for applications like chatbots.\n - Overall, the work offers a well-executed empirical study that contributes both to improving the human-likeness of generated text and to understanding how to detect it more reliably."}, "weaknesses": {"value": "The weaknesses are minor, mostly text-based.\n- There could be a more in-depth intro to the style-based detectors, for example, explaining what is the style feature space.\n- The paper could benefit from studying larger LLMs, say up to 32B, without fine-tuning, to give a perspective on the limits of the style-based detection applicability."}, "questions": {"value": "- Do these results generalize to larger LLMs, say GPT-4o?\n- Do you perform any human evaluation to confirm that reduced detectability corresponds to genuinely more human-like writing, rather than merely adversarial surface changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "49lAIg1h6q", "forum": "YCMDpRL7SP", "replyto": "YCMDpRL7SP", "signatures": ["ICLR.cc/2026/Conference/Submission21775/Reviewer_naRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21775/Reviewer_naRM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21775/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936652403, "cdate": 1761936652403, "tmdate": 1762941925634, "mdate": 1762941925634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}