{"id": "OiDYdsovrf", "number": 14812, "cdate": 1758244274611, "mdate": 1759897348078, "content": {"title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning", "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective.\n\nWe first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of examples from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.", "tldr": "A principled approach to selecting diverse and query-specific examples for in-context learning, improving prediction accuracy over standard nearest-neighbor methods in data-scarce classification tasks", "keywords": ["In-context Learning", "Linear Model", "Kernels", "Sherman-Morrison", "Submodular"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/432bd58c6b28dd1c19a9db539012e59b6e8059d0.pdf", "supplementary_material": "/attachment/c06582f71b3f7fe0b8236d5927a1f3d24ea2d455.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical problem of exemplar selection for in-context learning (ICL), a core challenge given the limited context windows of large language models (LLMs). The key limitation of existing methods (e.g., kNN-based KATE, DPPs) is their poor generalization in high-dimensional embedding spaces and insufficient diversity in selected examples. The authors model the LLM as a linear function in the embedding space and aim to select a subset of examples that minimizes prediction error on a given query. The key contributions are:\n1. A theoretical formulation of ICL exemplar selection as a subset selection problem with a surrogate objective that is approximately submodular.\n2. A greedy algorithm with an approximation guarantee, leveraging the Sherman–Morrison formula for efficient updates.\n3. Kernelization of the approach to handle nonlinear relationships via RKHS, without explicit high-dimensional mappings.\n4. A diversity regularizer inspired by optimal experimental design to improve generalization.\n5. Extensive experiments across multiple classification tasks and models, showing consistent improvements over strong baselines like DPP, BM25, and dense retrieval."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality：\nKITE’s most original contribution is the query-specific optimization framing—unlike prior work (e.g., KATE, DPPs) that optimizes for distribution-wide generalization, KITE targets error reduction for individual user queries. This aligns with real-world ICL, where performance on a single query (not a dataset) determines utility. \n2. Quality：\nThe theoretical analysis is rigorous: the derivation of the surrogate objective from prediction error bounds (via Chernoff inequalities) is logically sound. Empirically, the paper uses diverse datasets (sentiment, QA, entailment) and LLMs, demonstrating KITE’s robustness. The kernel ablation provides actionable insights, adding depth to the results.\n3. Clarity：\nThe paper avoids excessive jargon in key sections (e.g., the Introduction clearly explains ICL and exemplar selection). Algorithm 1 is well-documented, with explicit steps for initializing and computing relevance/diversity scores. \n4. Significance:\nKITE addresses a high-impact problem: ICL performance is highly sensitive to exemplar choice, and existing methods struggle in high-dimensional spaces. By providing a computationally efficient, unsupervised framework with theoretical guarantees, KITE enables practical deployment of ICL in data-scarce domains."}, "weaknesses": {"value": "* The KITE framework is fundamentally order-agnostic, as it is designed as a subset selection algorithm based on optimizing set functions. This theoretical formulation completely overlooks the well-documented sensitivity of large language models to exemplar ordering, a crucial factor in ICL performance that this method cannot account for.\n\n* The algorithm's generalizability is constrained by its reliance on a fixed, pre-compiled example bank. As a frozen and non-trainable retriever, it cannot update its parameters for a new task (like fine-tuning) or adapt by collecting new data (like online learning). Its applicability seems restricted to scenarios where a high-quality example bank for the target task is already available."}, "questions": {"value": "* The KITE framework's entire theoretical foundation is built on modeling the LLM's behavior as a (kernelized) linear regressor. This model inherently assumes a smooth performance landscape, where embedding-space proximity correlates with performance similarity. However, what if the true LLM performance landscape is highly discontinuous, where minor variations in exemplar embeddings can lead to drastic and non-linear shifts in task accuracy?\n\n* You motivate your work by highlighting the \"curse of dimensionality\" as a key drawback of kNN-based methods, a flaw typically associated with Euclidean distance. However, your \"Dense\" baseline (KATE) presumably uses **Cosine Similarity**, as is standard in NLP to mitigate this very issue. Could you clarify which metric your \"Dense\" baseline actually uses? Furthermore, could you provide an ablation study comparing KITE against both a Euclidean-kNN baseline and a Cosine-kNN baseline, to demonstrate that the choice of retrieval algorithm (KITE vs. kNN), and not just the distance metric, is the true source of performance gain?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "12zuuNGbK1", "forum": "OiDYdsovrf", "replyto": "OiDYdsovrf", "signatures": ["ICLR.cc/2026/Conference/Submission14812/Reviewer_tnXn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14812/Reviewer_tnXn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834804591, "cdate": 1761834804591, "tmdate": 1762925162948, "mdate": 1762925162948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents KITE, a theoretically grounded framework for exemplar selection in in-context learning (ICL). The method formulates exemplar retrieval as a query-specific optimization problem under an information-theoretic view. By modeling the LLM as approximately linear in its input embeddings, the authors derive a submodular surrogate objective and design a greedy selection algorithm with provable approximation guarantees. KITE extends this linear model via the kernel trick, allowing flexible, nonlinear feature mappings. Experiments on six few-shot classification benchmarks (SST-2/5, CMSQA, MRPC, QNLI, HellaSwag) and three relatively small-scale LLMs (GPT-Neo-2.7B, Qwen-1.5B, Llama-3B) show consistent improvements over common retrieval baselines such as BM25, dense embeddings, and DPP retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation and writing are clear, easy to follow, and grounded in theory. \n2. This paper provides a clear mathematical formulation with proofs of approximate submodularity and corresponding approximation guarantees.\n 3. The core idea is fresh and well-motivated. It goes beyond the usual heuristic retrieval methods and gives a principled way to think about the problem."}, "weaknesses": {"value": "1. Empirical scope is limited. Evaluation is restricted to classification tasks, which aligns with the linear assumption but leaves uncertainty about generalization to generation or structured prediction tasks (e.g., summarization, reasoning). Extending experiments beyond classification would strengthen the practical relevance.\n2. Dense baseline clarity. Table 1 refers to a “Dense” retriever, but the implementation details (e.g., whether it uses bi-encoder fine-tuning or off-the-shelf embeddings) are under-specified. This is critical since modern dense retrievers (e5, bge) are highly competitive for ICL retrieval.\n3. Model scale. The largest tested model is 3B parameters. Given recent trends, evaluating at least one 7B-scale model (e.g., Llama-3-8B or Qwen-2.5-7B) would better demonstrate scalability."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "C1MBDaRWyz", "forum": "OiDYdsovrf", "replyto": "OiDYdsovrf", "signatures": ["ICLR.cc/2026/Conference/Submission14812/Reviewer_Tuhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14812/Reviewer_Tuhf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920274301, "cdate": 1761920274301, "tmdate": 1762925162281, "mdate": 1762925162281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KITE, a theoretically grounded framework for exemplar selection in in-context learning (ICL). Specifically, the paper first proposes LITE, which models the ICL exemplar selection problem as a query-specific optimization problem under a linear model assumption, and derives a surrogate objective that is approximately submodular. This enables a greedy selection algorithm. To incorporate more complex non-linear scenarios, the paper further introduced KITE, which is a kernelized version of the algorithm. Empirically, KITE outperforms several existing retrieval baselines on six classification datasets across multiple LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and of mathematical rigor. \n2. Under the linear model assumption, the methodology and analysis are reasonable and well justified. \n3. Empirically, the proposed algorithm. KITE outperforms multiple traditional baselines."}, "weaknesses": {"value": "1. The paper lacks novelty and excitement. Theoretically, the analysis is straightforward in a standard linear model context. Methodologically, using an approximate submodular function for in-context learning has already been explored in prior works, such as [1]. \n2. Missing baselines. The paper only compares with very classic retrieval baselines, but most of the recent advances for exemplar selection in LLMs are missing, such as [1] [2]. In addition, the paper should compare with the most basic baseline where no example but only a task description is provided. \n3. The focus is primarily on classification tasks developed for classic NLP, and many LLMs can already perform very well on them. Nowadays, there are many different evaluation approaches, such as instruction following and reasoning. \n4. The proposed approach is model-agnostic, meaning that the exemplars are selected only using BERT without knowing the target LLM. \n\n[1] Wu, Z., Lin, X., Dai, Z., Hu, W., Shu, Y., Ng, S. K., ... & Low, B. K. H. (2024). Prompt optimization with EASE? efficient ordering-aware automated selection of exemplars. Advances in Neural Information Processing Systems, 37, 122706-122740.\n\n[2] Nguyen, T., & Wong, E. (2023). In-context example selection with influences. arXiv preprint arXiv:2302.11042."}, "questions": {"value": "1. Computational scalability. The proposed approach requires computing the in-context examples for each test query, which is computationally expensive. What’s the computational overhead introduced by this method over existing baselines? \n2. How does the model perform if the embedding is changed from BERT to other embeddings, such as E5 or RoBERTa?\n3. It is known that small LLMs exhibit weaker in-context learning ability. Will the method work for larger models?\n4. It is unclear how the approach works for multi-class classification. Does the author have one kernel for all classes, or each class has its own kernel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xqaHdb8L4P", "forum": "OiDYdsovrf", "replyto": "OiDYdsovrf", "signatures": ["ICLR.cc/2026/Conference/Submission14812/Reviewer_zKVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14812/Reviewer_zKVx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088630408, "cdate": 1762088630408, "tmdate": 1762925161329, "mdate": 1762925161329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}