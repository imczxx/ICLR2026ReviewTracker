{"id": "ojOkkp22Fa", "number": 3052, "cdate": 1757322978256, "mdate": 1763025254083, "content": {"title": "Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs", "abstract": "Video Large Language Models (VideoLLMs) face a critical bottleneck: increasing the number of input frames to capture fine-grained temporal detail leads to prohibitive computational costs and performance degradation from long context lengths. We introduce Video Parallel Scaling (VPS), an inference-time method that expands a model's perceptual bandwidth without increasing its context window. VPS operates by running multiple parallel inference streams, each processing a unique, disjoint subset of the video's frames. By aggregating the output probabilities from these complementary streams, VPS integrates a richer set of visual information than is possible with a single pass. We theoretically show that this approach effectively contracts the Chinchilla scaling law by leveraging uncorrelated visual evidence, thereby improving performance without additional training. Extensive experiments across various model architectures and scales (2B-32B) on benchmarks such as Video-MME and EventHallusion demonstrate that VPS consistently and significantly improves performance. It scales more favorably than other parallel alternatives (e.g. Self-consistency) and is complementary to other decoding strategies, offering a memory-efficient and robust framework for enhancing the temporal reasoning capabilities of VideoLLMs.", "tldr": "Running several parallel inference streams on disjoint frame subsets and aggregating their output probabilities results in enhanced video understanding capabilities", "keywords": ["VideoLLM", "Scaling Law", "Parallel Scaling", "Multimodal Understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/27b0c4825b740517796cf22bce736b9eeb7cafbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an inference-time strategy, Video Parallel Scaling (VPS), which expands a model’s perceptual bandwidth without increasing its context window. VPS spawns multiple independent streams, each sampling a different subset of frames to provide complementary visual cues, and then aggregates their predictions afterward to generate the final answer. The approach is validated on two benchmarks using multiple backbones, demonstrating the effectiveness of VPS compared to the baseline. The paper also includes ablation studies on different decoding and frame sampling strategies, further highlighting the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well organized and easy to read. It presents an interesting inference-time scaling strategy that boosts the performance of long-video understanding while maintaining constant memory usage, which could benefit real world on-device video understanding applications. The paper also includes solid theoretical analysis of parallel stream scaling and provides comprehensive experiments demonstrating the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The claim that increasing the number of input frames to capture fine-grained temporal details leads to performance degradation (L031–033, L066–L067) seems somewhat counterintuitive and inconsistent with empirical experience, such as the results shown in Table 9 of [1]. It would be helpful if the authors could provide references or related works supporting this claim. The results of Qwen2.5-VL and InternVL-3 in Figure 4 suggest that performance generally improves as more frames are included. Although Gemma-3 shows a performance drop when increasing the number of frames from 8 to 16, I doubt whether Gemma-3 is an appropriate backend model for long-video understanding tasks, given its much lower performance compared to Qwen2.5-VL and InternVL-3. It might also be useful to experiment with stronger backend models such as GLM-4.1/4.5 [2]\n2.\tSince each stream is uniformly sampled and equally weighted, I have doubts about the effectiveness of this approach when the streams contain very different content, leading to divergent probabilities and inconsistent results. For example, if some streams are sampled from background while others focus on the foreground, the model may produce inconsistent attention and descriptions when interpreting foreground object actions.\n3. It would be beneficial to include evaluations on more challenging benchmarks, such as Long Video Bench, EgoSchema, and MLVU, which involve complex temporal reasoning, to better demonstrate the effectiveness of VPS\n\n[1] Hu, K., Gao, F., Nie, X., Zhou, P., Tran, S., Neiman, T., Wang, L., Shah, M., Hamid, R., Yin, B. and Chilimbi, T., 2025. M-LLM based video frame selection for efficient video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 13702-13712).\n\n[2] V Team, Hong, W. and Yu, W., 2025. Glm-4.5 v and glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006."}, "questions": {"value": "1. How does the VPS workflow operate? Does it run sequentially on a single GPU for each stream, or in parallel across multiple GPUs? If it is the former, could the authors provide any insights into the overall latency performance compared to the baseline?\n2. The baseline results of Qwen2.5-VL-7B and InternVL3-8B reported in Table 1 are much lower than those obtained under the same 32 input frames setting in other works. Could the authors clarify the root cause? For reference, Qwen2.5-VL achieves 60.8 and InternVL-3 achieves 65.6 in Table 1 of [1], while Qwen2.5-VL reports 61.1 in Table 2 of [2] for the Video-MME w/o sub.\n3. How does performance scale as the number of streams $J$ increases?\n\n[1] Zou, Y., Jin, S., Deng, A., Zhao, Y., Wang, J. and Chen, C., 2025. AIR: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering. arXiv preprint arXiv:2510.04428.\n\n[2] Sun, G., Singhal, A., Uzkent, B., Shah, M., Chen, C. and Kessler, G., 2025. From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding. arXiv preprint arXiv:2510.02262."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5RsSBomKbT", "forum": "ojOkkp22Fa", "replyto": "ojOkkp22Fa", "signatures": ["ICLR.cc/2026/Conference/Submission3052/Reviewer_mnjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3052/Reviewer_mnjg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809528417, "cdate": 1761809528417, "tmdate": 1762916529537, "mdate": 1762916529537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Tf8sM7JMbU", "forum": "ojOkkp22Fa", "replyto": "ojOkkp22Fa", "signatures": ["ICLR.cc/2026/Conference/Submission3052/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3052/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763025253404, "cdate": 1763025253404, "tmdate": 1763025253404, "mdate": 1763025253404, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free inference framework Video Parallel Scaling (VPS) that can improve long-video understanding for video large language models. VPS uses a simple but effective mechanims to run multiple parallel inference streams, with each seeing a different subset of the video frames, and then aggregate the token-level distributions into the final output. The authors develops the insights with an analysis extending the Chinchilla framework. They show that the combining low-correlation streams can reduce prediction variances and yields faster effective loss contraction. \n\nCompared to prior work, e.g. ParScale, which similarly use multi-stream scaling through leaned prefix tuning, VPS shares similar insights and achieves similar gain but without additional training. Their experiments on Video-MM and EventHallucination across multiple model families demonstrate the consistent improvements in accuracy and reduce the hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is technical sound and simple. The authors demonstrate a simple idea backed up the theory and can be effectively applied to practice easily. The method does not require training, which makes a lot more practical to be used in real world practices. \n\nThe authors provide an theoretical analysis to Chinchilla's scaling law with bias-variance decomposition, which is a sound interpretable explanation for why the aggregation of frame subsets can lead improvement and what assumption we need to consider in practice. \n\nThe proposed model is model agnostic. Its performance gain evaluated under different family models are consistent and impressive. In addition, the method may also be able to be applied to other inference techniques."}, "weaknesses": {"value": "We may need to scrutinize some assumptions in various real world scenarios, which I am not sure understand in the practical use case well. For videos recorded as different FPS, I am not entirely sure what sampling strategy with combination of strides and frame subsets would be sufficient ensure the framesets are distinct enough while ensuring good phase offsets between the streams. For high frequency video, I would imagine the each frameset would be sufficiently close to each other and leads to high correlations while for low frequency video, choosing large number for frameset would lead to undersampling in each stream and lead to information loss. \n\nIn the related work, the authors mention ParScale as the most related work but I don't see a discussion of the evaluation for the two."}, "questions": {"value": "Besides the questions I raised in the weakness part, I am also interested to know when will VPS fail exactly. The authors only vaguely mentions a few improvement of frame sampling in the future work, but I will be more interested to know when will the current strategy fail and why. \n\nFrom the figure, it seems the increase of J will very likely to better performance but we have not seen when it may saturate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qNJ3jDyKmR", "forum": "ojOkkp22Fa", "replyto": "ojOkkp22Fa", "signatures": ["ICLR.cc/2026/Conference/Submission3052/Reviewer_CokG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3052/Reviewer_CokG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975775673, "cdate": 1761975775673, "tmdate": 1762916529345, "mdate": 1762916529345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Video Parallel Scaling (VPS), a training-free inference-time method to enhance the long clip coverage of Video Large Language Models (VideoLLMs). Specifically, instead of feeding longer sequences or more frames into the model, VPS feeds multiple disjoint subsets of video frames in parallel inference streams, which avoids quadratic attention costs. The predictions from these streams are then aggregated in token probabilities to predict an ensemble output. Experiments across several VideoLLMs show improvements on multiple benchmarks over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The paper targets an important problem in video understanding: how to efficiently handle long video coverage without quadratic cost. And it proposes a simple and practical inference-time remedy.\n3. Training-free design: VPS can be readily applied to existing VideoLLMs without additional training, which increases its usability.\n3. The proposed method is evaluated across a range of model sizes (2B–32B) and benchmarks, and the results are good across architectures and scales."}, "weaknesses": {"value": "1. Lack of sufficient novelty: The main method of this paper is running multiple inference streams on different frame subsets and ensembling their probabilities. The idea is quite straightforward and very similar to existing ensemble methods like self-consistency, best-of-N. The difference is just assigning different sampled frames to each stream, which is a small heuristic variation rather than a fundamentally new paradigm. \n2. Lack of strong baselines: The experimental section does not include quantitative comparisons against stronger or conceptually similar baselines that also target frame sampling problems, such as SlowFast-LLava and some attention-based methods (though many works are mentioned in related work).\n3. The weighting strategy when ensembling different streams is too straightforward - uniform weighting is used. This is quite an important component in the proposed method, but it is not studied."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xZAPPRTyk6", "forum": "ojOkkp22Fa", "replyto": "ojOkkp22Fa", "signatures": ["ICLR.cc/2026/Conference/Submission3052/Reviewer_rwEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3052/Reviewer_rwEJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073777242, "cdate": 1762073777242, "tmdate": 1762916529164, "mdate": 1762916529164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to extract parallel streams (token sequences) of a long video and average the response logits to enhance the video understanding performance. The author provides a theoretical analysis on why this parallel scaling brings improvement. The method is verified across multiple baseline models with various sizes and the video parallel scaling does improve the baseline model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is easy-to-use. Although not stated by the author, it can be inferred that this method is orthogonal to other long video understanding methods, such as token reduction/merging, context parallelism, and speculative decoding.\n2. The evaluation is robust in terms of baseline models. The author comprehensively evaluated Qwen2.5-VL, InternVL3, and Gemma3, each with different sizes."}, "weaknesses": {"value": "1. This paper **lack details**, especially for method and experiments. Here are some examples: 1) How is the parallelism achieved? By using more GPUs or parallel on the batch dimension? 2) What's your video sampling FPS? 3) What's the configuration of VPS for each experiment? How many frames are extracted for each stream? Is it always 4 as stated in line 203? 4) In Table2, what does the \"Nframe\" refer to? Is it the K?\n2. This paper **lacks evaluation on across different benchmarks**. There are many other long video benchmarks that are commonly used for evaluation, such as LongVideo, VideoChat, and NextQA. The main point here is that evaluating more benchmarks is more convincing and can show the robustness of VSP.\n3. The paper **lacks comparison**. The only comparison is made against the baseline model. However, given that there have been many long video understanding works so far, such as Video-XL[1], Streaming Long Video Understanding[2], and LongVLM[3], the author is supposed to provide a comprehensive comparison against some SOTA baselines. Or, if VSP is orthogonal to previous methods, the author can also show that previous SOTAs can be easily combined with VSP and achieve a even better result. Without any discussion, it's not convincing that VSP is effective enough.\n4. The paper claims to target at long video understanding, but experiments on scaling are not convincing. For example, in Figure 2, 3, and 4, the maximum number of frames is 16/64. Again, since the sampling fps is not specified, I have no clue about the actual video length. However, since long video understanding now usually refers to near hour videos, the scaling in this paper seems a bit trivial. \n\n\n[1] Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding, in CVPR 2025\n[2] Streaming Long Video Understanding with Large Language Models, in NeurIPS 2024\n[3] LongVLM: Efficient Long Video Understanding via Large Language Models, in ECCV 2024"}, "questions": {"value": "1. Configurations related questions are in W1\n2. Since the output logits come from the weighted sum of multiple streams, do multiple streams actually amplify the answer token logits? Specifically, are there any case studies of the logit distribution before and after the VSP?\n3. Is there any efficiency improvement such as reduced latency or FLOPs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3J1mhS7Fmd", "forum": "ojOkkp22Fa", "replyto": "ojOkkp22Fa", "signatures": ["ICLR.cc/2026/Conference/Submission3052/Reviewer_SuLw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3052/Reviewer_SuLw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3052/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762121086618, "cdate": 1762121086618, "tmdate": 1762916529025, "mdate": 1762916529025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}