{"id": "kvEGQWemVx", "number": 711, "cdate": 1756775875566, "mdate": 1763695416440, "content": {"title": "PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning", "abstract": "Visual In-Context Learning (VICL) uses input-output image pairs, referred to as in-context pairs (or examples), as prompts alongside query images to guide models in performing diverse vision tasks. However, VICL often suffers from over-reliance on a single in-context pair, which can lead to biased and unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual In-Context Learning (PANICL), a general training-free framework that mitigates this issue by leveraging multiple in-context pairs. PANICL smooths assignment scores across pairs, reducing bias without requiring additional training. Extensive experiments on a variety of tasks, including foreground segmentation, single object detection, colorization, multi-object segmentation, and keypoint detection, demonstrate consistent improvements over strong baselines. Moreover, PANICL exhibits strong robustness to domain shifts, including dataset-level shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and generalizes well to other VICL models such as SegGPT, Painter, and LVM, highlighting its versatility and broad applicability.", "tldr": "", "keywords": ["Visual In-Context Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8730925e5cd399124c47cef9bed9cd387e405d9b.pdf", "supplementary_material": "/attachment/8468fb580116134842f3ece8c97be6f9bd8d0b49.zip"}, "replies": [{"content": {"summary": {"value": "The paper tackles the task of Visual In-Context Learning (VICL). VICL often suffers from over-reliance on a single in-context pair. To mitigate this issue, the paper introduces PANICL, a training-free method that smooths token-level assignment scores across multiple retrieved visual examples (prompts) using Jensen–Shannon divergence-based weighting. Finally, the authors test the approach on three downstream tasks and several benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important problem and is fairly well written. The idea is simple and based on the experimental results seems quite effective. It may have broad applications on multiple down-stream tasks."}, "weaknesses": {"value": "My main concern is related to the novelty which seems a bit minimal. While it's an interesting idea, I feel like the proposed smoothing is a minor change on existing ensemble or multi-example strategies. I also feel that the “Model-agnostic” claim feels a bit exaggerated since the overall performance depends heavily on the architecture and retrieval quality even though technically can be applied to different models. However, as the experiments show, in some cases the performance improvements are small and often within noise margins.\n\nMinor\n* Tab 1 - wrong bolded value for m=6, 7 for fold-1"}, "questions": {"value": "Can you please share some more insights on why the performance degrades with increased m? Since the performance peaks with few samples, I believe that choosing those samples is very important so I think the performance might be very tightly coupled with the performance of the retrieval system. So, can you please elaborate on how different retrieval performance affects the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fH1CrzLnLa", "forum": "kvEGQWemVx", "replyto": "kvEGQWemVx", "signatures": ["ICLR.cc/2026/Conference/Submission711/Reviewer_rqcE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission711/Reviewer_rqcE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776985681, "cdate": 1761776985681, "tmdate": 1762915587847, "mdate": 1762915587847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank our reviewers for their time and effort in providing thorough and constructive feedback on our paper.\n***\nWe are encouraged to see that our reviewers recognize this work as a novel, effective, and broadly applicable inference-time strategy for mitigating single-prompt reliance in Visual In-Context Learning (VICL):\n- Reviewers ``Ujk5``, ``guEZ``, and ``rqcE`` acknowledge the empirical strength of our approach, noting that it achieves *\"consistent and significant performance gains,\"* *\"consistent gains across multiple VICL backbones\"* and is *\"quite effective.\"*\n- Reviewers ``Ujk5``, ``guEZ``, and ``rqcE`` commend our method's *\"robustness to domain shifts,\"* ability to generalize to *\"multiple VICL backbones,\"* and the *\"clean results\"* on several benchmarks.\n- Reviewers ``Ujk5`` and ``guEZ`` highlight the novelty of our specific design, stating that PANICL is *\"new in VICL,\"* *\"uniquely applies\"* smoothing at the *\"patch/token-level\"* and offers a *\"more granular way to debias the model\"* compared to prior methods.\n- Reviewers ``guEZ`` and ``rqcE`` appreciate the clarity of our work, describing the paper as *\"well-executed,\"* *\"practical,\"* and *\"fairly well written.\"*\n- Reviewers ``Ujk5`` and ``rqcE`` think our method addresses a *\"well-known and critical weakness in VICL,\"* and an *\"important problem\"* with an *\"intuitive, logical, and interesting\"* idea.\n- Reviewers ``Ujk5`` and ``guEZ`` highlight a *\"major strength\"* in PANICL’s *\"training-free\"* nature, noting that it requires *\"no fine-tuning\"* or *\"model changes,\"* making it an efficient, *\"plug-and-play improvement.\"*\n\n***\n\nWe highlight our contributions as follows:\n- PANICL explicitly introduces a novel and training-free **patch-level, neighbor-aware assignment score smoothing** mechanism to mitigate the over-reliance on single in-context pairs. By constructing an effective **dynamic prompt pool** that combines the query with retrieved examples, PANICL integrates multi-prompt features at the **an intermediate assignment score** level using similarity-based (JS divergence and $\\ell_2$ distance) weights. As a training-free and general framework, PANICL demonstrates consistent improvements across diverse VICL models (e.g., MAE-VQGAN, SegGPT, Painter, and LVM) and downstream tasks (e.g., segmentation, detection, colorization) without fine-tuning, retraining, or model changes.\n***\nAs suggested by the reviewers, we revised our manuscript (changes highlighted in orange) as follows:\n\n***Additional Experiments and Analyses:***\n\n- Added a comparison with a static prompt pool in Section 4.7 (Table 9) to demonstrate the necessity of our dynamic retrieval strategy, as suggested by Reviewer ``Ujk5``.\n- Added an ablation study on spatial retrieval window sizes in Section 4.7 (Table 8) to validate the effectiveness of per-patch retrieval, as suggested by Reviewer ``Ujk5``.\n- Added JS divergence results on SegGPT in Appendix Section A (Table 17) to confirm the applicability of our assignment score smoothing mechanism on pixel-space models, as suggested by Reviewer ``Ujk5``.\n- Added detailed results of per-class retrieval quality vs. performance in Appendix Section A (Tables 15 & 16), addressing concerns from Reviewer ``rqcE``.\n- Added an analysis explaining the performance saturation/degradation with increased prompt numbers ($m$) in Section 4.2, as suggested by Reviewers ``guEZ`` and ``rqcE``.\n\n***Textual Revisions and Clarifications:***\n\n- Expanded Sections 1 and 2 to explicitly clarify the differences between PANICL and prior methods (e.g., Large Canvas, Query Voting, and Feature Ensemble), addressing the novelty concerns from Reviewers ``Ujk5`` and ``rqcE``.\n- Added a discussion on the theoretical perspective of PANICL in Appendix Section F, as suggested by Reviewer ``guEZ``.\n- Added a *\"Practical Guidance\"* paragraph in Appendix Section A to summarize recommended default settings, as suggested by Reviewer ``Ujk5``.\n- Revised the claim from *\"model-agnostic\"* to *\"general\"* in Section 1, as suggested by Reviewer ``rqcE``.\n- Corrected the boldface entries in Table 1, as pointed out by Reviewer ``rqcE``."}}, "id": "5FdeOwBn3n", "forum": "kvEGQWemVx", "replyto": "kvEGQWemVx", "signatures": ["ICLR.cc/2026/Conference/Submission711/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission711/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission711/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763693010279, "cdate": 1763693010279, "tmdate": 1763693010279, "mdate": 1763693010279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PANICL introduces a technically simple but novel inference-time strategy to mitigate over-reliance on single prompts in Visual In-Context Learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While smoothing and retrieval are known techniques, PANICL uniquely applies them at the patch/token-level on decoder logits, using score pooling within the VQGAN codebook space. \n\n- PANICL requires no fine-tuning, no retraining, and no model changes, yet shows consistent gains across multiple VICL backbones.\n\n- Broad, well-controlled experiments on several tasks, e.g. segmentation, detection, colorisation, etc."}, "weaknesses": {"value": "- Some components are adapted from broader ML ideas, but their particular application and integration here is new in VICL.\n\n- Lack of theoretical analysis of the method.\n\n- compared to single in-context learning, there is computational overhead from multiple prompt passes.\n\nWhile the idea is intuitive and shows solid empirical improvements across several VICL backbones and tasks, the methodological contribution is relatively limited, as it largely repurposes known techniques like score averaging and prompt retrieval without substantial technical innovation. The paper is well-executed and practical, but the novelty and depth may not fully meet the bar for a strong ICLR contribution. I find myself between a 4 and a 6, but leaning generous due to the broad applicability and clean results."}, "questions": {"value": "- Why does PANICL in Table 1 show diminishing returns as the number of prompts m increases from 2 to 7? Shouldn’t more prompts consistently improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vxyC0CsDGR", "forum": "kvEGQWemVx", "replyto": "kvEGQWemVx", "signatures": ["ICLR.cc/2026/Conference/Submission711/Reviewer_guEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission711/Reviewer_guEZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844387333, "cdate": 1761844387333, "tmdate": 1762915587655, "mdate": 1762915587655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PANICL, a method to fix a key problem in Visual In-Context Learning, or VICL, where models over-rely on a single example prompt, leading to biased and unstable predictions. PANICL is a training-free framework that mitigates this issue by using multiple in-context pairs. For a given query image, it first retrieves several similar example pairs from a support set. It then processes each of these pairs through the VICL model to create a prompt pool of their patch-level assignment scores, which are the model's internal predictions. The query image is also processed to obtain its own initial scores. The core idea is assignment score smoothing, where PANICL finds the k nearest neighbors for the query's scores from the prompt pool using JS divergence and then calculates a weighted average. This smoothing step debiases the query's prediction by incorporating information from multiple examples. The authors show this method consistently improves performance on various tasks like segmentation, detection, and colorization, which is robust to domain shifts and can be generalized to other VICL models such as SegGPT, Painter, and LVM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper directly addresses a well-known and critical weakness in VICL. The over-reliance on a single prompt is a key bottleneck, and PANICL's approach of using multiple examples is an intuitive and logical solution.\n2. Instead of ensembling intermediate features or ensembling final predictions, PANICL operates at the assignment score level. This allows it to correct biases at a patch-by-patch level before the final prediction is rendered, offering a more granular way to debias the model.\n3. A major strength is that PANICL is a \"training-free\" framework. It does not require any model fine-tuning, making it an efficient, plug-and-play improvement.\n4. The method shows consistent and significant performance gains over strong baselines, including both single-example methods and other multi-example methods. The gains are not just in accuracy but also in robustness to domain shifts, a critical factor for real-world deployment."}, "weaknesses": {"value": "1. The method's effectiveness is highly dependent on the quality of the retrieved in-context pairs. The authors admit this in their failure case analysis (Section E, Figure 12), showing that when the retriever provides examples that are globally similar but semantically misaligned (e.g., different object scale or position), PANICL's performance can degrade significantly, sometimes even below the single-prompt baseline.\n2. The method appears to be heuristic and parameter sensitive, which may require careful tuning for new tasks or datasets.\n3. While it improves SegGPT, the gain on foreground segmentation is minor. Similarly, for keypoint detection with Painter, the gain is minimal.\n4. Although the method is intriguing, its novelty is limited. I feel this paper is leaning toward a strong engineering method instead of proposing a new framework."}, "questions": {"value": "1. Your method requires m additional forward passes to build the prompt pool. Have you explored cheaper alternatives, such as using a static pool of generic assignment scores rather than a dynamic pool retrieved for every query?\n2. You show that patch-level k-NN (Table 8, \"PANICL\") is better than retrieving neighbors from all patches (Table 8, \"All-patch\"). This observation is interesting, does forcing the model to find neighbors only in the same spatial location (e.g., top-left) prevent it from getting confused by patches from other parts of the image?\n3. You use JS Divergence to compare assignment score distributions. This works for token-based models like MAE-VQGAN and LVM. But for pixel-space models like SegGPT and Painter, you switch to l2 distance on intermediate features. Does this mean your core \"assignment score smoothing\" idea doesn't apply to these models, and you are instead performing a type of \"feature ensemble,\" similar to the baseline?\n\nI am willing to reconsider my evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8E5EM73PDE", "forum": "kvEGQWemVx", "replyto": "kvEGQWemVx", "signatures": ["ICLR.cc/2026/Conference/Submission711/Reviewer_Ujk5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission711/Reviewer_Ujk5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762283295208, "cdate": 1762283295208, "tmdate": 1762915587376, "mdate": 1762915587376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}