{"id": "7K1kXowjK1", "number": 9845, "cdate": 1758143546544, "mdate": 1759897692236, "content": {"title": "Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models", "abstract": "Although large language models (LLMs) have transformed AI, they still make mistakes and can explore unproductive reasoning paths. Self-correction capability is essential for deploying LLMs in safety-critical applications. We uncover a systematic failure: LLMs cannot correct errors in their own outputs while successfully correcting identical errors from external sources - a limitation we term the Self-Correction Blind Spot. To study this phenomenon, we introduce Self-Correction Bench, an evaluation framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 open-source non-reasoning models, we find an average 64.5% blind spot rate. We provide multiple lines of evidence suggesting this limitation may be influenced by training data: human demonstrations rarely include error-correction sequences (favoring error-free responses), whereas reinforcement learning (RL) trained models learn error correction via outcome feedback. Remarkably, appending a minimal \"Wait\" prompt activates a 89.3% reduction in blind spots, suggesting dormant capabilities that require triggering. Our work highlights a critical limitation potentially influenced by training distribution and offers a practical approach to enhance LLM reliability and trustworthiness - vital for safety-critical domains.", "tldr": "Large language models struggle to self-correct errors in their own outputs, with a 64.5% blind spot rate, but a simple \"Wait\" prompt reduces this by 89.3%, revealing trainable limitations.", "keywords": ["benchmark", "self correction", "large language model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88f5fe0a55d775f3518f372e93a214ff7542f90d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors target a known limitation in LLMs, self-correction, wherein it is harder to find errors in their own outputs, compared to finding similar / identical errors from external sources. They term this limitation Self-Correction Blind Spot, and introduce Self-Correction Bench. They argue that since human demonstrations rarely include self-correction, models only learn error-correction through outcome feedback during the RL phase. They also mention that self-correction is dormant in language models, and appending “Wait” improves the models ability to overcome blind spots."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Studying self reflection is a very relevant problem for LLMs.\n- The experiment of injecting incorrect responses in the output of the model and removing the stop token, and comparing it with the injection in the prompt is interesting.\n- The results in Appendix C, indicating that the results don’t change with temperature is intriguing.\n- Reducing the possibility for knowledge gaps by using easy, medium, and hard datasets is a good progressive difficulty metric. This isolates the self-correction capability from confounding factors.\n- The author’s conclusion that reasoning models are better than non reasoning models on self correction is intuitive. Also the fact that correction markers can reduce the gap.\n- Self-Correction Bench can be a useful benchmarks for identifying reflection capabilities in models."}, "weaknesses": {"value": "- Even though the conclusions about corrective markers is interesting, the paper does not introduce any theoretical / technical contributions.\n- The benchmarks are very limited, to SCLI5, GSM8K and MATH. It would be interesting to see this across different tasks like code, logic, and multimodal reasoning, etc.\n- Overall, while this dataset has potential to be a useful benchmark for studying LLM reflection capabilities, the paper lacks strong technical backing and explanations, and is also very limited in the domains it is targeting."}, "questions": {"value": "- It is unclear how authors are measuring Self Correction scores before and after committing to an answer. Is it using an LLM as a judge? More explanation is required here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1Hhwv1DREs", "forum": "7K1kXowjK1", "replyto": "7K1kXowjK1", "signatures": ["ICLR.cc/2026/Conference/Submission9845/Reviewer_DyNA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9845/Reviewer_DyNA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761433924313, "cdate": 1761433924313, "tmdate": 1762921320552, "mdate": 1762921320552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (1/2)"}, "comment": {"value": "We sincerely thank the reviewers for their precious time and constructive feedback. We are very encouraged by the feedback that our work studies a very relevant area in LLM today and recognition of Self-correction Bench’s potential to be a useful benchmark that could benefit researchers and practitioners.\n\nAcross different reviews, we identified two common themes (1) Lack of depth and contribution (2) on-policy error is not used, which are addressed below.\n\n## (1) Lack of Depth of Analysis and Contribution (EoiX, 57Bc, DyNA)\n### Systematic Understanding: Errors are Absent from SFT Data\nIn section 3.1, we start our framework with “Error-free generation is a **special** case of this framework - not the only path to correctness” through the marginalization of error states, which leads to the training recipe insight: including error and self-correction data as stated in section 7. \n\nWe note that SFT models trained mostly on error-free demonstration due to human preference. We demonstrate this in section 6.3 where correction markers are very limited in SFT dataset. In particular, the 90th percentile of correction marker frequency of OpenAssistant (both demonstrated and rated by human) is 1. In stark contrast, at least 99% of Mixture-of-Thoughts (distilled from DeepSeek-R1) and OpenThought3-1.2M (distilled from QwQ-32B) contains at least 1 correction marker.\n\nAlthough popular open source model Llama3.1 and Qwen2.5 does not open source their SFT data, from their technical report, Llama3.1 considers samples that obtain maximum score as rated by Llama 3 as high quality while Qwen3 only retains responses deemed flawless by all scoring systems. It suggests responses with error and self-correction are filtered away.\n\n### Causal Intervention\nInspired by the presence of correction markers when error is present in user prompt, we conducted a causal intervention study in section 6.1 to show that by appending “Wait”, “But” and “However” can mitigate self-correction blind spot. So naturally if we can make LLM predict correction markers as the next token upon seeing the error, we could activate self-correction. We can achieve so by finetuning LLM with error and self-correction data, such as data generated by reasoning models.\n\n### Can SFT with error and self-correction data mitigate the blind spot?\n[Additional experiment] We **additionally** evaluate DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Llama-70B. Both models are fine-tuned respectively from Llama-3.1-8B and Llama-3.3-70B-Instruct with 800k SFT data (600k reasoning trajectories generated by DeepSeek-R1 and 200k training samples unrelated to reasoning) for 2 epochs. While we cannot isolate error-correction data as the sole causal factor (as the training includes other components), the dominant intervention is the addition of error-correction sequences. The results show dramatic blind spot reductions, providing strong causal evidence that fine-tuning with error and self-correction data substantially mitigates this limitation.\n\n| Model                                              | Comparison Model |  Dataset   |   Accuracy Internal Error |  Accuracy External Error |   Blind Spot |   Blind Spot (Comparison Model) |\n|:---------------------------------------------------|:----------|:----------|-----------------:|----------------:|-------------:|--------------------------:|\n| DeepSeek-R1-Distill-Llama-8B  | Llama-3.1-8B-Instruct*| scli5     |           0.4615 |          0.9056 |       0.4903 |                    0.8289 |\n| | | gsm8k     |           0.5994 |          0.6923 |       0.1342 |                    0.9708 |\n| | | prm800k   |           0.4888 |          0.4911 |       0.0045 |                    0.8889 |\n| DeepSeek-R1-Distill-Llama-70B | Llama-3.3-70B-Instruct | scli5     |           0.8497 |          0.958  |       0.1131 |                    0.4577 |\n| || gsm8k     |           0.9162 |          0.8888 |      -0.0308 |                    0.6885 |\n| | | prm800k   |           0.6562 |          0.625  |      -0.05   |                    0.3168 |\n*Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Llama-8B share the same base model Llama-3.1-8B\n\nWe note the blind spot in SCLI5 is larger than GSM8k_SC and PRM800K_SC, which may be due to lower realism of error injected. \n\nTo conclude, our paper provides systematic evidence for understanding why the self-correction blind spot exists: we demonstrate strong correlations between the absence of error-correction sequences in training data and blind spot magnitude. Critically, we provide causal evidence that fine-tuning with error and self-correction data substantially reduces the blind spot. This leads to practical training recipe change - to include error and self-correction data during model training, echoing our insight through the marginalisation of error states.\n\n**Action**: We commit to emphasize our contribution and append the result of additional experiments in our manuscript.\n\n(to continue...)"}}, "id": "QQXml99XVd", "forum": "7K1kXowjK1", "replyto": "7K1kXowjK1", "signatures": ["ICLR.cc/2026/Conference/Submission9845/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9845/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9845/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763688164478, "cdate": 1763688164478, "tmdate": 1763688164478, "mdate": 1763688164478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the self-correction capability of large-language models (LLMs) by: 1) creating a dataset of self-correction prompts, 2) showing empirically that some LLMs do poorly on self-correction prompts (referred to as a self-correction \"blind spot\" in the paper), and 3) showing empirically that appending the self-correction prompts with certain conditioning tokens (such as the word \"wait\") can improve the self-correction performance of models that did poorly initially. A variety of non-reasoning and reasoning models were studied, across a number of model families (such as Qwen, Gemma, and DeepSeek). The self-correction prompts are all based on math problems, ranging from the trivial (SCLI5) to quite difficult."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Dataset contribution specifically focused on self-correction (or \"correction\", see weakness 1) by asking models to determine whether the given answer or reasoning in the prompt needs to be corrected, and indeed make the correction. This allows for the analysis in the paper to examine the correction ability of both reasoning and non-reasoning LLMs.\n\n2. Empirical comparative analysis of the correction capability of reasoning and non-reasoning LLMs reveals that one strong differentiating factor that specifically makes reasoning models better in performance is their ability to recognize when an error is present (in a given answer or reasoning trace), and make the necessary correction. Whereas, as this paper shows, non-reasoning models cannot.\n\n3. Empirical evidence for inserting conditioning tokens such as \"Wait\" that alerts a non-reasoning model to the fact that the given answer may not be correct can improve their correction performance, making up the gap to or even exceeding the performance of reasoning models. It is quite clear that even non-reasoning models can give the correct answer when they are made aware that correction is required."}, "weaknesses": {"value": "1. The dataset does not strictly study self-correction. By the construction description, all 3 sub-datasets were generated by inserting wrong answers or reasoning traces into a given prompt from standard datasets (with possibly a short sequence of model output) using off-the-self closed-source models (such as GPT 4.1). Since the incorrect answers were not generated by the models-under-test (open-source models with fewer parameters), it is unknown how likely they are under each model's own sampling distributions, this is not self-correction (i.e., the model may not even sample the incorrect solution). A proper self-correction dataset would sample strictly on-policy from each model and filter for incorrect answers. \n\nThe datasets are still valuable as benchmarks for correcting an off-policy incorrect answer or reasoning trace, but they do not study \"self\" correction.\n\n2. No actionable conclusions for reasoning models. The main result and contribution of the paper is that non-reasoning models have a self-correction blindspot, which could be corrected by conditioning on certain tokens. Moreover, reasoning models have small or no self-correction blindspot. As reasoning models are the stronger of these two types at solving math problems, this weakens the significance of the key findings of the work (i.e., Why not just use reasoning models?)."}, "questions": {"value": "Please show more examples of the self-correction dataset, e.g., 1 for each subset, and for when there are reasoning steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uQADvE2pEA", "forum": "7K1kXowjK1", "replyto": "7K1kXowjK1", "signatures": ["ICLR.cc/2026/Conference/Submission9845/Reviewer_ZE7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9845/Reviewer_ZE7F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620173922, "cdate": 1761620173922, "tmdate": 1762921320117, "mdate": 1762921320117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies LLMs' self-correction blind spot and introduces Self-Correction Bench, an evaluation framework to measure this phenomenon through controlled error injection."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. For originality and significance, I think the newly introduced evaluation framework is a nice addition to existing works.\n\n2. The paper is generally understandable, but I need more explanation/analysis described in the weaknesses and questions sections below."}, "weaknesses": {"value": "1. The paper discusses self-correction blind spot on LLMs, but closed-source LLMs are unfortunately not studied. Although it is explained as \"close-source models lack support for fine-grained control of prefix inject critical for our methodology\" in line 236, I do not think this would stop you from studying closed-source models. You may want to analyze their reasoning chains directly and compare model outputs side-by-side. Otherwise, the findings of this paper is very limited.\n\n2. The analysis offered appears to be coarse and is not very comprehensive. You mentioned that \"this limitation may be influenced by training data\", but I feel there can be much more reasons. For example, an LLM may tend to have relatively lower uncertainty on its own tokens than on tokens generated by another model.\n\n3. I do not see any quality check on the benchmark introduced by the authors in Section 4, so I am not very confident with its overall quality. A common practice of similar papers is to leverage human evaluation to do some manual verification.\n\n4.  A “Wait” prompt is not a novel idea. For example, [1] explores an ensemble of critics and the model's own feedback. This is a 2-year-old work. You may find more papers talking about something similar to your prompt.\n\n[1] \"N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics\""}, "questions": {"value": "1. Is it possible to design some customized methods to analyze closed-source LLMs?\n\n2. Do you think there are any other reasons of self-correction blind spot besides training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i6YGJz2eB3", "forum": "7K1kXowjK1", "replyto": "7K1kXowjK1", "signatures": ["ICLR.cc/2026/Conference/Submission9845/Reviewer_57Bc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9845/Reviewer_57Bc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984704799, "cdate": 1761984704799, "tmdate": 1762921319515, "mdate": 1762921319515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on isolating a failure in modern LLMs called \"Self-correction Blind Spot\", where the model can correct an error if attributed to an external factor (e.g. a user or a tool), but fails to do so for itself. It does so by quantifying the phenomena on 3 benchmarks of increasing complexity (SCLI5, GSM8K-SC, and PRM800K-SC).\n\nIt then points out that the lack of correction is due to a lack of activation (or intent to fix), rather than the inability to fix (e.g. not having the knowledge). Minimal intervention is design succesfully, namely by using the word \"Wait.\""}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There are several strengths to the manuscript:\n1. Rather clear isolation of the problem and extensive empirical analysis of multiple models on the behavior. The experiment design seems reasonable, where exact same sequence of tokens is presented but with different attribution to observe differing model behavior.\n2. The introduction of a toy benchmark (SCLI5) to explain clearly the phenomena, followed by studying on real-world benchmarks.\n3. Identifying an intervention, e.g. the word \"wait\"."}, "weaknesses": {"value": "I believe the overall contribution is somewhat naive and does not go sufficiently in-depth in understanding the mechanics of the model behavior. The weaknesses I would like to hear the author's opinion are:\n\n1. The entire work is prompt engineering - from the detection to the solution. The authors acknowledge this, but I find it necessary to go a step further and point out some training recipe changes that mitigates this to some degree. Naive fine-tuning with the word 'wait' may not work, while asking all researchers / developers to start using 'wait' seems also non-ideal.\n\n2. An off-policy (in the RL sense) / biased setup by design. The generation of the high-quality incorrect reasoning traces by another model and then feeding them into a current model making them looking like it's own output is somewhat flawed by design. The generated tokens would have to be from the model itself (e.g. it's weights) or otherwise the sequence would always be off-policy of the model state (and hence can't be used for claiming the blind spot)."}, "questions": {"value": "Please refer to weaknesses. \n\nFurthermore, can the code be released? Unfortunately the repository at 4openscience is returning an error for all files, despite some directory structure existing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rXtqBlaUaq", "forum": "7K1kXowjK1", "replyto": "7K1kXowjK1", "signatures": ["ICLR.cc/2026/Conference/Submission9845/Reviewer_EoiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9845/Reviewer_EoiX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120517398, "cdate": 1762120517398, "tmdate": 1762921319209, "mdate": 1762921319209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}