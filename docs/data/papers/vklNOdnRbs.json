{"id": "vklNOdnRbs", "number": 19163, "cdate": 1758294020868, "mdate": 1759897055509, "content": {"title": "COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents", "abstract": "This paper considers a contextual bandit problem involving multiple agents, where a learner sequentially observes the contexts and the agents' reported arms, and then selects the arm that maximizes the system's overall reward. Existing work in contextual bandits assumes that agents always truthfully report their arms, which is unrealistic in many real-life applications. For instance, consider an online platform with multiple sellers; some sellers may misrepresent product features to gain an advantage, such as having the platform preferentially recommend their products to its users. To address this challenge, we propose an algorithm, COBRA, for contextual bandit problems involving strategic agents that disincentivize their strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee. Our experimental results also validate our theoretical results and the different performance aspects of COBRA.", "tldr": "This paper proposes a contextual bandit algorithm that prevents strategic agents from misreporting while having approximate incentive compatibility and a sub-linear regret guarantee.", "keywords": ["Contextual bandits", "Strategic agents", "Incentive compatibility", "Regret minimization", "Mechanism design"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95357300dd3d6b7386bfaa50571e6c03024be53f.pdf", "supplementary_material": "/attachment/e48846115a8bafc46f7edbd7c238434371066746.zip"}, "replies": [{"content": {"summary": {"value": "**Disclaimer**: I reviewed this same paper for NeurIPS 2025. The main reason that this paper gets rejected from NeurIPS 2025 is because there seem some fundamental flaws in its approaches and proofs and some explicit proof bugs were also identified (see my detailed description below). There was a very long technical discussion during the rebuttal phase of NeurIPS 2025, but ultimately the discussion did not resolve the technical issues/bugs identified by the reviewers there. The ICLR 2026 draft did a bit re-writing, e.g., by re-phrasing some assumptions into a “LOOM-compatible” definition, but the rephrasing did not seem to change the earlier issues and just make it appear differently. \n\nThis paper develops a new algorithm for strategic contextual bandit problem where each arm is a strategic agent and can misreport their feature vector, though cannot manipulate reward. This problem was proposed and studied by Kleine Buening et al. NeurIPS'24, but the current paper develops a different algorithm inspired by VCG. Specifically, the paper proposes to use all agents' information, excluding arm a's, to estimate a reward function for agent a (in linear bandit case, this means estimate the theta parameter for a). The paper argues that this, coupled with an optimistic-pessimistic inequality called LOOM condition, can help to induce incentive compatibility."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Interesting research question."}, "weaknesses": {"value": "The paper’s writing has some clarity issues. For instance, in the definition of “LOOM-compatible contextual bandit algorithm”, it is unclear why “any contextual bandit algorithm” would always have an “estimated function”. We know UCB has a natural estimate function of its reward, but if we do Thomas Sampling, do we call that updated distribution mean as the “estimate”? What’s the definition of “estimated function”? How do you ensure that “any” bandit algorithm will have an “estimated function”? \n\nWhat is the difference between Theorem 1 and Theorem 4?  \n\nThe above are some smaller technical issues I found, but **my biggest concern is that there seem major flaws in the proposed approach, as well as in the technical proofs,** which makes the paper not acceptable. In particular, the paper's approach of using all other agent's information to estimate a parameter and reward for arm a could not work for linear contextual bandit. The no regret proof for standard linear context bandit, which this paper adapts from, crucially depends on a \"self-normalization lemma\", which roughly says if we use data along some direction d to estimate parameter theta, then in the future when we see feature roughly along direction d, our reward estimation error will be small. However, this paper's approach of using other arms' information to estimate i's arm reward basically excluded the possibility of such \"self-normalization\".\n\n**A potential counterexample**  Let us consider the standard multi-armed bandit as a special case of linear contextual bandit. This is a special case because we can view x_{a,t} as always along the canonical basis e_a direction, with rescaling factor as agent’s private information at each round t. In such special cases, LinUCB algorithm effectively becomes estimating the parameter theta_a (a'th dimension of theta) along direction e_a separately and independently using arm a's previous information. This can be verified by plugging in x_{a,t} as a scaled version of e_a into LinUCB descriptions in Line 300 to 306. However, using this paper's proposed approach, as explained in LOOM Condition in Equation (2) and algorithm description of COBRA, COBRA will use information orthogonal to direction e_a to decide whether e_a’s report is correct or not. This simply is impossible since the information from any arm other than a will not be useful for estimating \\theta_a. Concretely in Equation (2), the LCB for an arm a, estimated used all other arms orthogonal to a, will always much larger than the UCB on the right hand side, and will never change over the execution of the algorithm. \n\nThe paper’s technical writing is a bit difficult to follow, but I tried to check the detailed proof to see where concrete proof bugs may appear in the paper. I think one concrete bug is the following, inherently due to the issue I mentioned above. \n\n In the proof of Theorem 4 in the Appendix, I think the inequality from Line 1102 to Line 1105 is incorrect. The paper claims that a Det/Det term is always upper bounded by a constant C. This is not correct. In the standard multi-armed bandit special case where arms are canonic basis vectors, this term is lower bounded by the number of times arm a is pulled, which can be \\Omega(T) and cannot be upper bounded by a universal constant C. That makes the regret linear.\n\nAnother thing that strikes me as odd when diving deep into the proof of Theorem 4 is that I don’t see where they use the LOOM strategy of banning a misreported arm at all. If the proof were correct, they could arrive at the regret bound in equation (18) without ever using an argument about the NE and the regret bound would hold for ANY arm strategy?! I might be missing something here, but otherwise this obviously can’t be true (or their assumptions are overly strong so that there is generally no point in doing mechanism design in their setting)."}, "questions": {"value": "Feel free to respond to my concerns about proof bugs above. I am happy to be convinced and revise my ratings, but currently I do not see a way to overcome the intrinsic issue of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I am a bit concerned that the paper was re-submitted without any major changes to the originally flawed proofs."}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5kHeY3vAlT", "forum": "vklNOdnRbs", "replyto": "vklNOdnRbs", "signatures": ["ICLR.cc/2026/Conference/Submission19163/Reviewer_hcLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19163/Reviewer_hcLK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664678362, "cdate": 1761664678362, "tmdate": 1762931172501, "mdate": 1762931172501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers a contextual bandit problem with strategic arns who misreport contexts to maximize their number of selections over $T$ rounds. The authors propose a VCG-inspired algorithm that uses optimistic and pessimistic estimates of each agent’s reward to eliminate misreporting agents. It is shown that under the proposed algorithm truthfulness is an approximate Nash equilibrium and the authors establish sublinear regret bounds given that the arms play in NE. The authors also provide empirical results on synthetic problem instances to support their theoretical claims."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The studied problem is interesting, and the intersection of online regret minimization under uncertainty with mechanism design is a challenging but interesting domain. \n2. The authors motivate the model and the work well."}, "weaknesses": {"value": "I have concerns about the correctness of Theorem 4. Firstly, there a various typos in Appendix B.2.2 which make the proof of Theorem 4 hard to read. For example, Lemma 4 has various typos and it is unclear  what $a_a$ is, what the $x$ in the definition of $UCB_{t, -a} (x_{s, a_a})$ in line 996 is, etc. \n\nFollowing this, I am confused about line 1067 in the proof. You plug-in Lemma 4, but what is the $x$ in $\\lVert x \\rVert_{V_{t, -a}^{-1}}$. As far as I can tell, this $x$ should be $x_{t,a}$. However, then bounding $\\sum_{t=1}^T \\lVert x_{t,a} \\rVert_{V_{t,-a}^{-1}}$ is hard and I believe that you shouldn't be able to bound this the way you do. Consider the case where the context vectors of arm $a$ are linearly independent of the context vectors of all other arms $a'$ (for all time steps). Then, there is no good bound for the sum over $\\lVert x_{t,a} \\rVert_{V_{t,-a}^{-1}}$. In other words, if you are only using every other arm's data, your exploration bonueses can stay arbitrarily large. I have difficulties following the reasoning on the top of page 21 (partly due to typos), but I suspect that the issue is there. \n\nAnother sign that something is possibly wrong is that you arrive at your regret bound (18) on page 21 without ever using LOOM or any guarantee about the arm strategies. What would be the point of mechanism design when you can get the same regret guarantee without using that the arms play a NE under COBRA? \n\nOther weaknesses: \n- The presentation could be improved, and particularly Section 4 is difficult to read as it is extremely dense. Related to the issues in the presentation, you introduce COBRA(TS) and then present your theoretical guarantees as if they also hold for COBRA(TS). As far as I can tell you only prove results for COBRA(UCB); see e.g., proof of Theorem 2. It is often quite unclear what algorithm you are referring to when you just write COBRA.\n\nI'd be happy to increase my score if my concerns can be resolved."}, "questions": {"value": "1. In line 131, you say that the previous work's [1] method may not be practical when the true reward function is unknown. Could you please be precise about what you mean by this? The algorithm in [1] appears to be designed specifically for the case where the true reward function is unknown. \n2. Line 336: It should be COBRA(TS) instead of COBRA(UCB). \n\n[1] Thomas Kleine Buening, Aadirupa Saha, Christos Dimitrakakis, Haifeng Xu; Strategic Linear Contextual Bandits, NeurIPS 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a5D79D9erH", "forum": "vklNOdnRbs", "replyto": "vklNOdnRbs", "signatures": ["ICLR.cc/2026/Conference/Submission19163/Reviewer_VEb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19163/Reviewer_VEb8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912594034, "cdate": 1761912594034, "tmdate": 1762931172148, "mdate": 1762931172148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers incentive-copmatible contextual bandits, where one learner is interacting with strategic agents controlling the arms' contexts.\n\nAfter observing the context $c_t$, the learner selects $a_t$, resulting in a stochastic reward with mean $f(x_{t,a_t})$ where $x_{t,a} = \\phi(c_t, a) \\in R^d$ is the feature vector associated with $c_t$.\nIt is unclear, but I assume that neither $c_t$ or $\\phi$ are known to the agent.\n\nA linear version of this problem was previously studied by Buening et al, but the techniques used in this paper are substantially different.\n\nThe main idea is to test if agents are over-reporting and exlude them. The threat of exclusion provides the incentive. For that reason they define the following estimates:  $f_t$, which is based on everybody's estimate, and $f_{t,-a}$ which excludes $a$'s reports.\n\nThe algorithm *could* be combined with a large class of context bandit algorithms. Of course, this requires carefully looking whether various conditions are satisfied. This is captured in Assumption 1 where they asume that:\n  (a) $f(x) \\leq UCB_{t,a}(x)$,\n  (b) $UCB_t(x_{t,a}) \\leq UCB_{t,-a}(x_{t,a})$\nwhere\n$UCB_{t,a}(x) = f_{t,a}(x) + \\epsilon_t$\n$UCB_{t,-a}(x) =  f_{t,-a}(x) + \\epsilon_{t,-a}$,\nwith $\\epsilon$ denoting appropriate confidence intervals around estimates.\n\nThe assumption is examined page 30. Case 2, where one agent over-reports, is the basic scenario. But given, how central this assumption is to the proofs, this is really an inadequate proof for me. I guess the $-a$ interval should be wider than the $a$ one, but this should be proven more rigorously. \n\nThe other quantity is $LCB$, which structured differently:\n- $LCB_{t,-a} = f_{t,-a} - \\epsilon_{t,-a}$.\n- $LCB^{(x)}_{t,a} = \\sum_{s=1, a_s = a}^t LCB_{t,-a}(x_{s,a_s})$,\ni.e. it is the sum of lower bounds through other agents reports, where $x$ now defines a sequence of rewards (confusingly).\n\nThis is complemented with $UCB^{(y)}$, an upper bound on the total reward which does *not* use the contexts. Perhaps this notation is a bit counterintuitive. In any case the  LOOM condition can be summarised as follows:\n\nIf the lower bound, constructed through context-dependent estimates using agents reports, exceeds the upper bound constructed only through observed rewards from agent $a$, then $a$ is over-reporting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The main idea is nice and intuitive\n+ The results are an improvement and extension of previous work"}, "weaknesses": {"value": "- The presentation could be improved. I spent more time to understand what is going than I should have had to. \n- Assumption 1 can be quite restrictive. It should at least be cleanly proven for some special cases, but the discussion in Appendix D is inadequate. Intuitively, it should hold for the linear case based on the reasoning given."}, "questions": {"value": "? The paper also says it is inspired by VCG, but the connection is not\nspelled out. I assume it is because $f_{t,-a}$ uses the reports of the\nremaining agents."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C0uYLSem3b", "forum": "vklNOdnRbs", "replyto": "vklNOdnRbs", "signatures": ["ICLR.cc/2026/Conference/Submission19163/Reviewer_BSfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19163/Reviewer_BSfu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063569697, "cdate": 1762063569697, "tmdate": 1762931171773, "mdate": 1762931171773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses contextual bandit problems where strategic agents may misreport their features to maximize their selection probability. The authors propose COBRA (Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents), which uses a Leave-One-Out-based Mechanism (LOOM) inspired by VCG mechanisms to detect and disincentivize misreporting. The authors propose that reporting arm features truthfully is the best/dominant strategy for the agents which is achieved via LOOM and COBRA. Experimental results validate their theoretical results of sublinear regret."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Misreporting in contextual bandits is clearly motivated (food delivery/marketplace settings) and is an interesting/practically relevant problem.\n2) LOOM provides a theoretically grounded, drop-in mechanism compatible with common contextual bandit algorithms.\n3) The proofs in the appendix are well structured."}, "weaknesses": {"value": "1) Only synthetic evaluation. Considering that real world applications are well motivated and reiterated throughout the paper it would have been nice to see some experiments on real world data. \n2) The scale of the synthetic experiments is quite small as well. Having just 5 agents with only one of the agents over reporting (line 465) is a bit unsatisfactory in terms of scale. It would be better to see experiments on a larger scale particularly larger $d$ and $N$ than those found in the appendix, and with more than one over reporter.\n3) A limitations sections would benefit the paper. The term dominant strategy is only applicable for the case of over reporting and can be misleading considering the paper assumes that collusion and under reporting cannot happen (which are strong assumptions in their own right). The failure conditions of LOOM should also be mentioned in the main paper instead of being scattered around in remarks and in the appendix. \n4) The complexity of LOOM + COBRA should be discussed, especially in the case of agents with multiple arms. It would be great if the authors could go into more detail about how their algorithm scales to agents with multiple arms instead of the short paragraph in the appendix to determine application feasibility."}, "questions": {"value": "Questions: \n1) Modern applications including the ones motivated in the paper routinely handle thousand to millions of items at once. What is the computational complexity of LOOM+COBRA? Considering this and the dependence of the regret on $\\sqrt{N}$, do you believe it is applicable to this scenario ?\n2)  How does the complexity change if each agent picks multiple arms?\n3) Would it possible to add a semi synthetic or small real world experiments at a larger scale $(d \\ge 50, N\\ge 50$, more than one over reporter) than those in the appendix\n4) See 3) from Weaknesses.\n\nMinor Corrections/Presentation issues: \nThe paper has few typos and grammatical errors   \n- line 103: non-leaner --> non-linear \n- line 113,118: study an --> studied an \n- line 239: \"Note that the assumptions underlying contextual bandit algorithms need to satisfy in our setting\" (should be rephrased) \n- line 336: \"propose a TS-based variant COBRA(UCB)\" should be COBRA(TS) \n- line 373: drive --> derive and quite a few more in the appendix. \n \nAdditionally it would be great if the authors could state/explain the notation used in Table 1 of section C of the appendix, in the paper itself. I would also prefer if the core equations were set on their own display lines rather than wrapping across text lines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xGXlH5F3qb", "forum": "vklNOdnRbs", "replyto": "vklNOdnRbs", "signatures": ["ICLR.cc/2026/Conference/Submission19163/Reviewer_acmw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19163/Reviewer_acmw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762363396640, "cdate": 1762363396640, "tmdate": 1762931171391, "mdate": 1762931171391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}