{"id": "Ek1CztdpAB", "number": 13361, "cdate": 1758216970468, "mdate": 1763656738044, "content": {"title": "CELF: A Self-Supervised Multimodal Framework for Concept-Based Interpretability", "abstract": "Traditional XAI techniques in computer vision, such as heatmaps and saliency maps, highlight input regions that influence model predictions. However, they often lack precision and may introduce bias. Concept-based XAI approaches, such as concept bottleneck models or textual explanations of latent neurons, aim to provide more interpretable representations but typically rely on human-annotated concept sets, which are scarce in specialized domains. Moreover, Large Language Models (LLMs) used for automatic concept generation can hallucinate, reducing reliability and trust. To address these challenges, we propose the Concept Extraction and Learning Framework (CELF), a self-supervised multimodal method for extracting and retrieving human-interpretable concepts from vision-language data without manual annotations. CELF integrates attention-guided keyphrase extraction with contrastive learning, and applies a graph-guided refinement stage to promote semantic consistency. For controlled evaluation, we introduce C-MNIST, a configurable dataset generator with ground truth concepts. Experiments on C-MNIST, Visual Genome, and CUB demonstrate that CELF outperforms prior baselines in concept extraction and improves multi-label classification performance on C-MNIST.", "tldr": "Conceptual-MNIST, a vision-language dataset generator with controllable ground truth concepts, is proposed to support the evaluation of CELF, a novel framework that extracts concepts through self-supervised multimodal learning.", "keywords": ["Explainable Artificial Intelligence", "Concept-based XAI", "Large Language Models", "Concept Extraction", "Vision-Language Models", "Self-Supervised Learning", "Multimodal Representation Learning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4242faa3df5f00817178a2efb168942c296321ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CELF, a self-supervised framework to extract human-interpretable concepts from image-caption pairs. The goal is to address the unreliability of concepts generated by LLMs for XAI. The method involves using an LLM to generate pseudo-labels from captions, which then supervise a fine-tuned CLIP model to perform concept extraction. The authors also contribute a new synthetic benchmark C-MNIST and an evaluation metric SCS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core technical idea is a novel self-supervised approach that fine-tunes a CLIP-based concept extractor using virtual labels generated by LLM from free-text captions. This approach is valid and has been adequately tested.\n\n- The lack of standardized benchmarks is a known problem in concept-based XAI. The introduction of the C-MNIST dataset and the SCS metric are valuable contributions to the field."}, "weaknesses": {"value": "- The combination of LLM and CLIP was said to be unexplored, but it seems that Kim et al.[1] addressed the problem similarly in their paper.\n\n- As this is a paper about XAI, it seems that there should be results to see if it can actually be utilized in places where XAI is needed, such as the medical domain.\n\n[1] Kim, Injae, et al. \"Concept bottleneck with visual concept filtering for explainable medical image classification.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023."}, "questions": {"value": "- Please explicitly discuss the relationship between CELF and Kim et al [1].\n\n- It would be nice to see some qualitative results to show how useful it is in reality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ToqGYZBqzx", "forum": "Ek1CztdpAB", "replyto": "Ek1CztdpAB", "signatures": ["ICLR.cc/2026/Conference/Submission13361/Reviewer_cYEd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13361/Reviewer_cYEd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661851936, "cdate": 1761661851936, "tmdate": 1762924007424, "mdate": 1762924007424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the CELF framework, a self-supervised approach that utilizes a Large Language Model (LLM) for pseudo-label generation and fine-tunes a CLIP model with an additional keyphrase extraction task to achieve concept extraction and concept retrieval. The authors also introduce a new synthetic dataset called C-MNIST, consisting of transformed digits, and a novel evaluation metric, Semantic Cosine Similarity (SCS).\n\nHowever, the overall technical originality of the proposed dataset, method, and metric does not offer sufficient novelty or value to warrant acceptance at a top-tier venue at this time. Furthermore, significant concerns regarding the experimental setup and methodological consistency need to be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The proposed dataset C-MNIST is well-configurable and might be useful for controlled benchmarking."}, "weaknesses": {"value": "1. The paper fails to adequately discuss existing work and establish a fair comparative context.\n\n   **Synthetic Datasets**: The discussion regarding synthetic, concept-labeled datasets is incomplete. The paper misses important relevant work, such as the synthetic dataset SUB (ICCV 2025) proposed in [1], which provides 38,400 images combining a base class with a single target attribute modification. A comparative discussion or experimental inclusion of this work is necessary.\n\n   **VLM/LLM Concept Extraction**: The authors limit their comparison primarily to FALCON, XCB, LF-CBM, and Grad-ECLIP, overlooking several crucial and recent contributions in the area of VLM and LLM-based concept extraction and interpretability, such as [2], [3], [4], and [5]. This omission makes the claimed novelty and state-of-the-art comparison questionable.\n\n2. The paper contains a serious internal inconsistency. The authors claim that LLM-based methods suffer from hallucination and VLM-based methods require large-scale pre-training knowledge. Yet, the proposed CELF framework requires fine-tuning CLIP, which itself introduces a substantial computational burden (as noted by the 36 GB GPU usage) and carries the inherent risk of disrupting CLIP's robust, well-generalized semantic space which it was designed to leverage. \n\n3. Given that CELF includes an expensive, domain-specific fine-tuning step on CLIP, comparing its performance against off-the-shelf, non-fine-tuned CLIP-based baselines (such as the implementations of GRAD-ECLIP or LF-CBM methods) presents an unfair comparison.\n\n\n\n[1]  SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions (ICCV 2025)\n\n[2] Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE) (NeurIPS 2024)\n\n[3] V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer (AAAI 2025)\n\n[4] Text-To-Concept (and Back) via Cross-Model Alignment (ICML 2023)\n\n[5] From attribution maps to human-understandable explanations through Concept Relevance Propagation (Nature Machine Intelligence 2023)"}, "questions": {"value": "1. Please supplement the paper with a thorough discussion and fair comparison against the related recent work mentioned in Weakness 1. Please provide a detailed explanation of why the comparison was specifically focused on FALCON, XCB, LF-CBM, and Grad-ECLIP, and why these alone constitute a sufficient and representative set of baselines.\n2. Please explicitly address the critical methodological inconsistency regarding CLIP fine-tuning. Since CELF requires fine-tuning CLIP, how do the authors ensure this process does not negatively influence the generalization ability of the CLIP feature space? Furthermore, please justify the statement that the method \"avoids overfitting risks and retains transparent, easily interpretable attention-based concept scores\" when the core VLM weights are intentionally being altered. This fine-tuning paradigm significantly lowers the practical generality and value of the approach.\n3. I recommend the authors carefully reconsider the fundamental premise and methodology of this paper, as well as the structural organization of the manuscript for better paper quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hxMNfBBXNw", "forum": "Ek1CztdpAB", "replyto": "Ek1CztdpAB", "signatures": ["ICLR.cc/2026/Conference/Submission13361/Reviewer_YU1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13361/Reviewer_YU1w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810461746, "cdate": 1761810461746, "tmdate": 1762924007128, "mdate": 1762924007128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an automated method for extracting human-interpretable concepts from vision-language data for concept-based explainability methods, which has been limited primarily by manual efforts and LLM hallucinations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates superior concept extraction on the proposed C-MNIST dataset"}, "weaknesses": {"value": "1. The motivation for introducing C-MNIST to measure concept-extraction performance, rather than directly evaluating Concept Bottleneck Models (CBMs) [1, 2, 3, 4] with generated concepts, is unclear. Further, the authors also fail to provide any meaningful experiments or examples to demonstrate that the C-MNIST benchmark is superior to directly evaluating CBM approaches in terms of accuracy.\n2. The authors compare Concept Extraction performance with a very limited baseline and lack comparison with concept generation methods that utilize both language and vision data. Recent methods, including [4], have demonstrated improvement in LLM-generated concept sets by grounding with an open-vocabulary detection model.\n3. The paper lacks qualitative results demonstrating the generated concepts on standard datasets, such as CUB and CIFAR, which is crucial for demonstrating the performance of the proposed method in real-world settings.\n4. Formatting issues: Fig. 2, Fig. 9, and Fig.10 have a really small font size\n\n[1] Yang, Yue, et al. \"Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[2] Oikarinen, Tuomas, et al. \"Label-free concept bottleneck models.\" arXiv preprint arXiv:2304.06129 (2023). \n\n[3] Yan, An, et al. \"Learning concise and descriptive attributes for visual recognition.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[4] Srivastava, Divyansh, Ge Yan, and Lily Weng. \"Vlg-cbm: Training concept bottleneck models with vision-language guidance.\" Advances in Neural Information Processing Systems 37 (2024): 79057-79094."}, "questions": {"value": "My primary concerns are the lack of robust evaluation and the missing results on standard datasets. Please see weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mlCcVSF7tu", "forum": "Ek1CztdpAB", "replyto": "Ek1CztdpAB", "signatures": ["ICLR.cc/2026/Conference/Submission13361/Reviewer_CTq3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13361/Reviewer_CTq3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060864719, "cdate": 1762060864719, "tmdate": 1762924006807, "mdate": 1762924006807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a self-supervised approach to extract and align interpretable visual concepts from image–text pairs without manual labels. It introduces CELF, which uses attention-guided keyword extraction and contrastive learning to connect textual concepts with image regions. It also introduces C-MNIST, a configurable dataset providing ground-truth concepts for evaluation. CELF achieves superior concept-extraction accuracy and interpretability compared to existing methods such as FALCON and GRAD-ECLIP."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It proposes a self-supervised multimodal framework for concept extraction without manual labels, combining attention and contrastive learning to advance interpretable vision–language modeling.\n2. It integrates a full pipeline with the CELF framework, the C-MNIST dataset, and the SCS metric, ensuring reproducibility and objective evaluation.\n3. It achieves superior concept-extraction accuracy and interpretability across both synthetic and real-world datasets compared to existing baselines."}, "weaknesses": {"value": "1. The framework is mainly evaluated on datasets with clear visual–text correspondences, and its performance on more complex or abstract domains remains unverified.\n2. CELF relies heavily on CLIP and LLM-generated pseudo-labels, which may introduce bias or noise from those pretrained components."}, "questions": {"value": "How well would CELF generalize to domains where image–text alignment is noisy or abstract, such as medical or scientific data, and can the framework be adapted to handle such cases effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wt74uexa6x", "forum": "Ek1CztdpAB", "replyto": "Ek1CztdpAB", "signatures": ["ICLR.cc/2026/Conference/Submission13361/Reviewer_MR8J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13361/Reviewer_MR8J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13361/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762151525666, "cdate": 1762151525666, "tmdate": 1762924006373, "mdate": 1762924006373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}