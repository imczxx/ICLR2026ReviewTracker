{"id": "1T1SE9xxAB", "number": 11711, "cdate": 1758203263962, "mdate": 1759897559350, "content": {"title": "LLM-Based Social Simulations Require a Boundary", "abstract": "This work argues that large language model (LLM)-based social simulations must establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs' tendency toward an \"average persona\" that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. Our analysis reveals that these simulations are most valuable when focusing on collective patterns rather than individual trajectories, when agent behaviors align with real population averages despite limited variance, and when proper validation methods confirm simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.", "tldr": "We argue that LLM-based social simulations should establish applicable boundaries to enhance its contribution to social science research.", "keywords": ["LLM-based social simulation", "boundaries", "agent heterogeneity", "behavioral alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0b3d19b16e1535f9d085a395aaa336696a8e3d82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents counterarguments against existing LLM-based social simulation work through extensive research and analysis of current approaches. This paper focuses on three key issues: alignment, consistency, and robustness. Building on this foundation, the authors propose what they consider to be the criteria for high-value social simulation, centered around the three aforementioned evaluation dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper focuses on the rapidly evolving field of LLM-based social simulation and conducts extensive literature research to form its conclusions. The findings provide valuable reference and guidance for relevant researchers.\n2. The narrative flow and textual logic throughout the text are coherent and logical, making it easy to read and comprehend."}, "weaknesses": {"value": "1. This paper is purely perspective-based and does not introduce any new techniques or present significant new experiments. It appears to fall outside the scope of the ICLR community and would be better suited for publication in journals or workshops that are more receptive to such work.\n2. It is questionable whether the three evaluation aspects of alignment, consistency, and robustness proposed in this paper possess any particular characteristics specific to LLM-based social simulation. First, as emphasized by the author throughout the article, consistency and robustness are fundamental requirements for any simulation system. And alignment with the real world remains the ultimate goal of any predictive or simulation effort. Therefore, how does this paper demonstrate the innovation of the proposed evaluation system, and is this evaluation system unique to LLM-based social simulations?\n3. Throughout this paper, certain sections cite literature from before the advent of LLMs as supporting material, which appears to fall outside the scope of the discussion presented herein."}, "questions": {"value": "1. Can an evaluation method be designed based on the proposed evaluation framework to evalulate existing LLM-based social simulation work?\n2. Does the evaluation framework proposed in this paper account for the changes introduced by LLM to social simulation?\n3. Based on my understanding of the articles cited by the author, should the title of this paper be “Agent-based Social Simulation Require A Boundary”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "86kYtZAZej", "forum": "1T1SE9xxAB", "replyto": "1T1SE9xxAB", "signatures": ["ICLR.cc/2026/Conference/Submission11711/Reviewer_WPiB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11711/Reviewer_WPiB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760756280456, "cdate": 1760756280456, "tmdate": 1762922756306, "mdate": 1762922756306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses the background of social simulations using computational methods like agent-based modeling (ABM) to understand complex social phenomena, highlighting ABM's limitations in adaptability and representing human-like behaviors. It is motivated by the potential of large language models (LLMs) to create more flexible agents, but argues for establishing boundaries to ensure these simulations reliably contribute to social science by focusing on pattern discovery and hypothesis generation rather than replication or prediction. Key challenges include LLMs' tendency toward an \"average persona\" with low behavioral heterogeneity, misalignment with real-world patterns, inconsistency over time, and lack of robustness under perturbations. The solutions proposed involve heuristic boundaries emphasizing collective patterns over individual trajectories, alignment with population averages despite limited variance, and rigorous validation methods, along with a practical checklist to guide researchers on appropriate scopes and claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The heuristic boundaries and checklist offer actionable guidance for defining simulation scopes. They focus on collective patterns and validation availability. This bridges AI capabilities with social science needs effectively.\n\n2. The work synthesizes extensive literature to position LLM simulations responsibly. It advocates for avoiding overclaims and focusing on beneficial applications. This fosters interdisciplinary collaboration between AI and social science fields."}, "weaknesses": {"value": "1. Empirical demonstrations are absent as the paper relies solely on critiques of existing studies. No original simulations are conducted to illustrate the proposed boundaries. This limits the ability to verify the framework's practical utility.\n\n2. Potential differences across LLM models are not differentiated in the analysis. Assumptions about universal limitations may not hold for all models or future versions. This could lead to overly broad generalizations.\n\n3. Quantitative metrics for measuring boundaries like variance or alignment are missing. Evaluations rely on qualitative assessments. This hinders objective comparisons across simulations.\n\n4. Accessibility is reduced by dense terminology assuming prior knowledge in both AI and social science. Key concepts like \"average persona\" could be clarified further. This may limit its reach to interdisciplinary audiences.\n\n5. Overall, the major problem of this work is that its scope is too wide. It is very difficult to figure out the LLM boundary in one work. I think authors should consider shrink the scope, discuss the LLM boundary in some specific aspects and using experiments to provide actionable practical insights, not only the descriptions."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kl7qaEO0AE", "forum": "1T1SE9xxAB", "replyto": "1T1SE9xxAB", "signatures": ["ICLR.cc/2026/Conference/Submission11711/Reviewer_Z6Uo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11711/Reviewer_Z6Uo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760800607314, "cdate": 1760800607314, "tmdate": 1762922755942, "mdate": 1762922755942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a survey and reflection on large language model-based social simulation. The authors review existing studies, discuss the boundaries of what large language model-based social simulations can or cannot achieve, and propose a checklist of methodological considerations such as alignment, temporal consistency, and robustness. The stated goal of the paper is to help researchers design more credible and interpretable social simulations that can contribute to social scientific understanding rather than merely reproducing known behaviors."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The topic is timely. Large language model-based social simulation is a rapidly emerging area that indeed requires systematic methodological reflection.\n\n* The paper is clearly written and well-organized.\n\n* The authors try to connect social scientific perspectives with large language model agent research, which could be informative for newcomers to the field."}, "weaknesses": {"value": "1. Lack of novel research contribution.\n\nThe paper primarily summarizes and comments on existing works. It does not introduce a new theoretical framework, formal model, or empirical study. ICLR normally expects some form of novel insight, methodology, or evaluation rather than a descriptive review.\n\n2. Insufficient depth and evaluation.\n\nThe proposed “boundary checklist” remains conceptual and is not validated through adequate case studies or quantitative analysis. Without such evidence, it is difficult to assess its utility or correctness.\n\n3. Ambiguous positioning relative to ICLR scope.\n\nThe work fits better as a perspective or survey article rather than a research contribution in machine learning. ICLR generally values algorithmic, theoretical, or experimental advances, and this paper focuses more on methodological reflection and normative guidance.\n\n4. Some important literature is missing.\n\nThe paper omits several important studies. For example, \n\n[1] Gabriel, Iason, et al. \"Who’s to blame when AI agents mess up? We urgently need a new system of ethics.\" Nature (2025): 38-40.\n\n[2] Kozlowski, Austin C., and James Evans. \"Simulating Subjects: The Promise and Peril of Artificial Intelligence Stand-Ins for Social Agents and Interactions.\" Sociological Methods & Research (2025): 00491241251337316.\n\n[3] Grossmann, Igor, et al. \"AI and the transformation of social science research.\" Science 380.6650 (2023): 1108-1109.\n\n[4] Bail, Christopher A. \"Can Generative AI improve social science?.\" Proceedings of the National Academy of Sciences 121.21 (2024): e2314021121."}, "questions": {"value": "As the field of large language model-based agents for social science is still emerging, there are already several pioneering studies that reflect on its implications for social research and highlight methodological caveats when using such simulations. Some of these important works are missing from the current paper (for example, those listed above). Could the authors clarify how their contribution differs from and advances beyond these prior reflections, both conceptually and methodologically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "docoAiFXgL", "forum": "1T1SE9xxAB", "replyto": "1T1SE9xxAB", "signatures": ["ICLR.cc/2026/Conference/Submission11711/Reviewer_5MeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11711/Reviewer_5MeJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806443797, "cdate": 1761806443797, "tmdate": 1762922755548, "mdate": 1762922755548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a position and analysis of the use of large language models (LLMs) for social simulation. The authors argue that for LLM-based simulations to be meaningful to social science, researchers must recognize and operate within clear boundaries. The paper identifies three primary \"boundary problems\": alignment (do simulated patterns match reality?), consistency (does an agent maintain its persona/behavior over time?), and robustness (is the simulation reproducible?).\n\nThe paper's core thesis is that current LLMs are constrained by an \"average persona\" that exhibits low behavioral variance (lacks heterogeneity), which is a fundamental limitation for simulating complex social dynamics. The authors use a mean-variance framework to analyze this alignment problem. They conclude by proposing \"heuristic boundaries,\" chiefly that researchers should focus on collective patterns rather than individual agent trajectories, and that claims are most reliable when the mean of the collective behavior aligns with human data, even if the variance is low."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's greatest strength is its clear, concise, and persuasive writing. It's an excellent summary of the key challenges in the field.\n\n2. The \"mean-variance\" analysis of the alignment problem is a useful and intuitive way to decompose the \"average persona\" issue.\n\n3. The paper's central message—that the field must be more critical, define its boundaries, and move beyond simple \"replication\"—is a crucial and timely corrective for the community.\n\n4. The final \"heuristic boundaries\" (e.g., \"focus on collective patterns, not individual trajectories\") are practical, actionable, and well-justified by the preceding analysis."}, "weaknesses": {"value": "1. The primary weakness is the paper's failure to acknowledge and differentiate itself from \nWhat Limits LLM-based Human Simulation: LLMs or Our Design? arXiv:2501.08579. This prior work identified the exact same \"LLM-inherent limitations\" (lack of diversity/heterogeneity and inconsistency) as the core bottlenecks. This submission does not offer a new conceptual leap beyond what is already present in that paper. For a position paper, where the idea is the main contribution, this overlap is a critical flaw. And this paper does not cite that paper.\n\n2. As a position paper, it excels at identifying problems (lack of heterogeneity, consistency, robustness) but offers limited, high-level solutions. It does not propose new methods for how to measure robustness or how to quantitatively establish an \"aligned mean,\" which are left as open challenges."}, "questions": {"value": "1. Can you please explicitly state what the novel intellectual contribution of this paper is beyond the critiques already raised in that prior work? Why is your \"boundary problem\" framework (alignment, consistency, robustness) a more generative or insightful model than the \"LLM-inherent vs. Design\" framework?\n\n2. You identify the \"average persona\" (low variance) as a core issue. Do you believe this is a fundamental and perhaps unsolvable limitation of the current maximum likelihood training paradigm? Or do you see it as a solvable engineering problem that can be addressed with better sampling, prompting, or fine-tuning techniques?\n\n\n3. You propose focusing on \"collective patterns.\" However, many crucial social phenomena (e.g., innovation diffusion, radicalization, market panics) are specifically driven by atypical agents or \"long-tail\" behaviors, not the average. Does your analysis imply that this entire class of social phenomena is fundamentally outside the boundary of what LLMs can reliably simulate today?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UdQ7Tl7q1k", "forum": "1T1SE9xxAB", "replyto": "1T1SE9xxAB", "signatures": ["ICLR.cc/2026/Conference/Submission11711/Reviewer_WDSf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11711/Reviewer_WDSf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11711/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900764168, "cdate": 1761900764168, "tmdate": 1762922755087, "mdate": 1762922755087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}