{"id": "zPvdG1Va5Q", "number": 6594, "cdate": 1757990009415, "mdate": 1759897906270, "content": {"title": "MathNet: A Global Multimodal Benchmark for Mathematical Reasoning and Retrieval", "abstract": "Mathematical problem solving remains a demanding test of reasoning for large language and multimodal models, yet existing benchmarks are small, monolingual, and limited in scope. We present *MathNet*, the first large-scale, multilingual, and multimodal dataset of Olympiad-level problems. Spanning 40 countries, 10 languages, and two decades of competitions, MathNet contains 13,026 expert-authored problems with solutions across diverse domains.\n\nMathNet supports two tasks: (i) mathematical comprehension and (ii) mathematical retrieval, an underexplored but essential capability. For retrieval, we construct 39K pairs of mathematically equivalent problems to enable equivalence-based evaluation. Experimental results show that even state-of-the-art reasoning models (72% and 66% accuracy for GPT-5 and Gemini 2.5 Pro) are challenged, while embedding models exhibit substantial difficulty in retrieving equivalent problems.\n\nMathNet provides the largest multilingual Olympiad dataset and the first retrieval benchmark for mathematical equivalence, which we will publicly release.", "tldr": "A large-scale, multimodal, multilingual dataset of math problems for evaluating LLMs on equivalence retrieval and reasoning", "keywords": ["Mathematical retrieval", "Mathematical comprehension", "Large language models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f99dc6a2c5f6d30998ed22b798d6645961ea091.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MathNet, a large-scale, multilingual, and multimodal benchmark for mathematical reasoning and retrieval, specifically targeting Olympiad-level problems. MathNet comprises 13,026 expert-authored problems drawn from 40 countries, spanning 10 languages and two decades of national and international competitions. The dataset includes aligned LaTeX and natural-language problem statements, official solutions, and rich metadata.\n\nThe key contributions are:\n1. Dataset: A curated corpus of high-quality Olympiad problems and solutions from official national sources (not community platforms like AoPS), ensuring expert-level authenticity and diversity.\n2. Novel Retrieval Task: The paper defines and implements a mathematical retrieval task based on equivalence, constructing 39,078 synthetic problem pairs labeled by mathematical similarity.\n3. Taxonomy of Similarity: A fine-grained classification of mathematical relatedness into three modes.\n4. Comprehensive Evaluation: Benchmarking of 16 models across two tasks.\n\nThe paper makes a compelling case that solving ≠ retrieving: models can generate correct answers without understanding underlying mathematical structure well enough to retrieve equivalent formulations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is highly original in both problem formulation and dataset construction. While mathematical reasoning benchmarks exist (e.g., MATH, GSM8K, OlympiadBench), MathNet is the first to explicitly focus on mathematical retrieval based on equivalence, a critical but underexplored capability. The taxonomy of similarity (Invariance/Resonance/Affinity) provides a principled framework for evaluating analogical reasoning in mathematics—a novel conceptual contribution.\n\n2. The dataset construction pipeline is rigorous:\nSources are official national contest booklets, not crowd-sourced.\nProblem–solution alignment uses an LLM-assisted extraction pipeline with cross-model validation (GPT-4.1 + Claude 4 Opus).\nHuman validation by domain experts ensures high fidelity.\nThe retrieval benchmark includes hard negatives and near-miss distractors, making evaluation meaningful.\nExperimental design is thorough: multiple models, domains (Algebra, Geometry, Number Theory, Discrete Math), and metrics (Recall@k, cosine similarity distributions) are reported.\n\n3. The paper is exceptionally well-written and structured. Figures (e.g., Figure 1 overview, Figure 3 similarity distributions) and tables (e.g., Table 2 similarity taxonomy, Tables 3–4 results) are clear and informative. The distinction between comprehension and retrieval is articulated precisely, and the limitations of current models are illustrated with concrete examples.\n\n4. MathNet addresses a real gap in AI for mathematics: the inability to recognize structural equivalence across notations, languages, or modalities. This has implications beyond benchmarks—it affects retrieval-augmented reasoning, automated theorem proving, and mathematical search engines. By releasing the dataset and benchmark publicly, the authors provide a foundational resource for the community to develop models with a deeper mathematical understanding."}, "weaknesses": {"value": "1. Limited Analysis of Multimodality:\nWhile MathNet is described as multimodal, the paper reports only marginal gains from visual inputs and does not deeply analyze why vision-language models underperform. Are the diagrams too complex? Is OCR/captioning inadequate? A more detailed error analysis of multimodal failures (e.g., types of diagrams that confuse models) would strengthen the contribution.\n\n2. Synthetic Equivalence Pairs May Lack Real-World Validity:\nThe 39K equivalent pairs are described as “synthetic.” The paper does not clarify whether these were algorithmically generated (e.g., via symbolic rewriting) or human-curated. If synthetic, they may not reflect the kinds of equivalence that arise organically in contest design or research. This risks overestimating or mischaracterizing retrieval difficulty.\n\n3. Evaluation Protocol for Comprehension Relies on GPT-4o as Judge:\nWhile prior work (Omni-MATH) validates this approach, using a single LLM—even a strong one—as the sole evaluator introduces potential bias or blind spots, especially for non-English or non-standard solution styles. Including human evaluation on a subset (beyond extraction validation) would bolster credibility.\n\n4. Under-Specified Cross-Lingual Challenges:\nThe paper highlights multilingual coverage but does not report per-language performance or analyze whether retrieval/comprehension degrades in low-resource languages (e.g., Persian, Ukrainian). Without this, the claim of \"multilingual benchmark\" feels partially substantiated."}, "questions": {"value": "1. Equivalence Pair Generation:\nHow were the 39,078 mathematically equivalent problem pairs constructed? Were they generated via symbolic transformations, human rewriting, or LLM paraphrasing? If synthetic, how was the mathematical correctness of equivalence verified? Could you provide examples of non-trivial equivalences that required deep insight?\n\n2. Multimodal Content Details:\nWhat fraction of MathNet problems include diagrams or images? Can you share statistics on image types (e.g., geometric figures, graphs, tables)? Did you experiment with vision-specific embeddings (e.g., CLIP on diagrams) for retrieval, or only text embeddings?\n\n3. Cross-Lingual Performance:\nDo model performances vary significantly across languages? For instance, is retrieval harder for problems originally in Arabic or Russian vs. English? Could you include a small table or analysis of language-wise accuracy?\n\n4. Retrieval-Augmented Reasoning (RAG) Experiments:\nThe paper mentions that RAG only helps when retrievers surface structure-aligned neighbors. Did you conduct downstream RAG experiments (e.g., using retrieved equivalents to aid problem solving)? If so, what were the gains? If not, is this planned for future work?\n\n5. Human Evaluation of Solutions:\nBeyond validating extraction, did you conduct human grading of model-generated solutions (e.g., on a 100-problem subset)? How does GPT-4o’s judgment correlate with human graders for non-English or proof-based answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pxoM5AQDDh", "forum": "zPvdG1Va5Q", "replyto": "zPvdG1Va5Q", "signatures": ["ICLR.cc/2026/Conference/Submission6594/Reviewer_dueB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6594/Reviewer_dueB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761308881627, "cdate": 1761308881627, "tmdate": 1762918920822, "mdate": 1762918920822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MathNet, a new large-scale, multilingual, and multimodal benchmark for advanced mathematical reasoning. The dataset consists of 13,026 Olympiad-level problems sourced from official competition booklets from 40 countries over two decades, covering 10 languages. The paper's primary contributions are twofold: (1) the dataset itself, which is meticulously curated, expert-authored, and annotated with a rich taxonomy of mathematical topics and similarity types; and (2) a novel benchmark task focused on \"math-aware retrieval,\" which evaluates a model's ability to identify mathematically equivalent problems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "*   **Originality:** The most significant original contribution is the formulation and operationalization of the \"math-aware retrieval\" task. While other benchmarks focus on problem-solving, MathNet pioneers the evaluation of a model's ability to recognize structural equivalence, a cornerstone of mathematical thinking and analogical reasoning. \n\n*   **Quality:** \n    *   **Data Sourcing:** By curating problems from official national Olympiad booklets, the authors ensure expert-level quality, authority, and consistency, avoiding the noise often present in community-sourced platforms like AoPS.\n    *   **Curation Pipeline:** The multi-stage pipeline for extraction and validation is state-of-the-art. Using a specialized OCR model (`dots-ocr`), followed by LLM-based problem-solution alignment, and then cross-model verification (using both GPT-4.1 and Claude 4 Opus) is a robust and scalable approach that minimizes bias and error. This is further strengthened by human validation.\n\n*   **Significance:** \n    *   **Resource for the Community:** MathNet provides the largest multilingual Olympiad-level dataset, a critical resource that will fuel research in mathematical reasoning, cross-lingual transfer, and RAG for years to come. \n    *   **Practical Implications:** The work has clear connections to real-world applications, from improving tools for mathematicians and contest organizers to building more robust and generalizable AI reasoning systems."}, "weaknesses": {"value": "*   **Underdeveloped RAG Experiments:** The paper compellingly argues that math-aware retrieval is essential for effective RAG in this domain. However, it does not present any experiments to directly validate this claim. This is a significant missed opportunity. A controlled experiment showing that a solver's performance is significantly boosted when provided with ground-truth equivalent problems (an \"oracle\" retriever) versus a standard retriever (e.g., one based on `gemini-embedding-001`) would provide powerful, direct evidence for the benchmark's utility and the paper's core thesis.\n\n*   **Limited Analysis of Multimodality:** The paper positions MathNet as a \"multimodal\" benchmark, but this aspect feels underdeveloped in the analysis. The main text briefly mentions that augmenting models with diagrams yields \"only marginal improvements\" but provides little detail. The paper would be stronger with more statistics (e.g., what percentage of problems are inherently visual?), qualitative analysis (e.g., examples of diagrams that models fail to interpret), and a deeper discussion of the specific challenges posed by mathematical diagrams (e.g., geometric constructions vs. data plots).\n\n*   **Detail on Equivalence Pair Construction:** The methodology for creating the 39,078 \"synthetic problem pairs with labeled equivalence classes\" is not fully detailed in the main paper. While the taxonomy of similarity is explained, the process of generating these pairs—which are central to the novel retrieval task—could be described more transparently. Understanding how these pairs were generated is crucial for assessing the difficulty and validity of the retrieval benchmark."}, "questions": {"value": "1.  **Regarding RAG Experiments:** Have the authors considered running a controlled RAG experiment? For example, by comparing a solver's performance under three conditions: (a) zero-shot, (b) RAG with a standard off-the-shelf retriever, and (c) RAG with an \"oracle\" retriever using the ground-truth equivalent problems from MathNet. Such an experiment would provide a direct and powerful demonstration of the value of math-aware retrieval.\n\n2.  **Regarding the Retrieval Benchmark Construction:** Could you provide more detail on the process used to generate the 39K equivalent problem pairs? Were these created by human experts, generated by an LLM and then verified, or through some other procedure? Furthermore, how were the \"near-miss/hard negatives\" used in Figure 3 selected or defined? Understanding this is key to interpreting the retrieval results.\n\n3.  **Regarding Multimodality:** Could you provide more statistics on the multimodal component of MathNet? Specifically, what fraction of the 13K problems include diagrams that are essential for solving? Could you also include a few examples in the appendix showing where current LMMs fail on the visual reasoning component, and what a correct interpretation of the diagram would entail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o1Sz4hibuo", "forum": "zPvdG1Va5Q", "replyto": "zPvdG1Va5Q", "signatures": ["ICLR.cc/2026/Conference/Submission6594/Reviewer_qJLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6594/Reviewer_qJLT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898004247, "cdate": 1761898004247, "tmdate": 1762937443729, "mdate": 1762937443729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MathNet, a large-scale multilingual, multimodal benchmark of Olympiad‑level math problems (13,026 problems; 40 countries; 10 languages) with expert solutions, plus a new equivalence-based retrieval task (39,078 synthetic pairs) and a taxonomy of similarity (Invariance/Resonance/Affinity). It details an OCR→LLM pipeline for aligned problem–solution extraction, human/LLM validation, dataset statistics, and evaluates both comprehension (solution generation) and retrieval (Recall@k) across a range of LLMs/LMMs and embedding models. Main findings: frontier LLMs remain challenged on Olympiad problems (e.g., GPT‑5 macro‑avg 72.25%); equivalence retrieval is hard at top‑1 (≈5% R@1)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Scale, Authenticity, and Provenance:** 13,026 problems across 40 countries and two decades; sourced from official national booklets rather than community forums. This supports representativeness and reduces annotation noise. Appendix A.1 lists dozens of national/regional exams enabling wide topical coverage and cross‑year comparisons. Page numbers and source files are recorded during extraction, aiding auditability and reproducibility. \n2. **Novel Equivalence‑Focused Retrieval & Taxonomy:** 9,078 paired problems labeled for equivalence and relatedness (contributions, item 2; p. 2), enabling evaluation beyond lexical similarity. This is novel for Olympiad math IR. Invariance/Resonance/Affinity with concrete examples (Table 2 p. 6) clarifies what “sameness” means in math problems—important for designing retrievers. Cosine similarity distributions show overlap between equivalents and hard negatives (Fig. 3 p. 7), illustrating the task’s difficulty. \n3. **Clear, Multi‑Stage Extraction/Validation Pipeline:** OCR→problem extraction→solution retrieval→cross‑model validation is diagrammed (Fig. 2 p. 5) and explained (Sec. 3.3 p. 4–5), demonstrating a principled approach to noisy PDFs. 20 annotators on 100 pairs plus LLM distractor stress tests and expert consensus checks (Sec. 3.4 p. 5) bolster data quality. Full prompts for extraction/evaluation/metadata are given (A.5 pp. 18–20), supporting reproducibility. \n4. **Comprehensive Baselines and Findings:** Eight LLM/LMM systems compared across four domains (Table 3 p. 8), showing domain‑wise strengths/weaknesses and leaving headroom (e.g., Discrete/NT hardest). This is informative for future modeling. Eight embedding models with Recall@{1,5,10} per domain and overall (Table 4 p. 8) provide a solid baseline landscape. Misinterpretation/logical gaps/context over‑reliance are articulated (Sec. 4.4 p. 9), guiding where methods fail."}, "weaknesses": {"value": "1. **Equivalence Pair Construction/Validation Under‑specified**\n- Generation details missing: The paper states 39,078 synthetic equivalence pairs (p. 2) but does not detail how they were generated, filtered, or formally validated beyond taxonomy definitions—No direct evidence found in the manuscript. This limits trust in ground truth. \n- Limited expert audit: Only a 100‑problem subset received expert review (Sec. 3.4 p. 5), which may be small relative to 39k pairs. This constrains external validity.\n- Agreement metrics absent: No inter‑annotator agreement or judge‑consensus statistics are reported—No direct evidence found in the manuscript—reducing clarity on label reliability.\n2. **Multilingual & Multimodal Claims vs. Reporting**\n- Language imbalance: English dominates (89.88%; Table 7 p. 17), potentially limiting the strength of “multilingual” claims for training/evaluation comparability.\n- No per‑language scores: Comprehension/retrieval are not broken down by language—No direct evidence found in the manuscript—so cross‑lingual generalization remains unclear.\n- Multimodal share unquantified: The proportion of image‑interleaved problems is not reported—No direct evidence found in the manuscript—yet conclusions note marginal gains for image inputs\n3. **Mathematical Formulation & Notation Clarity (correctness/consistency)**\n- Ambiguous notation/examples: Intro uses expressions like “pi+1 − pi ≤ Πi” without defining Πi (p. 2,line087), which may confuse readers."}, "questions": {"value": "1. **How are the 39k equivalence pairs generated and validated at scale?**\n\nCould you detail pair generation (templates/transformations, LLM rewriting, symbolic rewrites) and filtering? If models assisted, how did you prevent leakage (Sec. 3.5 p. 6 mentions similarity modes; contributions p. 2).\n\n2. **Could you report multilingual and multimodal breakdowns?**\n\nAdd per‑language comprehension/retrieval metrics and cross‑lingual retrieval (query language A → target language B), given Table 7’s imbalance (p. 17).\n\n3. **Can you strengthen statistical rigor and reproducibility?**\n\nReport CIs/variances for Tables 3–4 via repeated runs or bootstrap (p. 8).\n\n4. **Could you tighten mathematical correctness/notation?**\n\nDefine symbols in Intro examples (e.g., clarify Πᵢ)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aIzKjZtS1z", "forum": "zPvdG1Va5Q", "replyto": "zPvdG1Va5Q", "signatures": ["ICLR.cc/2026/Conference/Submission6594/Reviewer_jSKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6594/Reviewer_jSKk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986797423, "cdate": 1761986797423, "tmdate": 1762918919771, "mdate": 1762918919771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a new dataset of 13k multimodal olympic-level math reasoning problems over 10 languages, as well as a new math retrieval task for locating the similar problems over shallow lexical overlap. The problems are crowd-sourced from textbook and websites and normalized through a data processing pipeline with human-AI incorporation. The evaluation of several powerful LLMs show that they can still struggle with retrieving equivalent math problems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The task of math retrieval can be used to test the reasoning ability of LLMs in a different but daily-use dimension. The community can benefit from the support of MathNet in such functionality.\n- The data collection process is introduced with rich detail, with deep human-AI cooperation in cross-validation and cross-reinforcement.\n- The definition of problem similarity is detailed discussed and categorized."}, "weaknesses": {"value": "- Only light analysis of the experimental results are provided, enhancing which could better inspire following studies in this line.\n- I still have a few questions which are listed below, and I am happy to adjust the estimation if more convincing details are provided to them."}, "questions": {"value": "- How are the golden answer of math retrieval determined?\n- Given the result in Recall@5 and Recall@10 (which are significantly greater than Recall@1), how well would model perform if the embedding-based retrieval is used only for a coarse-grained result before another model (perhaps smaller) is used to further identify the answer? And what would be the satisfactory threshold (i.e. is the Recall@10 good enough)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ub1XJLYhjy", "forum": "zPvdG1Va5Q", "replyto": "zPvdG1Va5Q", "signatures": ["ICLR.cc/2026/Conference/Submission6594/Reviewer_Rdfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6594/Reviewer_Rdfs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6594/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996144609, "cdate": 1761996144609, "tmdate": 1762918919473, "mdate": 1762918919473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}