{"id": "dk8zJMyndW", "number": 15079, "cdate": 1758247494012, "mdate": 1763651075538, "content": {"title": "OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing", "abstract": "While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive \"less is more\" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://anonymous.4open.science/r/omniplay.", "tldr": "", "keywords": ["Large Multimodals Models", "Game", "Omni-Modal Models", "Decision"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea49e2c923ba5319cc246a9b1b302a7e351d2e7c.pdf", "supplementary_material": "/attachment/1285872f02e3c8760fbe6dc4f0fc522d6f819a50.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a diagnostic benchmark designed to evaluate omni-modal AI systems capable of processing and integrating text, images, audio, and video in dynamic environments. It addresses the gap between static benchmarks, which lack interactivity, and existing interactive ones, which usually ignore important sensory modalities. The benchmark contains five interactive game environments that intentionally create both complementary and conflicting sensory conditions, allowing researchers to study how models perform cross-modal reasoning and resolve ambiguity. Evaluation of six state-of-the-art models reveals a clear contrast: they achieve superhuman memory performance but show fragile reasoning and planning, especially when modalities conflict. Interestingly, removing one modality sometimes improves performance (“less is more”). The findings suggest that advancing general intelligence requires focusing not on scaling model size, but on improving multi-sensory fusion and conflict resolution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It provides a comprehensive and realistic evaluation of omni-modal models, integrating text, image, audio, and video interaction, filling the gap left by older static or single-modality benchmarks.\n- It is theoretically well-grounded, built on a generalized MDP framework with explicit principles of modality interdependence, controlled conflict, and variable complexity.\n- It offers diagnostic precision, using systematic tests like modality conflict, ablation, and noise experiments to clearly reveal each model’s strengths and weaknesses in multi-modal fusion"}, "weaknesses": {"value": "- The benchmark lacks realistic, high-fidelity 3D environments like those used in DeepMind’s SIMA project (https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/). Its custom mini-games are controlled and simplified, limiting ecological validity and real-world generalization.\n- The “modality conflict” and “modality complementarity” in each game are artificially constructed scenarios (e.g., inserting misleading audio cues or removing video frames). While such controlled manipulation facilitates diagnostic analysis, it does not necessarily reflect the naturally occurring noise and uncertainty found in real multimodal tasks, thereby limiting the ecological validity of the benchmark.\n- The NPS is normalized using human and random baselines, but the underlying scoring functions differ significantly across tasks (especially in RTS and pathfinding scenarios). This “cross-task unified metric” implicitly assumes comparability between heterogeneous tasks, which in practice reduces the rigor and fairness of the evaluation. Moreover, certain tasks (e.g., Blasting Showdown) are excluded from the NPS, further undermining the overall consistency of the benchmark."}, "questions": {"value": "- The benchmark mainly tests one-shot performance. Have the authors considered evaluating continual adaptation or long-horizon reasoning?\n- How are difficulty levels calibrated across tasks to maintain balance and interpretability?\n- Do the authors consider modality-specific weighting based on task relevance rather than assuming equal importance across modalities?\n- Since modality conflicts are manually constructed, how can the authors ensure that these settings realistically capture natural multimodal inconsistencies rather than artificial noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKO2qRdEwq", "forum": "dk8zJMyndW", "replyto": "dk8zJMyndW", "signatures": ["ICLR.cc/2026/Conference/Submission15079/Reviewer_Qw1W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15079/Reviewer_Qw1W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275787916, "cdate": 1761275787916, "tmdate": 1762925404126, "mdate": 1762925404126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniPlay, a novel benchmark designed to evaluate the capabilities of omni-modal foundation models (like Gemini and GPT-4o) in interactive, dynamic environments, addressing the limitations of existing static or modally-restricted benchmarks. OmniPlay consists of a suite of five custom-built game environments that require agents to perceive, reason, and act using a combination of image, video, audio, and text inputs. The benchmark is specifically designed around the principle of \"modality interplay,\" systematically creating scenarios where sensory information is either complementary (requiring synergistic fusion) or conflicting (testing robustness and conflict resolution). Through comprehensive evaluations of six leading omni-modal models against human and random baselines, the authors find a stark dichotomy: the models exhibit superhuman performance on tasks heavily reliant on memory but show systemic failures in those demanding robust reasoning, strategic planning, and handling modality conflicts. A key finding is the \"less is more\" paradox, where removing sensory modalities sometimes paradoxically improves performance, suggesting immature fusion mechanisms are a critical bottleneck. The paper concludes that progress towards AGI requires focusing on synergistic fusion and conflict arbitration, not just model scaling, and offers OmniPlay as a diagnostic tool for these challenges."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper tackles the inadequacy of current benchmarks, which are either static (lacking agency) or interactive but modally limited (ignoring audio, etc.), by introducing an interactive benchmark designed for *omni-modal* agents using image, video, audio, and text.\n\n* It introduces five distinct, newly developed game environments, each crafted to test different capabilities (e.g., navigation, sequence replication, abstract reasoning, strategy) under varying modality combinations and complexities.\n\n* The evaluation reveals critical insights into current omni-modal models, such as their superhuman memory but weak reasoning/planning, fragility under modality conflict, and the paradoxical \"less is more\" effect where removing modalities can improve performance.\n\n* The paper establishes strong baselines (random agent and a diverse human expert cohort) and uses both overall performance metrics (like NPS) and detailed, task-specific diagnostic metrics.\n\n* The authors intend to release the entire OmniPlay platform, including environments and protocols, fostering further research in the community."}, "weaknesses": {"value": "- The games and scenarios involving modality complementarity and conflict are custom-designed based on the authors' principles. The \"naturalness\" or representativeness of these specific interaction patterns for general real-world tasks could be debated.\n\n- The paper highlights \"superhuman memory\" in the Myriad Echoes task. However, the qualitative analysis suggests this is largely due to the AI's perfect recall compared to human cognitive limits on working memory for long, arbitrary sequences, rather than a sign of superior general memory or reasoning.\n\n- Evaluating large omni-modal models in interactive environments across numerous episodes and seeds is computationally intensive. The paper does not detail the resources required, which could be a barrier for widespread adoption and replication of the benchmark."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sicleWZvOo", "forum": "dk8zJMyndW", "replyto": "dk8zJMyndW", "signatures": ["ICLR.cc/2026/Conference/Submission15079/Reviewer_TvLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15079/Reviewer_TvLB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742335383, "cdate": 1761742335383, "tmdate": 1762925403710, "mdate": 1762925403710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniPlay, a new diagnostic benchmark designed to evaluate the omni-modal (Image, Video, Audio, and Text) reasoning and agency of generalist foundation models. The authors argue that existing benchmarks create an \"evaluation chasm\" : static benchmarks (like VQA) lack agency and interactivity , while current interactive benchmarks (like ALFWorld) suffer from a bottleneck in modality. The core design of OmniPlay is \"modality interplay\" where the author tests complementarity (needing all senses) and conflict (handling contradictory information). The authors evaluate six multi-modal models and show that though models demonstrate \"superhuman memory\", they show \"brittle reasoning\" in comparison. I'll adjust my rating based on author's response."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the paper is clear, where there is a lack of benchmarks that test agency with a rich, multi-sensory environment.\n- There is solid open-source contribution for both the environments and evaluation protocols, and the appendix shows extensive details.\n- The paper is well-written."}, "weaknesses": {"value": "- Some tasks, while creative, appear to test very specific, narrow forms of reasoning. The Alchemist's Melody (rule discovery) can be reduced to a trial-and-error association problem where no reasoning is really involved. The Myriad Echoes and the Whispered Pathfinding are essentially complex perception-and-grounding tasks. They are not really testing \"strategic planning\" and \"robust reasoning\".\n- The paper heavily contrasts \"brittle reasoning\" with \"superhuman memory\". This \"dichotomy\" is not really an apple-to-apple comparison. The memory task (Myriad Echoes) is a requiring sequence replication which perfectly maps to the architectural strengths of transformers (perfect recall of a sequence from context) and leverages capabilities that are known to be far superior to human working memory. In contrast, the \"reasoning\" tasks require emergent, generalizable skills that are the field's current grand challenge. Finding that models are good at what they are architecturally designed to do (remember and recall) and bad at what they are not (immediate reasoning) is not a particularly insightful dichotomy; it is almost an expected result.\n\n\nNits (does not affect rating):\n- \"Full Sensory Spectrum\" is over-claimed. Modalities relevant to agentic intelligence, like haptics (touch), proprioception, and sensors (e.g., radar, lidar) are actually not considered. Some rephrasing is needed."}, "questions": {"value": "- Based on the zero-shot performance, the author claims \"scaling models may not be sufficient\". It is plausible that these \"brittle fusion mechanisms\" are an artifact of zero-shot generalization and could be fixed with even minimal fine-tuning on the OmniPlay tasks. Is there any fine-tuning result to distinguish a fundamental model incapacity v.s. a lack of task-specific adaptation? I am aware of the \"aided reasoning\" scenarios, but a full fine-tuning study is needed to substantiate the paper's primary conclusion.\n- For Phantom Soldiers in the Fog, what is the likely reason that the model fails? Is it due to failed long-horizon planning or failure to ground the (noisy) visual cues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uuU6ehPDEx", "forum": "dk8zJMyndW", "replyto": "dk8zJMyndW", "signatures": ["ICLR.cc/2026/Conference/Submission15079/Reviewer_WV1v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15079/Reviewer_WV1v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864394971, "cdate": 1761864394971, "tmdate": 1762925403240, "mdate": 1762925403240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OmniPlay, a game-playing benchmark for Omni-modal models to test their capabilites in dynamic interactive worlds. The benchmark is comprised of five custom-designed games that emphasize the interplay between different modalities.\nDifferent series of experiments evaluate six representative current omni-modal models, showcasing their performance limtiations. \nWhile aditional exploratoy experiments illustrate performance degradation under modality conflict across models and their seemly \"preference\" for specific modalities. The presented discussion of results points towards better fusion of modalities as key for improved capabilities and perfomance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a novel benchmark to probe omni-modal models in their actual effective use of different modalities. The use of targeted games is also especially key, as mentioned by authors, it allow for better experiment desing and traditional benchmarks fail to test model capabilities in both dynamic and interactive worlds.\n\nThe stated goal to \"explicitly address the foundational challenges of synergistic fusion, conflict arbitration, and resilient reasoning\" is an important impactful target. And the discussed experimental results both on the effectiveness of models' fusion of different modalities and how they deal with modality conflicts present interesting insights for furhter exploration, which can have significant impact on model performance and how to better evaluate their capabilties.\n\nThe design methodology for the benchmark can also inspire the creation of better evaluation assets for the overall community."}, "weaknesses": {"value": "While the paper tackles significant and timely issues and proposes an original benchmark with potential, the current manuscript suffers from\nnot fully adequate presentation and soundness issues for some of its conclusions.\n\nFirst, the paper presentation seems a bit backwards. It starts talking about specific experiment details without having really described the games, the core benchmark design (beyond just some principles), and not showing overall results. This makes the paper quite hard to interpret as the reader needs to find the details about a specific game, metrics, how they relate to issues, human performance, before they can parse what the discussion of what a figure like Figure 3 or some summary results like Table 2 actually mean.\n\nSecond, a core contribution of the benchmark is the proper design of the modality-interplay games. As such, I'd expect at least a short focused description of them in the main paper, as well as some analysis of how well they were designed. Unfortunately, even if games were also played by humans, there is no discussion on how well the games model their goals (gameplay or evaluation effectiveness as tools). Moreover, for self-contained context, at least a sentence description by game emphasizing its key goal and something like table 7 should be in main paper body. Table 1 doesn't really perform this job well as it's too high-level and could be re-designed or turned into a text paragraph.\n\nThe discussed experiments present nice findings regarindg multi-modality capabilites, like Gemini 2.5 Flash performance changes in different cross-modality conflicts. However, the paper doesn't provide any substantive discussion of \"reasoning and strategic planning\", as claimed as contribution. Table 2 is not enough evidence for the claim of dichotomy in performance or sub-par reasoning, as it lacks details and even comparison to human performance. The other analysis provided also delves mostly in modality representation issues.\n\nFull performance results only come in Appendix I, with human results only show in Table 19 (page 41) and could easily include NPS metrics for ease of comparison to model performance in the main paper. I understand the limitations of space, but some of the findings in Appendices F5, F6, and G, could also have been specific called out from main text for better understanding.\n\nRegarding the \"diiagnosis\" angle of the benchmark, while the described effect of modality conflicts are insightful, they seem a little high-level and left me also wondering how would humans handle such cases and what the performance impact would be. I feel like this is a critical aspect for proper undestanding the presented results.\n\nThe paper also claims to acknowledge the benchmark limitations, but there are not explicitly discuss discussed anywhere."}, "questions": {"value": "Scores for NPS only may false indicate actual performance due to manual weighting in their calculation. Why report only NPS? Other parts of the appendices don't even calculate it.\n\nSome of the image noise results seem to show a suprising performance drop, even if visually small (ex: Figure 24). This one seems more like \"text noise\", as the text in the images becomes undreadable. Shouldn't a more in-depth analysis be shown before some of these can be raised as \"key findings\" in sub-section 5.3? Same for the aided reasoning insight. The paper could benefit from better discussion of the principal findings claims.\n\nOverall I really like the paper idea and some of its contents, but it seems to try to claim too much at the same time, while missing more supporting evidence or in-depth discussion (even if point to future analysis).\n\nI also didn't see a mention of the release of the benchmark and its license, which are also key for its potential community impact and should be covered in the manuscript.\n\nMinor other issues:\n- Please fix citations. For example, \"Google’s Gemini Team et al. (2023)\", or \"and SEED-Bench Li et al. (2023)\".\n- Subsetion 3.1 could easily be moved to and appendix to open more space for game details and core results to move into the core paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VEa8F882mj", "forum": "dk8zJMyndW", "replyto": "dk8zJMyndW", "signatures": ["ICLR.cc/2026/Conference/Submission15079/Reviewer_3RE4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15079/Reviewer_3RE4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257268526, "cdate": 1762257268526, "tmdate": 1762925402754, "mdate": 1762925402754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}