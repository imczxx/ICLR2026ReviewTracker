{"id": "nY91ZOfB5M", "number": 23256, "cdate": 1758341357866, "mdate": 1759896823949, "content": {"title": "Improving MoE Performance and Efficiency with Plug-and-Play Intra-Layer Specialization and Cross-Layer Coupling Losses", "abstract": "Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap, where different experts process similar tokens and learn redundant functions, resulting in ambiguous routing and underutilized capacity. While architectural solutions like DeepSeek-style shared experts promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. We propose two plug-and-play auxiliary losses that enhance MoE specialization and routing efficiency without modifying routers or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary functions. Second, a cross-layer dependency loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with shared-expert and vanilla Top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.", "tldr": "We introduce two auxiliary losses for MoE—intra-layer specialization loss and cross-layer coupling loss—as plug-and-play modules within the Megatron-LM framework, enhancing both model performance and inference throughput.", "keywords": ["Mixture-of-Experts", "Auxiliary losses", "Intra-Layer Specialization", "Cross-layer Coupling", "Plug-and-Play"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e647d57c7833d8c80273c9231770fcc4a833ef4.pdf", "supplementary_material": "/attachment/bf7bc0a5afa2ee54b0e29fe135fc32af92690d16.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes two auxiliary loss terms to improve the training dynamics of MoE: an intra-layer specialization loss to enhance expert specialization and a cross-layer coupling loss to avoid unstable routing \"paths\" across depth. These losses are presented as plug-and-play additions that require no architectural or routing code changes, demonstrating improvements in perplexity for MoE pretraining / finetuning and accuracy on several downstream finetuning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is lightweight: it does not alter the architecture and is plug-and-play to integrate into standard MoE training codebases. \n- The experiments are conducted on more than one MoE variant (vanilla top-k MoE and a DeepSeek-style MoE with shared experts). It also reports downstream finetuning results, not just pretraining perplexity.\n- The argument that cross-layer coupling yields more stable expert pathways, which in turn improves inference throughput by making expert caching/batching easier, is an important claim from a deployment perspective. Inference efficiency is a real bottleneck for production MoE systems."}, "weaknesses": {"value": "- Limited conceptual novelty: The two proposed losses formalize ideas that have already appeared in prior MoE work: promoting expert specialization \\[1,2\\], and stabilizing routing across depth \\[3,4\\]. The paper’s main differentiator is doing so via loss terms instead of redesigning the router or the expert layout. That is useful, but it is incremental rather than a fundamentally new MoE training principle.\n- The paper claims that forcing experts to specialize and encouraging consistent cross-layer routing paths are what _cause_ the downstream quality improvements. However, there is little analysis of _why_ the model improves. For example: Do weights between experts actually become more orthogonal over training? Does routing entropy across depth actually drop? How do expert assignments cluster by token type over time?  Without these diagnostics, it’s still plausible that the reported gains are just another regularization effect that helps converge faster but not finally better under certain configurations.\n- The paper does not adequately compare against recent methods that pursue very similar goals. Right now, the paper largely compares to a vanilla load-balancing baseline. However, works like \\[1\\] also introduce auxiliary loss terms for similar motivations. The manuscript should compare against that line both conceptually and empirically.\n- The downstream evaluation focuses on five QA tasks. These are useful but narrow. Important benchmarks for LLM quality such as MMLU (knowledge and reasoning) and HellaSwag (commonsense inference) should be reported.\n\n\\[1\\] Advancing Expert Specialization for Better MoE https://arxiv.org/pdf/2505.22323\n\\[2\\] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing https://arxiv.org/pdf/2412.14711\n\\[3\\] Residual Mixture of Experts https://arxiv.org/pdf/2204.09636\n\\[4\\] Layerwise Recurrent Router for Mixture-of-Experts https://arxiv.org/pdf/2408.06793"}, "questions": {"value": "- How are the coefficients $\\lambda_{sp}/\\lambda_{cp}$ tuned in pretraining tasks? Are they consistent across different settings in Figure 3?\n- The load balance loss has a lower bound of one \\[5\\], but some reported numbers in Table 11 appear to go below that. Can you clarify how that table is computed?\n- How do the intra-layer specialization loss and the cross-layer coupling loss evolve during training? Do they saturate early, or continue dropping throughout pretraining? I believe showing these curves for both baseline and your methods would make the “experts specialize / paths stabilize” narrative more credible than just reporting downstream accuracy.\n- The coupling loss encourages certain expert layer-to-layer combinations and discourages others. Intuitively, that could reduce routing diversity (i.e., it could “lock in” a small number of dominant expert pipelines). How do you avoid collapsing to only a few expert-path templates? In other words: does cross-layer coupling suppress rare-but-useful expert configurations?\n\n\\[5\\] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity https://arxiv.org/pdf/2101.03961"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ioY6mnENCx", "forum": "nY91ZOfB5M", "replyto": "nY91ZOfB5M", "signatures": ["ICLR.cc/2026/Conference/Submission23256/Reviewer_Kwj5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23256/Reviewer_Kwj5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756446000, "cdate": 1761756446000, "tmdate": 1762942577914, "mdate": 1762942577914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a fundamental challenge in Sparse Mixture-of-Experts (MoE) models: **expert overlap and routing ambiguity**. While MoE models efficiently scale transformers by activating only a subset of experts per token, they suffer from:\n\n1. **Expert Overlap**: Different experts produce nearly identical activations for the same tokens, creating redundant representations\n2. **Routing Ambiguity**: Similar inputs are inconsistently dispatched across different experts, preventing clear specialization\n3. **Capacity Underutilization**: These issues cause multiple experts to learn redundant knowledge, wasting model capacity\n\n### Key Contributions\n\n1. **Two Novel Plug-and-Play Loss Functions**\n- **Intra-Layer Specialization Loss ( $R_{sp}$ ) (§4) -** Penalizes cosine similarity between activations of different experts processing the same token. Forces experts within each layer to develop orthogonal representations\n- **Cross-Layer Coupling Loss ($R_{cp}$) (§5) -** Maximizes joint routing probabilities between adjacent layers. Establishes coherent \"expert pathways\" through network depth\n1. **Theoretical Foundations**\n    - **Proposition 1**:\n        \n        The cosine similarity between gradients of different experts' down-projection matrices equals the cosine similarity of their activations\n        \n        This proves that orthogonal activations lead to orthogonal parameter gradients, ensuring distinct learning trajectories.\n        \n    - **Proposition 2**:\n        \n        Cross-layer coupling acts as a \"specialization amplifier\" - when layer $l$ exhibits well-specialized experts and maintains strong coupling with layer $l+1$, the specialization structure transfers with bounded degradation:\n        $$|\\cos(r^{(l+1,\\nu_1)}, r^{(l+1,\\nu_2)})| \\leq \\varepsilon + O(\\delta, \\iota)$$\n        \n2. **Compatibility Guarantees**\n    \n    The paper proves both losses are compatible with standard load-balancing objectives:\n    \n    - **Propositions 3 & 4** demonstrate that optimal routing configurations exist that minimize specialization/coupling losses while maintaining perfect load balance\n    - The losses operate on orthogonal principles: specialization determines partitioning scheme, load balancing constrains partition sizes"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Establishing the mathematical link between expert activations and learning dynamics. Proposition 1 proves that:\n    \n    $$\n    \\cos\\left(\\frac{\\partial L}{\\partial W^{(l,e)}_\\text{down}},\\frac{\\partial L}{\\partial W^{(l,\\nu)}_\\text{down}}\\right)=\\cos(z^{(l,e)}_i, z^{(l,\\nu)}_i)\n    $$\n    \n    this transforms the abstract goal of expert specialization into a **concrete, measurable optimization objective**. Rather than hoping specialization emerges naturally, the authors provide a direct control expert differentiation through activation geometry.\n    \n- The insight that cross-layer coupling can **propagate specialization through network depth** (Proposition 2) is particularly creative. The authors show that when layer $l$ has specialized experts and strong coupling with layer $l+1$, the specialization structure transfers with bounded degradation:\n    \n    $$\n    |\\cos(r^{(l+1,\\nu_1)}, r^{(l+1,\\nu_2)})| \\leq \\varepsilon + O(\\delta, \\iota)\n    $$\n    \n    This reveals a previously unexplored mechanism where **local specialization cascades globally** through the network - a novel perspective on how MoE models can achieve network-wide functional diversity.\n    \n- its just a **plug and play Megatron-LM module** activated by a single configuration flag represents significant engineering value.\n    - No modifications to core attention, FFN, or router logic\n    - Minimal computational overhead (essentially just computing cosine similarities)\n    \n    this lowers the barrier to adoption compared to architectural modifications.\n    \n- The paper's core philosophical contribution is treating expert specialization as a **primary training objective rather than an emergent property**. This could be potentially steps in the right direction to deal with expert specialization and making MoEs better"}, "weaknesses": {"value": "- **Questionable Inference Acceleration Claims:** The inference acceleration results (Table 9) require additional infrastructure (path-aware placement, bucketing, graph partitioning) beyond the training losses. The paper conflates the benefits of the losses with the benefits of this additional efforts, making it difficult to assess the true contribution.\n- **Hyperparameter sensitivity**: The method introduces two new hyperparameters ($\\lambda_{sp}$ and $\\lambda_{cp}$) that appear to require careful tuning (different values for different model sizes in experiments). Table 6 shows $\\lambda_{sp} = 2e-4$ and $\\lambda_{cp} = 1e-4$, but there's no systematic study of sensitivity or guidelines for setting these values.\n- The paper identifies \"expert overlap\" and \"routing ambiguity\" as key failure modes but provides limited empirical evidence:\n    - Figure 1 shows perplexity curves but not actual measurements of expert overlap or routing ambiguity over training.\n    - The paper doesn't quantify how much redundancy exists in baseline models or demonstrate that the proposed losses actually reduce it."}, "questions": {"value": "- majorly what I’m not sure is what percentage of the inference speedup is due to the losses themselves versus the engineering optimizations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1mSm3oS2T7", "forum": "nY91ZOfB5M", "replyto": "nY91ZOfB5M", "signatures": ["ICLR.cc/2026/Conference/Submission23256/Reviewer_tNbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23256/Reviewer_tNbn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898930287, "cdate": 1761898930287, "tmdate": 1762942577489, "mdate": 1762942577489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is motivated by a problem of expert specialization progressively deteriorates due to expert overlap (redundant functionality) and routing ambiguity (inconsistent token assignments)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "While the problem discussed is reasonable, but the solution is not clear or complete. The authors aim to eliminate ambiguity by eliminating routing function, but the presentation and model formulation didnt help me to understand the work. They provided several propositions to proof the claims, however, it is difficult to see how these solve the ambiguity and specializations."}, "weaknesses": {"value": "Please see the questions section."}, "questions": {"value": "The paper is difficult to evaluate because important details is missing or the presentation of the proposed model is weak. For example, there's no reason to believe that forcing expert activations to be orthogonal is better than, encouraging regularization in expert weights. \n\nThe main formulation of the proposed model (Eq. 3) only holds for the W_{down} matrix, and nothing about W_{gate} or W_{up}, which are the important components of a SwiGLU expert.  Indeed, the model uses a SwiGLU expert (Eq. 2), which has three weight matrices W_{gate}, W_{up}, and W_{down}. But, there is no reference or justification for the assumption that orthogonalizing the gradients for W_{down} will orthogonalize the gradients for W_{gate} and W_{up}.  \n\nMoreover, as stated in the paper, the loss_{sp} (Eq. 4) forcing expert activations to be orthogonal. So, what happens if the solution for a given token requires two or more experts to produce similar activations? \n\nAs stated R_{cp} promotes functional diversity. Where is the link between orthogonal router and functionally diverse experts? I couldnt see this justification in the proposed model. \n\nThe most critical experiments (ablations and throughput) are not available in the main paper, but provided in the supplementary file. They should be in the main paper.  As i found the proposed contributions adds only 0.08 PPL improvement (baseline + z-loss:12.07vs. baseline + z-loss + full model:11.99 PPL). The paper needs a clear discussion to justify this minimal gain and to help readers understand the contribution of each proposed component."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aJOvFBYKjZ", "forum": "nY91ZOfB5M", "replyto": "nY91ZOfB5M", "signatures": ["ICLR.cc/2026/Conference/Submission23256/Reviewer_yugS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23256/Reviewer_yugS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038451693, "cdate": 1762038451693, "tmdate": 1762942577272, "mdate": 1762942577272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper introduces two auxiliary losses to address expert overlap and routing ambiguity without architectural modifications: (1) an intra-layer specialization loss $R_{sp}$ that penalizes cosine similarity between expert activations on identical tokens, encouraging orthogonal representations, and (2) a cross-layer coupling loss $R_{cp}$ that maximizes joint routing probabilities across adjacent layers to establish stable expert pathways.\n\n- Proposition 1 establishes that orthogonal expert activations induce orthogonal parameter gradients, ensuring divergent learning trajectories. Proposition 2 demonstrates that strong cross-layer coupling propagates specialization through network depth, with layer $l+1$ inheriting specialization structure from layer $l$ with bounded error.\n\n- Experiments across Vanilla MoE and DeepSeek-style architectures show validation perplexity reductions of in pre-training and average improvements of 1.4% on supervised fine-tuning tasks (Qwen3-30B)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces an original perspective by treating expert specialization as a direct training objective rather than an architectural property. The intra-layer specialization loss $R_{sp}$ provides a principled mechanism to enforce functional diversity through activation orthogonality, linking representational geometry to gradient dynamics. This training-loss-centric approach is more flexible than architectural modifications, as it can be applied to any MoE variant without structural changes.\n\n- The cross-layer coupling loss $R_{cp}$ represents a creative insight that transforms an emergent phenomenon into an explicit learning objective. Proposition 2 establishes that strong inter-layer routing correlation propagates specialization through network depth (Equation 5), providing both theoretical justification and a mechanism for network-wide expert differentiation. \n\n- A significant practical strength is the proven compatibility with standard MoE training objectives. Propositions 3-4 formally establish that $R_{sp}$ and $R_{cp}$ operate orthogonally to load-balancing constraints—specialization determines partition structure while load balancing constrains partition sizes—enabling simultaneous optimization. The implementation as a single-flag Megatron-LM module with no router/architecture modifications, combined with consistent gains across both Vanilla and DeepSeek-style MoE (Table 2), demonstrates broad applicability."}, "weaknesses": {"value": "- While the paper claims \"consistent gains,\" the actual improvements are quite modest and often within noise margins. In Table 2, perplexity reductions range from only 0.09-0.38 points (0.7-1.9% relative), and Table 3 shows zero-shot accuracy improvements of merely 0.3-0.8% on average with no statistical significance testing. More critically, individual benchmark results are inconsistent—in Table 3, ARC-C actually degrades for Vanilla MoE with $L_{lb,z,sp,cp}$ (0.198 vs 0.204 baseline), and Table 8 shows numerous tasks where performance decreases. \n\n- The paper insufficiently distinguishes its contributions from existing work that already addresses expert specialization. DeepSeekMoE's auxiliary-loss-free load balancing explicitly promotes specialization by dynamically adjusting expert capacities based on routing entropy. The claim that \"targeted loss functions can compete with or surpass architectural variants\" is undermined by Table 2 showing DeepSeek-style MoE with only $L_{lb}$ (9.56) outperforms Vanilla MoE with full $L_{lb,z,sp,cp}$ (9.42) at large scale, suggesting architectural choices remain more impactful than the proposed losses.\n\n- The theoretical analysis relies on overly restrictive assumptions that may not hold in practice. Proposition 2's conditions require \"nearly orthogonal router weights\" and extremely high routing confidence, but no empirical verification is provided that these conditions are satisfied during actual training. The proof merely shows specialization *can* propagate under ideal conditions, not that $R_{cp}$ reliably creates these conditions. \n\n- The paper fails to quantify the computational overhead of computing $R_{sp}$ and $R_{cp}$, which require computing pairwise cosine similarities across all activated expert pairs (quadratic in $k$) and cross-layer routing probabilities for all expert pairs . While claiming \"plug-and-play,\" the paper provides no wall-clock training time comparisons, memory overhead analysis, or discussion of how these losses scale to models with hundreds of experts (like Mixture of Million Experts)."}, "questions": {"value": "- Can you provide error bars, confidence intervals, or results from multiple random seeds to demonstrate that the reported improvements  are statistically significant rather than optimization noise? \n\n- Can you empirically verify that Proposition 2's assumptions hold during training—specifically, measure the actual values of $\\epsilon$ (router weight orthogonality), $\\delta$ (representation continuity), and $\\iota$ (routing confidence) at different training stages? More importantly, can you provide direct comparisons showing that your method induces more expert specialization than existing mechanisms like DeepSeekMoE's auxiliary-loss-free load balancing, using quantitative metrics such as expert output diversity, routing entropy, or gradient orthogonality measurements?\n\n- What is the actual wall-clock training time overhead and memory cost of computing $R_{sp}$ and $R_{cp}$ compared to baseline training, especially as the number of experts scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BN0ohKY45Y", "forum": "nY91ZOfB5M", "replyto": "nY91ZOfB5M", "signatures": ["ICLR.cc/2026/Conference/Submission23256/Reviewer_pvd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23256/Reviewer_pvd3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23256/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120679979, "cdate": 1762120679979, "tmdate": 1762942576988, "mdate": 1762942576988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}