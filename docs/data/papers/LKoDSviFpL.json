{"id": "LKoDSviFpL", "number": 2345, "cdate": 1757061490633, "mdate": 1759898154679, "content": {"title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation", "abstract": "In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook (SGHC) that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within the research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel in both terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under the LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs. Our code and models will be open source.", "tldr": "This paper introduces a unified image tokenizer, named SemHiTok, which achieve a better trade-off between semantic information and texture information,  and obtains competitive performance on the unified MLLM based on this training.", "keywords": ["unified image tokenizer", "multimodal learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86a6e762e433569583c4a4198c0afb7dc1e038e9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a unified tokenzier for both image understanding and generation. By linking a set of pixel code with a semantic code, the proposed SemHiTok makes a good trade-off between semantic image understanding and pixel image generation. Experiments on image reconstruction, understanding and generation confirm its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation and illustration of the proposed method are clear and easy to follow.\n- Experiment results are good on multiple tasks, including image reconstruction, understanding and generation.\n- The ablation experiments are clear to demonstrate the effect of each component."}, "weaknesses": {"value": "- Image reconstruction. The rFID is not enough to prove the real performance of a tokenizer for image reconstruction. Other metrics like PSNR are encouraged. Besides, unified tokenizers like UniTok, MUSE-VL should be compared in Tab.1.\n- Training setting. Most unified models are jointly trained on a mixture of multimodal understanding and text-to-image data, the proposed method are only trained on LLaVA-1.5 and text-to-image settings seperately, which may not reflect the relation between image understanding and generation under a unified model.\n- Generation benchmarks. Recent popular benchmarks such as Gen-Eval and DPG are missing. Besides, it's better to include more recent methods on Tab.4."}, "questions": {"value": "no."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GoLlidpVcq", "forum": "LKoDSviFpL", "replyto": "LKoDSviFpL", "signatures": ["ICLR.cc/2026/Conference/Submission2345/Reviewer_bysp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2345/Reviewer_bysp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718977038, "cdate": 1761718977038, "tmdate": 1762916201703, "mdate": 1762916201703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SemHiTok, an image tokenizer for unified MLLMs, which provides both semantic features and pixel-level features for multimodal understanding and generation tasks. The tokenizer features a hierarchical architecture, where an image is first quantized into semantic codes and then the pixel-level tokens are selected based on the corresponding pixel sub-codebook. Experiments demonstrate the effectiveness of SemHiTok on both image understanding & generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The concept of the hierarchical codebook for high-level semantic features and low-level pixel features is novel, and well-motivated. The method decouples the optimization of the two conflicting objectives for semantics and pixel-level details.\n* There are comprehensive experiments which effectively demonstrate SemHiTok's strong performance both at the tokenizer level and in its application on a unified MLLM."}, "weaknesses": {"value": "* While the selection of pixel sub-codebooks is guided by the semantic codes, the training of the pixel-branch is decoupled, so the link between the two hierarchical levels is weak. The authors argue that SemHiTok is superior to naively combining two separately trained tokenizers (in Table 6 Exp 3 & 4), but the conceptual difference feels incremental. Perhaps the authors should provide a more explicit discussion on the specific advantages of this hierarchical design over a simpler, two-stage concatenation approach.\n\n* Though the authors state that SemHiTok avoids codebook overexpansion, the codebook size of SemHiTok is still large (196608), much larger than baseline methods. So the comparison of performances may not be fair, since a much larger codebook inherently allows for higher-fidelity reconstruction.\n* There are several grammatical errors, and maybe some mistakes in the formulations (Eq 2, 5)."}, "questions": {"value": "About the observation that image patches with the same semantic code tend to have similar pixel feature. Are there any quantitative verification (e.g., specific metrics or analysis) for this observation? This would better solidify the motivation for using the semantic code to index pixel sub-codebooks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DBYUlwoWjK", "forum": "LKoDSviFpL", "replyto": "LKoDSviFpL", "signatures": ["ICLR.cc/2026/Conference/Submission2345/Reviewer_HDbp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2345/Reviewer_HDbp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720808085, "cdate": 1761720808085, "tmdate": 1762916201206, "mdate": 1762916201206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SemHiTok, a unified image tokenizer enhanced by semantic information guidance. It innovatively introduces a hierarchical codebook structure, which builds a pixel sub-codebook based on a pre-trained semantic codebook. The semantic part and the pixel part are trained separately to decouple the structure and training strategy. This enables the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Additionally, SemHiTok is applied to the MLLM structure, and the experimental results demonstrate the performance of this method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The experiments are comprehensive, comparing with multiple state-of-the-art (SOTA) models and demonstrating the superiority of the SemHiTok method.\n- The writing language is accessible, and the diagrams are clear.\n- As a Tokenizer, it underwent complete training on the MLLM architecture, validating its effectiveness."}, "weaknesses": {"value": "- The paper only conducted ablation experiments on the joint training vs. phased training of the SemHiTok architecture, but did not compare joint training and phased training across other methods. Thus, the conclusion that \"Joint training degrades performance\" lacks sufficient support.\n- The roles of the sub-codebook and phased training have not been ablated individually in the experiments, making it insufficient to demonstrate the degree of validity of each component on its own."}, "questions": {"value": "- Typo: In the header of Table 5, \"Ehance\" should be corrected to \"Enhance\".\n- How is the size m of the sub-codebook determined, and does it impact the model's performance?\n- Is it necessary to retrain the entire LLM to verify SemHiTok's capabilities? Perhaps one can only replace the Tokenizer in the existing MLLM (Multimodal Large Language Model) structure. Is this new MLLM architecture necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "esJLASVS52", "forum": "LKoDSviFpL", "replyto": "LKoDSviFpL", "signatures": ["ICLR.cc/2026/Conference/Submission2345/Reviewer_JSAk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2345/Reviewer_JSAk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978018605, "cdate": 1761978018605, "tmdate": 1762916200938, "mdate": 1762916200938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SemHiTok, a unified image tokenizer designed to effectively capture both high-level semantic features for understanding and low-level pixel details for generation. The authors identify a key challenge in joint training: the inherent conflict between the feature priorities of multimodal understanding and generation tasks. To bridge this gap, they propose a novel Semantic-Guided Hierarchical Codebook (SGHC), which employs a set of sub-codebooks to model the pixel-level space under the guidance of each semantic code. A notable advantage of SemHiTok is its compatibility with existing next-token-prediction-based MLLMs, achieved through a straightforward codebook flattening operation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-structured and clearly written, making it a pleasure to read. The core idea of SemHiTok is both novel and compelling, presenting a fresh perspective on the problem. Consequently, I have no major questions regarding the technical content presented. Should I have overlooked any aspect of the work, I welcome the authors to clarify it in their rebuttal."}, "weaknesses": {"value": "Although this paper proposes an alternative method to bridge the gap between low-level visual cues and high-level semantic features, I still have several concerns.\n\nFirst, the hierarchical structure significantly increases the codebook size. As shown in Table 1, SemHiTok has a total size of K Ã— M = 196,000. It appears impractical to expand this codebook further. However, this already large size limits the value of K. I wonder whether a small K is sufficient to represent the full diversity of semantics in visual content.\n\nSecond, the comparisons regarding codebook capacity do not seem entirely fair. As shown in the table, methods like FQGAN and IBQ achieve better reconstruction quality with the same resolution and codebook dimension, yet have a smaller codebook size. This raises a question: given abundant data (far beyond ImageNet-50K), would simply using a large enough standard codebook be sufficient, thereby diminishing the necessity of the proposed SemHiTok? In other words, can SemHiTok maintain its competitiveness under such conditions? For instance, the EMU series (e.g., the recently released EMU-3.5) employs a very large visual codebook and massive pre-training data, which leads to strong performance.\n\nI would appreciate it if the authors could address these concerns."}, "questions": {"value": "* Does the authors mention the specific value of K and m for the SemHiTok's codebook? Have you ever carried out ablation of different value sets of (K, m) and their effectiveness? It could bring more insights if the authors could provide more details about how they decide the scale of codebook."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xSwWglMJwK", "forum": "LKoDSviFpL", "replyto": "LKoDSviFpL", "signatures": ["ICLR.cc/2026/Conference/Submission2345/Reviewer_YG9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2345/Reviewer_YG9D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991647056, "cdate": 1761991647056, "tmdate": 1762916200528, "mdate": 1762916200528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}